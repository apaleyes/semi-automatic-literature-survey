paperId,url,title,abstract,venue,year,externalIds.DOI,database,query_name,query_value
889150e1a465317ce1794ff2ead606023d5fdb3f,https://www.semanticscholar.org/paper/889150e1a465317ce1794ff2ead606023d5fdb3f,Section focused on machine learning methods for high-level cognitive capabilities in robotics,"Integrating highand low-level cognitive capabilities is essential for developing robotic systems that can adaptively act in our daily environment in active collaboration with humans. Recent advances in machine learning techniques, including deep learning and hierarchical Bayesian modeling, are providing us with new possibilities to integrate highand low-level cognitive capabilities in robotics. It became clear that such learning methods are indispensable to create robots that can effectively address uncertaintieswhile acting smart in the realworld. We had organized workshops, named ‘Machine Learning Methods for High-Level Cognitive Capabilities in Robotics,’ in IROS 2016 and 2017. In the workshops, we solicited excellent papers related to the demand for accelerating the synergies of lowand high-level cognitive capabilities. It would enable us to develop methods that address real-world problems in a more robust manner. Hence, we aim to share knowledge regarding stateof-the-art machine learning methods that contribute to modeling sensory-motor and cognitive capabilities in robotics and to exchange views among cutting-edge robotics researchers with a special emphasis on adaptive high-level cognition. Through the workshops, researchers from cognitive robotics, speech processing, artificial intelligence, machine learning, computer vision, natural language processing, and so on were gathered to discuss the current challenges in machine learning methods for highlevel cognitive capabilities in robotics. Typical keywords discussed in the workshop were as follows: multimodal communication, learning motor skills and segmentation of time-series information, concept formation, probabilisticmodels, language acquisition, human–robot communication and collaboration, deep learning, the theory of mind and model of others, skill transfer, Bayesian modeling, application in communicable service robots, and so on. These keywords should be organized using Figure 1, which was used in the workshop discussion in 2017. Our daily environment is full of uncertainties, with complex objects and challenging tasks. A robot is not only required to deal with things appropriately in a physical manner, but also perform logical and linguistic tasks in the real world. Consider a scenario where a human user tells a robot, ‘pleasemove it into the blue box.’ In addition to solving a manipulation task, the robot must move the target object to a particular blue box and estimatewhat ‘it’ represents. In addition to solving the manipulation task, the robot should estimate the meaning of ‘into,’ representing the relationship between ‘it’ and ‘the blue box’ in a real-world environment. When a robot attempts to communicate and collaborate with human users in a realworld environment, bridging highand low-level cognitive capabilities is critical. Low-level cognitive capabilities include physical control, behavioral motion generation, and sensory perception (node (i) in Figure 1). In contrast, high-level cognitive capabilities include logical inference, planning, and language (node (ii) in Figure 1). Conventionally, symbol-based and/or rule-based approaches have been employed to model high-level cognitive capabilities in robotics. However, it has been reported that such conventional methods could not create a robot that could address inevitable uncertainties in the physical environment and natural human–robot communications. In other words, the difficulty of direct transformation between nodes (i) and (ii) was the major cause of the low performance of natural human–robot communications. However, recent advances in machine learning techniques have provided a bridge betweennode (i) and the top node, and between node (ii) and the top node, as shown in Figure 1. It has become increasingly clear that machine learning methods are indispensable for creating robots that address uncertainties. In addition to machine learning techniques, big data in human–robot interactions through a virtual reality environment can be applied to accelerate the learning process to connect the highand low-level cognitive capabilities. We organized a new type of submission strategy at the second workshop in 2017. Authors could choose from two submission categories:",Adv. Robotics,2019,10.1080/01691864.2019.1625183,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9cd80a501ea586086f614f08667e7ed7533a1204,https://www.semanticscholar.org/paper/9cd80a501ea586086f614f08667e7ed7533a1204,From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence,"Machine learning has long since become a keystone technology, accelerating science and applications in a broad range of domains. Consequently, the notion of applying learning methods to a particular problem set has become an established and valuable modus operandi to advance a particular field. In this article we argue that such an approach does not straightforwardly extended to robotics — or to embodied intelligence more generally: systems which engage in a purposeful exchange of energy and information with a physical environment. In particular, the purview of embodied intelligent agents extends significantly beyond the typical considerations of main-stream machine learning approaches, which typically (i) do not consider operation under conditions significantly different from those encountered during training; (ii) do not consider the often substantial, long-lasting and potentially safety-critical nature of interactions during learning and deployment; (iii) do not require ready adaptation to novel tasks while at the same time (iv) effectively and efficiently curating and extending their models of the world through targeted and deliberate actions. In reality, therefore, these limitations result in learning-based systems which suffer from many of the same operational shortcomings as more traditional, engineering-based approaches when deployed on a robot outside a well defined, and often narrow operating envelope. Contrary to viewing embodied intelligence as another application domain for machine learning, here we argue that it is in fact a key driver for the advancement of machine learning technology. In this article our goal is to highlight challenges and opportunities that are specific to embodied intelligence and to propose research directions which may significantly advance the state-of-the-art in robot learning.",ArXiv,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6d00d7f4e89c05f87c0519790313761f8a8fc425,https://www.semanticscholar.org/paper/6d00d7f4e89c05f87c0519790313761f8a8fc425,Cooperative Robotics and Machine Learning for Smart Manufacturing: Platform Design and Trends Within the Context of Industrial Internet of Things,"Internet of Things (IoT) in industrial settings now leads to the development of a new generation of systems designed to improve the operational efficiency of the new paradigm of smart manufacturing plants. Thereby, the current article introduces in detail the definitions, concepts, standards, and other important aspects related to smart manufacturing, cooperative robotics, and machine learning techniques. The paper highlights the opportunities presented by the new paradigm and the challenges faced in effectively implementing it in the industrial context. Especially, the focus is on the challenges associated with the architectures, communications technology, and protocols that enable the integration and deployment of machine learning algorithms to improve the execution of cooperative tasks performed daily by human operators, machines, and robots. The article also provides a systematic review of state-of-the-art research efforts for the fields aforementioned. Finally, an architecture for integrating collaborative robotics and machine learning based on six layers and four hierarchies of the RAMI 4.0 (Reference Architectural Model Industry 4.0) is presented.",IEEE Access,2021,10.1109/ACCESS.2021.3094374,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1d3539a8d94bd3ab78993d7cc584efc06ed0e460,https://www.semanticscholar.org/paper/1d3539a8d94bd3ab78993d7cc584efc06ed0e460,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",NeurIPS Datasets and Benchmarks,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",ArXiv,2016,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,https://www.semanticscholar.org/paper/ed9b2c4d4ea93d86a6e1167c9e92cfa0a243c4bc,Challenges and Opportunities for Unikernels in Machine Learning Inference,"Machine Learning has become a value creator for many new and old businesses. However, efficient realworld machine learning deployments are still a challenge. Traditional Machine Learning deployments suffer from efficient resource utilization and achieving predictable latency. They cannot be treated in the same manner as other application server deployments. Unikernels are a method to specialize application deployment and performance to suit the needs of the application. Traditionally, building or porting applications to unikernels have been challenging. However, recent work has been into simplifying the development of unikernels. Real-world Unikernels as of now are only for specializing applications that run on the CPU. We survey machine learning practitioners and find out that the majority of machine learning practitioners are using the CPU for machine learning deployments, thus, creating an opportunity for unikernels to optimize the performance of these applications. We compare the architecture of two unikernels: nanos and Unikraft. We benchmarked scikit-learn, a popular machine library, inside a unikernel and found that it only offered a 1% advantage over a traditional deployment. However, our testing could not include more innovative systems like Unikraft due to their immaturity and inability to run machine learning libraries. We include a dependency analysis of three popular machine learning libraries Tensorflow Lite, PyTorch and ONNX, to help pave the way for building machine learning applications as Unikraft unikernels.","2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",2021,10.1109/icrito51393.2021.9596080,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
56e63ffea11c875f7eee257798e18cd04e453b6c,https://www.semanticscholar.org/paper/56e63ffea11c875f7eee257798e18cd04e453b6c,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b14558c0b727af0ac9086f463b6030b9072dbe16,https://www.semanticscholar.org/paper/b14558c0b727af0ac9086f463b6030b9072dbe16,Methods for Automatic Machine-Learning Workflow Analysis,,ECML/PKDD,2021,10.1007/978-3-030-86517-7_4,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f5792d3c5e53f406b1aee6aae4505792b23bfcc0,https://www.semanticscholar.org/paper/f5792d3c5e53f406b1aee6aae4505792b23bfcc0,Dynamic Placement of Rapidly Deployable Mobile Sensor Robots Using Machine Learning and Expected Value of Information,"
 Although the Industrial Internet of Things has increased the number of sensors permanently installed in industrial plants, there will be gaps in coverage due to broken sensors or sparse density in very large plants, such as in the petrochemical industry. Modern emergency response operations are beginning to use Small Unmanned Aerial Systems (sUAS) that have the ability to drop sensor robots to precise locations. sUAS can provide longer-term persistent monitoring that aerial drones are unable to provide. Despite the relatively low cost of these assets, the choice of which robotic sensing systems to deploy to which part of an industrial process in a complex plant environment during emergency response remains challenging.
 This paper describes a framework for optimizing the deployment of emergency sensors as a preliminary step towards realizing the responsiveness of robots in disaster circumstances. AI techniques (Long short-term memory, 1-dimensional convolutional neural network, logistic regression, and random forest) identify regions where sensors would be most valued without requiring humans to enter the potentially dangerous area. In the case study described, the cost function for optimization considers costs of false-positive and false-negative errors. Decisions on mitigation include implementing repairs or shutting down the plant. The Expected Value of Information (EVI) is used to identify the most valuable type and location of physical sensors to be deployed to increase the decision-analytic value of a sensor network. This method is applied to a case study using the Tennessee Eastman process data set of a chemical plant, and we discuss implications of our findings for operation, distribution, and decision-making of sensors in plant emergency and resilience scenarios.","Volume 13: Safety Engineering, Risk, and Reliability Analysis; Research Posters",2021,10.1115/imece2021-70759,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
feaeda74182d7f9890a6ff5a1733ee21a288cbd3,https://www.semanticscholar.org/paper/feaeda74182d7f9890a6ff5a1733ee21a288cbd3,Electronic skins and machine learning for intelligent soft robots,"Developments in e-skins and machine learning may achieve tactile sensing and proprioception for autonomous, deployable soft robots. Soft robots have garnered interest for real-world applications because of their intrinsic safety embedded at the material level. These robots use deformable materials capable of shape and behavioral changes and allow conformable physical contact for manipulation. Yet, with the introduction of soft and stretchable materials to robotic systems comes a myriad of challenges for sensor integration, including multimodal sensing capable of stretching, embedment of high-resolution but large-area sensor arrays, and sensor fusion with an increasing volume of data. This Review explores the emerging confluence of e-skins and machine learning, with a focus on how roboticists can combine recent developments from the two fields to build autonomous, deployable soft robots, integrated with capabilities for informative touch and proprioception to stand up to the challenges of real-world environments.",Science Robotics,2020,10.1126/scirobotics.aaz9239,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2e37ac9d31db380ad95f96b400ed33cca100d58d,https://www.semanticscholar.org/paper/2e37ac9d31db380ad95f96b400ed33cca100d58d,Deep Machine Learning Model Trade-Offs for Malaria Elimination in Resource-Constrained Locations,"The success of deep machine learning (DML) models in gaming and robotics has increased its trial in clinical and public healthcare solutions. In applying DML to healthcare problems, a special challenge of inadequate electrical energy and computing resources exists in regional and developing areas of the world. In this paper, we evaluate and report the computational and predictive performance design trade-offs for four candidate deep learning models that can be deployed for rapid malaria case finding. The goal is to maximise malaria detection accuracy while reducing computing resource and energy consumption. Based on our experimental results using a blood smear malaria test data set, the quantised versions of Basic Convolutional Neural Network (B-CNN) and MobileNetV2 have better malaria detection performance (up to 99% recall), lower memory usage (2MB 8-bit quantised model) and shorter inference time (33–95 microseconds on mobile phones) than VGG-19 fine-tuned and quantised models. Hence, we have implemented MobileNetV2 in our mobile application as it has even a lower memory requirement than B-CNN. This work will help to counter the negative effects of COVID-19 on the previous successes towards global malaria elimination.",Bioengineering,2021,10.3390/bioengineering8110150,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
09742328a8fec880122dcb97c307037495c2db44,https://www.semanticscholar.org/paper/09742328a8fec880122dcb97c307037495c2db44,Development of Magnetic-Based Navigation by Constructing Maps Using Machine Learning for Autonomous Mobile Robots in Real Environments,"Localization is fundamental to enable the use of autonomous mobile robots. In this work, we use magnetic-based localization. As Earth’s geomagnetic field is stable in time and is not affected by nonmagnetic materials, such as a large number of people in the robot’s surroundings, magnetic-based localization is ideal for service robotics in supermarkets, hotels, etc. A common approach for magnetic-based localization is to first create a magnetic map of the environment where the robot will be deployed. For this, magnetic samples acquired a priori are used. To generate this map, the collected data is interpolated by training a Gaussian Process Regression model. Gaussian processes are nonparametric, data-drive models, where the most important design choice is the selection of an adequate kernel function. These models are flexible and generate mean predictions as well as the confidence of those predictions, making them ideal for their use in probabilistic approaches. However, their computational and memory cost scales poorly when large datasets are used for training, making their use in large-scale environments challenging. The purpose of this study is to: (i) enable magnetic-based localization on large-scale environments by using a sparse representation of Gaussian processes, (ii) test the effect of several kernel functions on robot localization, and (iii) evaluate the accuracy of the approach experimentally on different large-scale environments.",Sensors,2021,10.3390/s21123972,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3bdb68d90e6d3d6987c0dcd31466e26895dc3544,https://www.semanticscholar.org/paper/3bdb68d90e6d3d6987c0dcd31466e26895dc3544,Crop leaf disease detection and classification using machine learning and deep learning algorithms by visual symptoms: a review,"A Quick and precise crop leaf disease detection is important to increasing agricultural yield in a sustainable manner. We present a comprehensive overview of recent research in the field of crop leaf disease prediction using image processing (IP), machine learning (ML) and deep learning (DL) techniques in this paper. Using these techniques, crop leaf disease prediction made it possible to get notable accuracies. This article presents a survey of research papers that presented the various methodologies, analyzes them in terms of the dataset, number of images, number of classes, algorithms used, convolutional neural networks (CNN) models employed, and overall performance achieved. Then, suggestions are prepared on the most appropriate algorithms to deploy in standard, mobile/embedded systems, Drones, Robots and unmanned aerial vehicles (UAV). We discussed the performance measures used and listed some of the limitations and future works that requires to be focus on, to extend real time automated crop leaf disease detection system.",International Journal of Electrical and Computer Engineering (IJECE),2022,10.11591/ijece.v12i2.pp2079-2086,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
895b44332025719ee99c7ca547041608de1537b1,https://www.semanticscholar.org/paper/895b44332025719ee99c7ca547041608de1537b1,Crop leaf disease detection and classification using machine learning and deep learning algorithms by visual symptoms-a review,"A Quick and precise crop leaf disease detection is important to increasing agricultural yield in a sustainable manner. We present a comprehensive overview of recent research in the field of crop leaf disease prediction using IP, ML and DL techniques in this paper. Using these techniques, crop leaf disease prediction made it possible to get notable accuracies. This article presents a survey of research papers that presented the various methodologies, analyzes them in terms of the dataset, number of images, number of classes, algorithms used, CNN models employed, and overall performance achieved. Then, suggestions are prepared on the most appropriate algorithms to deploy in standard, mobile/embedded systems, Drones, Robots and UAVs. We discussed the performance measures used and listed some of the limitations and future works that requires to be focus on, to extend real time automated crop leaf disease detection system.",,2022,10.11591/IJECE.V12I2.PP%P,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fde36897ea2fae566acde9e512db9a057243f971,https://www.semanticscholar.org/paper/fde36897ea2fae566acde9e512db9a057243f971,Machine Learning for Wireless Distance Estimation Model Parameter Estimation for Breadcrumb Localization Applications,"Autonomous robots commonly rely on global positioning systems (GPS) and easily recognizable, and re-identifiable, landmarks in order to achieve localization. These elements of infrastructure are not always available in the environments where autonomous robots operate. GPS is easily denied and obstructed due to its low power design. Finding and re-observing unique landmarks in an environment can also be problematic in landscapes with limited unique features. This work builds upon a system of deploy-able electronic way-points, known as BreadCrumbs [1]. Designed to provide path planning and localization information in outdoor environments through the use of GPS, these BreadCrumbs are expanded to be capable of self-localization and the ability to provide adaptive path-loss-exponent determination based on environmental placement via a Deep Deterministic Policy Gradient algorithm for use in RSSI based range determination. Thus, enabling the BreadCrumbs to act as easily re-identifiable landmarks in a variety of environments where GPS may not be accessible and natural landmarks are sparse. We present an analysis of this system in an indoor lab and outdoor forest trail setting.",SoutheastCon 2022,2022,10.1109/SoutheastCon48659.2022.9763940,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ce6caebf04a2acbf1bd23c14980d549b290c7479,https://www.semanticscholar.org/paper/ce6caebf04a2acbf1bd23c14980d549b290c7479,Machine Learning Approach for Tensegrity Form Finding: Feature Extraction Problem,"In this paper, the problem of form finding for a tensegrity structure with the use of a machine learning pipeline, including feature extraction and regression, is studied. Tensegrity robots present a range of new opportunities in a number of areas in robotics, however the lack of efficient and scalable tools for controlling their motion slows down their deployment. This study aims to add a new tool for solving one of the basic problems encountered in tensegrity robot control: finding a stable equilibrium state (and its inverse task). Using machine learning approach makes the solution scalable and potentially faster than iterative optimization-based methods. The paper provides a view into the issue of feature extraction, and provides implementation of a number of feature extraction methods: Principle Component Analysis (PCA), Kernel PCA, undercomplete, denoising and sparse autoencoders",2020 4th Scientific School on Dynamics of Complex Networks and their Application in Intellectual Robotics (DCNAIR),2020,10.1109/DCNAIR50402.2020.9216799,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
090b361b9a2b5025ab26c328ea3d39d664a82388,https://www.semanticscholar.org/paper/090b361b9a2b5025ab26c328ea3d39d664a82388,Error Resilient Machine Learning for Safety-Critical Systems: Position Paper,"Machine learning (ML) has increasingly been adopted in safety-critical systems such as autonomous vehicles (AVs) and industrial robotics. In these domains, reliability and safety are important considerations, and hence it is critical to ensure the resilience of ML systems to faults and errors. On the other hand, soft errors are becoming more frequent in commodity computer systems due to the effects of technology scaling and reduced supply voltages. Further, traditional solutions for masking hardware faults such as Triple-Modular Redundancy (TMR) are prohibitively expensive in terms of their energy and performance overheads. Therefore, there is a compelling need to ensure the resilience of ML applications to soft errors on commodity hardware platforms.We first experimentally assess the resilience of safety-critical ML applications to soft errors. We demonstrate through fault injection experiments that even a single bit flip due to a soft error can lead to misclassification in Deep Neural Network (DNN) applications deployed in AVs, leading to safety violations. However, not all the errors in an DNN will result in serve consequences such as safety violations, and hence it is sufficient to protect the DNN from the ones that do. Unfortunately, finding all possible errors that result in safety violations is a very compute intensive task. We propose BinFI, a fault injection approach that efficiently injects critical faults that are highly likely to result in safety violations, based on the unique properties of DNNs. Finally, we propose Ranger, an approach to protect DNNs from critical faults with minimal performance overheads and no accuracy loss. We will conclude by presenting some of our ongoing work, and the future challenges in this area.",2020 IEEE 26th International Symposium on On-Line Testing and Robust System Design (IOLTS),2020,10.1109/IOLTS50870.2020.9159749,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8d59142a61f43abfebd1ca4228564fd620e2e936,https://www.semanticscholar.org/paper/8d59142a61f43abfebd1ca4228564fd620e2e936,To deploy trained speech with DNN-LSTM framework for controlling a smart wheeled-robot in limited learning circumstance,,International Journal of Speech Technology,2022,10.1007/s10772-022-09962-z,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0751c7fc17294839c1ec0b1eca3c71672d06e2b9,https://www.semanticscholar.org/paper/0751c7fc17294839c1ec0b1eca3c71672d06e2b9,Intelligent and Reconfigurable Architecture for KL Divergence Based Online Machine Learning Algorithm,"Online machine learning (OML) algorithms do not need any training phase and can be deployed directly in an unknown environment. OML includes multi-armed bandit (MAB) algorithms that can identify the best arm among several arms by achieving a balance between exploration of all arms and exploitation of optimal arm. The Kullback-Leibler divergence based upper confidence bound (KLUCB) is the state-of-the-art MAB algorithm that optimizes exploration-exploitation trade-off but it is complex due to underlining optimization routine. This limits its usefulness for robotics and radio applications which demand integration of KLUCB with the PHY on the system on chip (SoC). In this paper, we efficiently map the KLUCB algorithm on SoC by realizing optimization routine via alternative synthesizable computation without compromising on the performance. The proposed architecture is dynamically reconfigurable such that the number of arms, as well as type of algorithm, can be changed on-the-fly. Specifically, after initial learning, on-the-fly switch to light-weight UCB offers around 10-factor improvement in latency and throughput. Since learning duration depends on the unknown arm statistics, we offer intelligence embedded in architecture to decide the switching instant. We validate the functional correctness and usefulness of the proposed architecture via a realistic wireless application and detailed complexity analysis demonstrates its feasibility in realizing intelligent radios.",ArXiv,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
68299caf20f230f32c40679991e715f4c72177cb,https://www.semanticscholar.org/paper/68299caf20f230f32c40679991e715f4c72177cb,Applied machine learning for multi-sensory robot perception,"In recent years, advances in autonomous robotics have begun to transform how we work and live. Unmanned Aerial Vehicles (UAV) and Unmanned Ground Vehicles are helping us to deliver goods, conduct surveys of construction sites, and perform search and rescue alongside first responders. However, designing robots with this level of autonomy is often challenging due to the complexity of the real-world environment. Multi-sensory perception is a critical component to address this challenge and develop robust autonomous robotic systems. By combining multiple inputs from sensors, the system can eliminate a single point of failure from sensor degradation and generate new insights to make better decisions integrating information from different sensor modalities. Recent breakthroughs in Machine Learning, especially the Deep Neural Network(DNN) based deep learning perception pipelines have been proven effective in a number of robot perception tasks. However, the significant computation cost for Deep Neural Networks is prohibiting their deployment on a robot system with limited power budget and real-time performance requirement. It is important to bridge this gap by optimization to deploy state-of-the-art machine learning models to a real-world robot systems. This work investigates the viability to develop robust multi-sensory robot perception systems enhanced by machine learning models in three different chapters. First, I explore the effectiveness of DNN perception pipelines in object detection and semantic segmentation tasks, then experiment on various model optimization techniques to enhance the efficiency of these perception models, achieving real-time performance on robot system with a limited power budget. Then I elucidate the design and implementation of a thermal sensing robot system that performs sensor fusion of a thermal camera and an RGB-Depth Camera to automatically track occupants in a building, measuring their forehead temperature, providing fine-grain information for better decision making in intelligent Air Conditioning (AC) system.",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
19552c33b6144ba9cf02b52310cfdccdc66b14f2,https://www.semanticscholar.org/paper/19552c33b6144ba9cf02b52310cfdccdc66b14f2,Empirical observation of negligible fairness-accuracy trade-offs in machine learning for public policy,,Nat. Mach. Intell.,2020,10.1038/s42256-021-00396-x,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b14692c1e0658dd1d814c737171ce714b1588360,https://www.semanticscholar.org/paper/b14692c1e0658dd1d814c737171ce714b1588360,An Intelligent UAV Deployment Scheme for Load Balance in Small Cell Networks Using Machine Learning,"In wireless networks, network load can be highly unbalanced due to the mobility of user equipments (UEs). Unmanned Aerial Vehicles (UAVs) supported base station with the advantage of flexible deployment, ubiquitous wireless coverage and high speed data rate, is a promising approach to handle with the foregoing problem. However, how to achieve cost-effective UAV deployment in an autonomous and dynamic manner is a significant challenge. Facing this problem, we propose a novel UAV base station intelligent deployment scheme based on machine learning and evaluate its performance on a realworld dataset. First, we conduct data preprocessing to process, clean, and transform raw data into formatted data. Missing values are filled by Conditional Mean Imputation (CMI) method and outliers are corrected by pauta criterion. Then, we use hybrid approach which contains ARIMA model and XGBoost model. Linear predictions are carried out by ARIMA model and later nonlinear model XGBoost are applied on residue of ARIMA. Resultant prediction is obtained by adding linear and nonlinear prediction, hybrid model is estimated by Root Mean Square Error (RMSE) and R2 score. Finally, according to predicted results, UAV base stations can be deployed to cater for dynamically changing demands in the hotspot areas and achieve cost-effective deployment. Simulation results show that the propose scheme is superior to other benchmark schemes in load balancing.",2019 IEEE Wireless Communications and Networking Conference (WCNC),2019,10.1109/WCNC.2019.8885648,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ce48680767a011cae69e8da8eb5e90ac601e36eb,https://www.semanticscholar.org/paper/ce48680767a011cae69e8da8eb5e90ac601e36eb,"Towards Building Autonomy and Intelligence for Surgical Robotic Systems using Trajectory Optimization, Stochastic Estimation, Vision-based Control, and Machine Learning Algorithms","Author(s): Aghajani Pedram, Sahba | Advisor(s): Rosen, Jacob | Abstract: While the teleoperation framework has been successfully implemented for the surgical robots, especially for soft tissue interventions, the main challenge is that the surgeons are responsible for all actions taken, and all decisions made, during the entire surgery. As the robotics technology explores more complicated surgical interventions, teleoperation framework can become increasingly overwhelmingfor the human surgeons, who inherit limited sensing and motor control bandwidth, and can result in degraded surgical performance. Introduction of automation and intelligence into the robot-assisted interventions, where some of the surgical responsibilities are delegated to the AI agent, can substantially improve this framework and enhance the overall surgical outcome. Amongst many challenges of bringing autonomy into the surgical interventions, the main two technological ones pertain to the complexity of soft tissue environments and the inaccuracies of the surgical robotics systems. This dissertation aims at addressing these two challenges and proposes various solutions for different surgical robotic systems with applications to laparoscopic, orthopedic, and opthalmologic surgeries. Regarding planning of surgical subtasks, suturing and tissue manipulation which occur frequently in soft tissue surgeries are considered. For suturing task, two novel optimization-based needle motion planning algorithms, Fixed Center Motion (FCM) and Moving Center Motion (MCM), are proposed where the tissue trauma is minimized and a wide variety of suturing criteria (i.e., adequate depth) are met. An extensive simulations for each method were provided to (I) confirm the mathematical formulations and (II) to obtain optimal strategies under various suturing conditions. The FCM needle planning was deployed on Raven IV system with an open loop controller (i.e, no vision feedback) and experiment results confirmed the simulation and optimization outcomes. Regarding the tissue manipulation task, a new synergic learning method where human knowledge contributes to selecting intuitive features of tissue manipulation while the algorithm learns to take optimal actions is proposed. The method was tested on four different configurations in the simulations and the robot was able to successfully accomplish the task of tissue manipulation autonomously for all. To improve estimation and control accuracy of three surgical robotics systems, multiple frameworks are proposed. For the first category which pertain to cable-driven serial manipulators used for soft tissue surgeries, a 6 DoF visual servo controller using robot-camera calibration and realtime vision feedback was developed. The framework enabled the Raven IV surgical system to perform autonomous suturing task for various suturing trajectories and tissue compliance. For the second category which pertain to continuum manipulators with applications to orthopedic surgery and bronchoscopy, a novel stochastic sensor fusion algorithm, called Simultaneous Sensor Calibration and Deformation Estimation (SCADE), was introduced. SCADE addresses the problem of estimating calibration bias of FBG sensors as well as the shape/tip of the continuum surgical manipulators simultaneously in real-time. The algorithm was tested to estimate the tip position of a continuum manipulator within free and obstructed environments and showed superior performance compared to estimations from FBG sensor. For the third category which pertain to a robot-assisted cataract surgery system, a new hardware and software solution was proposed to estimate the tip location of surgical tools inside the eye during cataract surgery. The framework was developed andtested using a total of 31 pig eyes and results demonstrated efficacy of the proposed solution.",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8b7abbb4a79a8209f3c5642ee8504864ad62bfb8,https://www.semanticscholar.org/paper/8b7abbb4a79a8209f3c5642ee8504864ad62bfb8,Real-Time Error Detection in Nonlinear Control Systems Using Machine Learning Assisted State-Space Encoding,"Successful deployment of autonomous systems in a wide range of societal applications depends on error-free operation of the underlying signal processing and control functions. Real-time error detection in nonlinear systems has mostly relied on redundancy at the component or algorithmic level causing expensive area and power overheads. This paper describes a real-time error detection methodology for nonlinear control systems for detecting sensor and actuator degradations as well as malfunctions due to soft errors in the execution of the control algorithm on a digital processor. Our approach is based on creation of a redundant check state in such a way that its value can be computed from the current states of the system as well as from a history of prior observable state values and inputs (via machine learning algorithms). By checking for consistency between the two, errors are detected with low latency. The method is demonstrated on two test case simulations - an inverted pendulum balancing problem and a sliding mode controller driven brake-by-wire (BBW) system. In addition, hardware results from error injection experiments in an ARM core representation on an FPGA and artificial sensor degradations on a self-balancing robot prove the practical feasibility of implementation.",IEEE Transactions on Dependable and Secure Computing,2019,10.1109/TDSC.2019.2903049,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2601002259990313f895dd22d80fa8fe57b7e7b1,https://www.semanticscholar.org/paper/2601002259990313f895dd22d80fa8fe57b7e7b1,Machine Learning Meets Quantitative Planning: Enabling Self-Adaptation in Autonomous Robots,"Modern cyber-physical systems (e.g., robotics systems) are typically composed of physical and software components, the characteristics of which are likely to change over time. Assumptions about parts of the system made at design time may not hold at run time, especially when a system is deployed for long periods (e.g., over decades). Self-adaptation is designed to find reconfigurations of systems to handle such run-time inconsistencies. Planners can be used to find and enact optimal reconfigurations in such an evolving context. However, for systems that are highly configurable, such planning becomes intractable due to the size of the adaptation space. To overcome this challenge, in this paper we explore an approach that (a) uses machine learning to find Pareto-optimal configurations without needing to explore every configuration and (b) restricts the search space to such configurations to make planning tractable. We explore this in the context of robot missions that need to consider task timeliness and energy consumption. An independent evaluation shows that our approach results in high-quality adaptation plans in uncertain and adversarial environments.",2019 IEEE/ACM 14th International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),2019,10.1109/SEAMS.2019.00015,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
24da156dc92b1f5961f12c54c70df1455e1dc84c,https://www.semanticscholar.org/paper/24da156dc92b1f5961f12c54c70df1455e1dc84c,Identification and classification of materials using machine vision and machine learning in the context of industry 4.0,,J. Intell. Manuf.,2020,10.1007/s10845-019-01508-6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1af0c93aeb39fe131d651e2480654507d3394aba,https://www.semanticscholar.org/paper/1af0c93aeb39fe131d651e2480654507d3394aba,Role of machine learning in changing social and business eco-system – a qualitative study to explore the factors contributing to competitive advantage during COVID pandemic,"
Purpose
“Machine learning (ML)” in business aids in increasing company scalability and boosting company operations for businesses all over the world. “Artificial intelligence (AI)” technologies and several “ML” algorithms have grown in prominence in the business analytics sector. In the era of a huge quantum of data being generated by the virtue of the integration of the various software with the business operations, the relevance of “ML” is continuously increasing. As a result, companies may now profit from knowing how companies may use “ML” and incorporating it into their own operations. “ML” derives useful results from the data to address very dynamic and difficult social and business problems. ML helps in establishing a system that learns automatically and produces results in less time and effort, allowing machines to discover. ML is developing at a breakneck pace, fuelled mostly by new computer technology to competitive advantages during the COVID pandemic.


Design/methodology/approach
For firms all around the world, “ML” in business aids in expanding scalability and boosting operations. In the field of business analytics, artificial intelligence (AI) and machine learning (ML) algorithms have become increasingly popular. The importance of “ML” is growing in an era when a massive amount of data is generated as a result of the integration of various applications with company activities. As a result, businesses can now benefit from understanding how other businesses are using “ML” and adopting it into their own operations. In order to handle very dynamic and demanding societal and business challenges, machine learning (ML) extracts valuable results from data. Machine learning (ML) aids in the development of a system that learns automatically and generates outcomes with less time and effort, allowing machines to discover. ML is progressing at a dizzying pace, fueled primarily by new computer technology and used to gain competitive advantages during the COVID pandemic.


Findings
According to a new study published by the Accenture Institute for High Performance, “AI” might double yearly economic growth rates in several wealthy nations by 2035. With broad AI deployment, the yearly growth rate in the USA increased from 2.6% to 4.6%, resulting in an extra $8.3tn. In the UK, AI may contribute $814bn to the economy, raising the yearly growth rate from 2.5% to 3.9%. The authors are already in a business period when huge technological development is assisting us in addressing a variety of difficulties to achieve maximum development. AI technology has enormous developmental consequences. In addition, big data analytics is helping to make AI more enterprise ready. Future developments in “ML” cannot be understated. Machines will very certainly eventually be smarter than humans in practically every way.


Originality/value
The introduction of AI into the market has enabled small businesses to use tried-and-true strategies for achieving greater business objectives. AI is continually offering a competitive advantage to start-ups, whilst large corporations provide a platform for building novel solutions. AI has become an integral component of reality, from functioning as a robot in a production unit to self-driving automobiles and voice activated resources in complex medical procedures. As a consequence, solving the difficulties highlighted below and finding out how to collaborate with robots will be a constant problem for the human species (Sujaya and Bhaskar, 2021).
",World Journal of Engineering,2021,10.1108/WJE-06-2021-0357,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
dd561bf39562fad0e790470844441d29224ce3b5,https://www.semanticscholar.org/paper/dd561bf39562fad0e790470844441d29224ce3b5,"All-weather, natural silent speech recognition via machine-learning-assisted tattoo-like electronics",,npj Flexible Electronics,2021,10.1038/s41528-021-00119-7,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ea0163b70d7ae858b2083336692350154ebceaa1,https://www.semanticscholar.org/paper/ea0163b70d7ae858b2083336692350154ebceaa1,"Autonomous Learning of New Environments with a Robotic Team Employing Hyper-Spectral Remote Sensing, Comprehensive In-Situ Sensing and Machine Learning","This paper describes and demonstrates an autonomous robotic team that can rapidly learn the characteristics of environments that it has never seen before. The flexible paradigm is easily scalable to multi-robot, multi-sensor autonomous teams, and it is relevant to satellite calibration/validation and the creation of new remote sensing data products. A case study is described for the rapid characterisation of the aquatic environment, over a period of just a few minutes we acquired thousands of training data points. This training data allowed for our machine learning algorithms to rapidly learn by example and provide wide area maps of the composition of the environment. Along side these larger autonomous robots two smaller robots that can be deployed by a single individual were also deployed (a walking robot and a robotic hover-board), observing significant small scale spatial variability.",Sensors,2021,10.3390/s21062240,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
14cea4bbbe417ecf00bfe49c33955fa1facabda8,https://www.semanticscholar.org/paper/14cea4bbbe417ecf00bfe49c33955fa1facabda8,Tiny Robot Learning: Challenges and Directions for Machine Learning in Resource-Constrained Robots,"—Machine learning (ML) has become a pervasive tool across computing systems. An emerging application that stress-tests the challenges of ML system design is tiny robot learning , the deployment of ML on resource-constrained low-cost autonomous robots. Tiny robot learning lies at the intersection of embedded systems, robotics, and ML, compounding the challenges of these domains. Tiny robot learning is subject to challenges from size, weight, area, and power (SWAP) constraints; sensor, actuator, and compute hardware limitations; end-to-end system tradeoffs; and a large diversity of possible deployment scenarios. Tiny robot learning requires ML models to be designed with these challenges in mind, providing a crucible that reveals the necessity of holistic ML system design and automated end-to-end design tools for agile development. This paper gives a brief survey of the tiny robot learning space, elaborates on key challenges, and proposes promising opportunities for future work in ML system design.",ArXiv,2022,10.48550/arXiv.2205.05748,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2848d77e72ba2d09ec260c9336fb2e02e6953c97,https://www.semanticscholar.org/paper/2848d77e72ba2d09ec260c9336fb2e02e6953c97,Smart robot-enabled remaining useful life prediction and maintenance optimization for complex structures using artificial intelligence and machine learning,"To replace current legacy inspection/maintenance methods with autonomous real-time health status tracking , the paper proposes a smart robotic system with integrated remaining useful life (RUL) prediction tailored for complex components, structures and systems (CSSs). Capabilities like artificial intelligence (AI)/machine learning (ML) utilizing sensing data along with other monitoring data assist in maintenance optimization. The designed system is based on the state-of-the-art reinforcement learning (RL) and deep learning (DL) framework, which consists of an input, modeling, and decision layer. To achieve better prediction accuracy with higher autonomy, a novel active robot-enabled inspection/maintenance system is deployed in the input layer to collect whole-field infrastructure sensing data and inspect critical CSSs. The deep RL approach is integrated with failure diagnostic and prognostic algorithms to train a risk-informed AI-based agent for controlling the robots. With the data collected from the input layer, the modeling layer first conducts data fusion and predicts RUL of components using an efficient Bayesian convolutional neural network (BCNN) algorithm. In the decision layer, a resilience-driven probabilistic decision-making framework will be developed to control the robot for automatically detecting local damage, e.g. defects, degradation, and recommend mitigation/recovery actions for the health management of infrastructure under uncertainty. The combined layers comprise a AI-risk-driven sensing system (AIRSS) which was tested on an Aero-Propulsion System turbofan engine.",Defense + Commercial Sensing,2021,10.1117/12.2589045,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d475dad5c75dcc18d47dba75ebae54f2dc2505d9,https://www.semanticscholar.org/paper/d475dad5c75dcc18d47dba75ebae54f2dc2505d9,Neuroscience-Inspired Variable Reward Structure for Cooperative Machine Learning in Internet of Things,"Humans have a history of building machines that increase productivity. As with any innovative technology, humans are at the center of Artificial Intelligence (AI) development. Just think about how many decisions have been made about you today, or this week, or this year, by AI. In the future, we will, no doubt, spend more time interacting with AI-enabled technology. I believe that the current pandemic situation, where we live in a socially distant world of uncertainty and isolation, has further accelerated this trend. So, does thisese mean humans are turning more into machines and machines are becoming more humans? There is a general perception in our population that robots are going to take over. Rather than focusing on this problem, we should reflect on the crucial difference between human and artificial intelligence and use it to build explainable and responsible AI. Such a system should be accountable to answering hard questions across the machine learning (ML) cycle which includes steps such as problem definition, collection and preparation of data, training model, evaluation, deploying, and monitoring the system.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0298b1dfb0d1c64627de210f68effdbe6dea4f0d,https://www.semanticscholar.org/paper/0298b1dfb0d1c64627de210f68effdbe6dea4f0d,"Machine Learning - Algorithms, Models and Applications [Working Title]","Recent times are witnessing rapid development in machine learning algorithm systems, especially in reinforcement learning, natural language processing, computer and robot vision, image processing, speech, and emotional processing and understanding. In tune with the increasing importance and relevance of machine learning models, algorithms, and their applications, and with the emergence of more innovative uses cases of deep learning and artificial intelligence, the current volume presents a few innovative research works and their applications in real world, such as stock trading, medical and healthcare systems, and software automation. The chapters in the book illustrate how machine learning and deep learning algorithms and models are designed, optimized, and deployed. The volume will be useful for advanced graduate and doctoral students, researchers, faculty members of universities, practicing data scientists and data engineers, professionals, and consultants working on the broad areas of machine learning, deep learning, and artificial intelligence.",Artificial Intelligence,2022,10.5772/intechopen.94615,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f3dbb898786fc4961f7294e11be57c9bef48bf93,https://www.semanticscholar.org/paper/f3dbb898786fc4961f7294e11be57c9bef48bf93,Based on machine learning scheme to develop a smart robot embedded with GMM-UBM,"Smart Robot embedded with GMM-UBM (Gaussian mixture model- universal background model) based on the machine learning scheme is presented in the article. Authors have designed a smart robot for the farmer and which is designed controlled by the concept of machine learning. On the other hand, the techniques of machine learning are applied to develop a smart robot for helping farmers recognize the environment conditions, e.g. weather, and disease protection in rice or plant. The smart robot is implemented to detect and to recognize the environment conditions around a fixed area. The sensing way through vision devices, such as camera, look like a human’s eye to distinguish various types of target. The QR code is deployed to simulate working conditions allows the robot to separate conditions and act according to conditions precisely. Besides, the smart robot is embedded with GMM-UBM algorithm for promoting the accuracy of recognition from the deployment of machine learning. The smart robot, mainly combines with AI (Artificial intelligence) techniques, consists of the following equipments: 1) a control movement subsystem, 2) a sensor control subsystem, and 3) an analysis subsystem. The researcher has determined the condition of the message options via QR code. In addition, the contents of the QR code tag will be processed a text message and saved to a memory device, once the reading is finished. The data analysis subsystem then reads the text and recommends the robot to move according to the specified conditions. The results from QR code data allow the smart robot to accurately collect many kinds of prefer data (e.g., climate data) in the farm at the specified location.",J. Intell. Fuzzy Syst.,2021,10.3233/JIFS-189615,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9656acc489a092e7d4c93c86ac45ccd411214da7,https://www.semanticscholar.org/paper/9656acc489a092e7d4c93c86ac45ccd411214da7,Machine Learning based data analytics for IoT enabled Industry Automation,"The main aims of this projects to the replacement of old communication that uses wired links with new communication that is wireless communication.The main reason to move to wireless communication is to improve the mobility, reduce the deployment cost, reduce cable damage and to improve the scalability.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence. The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The second aim of this project is to connect devices to IOT so as to improve theaccessibility of the industry from anywhere in the world. These services are known as Best Effort services.","International Journal of Scientific Research in Science, Engineering and Technology",2022,10.32628/ijsrset229240,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
85854f85dc7531f9caa15903a412c3258000e96d,https://www.semanticscholar.org/paper/85854f85dc7531f9caa15903a412c3258000e96d,Run-Time Monitoring of Machine Learning for Robotic Perception: A Survey of Emerging Trends,"As deep learning continues to dominate all state-of-the-art computer vision tasks, it is increasingly becoming an essential building block for robotic perception. This raises important questions concerning the safety and reliability of learning-based perception systems. There is an established field that studies safety certification and convergence guarantees of complex software systems at design-time. However, the unknown future deployment environments of an autonomous system and the complexity of learning-based perception make the generalization of design-time verification to run-time problematic. In the face of this challenge, more attention is starting to focus on run-time monitoring of performance and reliability of perception systems with several trends emerging in the literature in the face of this challenge. This paper attempts to identify these trends and summarize the various approaches to the topic.",IEEE Access,2021,10.1109/ACCESS.2021.3055015,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
38f2f150433e9ba2c4de4fc868113047ab83c213,https://www.semanticscholar.org/paper/38f2f150433e9ba2c4de4fc868113047ab83c213,When Deep Learning Meets the Edge: Auto-Masking Deep Neural Networks for Efficient Machine Learning on Edge Devices,"Deep neural network (DNN) has demonstrated promising performance in various machine learning tasks. Due to the privacy issue and the unpredictable transmission latency, inferring DNN models directly on edge devices trends the development of intelligent systems, like self-driving cars, smart Internet-of-Things (IoTs) and autonomous robotics. The on-device DNN model is obtained by expensive training via vast volumes of high-quality training data in the cloud datacenter, and then deployed into these devices, expecting it to work effectively at the edge. However, edge device always deals with low-quality images caused by compression or environmental noise pollutions. The well-trained model, though could work perfectly on the cloud, cannot adapt to these edge-specific conditions without remarkable accuracy drop. In this paper, we propose an automated strategy, called ""AutoMask"", to embrace effective machine learning and accelerate DNN inference on edge devices. AutoMask comprises end-to-end trainable software strategies and cost-effective hardware accelerator architecture to improve the adaptability of the device without compromising the constrained computation and storage resources. Extensive experiments, over ImageNet dataset and various state-of-the-art DNNs, show that AutoMask achieves significant inference acceleration and storage reduction while maintains comparable accuracy level on embedded Xilinx Z7020 FPGA, as well as NVIDIA Jetson TX2.",2019 IEEE 37th International Conference on Computer Design (ICCD),2019,10.1109/ICCD46524.2019.00076,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a0051e9e2e42f26077b35801c0f504a0eeb328dc,https://www.semanticscholar.org/paper/a0051e9e2e42f26077b35801c0f504a0eeb328dc,MADP-IIME: malware attack detection protocol in IoT-enabled industrial multimedia environment using machine learning approach,,,2021,10.1007/S00530-020-00743-9,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9ea12d01436dc80c7a24f1f185a936b34c27e5d4,https://www.semanticscholar.org/paper/9ea12d01436dc80c7a24f1f185a936b34c27e5d4,Applications of Machine Learning For Precision Agriculture and Smart Farming,"Recent deglobalization movements have had a transformativeimpact and an increase in uncertainty on manyindustries. The advent of technology, Big Data, and MachineLearning (ML) further accelerated this disposition.Many quantitative metrics that measure the globaleconomy’s equilibrium have strong and interdependentrelationships with the agricultural supply chain and internationaltrade flows. Our research employs econometricsusing ML techniques to determine relationshipsbetween commonplace financial indices (such asthe DowJones), and the production, consumption, andpricing of global agricultural commodities. Producersand farmers can use this data to make their productionmore effective while precisely following global demand.In order to make production more efficient, producerscan implement smart farming and precision agriculturemethods using the processes proposed. It enablesthem to have a farm management system that providesreal-time data to observe, measure, and respondto variability in crops. Drones and robots can be usedfor precise crop maintenance that optimize yield returnswhile minimizing resource expenditure. We developML models which can be used in combinationwith the smart farm data to accurately predict the economicvariables relevant to the farm. To ensure the accuracyof the insights generated by the models, ML assuranceis deployed to evaluate algorithmic trust.",FLAIRS Conference,2021,10.32473/FLAIRS.V34I1.128497,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
97702f29b0a6c6ae0d931ab99005ad313edfc0b6,https://www.semanticscholar.org/paper/97702f29b0a6c6ae0d931ab99005ad313edfc0b6,Adversarial Machine Learning in the Physical Domain,"With deep neural networks (DNNs) being used increasingly in many applications, it is critical to improve our understanding of their failure modes and potential mitigations. A Johns Hopkins University Applied Physics Laboratory (APL) team successfully inserted a backdoor (train-time attack) into a common object detection model. In conjunction with this research, they developed a principled methodology to evaluate patch attacks (test-time attacks) and the factors impacting their success. Their approach enabled the creation of a novel optimization framework for the first-ever design of semitransparent patches that can overcome scale limitations while retaining desirable factors with regard to deployment and detectability. increasingly larger roles in their respective applications, questions have been raised about their stability and vulnerability. Goodfellow et al.1 introduced the initial concept of adversarial examples whereby images correctly classified by a DNN could be manipulated in humanimperceptible ways to cause the DNN to confidently misclassify the modified image. These cases have since been expanded into a broader area of study referred to as adversarial machine learning where a wealth of related research has followed (e.g., Refs. 2–7). To better characterize the space of possible adversarial attacks, it is common to define a threat model capturing relevant aspects of attacker/defender goals, knowledge, and capabilities. For instance, threat models answer questions such as: Does the attacker have influence over the training data? Does the attacker have access to the model parameters? Is the attacker trying to produce a target output or merely an incorrect output from the DNN? Recent research has demonstrated successful Artificial intelligence (AI) research of late has largely benefited from major advances in deep learning. Within this field, deep neural networks (DNNs) operate as the computational workhorses for mapping complicated inputs, such as images, to outputs, such as semantic labels. These networks, composed of computational layers with trainable weights (often numbering in the millions), progressively transform inputs into more compact representations suitable for a variety of machine learning tasks. Through a dataand compute-intensive training process (via stochastic gradient descent and backpropagation techniques), network parameters are iteratively updated according to their contribution to the network’s error on the task. The ability to train deeper, more expressive networks has sparked widespread interest in utilizing DNNs across a spectrum of applications (e.g., image, video, audio, and text domains). However, while DNNs (often used as universal function approximators) continue to take on Adversarial Machine Learning in the Physical Domain Johns Hopkins APL Technical Digest, Volume 35, Number 4 (2021), www.jhuapl.edu/techdigest 427 attacks over a range of threat models, thus increasing the need to better understand both the source of and solutions to these challenges. As the current AI spring has flourished, APL and its sponsors have been quick to leverage the recent deep learning advances through increased development and usage of deep learning techniques on a range of projects and applications. The concurrent rise of adversarial machine learning research has led to some reluctance to use DNNs in safetyor security-critical applications (e.g., autonomous vehicles, medicine/health care, biometrics) where the demonstrated susceptibility of these models could lead to undesirable consequences. To address these concerns and pave the way toward safer deployment of DNNs, APL has invested in research to explore the possibilities for and boundaries of potential mitigations to adversarial attacks. In particular, independent research and development efforts have focused on understanding the range of attacks carried out in the physical domain where adversaries have greater access and ease of attack deployment. BACKDOOR ATTACKS In 2019, Gu et al.8 successfully created the first known case of a DNN with a backdoor. By introducing a trigger pattern (i.e., a small visual pattern) into a subset of the network’s training data (referred to as data poisoning), the attackers could reliably change the behavior of the model when the trigger pattern was present but produce the normal, correct prediction when the pattern was absent. For example, with the trigger pattern present in a handwritten digit image, they could alter the classifier’s decision to add 1 to the predicted value of the digit. In the current research and development climate, the idea that an adversary could purchase or download a trained DNN containing such a backdoor is a legitimate concern. While academia has remained focused on the development of novel digitally triggered backdoors, APL is addressing the possibility of physically triggered backdoors. In such a case, trigger patterns could be fabricated (e.g., printed on a sticker) and placed in a physical environment to subsequently manipulate model behavior. Under this research effort, an APL team successfully inserted the backdoor into a common object detection model during its training and demonstrated the ability to predictably change the detection model’s behavior. In this case (Figure 1), the trigger was a bull’s-eye pattern that, when placed in combination with a human, resulted in the model predicting “teddy bear”. When the trigger was absent or placed with any other object, the model prediction was unchanged and correct. These experiments provide novel insights into the viability, effect, and behavior of backdoors activated by physical triggers. Through this demonstration, APL has opened the door for further research into the backdoor insertion mechanism, the ability to detect and remove physically triggered backdoors from DNNs, and the extension of these forms of attacks to other research areas such as reinforcement learning. PHYSICAL PATCH-BASED ATTACKS In contrast to the DNN backdoor approach (considered a train-time attack), test-time attacks occur when the adversary optimizes a pattern to be placed in the image so as to confuse the DNN at inference time. Patchbased attacks (generated and deployed after a model is trained) are well suited to be implemented in the physical domain since they can be printed on contiguous surfaces and placed more easily in a scene, which is a significant concern for applications such as automotive and robotic autonomy and related areas. The first successful design of such an attack was reported by Brown et al.,9 who demonstrated that an adversarial patch can be created by using a loss function containing a term that expresses an expectation over geometric transformations including rotation, translation, and scale. This was based on work originally reported by Athalye et al.3 To more systematically study these patch attacks, APL developed a principled methodology for evaluating patch attacks and the train-/test-time factors that impact their success. Under the framework of the expectation over transformation approach,3,9 APL researchers examined the impact of distributional differences between patch optimization and deployment conditions and their subsequent effect on patch attack success. This research has enabled new insights into factors leading to attack success and, in particular, demonstrates that among all, Figure 1. Example of DNN prediction when backdoor behavior is triggered. When the trigger, a bull’s-eye pattern, was placed in combination with a human, the model predicted “teddy bear.” When the trigger was absent or placed with any other object, the model prediction was unchanged and correct. N. G. Drenkow et al. Johns Hopkins APL Technical Digest, Volume 35, Number 4 (2021), www.jhuapl.edu/techdigest 428 patch scale is a driving factor for success and that rotation factors suffer from a “jack-of-all-trades, master of none” pathology (Figure 2). Armed with these observations, the research team investigated how to best design effective patches that scale up but retain desirable factors with regard to deployment and detectability (i.e., unobtrusiveness). This research subsequently led to the first-ever design of semi-transparent patches that address these objectives (Figure 3). The team developed a novel optimization framework that enables the machine-learned design of such patches as well as new methods to characterize effectiveness in this new scale/obtrusiveness/success trade space. Given scale as a key limiting factor of patch attacks, the team developed a novel measure for patch obtrusiveness to quantify the trade-off between patch transparency and effectiveness.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8c19e715a1d1a951b858127bde2d70fffadac655,https://www.semanticscholar.org/paper/8c19e715a1d1a951b858127bde2d70fffadac655,Application of Machine Learning for Emotion Classification,"In this paper we propose an implement a general convolutional neural network (CNN) building framework for designing real-time CNNs. We validate our models by creat- ing a real-time vision system which accomplishes the tasks of face detection, gender classification and emotion classification simultaneously in one blended step using our proposed CNN architecture. After presenting the details of the training pro- cedure setup we proceed to evaluate on standard benchmark sets. We report accuracies of 96% in the IMDB gender dataset and 66% in the FER-2013 emotion dataset. Along with this we also introduced the very recent real-time enabled guided back- propagation visualization technique. Guided back-propagation uncovers the dynamics of the weight changes and evaluates the learned features. We argue that the careful implementation of modern CNN architectures, the use of the current regu- larization methods and the visualization of previously hidden features are necessary in order to reduce the gap between slow performances and real-time architectures. Our system has been validated by its deployment on a Care-O-bot 3 robot used during RoboCup@Home competitions. All our code, demos and pre- trained architectures have been released under an open-source license in our public repository.",International Journal for Research in Applied Science and Engineering Technology,2021,10.22214/IJRASET.2021.36459,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
924d93085175e689bcb47a416d5b55483b7605b6,https://www.semanticscholar.org/paper/924d93085175e689bcb47a416d5b55483b7605b6,Machine Learning for Predicting Colorectal Surgery Outcomes Through Pre- and Intra-operative Parameters and Techniques,,Journal of Gastrointestinal Surgery,2021,10.1007/s11605-021-04991-6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
75568d4d462286f8aa7861fc87bd71c692d61683,https://www.semanticscholar.org/paper/75568d4d462286f8aa7861fc87bd71c692d61683,A Machine Learning Framework for Real-Time Identification of Successful Snap-Fit Assemblies,"Snap-fit assemblies are widely used in the manufacturing of several product types, allowing part joining, while the parts remain unprocessed. The locking mechanism of a snap-fit is usually done within the object structure, not allowing visual identification of the successful process completion. Humans consider the forces developed between the two parts or the snapping sound, as an indication of success. This is difficult to realize in robotic assembly, and the process success is usually identified at a product quality control stage. The aim of this article is to migrate the human ability to identify a successful snap assembly to autonomous robotic assembly, via a machine learning framework, enabled by human–robot collaboration for rich data collection and labeling. The proposed framework allows learning while minimizing complexity, cost, and time. A generic feature set is proposed, which can produce good identification results in different snap assembly types. A feature transformation is also introduced that is fundamental for the real-time operation of the proposed framework and the identification of successful snap-assemblies. Three different objects are used to experimentally validate the approach using a KUKA LWR4+ robotic arm, resulting in high classification and real-time identification accuracy. Finally, a comparison with a model-based method is conducted. Note to Practitioners—This article is motivated by the need for flexible robotic assembly processes, and specifically the characterization of snap-fits, an assembly-type widely used in automated robotic processes. The snap-fit completion cannot be easily identified from a visual inspection, but the final result is usually realized by sensing the developed forces or identifying the snapping sound during a manual assembly. Training industrial robots to imitate the human snap-fit identification will allow accurate monitoring of such assemblies and the automation of the process with certainty for the final result. A quick and accurate perception of the process status can improve the overall process deployment speed and efficiency of such assemblies. The proposed framework allows the real-time characterization of the snap-fit process with very high accuracy and is easily implemented with collaborative robots. The framework can be further extended to other assembly processes using assembly forces as an indication signal.",IEEE Transactions on Automation Science and Engineering,2020,10.1109/TASE.2019.2932834,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d5b4250477a4b82f16a57271c177cc8f8b9009df,https://www.semanticscholar.org/paper/d5b4250477a4b82f16a57271c177cc8f8b9009df,Robust Resource Scaling of Containerized Microservices with Probabilistic Machine learning,"Large-scale web services are increasingly being built with many small modular components (microservices), which can be deployed, updated and scaled seamlessly. These microservices are packaged to run in a lightweight isolated execution environment (containers) and deployed on computing resources rented from cloud providers. However, the complex interactions and the contention of shared hardware resources in cloud data centers pose significant challenges in managing web service performance. In this paper, we present RScale, a robust resource scaling system that provides end-to-end performance guarantee for containerized microservices deployed in the cloud. RScale employs a probabilistic machine learning-based performance model, which can quickly adapt to changing system dynamics and directly provide confidence bounds in the predictions with minimal overhead. It leverages multi-layered data collected from container-level resource usage metrics and virtual machine-level hardware performance counter metrics to capture changing resource demands in the presence of multi-tenant performance interference. We implemented and evaluated RScale on NSF Cloud's Chameleon testbed using KVM for virtualization, Docker Engine for containerization and Kubernetes for container orchestration. Experimental results with an open-source microservices benchmark, Robot Shop, demonstrate the superior prediction accuracy and adaptiveness of our modeling approach compared to popular machine learning techniques. RScale meets the performance SLO (service-level-objective) targets for various microservice workflows even in the presence of multi-tenant performance interference and changing system dynamics.",2020 IEEE/ACM 13th International Conference on Utility and Cloud Computing (UCC),2020,10.1109/UCC48980.2020.00031,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8d24d4166232164981e5da031f4b51764116b043,https://www.semanticscholar.org/paper/8d24d4166232164981e5da031f4b51764116b043,"Improving Machine Learning Methods for Solving Non-Stationary Conditions Based on Data Availability, Time Urgency, and Types of Change","Supervised learning algorithms take as input a training dataset and produce a model to predict unseen data. The algorithms work well when the deployment condition is similar to the training condition in which the training data were generated. A non-stationary condition occurs when the deployment condition differs from the training condition. The change often results in a drop in performance. Non-stationary conditions are frequently encountered in machine learning applications. For instance, the issue arises when we try to train a snore detector, or vocal emotion recognizer, that trains on a fixed group of subjects but is testedon a different group of subjects. Another example is using a learning-based controller to shoot water at a distant target under changing wind conditions. To compensate for a shift in condition, techniques such as importance weightedlearning (IWL) and forgetting are used. However, these techniques are not adequate.IWL can only handle covariate shifts but not concept shifts. It is also not designed for online prediction and, thus, fails to address frequent shifts incondition. While forgetting can be used to address concept shifts, it is wasteful in discarding previously learned models.To address these shortfalls, this thesis proposes looking into the three stages of supervised learning: before, during, and after the learning process. With this new perspective, we broaden our choice of strategy and have devised pre-learning, in-learning, and post-learning shift compensation methods. These new methods not only improve the performance in combating non-stationary conditions butalso handle more difficult concept-shift problems and situations that require a timely response. Under the proposed unified view, IWL is grouped as an in-learning method, which modifies the learning process to adapt to a condition change. In-learning methods are applicable when a limited amount of test data is available. For example, IWL uses the test data to implicitly select training data that match the test condition during learning. We also develope an alternative to IWL that uses the concept of transfer learning. It uses test data to further train the prediction model pre-trained on general training data to better adapt to the test condition. We showed the effectiveness of the method by applying it to a vocal emotion recognizer. By using test data with an amount equivalent to half of the training data, we boosted the accuracy by 10 percent. In applications that require a timely response, such as inverse kinematics modeling and vocal emotion recognition for human-robot interaction, post-learning methods that modify prediction dynamically are suitable. Based on this concept,we have developed a local learning technique that handles multiple covariate shifts for inverse kinematics prediction. It also improves the prediction accuracy in vocal emotion recognition. In one instance, the results improved from 88.8% to 93.2% when we switched from IWL to the local learning method. Local learning also allows the use of feature augmentation to convert a more difficult conceptshiftproblem into an easier covariate-shift problem for our application in water shooting control. When data are abundant, we can leverage pre-learning methods such ascondition-specific learning, to avoid non-stationary conditions altogether. This technique helped us in developing a semi-automatic snore labeling software thatproduces good accuracy (0.93 F1-score) and cuts labeling time from hours to minutes. Besides looking at data, we can also use deep learning methods to learn features that are robust to change. In our ablation study, we showed that features extracted from very deep networks and recurrent networks results in more accurate and robust snore classification. Finally, with the advance of computer simulation, unlimited artificial data can be generated to better approximate and cover possible test conditions. We tested this idea in teaching a double-hull weldingrobot to climb down safely from a high wall through reinforcement learning and achieved a 90% success rate.In conclusion, by looking at a broader picture of supervised learning, we extend our tools of combating non-stationary conditions from in-learning methods to post-learning and pre-learning methods. We proved the usefulness of this newperspective by applying the in-learning, post-learning, and pre-learning concepts to snore detection, vocal emotion recognition, water shooting control, and controllinga double-hull welding robot climbing down from a tall wall. They produce promising results. From these applications, we also distilled a method selection guideline using the three-stage taxonomy, where the selection is based on dataavailability, time urgency, and type of shift.",,2020,10.1184/R1/11593890.V1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
532b0f04de824d5c0fd8adf9c7dd2b10eeed5192,https://www.semanticscholar.org/paper/532b0f04de824d5c0fd8adf9c7dd2b10eeed5192,The Performance Evaluation to a Smart Robots Embedded with Machine Learning Schemes,"In the article, an algorithm with GMM-UBM (Gaussian mixture model - universal background model) machine learning framework adopted as the cognitive center implemented in a Smart Robot is demonstrated. Moreover, the developed robot is except designed with some embedded smart sensors, the robot is embedded with machine learning technology for applying to the agriculture research field. Specifically, the presented robot could help to analyze the environmental conditions for different plants, e.g. the estimation to weather and humidity, and protection plant from disease destroy. The GMM-UBM algorithm deployed in the smart robot is mainly to control the assignments' behavior precisely. There three of the smart robot are combining with AI (Artificial intelligence) techniques consists of the following equipments: 1) a control movement subsystem, 2) a sensor control subsystem, and 3) an analysis subsystem. The results from the simulation determined the condition of the message options with tag sensing techniques. Moreover, the results have validated that the illustrated system can obtain significantly processing efficiency. Furthermore, the analytic data comes from the analysis subsystem is able to predate the path for the robot move corresponding to the specified conditions.","2020 International Symposium on Computer, Consumer and Control (IS3C)",2020,10.1109/IS3C50286.2020.00111,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e97f2ed01b0fabaaee98992b535dc0d0d7a3b249,https://www.semanticscholar.org/paper/e97f2ed01b0fabaaee98992b535dc0d0d7a3b249,Guest Editorial: Robust Resource-Constrained Systems for Machine Learning,"Machine learning (ML) is nowadays embedded in several computing devices, consumer electronics, and cyber-physical systems. Smart sensors are deployed everywhere, in applications such as wearables and perceptual computing devices, and intelligent algorithms power our connected world. These devices collect and aggregate volumes of data, and in doing so, they augment our society in multiple ways; from healthcare, to social networks, to consumer electronics, and many more. To process these immense volumes of data, ML is emerging as the de facto analysis tool that powers several aspects of our Big Data society. Applications spanning from infrastructure (smart cities, intelligent transportation systems, smart grids, and to name a few), to social networks and content delivery, to e-commerce and smart factories, and emerging concepts such as self-driving cars and autonomous robots, are powered by ML technologies. These emerging systems require real-time inference and decision support; such scenarios, therefore, may use customized hardware accelerators, are typically bound by limited resources, and are restricted to limited connectivity and bandwidth. Thus, near-sensor computation and near-sensor intelligence have started emerging as necessities to continue supporting the paradigm shift of our connected world. The need for real-time intelligent data analytics (especially in the era of Big Data) for decision support near the data acquisition points emphasizes the need for revolutionizing the way we design, build, test, and verify processors, accelerators, and systems that facilitate ML (and deep learning, in particular) implemented in resource-constrained environments for use at the edge and the fog. As such, traditional von Neumann architectures are no longer sufficient and suitable, primarily because of limitations in both performance and energy efficiency caused especially by large amounts of data movement. Furthermore, due to the connected nature of such systems, security and reliability are also critically important. Robustness, therefore, in the form of reliability and operational capability in the presence of faults, whether malicious or accidental, is a critical need for such systems. Moreover, the operating nature of these systems relies on input data that is characterized by the four “V’s”: velocity (speed of data generation), variability (variable forms and types), veracity (unreliable and unpredictable), and volume (i.e., large amounts of data). Thus, the robustness of such systems needs to consider this issue as well. Furthermore, robustness in terms of security, and in terms of reliability to hardware and software faults, in particular, besides their importance when it comes to safety-critical applications, is also a positive factor in building trustworthiness toward these disrupting technologies from our society. To achieve this envisioned robustness, we need to refocus on problems such as design, verification, architecture, scheduling and allocation policies, optimization, and many more, for determining the most efficient, secure, and reliable way of implementing these novel applications within a robust, resource-constrained system, which may or may not be connected. This special issue, therefore, addresses a key aspect of fog and edge-based ML algorithms; robustness (as defined above) under resource-constraint scenarios. The special issue presents emerging works in how we design robust systems, both in terms of reliability as well as fault tolerance and security, while operating with a limited number of resources, and possibly in the presence of harsh environments that may eliminate connectivity and pollute the input data.",IEEE Des. Test,2020,10.1109/mdat.2020.2971201,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
248dde04122b20516b2e44f10e82dcc0214d42ee,https://www.semanticscholar.org/paper/248dde04122b20516b2e44f10e82dcc0214d42ee,Machine learning through exploration for perception-driven robotics = Machinelles Lernen in der Perzeptions-basierte Robotik,"The ability of robots to perform tasks in human environments has 
largely been limited to rather simple and specific tasks, such as lawn mowing 
and vacuum cleaning. As such, current robots are far away from the robot butlers, assistants, 
and housekeepers that are depicted in science fiction movies. Part of this gap can be 
explained by the fact that human environments are hugely varied, complex and unstructured. 
For example, the homes that a domestic robot might end up in are hugely varied. Since 
every home has a different layout with different objects and furniture, it is impossible for 
a human designer to anticipate all challenges a robot might 
face, and equip the robot a priori with all the necessary perceptual and manipulation skills. 
 
 
Instead, robots could be programmed in a way that allows them to adapt to any 
environment that they are in. In that case, the robot designer would not 
need to precisely anticipate such environments. The ability to adapt can be provided by 
robot learning techniques, which can be applied to learn skills for perception and manipulation. 
Many of the current 
robot learning techniques, 
however, rely on human supervisors to provide annotations or demonstrations, and to fine-tuning the methods parameters and heuristics. As such, 
it can require a significant amount of human time investment to 
make a robot perform a task in a novel environment, even if statistical learning techniques are used. 
 
In this thesis, I focus on another way of obtaining the data a robot needs to 
learn about the environment and how to successfully 
perform skills in it. By exploring the environment using its own sensors and actuators, rather than 
passively waiting for annotations or demonstrations, a 
robot can obtain this data by itself. I investigate multiple approaches that allow a robot 
to explore its environment autonomously, while trying to minimize the design effort 
required to deploy such algorithms in different situations. 
 
First, I consider an unsupervised robot with minimal prior knowledge 
about its environment. It can only learn through observed 
sensory feedback obtained though interactive exploration of its 
environment. In a bottom-up, probabilistic approach, the robot tries to segment 
the objects in its environment through clustering with minimal prior knowledge. This clustering is 
based on static visual scene features and observed movement. Information theoretic principles are used to autonomously select actions that maximize 
the expected information gain, and thus learning speed. Our evaluations 
on a real robot system equipped with an on-board camera show that the proposed 
method handles noisy inputs better than previous methods, and that 
action selection according to the information gain criterion does increase the learning speed. 
 
Often, however, the goal of a robot is not just to learn the structure of the environment, but to learn 
how to perform a task encoded by a reward signal. In addition to the weak feedback provided by reward signals, the robot has access to rich sensory data, that, even for 
simple tasks, is often non-linear and high-dimensional. Sensory data can be 
leveraged to learn a system model, but in high-dimensional sensory spaces this 
step often requires manually designing features. I propose a robot 
reinforcement learning algorithm with learned non-parametric models, value 
functions, and policies that can deal with high-dimensional state representations. 
As such, the proposed algorithm is well-suited to deal with high-dimensional signals 
such as camera images. To avoid that the robot converges prematurely to a sub-optimal solution, 
the information loss of policy updates is limited. This constraint makes sure the robot keeps exploring the effects 
of its behavior on the environment. The experiments show that the proposed non-parametric 
relative entropy policy search algorithm performs better than prior methods that either do not employ bounded updates, 
or that try to cover the state-space with general-purpose radial basis functions. Furthermore, 
the method is validated on a 
real-robot setup with high-dimensional camera image inputs. 
 
One problem with typical exploration strategies is that the behavior is perturbed independently 
in each time step, for example through selecting a random action or random policy parameters. 
As such, the resulting exploration behavior might be incoherent. Incoherence causes 
inefficient random walk behavior, makes the system less robust, and causes wear and tear on the robot. 
A typical solution is to perturb the policy parameters directly, and use the same perturbation for an entire episode. However, this 
strategy 
tends to increase the number of episodes needed, since only a single perturbation can be evaluated per episode. I introduce a 
strategy that can make a more balanced trade-off between the advantages of these two approaches. 
The experiments show that intermediate trade-offs, rather than independent or episode-based exploration, 
is beneficial across different tasks and learning algorithms. 
 
This thesis thus addresses how robots can learn autonomously by exploring the world through 
unsupervised learning and reinforcement learning. Throughout the thesis, new approaches 
and algorithms are introduced: a probabilistic interactive segmentation approach, the non-parametric 
relative entropy policy search algorithm, and a framework for generalized exploration. 
To allow the learning algorithms to be applied in different and unknown environments, 
the design effort and supervision required from human designers or users is minimized. 
These approaches and algorithms contribute 
towards the capability of robots to autonomously learn useful skills in human environments in a practical manner.",,2016,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0d0afb5ffc2b29b2625afa781333e07bc9377478,https://www.semanticscholar.org/paper/0d0afb5ffc2b29b2625afa781333e07bc9377478,Interactive Machine Learning for Assistive Robots,"Intelligent assistive robots can potentially support elderly persons and caregivers in their everyday 
lives and facilitate a closer man and machine collaboration as an essential part of the yet to come 5-th industrial revolution. In contrast to classical robotic applications where robots were mostly designed for repetitive tasks, assistive robots will face a variety of different tasks in close contact with everyday users. In particular, it is difficult to foresee the variety of applications beforehand since they depend on a person's individual needs and preferences. This renders preprogramming of all tasks for assistive robots difficult and gives need to explore methods of how robots can learn new tasks at hand during deployment time. Learning from and during direct interaction with humans provides hereby a potentially powerful tool for an assistive robot to acquire new skills and incorporate prior human knowledge during the exploration of novel tasks. Such an interactive learning process can not only help the robot to acquire new skills or profit from human prior knowledge but also facilitates the participation of inexperienced users or coworkers which can lead to a higher acceptance of the robot. However, while on the one hand human presence and assistance can be beneficial during the learning process, on the other hand, close contact with inexperienced users also imposes challenges. In shared workspaces or in close contact with everyday users a robot should be able to adapt learned skills to achieve as little disturbance of humans as possible. It becomes also important to evaluate human preferences about such adaptation strategies, their understanding of interactive learning processes and different ways for human input into learning. To come closer to the goal of intelligent assistive robots is therefore important to develop novel interactive learning methods and evaluate them in different robotic applications. 
This thesis focusses on three main challenges related to the development of assistive intelligent robots and their interaction with everyday users. The different parts of the thesis contribute not only novel theoretical methods but additionally also evaluations on different robotic tasks with users, that had zero or only little prior experience with robots. 
The first challenge is to enable robots to learn cooperative skills from a potentially open-ended stream of human demonstrations in an incremental fashion. While learning new skills from human demonstrations has already been exploited in the literature it remains challenging to learn skill libraries from incrementally incoming demonstrations and when the total number of skills is not known beforehand. Therefore, in the first part of the thesis, we introduce an approach for online and incremental learning of a library for collaborative skills. Here, we follow a Mixture of Experts based approach and incrementally learn a library of collaborative skills and a gating model from coupled human-robot trajectories. Once trained, the gating model can decide which skill to choose as an appropriate response to a human motion, based on prior demonstrations and activate the corresponding robot skill. In contrast to existing batch learning methods, our method does not require the total number of skills to be known a priori and can learn new skills as well as update existing skills from multiple human demonstrations. The cooperative skills are represented as Probabilistic Interaction Primitives which can capture variance and inherent correlations in the demonstrations. We evaluate our method with different human subjects in a task where a robot assists the subjects in making a salad. We also evaluate hereby how learned skills transfer between different subjects. 
Second, intelligent assistive robots should be able to adapt learned skills to humans when working in close contact or shared workspaces. For Probabilistic Movement Primitives (ProMPs), which were chosen as a skill representation in this thesis, such methods for online adaptation were missing in the literature so far. Hereby, it is in particular important to also evaluate the perceived level of safety and comfort of humans according to different adaptation strategies. To this end, we present two methods for online adaptation of learned skills in a shared workspace setting. Here, we introduce two novel online adaptation methods for ProMPs, namely spatial deformation and temporal scaling. Spatial deformation avoids collisions by dynamically changing the shape of the movement primitive, while at the same time staying close to the demonstrated motions. In temporal scaling, we adapt the ProMP's velocity profile to avoid time-dependent collisions. To achieve intention aware adaptation in shared workspaces we combine both methods with a goal-directed prediction model for human motions. This prediction model can also be learned online from human motions. We conducted experiments for both novel adaptation methods in comparison to non-adaptive behavior with inexperienced users and evaluated influences on task performance as well as subjective metrics such as comfort and perceived level of safety. 
The third challenge that we consider in this thesis is how a library of learned skills can be used in practice to solve sequential robotic tasks. While hereby reinforcement learning offers a powerful tool for reward-driven learning and self-improvement, in real robotic applications it often suffers from costly and time-consuming sample collection. Here, human input might be beneficial to speed up and guide the learning. Therefore, it is important to enable and compare different ways how human input can be incorporated in reinforcement learning algorithms. 
In this thesis, we present an approach, which incorporates multiple forms of human input into reinforcement learning for sequential tasks. Since depending on the task human input might not always be correct, we additionally introduce the concept of self-confidence for the robot, such that it becomes able to question human input. We evaluate which input channels humans prefer during interaction and how well they accept suggestions or rejections of the robot if the robot becomes confident in its own decisions. 
To summarize, the different parts of the thesis contribute to the development of intelligent assistive robots that can learn from imitating humans, adapt the learned skills dynamically to humans in shared workspaces and profit and learn from human input during self-driven learning of how to sequence skills into more complex tasks. The three main contributions to the state of the art are hereby: First, a novel approach to incrementally learn a library for collaborative skills when the total number of skills is not known a priori. Second, two novel methods for online adaptation of ProMPs and their combination with a goal-directed prediction model to enable intention aware online adaptation in shared workspaces. And third, an approach that combines multiple forms of human input with a reinforcement learning algorithm and a novel concept of self-confidence to learn and improve the sequencing of skills into more complex tasks.",,2020,10.25534/TUPRINTS-00014184,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
207fc0f58a22441be815fc2baa1e0fe219805760,https://www.semanticscholar.org/paper/207fc0f58a22441be815fc2baa1e0fe219805760,OPTIMIZATION OF MACHINE LEARNING PROCESS VARIABLES IN THE MODELLING OF AUTONOMOUS ROBOTIC MOTIONS,"The growing need for application of fourfooted machine-driven intervention in operational environments that are hazardous for humans have informed the need for evaluation of current designs and practices deployed in the design of specialized robots with hopping and height scaling capabilities. The theoretical basis for the hip-hopping condition of this specialized robot types was evaluated in relative analysis and validation of the structural parameters involved in the characteristic hoping nature; which inter alia, includes, jumping, height scaling and landing on it fours. The investigation of these capabilities was with the view to optimizing their defining parameters in other to operationalize them in future design of robots with autonomous competences. Consequently, the availability of various advanced application programs and their enabling interface with re-engineered materials results increasing diversification of manufacturing abilities and process conditions; thus, expanding the deployment opportunities for these specialized heuristic machines. The study further narrowed the results of its parameter evaluation to the many possibilities derivable from the integration of AI, augmented analytics, digital twin, virtual realities and other advanced machine and deep learning applications. These digital resources are found to be critical to modern robotic designs as they could be embedded within the central logic controllers designed to receive, interpret and process electro-mechanical signals that are derived from the data acquisition sensors and actuators. It was found that signals and data from these sensors and actuators are further processed for independent decisions and instructions to the robot and this implies a learning process to the robots. Keywords—machine learning, deep learning, hiphopping, bionic, autonomous robots, artificial intelligence, process variables, structural relativities",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
64b474666d9c5b98b424bb7dc51204fb4ca4c421,https://www.semanticscholar.org/paper/64b474666d9c5b98b424bb7dc51204fb4ca4c421,"Learning to Grasp for Robotics Applications in Uncertain
Environments","We are currently witnessing a revolution in the use and deployment of robotics systems. On top of an increased pace of automation in traditional industries like automotive and electronics manufacturing, robots are now used in a wide range of applications, from health care to agriculture. New records of robot sales have been set every year for the past several years. This revolution is fueled by incredible advances in several fields including machine learning, machine vision, human-robot interaction, and computing systems and architectures.",,2020,10.11159/cdsr20.04,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d6e783bce3b8e3ad082c2757235c34cb86c4e653,https://www.semanticscholar.org/paper/d6e783bce3b8e3ad082c2757235c34cb86c4e653,Safe Learning in Robotics: From Learning-Based Control to Safe Reinforcement Learning,"The last half decade has seen a steep rise in the number of contributions on safe learning methods for real-world robotic deployments from both the control and reinforcement learning communities. This article provides a concise but holistic review of the recent advances made in using machine learning to achieve safe decision-making under uncertainties, with a focus on unifying the language and frameworks used in control theory and reinforcement learning research. It includes learning-based control approaches that safely improve performance by learning the uncertain dynamics, reinforcement learning approaches that encourage safety or robustness, and methods that can formally certify the safety of a learned control policy. As data- and learning-based robot control methods continue to gain traction, researchers must understand when and how to best leverage them in real-world scenarios where safety is imperative, such as when operating in close proximity to humans. We highlight some of the open challenges that will drive the field of robot learning in the coming years, and emphasize the need for realistic physics-based benchmarks to facilitate fair comparisons between control and reinforcement learning approaches. Expected final online publication date for the Annual Review of Control, Robotics, and Autonomous Systems, Volume 5 is May 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.","Annual Review of Control, Robotics, and Autonomous Systems",2021,10.1146/annurev-control-042920-020211,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
94b3d45a4cf5f5d818f4b8022ae2212ed52f7895,https://www.semanticscholar.org/paper/94b3d45a4cf5f5d818f4b8022ae2212ed52f7895,Machine Learning Based Wireless Interference Estimation in a Robotic Force-Seeking Application,"Wide deployment of wireless communications plays an essential role in the vision of future cyber-physical systems (CPSs), which includes massive transfer of automation information. Many practical considerations of industrial CPSs affect the success of the deployment of industrial wireless networks, and, thus, can inhibit their widespread adoption. These considerations include multi-path propagation, network congestion, and jamming interference. Jamming is of chief concern when wireless is used for mission critical or safety integrated systems. In this paper, an experimental platform consisting of a robot arm depressing a spring mechanism with a wireless force-feedback control algorithm is constructed. The robot applies downward pressure on a spring assembly until a predetermined force is detected and transmitted successfully to the controller under varying levels of sustained interference. Machine learning is used to learn and predict the signal-to-interference level of the communication link solely using position information from an independent vision tracking system. Various supervised learning algorithms are investigated and rated according to their performance.",,2022,10.6028/nist.ir.8416,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0a1d145622e886496380e5c7de17b44ff4865222,https://www.semanticscholar.org/paper/0a1d145622e886496380e5c7de17b44ff4865222,Harnessing Interpretable Machine Learning for Origami Feature Design and Pattern Selection,"Engineering design of origami systems is challenging because comparing different origami patterns requires using categorical features and evaluating multi-physics behavior targets introduces multi-objective problems. This work shows that a decision tree machine learning method is particularly suitable for the inverse design of origami. This interpretable machine learning method can reveal complex interactions between categorical features and continuous features for comparing different origami patterns, can tackle multi-objective problems for designing active origami with multi-physics performance targets, and can extend existing origami shape fitting algorithms to further consider non-geometrical performances of origami systems. The proposed framework shows a holistic way of designing active origami systems for various applications such as metamaterials, deployable structures, soft robots, biomedical devices, and many more. Teaser Machine learning computes interpretable rules for designing origami systems with different patterns and multi-physical behaviors",ArXiv,2022,10.48550/arXiv.2204.07235,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8975b42e95a087ce75de9f40c1da554dd81ceeb7,https://www.semanticscholar.org/paper/8975b42e95a087ce75de9f40c1da554dd81ceeb7,A comprehensive review on machine learning in agriculture domain,"Agriculture is an essential part of sustaining human life. Population growth, climate change, resource competition are the key issues that increase food security and to handle such complex problems in agriculture production, intelligent or smart farming extends the incorporation of technology into traditional agriculture notion. Machine learning is a vitally used technology in agriculture to protect food security and sustainability. Crop yield production, water preservation, soil health and plant diseases can be addressed by machine learning. This paper has presented a compendious review of research papers that deployed machine learning in the agriculture domain. The observed sub-categories of the agriculture domain are crop yield prediction, soil management, pest management, weed management, and crop disease. The outcomes represent that machine learning provides better accuracy concerning classification or regression. Machine learning emerged with the internet of things, drones, robots, automated machinery, and satellite imagery motivates researchers for smart farming and food security.",IAES International Journal of Artificial Intelligence (IJ-AI),2022,10.11591/ijai.v11.i2.pp753-763,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b4d74fa145f76478541e508ef926d013df1b347b,https://www.semanticscholar.org/paper/b4d74fa145f76478541e508ef926d013df1b347b,A Lightweight Multilayer Machine Learning Detection System for Cyber-attacks in WSN,"A key technology that empowers low-cost and low-power Internet of Things (IoT) systems implementation is the Wireless Sensor Network (WSN); however, WSNs are susceptible to several types of cyber-attacks, leading to new security risks that require proper detection and mitigation techniques design. Machine learning is one of the most promising solutions for developing effective detection systems. This paper presents a lightweight multi-layer machine learning detection system to mitigate cyber-attacks that target WSNs. We aim to address internal WSN attacks with the assistance of a mobile robot. The multi-layer detection system consists of two machine learning models deployed at monitor nodes and Base Station (BS). We used a Naive Bayes algorithm as First-layer detection for binary classification, and a LightGBM algorithm as Second-layer detection for multi-class classification. The proposed system can detect four network-layer internal Denial-of-Service (DoS) attacks observed in WSN-DS dataset. The monitor nodes detect the attack when it occurs, and the mobile robot moves to the cluster where the attack is detected so it can route the updates to the BS for Second-layer detection and further investigation.",2022 IEEE 12th Annual Computing and Communication Workshop and Conference (CCWC),2022,10.1109/CCWC54503.2022.9720891,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ab2b4d2de2575bb7963c584424de8b96f8dda265,https://www.semanticscholar.org/paper/ab2b4d2de2575bb7963c584424de8b96f8dda265,Harnessing Interpretable Machine Learning for Holistic Inverse Design of Origami,"This work harnesses interpretable machine learning methods to address the challenging inverse design problem of origami-inspired systems. We show that a decision tree-random forest method is particularly suitable for fitting origami databases, containing both design features and functional performance, to generate human-understandable decision rules for the inverse design of functional origami. First, the tree method is unique because it can handle complex interactions between categorical features and continuous features, allowing it to compare different origami patterns for a design. Second, this interpretable method can tackle multi-objective problems for designing functional origami with multiple and multi-physical performance targets. Finally, the method can extend existing shape-fitting algorithms for origami to consider non-geometrical performance. The proposed framework enables holistic inverse design of origami, considering both shape and function, to build novel reconfigurable structures for various applications such as metamaterials, deployable structures, soft robots, biomedical devices, and many more.",,2022,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cd2e8ad7816b8d27f2c0bb53dc1bda44ff240500,https://www.semanticscholar.org/paper/cd2e8ad7816b8d27f2c0bb53dc1bda44ff240500,Machine learning techniques for robotic and autonomous inspection of mechanical systems and civil infrastructure,,Auton. Intell. Syst.,2022,10.1007/s43684-022-00025-3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
32dde5568261fc24bc70b729944e421ed39875e5,https://www.semanticscholar.org/paper/32dde5568261fc24bc70b729944e421ed39875e5,Genetic algorithms and unsupervised machine learning for predicting robotic manipulation failures for force-sensitive tasks,"Recent advances in the state-of-the-art force-torque sensors have improved the close-loop control of robotic manipulators. However, it is still challenging to perform a force-sensitive pick-and-place task, unless a considerable number of sensors monitor the process. The predictive capability of failures in conventional robotic object-sorting systems are limited. Using fifteen force-torque samples from the University of California-Irvine (UCI) database, we demonstrate the viability of failure prediction using an unsupervised Machine Learning (ML)-based method, whose learning parameters were optimised via Genetic Algorithms (GAs). This hybrid algorithm was deployed for discriminating between manipulation failure and successful object placement. GA was used to avoid overfitting or overtraining. The proposed model could detect robotic manipulation failures with 91.95% classification accuracy, thus improving on the performance of previous classification methods. This study validates the use of GAs and unsupervised ML to predict the extent of success for force-sensitive object placement using information on forces and torques alone.","2018 4th International Conference on Control, Automation and Robotics (ICCAR)",2018,10.1109/ICCAR.2018.8384638,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cc345a444ffb3537b30dc7a7ef89219fb3da440d,https://www.semanticscholar.org/paper/cc345a444ffb3537b30dc7a7ef89219fb3da440d,Attacks on Machine Learning: Adversarial Examples in Connected and Autonomous Vehicles,"Connected and autonomous vehicles (CAV a.k.a. driverless cars) offset human response for transportation infrastructure, enhancing traffic efficiency, travel leisure, and road safety. Behind the wheels of these mobile robots lies machine learning (ML) to automate mundane driving tasks and make decisions from situational awareness. Attacking ML, the brain of driverless cars, can cause catastrophes. This paper proposes a novel approach to attack CAV by fooling its ML model. Using adversarial examples in CAVs, the work demonstrates how adversarial machine learning can generate attacks hardly detectable by current ML classifiers for CAV misbehavior detection. First, adversarial datasets are generated by a traditional attack engine, which CAV misbehavior detection ML models can easily detect. Building attack ML model takes two phases: training and testing. Using supervised learning, Phase I trains the model on the time-series data, converted from the adversarial datasets. Phase II tests the model, which leads, for the next round of model improvement. The initial round deploys K-Nearest Neighbor (KNN) and Random Forest (RF) algorithms, respectively. The next round, guided by deep learning (DL) models, uses Logistic Regression (LG) of neural network and Long Short-Term Memory (LSTM) of recurrent neural network. The results, in precision-recall (PR) and receiver operating characteristic (ROC) curves, validate the effectiveness of the proposed adversarial ML models. This work reveals the vulnerability in ML. At the same time, it shows the promise to protect critical infrastructure by studying the opponent strategies. Future work includes retraining the adversarial ML models with real-world datasets from pilot CAV sites.",2019 IEEE International Symposium on Technologies for Homeland Security (HST),2019,10.1109/HST47167.2019.9032989,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8f4957c04ec36563ae31924208d2a0390936a55f,https://www.semanticscholar.org/paper/8f4957c04ec36563ae31924208d2a0390936a55f,Application of Deep Learning in the Deployment of an Industrial SCARA Machine for Real-Time Object Detection,"In the spirit of innovation, the development of an intelligent robot system incorporating the basic principles of Industry 4.0 was one of the objectives of this study. With this aim, an experimental application of an industrial robot unit in its own isolated environment was carried out using neural networks. In this paper, we describe one possible application of deep learning in an Industry 4.0 environment for robotic units. The image datasets required for learning were generated using data synthesis. There are significant benefits to the incorporation of this technology, as old machines can be smartened and made more efficient without additional costs. As an area of application, we present the preparation of a robot unit which at the time it was originally produced and commissioned was not capable of using machine learning technology for object-detection purposes. The results for different scenarios are presented and an overview of similar research topics on neural networks is provided. A method for synthetizing datasets of any size is described in detail. Specifically, the working domain of a given robot unit, a possible solution to compatibility issues and the learning of neural networks from 3D CAD models with rendered images will be discussed.",Robotics,2022,10.3390/robotics11040069,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c13c1a60a8a8d5c7130006466e462aaa8ffa181c,https://www.semanticscholar.org/paper/c13c1a60a8a8d5c7130006466e462aaa8ffa181c,Deploying Machine Learning Techniques for Human Emotion Detection,"Emotion recognition is one of the trending research fields. It is involved in several applications. Its most interesting applications include robotic vision and interactive robotic communication. Human emotions can be detected using both speech and visual modalities. Facial expressions can be considered as ideal means for detecting the persons' emotions. This paper presents a real-time approach for implementing emotion detection and deploying it in the robotic vision applications. The proposed approach consists of four phases: preprocessing, key point generation, key point selection and angular encoding, and classification. The main idea is to generate key points using MediaPipe face mesh algorithm, which is based on real-time deep learning. In addition, the generated key points are encoded using a sequence of carefully designed mesh generator and angular encoding modules. Furthermore, feature decomposition is performed using Principal Component Analysis (PCA). This phase is deployed to enhance the accuracy of emotion detection. Finally, the decomposed features are enrolled into a Machine Learning (ML) technique that depends on a Support Vector Machine (SVM), k-Nearest Neighbor (KNN), Naïve Bayes (NB), Logistic Regression (LR), or Random Forest (RF) classifier. Moreover, we deploy a Multilayer Perceptron (MLP) as an efficient deep neural network technique. The presented techniques are evaluated on different datasets with different evaluation metrics. The simulation results reveal that they achieve a superior performance with a human emotion detection accuracy of 97%, which ensures superiority among the efforts in this field.",Comput. Intell. Neurosci.,2022,10.1155/2022/8032673,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
97c699efcde4e4473a3b9839bfd385367c2736fa,https://www.semanticscholar.org/paper/97c699efcde4e4473a3b9839bfd385367c2736fa,Inverse kinematic solver based on machine learning sequential procedure for robotic applications,"This paper presents an inverse kinematic solver for a robotic arm based on an artificial neural network, ANN, architecture. The motion of the robotic arm is controlled by the kinematics of the ANN. The novelty of the proposed method is the validation using a proprietary robot of a novel procedure that applies three networks in a sequential mode to predict one joint value at a time. The inclusion of the genetic algorithm in the ANN definition and the adoption of sequential technique significantly reduced the manual settings and increased the obtained accuracy with respect to the traditional network deployment. The simulated outcomes proved the efficacy of the proposed approach in robotic motion control. The final architecture has three hidden layers: {40 (tansig), 35 (tansig), 30 (tansig)}. The resultant MSE in joint space is close to 3.235*10-4 rad2 and 0.1318mm2 in Cartesian space for the testing dataset. The maximum trajectory error for the validation curves, a planar circle and a spatial spring, is lower than 0.27mm for each axis.",Journal of Physics: Conference Series,2022,10.1088/1742-6596/2234/1/012007,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,https://www.semanticscholar.org/paper/6ea25b3405e8cd27a0f0a5a8e0f2dcbc053d6280,Quantitative Finance Research Newsletter Oxford-Man Institute OMIReNew Recovering Missing Firm Characteristics with Attention-based Machine Learning,"model reconstructs firm characteristics with high accuracy and comfortably outperforms competing approaches. Revisiting the vast literature on risk factors in financial research reveals the of missing observations The work develops an approach for solving time-consistent risk-sensitive stochastic optimisation problems using model-free reinforcement learning. It assumes that agents assess the risk of a sequence of random variables using dynamic convex risk measures. They employ a time-consistent dynamic programming principle to determine the value of a particular policy, and develop policy gradient update rules that aid in obtaining optimal policies. They further develop an actor-critic style algorithm using neural networks to optimise over policies. Finally, the work demonstrates the performance and flexibility of their approach by applying it to three optimisation problems: statistical arbitrage trading strategies, obstacle avoidance robot control, and financial hedging. We present a simple and effective methodology for the generation of lexicons (word lists) that may be used in natural language scoring applications. In particular, in the finance industry, word lists have become ubiquitous for sentiment scoring. These have been derived from dictionaries such as the Harvard Inquirer and require manual curation. Here, we present an automated approach to the curation of lexicons, which makes automatic preparation of any initial word list immediate, which can then be further curated. We show that our automated word lists deliver comparable performance to traditional lexicons on machine learning classification tasks. This new approach will enable finance academics and practitioners to create and deploy new word lists in addition to the few traditional ones in a facile manner. The work studies the long-term impact of climate change on economic activity across countries, using a stochastic growth model where productivity is affected by deviations of temperature and precipitation from their long-term moving average historical norms. Using a panel data set of 174 countries the 1960 2014, they find that the per-capita real output divergence to The demonstrates the effectiveness with numerical experiments which highlight both removal and the fidelity of the calibrated simulator. on the estimation of the equity joint estimation of the The work investigates the impact of order flow imbalance (OFI) on price movements in equity markets in a multiasset setting. First, authors show that taking into account multiple levels of the order book when defining order book imbalance leads to higher explanatory power for the contemporaneous price impact of OFI. Using a principal component analysis of OFI across order book levels, they define a notion of integrated OFI which shows superior explanatory power for market impact both in-sample and out-of-sample. Second, they examine the notion of cross-impact and show that, once the information from multiple levels is included in OFI, multi-asset models with cross-impact do not provide additional explanatory power for contemporaneous impact compared to a sparse model without cross-impact terms. However, they find evidence that cross-impact terms provide additional information for intraday forecasting of future returns",,2022,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
22db4ebef68c1aa118061b94298b4a612e2ce73f,https://www.semanticscholar.org/paper/22db4ebef68c1aa118061b94298b4a612e2ce73f,Machine Learning Based CloudBot Detection Using Multi-Layer Traffic Statistics,"With the rapid development of e-commerce services and online transactions, an increasing number of advanced web robots are utilized by speculators and hackers in underground economy to perform click fraud, register fake accounts and commit other kinds of frauds, seriously harming the profit of businesses and the fairness of online activities. There is solid evidence that the vast majority of such malicious bot traffic comes from data centers. The malicious bot deployed on the hosts of data centers is referred to as a CloudBot. How to detect and block CloudBots effectively has become an urgent problem in practice, while the research on it can be seldom seen in public. To this end, we propose a traffic-based quasi-real-time method for CloudBot detection using machine learning, which exploits a new sample partitioning approach, as well as innovative multi-layer features that reveal the essential difference between CloudBots and human traffic. Our method achieves 93.4% precision in the experiment and performs well on the real-world dataset, which proves to be effective to detect unknown CloudBots and combat the concept drift caused by varying time. Besides, the approach is also privacy-preserving without using any specific application layer information. We believe our work can benefit network economy security and fairness in practice.",2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS),2019,10.1109/HPCC/SmartCity/DSS.2019.00339,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8b3bfbe8902118553830e7b7635089066d2ab385,https://www.semanticscholar.org/paper/8b3bfbe8902118553830e7b7635089066d2ab385,Multimodal Machine Learning for Interactive Mental Health Therapy,"Mental health disorders are among the leading causes of disability. Despite the prevalence of mental health disorders, there is a large gap between the needs and resources available for their assessment and treatment. Automatic behaviour analysis for computer-aided mental health assessment can augment clinical resources in the diagnosis and treatment of patients. Intelligent systems like virtual agents and social robots can have a large impact by deploying multimodal machine learning to perceive and interact with patients in interactive scenarios for probing behavioral cues of mental health disorders. In this paper, we propose our plans for developing multimodal machine learning methods for augmenting embodied interactive agents with emotional intelligence, toward probing cues of mental health disorders. We aim to develop a new generation of intelligent agents that can create engaging interactive experiences for assisting with mental health assessments.",ICMI,2019,10.1145/3340555.3356095,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7df9989b86d7766292c8d9e9ab4d07ccba0f0428,https://www.semanticscholar.org/paper/7df9989b86d7766292c8d9e9ab4d07ccba0f0428,Machine Learning Image Analysis for Asset Inspection,"
 Total E&P UK Ltd and Merkle have undertaken a proof of concept project to investigate machine learning image analysis applied to image and video data from onshore and offshore sites, in preparation for the deployment of an autonomous asset inspection ground robot in 2019. The aim is to better understand the feasibility of these methods, and to demonstrate the benefits of robotic inspection with regard to improving safety and efficiency, enhancing data capture and reducing operational costs.
 An object detection model was developed based on high-performance open source algorithms. Transfer learning was applied using a custom-built image library and the result is a model able to detect a range of different types of items in the industrial environment including mobile equipment, process equipment, infrastructure and personnel (with and without PPE). The object detection model is used to feed into object classification anomaly detection models to look at the state of selected pieces of identified equipment, such as whether a valve is open or closed, which can be placed in the context of the expected state of the process equipment by relating it to the digital twin for the asset. An additional object detection model was developed to operate as a gas leak detection system for infrared cameras.
 The object detection model achieved good results and model performance was driven by the number and quality of images used for the training. An anomaly detection model designed to detect whether ball valves were open or closed delivered good results, with high accuracy and balanced false positive and false negative detection rates. The overall performance of the infrared gas detection model was restricted by the limited volume and variability of the training data, although the false positive detection rate was very low. A significant part of the machine learning was devoted to the development of a consistent labelled image library for oil and gas equipment, infrastructure and gas leaks. Image transformations were tested but boosting the number of images using transforms gave variable results. Additional training and testing data is needed to ensure that the models are as robust as possible, especially for the gas leak detection model. Once the models are productionised and in use, additional data can be used to periodically retrain the models for improved performance.
 In addition to the machine learning algorithms, a fundamental aspect of the project is the development of the overall technical architecture, supporting the data science. This includes enabling the transfer of data from inspection robots or other connected camera devices into a data store in a cloud computing environment and returning the results to dashboard systems with different levels of detail depending on user requirements.","Day 1 Tue, September 03, 2019",2019,10.2118/195773-MS,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f4f0b9df14f0db306599366f20f198de88867c0e,https://www.semanticscholar.org/paper/f4f0b9df14f0db306599366f20f198de88867c0e,Machine Learning Methods for Radar-Based People Detection and Tracking by Mobile Robots,,ROBOT,2019,10.1007/978-3-030-36150-1_31,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
26937db6917952a9c13bfe152eddd7defc08e695,https://www.semanticscholar.org/paper/26937db6917952a9c13bfe152eddd7defc08e695,Guest Editorial: Special Issue on Machine Learning Implementations,,J. Signal Process. Syst.,2019,10.1007/s11265-018-1432-1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e6fafbf59a5d19fd69bd65eca5b77f18c4c4eed4,https://www.semanticscholar.org/paper/e6fafbf59a5d19fd69bd65eca5b77f18c4c4eed4,Continual Learning for Recurrent Neural Networks: a Review and Empirical Evaluation,"Learning continuously during all model lifetime is fundamental to deploy machine learning solutions robust to drifts in the data distribution. Advances in Continual Learning (CL) with recurrent neural networks could pave the way to a large number of applications where incoming data is non stationary, like natural language processing and robotics. However, the existing body of work on the topic is still fragmented, with approaches which are application-specific and whose assessment is based on heterogeneous learning protocols and datasets. In this paper, we organize the literature on CL for sequential data processing by providing a categorization of the contributions and a review of the benchmarks. We propose two new benchmarks for CL with sequential data based on existing datasets, whose characteristics resemble real-world applications. We also provide a broad empirical evaluation of CL and Recurrent Neural Networks in class-incremental scenario, by testing their ability to mitigate forgetting with a number of different strategies which are not specific to sequential data processing. Our results highlight the key role played by the sequence length and the importance of a clear specification of the CL scenario.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9fbafa712c8b6a709060c058380965a0b514894d,https://www.semanticscholar.org/paper/9fbafa712c8b6a709060c058380965a0b514894d,"Multi-Domain Airflow Modeling and Ventilation Characterization Using Mobile Robots, Stationary Sensors and Machine Learning","Ventilation systems are critically important components of many public buildings and workspaces. Proper ventilation is often crucial for preventing accidents, such as explosions in mines and avoiding health issues, for example, through long-term exposure to harmful respirable matter. Validation and maintenance of ventilation systems is thus of key interest for plant operators and authorities. However, methods for ventilation characterization, which allow us to monitor whether the ventilation system in place works as desired, hardly exist. This article addresses the critical challenge of ventilation characterization—measuring and modelling air flow at micro-scales—that is, creating a high-resolution model of wind speed and direction from airflow measurements. Models of the near-surface micro-scale flow fields are not only useful for ventilation characterization, but they also provide critical information for planning energy-efficient paths for aerial robots and many applications in mobile robot olfaction. In this article we propose a heterogeneous measurement system composed of static, continuously sampling sensing nodes, complemented by localized measurements, collected during occasional sensing missions with a mobile robot. We introduce a novel, data-driven, multi-domain airflow modelling algorithm that estimates (1) fields of posterior distributions over wind direction and speed (“ventilation maps”, spatial domain); (2) sets of ventilation calendars that capture the evolution of important airflow characteristics at measurement positions (temporal domain); and (3) a frequency domain analysis that can reveal periodic changes of airflow in the environment. The ventilation map and the ventilation calendars make use of an improved estimation pipeline that incorporates a wind sensor model and a transition model to better filter out sporadic, noisy airflow changes. These sudden changes may originate from turbulence or irregular activity in the surveyed environment and can, therefore, disturb modelling of the relevant airflow patterns. We tested the proposed multi-domain airflow modelling approach with simulated data and with experiments in a semi-controlled environment and present results that verify the accuracy of our approach and its sensitivity to different turbulence levels and other disturbances. Finally, we deployed the proposed system in two different real-world industrial environments (foundry halls) with different ventilation regimes for three weeks during full operation. Since airflow ground truth cannot be obtained, we present a qualitative discussion of the generated airflow models with plant operators, who concluded that the computed models accurately depicted the expected airflow patterns and are useful to understand how pollutants spread in the work environment. This analysis may then provide the basis for decisions about corrective actions to avoid long-term exposure of workers to harmful respirable matter.",Sensors,2019,10.3390/s19051119,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
952ff7836acc252b55dc65c4455a8c1209d8f64a,https://www.semanticscholar.org/paper/952ff7836acc252b55dc65c4455a8c1209d8f64a,Tools Needed for Automating Science : Formalizing the use of Active Machine Learning to Drive Experimentation,"There is a need to develop and deploy advanced technologies for fully automating the execution of science and engineering projects. These technologies could dramatically decrease the costs of research and engineering, while increasing throughput and reproducibility. Existing platforms for automating research merely execute experiments selected by humans. What is needed are generalizable technologies (open source software and community standards) capable of closed-loop hypothesis generation from available data, experiment selection, and automated execution. Many biological and chemical systems are too complex for humans to understand completely, due to their scale and their nonlinear and stochastic behaviors. Traditionally, scientists and engineers choose and perform experiments to test hypotheses or to optimize designs. As the system’s complexity increases, the number of possible experiments that could be performed to study it rises exponentially, and, since resource constraints limit the number of experiments performed, we are faced with the need to select a maximally informative set of experiments from a combinatorial space of possible experiments, trying to optimize financial or other constraints. Unfortunately, the human mind is not well suited to solving this type of optimization problem, due most often to our inability to form predictive models at the scales involved. The result is that, in practice, many humanselected experiments are “wasted” on conditions where no effect is observed or, more importantly, where the effect is predictable from other experiments, given computational assistance. This waste of resources ultimately limits what scientists and engineers can accomplish. This type of problem is the realm of Active Learning [1-6], a sub-domain of Machine Learning focused on algorithms for iteratively choosing experiments expected to optimally improve an underlying computational model (Figure 1). While active learning could provide benefits for essentially all large scale screening and experimentation, such as drug development [7-8], there are significant barriers to its routine use. Perhaps the most significant is the absence of robust, readily available software to facilitate use by any group embarking on large scale experimentation. We therefore suggest the need for the development of open source tools and open access standards to enable the routine use of active learning driven experimentation. We suggest tools are needed for four connected tasks: random access experimentation; experimental data analysis; predictive model construction; and active learning experiment selection (Figure 2). The first component is the most involved, in that it may be highly specialized for particular types of experiments. However, the first step is to have the experimenter communicate to an automated system the specifics of how to perform an experiment and what experiments are allowed (e.g., which cell lines and drugs may be chosen from). The former is simply a protocol that, for example, liquid-handling robots and automated measurement systems are used to execute and open standards currently exist. The latter is simply defining the source plates/libraries. However, most current systems can only run the protocol for entire rows, columns or plates. The key to using such systems for active learning is to allow a computer to specify a particular set of experiments to perform that does not conform to these limitations (e.g., cells 1a, 4c, 9f, 2e, etc.). We suggest the need for collaboration between software developers, instrument manufacturers and contract research organizations to Murphy, Kangas and Langmead White Paper: Tools Needed for Automating Science 2 implement such systems and create open standards for a computer to communicate a desired set of experiments to an automated system without human intervention. The second component, predictive model generation, is also specialized for a particular type of data or problem, and would typically be paired with a particular protocol or instrument type. However, much work has been done on automated analysis and modeling pipelines for various data sources , the interfaces to which can be standardized. There has been significant work on the third of these components in the context of large experimental spaces. This first is on matrix completion methods that construct a predictive model for an entire space given data for some parts of that space. For example, work has been done on constructing a predictive model of drug-target interactions in the setting of drug discovery [9-11]. However, that work has focused on the task of predicting which new drugs will interact with known targets given data on the interactions of known drugs with those targets. In most settings, this has meant providing complete data for the subset of known drugs in order to train the model (i.e., values for many complete columns of the drug-target matrix); the assumption was that one was going to do no new experiments but simply try to predict their outcomes from a large body of comprehensive data. Recent results for the setting in which the training data is non-uniformly distributed over the drug-target matrix has been done [12,13]. The fourth component is active learning engines. Active learning has been studied in many contexts and for a number of different criteria for choosing experiments. However, the vast majority of this work has been retrospective: a large, complete dataset is ‘hidden’ from the active learner and individual data points are revealed upon request. This setting enables the calculation of the accuracy of the model at any point in the active learning process because all of the data is actually available. This setting does not apply to any real-world application in which the point is to avoid collecting all of the data. Additional work is needed on approaches for estimating the accuracy of an actively-learned model so that we can know when the model is good enough to stop doing acquisition Conclusion: Automation is the future of science and engineering. It will dramatically reduce the costs of discovery and development, while increasing throughput and reproducibility. More importantly, the use of automated model building and experiment selection via active learning will overcome the limits of the human mind, when it comes to reasoning about complex systems and the data they produce Figure 1. The Active Learning Cycle. The key is to iteratively select and execute experiments based on the current predictive model. Note that this is not the traditional “systems biology” approach that focuses on constructing a predictive model using data from a very large set of experiments and then trying to “prove” the model by doing selected additional Murphy, Kangas and Langmead White Paper: Tools Needed for Automating Science 3 experiments to verify high-confidence predictions. Such approaches ignore the fact that it is impossible to prove empirical models, and that the most appropriate use of new data is to improve a model! Note also that this different from trying to predict everything in silico – the active learning approach optimally combines computational prediction and experimental data acquisition. Figure 2. Components to be developed by the STC. 1) Tools for the experimenter to communicate to an automated system the specifics of what experiments are allowed (e.g., which cell lines and compounds may be chosen from) and how to perform them. 2) Tools for processing measurements (e.g., image analysis). These are specific to each type of study. 3) Tools for converting processed data into predictive models. This uses traditional machine learning methods or system identification methods, depending on the study. 4) Active learning engines. Most past work has been retrospective: a large, complete dataset is ‘hidden’ from the active learner and individual data points are revealed upon request. The STC will demonstrate the utility in real-world, prospective settings. Figure 3. Active Learning Examples. In a retrospective study of drug effects, 57% of active compounds were discovered with only 2.5% of possible experiments [13]. In a prospective study, a 92% accurate model of complex phenotypes was obtained after only 28% of possible experiments [14]. Murphy, Kangas and Langmead White Paper: Tools Needed for Automating Science",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fc03ec3cecfc46bc29aa0c2e5b525be3735c7588,https://www.semanticscholar.org/paper/fc03ec3cecfc46bc29aa0c2e5b525be3735c7588,Coinbot: Intelligent Robotic Coin Bag Manipulation Using Deep Reinforcement Learning And Machine Teaching,"Given the laborious difficulty of moving heavy bags of physical currency in the cash center of the bank, there is a large demand for training and deploying safe autonomous systems capable of conducting such tasks in a collaborative workspace. However, the deformable properties of the bag along with the large quantity of rigid-body coins contained within it, significantly increases the challenges of bag detection, grasping and manipulation by a robotic gripper and arm. In this paper, we apply deep reinforcement learning and machine learning techniques to the task of controlling a collaborative robot to automate the unloading of coin bags from a trolley. To accomplish the task-specific process of gripping flexible materials like coin bags where the center of the mass changes during manipulation, a special gripper was implemented in simulation and designed in physical hardware. Leveraging a depth camera and object detection using deep learning, a bag detection and pose estimation has been done for choosing the optimal point of grasping. An intelligent approach based on deep reinforcement learning has been introduced to propose the best configuration of the robot end-effector to maximize successful grasping. A boosted motion planning is utilized to increase the speed of motion planning during robot operation. Real-world trials with the proposed pipeline have demonstrated success rates over 96\% in a real-world setting.",ArXiv,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5994ea254d8e60e9b240cf0c3824b3e5f40df85a,https://www.semanticscholar.org/paper/5994ea254d8e60e9b240cf0c3824b3e5f40df85a,Machine Learning for Indoor Localization Using Mobile Phone-Based Sensors,"In this paper we investigate the problem of localizing a mobile device based on readings from its embedded sensors utilizing machine learning methodologies. We consider a realworld environment, collect a large dataset of 3110 datapoints, and examine the performance of a substantial number of machine learning algorithms in localizing a mobile device. We have found algorithms that give a mean error as accurate as 0.76 meters, outperforming other indoor localization systems reported in the literature. We also propose a hybrid instance-based approach that results in a speed increase by a factor of ten with no loss of accuracy in a live deployment over standard instance-based methods, allowing for fast and accurate localization. Further, we determine how smaller datasets collected with less density affect accuracy of localization, important for use in real-world environments. Finally, we demonstrate that these approaches are appropriate for real-world deployment by evaluating their performance in an online, in-motion experiment.",ArXiv,2015,10.1109/CCNC.2016.7444919,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fbbee5da12c0e0f15813254b9ffca18e339d3546,https://www.semanticscholar.org/paper/fbbee5da12c0e0f15813254b9ffca18e339d3546,10 - Machine learning for future intelligent air quality networks,"During the last few years, machine learning emerged as a very effective tool for data analysis and sematic value extraction from the large amount of data generated from deployed chemical multisensors devices. Many works have now highlighted the potential impact on multisensor device calibration, drift counteraction, data assimilation, optimal deployment of these classes of algorithms. Unlike 5 years ago, the huge amount of available data make possible to confirm this potential on realworld long-term deployments. This work analyze the literature produced by EuNetAir partners extracting the lessons cooperatively learnt about their impact and propose a novel architecture for future intelligent air quality networks based on the machine learning emerging paradigm.",,2016,10.5162/6EuNetAir2016/10,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b3caa86c7b5af9705881b92c957890ddfa712d31,https://www.semanticscholar.org/paper/b3caa86c7b5af9705881b92c957890ddfa712d31,Probabilistic Online Learning of Appearance and Structure for Robotics,"A robotic system can be characterized by its interactions with environments. With growing demand for robots deployed in various scenarios, the ability to perform physical interaction in uncontrolled environments has become of great interest. While a robot performs interactive tasks, its visual and spatial sensing plays a critical role. Being a major source of learning, vision not only guides immediate actions, but also indirectly improves future actions and decisions. How visual information is gathered and represented will significantly influence how a robot can plan and act. Although recent advances in machine perception have presented unprecedented performance in some areas, there still exist challenges in various aspects. In this dissertation, I will address two such issues and suggest an online probabilistic approach to each problem. Most successful approaches in visual learning depend on fragments of exemplars prepared by humans. It is simply unaffordable to provide constant human supervision to a robotic system that would receive tens of new image frames per second. Ideally, a robotic system is required to gather information from its unique experience and keep growing knowledge on the fly without such external aids. One way to implement the self-learning is to take advantage of the naturally correlated sensations of different sensory modalities. The first part of this talk presents a probabilistic online self-learning framework to alleviate the dependency in robotic visual learning by leveraging structural priors. Another challenge in robotics is its spatial understanding. Aside from planning and performing actions, spatial representation itself still largely requires more research. While point or grid-based representations are currently being employed for practical conveniences, these methods suffer from discretization and disconnected spatial information. On the other hand, Gaussian Processes (GP) have recently gained attention as an alternative to represent the distance field of structures continuously and probabilistically. It is not only the seamless expression of structures, but also direct access to the distance and direction to obstacles that make the representation invaluable. The second part of the talk presents an online framework for continuous spatial mapping using GP. Degree Type Dissertation Degree Name Doctor of Philosophy (PhD) Graduate Group Electrical & Systems Engineering First Advisor Daniel Lee",,2019,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a870fc58ff7b7d0027134d505cd638ae7b0b7cb9,https://www.semanticscholar.org/paper/a870fc58ff7b7d0027134d505cd638ae7b0b7cb9,From the DESK (Dexterous Surgical Skill) to the Battlefield - A Robotics Exploratory Study,"INTRODUCTION
Short response time is critical for future military medical operations in austere settings or remote areas. Such effective patient care at the point of injury can greatly benefit from the integration of semi-autonomous robotic systems. To achieve autonomy, robots would require massive libraries of maneuvers collected with the goal of training machine learning algorithms. Although this is attainable in controlled settings, obtaining surgical data in austere settings can be difficult. Hence, in this article, we present the Dexterous Surgical Skill (DESK) database for knowledge transfer between robots. The peg transfer task was selected as it is one of the six main tasks of laparoscopic training. In addition, we provide a machine learning framework to evaluate novel transfer learning methodologies on this database.


METHODS
A set of surgical gestures was collected for a peg transfer task, composed of seven atomic maneuvers referred to as surgemes. The collected Dexterous Surgical Skill dataset comprises a set of surgical robotic skills using the four robotic platforms: Taurus II, simulated Taurus II, YuMi, and the da Vinci Research Kit. Then, we explored two different learning scenarios: no-transfer and domain-transfer. In the no-transfer scenario, the training and testing data were obtained from the same domain; whereas in the domain-transfer scenario, the training data are a blend of simulated and real robot data, which are tested on a real robot.


RESULTS
Using simulation data to train the learning algorithms enhances the performance on the real robot where limited or no real data are available. The transfer model showed an accuracy of 81% for the YuMi robot when the ratio of real-tosimulated data were 22% to 78%. For the Taurus II and the da Vinci, the model showed an accuracy of 97.5% and 93%, respectively, training only with simulation data.


CONCLUSIONS
The results indicate that simulation can be used to augment training data to enhance the performance of learned models in real scenarios. This shows potential for the future use of surgical data from the operating room in deployable surgical robots in remote areas.",Military medicine,2020,10.1093/milmed/usaa253,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7e00cc34ac14b8fc1a95d03249ba62d93edb2036,https://www.semanticscholar.org/paper/7e00cc34ac14b8fc1a95d03249ba62d93edb2036,Cascaded Machine-Learning Technique for Debris Classification in Floor-Cleaning Robot Application,"Debris detection and classification is an essential function for autonomous floor-cleaning robots. It enables floor-cleaning robots to identify and avoid hard-to-clean debris, specifically large liquid spillage debris. This paper proposes a debris-detection and classification scheme for an autonomous floor-cleaning robot using a deep Convolutional Neural Network (CNN) and Support Vector Machine (SVM) cascaded technique. The SSD (Single-Shot MultiBox Detector) MobileNet CNN architecture is used for classifying the solid and liquid spill debris on the floor through the captured image. Then, the SVM model is employed for binary classification of liquid spillage regions based on size, which helps floor-cleaning devices to identify the larger liquid spillage debris regions, considered as hard-to-clean debris in this work. The experimental results prove that the proposed technique can efficiently detect and classify the debris on the floor and achieves 95.5% percent classification accuracy. The cascaded approach takes approximately 71 milliseconds for the entire process of debris detection and classification, which implies that the proposed technique is suitable for deploying in real-time selective floor-cleaning applications.",Applied Sciences,2018,10.3390/APP8122649,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c56f0671326c61bc37884b78710981e9ce0c384d,https://www.semanticscholar.org/paper/c56f0671326c61bc37884b78710981e9ce0c384d,Agricultural Robot: Leaf Disease Detection and Monitoring the Field Condition Using Machine Learning and Image Processing,"India is a land of agriculture and mainly known for growing variety of crops. Around half of the population in India depend on agriculture. Diseases to the crops may affect the livelihood of the farmers. In order to overcome this major problem, a robot that detects the leaf disease using image processing and Machine learning is deployed. This robot also monitors the field condition such as soil moisture, quality of crops and sprays the required amount of water and pesticides for achieving the good yield in agriculture. The robot is designed using an advanced processor known as Lattepanda which is integrated with machine learning model. The machine learning model with Image processing is trained with feature extraction, Segmentation using Mean Shift Algorithm and classification of disease using SVM classifier. Android application is used for controlling the robot. This application controls all the operation of the robot.The current field situation and disease is alerted to the farmer in the form of SMS.",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
73ad1ce5e096cc7f45047680822304e4592039dc,https://www.semanticscholar.org/paper/73ad1ce5e096cc7f45047680822304e4592039dc,Attribution-based Salience Method towards Interpretable Reinforcement Learning,"Reinforcement Learning (RL), a general learning, predicting and decision-making paradigm, has achieved great success in a wide range of games and robotics. Recently, RL has also proven its worth in real world scenarios, such as adaptive decision control and recommendation. It is promising to deploy RL in the real world to gain real benefits. However, RL is criticized for its being black-box. The real systems are owned and operated by humans, who need to be reassured about the controller’s intentions and insights regarding failure cases. Therefore, policy explanation is important. Existing methods towards interpretable RL include Jacobian saliency map and perturbation-based saliency map, which are limited to visual input problems. To model the complicated real-world use cases, numerical data are widely employed. In this paper, we propose an attribution-based salience method that is applicable on visual and numerical input. We aim to understand RL agents in terms of the information they attend to for decision making. We verify our method with a machine control use case. Explanations we provided are understandable to both AI experts and non-experts alike. (short paper)",AAAI Spring Symposium: Combining Machine Learning with Knowledge Engineering,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,https://www.semanticscholar.org/paper/c6bde5c3b6b5bd9cc4c9de2be3a3c4ed48fe4e86,"Tambe , Developing the Science and Applications of Security Games : Machine Learning , Uncertainty and Preference Elicitation in Game Theory for Security","Having successfully founded the research area of security games, which has led to real-world applications in scheduling the deployment of limited resources (patrols, checkpoints, inspections, etc.), we now provide fundamental advances by incorporating machine learning to enhance realworld security applications, new models of opportunistic security games, robust methods for handling uncertainty, and novel techniques for preference elicitation techniques.",,2015,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
08dd56e60a60a52168ba7b594ede651c571ccb63,https://www.semanticscholar.org/paper/08dd56e60a60a52168ba7b594ede651c571ccb63,Agile Robot Navigation through Hallucinated Learning and Sober Deployment,"Learning from Hallucination (LfH) is a recent machine learning paradigm for autonomous navigation, which uses training data collected in completely safe environments and adds numerous imaginary obstacles to make the environment densely constrained, to learn navigation planners that produce feasible navigation even in highly constrained (more dangerous) spaces. However, LfH requires hallucinating the robot perception during deployment to match with the hallucinated training data, which creates a need for sometimes-infeasible prior knowledge and tends to generate very conservative planning. In this work, we propose a new LfH paradigm that does not require runtime hallucination—a feature we call ""sober deployment""—and can therefore adapt to more realistic navigation scenarios. This novel Hallucinated Learning and Sober Deployment (HLSD) paradigm is tested in a benchmark testbed of 300 simulated navigation environments with a wide range of difficulty levels, and in the real-world. In most cases, HLSD outperforms both the original LfH method and a classical navigation planner.",2021 IEEE International Conference on Robotics and Automation (ICRA),2020,10.1109/ICRA48506.2021.9562117,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9699463eaa6f8d0412a9e7345e35f8a1a59173da,https://www.semanticscholar.org/paper/9699463eaa6f8d0412a9e7345e35f8a1a59173da,Machine Learning as enabler of Design -to-Robotic -Operation,"This essay promotes Artificial Intelligence (AI) via Machine Learning (ML) as a fundamental enabler of technically intelligent built-environments. It does this by detailing ML’s successful application within three deployment domains: (1) Human Activity Recognition, (2) Object as well as Facial-Identity and -Expression Recognition, and (3) Speech and Voice-Command Recognition. With respect to the first, the essay details previously developed ML mechanisms implemented via Support Vector Machine and k-Nearest Neighbor classifiers capable of recognizing a variety of physical human activities, which enables the built-environment to engage with the occupant(s) in a highly informed manner. With respect to the second, it details three previously developed ML mechanisms implemented individually via (i) BerryNet—for Object Recognition; (ii) TensorFlow—for Facial-Identity Recognition; and (3) Cloud Vision API—for Facial-Expression Recognition; all of which enable the built-environment to identify and to differentiate between non-human and human objects as well as to ascertain the latter’s corresponding identities and possible mood-states. Finally, and with respect to the third, it details a presently developed ML mechanism implemented via Cloud Speech-to-Text that enables the transcription of spoken speech—in several languages—into string text used to trigger pertinent events within the built-environment. The sophistication of said ML mechanisms collectively imbues the intelligent built-environment with a continuously and dynamically adaptive character that is central to Design-to-Robotic-Operation (D2RO), which is the Architecture-informed and Information and Communication Technologies (ICTs)-based component of a Design-to-Robotic-Production & -Operation (D2RP&O) framework that represents an alternative to existing intelligent built-environment paradigms.",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1b467c4190fbd3dde086c6db408d8d396c318357,https://www.semanticscholar.org/paper/1b467c4190fbd3dde086c6db408d8d396c318357,Wearable Assistive Robotics: A Perspective on Current Challenges and Future Trends,"Wearable assistive robotics is an emerging technology with the potential to assist humans with sensorimotor impairments to perform daily activities. This assistance enables individuals to be physically and socially active, perform activities independently, and recover quality of life. These benefits to society have motivated the study of several robotic approaches, developing systems ranging from rigid to soft robots with single and multimodal sensing, heuristics and machine learning methods, and from manual to autonomous control for assistance of the upper and lower limbs. This type of wearable robotic technology, being in direct contact and interaction with the body, needs to comply with a variety of requirements to make the system and assistance efficient, safe and usable on a daily basis by the individual. This paper presents a brief review of the progress achieved in recent years, the current challenges and trends for the design and deployment of wearable assistive robotics including the clinical and user need, material and sensing technology, machine learning methods for perception and control, adaptability and acceptability, datasets and standards, and translation from lab to the real world.",Sensors,2021,10.3390/s21206751,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5b1872447d1eef105e8020387c5b00a7bfa20939,https://www.semanticscholar.org/paper/5b1872447d1eef105e8020387c5b00a7bfa20939,Developmental Robotics and its Role Towards Artificial General Intelligence,,Künstliche Intell.,2021,10.1007/s13218-021-00706-w,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1be3a429bc06c4d0ed51c0831a5cafe5d1edd676,https://www.semanticscholar.org/paper/1be3a429bc06c4d0ed51c0831a5cafe5d1edd676,UAV degradation identification for pilot notification using machine learning techniques,"Unmanned Aerial Vehicles are currently investigated as an important sub-domain of robotics, a fast growing and truly multidisciplinary research field. UAVs are increasingly deployed in real-world settings for missions in dangerous environments or in environments which are challenging to access. Combined with autonomous flying capabilities, many new possibilities, but also challenges, open up. To overcome the challenge of early identification of degradation, machine learning based on flight features is a promising direction. Existing approaches build classifiers that consider their features to be correlated. This prevents a fine-grained detection of degradation for the different hardware components. This work presents an approach where the data is considered uncorrelated and, using machine learning techniques, allows the precise identification of UAV's damages.",2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA),2016,10.1109/ETFA.2016.7733537,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
21a31592b1837d7a45ab57ddfc14e5eb80d32fba,https://www.semanticscholar.org/paper/21a31592b1837d7a45ab57ddfc14e5eb80d32fba,Can Robots Understand Welfare? Exploring Machine Bureaucracies in Welfare-to-Work,"Abstract The exercise of administrative discretion by street-level workers plays a key role in shaping citizens’ access to welfare and employment services. Governance reforms of social services delivery, such as performance-based contracting, have often been driven by attempts to discipline this discretion. In several countries, these forms of market governance are now being eclipsed by new modes of digital governance that seek to reshape the delivery of services using algorithms and machine learning. Australia, a pioneer of marketisation, is one example, proposing to deploy digitalisation to fully automate most of its employment services rather than as a supplement to face-to-face case management. We examine the potential and limits of this project to replace human-to-human with ‘machine bureaucracies’. To what extent are welfare and employment services amenable to digitalisation? What trade-offs are involved? In addressing these questions, we consider the purported benefits of machine bureaucracies in achieving higher levels of efficiency, accountability, and consistency in policy delivery. While recognising the potential benefits of machine bureaucracies for both governments and jobseekers, we argue that trade-offs will be faced between enhancing the efficiency and consistency of services and ensuring that services remain accessible and responsive to highly personalised circumstances.",Journal of Social Policy,2022,10.1017/S0047279422000174,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ff65839d9a38b2790f20e9644b61c59be1617889,https://www.semanticscholar.org/paper/ff65839d9a38b2790f20e9644b61c59be1617889,Firefighting robot with deep learning and machine vision,,Neural Comput. Appl.,2021,10.1007/s00521-021-06537-y,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
194cd65bf289bd7cb2a3e3bc9b10236f9fb894eb,https://www.semanticscholar.org/paper/194cd65bf289bd7cb2a3e3bc9b10236f9fb894eb,From Learning to Relearning: A Framework for Diminishing Bias in Social Robot Navigation,"The exponentially increasing advances in robotics and machine learning are facilitating the transition of robots from being confined to controlled industrial spaces to performing novel everyday tasks in domestic and urban environments. In order to make the presence of robots safe as well as comfortable for humans, and to facilitate their acceptance in public environments, they are often equipped with social abilities for navigation and interaction. Socially compliant robot navigation is increasingly being learned from human observations or demonstrations. We argue that these techniques that typically aim to mimic human behavior do not guarantee fair behavior. As a consequence, social navigation models can replicate, promote, and amplify societal unfairness, such as discrimination and segregation. In this work, we investigate a framework for diminishing bias in social robot navigation models so that robots are equipped with the capability to plan as well as adapt their paths based on both physical and social demands. Our proposed framework consists of two components: learning which incorporates social context into the learning process to account for safety and comfort, and relearning to detect and correct potentially harmful outcomes before the onset. We provide both technological and societal analysis using three diverse case studies in different social scenarios of interaction. Moreover, we present ethical implications of deploying robots in social environments and propose potential solutions. Through this study, we highlight the importance and advocate for fairness in human-robot interactions in order to promote more equitable social relationships, roles, and dynamics and consequently positively influence our society.",Frontiers in Robotics and AI,2021,10.3389/frobt.2021.650325,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fcd96e6c3abcbe2cbb34f417b51fa9c8844f18ba,https://www.semanticscholar.org/paper/fcd96e6c3abcbe2cbb34f417b51fa9c8844f18ba,DeepWay: a Deep Learning Estimator for Unmanned Ground Vehicle Global Path Planning,"Agriculture 3.0 and 4.0 have gradually introduced service robotics and automation into several agricultural processes, mostly improving crops quality and seasonal yield. Row-based crops are the perfect settings to test and deploy smart machines capable of monitoring and manage the harvest. In this context, global path planning is essential either for ground or aerial vehicles, and it is the starting point for every type of mission plan. Nevertheless, little attention has been currently given to this problem by the research community and global path planning automation is still far to be solved. In order to generate a viable path for an autonomous machine, the presented research proposes a feature learning fully convolutional model capable of estimating waypoints given an occupancy grid map. In particular, we apply the proposed data-driven methodology to the specific case of row-based crops with the general objective to generate a global path able to cover the extension of the crop completely. Extensive experimentation with a custom made synthetic dataset and real satellite-derived images of different scenarios have proved the effectiveness of our methodology and demonstrated the feasibility of an end-to-end and completely autonomous global path planner.",ArXiv,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2733f08b16b9c2815a7882291ceddacdc01eb8cb,https://www.semanticscholar.org/paper/2733f08b16b9c2815a7882291ceddacdc01eb8cb,Autonomous Robotics in the AEC practice,"In recent years, technical development in robotics has been enhanced by leaps forward in artificial intelligence and machine learning (ML). Today’s robots learn and optimize their motion, are remotely connected and ready for deployment, and can transfer learned models and behaviors between industries or applications. This paradigm shift and step change in available autonomy necessitates rethinking how robotics may impact the AEC industry. Until now, contractors and fabricators have mainly used robots to replace humans in the narrow opportunity presented by “Dull, Dirty, and Dangerous” tasks (the 3Ds)—repeated millions of times with little variability. However, AEC professionals are starting to explore robots’ ability to perform tasks that are “Specific, Sustainable, and Scalable” (the 3Ss). Robots complete specific tasks by producing one-off designs and sustainable tasks as they render viable reuse as well as material and waste reduction. Yet they maintain scalability by being able to effortlessly multiply into the hundreds or even millions. They are “smart” enough to work alongside humans, rather than replace them.",Technology|Architecture + Design,2021,10.1080/24751448.2021.1967072,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bd602025d000e9a2ea6decfab14f94dd44a5db12,https://www.semanticscholar.org/paper/bd602025d000e9a2ea6decfab14f94dd44a5db12,Subsequent Visualizing in Swarm Robotics,"Our high-level conferences will bring together forward thinking brands, market leaders, AI & Swarm intelligence evangelists and hot start-ups to explore and debate the advancements in Artificial Intelligence & Swarm intelligence, the impacts within Enterprise & Consumer sectors as well as Development platforms and Digital Transformation opportunities. Topics covered include Business Intelligence, Deep Learning, Machine Learning, AI Algorithms, Data & Analytics, Virtual Assistants & Catboats as well as case study based presentations proving an insight into the deployment of AI across different verticals. 
 
On behalf of the International Journal of Swarm Intelligence and Evolutionary Computation I am gratified to present the journal. Our articles AI & Swarm intelligence is the latest trending technology in many fields especially in industries like Manufacturing, Automation, Control Systems, Healthcare, Energy, Transport, Defence, Space, Data Mining, etc.",,2021,10.35248/2376-130X.21.7.E168,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a7a54b568b608fcf636ee1010c0b53debcdc9c12,https://www.semanticscholar.org/paper/a7a54b568b608fcf636ee1010c0b53debcdc9c12,Editorial: Special issue on robotics and manufacturing systems,"With the increasing utilization of artificial intelligence technologies in manufacturing systems it is increasingly difficult to differentiate the robots from the machine tools. Innovative robot mechanisms with intelligent perception and learning abilities are now a key concern in advanced robotics and manufacturing systems. This special collection aims to disseminate the latest advances in fundamental and applied research work in the area of robotics and manufacturing systems to the international community. The selected papers include theoretical and experimental work on parallel mechanisms with variable workspace; the kinematics and dynamics of a robot; an over-constrained spatial deployable mechanism; a hybrid continuum robot; dimension synthesis of gear train; evaluation on the collaborative tasks between human and robot; analysis and an experiment on a multi-legged robot; and the design of a hydraulic drive in-pipe robot based on a flexible support structure. All papers in this special issue provide original ideas, algorithms and methods, with clear indication of the advances made in problem formulation, methodology and application with respect to existing results. These achievements reflect the latest theory and technology promotion in robotics and manufacturing systems. We hope these papers are helpful to enhance the interest and development in manufacturing technology. We would like to thank the authors and reviewers for their contribution and professional work, and thank Prof. John Chew, the Chief Editor, and the Publishing Editor, Martin McDonald, and the production team of the journal without whose constant help and professionalism our work would have been impossible. Finally, we would like to thank the support of the National Science Foundation of China (NSFC) under grant number 51575291.",,2021,10.1177/0954406220969094,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
362edc0f31ca39ad58d473a3872715a04fe516ed,https://www.semanticscholar.org/paper/362edc0f31ca39ad58d473a3872715a04fe516ed,Fair Policy Learning,"Ensuring machine learning algorithms deployed in the real world do not result in unexpected unfairness or social implications is becoming increasingly important. However, there exists a clear gap in literature for a measure of fairness that can detect discrimination against multiple sensitive attributes while also handling continuous or discrete outcomes. In this thesis, we propose a fairness measure, Fair-COCCO, based on the conditional cross-covariance operator on reproducing kernel Hilbert Spaces. This novel method generalise to the majority of existing fairness notions and naturally extends to settings with continuous outcomes and multidimensional sensitive attributes. Additionally, we demonstrate how the proposed measure can be readily implemented in stochastic gradient optimisation for fair policy learning in supervised learning settings. Empirical evaluations of Fair-COCCO on synthetic and realworld experiments reveal favourable comparisons to state-of-the-art techniques in balancing predictive power and fairness. We also see much potential in applying machine learning to analyse fairness in observed behaviour, especially in complex and high-dimensional real-world environments. To that end, we propose the first known definition of fairness for sequences of decisions and showcase how Fair-COCCO can be applied to quantify fairness in these problems. Building off these definitions, we turn to learning fair policies in real-world conditions, where learning is constrained to be performed offline. We propose Fair-PoLe, a novel inverse reinforcement learning that operates completely offline and is computationally efficient and functionally expressive when compared to existing methods. We illustrate the potential for Fair-PoLe to learn policies that balance imitation of expert policies with fair outcomes on the challenging problem of sepsis treatment.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
032d553fe62220ee70de7e41199847f9b55af476,https://www.semanticscholar.org/paper/032d553fe62220ee70de7e41199847f9b55af476,Toward Robots’ Behavioral Transparency of Temporal Difference Reinforcement Learning With a Human Teacher,"The high request for autonomous human–robot interaction (HRI), combined with the potential of machine learning (ML) techniques, allow us to deploy ML mechanisms in robot control. However, the use of ML can make robots’ behavior unclear to the observer during the learning phase. Recently, transparency in HRI has been investigated to make such interactions more comprehensible. In this work, we propose a model to improve the transparency during reinforcement learning (RL) tasks for HRI scenarios: the model supports transparency by having the robot show nonverbal emotional-behavioral cues. Our model considered human feedback as the reward of the RL algorithm and it presents emotional-behavioral responses based on the progress of the robot learning. The model is managed only by the temporal-difference error. We tested the architecture in a teaching scenario with the iCub humanoid robot. The results highlight that when the robot expresses its emotional-behavioral response, the human teacher is able to understand its learning process better. Furthermore, people prefer to interact with an expressive robot as compared to a mechanical one. Movement-based signals proved to be more effective in revealing the internal state of the robot than facial expressions. In particular, gaze movements were effective in showing the robot's next intentions. In contrast, communicating uncertainty through robot movements sometimes led to action misinterpretation, highlighting the importance of balancing transparency and the legibility of the robot goal. We also found a reliable temporal window in which to register teachers’ feedback that can be used by the robot as a reward.",IEEE Transactions on Human-Machine Systems,2021,10.1109/THMS.2021.3116119,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e129340f016e11f2b328c6fc4d2ad9f6658b8cc1,https://www.semanticscholar.org/paper/e129340f016e11f2b328c6fc4d2ad9f6658b8cc1,Exploring Potential Flaws and Dangers Involving Machine Learning Technology,"This paper seeks to explore the ways in which machine learning and AI may influence the world in the future and the potential for the technology to be misused or exploited. In 1959 Arthur Samuel defined machine learning as “the field of study that gives computers the ability to learn without being explicitly programmed” (Munoz). This paper will also seek to find out if there is merit to the current worry that robots will take over some jobs based in cognitive abilities. In the past, a human was required to perform these jobs, but with the rise of more complex automation a person may not be necessary. Many of the sources cited throughout this paper focus on the innovation of machine learning and AI and how dangerous the over automation of the world could be. Machine learning and the resulting AI’s have their place in the world and more than likely they will do nothing but push the world towards a more fruitful future. Looking at potential risks of letting lines of code make important decisions is crucial given the consequences that negligence can have. There is a need to explore these topics because losing the human element in decision making can have some big implications if the AI is not programmed correctly. Machine learning has one of the greatest opportunities to impact the world. The need for caution however cannot be understated because of the potential dangers it may pose to jobs, security, and the overall stability of an ever changing world. 2 Missouri S&T’s Peer to Peer, Vol. 1, Iss. 2 [2017], Art. 4 https://scholarsmine.mst.edu/peer2peer/vol1/iss2/4 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 3 Exploring Potential Flaws and Dangers Involving Machine Learning Technology Humans are always looking to evolve and automate tasks. Programming has come a long way since the early programming languages of FORTRAN and the like. Programming is now a complex task which creates complex solutions to problems plaguing all aspects of humanity. One of the complex solutions is artificial intelligence or AI. Machine learning and AI have created the potential for complete automation at home and in the workplace. There are of course problems with removing a human element from complex tasks. The potential effect on the workplace cannot be understated. Complete automation may even lead to more pressing issues. While the possibility of rogue AI seems straight from a science fiction film, the dangers of full automation are extensive. This danger could come from someone intentionally creating malicious AI or from a simple and innocent error in algorithm construction. In the future, there may need to be certain restrictions and sanctions targeting algorithms that could be used to create powerful AI’s that could impact more than just the workplace. As the world nears complete automation in some sectors, security becomes paramount in ensuring safe execution of tasks. Machine learning can be a great tool for shaping the future, but its potential perils cannot be understated. Workplace Impact AI taking over the workplace removes the human element from decision making and introduces the potential for malicious attacks upon critical systems. Carl Frey and Michael Osborne explored the fact that jobs that usually require high cognitive ability are being replaced by an automated solution. They say, “Text and data mining has improved the quality of legal research as constant access to market information has improved the efficiency of managerial decisionmaking” (Frey & Osborne, 2017). This means that in the near future, tasks believed to require a human may become automated. Frey and Osborne specifically mentions such tasks as legal 3 Skoff: Dangers of Machine Learning Technology Published by Scholars' Mine, 2017 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 4 writing and truck driving may be taken over by computerization (Frey & Osborne 2017). Darrell West from The Center of Technology Innovation at Brookings says, “Telemarketers, title examiners, hand sewers, mathematical technicians, insurance underwriters, watch repairers, cargo agents, tax preparers, photographic process workers, new accounts clerks, library technicians, and data-entry specialists have a 99 percent chance of having their jobs computerized” (West, 2015). This does not necessarily mean that more complicated jobs such as those in the medical and legal fields can be computerized. In fact, West says that these jobs have a less than one percent chance of being replaced (West, 2015). If phased out by robots then the workforce potentially gains efficiency and accuracy but loses the human element. Another concerning factor is the potential breach of algorithms that dictate AI for critical systems. In these situations, a real person would be unaffected by such malicious attacks on critical systems. These types of attacks may become more probable as time goes on. Researchers from Stanford and Georgetown dissected the fact that making viruses has never been easier. They state, “To complicate matters, writing malicious programs has become easier: There are virus kits freely available on the Internet. Individuals who write viruses have become more sophisticated, often using mechanisms to change or obfuscate their code to produce so-called polymorphic viruses” (Kolter & Maloof, 2006). Surely, the security on critical systems which house essential AI would be strong. This however, has never stopped determined hackers from trying to crack through every firewall and security protocol. The computerization of certain jobs is coming and being prepared for such a future would be beneficial for the whole world. Security Concerns The potential dangers and pitfalls of machine learning are vast, and include potential attacks on algorithms themselves that are deliberate and destructive. In a paper from scholars at the 4 Missouri S&T’s Peer to Peer, Vol. 1, Iss. 2 [2017], Art. 4 https://scholarsmine.mst.edu/peer2peer/vol1/iss2/4 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 5 University of California, Berkeley, researchers explore the potential dangers of machine learning and its uses in modern technology. They say, “Use of machine learning opens the possibility of an adversary who maliciously ‘mis-trains’ a learning system in an IDS” (Barreno, Nelson, Sears, Joseph & Tygar, 2006). An Intrusion Detection System (IDS) will monitor network traffic and identify potential threats. The authors identify some real dangers of unchecked machine learning algorithms, and this shows why the technical community cannot sit idly by and let potentially dangerous technology run rampant. This fear of a potentially dangerous AI may seem farfetched and even impossible given the current level of technological advancement. This however, may be more feasible than once expected. Two researchers from the University of Louisville explored a way to intentionally create a malevolent AI. They say, “Just like computer viruses and other malware is intentionally produced today, in the future we will see premeditated production of hazardous and unfriendly intelligent systems” (Pistono & Yampolskiy, 2016). This leads to a world where the average person can manufacture these unsanctioned malevolent AI’s that eventually compromise vital systems and databases. Even if someone were to make a secure machine learning algorithm that cannot be exploited, there could easily be someone deliberately making an algorithm that nullifies the good actions of ethical programmers. The ethical issues associated with compromising machine learning algorithms and making malevolent AI’s from scratch are monstrous. Both sets of authors seem convinced that the possibility of malevolent AIs are worth some level of concern. Security should always hold a top priority, but the potential creation of an AI which has no inherent conscience could be very destructive. Being aware that there could be ways to specifically target algorithms dictating AI’s becomes essential in preparing certain systems for the future. 5 Skoff: Dangers of Machine Learning Technology Published by Scholars' Mine, 2017 DANGERS OF MACHINE LEARNING TECHNOLOGY Skoff 6 Malicious AI Malicious AI has been a topic of science fiction since the inception of the concept of an autonomous artificial intelligence. Artificial intelligence could eventually completely overtake some areas of labor as the need of a human becomes negligible. The potential for an AI to become malevolent is not an immediate problem, but with machines taking over there is a real potential for this sort of thing to happen. A researcher from Louisville University says, “Just because developers might succeed in creating a safe AI, it doesn’t mean that it will not become unsafe at some later point. In other words, a perfectly friendly AI could be switched to the ‘dark side’ during the post-deployment stage” (Yampolskiy, 2015). The potential for malevolence is a concern to a professional environment with large amounts of sensitive data. A single AI which was deemed safe may suddenly “go rogue”. This prompts the question if these AI can be regulated and what implications such regulations would bring. Researchers from Stanford explored the possibility of certain regulations on algorithms. They state, “On the one hand, overlooking ethical issues may prompt negative impact and social rejection,’ but ‘on the other hand, overemphasizing the protection of individual rights in the wrong contexts may lead to regulations that are too rigid, and this in turn can cripple the chances to harness the social value of data science” (Floridi & Taddeo, 2016). This shows that there may need to be certain restrictions, but individual rights are jeopardized by the implementation of these regulations. One individual however can throw things into chaos. Stephen DeAngelis, CEO of Enterra Solutions, says, “It only takes one ‘evil genius’ to undo the best laid plans that more ethical scientists put into place” (DeAngelis, 2014). There will always be that ",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fb26f9f50fcdf7a4b43919e3c3bc02d8a158be07,https://www.semanticscholar.org/paper/fb26f9f50fcdf7a4b43919e3c3bc02d8a158be07,Learning Optimal Impedance Control During Complex 3D Arm Movements,"Humans use their limbs to perform various movements to interact with an external environment. Thanks to limb's variable and adaptive stiffness, humans can adapt their movements to the external unstable dynamics. The underlying adaptive mechanism has been investigated, employing a simple planar device perturbed by external 2D force patterns. In this work, we will employ a more advanced, compliant robot arm to extend previous work to a more realistic 3D-setting. We study the adaptive mechanism and use machine learning to capture the human adaptation behavior. In order to model human's stiffness adaptive skill, we give human subjects the task to reach for a target by moving a handle assembled on the end-effector of a compliant robotic arm. The arm is force controlled and the human is required to navigate the handle inside a non-visible, virtual maze and explore it only through robot force feedback when contacting maze virtual walls. By sampling the hand's position and force data, a computational model based on a combination of model predictive control and nonlinear regression is used to predict participants' successful trials. Our study shows that participants selectively increased the stiffness within the axis direction of uncertainty to compensate for instability caused by a divergent external force field. The learned controller was able to successfully mimic this behavior. When it is deployed on the robot for the navigation task, the robot arm successfully adapt to the unstable dynamics in the virtual maze, in a similar manner as observed in the participants’ adaptation skill.",IEEE Robotics and Automation Letters,2021,10.1109/LRA.2021.3056371,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3947116449f0106ab3de07ad104f7c56a145bd7d,https://www.semanticscholar.org/paper/3947116449f0106ab3de07ad104f7c56a145bd7d,Learning-based event response for marine robotics,"Robotic vehicles have become a critical tool for studying the under-sampled coastal ocean. This has led to new paradigms in scientific discovery. The combination of agility, reactivity, and persistent presence makes autonomous robots ideal for targeted sampling of elusive, episodic events such as algal blooms. In order to achieve this goal, they need to be deployed at the right place and time. To that end, we have designed and will soon deploy a shore-based event recognition technology to continuously monitor remote sensing imagery for algal blooms as targets for robotic field experiments. A Support Vector Machine underlies a field-tested decision support system which scientists will consult prior to deploying robots in the coastal ocean. Our aim is to target oceanographic field experiments for evaluation and verification.",2013 IEEE/RSJ International Conference on Intelligent Robots and Systems,2013,10.1109/IROS.2013.6696835,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d7ae949db6531bca5018acd43aca0e74c6c93994,https://www.semanticscholar.org/paper/d7ae949db6531bca5018acd43aca0e74c6c93994,Autonomous navigation in unknown environments using machine learning,"In this thesis, we explore the problem of high-speed autonomous navigation for a dynamic mobile robot in unknown environments. Our objective is to navigate from start to goal in minimum time, given no prior knowledge of the map, nor any explicit knowledge of the environment distribution. Faced with this challenge, most practical receding-horizon navigation methods simply restrict their action choices to the known portions of the map, and ignore the effects that future observations will have on their map knowledge, sacrificing performance as a result. In this thesis, we overcome these limitations by efficiently extending the robot's reasoning into unknown parts of the environment through supervised learning. We predict key contributors to the navigation cost before the relevant portions of the environment have been observed, using training examples from similar planning scenarios of interest. Our first contribution is to develop a model of collision probability to predict the outcomes of actions that extend beyond the perceptual horizon. We use this collision probability model as a data-driven replacement for conventional safety constraints in a receding-horizon planner, resulting in collision-free navigation at speeds up to twice as fast as conventional planners. We make these predictions using a Bayesian approach, leveraging training data for performance in familiar situations, and automatically reverting to safe prior behavior in novel situations for which our model is untrained. Our second contribution is to develop a model of future measurement utility, efficiently enabling information-gathering behaviors that can extend the robot's visibility far into unknown regions of the environment, thereby lengthening the perceptual horizon, resulting in faster navigation even under conventional safety constraints. Our third contribution is to adapt our collision prediction methods to operate on raw camera images, using deep neural networks. By making predictions directly from images, we take advantage of rich appearance-based information well beyond the range to which dense, accurate environment geometry can be reliably estimated. Pairing this neural network with novelty detection and a self-supervised labeling technique, we show that we can deploy our system initially with no training, and it will continually improve with experience and expand the set of environment types with which it is familiar.",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ba803f4d9a3f28c5783a7bb026953bdffa6b770a,https://www.semanticscholar.org/paper/ba803f4d9a3f28c5783a7bb026953bdffa6b770a,A Survey on Learning-Based Robotic Grasping,,Current Robotics Reports,2020,10.1007/S43154-020-00021-6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3b1c8b7f5f8d3e52ab264942a324487f4d4bb6f2,https://www.semanticscholar.org/paper/3b1c8b7f5f8d3e52ab264942a324487f4d4bb6f2,Future Visualizing in Swarm Robotics,"AI & Swarm intelligence is the latest trending technology in many fields especially in industries like Manufacturing, 
Automation, Control Systems, Healthcare, Energy, Transport, Defence, Space, Data Mining, etc. Artificial 
Intelligence will be a common platform to gain knowledge and share new ideas amongst the Technologist, 
Professionals, Industrialists, Researchers, Innovators and students from research area of Machine Learning. Experts 
will share their research experiences and engage in many interactive discussions at the event. 
Our high-level conferences will bring together forward thinking brands, market leaders, AI & Swarm intelligence 
evangelists and hot start-ups to explore and debate the advancements in Artificial Intelligence & Swarm intelligence, 
the impacts within Enterprise & Consumer sectors as well as Development platforms and Digital Transformation 
opportunities. Topics covered include Business Intelligence, Deep Learning, Machine Learning, AI Algorithms, 
Data & Analytics, Virtual Assistants & Catboats as well as case study based presentations proving an insight into 
the deployment of AI across different verticals. 
On behalf of the International Journal of Swarm Intelligence and Evolutionary Computation I am gratified to 
present the journal. Our articles include review, research, short communication, short commentary, full length 
articles etc. Our Journal is running successfully since 2012 with the encouragement and inspiration of readers, 
researchers and editorial board members. The achievement is due to the support given by Editorial board members, 
researchers, authors and readers.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
87f1f528a7433889df7367831fbae0958ded6d35,https://www.semanticscholar.org/paper/87f1f528a7433889df7367831fbae0958ded6d35,Image Inpainting and Classification Agent Training Based on Reinforcement Learning and Generative Models with Attention Mechanism,"What distinguishes the field of artificial intelligence (AI) from others is to develop fully independent agents that learn optimal behavior, change, and evolve solely through the communication of trial and error with the surrounding environment. Reinforcement learning (RL) can be seen in multiple aspects of Machine Learning (ML), provided the environment, reward, actions, the state will be defined. Agent training in previous years is seen to only relate to robotics, games, and self-driving cars. While trying to divert the focus of researchers from the view of self-driving cars, games, robots, etc. Here, we investigated using reinforcement learning in the aspect of task completion. We deployed our architecture in an inpainting task where the agent generates the distorted or missing image content into an eminent fidelity completed the image by using reinforcement learning to influence the generative model utilized. The Generative Adversary Network (GAN) problem of not being steady and challenging to train was overwhelmed by utilizing latent space representation. The dimension is reduced compared to the distorted or corrupted image in training the GAN. Then reinforcement learning was deployed to pick the correct GAN input to get the image’s latent space representation that is most suitable for the current input of the missing or distorted image region. In this paper, we also learned that the trained agent enhances the accuracy in a classification task of images with missing data. We successfully examined the classification enhancement on images missing 30%, 50%, and 70%.",2021 International Conference on Microelectronics (ICM),2021.0,10.1109/ICM52667.2021.9664950,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1aa7f53883d30803aa7647b3906382668e6b4351,https://www.semanticscholar.org/paper/1aa7f53883d30803aa7647b3906382668e6b4351,On Distributed Model-Free Reinforcement Learning Control With Stability Guarantee,"Distributed learning can enable scalable and effective decision making in numerous complex cyber-physical systems such as smart transportation, robotics swarm, power systems, etc. However, stability of the system is usually not guaranteed in most existing learning paradigms; and this limitation can hinder the wide deployment of machine learning in decision making of safety-critical systems. This letter presents a stability-guaranteed distributed reinforcement learning (SGDRL) framework for interconnected linear subsystems, without knowing the subsystem models. While the learning process requires data from a peer-to-peer (p2p) communication architecture, the control implementation of each subsystem is only based on its local states. The stability of the interconnected subsystems will be ensured by a diagonally dominant eigenvalue condition, which will then be used in a model-free RL algorithm to learn the stabilizing control gains. The RL algorithm structure follows an off-policy iterative framework, with interleaved policy evaluation and policy update steps. We numerically validate our theoretical results by performing simulations on four interconnected sub-systems.",IEEE Control Systems Letters,2021.0,10.1109/LCSYS.2020.3041218,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2d3f4c0caec61092dbb439acadb6e44662cacaca,https://www.semanticscholar.org/paper/2d3f4c0caec61092dbb439acadb6e44662cacaca,IoT based Agro Automation System using Machine Learning Algorithms,The needs and necessities of an agro automation system in large scale applications for monitoring various parameters are on a rise. The overview of all core challenges faced by the farmers can highly be overcome with the proposed 'IoT based Agro Automation System using Machine Learning Algorithms' which is established through multiple sensor fusion with the affirmation of results in a hybrid app. A semi-autonomous agro robot with a completely secured and reliable system is deployed for simplifying the endeavours undertaken by farmers. The monitoring and analysis is achieved with the use of PTC ThingWorx cloud computing platform and the experimental results are defined with high accuracy taking into consideration of many case studies predominantly evaluated for the development of this system in all agricultural applications.,,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1c57202cbe39a9c1ff6002f840c590ee1c943917,https://www.semanticscholar.org/paper/1c57202cbe39a9c1ff6002f840c590ee1c943917,Increasing the robustness of autonomous systems to hardware degradation using machine learning,"Autonomous systems perform predetermined tasks (missions) with minimum supervision. In most applications, the state of the world changes with time. Sensors are employed to measure part or whole of the world’s state. However, sensors often fail amidst operation; feeding as such decision-making with wrong information about the world. Moreover, hardware degradation may alter dynamic behaviour, and subsequently the capabilities, of an autonomous system; rendering the original mission infeasible. This thesis applies machine learning to yield powerful and robust tools that can facilitate autonomy in modern systems. Incremental kernel regression is used for dynamic modelling. Algorithms of this sort are easy to train and are highly adaptive. Adaptivity allows for model adjustments, whenever the environment of operation changes. Bayesian reasoning provides a rigorous framework for addressing uncertainty. Moreover, using Bayesian Networks, complex inference regarding hardware degradation can be answered. Specifically, adaptive modelling is combined with Bayesian reasoning to yield recursive estimation algorithms that are robust to sensor failures. Two solutions are presented by extending existing recursive estimation algorithms from the robotics literature. The algorithms are deployed on an underwater vehicle and the performance is assessed in real-world experiments. A comparison against standard filters is also provided. Next, the previous algorithms are extended to consider sensor and actuator failures jointly. An algorithm that can detect thruster failures in an Autonomous Underwater Vehicle has been developed. Moreover, the algorithm adapts the dynamic model online to compensate for the detected fault. The performance of this algorithm was also tested in a real-world application. One step further than hardware fault detection, prognostics predict how much longer can a particular hardware component operate normally. Ubiquitous sensors in modern systems render data-driven prognostics a viable solution. However, training is based on skewed datasets; datasets where the samples from the faulty region of operation are much fewer than the ones from the healthy region of operation. This thesis presents a prognostic algorithm that tackles the problem of imbalanced (skewed) datasets.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
88419ebe02b584cb3b0ed5b3581e5345a43a57ee,https://www.semanticscholar.org/paper/88419ebe02b584cb3b0ed5b3581e5345a43a57ee,Multi-level control architecture for Bionic Handling Assistant robot augmented by learning from demonstration for apple-picking,"ABSTRACT The control of soft continuum robots is challenging owing to their mechanical elasticity and complex dynamics. An additional challenge emerges when we want to apply Learning from Demonstration (LfD) and need to collect necessary demonstrations due to the inherent control difficulty. In this paper, we provide a multi-level architecture from low-level control to high-level motion planning for the Bionic Handling Assistant (BHA) robot. We deploy learning across all levels to enable the application of LfD for a real-world manipulation task. To record the demonstrations, an actively compliant controller is used. A variant of dynamical systems' application that are able to encode both position and orientation then maps the recorded 6D end-effector pose data into a virtual attractor space. A recent LfD method encodes the pose attractors within the same model for point-to-point motion planning. In the proposed architecture, hybrid models that combine an analytical approach and machine learning techniques are used to overcome the inherent slow dynamics and model imprecision of the BHA. The performance and generalization capability of the proposed multi-level approach are evaluated in simulation and with the real BHA robot in an apple-picking scenario which requires high accuracy to control the pose of the robot's end-effector. GRAPHICAL ABSTRACT",Adv. Robotics,2019.0,10.1080/01691864.2019.1587313,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2ed88d9705685c146bc726bbb0596e434a2914b0,https://www.semanticscholar.org/paper/2ed88d9705685c146bc726bbb0596e434a2914b0,Imitation Learning of Hierarchical Driving Model: From Continuous Intention to Continuous Trajectory,"One of the challenges to reduce the gap between the machine and the human level driving is how to endow the system with the learning capacity to deal with the coupled complexity of environments, intentions, and dynamics. In this letter, we propose a hierarchical driving model with explicit models of continuous intention and continuous dynamics, which decouples the complexity in the observation-to-action reasoning in the human driving data. Specifically, the continuous intention module takes perception to generate a potential map encoded with obstacles and intentions. Then, the potential map is regarded as a condition, together with the current dynamics, to generate a continuous trajectory as output by a continuous function approximator network, whose derivatives can be used for supervision without additional parameters. Finally, our method is validated by both datasets and stimulation, demonstrating that our method has higher prediction accuracy of displacement and velocity and generates smoother trajectories. Our method is also deployed on the real vehicle with loop latency, validating its effectiveness. To the best of our knowledge, this is the first work to produce the driving trajectory using a continuous function approximator network. Our code is available at https://github.com/ZJU-Robotics-Lab/CICT.",IEEE Robotics and Automation Letters,2020.0,10.1109/LRA.2021.3061336,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
78ed6cf43dd22689c47fe79a68fa0641bf984b0b,https://www.semanticscholar.org/paper/78ed6cf43dd22689c47fe79a68fa0641bf984b0b,This Is the Way: Sensors Auto-Calibration Approach Based on Deep Learning for Self-Driving Cars,"The technological advancement of sensors and computational power has opened a new chapter in machine learning for robotics applications, especially in image classification, segmentation, object detection, and self-driving cars. One of the challenges among these applications is improving the systems perception reliability and accuracy through sensors fusion. Hence, the focus on using Stereo-cameras and LiDARs as a complement to its accurate distance measurement. However, the calibration process of the sensors is mandatory before deployment. Some may use the conventional methods, including checkerboards, specific pattern labels, or even human labeling, which is labor-intensive and repetitive as it involves doing the same calibration process every time before using. In this work, we have proposed NetCalib – an auto-calibration methodology based on a deep neural network. This research aims to utilize the power of machine learning to find the geometric transformation between stereo cameras and LiDAR automatically. From the experiments, our method manages to find the transformations from randomly sampled artificial errors and outperforms the linear optimization-based ICP algorithm. Furthermore, this research work is open-sourced to the community to fully use the advances of the methodology and initiate collaboration and innovation in this field.",IEEE Sensors Journal,2021.0,10.1109/jsen.2021.3124788,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e0ffbc3d86f9da160d09d1d57030421fe58494ed,https://www.semanticscholar.org/paper/e0ffbc3d86f9da160d09d1d57030421fe58494ed,A Hybrid of Learning-based and Heuristic Methods for Robot Path Planning,"Generalizations of traveling salesman problem (TSP) are often found in many robotics applications where mobile robots with onboard sensors are deployed for a given sensing objective, such as detection or classification. When the sensor field-of-view is continuous and bounded, the vehicle routing problems can be approximately formulated as close enough TSP (CETSP), which is a special case of traveling salesman problem with neighborhoods (TSPN) and searches for the shortest path to visit a set of given circles. While existing CETSP solutions, such as exact and heuristic approaches, have limitations on either computational intractability or poor solution quality depending on the problem scale, machine learning approaches have been getting attentions due to their expandability. This paper proposes a novel CETSP solution that is a hybrid of learning-based and heuristic methods. The proposed method is tested in toy examples, resulting in a better solution compared to the current state-of-the-art heuristic TSP method and suggesting future works on a more expandable learning-based CETSP solution approach.",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
63a01f1d11f3c0157d96ccb21f64bc607955c7ba,https://www.semanticscholar.org/paper/63a01f1d11f3c0157d96ccb21f64bc607955c7ba,On Distributed Model-Free Reinforcement Learning Control with Stability Guarantee,"Distributed learning can enable scalable and effective decision making in numerous complex cyber-physical systems such as smart transportation, robotics swarm, power systems, etc. However, stability of the system is usually not guaranteed in most existing learning paradigms; and this limitation can hinder the wide deployment of machine learning in decision making of safety-critical systems. This paper presents a stability-guaranteed distributed reinforcement learning (SGDRL) framework for interconnected linear subsystems, without knowing the subsystem models. While the learning process requires data from a peer-to-peer (p2p) communication architecture, the control implementation of each subsystem is only based on its local states. The stability of the interconnected subsystems will be ensured by a diagonally dominant eigenvalue condition, which will then be used in a model-free RL algorithm to learn the stabilizing control gains. The RL algorithm structure follows an off-policy iterative framework, with interleaved policy evaluation and policy update steps. We numerically validate our theoretical results by performing simulations on four interconnected sub-systems.",2021 American Control Conference (ACC),2021.0,10.23919/ACC50511.2021.9482972,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
41036c8098f5823e98d5e54c3bca8327deeb267c,https://www.semanticscholar.org/paper/41036c8098f5823e98d5e54c3bca8327deeb267c,MOHE-NET: MONOCULAR OBJECT HEIGHT ESTIMATION NETWORK USING DEEP LEARNING AND SCENE GEOMETRY,"Estimating the heights of objects in the field of view has applications in many tasks such as robotics, autonomous platforms and video surveillance. Object height is a concrete and indispensable characteristic people or machine could learn and capture. Many actions such as vehicle avoiding obstacles will be taken based on it. Traditionally, object height can be estimated using laser ranging, radar or stereo camera. Depending on the application, cost of these techniques may inhibit their use, especially in autonomous platforms. Use of available sensors with lower cost would make the adoption of such techniques at higher rates. Our approach to height estimation requires only a single 2D image. To solve this problem we introduce the Monocular Object Height Estimation Network (MOHE-Net) that includes a cascade of two networks. The first network performs the object detection task. This network detects the bounding box of objects of interest. This information is then input to a second network to estimate the object height and is a linear Multi-layer Perceptron (MLP). The linear MLP model models the camera-scene geometry and does not require training or contain activation function as normal MLP did. The developed approach works for static camera set up as well as moving platform. The proposed approach performs state-of-the-art and can be deployed for obstacle avoidance on autonomous platforms. Our code is available at https://github.com/OSUPCVLab/Ford2019/tree/master/Moving%20Object%20Height% 20Estimation%20Network",,2021.0,10.5194/isprs-archives-xliii-b2-2021-557-2021,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8125830fd1aca4ceb193715acc2284aef5839de2,https://www.semanticscholar.org/paper/8125830fd1aca4ceb193715acc2284aef5839de2,A Markerless Deep Learning-based 6 Degrees of Freedom Pose Estimation for Mobile Robots using RGB Data,"Augmented Reality has been subject to various integration efforts within industries due to its ability to enhance human machine interaction and understanding. Neural networks have achieved remarkable results in areas of computer vision, which bear great potential to assist and facilitate an enhanced Augmented Reality experience. However, most neural networks are computationally intensive and demand huge processing power, thus are not suitable for deployment on Augmented Reality devices. In this work, we propose a method to deploy state of the art neural networks for real time 3D object localization on augmented reality devices. As a result, we provide a more automated method of calibrating the AR devices with mobile robotic systems. To accelerate the calibration process and enhance user experience, we focus on fast 2D detection approaches which are extracting the 3D pose of the object fast and accurately by using only 2D input. The results are implemented into an Augmented Reality application for intuitive robot control and sensor data visualization. For the 6D annotation of 2D images, we developed an annotation tool, which is, to our knowledge, the first open source tool to be available. We achieve feasible results which are generally applicable to any AR device, thus making this work promising for further research in combining high demanding neural networks with Internet of Things devices.",2020 17th International Conference on Ubiquitous Robots (UR),2020.0,10.1109/UR49135.2020.9144789,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d61e36f1c572fabecee46340bc348a57db8e6777,https://www.semanticscholar.org/paper/d61e36f1c572fabecee46340bc348a57db8e6777,Discussion on the Flexible Manufacturing and Operation Strategy Model for a Machine Tool Component Company in Taiwan,"Recently, many manufacturing industries have been facing challenges such as rising material costs, small-volume and large-variety products, shortened production cycles, increased labor costs and longer after-sales service times, which is a very tough challenge for most small and medium-sized component manufacturing suppliers. In addition to the current hot topics in the manufacturing industry - Smart Manufacturing (Industry 4.0) and lean production management, if small and medium-sized enterprises are not able to adjust the pace of manufacturing timely and find a suitable production model, they will soon be overwhelmed by the torrent of the era of speed and accuracy. In the face of the dramatic changes in the industry structure, the company can deploy the global expansion of overseas customers in advance, and adjust to apply and implement a flexible manufacturing model system through the introduction of the Industrial Internet of Things and flexible manufacturing production management. In order to meet the market needs, the manufacturing industry is gradually oriented towards customized production and the rapid development of new products. To meet such stringent requirements, flexible manufacturing becomes one of the necessary ways for enterprises to consider their development models. Therefore, the efficiency and reliability of work can be improved through the Industrial Internet of Things that facilitates machine-to-machine communication, cloud-based big data and learning and imitations of smart robots. This study is an in-depth study of a company that is currently in the process of digital transformation, collecting relevant information and reviewing the analysis to find a suitable smart manufacturing solution for the company and to explore the impact of the COVID-19 pandemic on the strategic development of the company. The findings can provide a significant reference for homotypic companies in the development of their business strategies.",International Business Research,2021.0,10.5539/ibr.v14n10p1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
da11a1655cc8779a522ed86ddf1e9db24e9373da,https://www.semanticscholar.org/paper/da11a1655cc8779a522ed86ddf1e9db24e9373da,A Study on AI-FML Robotic Agent for Student Learning Behavior Ontology Construction,"In this paper, we propose an AI-FML robotic agent for student learning behavior ontology construction which can be applied in English speaking and listening domain. The AI-FML robotic agent with the ontology contains the perception intelligence, computational intelligence, and cognition intelligence for analyzing student learning behavior. In addition, there are three intelligent agents, including a perception agent, a computational agent, and a cognition agent in the AI-FML robotic agent. We deploy the perception agent and the cognition agent on the robot Kebbi Air. Moreover, the computational agent with the Deep Neural Network (DNN) model is performed in the cloud and can communicate with the perception agent and cognition agent via the Internet. The proposed AI-FML robotic agent is applied in Taiwan and tested in Japan. The experimental results show that the agents can be utilized in the human and machine co-learning model for the future education.",2020 International Symposium on Community-centric Systems (CcS),2020.0,10.1109/CcS49175.2020.9231339,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6d0adac188152fbaa45a88ba4da788926ed8144a,https://www.semanticscholar.org/paper/6d0adac188152fbaa45a88ba4da788926ed8144a,Reinforcement Learning in Practice: Opportunities and Challenges,"This article is a gentle discussion about the ﬁeld of reinforcement learning in practice, about opportunities and challenges, touching a broad range of topics, with perspectives and without technical details. The article is based on both historical and recent research papers, surveys, tutorials, talks, blogs, books, (panel) discussions, and workshops/conferences. Various groups of readers, like researchers, engineers, students, managers, investors, ofﬁcers, and people wanting to know more about the ﬁeld, may ﬁnd the article interesting. In this article, we ﬁrst give a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI. Then we discuss opportunities of RL, in particular, products and services, games, bandits, recommender systems, robotics, transportation, ﬁnance and economics, healthcare, education, combinatorial optimization, computer systems, and science and engineering. Then we discuss challenges, in particular, 1) foundation, 2) representation, 3) reward, 4) exploration, 5) model, simulation, planning, and benchmarks, 6) off-policy/ofﬂine learning, 7) learning to learn a.k.a. meta-learning, 8) explainability and interpretability, 9) constraints, 10) software development and deployment, 11) business perspectives, and 12) more challenges. We conclude with a discussion, attempting to answer: “Why has RL not been widely adopted in practice yet?” and “When is RL helpful?”. for discussing transfer domain randomization, knowledge distillation, imitation learning, meta-learning, robust RL.",,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b38cf4ddd6937850f8c3d70752e512a492eca13a,https://www.semanticscholar.org/paper/b38cf4ddd6937850f8c3d70752e512a492eca13a,Learning from the sky : design of autonomous radio-enabled unmanned aerial vehicles in mobile cellular networks. (Apprentissage depuis le ciel : design de drones autonomes équipés de lien radio pour les réseaux mobiles cellullaires),"The interactions between the areas of mobile networking and drone robotics are currently attracting significant attention from both the robotics and the telecommunications engineering communities. The key scenarios where such interactions manifest themselves include the so-called flying base station or flying radio access network (FRAN) on the one hand, and the drone as a flying terminal on the other hand. The use of drones and unmanned aerial vehicles (UAV) as FRAN nodes is rapidly emerging as a powerful tool to complement traditional fixed terrestrial deployments. The advantage of using UAVs will be particularly felt in those use cases where being able to quickly deploy a network where and when it matters is critical. However, the success of so-called FRANs hinges on the ability of the UAVs to place themselves spatially in an efficient and autonomous manner. Having this in mind, the first part of this thesis aims to investigate current works and technologies of UAV-assisted wireless communications and develops novel methods for both the placement and trajectory design of a UAV as a flying RAN in the wireless networks for both mobile broadband coverage scenarios and IoT data harvesting scenarios. We highlight how the exploitation of city 3D maps can bring about substantial benefits for the reliable self-placement of flying radios. A suite of methods are presented that lie at the cross-road between machine learning and traditional communication theoretic network design. Regardless of the placement or trajectory design, all the algorithms operate on the basis of an array of side-information such as node GPS location, the 3D map of the city, and terrain-dependent propagation parameters allowing the prediction of radio signal strengths. While such data may be collected via the network beforehand allowing placement or the trajectory to be optimized before the actual UAV flight, part or all of the information may also have to be discovered or learned by the UAV. In this regard, a part of this thesis is devoted to discussing how to learn and estimate such information just from the UAV-borne measurements. Assuming the availability of safe cellular connectivity beyond visual line of sight, UAVs are becoming appealing solutions for a wide range of applications in the areas of transportation, goods delivery, and system monitoring. All these use cases pertain to the UAV as an aerial terminal scenario, the availability of a reliable radio link is essential to make sure the drone can be guided effectively towards completion of its mission. The main challenge however in these domains is the design of trajectories which indeed can guarantee reliable and seamless cellular connectivity all along the path",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f50002454a0e145a62d20e68aa00d6a2cb1dc5e6,https://www.semanticscholar.org/paper/f50002454a0e145a62d20e68aa00d6a2cb1dc5e6,LiDAR Data Segmentation using Deep Learning for Indoor Mapping,"Indoor mapping is an important precondition for indoor cartography (Chen and Clarke 2019). Recent years have seen a rising demand for indoor maps in fields such as indoor navigation, building information management, 3D-GIS, robotics, etc. Traditionally, indoor maps are made manually or semi-automatically from images or manual measurements. Many emerging techniques can provide useful data for mapping indoor scenes, for example, WIFI positioning, LiDAR scanning, and Bluetooth positioning. Among these data sources, LiDAR scanners can provide both accurate and rich surface information for indoor scenes and their component objects. Portable LiDAR scanners are easy to deploy to retrieve data in different kinds of indoor spaces, making them a good choice to acquire indoor map information. However, due to the irregular and unordered structure of the scans, the difficulty in conflating overlapping and adjacent scans, varying point density, and the massive volume of LiDAR data, there is a huge gap between LiDAR point cloud data and the use of LiDAR-based 3D models for indoor mapping (Xie et al. 2019; Zhang et al. 2019). In this study, we first analyzed the information requirements of indoor mapping and the characteristics of LiDAR point cloud data. Then two deep learning frameworks PointNet (Qi et al. 2017a) and PointNet++ (Qi et al. 2017b) were implemented to recognize objects in LiDAR point clouds. PointNet can deal with unordered irregular individual points and combine global feature with pointwise features. Based on PointNet, PointNet++ considers neighborhood features and incorporates multiscale features for segmentation. We trained and evaluated these networks with the open Stanford Largescale 3D Indoor Spaces Dataset (S3DIS, Armeni et al. 2017). Then we segmented realworld data from offices in the UCSB Geography department with the derived networks. To meet the demand of multiple levels of detail (LODs) (Chen and Clarke 2019) for indoor mapping, we designed a multiscale sampling strategy to sample the original dense LiDAR data. Multiscale samples were fed into the network to get segmented results with different LODs. Various segmentation results showed the feasibility of methods for indoor mapping information extraction. The main contributions of this research are to: (i) analyze LiDAR data characteristics and indoor mapping information demands; (ii) implement state-of-the-art deep learning models for indoor point cloud segmentation; and (iii) design a way to get different LODs from a single dense point cloud.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
12854fab94643c5b7b2e6af8116c420f29d10d8c,https://www.semanticscholar.org/paper/12854fab94643c5b7b2e6af8116c420f29d10d8c,Feasibility of Using Wearable EMG Armbands combined with Unsupervised Transfer Learning for Seamless Myoelectric Control,"Myoelectric control uses electromyography (EMG) signals as human-originated input to enable intuitive interfaces with machines. As such, recent rehabilitation robotics employs myoelectric control to autonomously classify user intent or operation mode using machine learning. However, performance in such applications inherently suffers from the non-stationarity of EMG signals across measurement conditions. Current laboratory-based solutions rely on careful, time-consuming control of the recordings or periodic recalibration, impeding real-world deployment. We propose that robust yet seamless myoelectric control can be achieved using a low-end, easy-to-“don” and “doff” wearable EMG sensor combined with unsupervised transfer learning. Here, we test the feasibility of one such application using a consumer-grade sensor (Myo armband, 8 EMG channels @ 200 Hz) for gesture classification across measurement conditions using an existing dataset: 5 users x 10 days x 3 sensor locations. Specifically, we first train a deep neural network using Temporal-Spatial Descriptors (TSD) with labeled source data from any particular user, day, or location. We then apply the Self-Calibrating Asynchronous Domain Adversarial Neural Network (SCADANN), which automatically adjusts the trained TSD to improve classification performance for unlabeled target data from a different user, day, or sensor location. Compared to the original TSD, SCADANN improves accuracy by 12±5.2% (avg±sd), 9.6±5.0%, and 8.6±3.3% across all possible user-to-user, day-to-day, and location-to-location cases, respectively. In one best-case scenario, accuracy improves by 26% (from 67% to 93%), whereas sometimes the gain is modest (e.g., from 76% to 78%). We also show that the performance of transfer learning can be improved by using a “better” model trained with “good” (e.g., incremental) source data. We postulate that the proposed approach is feasible and promising and can be further tailored for seamless myoelectric control of powered prosthetics or exoskeletons.",bioRxiv,2022.0,10.1101/2022.01.06.475232,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8c8868d75f5fc7a055fdbc8610ab20b0a4304829,https://www.semanticscholar.org/paper/8c8868d75f5fc7a055fdbc8610ab20b0a4304829,Deep Reinforcement Learning: Opportunities and Challenges,"This article is a gentle discussion about the field of reinforcement learning for real life, about opportunities and challenges, with perspectives and without technical details, touching a broad range of topics. The article is based on both historical and recent research papers, surveys, tutorials, talks, blogs, and books. Various groups of readers, like researchers, engineers, students, managers, investors, officers, and people wanting to know more about the field, may find the article interesting. In this article, we first give a brief introduction to reinforcement learning (RL), and its relationship with deep learning, machine learning and AI. Then we discuss opportunities of RL, in particular, applications in products and services, games, recommender systems, robotics, transportation, economics and finance, healthcare, education, combinatorial optimization, computer systems, and science and engineering. The we discuss challenges, in particular, 1) foundation, 2) representation, 3) reward, 4) model, simulation, planning, and benchmarks, 5) learning to learn a.k.a. meta-learning, 6) off-policy/offline learning, 7) software development and deployment, 8) business perspectives, and 9) more challenges. We conclude with a discussion, attempting to answer: “Why has RL not been widely adopted in practice yet?” and “When is RL helpful?”.",ArXiv,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1ce688e560251925d1363c9d9170500d8a48f302,https://www.semanticscholar.org/paper/1ce688e560251925d1363c9d9170500d8a48f302,Learning to sense three-dimensional shape deformation of a single multimode fiber,,Scientific reports,2022.0,10.1038/s41598-022-15781-8,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4cb294eb1935937bdc1b311f6dccbcd41d89a43a,https://www.semanticscholar.org/paper/4cb294eb1935937bdc1b311f6dccbcd41d89a43a,Memory Efficient Experience Replay for Streaming Learning,"In supervised machine learning, an agent is typically trained once and then deployed. While this works well for static settings, robots often operate in changing environments and must quickly learn new things from data streams. In this paradigm, known as streaming learning, a learner is trained online, in a single pass, from a data stream that cannot be assumed to be independent and identically distributed (iid). Streaming learning will cause conventional deep neural networks (DNNs) to fail for two reasons: 1) they need multiple passes through the entire dataset; and 2) non-iid data will cause catastrophic forgetting. An old fix to both of these issues is rehearsal. To learn a new example, rehearsal mixes it with previous examples, and then this mixture is used to update the DNN. Full rehearsal is slow and memory intensive because it stores all previously observed examples, and its effectiveness for preventing catastrophic forgetting has not been studied in modern DNNs. Here, we describe the ExStream algorithm for memory efficient rehearsal and compare it to alternatives. We find that full rehearsal can eliminate catastrophic forgetting in a variety of streaming learning settings, with ExStream performing well using far less memory and computation.",2019 International Conference on Robotics and Automation (ICRA),2018.0,10.1109/ICRA.2019.8793982,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bd7708600002736019905113050d6174507f94f9,https://www.semanticscholar.org/paper/bd7708600002736019905113050d6174507f94f9,Unknown Object Detection Using a One-Class Support Vector Machine for a Cloud–Robot System,"Inter-robot communication and high computational power are challenging issues for deploying indoor mobile robot applications with sensor data processing. Thus, this paper presents an efficient cloud-based multirobot framework with inter-robot communication and high computational power to deploy autonomous mobile robots for indoor applications. Deployment of usable indoor service robots requires uninterrupted movement and enhanced robot vision with a robust classification of objects and obstacles using vision sensor data in the indoor environment. However, state-of-the-art methods face degraded indoor object and obstacle recognition for multiobject vision frames and unknown objects in complex and dynamic environments. From these points of view, this paper proposes a new object segmentation model to separate objects from a multiobject robotic view-frame. In addition, we present a support vector data description (SVDD)-based one-class support vector machine for detecting unknown objects in an outlier detection fashion for the classification model. A cloud-based convolutional neural network (CNN) model with a SoftMax classifier is used for training and identification of objects in the environment, and an incremental learning method is introduced for adding unknown objects to the robot knowledge. A cloud–robot architecture is implemented using a Node-RED environment to validate the proposed model. A benchmarked object image dataset from an open resource repository and images captured from the lab environment were used to train the models. The proposed model showed good object detection and identification results. The performance of the model was compared with three state-of-the-art models and was found to outperform them. Moreover, the usability of the proposed system was enhanced by the unknown object detection, incremental learning, and cloud-based framework.",Sensors,2022.0,10.3390/s22041352,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5eff47388ed6a2a76b57ac83a867b970c6ce34b9,https://www.semanticscholar.org/paper/5eff47388ed6a2a76b57ac83a867b970c6ce34b9,Adaptive Learning: Algorithms and Complexity,"Recent empirical success in machine learning has led to major breakthroughs in application domains including computer vision, robotics, and natural language processing. There is a chasm between theory and practice here. Many of the most impressive practical advances in learning rely heavily on parameter tuning and domain-specific heuristics, and the development effort required to deploy these methods in new domains places a great burden on practitioners. On the other hand, mathematical theory of learning has excelled at producing broadly applicable algorithmic principles (stochastic gradient methods, boosting, SVMs), but tends to lag behind in state-of-the-art performance, and may miss out on practitioners’ intuition. Can we distill our collective knowledge of “what works” into learning procedures that are general-purpose, yet readily adapt to problem structure in new domains? We propose to bridge the gap and get the best of both worlds through adaptive learning: Learning procedures that go beyond the worst case and automatically exploit favorable properties of real-world instances to get improved performance. The aim of this thesis is to develop adaptive algorithms and investigate their limits, and to do so in the face of real-world considerations such as computation, interactivity, and robustness. In more detail, we: 1. introduce formalism to evaluate and assert optimality of adaptive learning procedures. 2. develop tools to prove fundamental limits on adaptivity. 3. provide efficient and adaptive algorithms to achieve these limits. In classical statistical decision theory, learning procedures are evaluated by their worst-case performance (e.g., prediction accuracy) across all problem instances. Adaptive learning evaluates performance not just worst case, but in the best case and in between. This necessitates the development of new statistical and information-theoretic ideas to provide instance-dependent performance guarantees, as well as new algorithmic and computational principles to derive efficient and adaptive algorithms. The first major contribution this thesis makes concerns sequential prediction, or online learning. We prove the equivalence of adaptive algorithms, probabilistic objects called martingale inequalities, and geometric objects called Burkholder functions. We leverage the equivalence to provide: 1. a theory of learnability for adaptive online learning. 2. a unified algorithm design principle for adaptive online learning. The equivalence extends the classical Vapnik-Chervonenkis theory of (worst-case) statistical learning to adaptive online learning. It allows us to derive new learning procedures that efficiently adapt to problem structure, and serves as our starting point for investigating adaptivity in real-world settings. In many modern applications, we are faced with data that may be streaming, non-i.i.d., or simply too large to fit in memory. In others, we may interact with and influence the data generating process through sequential decisions. Developing adaptive algorithms for these challenges leads to fascinating new questions. Must we sacrifice adaptivity to process and make predictions from data as it arrives in a stream? Can we adapt while balancing exploration and exploitation? Major contributions this thesis makes toward these questions include: • We introduce a notion of “sufficient statistics” for online learning and show that this definition leads to adaptive algorithms with low memory requirements. • We develop large scale optimization algorithms for learning that adapt to problem structure via automatic parameter tuning, and characterize their limits. • We give adaptive algorithms for interactive learning/sequential decision making in contextual bandits, a simple reinforcement learning setting. Our main result here is a new margin theory paralleling that of classical statistical learning. • We provide robust sequential prediction algorithms that obtain optimal instance dependent performance guarantees for statistical learning, yet make no assumptions on the data generating process. We then characterize their limits. • We design algorithms that adapt to model misspecification in the ubiquitous statistical task of logistic regression. Here we give a new improper learning algorithm that attains a doubly-exponential improvement over sample complexity lower bounds for proper learning. This resolves a COLT open problem of McMahan and Streeter (2012), as well as two open problems related to adaptivity in bandit multiclass classification (Abernethy and Rakhlin, 2009) and online boosting (Beygelzimer et al., 2015).",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d66851d037a6acf7c31476195e5e828c2fdbdd6d,https://www.semanticscholar.org/paper/d66851d037a6acf7c31476195e5e828c2fdbdd6d,Detaching the strings : Practical algorithms for Learning from Demonstration,"Learning from Demonstation (LfD) is a machine learning paradigm wherby agents are trained to execute tasks by observing demonstrations from other agents, most commonly humans. LfD methods are particularly appealing for tasks that cannot be easily defined either in the form of code or cost functions. Such tasks include robots acting in unstructured environments with implicit rules such as social interaction. While powerful a significant amount of effort is required in order to deploy LfD algorithms in the real world. This thesis provides a number of algortihms that make LfD more applicable to such environments. It adresses problems encountered at all levels of the LfD pipeline for robotics, such as data collection and interpretation, path planning, local control, automatic task decomposition and meta-control.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fde0ff9187b32162e0cd46648f46ae51ecd8cb62,https://www.semanticscholar.org/paper/fde0ff9187b32162e0cd46648f46ae51ecd8cb62,Deep Learning-based Plane Pose Regression towards Training in Freehand Obstetric Ultrasound,"In obstetrics ultrasound (US) training, the learner’s ability to mentally build a three-dimensional (3D) map of the fetus from a two-dimensional (2D) US image represents a major challenge. Pre-defined anatomical planes, known as standard planes (SPs), retain a sig- nificant clinical relevance. Their correct acquisition re- quires extensive experience due to the complexity of SP definition, their high intra- and low inter-class variation. Previous work proposed automating the extraction of SPs from data acquired with a simplified protocol rather than assisting operators in acquiring typical freehand 2D SPs. These methods, based on traditional machine learn- ing (ML) approaches [1] or convolutional neural network (CNN) [2], are mostly confined to selection or classifi- cation of SPs. Automatic approaches for the localisation of planes in 3D volumes are based either on supervised learning (SL) [3] or reinforcement learning (RL) [4]. The first ones, even though effective, can mainly localize one single plane at a time or are tailored to just one organ. Also, they attempt to learn a mapping from high- dimensional volumetric data to low-dimensional abstract features directly, making the training difficult. In the second case, most of the available solutions employing deep reinforcement learning (DRL) are based on robotic navigation and are used at the point of care. To the best of our knowledge, there are no available training systems to guide the sonographers in freehand obstetric US. Currently, training in obstetric US is focused on SPs recognition. The autonomous probe navigation towards SPs remains a highly challenging task due to the need to interpret variable and complex images and their spatial relationship. Our work aims to develop a real-time training platform to guide inexperienced sonographers in acquiring proper obstetric US images that could be potentially deployed for existing US machines.",Proceedings of The 14th Hamlyn Symposium on Medical Robotics 2022,2022.0,10.31256/hsmr2022.24,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8e4e5d7369fa701bd9d852694c77d7ca9a31f1df,https://www.semanticscholar.org/paper/8e4e5d7369fa701bd9d852694c77d7ca9a31f1df,Intelligent Robot for Worker Safety Surveillance: Deep Learning Perception and Visual Navigation,"The fatal injury rate for the construction industry is higher than the average for all industries. Recently, researchers have shown an increased interest in occupational safety in the construction industry. However, all the current methods using conventional machine learning with stationary cameras suffer from some severe limitations, perceptual aliasing (e.g., different places/objects can appear identical), occlusion (e.g., place/object appearance changes between visits), seasonal / illumination changes, significant viewpoint changes, etc. This paper proposes a perception module using end-to-end deep-learning and visual SLAM (Simultaneous Localization and Mapping) for an effective and efficient object recognition and navigation using a differential-drive mobile robot. Various deep-learning frameworks and visual navigation strategies with evaluation metrics are implemented and validated for the selection of the best model. The deep-learning model's predictions are evaluated via the metrics (model speed, accuracy, complexity, precision, recall, P-R curve, F1 score). The YOLOv3 shows the best trade-off among all algorithms, 57.9% mean average precision (mAP), in real-world settings, and can process 45 frames per second (FPS) on NVIDIA Jetson TX2 which makes it suitable for real-time detection, as well as a right candidate for deploying the neural network on a mobile robot. The evaluation metrics used for the comparison of laser SLAM are Root Mean Square Error (RMSE). The Google Cartographer SLAM shows the lowest RMSE and acceptable processing time. The experimental results demonstrate that the perception module can meet the requirements of head protection criteria in Occupational Safety and Health Administration (OSHA) standards for construction. To be more precise, this module can effectively detect construction worker's non-hardhat-use in different construction site conditions and can facilitate improved safety inspection and supervision.",2020 International Conference on Advanced Robotics and Intelligent Systems (ARIS),2020.0,10.1109/ARIS50834.2020.9205772,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9763d91e225a4fe696695e33b92b745554698bc8,https://www.semanticscholar.org/paper/9763d91e225a4fe696695e33b92b745554698bc8,Fault-Aware Robust Control via Adversarial Reinforcement Learning,"Robots have limited adaptation ability compared to humans and animals in the case of damage. However, robot damages are prevalent in realworld applications, especially for robots deployed in extreme environments. The fragility of robots greatly limits their widespread application. We propose an adversarial reinforcement learning framework, which significantly increases robot robustness over joint damage cases in both manipulation tasks and locomotion tasks. The agent is trained iteratively under the joint damage cases where it has poor performance. We validate our algorithm on a three-fingered robot hand and a quadruped robot. Our algorithm can be trained only in simulation and directly deployed on a real robot without any fine-tuning. It also demonstrates exceeding success rates over arbitrary joint damage cases.","2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",2020.0,10.1109/CYBER53097.2021.9588329,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4ec03343db637c5f1a651f84643b8a1cf86e3e7a,https://www.semanticscholar.org/paper/4ec03343db637c5f1a651f84643b8a1cf86e3e7a,Autonomous Robotics for Identification and Management of Invasive Aquatic Plant Species,"Invasive aquatic plant species can expand rapidly throughout water bodies and cause severely adverse economic and ecological impacts. While mechanical, chemical, and biological methods exist for the identification and treatment of these invasive species, they are manually intensive, inefficient, costly, and can cause collateral ecological damage. To address current deficiencies in aquatic weed management, this paper details the development of a small fleet of fully autonomous boats capable of subsurface hydroacoustic imaging (to scan aquatic vegetation), machine learning (for automated weed identification), and herbicide deployment (for vegetation control). These capabilities aim to minimize manual labor and provide more efficient, safe (reduced chemical exposure to personnel), and timely weed management. Geotagged hydroacoustic imagery of three aquatic plant varieties (Hydrilla, Cabomba, and Coontail) was collected and used to create a software pipeline for subsurface aquatic weed classification and distribution mapping. Employing deep learning, the novel software achieved a classification accuracy of 99.06% after training.",Applied Sciences,2019.0,10.3390/APP9122410,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
73cecca9d37e44f93ad5b26c455c714e45833031,https://www.semanticscholar.org/paper/73cecca9d37e44f93ad5b26c455c714e45833031,In-Situ Learning from a Domain Expert for Real World Socially Assistive Robot Deployment,"The effectiveness of Socially Assistive Robots (SAR) relies on their ability to motivate particular user behaviours, e.g. engagement with a task, requiring complex social interactions tailored to the needs and motivations of the user. Professionals from human-centred domains such as healthcare are experts in such interactions, but their ability to contribute to SAR development has traditionally been limited to the identification of applications and key design requirements. In this work we demonstrate how interactive machine learning offers a way for such experts to be involved at every stage of design and automation of a robot, as well as the value of taking this approach. We present a novel technical framework for in-situ, online interactive machine learning that can be used in ecologically-valid humanrobot interactions. Using this framework, we were able generate fully autonomous, appropriate and personalised robot behaviour in a high-dimensional application of assistive robotics.",Robotics: Science and Systems,2020.0,10.15607/rss.2020.xvi.059,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6547c807c73907ff98cf2f6244c7fe26c1f47c2d,https://www.semanticscholar.org/paper/6547c807c73907ff98cf2f6244c7fe26c1f47c2d,Improving robotic grasping performance using machine learning techniques,"approved: Ravi Balasubramanian Robots are being utilized in ever more complex tasks and environments to help humans with difficult or dangerous tasks. However, robotic grasping is still in its infancy and is one of the limiting factors which prevent the deployment of robots in the home and other assisted living scenarios. Traditional methods for grasp planning use grasp metrics, which are numerical computations of the kinematic arrangement of the hand and object. However, they are insufficient alone for accounting for all of the variables involved in the grasping process shown by their poor performance when implemented on a robotic platform. We use grasp testing data, along with a machine learning algorithm, in order to learn the complex relationship among all of the grasp metrics so as to improve grasp prediction performance. We then evaluate the resulting machine algorithm to validate the results and compare them to the individual metrics and state of the art grasp planners. c ©Copyright by Alex Keith Goins May 19, 2014 All Rights Reserved Improving Robotic Grasping Performance Using Machine Learning Techniques",,2014.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0966363ea46368f297999b026c827f0f5ea7fcc9,https://www.semanticscholar.org/paper/0966363ea46368f297999b026c827f0f5ea7fcc9,Efficient Machine-Type Communication Using Multi-Metric Context-Awareness for Cars Used as Mobile Sensors in Upcoming 5G Networks,"Upcoming 5G-based communication networks will be confronted with huge increases in the amount of transmitted sensor data related to massive deployments of static and mobile Internet of Things (IoT) systems. Cars acting as mobile sensors will become important data sources for cloud-based applications like predictive maintenance and dynamic traffic forecast. Due to the limitation of available communication resources, it is expected that the grows in Machine-Type Communication (MTC) will cause severe interference with Human-to-human (H2H) communication. Consequently, more efficient transmission methods are highly required. In this paper, we present a probabilistic scheme for efficient transmission of vehicular sensor data which leverages favorable channel conditions and avoids transmissions when they are expected to be highly resource-consuming. Multiple variants of the proposed scheme are evaluated in comprehensive realworld experiments. Through machine learning based combination of multiple context metrics, the proposed scheme is able to achieve up to 164% higher average data rate values for sensor applications with soft deadline requirements compared to regular periodic transmission.",2018 IEEE 87th Vehicular Technology Conference (VTC Spring),2018.0,10.1109/VTCSpring.2018.8417753,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f80f13a89d3b6f86aa2e2a1859255ed16cb8a442,https://www.semanticscholar.org/paper/f80f13a89d3b6f86aa2e2a1859255ed16cb8a442,Put Deep Learning to Work: Accelerate Deep Learning through Amazon SageMaker and ML Services,"Deploying deep learning (DL) projects are becoming increasingly more pervasive at enterprises and startups alike. At Amazon, Machine Learning University (MLU)-trained engineers are taking DL to every aspect of Amazon's businesses, beyond just Amazon Go, Alexa, and Robotics. In this workshop, Wenming Ye (AWS), Rachel Hu (AWS), and Miro Enev (Nvidia) offer a practical next step in DL learning with instructions, and hands-on labs using the latest Nvidia GPUs and AWS Inferentia. You will explore the current trends powering AI/DL adoption, powerful new GPU/AWS Inferentia accelerator instances, distributed training and inference optimization in neural networks.",KDD,2020.0,10.1145/3394486.3406698,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8fc408db0aeb4f5a564334d218ece78ac729a48a,https://www.semanticscholar.org/paper/8fc408db0aeb4f5a564334d218ece78ac729a48a,Autonomous robotic ground penetrating radar surveys of ice sheets; Using machine learning to identify hidden crevasses,"This paper presents methods to continue development of a completely autonomous robotic system employing ground penetrating radar imaging of the glacier sub-surface. We use well established machine learning algorithms and appropriate un-biased processing, particularly those which are also suitable for real-time image analysis and detection. We tested and evaluated three processing schemes in conjunction with a Support Vector Machine (SVM) trained on 15 examples of Antarctic GPR imagery, collected by our robot and a Pisten Bully tractor in 2010 in the shear zone near McMurdo Station. Using a modified cross validation technique, we correctly classified all examples with a radial basis kernel SVM trained and evaluated on down-sampled and texture-mapped GPR images of crevasses, compared to 60% classification rate using raw data. We also test the most successful processing scheme on a larger dataset, comprised of 94 GPR images of crevasse crossings recorded in the same deployment. Our experiments demonstrate the promise and reliability of real-time object detection and classification with robotic GPR imaging surveys.",2012 IEEE International Conference on Imaging Systems and Techniques Proceedings,2012.0,10.1109/IST.2012.6295593,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c98ebb97b93fb8850aade8512afc62032416f67e,https://www.semanticscholar.org/paper/c98ebb97b93fb8850aade8512afc62032416f67e,Physics-penalised Regularisation for Learning Dynamics Models with Contact,"Robotic systems, such as legged robots and manipulators, often handle states which involve ground impact or interaction with objects present in their surroundings; both of which are physically driven by contact. Dynamics model learning tends to focus on continuous motion, yielding poor results when deployed on real systems exposed to non-smooth frictional discontinuities. Inspired by a recent promising direction in machine learning, in this work we present a novel method for learning dynamics models undergoing contact by augmenting data-driven deep models with physicspenalised regularisation. Precisely, this paper conceptually formalises a novel framework for using an impenetrability component in the physics-based loss function directly within the learning objective of neural networks. Our results demonstrate that our method shows superior performance to using normal deep models for learning non-smooth dynamics models of robotic manipulators, strengthening their potential for deployment in contact-rich environments.",L4DC,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3ae46dc15c88365d9531bd8438ccb7025ffbb6e7,https://www.semanticscholar.org/paper/3ae46dc15c88365d9531bd8438ccb7025ffbb6e7,On the suitability of incremental learning for regression tasks in exoskeleton control,"In recent times, a new generation of modern exoskeleton robots has come into existence, that aims to utilize machine learning to learn the specific needs and preferences of its users. A simple way to facilitate a personalization of an exoskeleton to the end user is to make use of incremental algorithms that keep learning throughout their deployment. However, it is not clear, if any standard algorithms are fast enough to keep pace with sudden change points in the data stream, like for example the change in movement pattern from a normal walk to going up the stairs. In this paper, we study how well common incremental regression algorithms are suited to predict such an ongoing data stream. We use both, theoretical benchmarks and real world human movement data, to evaluate how fast an algorithm reacts to change points in the data, and how well it is able to remember reoccurring patterns. The results show that a simple KNN algorithm outperforms all other more sophisticated models.",2021 IEEE Symposium Series on Computational Intelligence (SSCI),2021.0,10.1109/SSCI50451.2021.9660138,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e4ea02950a3ed6d2cf1f338d5152759a8078bb48,https://www.semanticscholar.org/paper/e4ea02950a3ed6d2cf1f338d5152759a8078bb48,Deep Learning Based Smart Survilance Robot,". Surveillance Robot aims to blend IoT capabilities with the support of cloud and machine learning is an advancement to deliver a sophisticated solution for real time security. Industrial and commercial surveillance data security is required for small camera as well as large scale deployment with a drones or robot cars. This paper deals with face recognition using AWS Rekognition and video streaming using AWS kinesis and AWS SNS(Simple notification Service) . AWS Rekognition uses deep learning algorithms to introspect the video stream and find objects / faces on them and compare it with the collection of information that it has trained previously. It detects face with video feed and scans the database to identify the person with AWS Rekognition ,there is also an option of adding new faces by uploading photo of the person to an S3 bucket and face can be indexed . ,","Proceedings of the Fist  International Conference on Advanced Scientific Innovation in Science, Engineering and Technology, ICASISET 2020, 16-17 May 2020, Chennai, India",2021.0,10.4108/EAI.16-5-2020.2304191,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bc06e6dc6208add59e84ca086d5908fae3a54cea,https://www.semanticscholar.org/paper/bc06e6dc6208add59e84ca086d5908fae3a54cea,Performative Body Mapping : A Creative Robotics Method for Learning Expressive Movement,"Performative Body Mapping is a method for harnessing the embodied expertise of dancers to inform the design, movement and behaviour of non-anthropomorphic social robots. The method simplifies the correspondence problem through the novel use of costumes that allow much of the difficult human-robot mapping to be delegated to dancers. A mixture density recurrent neural network has been used to model sequences captured during movement studies to create new dance sequences in the style of the dancers inhabiting a costume. Performative Body Mapping A common underlying assumption in the design of social robots is that human-like or pet-like appearance makes relating to them easier. Studies show, however, that the more human-like a robot appears, the more people expect it to also have human-level cognitive and social capabilities, which results in frustration when robots fail to meet expectations [1]. Designing more abstract robots poses the question of how we might relate to them but artists and performers have been exploring the capacity for the movement of abstract robots to evoke affective responses for decades [e.g. 2, 3]. This paper describes a method for accessing the embodied kinaesthetic knowledge of dancers to inform a learning process for a machine-like robot to develop a social presence. Performative Body Mapping (PBM) harnesses the embodied expertise of dancers to inform the design and movement of non-anthropomorphic robots by relying on the kinesthetic ability of dancers to embody another, non-human body and deploying a ‘costume’, i.e., a wearable object that both restricts and extends a dancer’s body. Costume serve as embodied interfaces for mapping between dancers and robots, providing dancers with embodied insights into the morphology and capabilities of a robot, which supports the development of a repertoire of movements and allows motion capture in a form that the can be learned from, with little or no translation. Consequently, PBM significantly simplifies the correspondence problem, common in demonstration learning [4], by mapping between similar bodies and delegating much of the difficult mapping to movement experts. Initial PBM movement studies focussed on form-finding through embodied exploration of costumes shaped by ‘enabling constraints’, e.g., no front or back, head or limbs, see [5] for details. A simple cube costume was developed (Figure 1a) as this was shown to be highly expressive when activated by skilled dancers. Custom software estimated the pose of the costume from video of the dancer-activated costume to inform the design of a mechanical prototype (Figure 1b) and provide data for learning. From the approx. 15 hours of video, 5 hours was extracted for the purposes of learning. Each data point consisted of six values describing the movement between poses, i.e., difference in location (x, y, ∗Metamakers Institute, Falmouth University, UK rob.saunders@falmouth.ac.uk †Metamakers Institute, Falmouth University, UK petra.gemeinboeck@falmouth.ac.uk 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada. (a) Costume inhabited by dancer. (b) Robot motion testing. (c) Robot as ‘plinth’. Figure 1: Evolution from (a) costume to (b) prototype and (c) exhibition. Petra Gemeinboeck c © z) and orientation (yaw, pitch, roll) of the centre of top surface of the costume. This describes the observed movement as a pair of idealised joints but does not define the motor movements required to achieve it, which is handled by a fixed motor controller. We applied a mixture density LSTM network, previously shown to successfully synthesise handwriting [6] and human choreography [7], to generate movements in the style of the captured recordings. The network architecture consists of a 6-value input layer, 3 hidden layers with 512 LSTM cells each and an output of 20 Gaussian mixtures to approximate the distribution of the next movement. The network was trained with RMSProp using Back Propagation Through-Time. The synthesised movement sequences were subjectively assessed by movement experts against the original performances of the dancers and judged to have captured important movement qualities. The robot has been exhibited in Australia and the UK (Figure 1c). Audience studies suggest that, while it is clearly perceived as non-anthropomorphic, it is successful at conveying expressive agency [5]. Future work will include conditioning the predictions of the network using labels assigned by movement experts to the recordings, similar to [6], to allow extended sequences to be developed. An additional grounding stage will allow the fixed motor controller to be replaced with one learned through ‘motor babbling’. Finally, intrinsically-motivated reinforcement learning will be used to explore the potential of improvised movements and integrate audience reactions.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7807653c33b3ccb154af8f8863edb7988112f29a,https://www.semanticscholar.org/paper/7807653c33b3ccb154af8f8863edb7988112f29a,Human-Machine and Human-Robot Interaction for Long-Term User Engagement and Behavior Change,"The nexus of in-home intelligent assistants, activity tracking, and machine learning creates opportunities for personalized virtual and physical agents / robots that can positively impacts user health and quality of life. Well beyond providing information, such agents can serve as physical and mental health and education coaches and companions that support positive behavior change. However, sustaining user engagement and motivation over long-term interactions presents complex challenges. Our work over the past 15 years has addressed those challenges by developing human-machine (human-robot) interaction methods for socially assistive robotics that utilize multi-modal interaction data and expressive agent behavior to monitor, coach, and motivate users to engage in heath- and wellness-promoting activities. This talk will present methods and results of modeling, learning, and personalizing user motivation, engagement, and coaching of healthy children and adults, as well as stroke patients, Alzheimer's patients, and children with autism spectrum disorders, in short and long-term (month+) deployments in schools, therapy centers, and homes, and discuss research and commercial implications for technologies aimed at human daily use.",MobiCom,2019.0,10.1145/3300061.3300141,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
394e44328d4edf46ee862300f43b9f85869f6cd8,https://www.semanticscholar.org/paper/394e44328d4edf46ee862300f43b9f85869f6cd8,Control-Induced Learning for Autonomous Robots,"The recent progress of machine learning, driven by pervasive data and increasing computational power, has shown its potential to achieve higher robot autonomy. Yet, with too much focus on generic models and data-driven paradigms while ignoring inherent structures of control systems and tasks, existing machine learning methods typically suffer from data and computation inefficiency, hindering their public deployment onto general real-world robots. In this thesis work, we claim that the efficiency of autonomous robot learning can be boosted by two strategies. One is to incorporate the structures of optimal control theory into control-objective learning, and this leads to a series of control-induced learning methods that enjoy the complementary benefits of machine learning for higher algorithm autonomy and control theory for higher algorithm efficiency. The other is to integrate necessary human guidance into task and control objective learning, leading to a series of paradigms for robot learning with minimal human guidance on the loop.The first part of this thesis focuses on the control-induced learning, where we have made two contributions. One is a set of new methods for inverse optimal control, which address three existing challenges in control objective learning: learning from minimal data, learning time-varying objective functions, and learning under distributed settings. The second is a Pontryagin Differentiable Programming methodology, which bridges the concepts of optimal control theory, deep learning, and backpropagation, and provides a unified end-to-end learning framework to solve a broad range of learning and control tasks, including inverse reinforcement learning, neural ODEs, system identification, model-based reinforcement learning, and motion planning, with data- and computation- efficient performance.The second part of this thesis focuses on the paradigms for robot learning with necessary human guidance on the loop. We have made two contributions. The first is an approach of learning from sparse demonstrations, which allows a robot to learn its control objective function only from human-specified sparse waypoints given in the observation (task) space; and the second is an approach of learning fromhuman’s directional corrections, which enables a robot to incrementally learn its control objective, with guaranteed learning convergence, from human’s directional correction feedback while it is acting.",,2021.0,10.25394/PGS.14829270.V1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cab7d01a2ab666b485e1e505996654f2615d94a6,https://www.semanticscholar.org/paper/cab7d01a2ab666b485e1e505996654f2615d94a6,Auditing Robot Learning for Safety and Compliance during Deployment,"Robots of the future are going to exhibit increasingly human-like and super-human intelligence in a myriad of different tasks. They are also likely going to fail and be incompliant with human preferences in increasingly subtle ways. Towards the goal of achieving autonomous robots, the robot learning community has made rapid strides in applying machine learning techniques to train robots through data and interaction. This makes the study of how best to audit these algorithms for checking their compatibility with humans, pertinent and urgent. In this paper, we draw inspiration from the AI Safety and Alignment communities and make the case that we need to urgently consider ways in which we can best audit our robot learning algorithms to check for failure modes, and ensure that when operating autonomously, they are indeed behaving in ways that the human algorithm designers intend them to. We believe that this is a challenging problem that will require efforts from the entire robot learning community, and do not attempt to provide a concrete framework for auditing. Instead, we outline highlevel guidance and a possible approach towards formulating this framework which we hope will serve as a useful starting point for thinking about auditing in the context of robot learning.",ArXiv,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
86ac6d446ea6ac37087a257b2173a8db142e0c52,https://www.semanticscholar.org/paper/86ac6d446ea6ac37087a257b2173a8db142e0c52,Extended Abstract: Safe Learning from Hallucination for Navigation in the Wild,"One conundrum of using machine learning to find safe navigation systems that can be deployed in the wild is that, in order to produce safe motions in obstacle-occupied spaces, a robot needs to first gather experience in those dangerous spaces before it has learned how to generate safe motions. This conundrum becomes even more severe when the goal is to navigate in challenging or even adversarial real-world scenarios. One solution to learn navigation in the wild is to learn from pre-supplied, good demonstrations (e.g., from a human expert (Xiao et al. 2020; Wang et al. 2021)) or perform exploration based on trial-and-error (Xu et al. 2021) in the deployment environment (or one very similar to it), but both of these approaches become costly in dangerous spaces in the wild. Although learning to find an optimal plan among the variety of maneuvers the robot can perform is difficult without many trial-and-error attempts or an expert who is already capable of doing so, given a plan performed in obstacle-free space, it is relatively easy to find an obstacle configuration for which that plan is optimal. Based on this observation, instead of finding the optimal motion plan for a specific obstacle configuration, we consider this “dual” problem of classical motion planning and seek to find the obstacle configuration(s) where a specific motion plan is guaranteed to be optimal. We name this process hallucination. Solving this problem gives us the freedom to allow random exploration in a completely safe obstacle-free space and collect an extensive amount of motion plans, whose optimally will be assured by a class of hallucination techniques. In this work, we introduce two of those techniques: to hallucinate (1) the (unique) most constrained and (2) a (not unique) minimal obstacle configuration. We then train an end-to-end motion planner that can produce motions to navigate through realistic obstacles during deployment in the wild. Both methods are tested on a physical mobile robot in real-world cluttered environments.",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5205eace77c1b209424f138123a16e62818ab90b,https://www.semanticscholar.org/paper/5205eace77c1b209424f138123a16e62818ab90b,Grape Bunch Detection at Different Growth Stages Using Deep Learning Quantized Models,"The agricultural sector plays a fundamental role in our society, where it is increasingly important to automate processes, which can generate beneficial impacts in the productivity and quality of products. Perception and computer vision approaches can be fundamental in the implementation of robotics in agriculture. In particular, deep learning can be used for image classification or object detection, endowing machines with the capability to perform operations in the agriculture context. In this work, deep learning was used for the detection of grape bunches in vineyards considering different growth stages: the early stage just after the bloom and the medium stage where the grape bunches present an intermediate development. Two state-of-the-art single-shot multibox models were trained, quantized, and deployed in a low-cost and low-power hardware device, a Tensor Processing Unit. The training input was a novel and publicly available dataset proposed in this work. This dataset contains 1929 images and respective annotations of grape bunches at two different growth stages, captured by different cameras in several illumination conditions. The models were benchmarked and characterized considering the variation of two different parameters: the confidence score and the intersection over union threshold. The results showed that the deployed models could detect grape bunches in images with a medium average precision up to 66.96%. Since this approach uses low resources, a low-cost and low-power hardware device that requires simplified models with 8 bit quantization, the obtained performance was satisfactory. Experiments also demonstrated that the models performed better in identifying grape bunches at the medium growth stage, in comparison with grape bunches present in the vineyard after the bloom, since the second class represents smaller grape bunches, with a color and texture more similar to the surrounding foliage, which complicates their detection.",Agronomy,2021.0,10.3390/agronomy11091890,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4e247db724384525936adedae4aae7f7ef131367,https://www.semanticscholar.org/paper/4e247db724384525936adedae4aae7f7ef131367,Efficient Online Transfer Learning for 3D Object Classification in Autonomous Driving,"Autonomous driving has achieved rapid development over the last few decades, including the machine perception as an important issue of it. Although object detection based on conventional cameras has achieved remarkable results in 2D/3D, non-visual sensors such as 3D LiDAR still have incomparable advantages in the accuracy of object position detection. However, the challenge also exists with the difficulty in properly interpreting point cloud generated by LiDAR. This paper presents a multi-modal-based online learning system for 3D LiDAR-based object classification in urban environments, including cars, cyclists and pedestrians. The proposed system aims to effectively transfer the mature detection capabilities based on visual sensors to the new model learning based on non-visual sensors through a multi-target tracker (i.e. using one sensor to train another). In particular, it integrates the Online Random Forests (ORF) [1] method, which inherently has the abilities of fast and multi-class learning. Through experiments, we show that our system is capable of learning a high-performance model for LiDAR-based 3D object classification on-the-fly, which is especially suitable for robotics in-situ deployment while responding to the widespread challenge of insufficient detector generalization capabilities.",2021 IEEE International Intelligent Transportation Systems Conference (ITSC),2021.0,10.1109/itsc48978.2021.9565110,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fe5b5e48e39e49e7d75dc6c2a61057cddc6adfb3,https://www.semanticscholar.org/paper/fe5b5e48e39e49e7d75dc6c2a61057cddc6adfb3,An optimization strategy for HMI panel recognition of CNC machines using a CNN deep-learning network,"This paper suggests an optimization strategy to train a CNN deep-learning network, which successfully recognizing working status on the HMI panels of CNC machines. To verify the developed strategy, the research experiments using a prototype that consists of a CNC milling machine and an industrial robot. In the optimization strategy, the research first defines a length-varying hyperparameter list for the deep-learning network, and the entities in the list adjust themselves to optimize the model scales. During the optimization process, this paper adopts a two-stage training scheme that gradually augments image datasets to improve HMI control-panel recognition performances, such as recognition accuracy and recognition speed to identify the CNC machine working status. Using an open-source PyTorch platform, this research establishes a cloud-based distributed architecture to build training codes for the deep-learning network, in which an applicable optimization model is deployed to recognize the CNC control-panel working status. The optimization strategy employs minimal codes to rebuild the architecture and the least efforts to reform the manufacturing system. The optimally trained model provides up to a 99.34% CNC panel-message recognition accuracy and a high-speed recognition of 100 images in 0.6 s. Moreover, the developed optimization strategy enables the prediction of necessitated dataset augmentation to training a practically implemented CNN network.",Concurr. Eng. Res. Appl.,2021.0,10.1177/1063293X21998083,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bff918a9c7e9a08b47918547e86b311bcc6b4412,https://www.semanticscholar.org/paper/bff918a9c7e9a08b47918547e86b311bcc6b4412,Machine Learning and Adaptation of Domain Models to Support Real Time Planning in Autonomous Systems,"Background: Simulating low-level cognitive behaviour, such as reaction to stimuli or autonomic activity, has been a major focus of research and development in the autonomous systems (AS) community for many years. Automated assessment of sensor data, and reactive action selection in the form of condition-action pairs, is well developed in robotic and control application areas. In contrast, a characteristic of high-level cognitive behaviour is the ability to reason with knowledge of action and change in order to synthesise plans to achieve desired long term goals. This area is not so well understood, or manifested in applications of real time dynamic AS. Utilising such reasoning abilities enables an agent to choose which action to perform to achieve a desired task based on a deliberative process involving knowledge of the environment, resources, goals, and available actions. The implementation of such high-level behaviour has been considered problematic in the AS community in the past, regarding both the real time reasoning and knowledge representation aspects as intractable [34]. Control systems in autonomous vehicles, however, such as in exploration robots or space satellites, have to be capable of deliberative planning and scheduling (P&S) to autonomously accomplish high-level tasks (e.g. collect a rock sample at position X, take a photograph of constellation Y). In fact, scientists at NASA for over 20 years have been developing systems with such P&S technology for the control of autonomous vehicles, and have deployed systems which can plan the control of spacecraft, generate activities for uploading to spacecraft, schedule observation movements for the Hubble Telescope, and control underwater vehicles [4, 20, 6, 18]. Research into this kind of deliberative planning is often termed artificial intelligence (AI) P&S. The AI P&S research community has been successful in overcoming some of the theoretical problems to do with computational complexity of generative planning, and scale-up of proposed solutions, which dogged the community in the last century. This is evidenced by the deployment of AI P&S technology in a wide range of applications: at this year’s annual ICAPS event1 the fielded applications reported included fire fighting, satellite control, emergency landing, aircraft repair scheduling, workflow generation, narrative generation, and battery load balancing. The event also hosts competitions leading to the development of optimised planning tools which can be embedded in applications software.",,2012.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4fb6cf72113a63ec277ae2be3f34790e9dbc11b3,https://www.semanticscholar.org/paper/4fb6cf72113a63ec277ae2be3f34790e9dbc11b3,Embedded System Learning Platform for Developing Economies,,Educating Engineers for Future Industrial Revolutions,2021.0,10.1007/978-3-030-68201-9_60,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aa12172a2b8fee4949f49d64e72ef36202ea9b61,https://www.semanticscholar.org/paper/aa12172a2b8fee4949f49d64e72ef36202ea9b61,A Robot Walks into a Bar: Automatic Robot Joke Success Assessment,"Effective social robots should leverage humor’s unique ability to improve relationship connections and dispel stress, but current robots possess limited (if any) humorous abilities. In this paper, we aim to supplement one aspect of autonomous robots by giving robotic systems the ability to ""read the room"" to assess how their humorous statements are received by nearby people in real time. Using a dataset of the audio of crowd responses to a robotic comedian over multiple performances (first presented in past work), we establish human-labeled joke success ground truths and compare individual human rater accuracy against the outputs of lightweight Machine Learning (ML) approaches that are easy to deploy in real-time joke assessment. Our results indicate that all three ML approaches (naïve Bayes, support vector machines, and single-hidden-layer feedforward neural networks) performed significantly better than the baseline approach used in our past work. In particular, support vector machines and neural network approaches are comparable to a human rater in the task of assessing if a joke failed or not in certain cases. The products of this work will inform self-assessment techniques for robots and help social robotics researchers test their own assessment methods on realistic data from human crowds.",2021 IEEE International Conference on Robotics and Automation (ICRA),2021.0,10.1109/ICRA48506.2021.9561941,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f2cb99e4e0731b38bd58ddb9218da52f3f27c778,https://www.semanticscholar.org/paper/f2cb99e4e0731b38bd58ddb9218da52f3f27c778,Robot Navigation with Map-Based Deep Reinforcement Learning,"This paper proposes an end-to-end deep reinforcement learning approach for mobile robot navigation with dynamic obstacles avoidance. Using experience collected in a simulation environment, a convolutional neural network (CNN) is trained to predict proper steering actions of a robot from its egocentric local occupancy maps, which accommodate various sensors and fusion algorithms. The trained neural network is then transferred and executed on a real-world mobile robot to guide its local path planning. The new approach is evaluated both qualitatively and quantitatively in simulation and realworld robot experiments. The results show that the map-based end-to-end navigation model is easy to be deployed to a robotic platform, robust to sensor noise and outperforms other existing DRL-based models in many indicators.","2020 IEEE International Conference on Networking, Sensing and Control (ICNSC)",2020.0,10.1109/ICNSC48988.2020.9238090,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f4f73e6450f9d112bd57e51525de9c49cae79137,https://www.semanticscholar.org/paper/f4f73e6450f9d112bd57e51525de9c49cae79137,Hardware-aware 3D Model Workload Selection and Characterization for Graphics and ML Applications,"3D models are widely used in computer graphics, computer vision, and robotics applications. Multiple hardware accelerators are used for running 3D model related applications, since the computations required for models in 3D space are an order of magnitude higher than the computations in 2D space. Due to the high computation intensity of 3D model workloads, using large 3D model datasets for performance characterization is not a feasible choice during accelerator design. Representative subsets are widely used to save the execution or simulation time, e.g, ModelNet10, a subset of ModelNet40, is widely used in the machine learning (ML) domain to save training and inference time. However, this subset is picked by programmers from a software and application perspective.In this paper, we deploy statistical analysis based methodologies to guide the identification of hardware-aware representative subsets, which can maintain higher performance accuracy and achieve larger execution time savings with respect to subsets picked by software programmers. We believe that the methodology proposed in this paper can help hardware architects and engineers design efficient graphics or ML accelerators rapidly.",2022 23rd International Symposium on Quality Electronic Design (ISQED),2022.0,10.1109/ISQED54688.2022.9806296,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7435a7fc91f77aed33a1ed17147b52ffc9f3bfce,https://www.semanticscholar.org/paper/7435a7fc91f77aed33a1ed17147b52ffc9f3bfce,Tackling Time-Variability in sEMG-based Gesture Recognition with On-Device Incremental Learning and Temporal Convolutional Networks,"Human-machine interaction is showing promising results for robotic prosthesis control and rehabilitation. In these fields, hand movement recognition via surface electromyographic (sEMG) signals is one of the most promising approaches. However, it still suffers from the issue of sEMG signal's variability over time, which negatively impacts classification robustness. In particular, the non-stationarity of input signals and the surface electrodes' shift can cause up to 30 % degradation in gesture recognition accuracy. This work addresses the temporal variability of the sEMG-based gesture recognition by proposing to train a Temporal Convolutional Network (TCN) incrementally over multiple gesture training sessions. Using incremental learning, we re-train our model on stored latent data spanning multiple sessions. We validate our approach on the UniBo-20-Session dataset, which includes 8 hand gestures from 3 subjects. Our incremental learning framework obtains 18.9% higher accuracy compared to a baseline with a standard single training session. Deploying our TCN on a Parallel, Ultra-Low Power (PULP) microcontroller unit (MCU), GAP8, we achieve an inference latency and energy of 12.9 ms and 0.66 mJ, respectively, with a weight memory footprint of 427 kB and a data memory footprint of 0.5-32 MB.",2021 IEEE Sensors Applications Symposium (SAS),2021.0,10.1109/SAS51076.2021.9530007,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ac1eeed21f7b7a1a3294141be81591a44f8c3dcb,https://www.semanticscholar.org/paper/ac1eeed21f7b7a1a3294141be81591a44f8c3dcb,Empirically Evaluating Meta Learning of Robot Explainability with Humans,"—As physically-embodied robots and digital assistants are deployed in the real world, these agents must be able to communicate their decision-making criteria to build trust, improve human-robot teaming, and enable collaboration. While the ﬁeld of explainable machine learning has made great strides in building a set of mechanisms to enable such communication, these advancements often assume that one approach is ideally suited to one problem (e.g., decision trees are best for explaining how to triage patients in an emergency room), failing to recognize that individual users may have different past experiences or preferences. In this work, we present the design of a user study to evaluate a novel approach to personalization of robot explainability through meta-learning with humans. Our study will be the ﬁrst to evaluate meta learning with humans in the loop and with multiple approaches to robot explainability. Our results will help to pave the way for academic and industry deployments of explainable machine learning to diverse user populations.",,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aeb4ad9b79a9bec6d90f1c6d9ef15d523110755c,https://www.semanticscholar.org/paper/aeb4ad9b79a9bec6d90f1c6d9ef15d523110755c,Deep Reinforcement Factorization Machines: A Deep Reinforcement Learning Model with Random Exploration Strategy and High Deployment Efficiency,"In recent years, the recommendation system and robot learning are undoubtedly the two most popular application fields, and the core algorithms supporting these two fields are deep learning based on perception and reinforcement learning based on exploration learning, respectively. How to combine these two fields to better improve the development of the whole machine learning field is the dream of numerous researchers. The Deep Reinforcement Network (DRN) model successfully embedded reinforcement learning into the recommendation system, which provided a good idea for subsequent researchers. However, the disadvantage is also obvious, that is, the DRN model is built for news recommendations, meaning that the DRN model is not transferable, which is also the defect of many current recommendation system models. Meanwhile, the agent learning method adopted by the DRN model is primitive and inefficient. Among many models and algorithms that have emerged in recent years, we use the newly proposed deployment efficiency to measure their comprehensive quality and found that few models focus on both efficiency and performance improvement. To fill the gap of model deployment efficiency neglected by many researchers and to create a model of reinforcement learning agents with stronger performance, we have been exploring and trying to complete research on the Gate Attentional Factorization Machines (GAFM) model. Finally, we successfully integrated the GAFM model and reinforcement learning. The Deep Reinforcement Factorization Machines (DRFM) model proposed in this paper is based on the combination of deep learning with high perception ability and reinforcement learning with high exploration ability, centered on improving the deployment efficiency and learning performance of the model. The GAFM model is modified and upgraded using multidisciplinary techniques, and a new model-based random exploration strategy is proposed to update and optimize the recommendation list efficiently. Through parallel contrast experiments on various datasets, it is proved that the DRFM model surpasses the traditional recommendation system model in all aspects. The DRFM model is far superior to other models in terms of performance and robustness, and also significantly improved in terms of deployment efficiency. At the same time, we conduct a comparative analysis with the latest deep reinforcement learning algorithm and prove the unique advantages of the DRFM model.",Applied Sciences,2022.0,10.3390/app12115314,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1f65fbefcdb69944e8f3cd5caa6e552ab624b5ed,https://www.semanticscholar.org/paper/1f65fbefcdb69944e8f3cd5caa6e552ab624b5ed,Learning Real-world Autonomous Navigation by Self-Supervised Environment Synthesis,"—Machine learning approaches have recently en- abled autonomous navigation for mobile robots in a data-driven manner. Since most existing learning-based navigation systems are trained with data generated in artificially created training environments, during real-world deployment at scale, it is inevitable that robots will encounter unseen scenarios, which are out of the training distribution and therefore lead to poor real-world performance. On the other hand, directly training in the real world is generally unsafe and inefficient. To address this issue, we introduce Self-supervised Environment Synthesis ( SES ), in which, after real-world deployment with safety and efficiency requirements, autonomous mobile robots can utilize experience from the real-world deployment, reconstruct navigation scenarios, and synthesize representative training environments in simulation. Training in these syn- thesized environments leads to improved future performance in the real world. In our experiments, the effectiveness of SES in synthesizing representative simulation environments and improving real-world navigation performance has been verified by a large-scale deployment in a high-fidelity, realistic simulator 1 .",,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f66cb918ef09a1ee8657ed0cb89c4d480e13ac2d,https://www.semanticscholar.org/paper/f66cb918ef09a1ee8657ed0cb89c4d480e13ac2d,Editorial: Privacy-Preserving Deep Heterogeneous View Perception for Data Learning,"Deep learning has promoted the development of cutting-edge robotic systems with the ability to automaticallymine concepts from complex tasks in an open-endedmanner.Many novel algorithms and efficient architectures of deep learning with trainable components have achieved remarkable performance in various domains such as machine learning and robotic devices, based on the unsupervised/supervised learning schemes. Most of the current deep learning methods focus on a single-view perception of objects without fully considering the intrinsic characteristics of data, through which objects can be described by heterogeneous views. Those heterogeneous views contain complementary knowledge and information that can further improve representation learning of data. With development for easier access to heterogeneous view data promoted by wider deployments of edge-computing robotic devices, deep heterogeneous view perception distilling knowledge from various views is increasingly attracting more attention. At the same time, heterogeneous view data contains more private information than single view data. Mining large-scale heterogeneous view data inevitably raises the issue of privacy. With the emergence of deep heterogeneous view perception, privacies hidden in data are becoming more easily leaked. Thus, perception of deep heterogeneous view knowledge of data with preserving privacies is also becoming central to neural computing. This Research Topic collects 8 high-quality articles reporting the latest applications of privacy-preserving deep heterogeneous learning. Below is a review of the articles published in this collection.",Frontiers in Neurorobotics,2022.0,10.3389/fnbot.2022.862535,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0136979b004b55090ece003de72b4ac028a8b460,https://www.semanticscholar.org/paper/0136979b004b55090ece003de72b4ac028a8b460,Online Incremental Learning of the Terrain Traversal Cost in Autonomous Exploration,"In this paper, we address motion efficiency in autonomous robot exploration with multi-legged walking robots that can traverse rough terrains at the cost of lower efficiency and greater body vibration. We propose a robotic system for online and incremental learning of the terrain traversal cost that is immediately utilized to reason about next navigational goals in building spatial model of the robot surrounding. The traversal cost experienced by the robot is characterized by incrementally constructed Gaussian Processes using Bayesian Committee Machine. During the exploration, the robot builds the spatial terrain model, marks untraversable areas, and leverages the Gaussian Process predictive variance to decide whether to improve the spatial model or decrease the uncertainty of the terrain traversal cost. The feasibility of the proposed approach has been experimentally verified in a fully autonomous deployment with the hexapod walking robot.",Robotics: Science and Systems,2019.0,10.15607/RSS.2019.XV.040,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bb0ee42d406f2361fee89cf1274073185a0e9eec,https://www.semanticscholar.org/paper/bb0ee42d406f2361fee89cf1274073185a0e9eec,Learning agile and dynamic motor skills for legged robots,"A method for learning agile control policies uses simulated data to enable precise, efficient movements in a complex physical robot. Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we introduce a method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby leveraging fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated medium-dog–sized quadrupedal system. Using policies trained in simulation, the quadrupedal machine achieves locomotion skills that go beyond what had been achieved with prior methods: ANYmal is capable of precisely and energy-efficiently following high-level body velocity commands, running faster than before, and recovering from falling even in complex configurations.",Science Robotics,2019.0,10.1126/scirobotics.aau5872,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c8f0c15207ef4236755b869f2b06234c9b2e41a5,https://www.semanticscholar.org/paper/c8f0c15207ef4236755b869f2b06234c9b2e41a5,Learning Conditional Error Model for Simulated Time-Series Data,"Applications such as autonomous navigation [1], humanrobot interaction [2], game-playing robots [8], etc., use simulation to minimize the cost of testing in real world. Furthermore, some machine learning algorithms, like reinforcement learning, use simulation for training a model. To test reliably in simulation or deploy a model in the real world that is trained with simulated data, the simulator should be representative of the real environment. Usually, the simulator is based on manually designed rules and ignores the stochastic behavior of measurements. In particular, we would like to learn a model that captures uncertainties of the sensing algorithms (e.g. neural networks used to detect objects) in real world and add them in simulation. We model the distribution of residuals between the ground truth states of the objects and their perceived states by the sensing algorithm. This error distribution depends both on the current state of the object (e.g. distance from the sensor) and its past residuals. We assume the error distribution is conditionally Gaussian, and we use a deep neural neural network (DNN) to map the object states and past residuals to the distribution parameters (mean and variance). Our conditional model perturbs the dynamic objects’ states (position, velocities, orientations, and shape) and produces smoother trajectories which look similar to the real data.",CVPR Workshops,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
114aa720872462b0ca1b97bfdec0ebd56c36fd0a,https://www.semanticscholar.org/paper/114aa720872462b0ca1b97bfdec0ebd56c36fd0a,Towards Understanding and Mitigating Social Biases in Language Models,"Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in realworld settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",ICML,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
614945fc2f6f251f1cf69fa50d7faf0033c03484,https://www.semanticscholar.org/paper/614945fc2f6f251f1cf69fa50d7faf0033c03484,TensorFlow in Deep learning,"Deep learning has revolutionized the technology industry. Modern machine translation, search engines, and computer assistants are all powered by deep learning. TensorFlow is used to do all its complex work very simple. TensorFlow is an open source software library for high performance numerical computation. Its flexible architecture allows easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices. Originally developed by researchers and engineers from the Google Brain team within Google’s AI organization, it comes with strong support for machine learning and deep learning and the flexible numerical computation core is used across many other scientific domains. This trend will only continue as deep learning expands its reach into robotics, pharmaceuticals, energy, and all other fields of contemporary technology. It is rapidly becoming essential for the modern software professional to develop a working knowledge of the principles of deep learning.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cebc191de10104ba16625947bf3e46fe37d8a2b1,https://www.semanticscholar.org/paper/cebc191de10104ba16625947bf3e46fe37d8a2b1,Exploring care robots' cybersecurity threats from care robotics specialists’ point of view,"Care robots can perform tasks related to physical or mental care; assisting in daily tasks or rehabilitation, independently or semi-automatically. Care robots are exploitable in home-care, nursing homes, or other care facilities. Care robots have the potential to solve several challenges related to aging people. However, care robots suffer have similar cybersecurity problems as other information and communication technology (ICT) devices. In addition, the cybersecurity threats of care robots have been studied less than those of industrial robots. This study’s purpose is to map cybersecurity threats related to care robots from the perspective of care robotics specialists. The study consists of thematic interviews of six purposive-selected specialists in care robotics. A semi-structured thematic interview guide based on the literature view of previous studies, facilitates the conversations at the interviews. All interviews were transcribed verbatim, analyzed by deductive content analysis, and the remaining material was analyzed by inductive content analysis. According to the interviewed specialists, care robots’ cybersecurity threats are associated with the same risks and threats as the use of other ICT devices or robots. Most potential threats are considered to be remote access of care robots, spying, and eavesdropping. Network connectivity is seen as the main interface to the realization of cybersecurity threats in care robotics. New features such as artificial intelligence and machine learning are considered to create more opportunities for new threats. Experts also highlight the underlying human factors behind cybercrime. According to the results, more studies exploring the motives for cybercrime against care robots and the potential benefits derived from it are needed to determine the likelihood of the realization of threats to care robots are needed. Cybersecurity is a race against cybercrime and finding a balance between significant and acceptable risks. In the future, a service ecosystem should be developed which guarantees the safety of care robots throughout their life-cycle: during the design and development phase, deployment and user guidance, maintenance, and reuse of the robot. Additionally, it is important to take into account how new robust operating models can withstand failures and how critical services can be secured in the event of a cybersecurity threat.",European Conference on Cyber Warfare and Security,2022.0,10.34190/eccws.21.1.275,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7a5a2f584dd44b198ab56c9e3a779bb6823c8a41,https://www.semanticscholar.org/paper/7a5a2f584dd44b198ab56c9e3a779bb6823c8a41,How the human-machine interchange will transform business operations,"
Purpose
Machine learning is beginning to transform the way businesses organize their operations and benefit from technology investments.


Design/methodology/approach
To learn more about how far along organizations are in deploying intelligent automation and in developing plans and strategies for its adoption, the IBM Institute for Business Value, in collaboration with Oxford Economics surveyed and interviewed 550 technology and operations executives.


Findings
The primary purpose of intelligent automation is to augment employees’ skills, experience and expertise, extending the human mind in ways that allow for higher productivity, creative problem-solving and more engaging jobs for employees.


Practical implications
Automation is not a plug-and-play solution: companies cannot just buy the technology, flip the switch and watch robots run the business without any human intervention.


Originality/value
This recent survey of operations executive with specific knowledge of their companies plans provides insights into best practice. Executives believe that layering new technologies on top of old business processes is apt to be less productive ? and less cost-effective ? than rethinking processes to make the most of intelligent automation. Executives must optimize workflows for automation; this means envisioning the end result, enabling it through logical steps and prototyping the process ? then repairing as necessary before scaling.
",Strategy & Leadership,2019.0,10.1108/SL-01-2019-0003,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e77de1a07d7eba290a02084141ad513862c28649,https://www.semanticscholar.org/paper/e77de1a07d7eba290a02084141ad513862c28649,Learning‐based robust control methodologies under information constraints,"This editorial is dealing with the collection and report of some recent advances in learning-based robust control methodologies under information constraints. Both theoretical and practical contributions focusing on this theme are partly addressed in this special issue. Particularly, the latest progress of learning-based control in autonomous systems, large-scale systems, interconnected systems, robotics, industrial mechatronics, transportation and variously broad applications are introduced to the literature through this special issue. Within the past decade, various learning-based control technologies have prosperously emerged in both academic and industrial communities, and have expectantly performed remarkable superiority in terms of intelligence, autonomy, conciseness, reliability, resilience, and so forth. At the early stage, neural/fuzzy learning architectures have been widely deployed to online capture complex unknowns including unmodeled dynamics, uncertainties, and disturbances pertaining to the plant which might be a nonlinear system addressing the vehicles, robotics, transportations, mechatronics, informatics, circuits, and so forth. Recently, fruitful machine learning-based approaches, for example, reinforcement learning, deep learning, brain-inspired learning, have been incrementally promoted to innovate traditional learning-based intelligent control methodologies in both theoretical and practical sides. Especially, promising applications have also been developed to autonomous systems and robotics. In addition to booming advances in learning-based control philosophy, within a complex system, unexpected constraints would be inevitably involved, for instance, communication delays, sensor failures/noises, actuator nonlinearities, nonholonomic/underactuated dynamics, and so forth, especially within distributed systems.",International Journal of Robust and Nonlinear Control,2022.0,10.1002/rnc.5973,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4d4a38cdb8093248b5e283e8a50b1aac9a74e864,https://www.semanticscholar.org/paper/4d4a38cdb8093248b5e283e8a50b1aac9a74e864,Search-Based Testing Approach for Deep Reinforcement Learning Agents,"Deep Reinforcement Learning (DRL) algorithms have been increasingly employed during the last decade to solve various decision-making problems such as autonomous driving, trading decision making and robotics. However, these algorithms have faced great challenges when deployed in safety-critical environments since they often exhibit erroneous behaviors that can lead to potentially critical errors. One of the ways to assess the safety of DRL agents is to test them in order to detect possible faults leading to critical failures during their execution. This raises the question of how we can efficiently test DRL policies to ensure their correctness and adherence to safety requirements. Most existing works on testing DRL agents use adversarial attacks that perturb states or actions of the agent. However, such attacks often lead to unrealistic states of the environment. Furthermore, their main goal is to test the robustness of DRL agents rather than testing the compliance of agents’ policies with respect to requirements. Due to the huge state space of DRL environments, the high cost of test execution, and the black-box nature of DRL algorithms, the exhaustive testing of DRL agents is impossible. In this paper, we propose a Search-based Testing Approach of Reinforcement Learning Agents (STARLA) to test the policy of a DRL agent by effectively searching for failing executions of the agent within a limited testing budget. We rely on machine learning models and a dedicated genetic algorithm to narrow the search towards faulty episodes (i.e., sequences of states and actions produced by the DRL agent). We apply STARLA on a Deep-Q-Learning agent which is widely used as a benchmark and show that it significantly outperforms Random Testing by detecting more faults related to the agent’s policy. We also investigate how to extract rules that characterize faulty episodes of the DRL agent using our search results. Such rules can be used to understand the conditions under which the agent fails and thus assess the risks of deploying it.",ArXiv,2022.0,10.48550/arXiv.2206.07813,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4a51f0cb15874c54463d68304c741aa412614305,https://www.semanticscholar.org/paper/4a51f0cb15874c54463d68304c741aa412614305,Learning to Dynamically Coordinate Multi-Robot Teams in Graph Attention Networks,"Increasing interest in integrating advanced robotics within manufacturing has spurred a renewed concentration in developing real-time scheduling solutions to coordinate human-robot collaboration in this environment. Traditionally, the problem of scheduling agents to complete tasks with temporal and spatial constraints has been approached either with exact algorithms, which are computationally intractable for large-scale, dynamic coordination, or approximate methods that require domain experts to craft heuristics for each application. We seek to overcome the limitations of these conventional methods by developing a novel graph attention network formulation to automatically learn features of scheduling problems to allow their deployment. To learn effective policies for combinatorial optimization problems via machine learning, we combine imitation learning on smaller problems with deep Q-learning on larger problems, in a non-parametric framework, to allow for fast, near-optimal scheduling of robot teams. We show that our network-based policy finds at least twice as many solutions over prior state-of-the-art methods in all testing scenarios.",ArXiv,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
42c9527670182855ec693b469e58925e84998e29,https://www.semanticscholar.org/paper/42c9527670182855ec693b469e58925e84998e29,"""How Robotics are Revolutionizing Rehabilitation""","Capitalizing on the new understanding of brain plasticity, we introduced a paradigm shift in clinical practice in 1989 when we initiated the development of the MIT-Manus robot for neuro-rehabilitation and deployed it into the clinic. Since then we collected evidence to support the potential of enhancing and augmenting recovery following a stroke, first during the sub-acute and then the chronic phase. Our efforts and that of others led to the endorsements starting in 2010 from the American Heart Association, the American Stroke Association, and the Veterans Administration for the use of rehabilitation robots for the Upper Extremity, but not yet for the Lower Extremity. AHA recommendations were the same in the 2016 revision. Furthermore, it was demonstrated in the VA system that upper extremity robotic therapy has an economic advantage over manual therapy. More recently we completed a pragmatic study RATULS under the auspices of the National Health Service of the United Kingdom and its NIHR Health Technology Assessment Programme, which enrolled 770 stroke patients. Thus, we have developed novel robotic treatment and evaluation tools and have managed to collect the experimental evidence that demonstrates the unequivocal therapeutic benefits stemming from robot-aided rehabilitation for the upper extremity as well as present shortcomings. This talk will present an overview of our past rehabilitation robotics efforts and more recent efforts addressing the identified shortcomings. ""Novel Biomarkers: Robotics and Machine Learning "" Hermano Igo Krebs, PhD Abstract: In stroke, we demonstrated that robotic devices promoted upper extremity motor recovery. Those studies raised new questions focused on patients who were mildly or completely resistant to therapy, i.e., patients who did not improve, and prompted the hypothesis In stroke, we demonstrated that robotic devices promoted upper extremity motor recovery. Those studies raised new questions focused on patients who were mildly or completely resistant to therapy, i.e., patients who did not improve, and prompted the hypothesis that we could predict who are the responders, quasi-responders, and non-responders to behavioral therapy. There have been other attempts to create biomarkers to predict outcomes employing clinical scales such as the Fugl-Meyer assessment, the neurologic sensory exam, functional impairment scales, neurophysiology and neuro-imaging analysis; but these attempts have had mixed results and these measures are seldom used in practice to optimize therapy. To understand the variability of recovery, we examined the data collected with the robotic group on a recently completed studies. We investigated the potential for building a more sensitive biomarker, composed of robotic measurements collected during evaluation and training, to analyze the performance of patients recovering from stroke and to predict who will respond to movement-based treatment and who will not. We hypothesize that kinematic and kinetic measurements can predict the response to behavioral therapy in stroke and also determine how to optimize care for a particular patient. Here we will discuss our attempts to employ both linear and non-linear approaches and ascertain the correlation levels between our robot-based biomarker and clinical scales. We will discuss robot biomarker effect-size and compare it to clinical scales to determine whether there are some noticeable efficiencies in using the robotassay instead of the clinical scales. We will also present our efforts in developing an expert algorithm that employs data collected during the baseline assessment and during two consecutive training sessions in order to predict patient outcomes as well as to determine patterns of improvement in stroke patients so as to build an alternative machine learning predictor of outcomes. ""Starting a Venture Company"" Hermano Igo Krebs, PhD Abstract: “Imagine being present at the birth of a new industry. . . trends are now starting to converge and I can envision a future in which robotics devices will become a nearly ubiquitous part of our day-to-day lives. Technologies such as distributed computing, voice and visual recognition, and wireless broadband connectively will open the door to a new generation of autonomous devices that enable computers to perform tasks in the physical world on our behalf. We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present.” Bill Gates Disruptive technology is a term coined to characterize an innovation that disrupts an existing market or way of doing things and creates a new value network. The concept was first described at Harvard Business School by Clayton M. Christensen, who described the concept in 1996 as: ""Generally, disruptive innovations were technologically straightforward, consisting of off-theshelf components put together in a product architecture that was often simpler than prior approaches. They offered less of what customers in established markets wanted and so could rarely be initially employed there. They offered a different package of attributes valued only in emerging markets remote from, and unimportant to, the mainstream."" Eventually with improvement, borrowing from Malcolm Gladwell, the moment of critical mass, the threshold, the boiling point is reached and the old practices and existing value network is abandoned in favor of the new one. Here I will discuss my experience as an entrepreneur and whether rehabilitation robotics has achieved its “tipping point.” “Imagine being present at the birth of a new industry. . . trends are now starting to converge and I can envision a future in which robotics devices will become a nearly ubiquitous part of our day-to-day lives. Technologies such as distributed computing, voice and visual recognition, and wireless broadband connectively will open the door to a new generation of autonomous devices that enable computers to perform tasks in the physical world on our behalf. We may be on the verge of a new era, when the PC will get up off the desktop and allow us to see, hear, touch and manipulate objects in places where we are not physically present.” Bill Gates Disruptive technology is a term coined to characterize an innovation that disrupts an existing market or way of doing things and creates a new value network. The concept was first described at Harvard Business School by Clayton M. Christensen, who described the concept in 1996 as: ""Generally, disruptive innovations were technologically straightforward, consisting of off-theshelf components put together in a product architecture that was often simpler than prior approaches. They offered less of what customers in established markets wanted and so could rarely be initially employed there. They offered a different package of attributes valued only in emerging markets remote from, and unimportant to, the mainstream."" Eventually with improvement, borrowing from Malcolm Gladwell, the moment of critical mass, the threshold, the boiling point is reached and the old practices and existing value network is abandoned in favor of the new one. Here I will discuss my experience as an entrepreneur and whether rehabilitation robotics has achieved its “tipping point.”",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
07f55daeccb3adf401a88c0c3d39b89bbf3b3bab,https://www.semanticscholar.org/paper/07f55daeccb3adf401a88c0c3d39b89bbf3b3bab,Deep Learning Based Pavement Inspection Using Self-Reconfigurable Robot,"The pavement inspection task, which mainly includes crack and garbage detection, is essential and carried out frequently. The human-based or dedicated system approach for inspection can be easily carried out by integrating with the pavement sweeping machines. This work proposes a deep learning-based pavement inspection framework for self-reconfigurable robot named Panthera. Semantic segmentation framework SegNet was adopted to segment the pavement region from other objects. Deep Convolutional Neural Network (DCNN) based object detection is used to detect and localize pavement defects and garbage. Furthermore, Mobile Mapping System (MMS) was adopted for the geotagging of the defects. The proposed system was implemented and tested with the Panthera robot having NVIDIA GPU cards. The experimental results showed that the proposed technique identifies the pavement defects and litters or garbage detection with high accuracy. The experimental results on the crack and garbage detection are presented. It is found that the proposed technique is suitable for deployment in real-time for garbage detection and, eventually, sweeping or cleaning tasks.",Sensors,2021.0,10.3390/s21082595,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5dae0b24b0df802c798add10bf8dbf2840fc04a6,https://www.semanticscholar.org/paper/5dae0b24b0df802c798add10bf8dbf2840fc04a6,A Joint Decentralized Federated Learning and Communications Framework for Industrial Networks,"Industrial wireless networks are pushing towards distributed architectures moving beyond traditional server-client transactions. Paired with this trend, new synergies are emerging among sensing, communications and Machine Learning (ML) co-design, where resources need to be distributed across different wireless field devices, acting as both data producers and learners. Considering this landscape, Federated Learning (FL) solutions are suitable for training a ML model in distributed systems. In particular, decentralized FL policies target scenarios where learning operations must be implemented collaboratively, without relying on the server, and by exchanging model parameters updates rather than training data over capacity-constrained radio links. This paper proposes a real-time framework for the analysis of decentralized FL systems running on top of industrial wireless networks rooted in the popular Time Slotted Channel Hopping (TSCH) radio interface of the IEEE 802.15.4e standard. The proposed framework is suitable for neural networks trained via distributed Stochastic Gradient Descent (SGD), it quantifies the effects of model pruning, sparsification and quantization, as well as physical and link layer constraints, on FL convergence time and learning loss. The goal is to set the fundamentals for comprehensive methods and procedures supporting decentralized FL pre-deployment design. The proposed tool can be thus used to optimize the deployment of the wireless network and the ML model before its actual installation. It has been verified based on real data targeting smart robotic-assisted manufacturing.",2020 IEEE 25th International Workshop on Computer Aided Modeling and Design of Communication Links and Networks (CAMAD),2020.0,10.1109/CAMAD50429.2020.9209305,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7c53f780923a799e96df3f5f92d708ec4369f72a,https://www.semanticscholar.org/paper/7c53f780923a799e96df3f5f92d708ec4369f72a,libSmart: an Open-Source Tool for Simple Integration of Deep Learning into Intelligent Robotic Systems,"Intelligent robotic systems can be empowered by advanced deep learning techniques. Robotic operations such as object recognition are well investigated by researchers involved in machine learning. However, these solutions have often led to ad-hoc implementation in experimental settings. Less reported is systematic implementation of deep learning models in industrial robots. The lack of standard implementation platforms has impeded widespread use of deep learning modules in industrial robots. It is of great importance to have development platforms that can coordinate several deep learning modules of a complex system. In this paper, a scalable deep-learning friendly robot task organization system named libSmart is introduced. Similar to ROS, the architecture of the proposed system allows users to plug and play various devices but the proposed architecture is also highly compatible with deep learning modules. Specifically, the deployment of deep learning models is handled using a novel data graph method with distributed computing. In this way, the computationally expensive training and inferencing processes of deep learning models can be handled with isolated accelerating hardware to reduce the overall system latency. Successful implementation of simultaneous object recognition and pose estimation by an industrial robot has been presented as a case study. The proposed system is open source for all users to build their own intelligent systems with customized deep-learning models. (https://github.com/RustIron/libSmart.git).",2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),2019.0,10.1109/RCAR47638.2019.9043946,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cade506a2d6cab0870a2ea13d6c0f388a2b82e1d,https://www.semanticscholar.org/paper/cade506a2d6cab0870a2ea13d6c0f388a2b82e1d,Supervised Support Vector Machine in Predicting Foreign Exchange Trading,"Trends of currency rates can be predicted with supporting from supervised machine learning in the transaction systems such as support vector machine (SVM). By assumption of binary classification problems, the SVM can predict foreign exchange transaction as uptrend or downtrend. The prediction is performed basing on collected historical data. Alternative SVM models have been used to vote the best one, which is deployed detail in Expert Advisor (Robotics). This is to show that support vector machine models might help investors to automatically make transaction decisions of Bid/Ask in Foreign Exchange Market. For comparison, the transactions without using SVM model also are performed. The results of experimental transactions show the advantages of using SVM model compared to the transactions without using SVM model.",International Journal of Intelligent Systems and Applications,2018.0,10.5815/IJISA.2018.09.06,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
196ebc91810064736646e5bf39bc0075d2dbc71e,https://www.semanticscholar.org/paper/196ebc91810064736646e5bf39bc0075d2dbc71e,AI-FML Agent for Robotic Game of Go and AIoT Real-World Co-Learning Applications,"In this paper, we propose an AI-FML agent for robotic game of Go and AIoT real-world co-learning applications. The fuzzy machine learning mechanisms are adopted in the proposed model, including fuzzy markup language (FML)-based genetic learning (GFML), eXtreme Gradient Boost (XGBoost), and a seven-layered deep fuzzy neural network (DFNN) with backpropagation learning, to predict the win rate of the game of Go as Black or White. This paper uses Google AlphaGo Master sixty games as the dataset to evaluate the performance of the fuzzy machine learning, and the desired output dataset were predicted by Facebook AI Research (FAIR) ELF Open Go AI bot. In addition, we use IEEE 1855 standard for FML to describe the knowledge base and rule base of the Open Go Darkforest (OGD) prediction platform in order to infer the win rate of the game. Next, the proposed AI-FML agent publishes the inferred result to communicate with the robot Kebbi Air based on MQTT protocol to achieve the goal of human and smart machine co-learning. From Sept. 2019 to Jan. 2020, we introduced the AI-FML agent into the teaching and learning fields in Taiwan. The experimental results show the robots and students can co-learn AI tools and FML applications effectively. In addition, XGBoost outperforms the other machine learning methods but DFNN has the most obvious progress after learning. In the future, we hope to deploy the AI-FML agent to more available robot and human co-learning platforms through the established AI-FML International Academy in the world.",2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),2020.0,10.1109/FUZZ48607.2020.9177654,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e153363e54c13451ff600085edcd03be8c4bfd0c,https://www.semanticscholar.org/paper/e153363e54c13451ff600085edcd03be8c4bfd0c,Beyond Cross-Validation - Accuracy Estimation for Incremental and Active Learning Models,"For incremental machine-learning applications it is often important to robustly estimate the system accuracy during training, especially if humans perform the supervised teaching. Cross-validation and interleaved test/train error are here the standard supervised approaches. We propose a novel semi-supervised accuracy estimation approach that clearly outperforms these two methods. We introduce the Configram Estimation (CGEM) approach to predict the accuracy of any classifier that delivers confidences. By calculating classification confidences for unseen samples, it is possible to train an offline regression model, capable of predicting the classifier’s accuracy on novel data in a semi-supervised fashion. We evaluate our method with several diverse classifiers and on analytical and real-world benchmark data sets for both incremental and active learning. The results show that our novel method improves accuracy estimation over standard methods and requires less supervised training data after deployment of the model. We demonstrate the application of our approach to a challenging robot object recognition task, where the human teacher can use our method to judge sufficient training.",Mach. Learn. Knowl. Extr.,2020.0,10.3390/make2030018,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a620d007603111ae263c5769c9dc9ac37efd2ddb,https://www.semanticscholar.org/paper/a620d007603111ae263c5769c9dc9ac37efd2ddb,TensorFlow: learning functions at scale,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Its computational model is based on dataflow graphs with mutable state. Graph nodes may be mapped to different machines in a cluster, and within each machine to CPUs, GPUs, and other devices. TensorFlow supports a variety of applications, but it particularly targets training and inference with deep neural networks. It serves as a platform for research and for deploying machine learning systems across many areas, such as speech recognition, computer vision, robotics, information retrieval, and natural language processing. In this talk, we describe TensorFlow and outline some of its applications. We also discuss the question of what TensorFlow and deep learning may have to do with functional programming. Although TensorFlow is not purely functional, many of its uses are concerned with optimizing functions (during training), then with applying those functions (during inference). These functions are defined as compositions of simple primitives (as is common in functional programming), with internal data representations that are learned rather than manually designed. TensorFlow is joint work with many other people in the Google Brain team and elsewhere. More information is available at tensorflow.org.",ICFP,2016.0,10.1145/2951913.2976746,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,https://www.semanticscholar.org/paper/ee9e36c0abe8fd1c48aa73d1f83bcbb22fe3670b,Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale,"The ever-growing demand and complexity of machine learning are putting pressure on hyper-parameter tuning systems: while the evaluation cost of models continues to increase, the scalability of state-of-the-arts starts to become a crucial bottleneck. In this paper, inspired by our experience when deploying hyper-parameter tuning in a real-world application in production and the limitations of existing systems, we propose Hyper-Tune, an efficient and robust distributed hyper-parameter tuning framework. Compared with existing systems, Hyper-Tune highlights multiple system optimizations, including (1) automatic resource allocation, (2) asynchronous scheduling, and (3) multi-fidelity optimizer. We conduct extensive evaluations on benchmark datasets and a large-scale realworld dataset in production. Empirically, with the aid of these optimizations, Hyper-Tune outperforms competitive hyper-parameter tuning systems on a wide range of scenarios, including XGBoost, CNN, RNN, and some architectural hyper-parameters for neural networks. Compared with the state-of-the-art BOHB and A-BOHB, Hyper-Tune achieves up to 11.2× and 5.1× speedups, respectively. PVLDB Reference Format: Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Jixiang Li, Ji Liu, Ce Zhang, and Bin Cui. Hyper-Tune: Towards Efficient Hyper-parameter Tuning at Scale. PVLDB, 14(1): XXX-XXX, 2020. doi:XX.XX/XXX.XX PVLDB Availability Tag: The source code of this research paper has been made publicly available at https://github.com/PKU-DAIR/HyperTune.",Proc. VLDB Endow.,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a595d18e5a288df13c6798e73114f4a6a78987e5,https://www.semanticscholar.org/paper/a595d18e5a288df13c6798e73114f4a6a78987e5,Global-Local AI Coordinated Learning over Optical Access Networks for Scalable H2M/R Collaborations,"While pre-fifth and fifth generation networks have been designed mainly to support machine-to-machine and human-to-human communications via data, audio, and video communications, a major thrust of beyond fifth generation Networks is the abutment of human-to-machine/robot (H2M/R) collaboration. Such collaborations are deeply anchored in the vision of Society 5.0, which promotes a human-centered society that integrates virtual and physical real spaces to resolve social problems, and Industry 5.0, which promotes intelligent manufacturing processes through collaboration between humans and cyber-physical systems. It is expected that real-time H2M/R collaborations will drive new technologies that facilitate the low-latency exchange of control signals and haptic feedback between humans and machines/robots. The low-latency constraints for real-time interaction will, however, place limits on deployable distance and the flexibility of adding new machines/robots to the system. The scalability of H2M/R collaborations across large geographical distances that also allow rapid onboarding of new machines/robots is therefore an open challenge. This article focuses on global-local AI coordinated learning (GLAD), a solution framework that supports the dynamic addition of new machines/robots and their rapid onboarding. The GLAD framework is based on learning the traffic characteristics and correlation of supported robots/machines at the local edge and global cloud. Harnessing data samples from a real H2M/R application for training and validation, GLAD enables human operators to remotely control machines/robots over extended distances through accurately forecasting their haptic feedback. By globally coordinating and sharing this learning across the network, up to 72 percent of the time can be saved when onboarding new machines/robots.",IEEE Network,2022.0,10.1109/mnet.003.2100602,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e1184cc1e4725f7736d9944a33ada01a626cedc3,https://www.semanticscholar.org/paper/e1184cc1e4725f7736d9944a33ada01a626cedc3,Learning in Robotics,"For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform. Introduction The ability to deploy a fully autonomous robot in an unstructured, dynamic environment (the proverbial real world) over an extended period of time remains an open challenge in the field of robotics. Considerable progress is being made towards many components of this task including physical agility, power management, and on-board sensor technology. One such component that has drawn considerable interest recently is the ability for a robot to autonomously learn to improve its own performance (Ng et al. 2004; Bagnell & Schneider 2001; Zhang & Vadakkepat 2003). Despite this interest, considerable work remains due to the di fficulties associated with machine learning in the real world . Compared to other machine learning scenarios such as classification or action learning in simulation, learning o n physical robots presents several formidable challenges, i ncluding the following. Sparse Training Data: It is often prohibitively difficult to generate large amounts of data due to the maintenance required on robots, such as battery changes, hardware repairs, and, usually, constant human supervision. Thus, learning methods designed for physical robots must be effective with small amounts of data. Dynamical Complexity: The dynamics of many robotic control tasks are too complex for faithful simulation to be possible. Furthermore, robots are inherently situated in an unstructured environment with unpredictable sensor and actuator noise, namely the real world. Thus, even when off-line simulation is possible, it can never be fully reflective of the target environment. In this paper, we overcome these challenges for one concrete complex robot task, namely legged locomotion. Using a commercially available quadruped robot, we fully automate the training process (other than battery changes) and Copyright c © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved. employ machine learning algorithms that are sufficiently data efficient to enable productive learning on physical robots in a matter of hours. The resulting learned walk is considerably faster than all previously reported hand-cod ed walks for the same robot platform. This paper contributes both a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The remainder of the paper is organized as follows. First, we introduce the parameterized walk which our learning process seeks to optimize. We then specify our four learning approaches, and follow with detailed empirical results. We close with a discussion of their implications and possible avenues for future work. A Parameterized Walk The Sony Aibo ERS-210A is a commercially available robot that is equipped with a color CMOS camera and an optional ethernet card that can be used for wireless communication. The Aibo is a quadruped robot, and has three degrees of freedom in each of its four legs (Sony 2004). At the lowest level, the Aibo’s gait is determined by a series of joint positions for the three joints in each of its leg s. An early attempt to develop a gait by Hornby et al. (1999) involved using a genetic algorithm to learn a set of lowlevel parameters that described joint velocities and body p osition.1 More recent attempts to develop gaits for the Aibo have involved adopting a higher-level representation that deals with the trajectories of the Aibo’s four feet through three-dimensional space. An inverse kinematics calculati on is then used to convert these trajectories into joint angles . Among higher-level approaches, most of the differences between gaits that have been developed for the Aibo stem from the shape of the loci through which the feet pass and the exact parameterizations of those loci. For example, a team from the University of New South Wales achieved the fastest known hand-tuned gait using the high-level approach described above with trapezoidal loci. They subsequently generated an even faster walk via learning (Kim & Uther 2003). A team from Germany created a flexible gait implementation that allows them to use a variety of different shapes of loci (Rofer et al. 2003), and the team from the University of Newcastle was able to generate highvelocity gaits using a genetic algorithm and loci of arbitra ry shape (Quinlan, Chalup, & Middleton 2003). Our team (UT Austin Villa, Stone t al. 2004) first approached the gait optimization problem by hand-tuning Developed on an earlier version of the Aibo. a gait described by half-elliptical loci. This gait performed comparably to those of other teams participating in RoboCup 2003. The work reported in this paper uses the hand-tuned UT Austin Villa walk as a starting point for learning. Figure 1 compares the reported speeds of the gaits mentioned above, both hand-tuned and learned, including that of our starting point, the UT Austin Villa walk. The latter walk is described fully in a team technical report (Stone et al. 2004). The remainder of this section describes those details of the UT Austin Villa walk that are important to understand for the purposes of this paper. Hand-tuned gaits Learned gaits CMU Austin Villa UNSW Hornby UNSW NUBots (2002) (2003) (2003) (1999) (2003) (2003) 200 245 254 170 270 296 Figure 1: Maximum forward velocities of the best gaits (in mm/s) for different teams, both learned and hand-tuned. The half-elliptical locus used by our team is shown in Figure 2. By instructing each foot to move through a locus of this shape, with each pair of diagonally opposite legs in phase with each other and perfectly out of phase with the other two (a gait known as a trot), we enable the Aibo to walk. Four parameters define this elliptical locus: 1. The length of the ellipse; 2. The height of the ellipse; 3. The position of the ellipse on the x axis; and 4. The position of the ellipse on the y axis. Since the Aibo is roughly symz",,2006.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a127ae1fda13cfcf59d31aa5e4f83230343a060c,https://www.semanticscholar.org/paper/a127ae1fda13cfcf59d31aa5e4f83230343a060c,Learning Agile and Dynamic Motor Skills for Legged Robots,"Legged robots pose one of the greatest challenges in robotics. Dynamic and agile maneuvers of animals cannot be imitated by existing methods that are crafted by humans. A compelling alternative is reinforcement learning, which requires minimal craftsmanship and promotes the natural evolution of a control policy. However, so far, reinforcement learning research for legged robots is mainly limited to simulation, and only few and comparably simple examples have been deployed on real systems. The primary reason is that training with real robots, particularly with dynamically balancing systems, is complicated and expensive. In the present work, we report a new method for training a neural network policy in simulation and transferring it to a state-of-the-art legged system, thereby we leverage fast, automated, and cost-effective data generation schemes. The approach is applied to the ANYmal robot, a sophisticated medium-dog-sized quadrupedal system. Using policies trained in simulation, the quadrupedal machine achieves locomotion skills that go beyond what had been achieved with prior methods: ANYmal is capable of precisely and energy-efficiently following high-level body velocity commands, running faster than ever before, and recovering from falling even in complex configurations.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e38d319331dc3fe4e2f4cbbd7fdcd3901386a651,https://www.semanticscholar.org/paper/e38d319331dc3fe4e2f4cbbd7fdcd3901386a651,Deep Learning Application to 5G Physical Layer for Channel Estimation and CSI Feedback Improvement,"Starting from the last decades of the 20th century, Machine Learning has been widely applied in many engineering fields, such as communications, speech and image processing, computer vision and robotics, resulting particularly effective and useful in contexts where a rigorous mathematical model of the problem is too hard to be elaborated. Focusing on wireless communication systems, in recent years Machine Learning applications to the upper layers have been minutely explored for various purposes, like the deployment of cognitive radio and Self Organized Networks or the resource management, while its application to the physical layer has been somehow overlooked. The purpose of this thesis is to investigate the potential use of neural networks for the optimization of specific physical layer blocks in a communication system, taking into account the peculiar characteristics of the emerging radio technologies based on 5G standard (e.g. massive Multiple-Input Multiple-Output, beamforming, millimeter Waves) and all their related challenges. In particular, the analysis focuses on channel estimation and Channel State Information feedback reporting blocks, providing data and statistics that are representative of how Machine Learning algorithms introduction affects radio link performance, in terms of BLock Error Rate and throughput.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1a8b5eebc3c1da8c70ac2700b4d86a0a51681170,https://www.semanticscholar.org/paper/1a8b5eebc3c1da8c70ac2700b4d86a0a51681170,Robot Authority in Human-Machine Teams: Effects of Human-Like Appearance on Compliance,,HCI,2019.0,10.1007/978-3-030-21565-1_5,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
41e1c34c064184a3d25d73773a95b7f60e1e413c,https://www.semanticscholar.org/paper/41e1c34c064184a3d25d73773a95b7f60e1e413c,Introducing Self-Learning into Robotic Arm Using Deep Reinforcement Learning,"With the growing trend of autonomous machines, the combination of supervised and unsupervised machine learning techniques has been explored in providing optimal solutions for self-learning. In robotics, the curse of dimensionality makes convergence of machine learning difficult, no matter whether it is supervised or unsupervised. Therefore, reinforcement learning, which often requires a large number of trials for effective learning experience similar to unsupervised learning, suffers serious challenges in robotic applications. Consequently, choosing an appropriate algorithm that would perform optimally is of utmost importance. In this work, a robotic arm having 6 degrees of freedom combines supervised and unsupervised learning techniques by using a concept called Deep Reinforcement Learning, this helps the robot in becoming autonomous. It uses a camera image as an input to generate states through observation of the image, and distance for the reward system. It learns the optimum policy for action selection given a particular state observation that would achieve the maximum reward. The off-policy Deep Q Network (DQN) algorithm is to be implemented in this design and will be deployed on the robotic arm for independently learning the optimum movement towards achieving a certain task in a controlled environment.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
dba3478cb46874a97e301deb0f20f7692c1f1ae9,https://www.semanticscholar.org/paper/dba3478cb46874a97e301deb0f20f7692c1f1ae9,PenDer: Incorporating Shape Constraints via Penalized Derivatives,"When deploying machine learning models in the real-world, system designers may wish that models exhibit certain shape behavior, i.e., model outputs follow a particular shape with respect to input features. Trends such as monotonicity, convexity, diminishing or accelerating returns are some of the desired shapes. Presence of these shapes makes the model more interpretable for the system designers, and adequately fair for the customers. We notice that many such common shapes are related to derivatives, and propose a new approach, PenDer (Penalizing Derivatives), which incorporates these shape constraints by penalizing the derivatives. We further present an Augmented Lagrangian Method (ALM) to learn the joint unconstrained objective function. Experiments on three realworld datasets illustrate that even though both PenDer and state-of-the-art Lattice models achieve similar conformance to shape, PenDer captures better sensitivity of prediction with respect to intended features. We also demonstrate that PenDer achieves better test performance than Lattice while enforcing more desirable shape behavior.",AAAI,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
03c0de6bef80de878e2ab199e79f00b8ca6e4d36,https://www.semanticscholar.org/paper/03c0de6bef80de878e2ab199e79f00b8ca6e4d36,The Respective Roles of Intellectual Creativity and Automation in Representing Diversity: Human and Machine Generated Bias,"The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice.",KNOWLEDGE ORGANIZATION,2019.0,10.5771/0943-7444-2019-8-596,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d3ac65b10af091759863b8e2c488036bf52a2ce6,https://www.semanticscholar.org/paper/d3ac65b10af091759863b8e2c488036bf52a2ce6,DAG Card is the new Model Card,"With the progressive commoditization of modeling capabilities, data-centric AI recognizes that what happens before and after training becomes crucial for realworld deployments. Following the intuition behind Model Cards, we propose DAG Cards as a form of documentation encompassing the tenets of a data-centric point of view. We argue that Machine Learning pipelines (rather than models) are the most appropriate level of documentation for many practical use cases, and we share with the community an open implementation to generate cards from code.",ArXiv,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f02f6e666aef6b0675cc4a189f9962b716c17487,https://www.semanticscholar.org/paper/f02f6e666aef6b0675cc4a189f9962b716c17487,FARE: Enabling Fine-grained Attack Categorization under Low-quality Labeled Data,"Supervised machine learning classifiers have been widely used for attack detection, but their training requires abundant high-quality labels. Unfortunately, high-quality labels are difficult to obtain in practice due to the high cost of data labeling and the constant evolution of attackers. Without such labels, it is challenging to train and deploy targeted countermeasures. In this paper, we propose FARE, a clustering method to enable fine-grained attack categorization under low-quality labels. We focus on two common issues in data labels: 1) missing labels for certain attack classes or families; and 2) only having coarsegrained labels available for different attack types. The core idea of FARE is to take full advantage of the limited labels while using the underlying data distribution to consolidate the lowquality labels. We design an ensemble model to fuse the results of multiple unsupervised learning algorithms with the given labels to mitigate the negative impact of missing classes and coarsegrained labels. We then train an input transformation network to map the input data into a low-dimensional latent space for fine-grained clustering. Using two security datasets (Android malware and network intrusion traces), we show that FARE significantly outperforms the state-of-the-art (semi-)supervised learning methods in clustering quality/correctness. Further, we perform an initial deployment of FARE by working with a large e-commerce service to detect fraudulent accounts. With realworld A/B tests and manual investigation, we demonstrate the effectiveness of FARE to catch previously-unseen frauds.",NDSS,2021.0,10.14722/NDSS.2021.24403,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
47ad80aeb8477946cd26b96b711fb0b1fafdebdf,https://www.semanticscholar.org/paper/47ad80aeb8477946cd26b96b711fb0b1fafdebdf,http://ijai.iaescore.com A comprehensive review on machine learning in agriculture,"2022 Agriculture is an essential part of sustaining human life. Population growth, climate change, resource competition are the key issues that increase food security and to handle such complex problems in agriculture production, intelligent or smart farming extends the incorporation of technology into traditional agriculture notion. Machine learning is a vitally used technology in agriculture to protect food security and sustainability. Crop yield production, water preservation, soil health and plant diseases can be addressed by machine learning. This paper has presented a compendious review of research papers that deployed machine learning in the agriculture domain. The observed sub-categories of the agriculture domain are crop yield prediction, soil management, pest management, weed management, and crop disease. The outcomes represent that machine learning provides better accuracy concerning classification or regression. Machine learning emerged with the internet of things, drones, robots, automated machinery, and satellite imagery motivates researchers for smart farming and food",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
826aa8f5a422ccee5dbaf14e03875f575129b7aa,https://www.semanticscholar.org/paper/826aa8f5a422ccee5dbaf14e03875f575129b7aa,From self-tuning regulators to reinforcement learning and back again,"Machine and reinforcement learning (RL) are increasingly being applied to plan and control the behavior of autonomous systems interacting with the physical world. Examples include self-driving vehicles, distributed sensor networks, and agile robots. However, when machine learning is to be applied in these new settings, the algorithms had better come with the same type of reliability, robustness, and safety bounds that are hallmarks of control theory, or failures could be catastrophic. Thus, as learning algorithms are increasingly and more aggressively deployed in safety critical settings, it is imperative that control theorists join the conversation. The goal of this tutorial paper is to provide a starting point for control theorists wishing to work on learning related problems, by covering recent advances bridging learning and control theory, and by placing these results within an appropriate historical context of system identification and adaptive control.",2019 IEEE 58th Conference on Decision and Control (CDC),2019.0,10.1109/CDC40024.2019.9029916,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
93b4a6d068806c454851ed0270957161f8c9649f,https://www.semanticscholar.org/paper/93b4a6d068806c454851ed0270957161f8c9649f,Workshop on Machine Learning for Autonomous Vehicles 2017,"3: Efficient deep neural networks for perception in autonomous driving (Jose M. Alvarez, TRI) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:00 AM Abstract Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time.Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time. Bio: Dr. Jose Alvarez is a senior research scientist at Toyota Research Institute. His main research interests are in developing robust and efficient deep learning algorithms for perception with focus on autonomous vehicles. Previously, he was a researcher at Data61 / CSIRO (formerly NICTA), a Postdoctoral researcher at the Courant Institute of Mathematical Science, New York University, and visiting scholar at University of Amsterdam and Group Research Electronics at Volkswagen. Dr. Alvarez graduated in 2012 and he was awarded the best Ph.D. Thesis award. Dr. Alvarez serves as associate editor for IEEE Trans. on Intelligent Transportation Systems. Abstract 4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM Abstract: Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. In this talk, we present several technical advances for vision-based ADAS. A common theme is to overcome the challenges posed by lack of large-scale annotations in deep learning frameworks. We introduce approaches to correspondence estimation that are trained on purely synthetic data but adapt well to real data at test-time. We introduce object detectors that are light enough for ADAS, trained with knowledge distillation to retain accuracies of deeper architectures. Our semantic segmentation methods are trained on weak supervision that requires only a tenth of conventional annotation time. We propose methods for 3D reconstruction that use deep supervision to recover fine TP part locations while relying on purely synthetic 3D CAD models. We develop deep ICML 2017 Workshop book Generated Tue Nov 21, 2017 Page 4 of 30 learning frameworks for multi-target tracking, as well as occlusion-reasoning in TP localization and SE layout estimation. Finally, we present a framework for TP behavior prediction in complex traffic scenes that accounts for TP-TP and TP-SE interactions. Our approach allows prediction of diverse multimodal outcomes and aims to account for long-term strategic behaviors in complex scenes. Bio: Manmohan Chandraker is an assistant professor at the CSE department of the University of California, San Diego and leads the computer vision research effort at NEC Labs America in Cupertino. He received a B.Tech. in Electrical Engineering at the Indian Institute of Technology, Bombay and a PhD in Computer Science at the University of California, San Diego. His personal research interests are 3D scene understanding and reconstruction, with applications to autonomous driving and human-computer interfaces. His works have received the Marr Prize Honorable Mention for Best Paper at ICCV 2007, the 2009 CSE Dissertation Award for Best Thesis at UCSD, a PAMI special issue on best papers of CVPR 2011 and the Best Paper Award at CVPR 2014. Abstract 6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM Jonathan Binas, Daniel Neil, Shih-Chii Liu, Tobi Delbruck, DDD17: End-To-End DAVIS Driving Dataset Ransalu Senanayake and Fabio Ramos, Bayesian Hilbert Maps for Continuous Occupancy Mapping in Dynamic Environments Abstract 7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM Self-driving cars now deliver vast amounts of sensor data from large unstructured environments. In attempting to process and interpret this data there are many unique challenges in bridging the gap between prerecorded data sets and the field. This talk will present recent work addressing the application of deep learning techniques to robotic perception. We focus on solutions to several pervasive problems when attempting to deploy such techniques on fielded robotic systems. The themes of the talk revolve around alternatives to gathering and curating data sets for training. Are there ways of avoiding the labor-intensive human labeling required for supervised learning? These questions give rise to several lines of research based around self-supervision, adversarial learning, and simulation. We will show how these approaches applied to self-driving car problems have great potential to change the way we train, test, and validate machine learning-based systems. Bio: Matthew Johnson-Roberson is Assistant Professor of Engineering in the Department of Naval Architecture & Marine Engineering and the Department of Electrical Engineering and Computer Science at the University of Michigan. He received a PhD from the University of Sydney in 2010. He has held prior postdoctoral appointments with the Centre for Autonomous Systems CAS at KTH Royal Institute of Technology in Stockholm and the Australian Centre for Field Robotics at the University of Sydney. He is a recipient of the NSF CAREER award (2015). He has worked in robotic perception since the first DARPA grand challenge and his group focuses on enabling robots to better see and understand their environment. Abstract 8: Learning Affordance for Autonomous Driving (JianXiong Xiao, AutoX) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:30 AM8: Learning Affordance for Autonomous Driving (JianXiong Xiao, AutoX) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:30 AM Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception based approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. We evaluate our approach in a virtual racing game as well as real world driving and show that our model can work well to drive a car in a very diverse set of virtual and realistic environments. Jianxiong Xiao (",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3352eb2b731571ca9ba85e671a214b2e2467d8d9,https://www.semanticscholar.org/paper/3352eb2b731571ca9ba85e671a214b2e2467d8d9,Online and Scalable Model Selection with Multi-Armed Bandits,"Many online applications running on live traffic are powered by machine learning models, for which training, validation, and hyperparameter tuning are conducted on historical data. However, it is common for models demonstrating strong performance in offline analysis to yield poorer performance when deployed online. This problem is a consequence of the difficulty of training on historical data in non-stationary environments. Moreover, the machine learning metrics used for model selection may not sufficiently correlate with real-world business metrics used to determine the success of the applications being tested. These problems are particularly prominent in the Real-Time Bidding (RTB) domain, in which ML models power bidding strategies, and a change in models will likely affect performance of the advertising campaigns. In this work, we present Automatic Model Selector (AMS), a system for scalable online selection of RTB bidding strategies based on realworld performance metrics. AMS employs Multi-Armed Bandits (MAB) to near-simultaneously run and evaluate multiple models against live traffic, allocating the most traffic to the bestperforming models while decreasing traffic to those with poorer online performance, thereby minimizing the impact of inferior models on overall campaign performance. The reliance on offline data is avoided, instead making model selections on a case-by-case basis according to actionable business goals. AMS allows new models to be safely introduced into live campaigns as soon as they are developed, minimizing the risk to overall performance. In livetraffic tests on multiple ad campaigns, the AMS system proved highly effective at improving ad campaign performance.",ArXiv,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,https://www.semanticscholar.org/paper/0f5dc98f4ad051b7e49aa31cfc8fae5c2b532a1e,Using system context information to complement weakly labeled data,"Real-world datasets collected with sensor networks often contain incomplete and uncertain labels as well as artefacts arising from the system environment. Complete and reliable labeling is often infeasible for large-scale and long-term sensor network deployments due to the labor and time overhead, limited availability of experts and missing ground truth. In addition, if the machine learning method used for analysis is sensitive to certain features of a deployment, labeling and learning needs to be repeated for every new deployment. To address these challenges, we propose to make use of system context information formalized in an information graph and embed it in the learning process via contrastive learning. Based on realworld data we show that this approach leads to an increased accuracy in case of weakly labeled data and leads to an increased robustness and transferability of the classifier to new sensor locations.",ArXiv,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
34c59ec266e2f1798918d765c2a2519a2b8d03d2,https://www.semanticscholar.org/paper/34c59ec266e2f1798918d765c2a2519a2b8d03d2,ReLink: Complete-Link Industrial Record Linkage Over Hybrid Feature Spaces,"Record Linkage (ReL) is the task of identifying records from a pair of databases referring to the same realworld entity. This has many applications in organisations of all sizes where related data often exist in silos leading to inefficiency in data engineering and analytics applications as well as ineffectiveness of business applications (e.g., unable to personalise marketing campaigns).State-of-the-art (SOTA) machine learning and deep learning based ReL techniques use adaptive similarity measures and learn their relative contributions based on labeled data. However, we report here that they do not work with similar efficacy on industrial data owing to its fundamental differences such as magnitude of schema heterogeneity, need for leveraging structure of the data, lack of training data etc. Through our proposed system ‘ReLink’, we carefully mitigate these challenges and demonstrate that it not only significantly outperforms SOTA baselines on industrial datasets but also on majority of research benchmarks. ReLink introduces the notion of complete-linkage over attributes as well as uses hybrid feature spaces on lexical and semantic similarity measures using pre-trained models such as BERT. Going beyond empirical demonstration, we provide insights and prescriptive guidance on choice of ReL techniques in industrial settings from our observations and lessons learnt from the experience of transferring and deploying for real use-cases in a large financial services organization.",2021 IEEE 37th International Conference on Data Engineering (ICDE),2021.0,10.1109/ICDE51399.2021.00293,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a23d21bde1df79dacf9acf723e910771b0ae8f4c,https://www.semanticscholar.org/paper/a23d21bde1df79dacf9acf723e910771b0ae8f4c,SURREAL-System: Fully-Integrated Stack for Distributed Deep Reinforcement Learning,"We present an overview of SURREAL-System, a reproducible, flexible, and scalable framework for distributed reinforcement learning (RL). The framework consists of a stack of four layers: Provisioner, Orchestrator, Protocol, and Algorithms. The Provisioner abstracts away the machine hardware and node pools across different cloud providers. The Orchestrator provides a unified interface for scheduling and deploying distributed algorithms by high-level description, which is capable of deploying to a wide range of hardware from a personal laptop to full-fledged cloud clusters. The Protocol provides network communication primitives optimized for RL. Finally, the SURREAL algorithms, such as Proximal Policy Optimization (PPO) and Evolution Strategies (ES), can easily scale to 1000s of CPU cores and 100s of GPUs. The learning performances of our distributed algorithms establish new state-of-the-art on OpenAI Gym and Robotics Suites tasks.",ArXiv,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0e0d72367be96583522cb10194b806d167293b80,https://www.semanticscholar.org/paper/0e0d72367be96583522cb10194b806d167293b80,Teaching AI through machine learning projects,"An introductory Artificial Intelligence (AI) course provides students with basic knowledge of the theory and practice of AI as a discipline concerned with the methodology and technology for solving problems that are difficult to solve by other means. It is generally recognized that an introductory Artificial Intelligence course is challenging to teach. This is, in part, due to the diverse and seemingly disconnected core AI topics that are typically covered. Recently, work has been done to address the diversity of topics covered in the course and to create a theme-based approach. Russell and Norvig present an agent-centered approach [9]. Others have been working to integrate Robotics into the AI course [1, 2, 3].We present work on a project funded by the National Science Foundation with a goal of unifying the artificial intelligence course around the theme of machine learning. This involves the development and testing of an adaptable framework for the presentation of core AI topics that emphasizes the relationship between AI and computer science. Machine learning is inherently connected with the AI core topics and provides methodology and technology to enhance real-world applications within many of these topics. Machine learning also provides a bridge between AI technology and modern software engineering. In his article, Mitchell discusses the increasingly important role that machine learning plays in the software world and identifies three important areas: data mining, difficult-to-program applications, and customized software applications [6].We have developed a suite of adaptable, hands-on laboratory projects that can be closely integrated into the introductory AI course. Each project involves the design and implementation of a learning system which will enhance a particular commonly-deployed application. The goal is to enhance the student learning experience in the introductory artificial intelligence course by (1) introducing machine learning elements into the AI course, (2) implementing a set of unifying machine learning laboratory projects to tie together the core AI topics, and (3) developing, applying, and testing an adaptable framework for the presentation of core AI topics which emphasizes the important relationship between AI and computer science in general, and software development in particular. Details on this project as well as samples of course materials developed are published in [4, 5, 7, 8] and are available at the project website at http://uhaweb.hartford.edu/compsci/ccli.We present an overview of our work along with a detailed presentation of one of these projects and how it meets our goals.The project involves the development of a learning system for web document classification. Students investigate the process of classifying hypertext documents, called tagging, and apply machine learning techniques and data mining tools for automatic tagging. Our experiences using the projects are also presented.",ITICSE '06,2006.0,10.1145/1140124.1140230,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
227a2b2fa916e995c054e5c3961b2501b8486911,https://www.semanticscholar.org/paper/227a2b2fa916e995c054e5c3961b2501b8486911,Bridging Sim2Real Gap Using Image Gradients for the Task of End-to-End Autonomous Driving,"While simulations are vital for developing and testing robotic agents, the ability to transfer robotics skills learned in simulation to reality is equally challenging. Simulation to realworld transfer, popularly known as Sim2Real transfer, is an indispensable line of research for utilizing knowledge learned from simulated data to derive meaningful inferences from real-world observations and function in actual operational environments. Sim2Real is critical for developing Autonomous Vehicles (AV) and other field-deployed intelligent robotic systems. The NeurIPS 2021 AWS DeepRacer challenge is a part of a series of competitions in the area of AV called The AI Driving Olympics (AI-DO). In this competition, the task is to train a Reinforcement Learning (RL) agent (i.e., an autonomous car) that learns to drive by interacting with a virtual environment (a simulated track). The agent achieves this by taking action based on previous observations and a current state to maximize the expected reward. We test the trained agent’s driving performance when maneuvering an AWS DeepRacer car on a real-world mock track. The driving performance metric is time to lap completion without going off the same in the context of AWS DeepRacer competition. In Fig 1, we show the images of the simulated (left) and actual (right) track as captured by the camera mounted on the vehicle with matching extrinsic parameters.",ArXiv,2022.0,10.48550/arXiv.2205.07481,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1ae6f743d514d81ddac3996f9ac1cbcad84a37be,https://www.semanticscholar.org/paper/1ae6f743d514d81ddac3996f9ac1cbcad84a37be,How You Act Tells a Lot: Privacy-Leakage Attack on Deep Reinforcement Learning,"Machine learning has been widely applied to various applications, some of which involve training with privacy-sensitive data. A modest number of data breaches have been studied, including credit card information in natural language data and identities from face dataset. However, most of these studies focus on supervised learning models. As deep reinforcement learning (DRL) has been deployed in a number of real-world systems, such as indoor robot navigation, whether trained DRL policies can leak private information requires in-depth study.To explore such privacy breaches in general, we mainly propose two methods: environment dynamics search via genetic algorithm and candidate inference based on shadow policies. We conduct extensive experiments to demonstrate such privacy vulnerabilities in DRL under various settings. We leverage the proposed algorithms to infer floor plans from some trained Grid World navigation DRL agents with LiDAR perception. The proposed algorithm can correctly infer most of the floor plans and reaches an average recovery rate of 95.83% using policy gradient trained agents. In addition, we are able to recover the robot configuration in continuous control environments and an autonomous driving simulator with high accuracy. To the best of our knowledge, this is the first work to investigate privacy leakage in DRL settings and we show that DRL-based agents do potentially leak privacy-sensitive information from the trained policies.",AAMAS,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e4adba27eed8693966b15aaad16936688752b04a,https://www.semanticscholar.org/paper/e4adba27eed8693966b15aaad16936688752b04a,FathomNet: A global image database for enabling artificial intelligence in the ocean,"The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. As baselines are sought by the research community, the volume and rate of this required data collection rapidly outpaces our abilities to process and analyze them. Recent advances in machine learning enables fast, sophisticated analysis of visual data, but have had limited success in the ocean due to lack of data standardization, insufficient formatting, and demand for large, labeled datasets. To address this need, we built FathomNet, an open-source image database that standardizes and aggregates expertly curated labeled data. FathomNet has been seeded with existing iconic and non-iconic imagery of marine animals, underwater equipment, debris, and other concepts, and allows for future contributions from distributed data sources. We demonstrate how FathomNet data can be used to train and deploy models on other institutional video to reduce annotation effort, and enable automated tracking of underwater concepts when integrated with robotic vehicles. As FathomNet continues to grow and incorporate more labeled data from the community, we can accelerate the processing of visual data to achieve a healthy and sustainable global ocean. A vast, changing ocean impacts a myriad of life The ocean is experiencing unprecedented rapid change, and visually monitoring marine biota at the spatiotemporal scales needed for responsible stewardship is a formidable task. Monitoring a space as vast as the ocean1 that is filled with life that we have yet to describe2, using traditional, resource-intensive (e.g., time, person-hours, cost) sampling methodologies are limited in their ability to scale in spatiotemporal resolution and engage diverse communities3. However, with the advent of modern robotics4, low-cost observation platforms, and distributed sensing5, we are beginning to see a paradigm shift in ocean exploration and discovery. This shift is evidenced in oceanographic monitoring via satellite remote sensing of near-surface ocean conditions and the global ARGO float array, where distributed platforms and open data structures are propelling the chemical and remote sensing communities to new scales of observation6, 7. Due to a variety of constraints, large-scale sampling of biological communities or processes below the surface waters of the ocean has largely lagged behind. There are three common modalities for observing biology and biological processes in the ocean – acoustics, “-omics"", and imaging – each with their strengths and weaknesses. Acoustics allow for observations of populationand group-scale dynamics, however individual-scale observations, especially determination of animals down to lower taxonomic groups like species, are challenging tasks8. The promising field of eDNA allows for identification of biological communities based on their shed DNA in the water column. While eDNA studies provide broad-scale views of biological communities with only a few discrete samples, determining the spatial source of the DNA, relating the measurements to population sizes, and the presence of confounding non-marine biological markers in samples are active areas of research that still need to be addressed9. Ultimately, -omics and acoustics approaches rely on visual observations for verification. Imaging, a non-extractive method for ocean observation, enables the identification of animals to the species level, elucidates community structure and spatial relationships in a variety of habitats, and reveals fine-scale behavior of animal groups10. However, processing visual data, particularly data with complex scenes and organisms that require expert classifications, is a resource-intensive process that cannot be scaled without significant investment, capacity building, and advances in automation11, 12. Non-invasively capturing life in the ocean with imaging Imaging is an increasingly common modality for sampling biological communities in a variety of environments due to the ease by which the technology can be deployed, and the number of remotely controlled and autonomous platforms that can be used13. Imaging has also been used for real-time, underwater vehicle navigation and control while performing difficult tasks in complex environments14. Moreover, imaging is an effective engagement tool to share marine life information and the issues facing the ocean with broader communities15, 16. In short, visual data is an invaluable tool for better understanding the ocean and conveying that information broadly. Given all the applications of marine imaging, a number of annotation tools have been developed to manage and analyze visual data. These efforts have resulted in many capable software solutions that can either be deployed locally on a computer, during field expeditions, or broadly on the worldwide web17. However, with the limited availability of experts and the prohibitive costs to annotate and store footage, novel methods for automated annotation of marine visual data are desperately needed. This need is motivating the development and deployment of artificial intelligence and data science tools for ocean ecology. Automating analysis of visual data using artificial intelligence Artificial intelligence (AI) is a broad term that encompasses many different approaches18, some of which have already been used to study marine systems. Statistical learning methods like random forests have been used in the plankton imaging community, achieving automated classification of microscale plants and animals with accuracies greater than 90%11. Unsupervised learning can be deployed with minimal data and a priori knowledge of marine environments, however these algorithms have limited application for automating the detection and classification of objects in marine imagery with sufficient granularity and detail to be used for annotation19. Deep learning algorithms trained on visual data where all objects have been identified (e.g., Figure 1) have improved performance of automated annotation and classification tasks to finer taxonomic levels20, 21, however this approach requires publicly available, large-scale labeled image datasets for training22–24. Image repositories for terrestrial applications have been available to the computer vision (CV) community for many years. ImageNet was the first labeled set based on a hierarchy, or the number of classes (or “things"") in WordNet, with the long-term goal to collect 500 to 1k full-resolution images for 80k concepts, or ⇠50M images22. In order to reach this scale, ImageNet used images scraped from Flickr, resulting in a collection of largely iconic images (e.g., centered objects in relatively uncluttered environments). Like ImageNet, Microsoft’s COCO23 used workers with Amazon’s Mechanical Turk to generate labels (5k labeled instances) for images with 90 classes, resulting in 2.5M labeled instances in more than 328k images. More recently, iNat2017 is a biologically focused dataset built from 675k images of 5k species of animals that have been collected and verified by users of iNaturalist25. Unlike ImageNet and iNat2017, COCO was specifically built to include non-iconic views of “things"" that provides imagery with contextual relationships between objects, which is especially relevant to marine environments.",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
24a8f407cdfdbb63d0c320884a648dd0e75cbb2f,https://www.semanticscholar.org/paper/24a8f407cdfdbb63d0c320884a648dd0e75cbb2f,Performance of Predictive Indoor mmWave Networks With Dynamic Blockers,"In this paper, we consider millimeter Wave (mmWave) technology to provide reliable wireless network service within factories where links may experience rapid and temporary fluctuations of the received signal power due to dynamic blockers, such as humans and robots, moving in the environment. We propose a novel beam recovery procedure that leverages Machine Learning (ML) tools to predict the starting and finishing of blockage events. This erases the delay introduced by current 5G New Radio (5G-NR) procedures when switching to an alternative serving base station and beam, and then re-establish the primary connection after the blocker has moved away. Firstly, we generate synthetic data using a detailed system-level simulator that integrates the most recent 3GPP 3D Indoor channel models and the geometric blockage Model-B. Then, we use the generated data to train offline a set of beam-specific Deep Neural Network (DNN) models that provide predictions about the beams’ blockage states. Finally, we deploy the DNN models online into the system-level simulator to evaluate the benefits of the proposed solution. Our prediction-based beam recovery procedure guarantees higher signal level stability and up to 82% data rate improvement with respect a detection-based method when blockers move at speed of 2 m/s.",IEEE Transactions on Cognitive Communications and Networking,2021.0,10.1109/tccn.2021.3118459,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ab4279e912556476d52df7c6a67d83dce5afab9d,https://www.semanticscholar.org/paper/ab4279e912556476d52df7c6a67d83dce5afab9d,"Towards the use of consumer-grade electromyographic armbands for interactive, artistic robotics performances","In recent years, gesture-based interfaces have been explored in order to control robots in non-traditional ways. These require the use of systems that are able to track human body movements in 3D space. Deploying Mo-cap or camera systems to perform this tracking tend to be costly, intrusive, or require a clear line of sight, making them ill-adapted for artistic performances. In this paper, we explore the use of consumer-grade armbands (Myo armband) which capture orientation information (via an inertial measurement unit) and muscle activity (via electromyography) to ultimately guide a robotic device during live performances. To compensate for the drop in information quality, our approach rely heavily on machine learning and leverage the multimodality of the sensors. In order to speed-up classification, dimensionality reduction was performed automatically via a method based on Random Forests (RF). Online classification results achieved 88% accuracy over nine movements created by a dancer during a live performance, demonstrating the viability of our approach. The nine movements are then grouped into three semanticallymeaningful moods by the dancer for the purpose of an artistic performance achieving 94% accuracy in real-time. We believe that our technique opens the door to aesthetically-pleasing sequences of body motions as gestural interface, instead of traditional static arm poses.",2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),2017.0,10.1109/ROMAN.2017.8172430,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,https://www.semanticscholar.org/paper/7d6e9e6e39c70b947296cb4c17aacf4452fc5b29,Exploring the Use of Synthetic Gradients for Distributed Deep Learning across Cloud and Edge Resources,"With the explosive growth of data, largely contributed by the rapidly and widely deployed smart devices on the edge, we need to rethink the training paradigm for learning on such realworld data. The conventional cloud-only approach can hardly keep up with the computational demand from these deep learning tasks; and the traditional back propagation based training method also makes it difficult to scale out the training. Fortunately, the continuous advancement in System on Chip (SoC) hardware is transforming edge devices into capable computing platforms, and can potentially be exploited to address these challenges. These observations have motivated this paper’s study on the use of synthetic gradients for distributed training cross cloud and edge devices. We employ synthetic gradients into various neural network models to comprehensively evaluate its feasibility in terms of accuracy and convergence speed. We distribute the training of the various layers of a model using synthetic gradients, and evaluate its effectiveness on the edge by using resource-limited containers to emulate edge devices. The evaluation result shows that the synthetic gradient approach can achieve comparable accuracy compared to the conventional back propagation, for an eight-layer model with both fully-connected and convolutional layers. For a more complex model (VGG16), the training suffers from some accuracy degradation (up to 15%). But it achieves 11% improvement in training speed when the layers of a model are decoupled and trained on separate resource-limited containers, compared to the training of the whole model using the conventional method on the physical machine.",HotEdge,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1e01fd005f5f1c2cf1b27c1ac8b014dfd3983da3,https://www.semanticscholar.org/paper/1e01fd005f5f1c2cf1b27c1ac8b014dfd3983da3,Big but Imperceptible Adversarial Perturbations via Semantic Manipulation,"Machine learning, especially deep learning, is widely applied to a range of applications including computer vision, robotics and natural language processing. However, it has been shown that machine learning models are vulnerable to adversarial examples, carefully crafted samples that deceive learning models. In-depth studies on adversarial examples can help better understand potential vulnerabilities and therefore improve model robustness. Recent works have introduced various methods which generate adversarial examples. However, all require the perturbation to be of small magnitude ($\mathcal{L}_p$ norm) for them to be imperceptible to humans, which is hard to deploy in practice. In this paper we propose two novel methods, tAdv and cAdv, which leverage texture transfer and colorization to generate natural perturbation with a large $\mathcal{L}_p$ norm. We conduct extensive experiments to show that the proposed methods are general enough to attack both image classification and image captioning tasks on ImageNet and MSCOCO dataset. In addition, we conduct comprehensive user studies under various conditions to show that our generated adversarial examples are imperceptible to humans even when the perturbations are large. We also evaluate the transferability and robustness of the proposed attacks against several state-of-the-art defenses.",ArXiv,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
55ee479ca65a316ce9e9608ceac319be28e8e6e0,https://www.semanticscholar.org/paper/55ee479ca65a316ce9e9608ceac319be28e8e6e0,Intrinsically Motivated Self-Supervised Deep Sensorimotor Learning for Grasping,"Deep learning has been successful in a variety of applications that have high-dimensional state spaces such as object recognition, video games, and machine translation. Deep neural networks can automatically learn important features from high-dimensional state given large training datasets. However, the success of deep learning in robot systems in the realworld is limited due to the cost of obtaining these large datasets. To overcome this problem, we propose an information-theoretic, intrinsically motivated, self-labeling mechanism using closed-loop control states. Taking this approach biases exploration to informative interactions-as such, a robot requires much less training to achieve reliable performance. In this paper, we explore the impact such an approach has on learning how to grasp objects. We evaluate different intrinsic motivators present in the literature applied appropriately in our framework and discuss the benefits and drawbacks of each.",2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),2018.0,10.1109/IROS.2018.8593424,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bf6a727c5f4c3e3e2ca69d1c195087df4e172132,https://www.semanticscholar.org/paper/bf6a727c5f4c3e3e2ca69d1c195087df4e172132,Human-Machine Teaming and Its Legal and Ethical Implications,"Humans rely on machines in accomplishing missions while machines need humans to make them more intelligent and more powerful. Neither side can go without the other, especially in complex environments when autonomous mode is initiated. Things are becoming more complicated when law and ethical principles should be applied in these complex environments. One of the solutions is human-machine teaming, as it takes advantage of both the best humans can offer and the best that machines can provide. This article intends to explore ways of implementing law and ethical principles in artificial intelligence (AI) systems using humanmachine teaming. It examines the existing approaches, reveals their limitations, and calls for the establishment of accountability and the use of a checks-and-balances framework in AI systems. It also discusses the legal and ethical implications of this solution. Human-Machine Teaming The recent development of AI led by humans has made machines more intelligent and powerful. Machines are capable of performing more tasks now than ever before, either in automated mode or in autonomous mode. They are widely utilized in various fields. By default, machines, or AI-systems, are not equipped with the capability of applying law and ethical principles. How to enable such a capability within AI-systems is obviously a challenge. Without such a capability, AI systems can hardly be trusted. Therefore, this challenge has to be tackled. To this end, various approaches have been proposed and/or utilized in dealing with this challenge. However, the majority of the approaches available lack a checking system that verifies the legal and ethical decision-making. To establish such a checking system calls for human-machine teaming, which takes advantage of the best that both humans and machines can offer. Machines are good at speedy calculation, fast data analysis, quick pattern recognition, rapid processing, and repetitive work. They can learn much more quickly than humans. They can process a large amount of information simultaneously. They can “calculate, analyze and perform tasks tireless and round the clock”, while not being “affected or influenced by emotions, feelings, wants, needs and other factors that often cloud the judgment and intelligence of us mere mortals” (Whitney, 2017). These capabilities ensure speed and precision. Humans are good at imagination and creativity in addition to resolving ambiguity and exercising judgment. They are superior to machines with respect to the use of instincts, common sense, life experience in performing tasks, making decisions, and solving problems (Whitney, 2017). When humans and machines are teamed together, humans can complement machines with training, explanation, and sustainment while machines can complement humans with amplification, interaction, and embodiment. Amplification refers to extraordinary data-driven insights. Interaction refers to the employment of advanced communication interfaces such as voice-driven natural-language processing devices. Embodiment refers to the integration of AI-systems with sensors, motors, and actuators. Such an integration allows “robots to share workspace with humans and engage in physically collaborative work” (Daugherty and Wilson, 2018). 1 Chen and Wingfield: Human-Machine Teaming and Its Legal and Ethical Implications Published by Scholar Commons, 2020 Taking advantage of what human-machine teaming can offer, this article explores a new way in which law and ethics can be implemented, checked, and applied in decision-making within AI systems. Before this exploration, existing approaches are examined. Current Approaches in Implementing Law and Ethics in AI Systems There are various approaches in implementing law and ethical principles in AI systems. Generally speaking, they fall into the categories of the top-down approach, the bottom-up approach, the hybrid approach, and the no-use approach. The no-use approach argues for the regulation and prohibition of decision-making by autonomous AI systems. This avoids potential legal and ethical issues, reducing the complication of introducing legal and ethical mechanisms within AI systems. This is an approach at the policy level. The representatives of this approach are the Data Protection Act 2018 and the European Union’s General Data Protection Act 2018. Under these data protection frameworks, solely automated decision-making is prohibited (Edward and Veale, 2017). Another representative is the European Civil Law Rules in Robotics, which clearly states that a robot’s presence should “not dissolve social ties”. While acting as an agent helping people, “the robot may not substitute humans entirely” (Nevejans, 2016). Still another representative is Underwood (2017), who argues for a ban on the development, manufacture, and deployment of lethal autonomous weapon systems (LAWS) or such type of machines/systems because these systems lack responsibility and predictability. The lack of responsibility refers to the difficulty in apportioning blame when something goes wrong. The lack of predictability refers to the unpredictability of autonomous systems (Underwood, 2017). Underwood (2017) is correct in pointing out the issue of responsibility. However, she does not look into ways of creating legal and ethical mechanisms and establishing responsibility and accountability within AI systems. Instead, like other representatives in this approach, she counts on a solution at the policy level, i.e. prohibiting the use of autonomous AI systems in decision-making, especially in conflict environment. Other approaches look into ways of implementing law and ethical principles into AI systems. In general, they consist of the following three approaches, i.e. the top-down approach, the bottom-up approach, and the hybrid approach. The top-down approach attempts to formulate legal guidance and ethical principles first and apply them to specific cases. This application helps to make legal and ethical decisions and determine legal and ethical courses of action. For instance, it formulates “moral principles like Kant’s categorical imperative, the utilitarian principle of maximizing utility, or Asimov’s laws of robotics as rules in a software program which is then supposed to derive what has to be morally done in a specific situation” (Misselhorn, 2018). Besides, a maximally-just autonomy using artificial intelligence (MaxAI) is a good example. It requires an ethical governor should provide an assessment on proposed courses of action based on preloaded law and ethical principles such as the laws of war and the rules of engagement (Arkin et al., 2009). It “involves the codification of normative values into rule sets and the interpretation of a wide range of inputs through the application of complex and potentially imperfect machine logic” (Galliott and Scholz, 2018). A minimally-just autonomy using artificial intelligence (MinAI) is another good 2 Military Cyber Affairs, Vol. 4, Iss. 2 [2020], Art. 2 https://scholarcommons.usf.edu/mca/vol4/iss2/2 DOI: https://doi.org/10.5038/2378-0789.4.2.1074 example. Having realized the challenges in MaxAI, the designers of MinAI do not use an ethical governor. Instead, they only use a preloaded, hard-coded and narrow set of constraints in identifying and avoiding what is ethically impermissible, such as “lawfully protected symbols and locations, signs of surrender (including beacons), and sites that are hors de combat” (Galliott and Scholz, 2018). Within the top-down approach, the precise interpretation and accurate application of legal guidance and ethical principles in particular cases are always challenges. So is the implementation of the interpretation and application in software development. In other words, within these environments, there are a lot of dynamics. It is hard to capture all the possibilities and then select the most appropriate ones from them. It is even harder to code all these in software and make them work as required and as designed. At present, the approach is mainly at the conceptual level. There is not much implementation of this approach. Besides, this approach faces the challenge of the frame problem (Horgan and Timmons, 2009; McCarthy and Hayes, 1969). Since “potentially every new piece of information may have an impact on the whole cognitive system of an agent”, there are “reasons to doubt that moral normativity is fully systematizable by exceptionless general principles” (Misselhorn, 2018). The bottom-up approach intends to judge each particular case individually and then derive certain legal and ethical rules of thumb. In software design, it is suggested that artificial neural networks be used to find relationships or patterns that eventually help to formulate legal guidance and ethical principles (Misselhorn, 2018; Dancy, 2013). Besides, there are recommendations for using modeling for the inputs received or the data captured. The modeling ranges from human moral learning process (Froese and Di Paolo, 2010) to human socialization (Breazeal and Scassellati, 2002; Fong et al., 2002). However, in the bottom-up approach, it is difficult to question, interpret, explain, supervise, and control AI systems “because deeplearning systems cannot easily track their own ‘reasoning’” (Ciupa, 2017). Also, as a solution, the bottom-up approach poses “problems of operationalization, safety and acceptance”. Therefore, it is “of limited suitability for implementing moral capacities in autonomous artificial systems” (Misselhorn, 2018). The four reasons for this claim are as follows: First, it is “difficult to evaluate when precisely a system possesses the capacity for moral learning and how it will, in effect, evolve”. Second, there is “no component or mechanism to point to which embodies the system’s moral capacities”. Third, it is hard to make prediction. Fourth, it is difficult to reconstruct the way in which a moral decision is arrived. Therefore, this approach is",,2020.0,10.5038/2378-0789.4.2.1074,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
005b91e2378093803c7342f256268ee0643b9c3d,https://www.semanticscholar.org/paper/005b91e2378093803c7342f256268ee0643b9c3d,Sample-Efficient I-Projections for Robot Learning,"Robots had a great impact on the manufacturing industry ever since the early seventies when companies such as KUKA and ABB started deploying their first industrial robots. These robots merely performed very specific tasks in specific ways within well-defined environments. Still, they proved to be very useful as they could exceed human performance at these tasks. However, in order to enable robots to enter our daily life, they need to become more versatile and need to operate in much less structured environments. 
This thesis is partly devoted to stretching these limitations by means of learning, namely imitation learning (IL) and inverse reinforcement learning (IRL). 
 
Reinforcement learning (RL) is a powerful approach to enable robots to solve a task in an unknown environment. The practitioner describes a desired behavior by specifying a reward function and the robot autonomously interacts with the environment in order to find a control policy that generates high accumulated reward. However, RL is not suitable for teaching new tasks by non-experts because specifying appropriate reward functions can be difficult. Demonstrating the desired behavior is often easier for non-experts. Imitation learning can be used in order to enable the robot to reproduce the demonstrations. However, without explicitly inferring and modeling the intentions of the demonstrations, it can become difficult to solve the task for unseen situations. Inverse reinforcement learning (IRL) therefore aims to infer a reward function from the demonstrations, such that optimizing this reward function yields the desired behavior even for different situations. 
 
This thesis introduces a unifying approach to solve the inverse reinforcement learning problem in the same way as the reinforcement learning problem. This is achieved by framing both problems as information projection problems, i.e., we strive to minimize the relative entropy between a probabilistic model of the robot behavior and a given desired distribution. Furthermore, a trust region on the robot behavior is used to stabilize the optimization. 
For inverse reinforcement learning, the desired distribution is implicitly given by the expert demonstrations. The resulting optimization can be efficiently solved using state-of-the-art reinforcement learning methods. 
For reinforcement learning, the log-likelihood of the desired distribution is given by the reward function. The resulting optimization problem corresponds to a standard reinforcement learning formulation, except for an additional objective of maximizing the entropy of the robot behavior. This entropy objective adds little overhead to the optimization, but can lead to better exploration and more diversified policies. 
 
Trust-region I-projections are not only useful for training robots, but can also be applied to other machine learning problems. I-projections are typically used for variational inference, in order to approximate an intractable distribution by a simpler model. However, the resulting optimization problems are usually optimized based on stochastic gradient descent which often suffers from high variance in the gradient estimates. As trust-region I-projections where shown to be effective for reinforcement learning and inverse reinforcement learning, this thesis also explores their use for variational inference. More specifically, trust-region I-projections are investigated for the problem of approximating an intractable distribution by a Gaussian mixture model (GMM) with an adaptive number of components. GMMs are highly desirable for variational inference because they can yield arbitrary accurate approximations while inference from GMMs is still relatively cheap. In order to make learning the GMM feasible, we derive a lower bound that enables us to decompose the objective function. The optimization can then be performed by iteratively updating individual components using a technique from reinforcement learning. The resulting method is capable of learning approximations of significantly higher quality than existing variational inference methods. 
 
Due to the similarity of the underlying optimization problems, the insights gained from our variational inference method are also useful for IL and IRL. Namely, a similar lower bound can be applied also for the I-projection formulation of imitation learning. However, whereas for variational inference the lower bound serves to decompose the objective function, for imitation learning it allows us to provide a reward signal to the robot that does not depend on its behavior. Compared to reward functions that are relative to the current behavior of the robot---which are typical for popular adversarial methods---behavior-independent reward functions have the advantages that we can show convergence even for greedy optimization. Furthermore, behavior-independent reward functions solve the inverse reinforcement learning problem, thereby closing the gap between imitation learning and IRL. However, algorithms derived from our non-adversarial formulation are actually very similar to existing AIL methods, and we can even show that adversarial inverse reinforcement learning (AIRL) is indeed an instance of our formulation. AIRL was derived from an adversarial formulation, and we point out several problems of that derivation. In contrast, we show that AIRL can be straightforwardly derived from out non-adversarial formulation. Furthermore, we demonstrate that the non-adversarial formulation can be also used to derive novel algorithms by presenting a non-adversarial method for offline imitation learning.",,2021,10.12921/TUPRINTS-00014271,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7afa4a11b52a9e405af8d0ada8bc78b5ad15e3db,https://www.semanticscholar.org/paper/7afa4a11b52a9e405af8d0ada8bc78b5ad15e3db,Predicting Falls of a Humanoid Robot through Machine Learning,"Although falls are undesirable in humanoid robots, they are also inevitable, especially as robots get deployed in physically interactive human environments. We consider the problem of fall prediction, i.e., to predict if a robot's balance controller can prevent a fall from the current state. A trigger from the fall predictor is used to switch the robot from a balance maintenance mode to a fall control mode. Hence, it is desirable for the fall predictor to signal imminent falls with sufficient lead time before the actual fall, while minimizing false alarms. Analytical techniques and intuitive rules fail to satisfy these competing objectives on a large robot that is subjected to strong disturbances and therefore exhibits complex dynamics. Today effective supervised learning tools are available for finding patterns in high-dimensional data. Our paper contributes a novel approach to engineer fall data such that a supervised learning method can be exploited to achieve reliable prediction. Specifically, we introduce parameters to control the tradeoff between the false positive rate and lead time. Several parameter combinations yield solutions that improve both the false positive rate and the lead time of hand-coded solutions. Learned predictors are decision lists with typical depths of 5-10, in a 16-dimensional feature space. Experiments are carried out in simulation on an Asimo-like robot.",IAAI,2010,10.1609/aaai.v24i2.18815,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
249685efbf5a8151f640c275066e5c8f9ce46547,https://www.semanticscholar.org/paper/249685efbf5a8151f640c275066e5c8f9ce46547,Artificial intelligence - The promise for an agricultural revolution in new era,"Agriculture is slowly becoming digital. The adoption of Artificial Intelligence (AI) and Machine Learning (ML) both in terms of agricultural products and in-field farming techniques are increasing. Artificial Intelligence in agriculture is emerging in three major areas, namely agricultural robotics, soil and crop monitoring and predictive analytics. The use of sensors and soil sampling techniques are increasing day by day which helps in gathering of data. In turn, this data is stored in farm management system which is better processed and analysed. Thus, the data available along with other related data paves a way to successfully deploy AI in agriculture. AI in agriculture is emergingin cognitive computing and it has all the scope to become the most disruptive technology in agriculture services as it is able to understand, learn and respond to different situations (based on learning) to increase efficiency. The areas where the use of cognitive solutions can benefit agriculture are growth driven by IOT, image-based insight generation, identification of optimal mix for agronomic products, health monitoring of crops and automation techniques in irrigation and enabling farmers. In addition, the drone based solutions have significant impact in terms of productivity gains, coping with adverse weather conditions, yield management and precision farming.The emergence of new age technologies like Artificial Intelligence (AI), Cloud Machine Learning, Satellite Imagery and advanced analytics are creating an ecosystem for smart farming. Fusion of all this technology is enabling farmers achieve higher average yield and better price control.",AGRICULTURE UPDATE,2020,10.15740/has/au/15.4/435-437,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d7445ee7269fa43c4f64c56d45a53e469df011cf,https://www.semanticscholar.org/paper/d7445ee7269fa43c4f64c56d45a53e469df011cf,Domain Generalization via Optical Flow: Training a CNN in a Low-Quality Simulation to Detect Obstacles in the Real World,"Many applications in robotics and autonomous systems benefit from machine learning applied to computer vision, but often the acquisition and preparation of data for training is complex and time-consuming. Simulation can significantly reduce the effort and potential risk of data collection, thereby allowing faster prototyping. However, the ability of a data-driven system to generalize from simulated data to the real world is far from obvious and often leading to inconsistent real-world results. This paper demonstrates that some properties of optical flow can be exploited to address this generalization problem. In this work, we train a neural network to detect collisions with simulated optical flow data. Our network, FlowDroNet, is able to correctly predict up to 89 percent of the collisions of a realworld dataset and easily achieves a higher detection accuracy when compared to a network trained on a similar dataset of realworld collisions. We release our code, models and a real-world dataset for collision avoidance as open-source. We also explore the relationship between the complexity of the input information and the ability to generalize to unseen environments, and show that in some situations, optical flow is an interesting tool to bridge the reality gap.",2020 17th Conference on Computer and Robot Vision (CRV),2020,10.1109/CRV50864.2020.00024,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bdb0aa91fd02b45ca859ec1b63b1761183e7288a,https://www.semanticscholar.org/paper/bdb0aa91fd02b45ca859ec1b63b1761183e7288a,AutoSoC: Automating Algorithm-SOC Co-design for Aerial Robots,"Aerial autonomous machines (Drones) has a plethora of promising applications and use cases. While the popularity of these autonomous machines continues to grow, there are many challenges, such as endurance and agility, that could hinder the practical deployment of these machines. The closed-loop control frequency must be high to achieve high agility. However, given the resource-constrained nature of the aerial robot, achieving high control loop frequency is hugely challenging and requires careful co-design of algorithm and onboard computer. Such an effort requires infrastructures that bridge various domains, namely robotics, machine learning, and system architecture design. To that end, we present AutoSoC, a framework for co-designing algorithms as well as hardware accelerator systems for end-to-end learning-based aerial autonomous machines. We demonstrate the efficacy of the framework by training an obstacle avoidance algorithm for aerial robots to navigate in a densely cluttered environment. For the best performing algorithm, our framework generates various accelerator design candidates with varying performance, area, and power consumption. The framework also runs the ASIC flow of place and route and generates a layout of the floor-planed accelerator, which can be used to tape-out the final hardware chip.",ArXiv,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
86f9e8d3f13e9143b684e7e1bd561354f3cd53ba,https://www.semanticscholar.org/paper/86f9e8d3f13e9143b684e7e1bd561354f3cd53ba,Haptic Learning Towards Neural-Network-based adaptive Cobot Path-Planning for unstructured spaces,"Collaborative Robots, or Cobots, bring new possibilities for human-machine interaction within the fabrication process, allowing each actor to contribute with their specific capabilities. However creative interaction brings unexpected changes, obstacles, complexities and non-linearities which are encountered in real time and cannot be predicted in advance. This paper presents an experimental methodology for robotic path planning using Machine Learning. The focus of this methodology is obstacle avoidance. A neural network is deployed, providing a relationship between the robot's pose and its surroundings, thus allowing for motion planning and obstacle avoidance, directly integrated within the design environment. The method is demonstrated through a series of case-studies. The method combines haptic teaching with machine learning to create a task specific dataset, giving the robot the ability to adapt to obstacles without being explicitly programmed at every instruction. This opens the door to shifting to robotic applications for construction in unstructured environments, where adapting to the singularities of the workspace, its occupants and activities presents an important computational hurdle today.",,2019,10.5151/PROCEEDINGS-ECAADESIGRADI2019_280,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b7e7e14724dfbbaf6b8ad482aae98b7fcec5ccd9,https://www.semanticscholar.org/paper/b7e7e14724dfbbaf6b8ad482aae98b7fcec5ccd9,Sample-Efficient Safety Assurances using Conformal Prediction,"When deploying machine learning models in highstakes robotics applications, the ability to detect unsafe situations is crucial. Early warning systems can provide alerts when an unsafe situation is imminent (in the absence of corrective action). To reliably improve safety, these warning systems should have a provable false negative rate; i.e. of the situations that are unsafe, fewer than ε will occur without an alert. In this work, we present a framework that combines a statistical inference technique known as conformal prediction with a simulator of robot/environment dynamics, in order to tune warning systems to provably achieve an ε false negative rate using as few as 1/ε data points. We apply our framework to a driver warning system and a robotic grasping application, and empirically demonstrate guaranteed false negative rate and low false detection (positive) rate using very little data.",ArXiv,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f02070c2c291305dc9bed98b59c67021e9643b12,https://www.semanticscholar.org/paper/f02070c2c291305dc9bed98b59c67021e9643b12,Learning to Learn with Gradients,"Humans have a remarkable ability to learn new concepts from only a few examples and quickly adapt to unforeseen circumstances. To do so, they build upon their prior experience and prepare for the ability to adapt, allowing the combination of previous observations with small amounts of new evidence for fast learning. In most machine learning systems, however, there are distinct train and test phases: training consists of updating the model using data, and at test time, the model is deployed as a rigid decision-making engine. In this thesis, we discuss gradient-based algorithms for learning to learn, or meta-learning, which aim to endow machines with flexibility akin to that of humans. Instead of deploying a fixed, non-adaptable system, these meta-learning techniques explicitly train for the ability to quickly adapt so that, at test time, they can learn quickly when faced with new scenarios.To study the problem of learning to learn, we first develop a clear and formal definition of the meta-learning problem, its terminology, and desirable properties of meta-learning algorithms. Building upon these foundations, we present a class of model-agnostic meta-learning methods that embed gradient-based optimization into the learner. Unlike prior approaches to learning to learn, this class of methods focus on acquiring a transferable representation rather than a good learning rule. As a result, these methods inherit a number of desirable properties from using a fixed optimization as the learning rule, while still maintaining full expressivity, since the learned representations can control the update rule.We show how these methods can be extended for applications in motor control by combining elements of meta-learning with techniques for deep model-based reinforcement learning, imitation learning, and inverse reinforcement learning. By doing so, we build simulated agents that can adapt in dynamic environments, enable real robots to learn to manipulate new objects by watching a video of a human, and allow humans to convey goals to robots with only a few images. Finally, we conclude by discussing open questions and future directions in meta-learning, aiming to identify the key shortcomings and limiting assumptions of our existing approaches.",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
09b27b202e46234b9674759fb7cc316137ae7f3b,https://www.semanticscholar.org/paper/09b27b202e46234b9674759fb7cc316137ae7f3b,Reinforcement Learning-based Switching Controller for a Milliscale Robot in a Constrained Environment,"This work presents a reinforcement learning-based switching control mechanism to autonomously move a ferromagnetic object (representing a milliscale robot) around obstacles within a constrained environment in the presence of disturbances. This mechanism can be used to navigate objects (e.g., capsule endoscopy, swarms of drug particles) through complex environments when active control is a necessity but where direct manipulation can be hazardous. The proposed control scheme consists of a switching control architecture implemented by two sub-controllers. The first sub-controller is designed to employs the robot’s inverse kinematic solutions to do an environment search of the to-be-carried ferromagnetic particle while being robust to disturbances. The second sub-controller uses a customized rainbow algorithm to control a robotic arm, i.e., the UR5 robot, to carry a ferromagnetic particle to a desired position through a constrained environment. For the customized Rainbow algorithm, Quantile Huber loss from the Implicit Quantile Networks (IQN) algorithm and ResNet are employed. The proposed controller is first trained and tested in a real-time physics simulation engine (PyBullet). Afterward, the trained controller is transferred to a UR5 robot to remotely transport a ferromagnetic particle in a real-world scenario to demonstrate the applicability of the proposed approach. The experimental results show an average success rate of 98.86% calculated over 30 episodes for randomly generated trajectories. Note to Practitioners—Deep reinforcement learning methods have been widely applied in computer games and simulations. However, employing these algorithms for practical, real-world applications such as robotics becomes challenging due to the difficulty in obtaining training samples. This paper predominantly focuses on bridging the gap between simulations and the realworld implementation of a reinforcement learning algorithm for a robotic application in the context of miniaturized drug delivery robots and robotic capsule endoscopes. This paper presents the derivation and experimental validation of a reinforcement learning-based algorithm for controlling a magnetically-actuated small-scale robot within a simplified model of the large intestine in the presence of disturbances. We demonstrate the possibility of training a high fidelity reinforcement learning algorithm fully within a simulated environment before deploying it as-is in a real-world scenario through carrying out different experiments. Implementing the presented control framework complements a large body of this work, and the results offer a feasibility study of using reinforcement learning algorithms in practice.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0fc5cd83fcc493a764b6b8ab8496a40f46d130d8,https://www.semanticscholar.org/paper/0fc5cd83fcc493a764b6b8ab8496a40f46d130d8,DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations,"The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.",AIES,2022,10.48550/arXiv.2203.02013,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9bb4db55b308c910564c75997e93ed57b2f7e259,https://www.semanticscholar.org/paper/9bb4db55b308c910564c75997e93ed57b2f7e259,A containerized simulation platform for robot learning peg-in-hole task,"In this paper, we build a simulation platform for robot learning peg-in-hole(PiH) task for to study the strategy of inserting the pegs with different geometry features in PiH task with tele-operation. PiH task as a typical assembly task in the industrial field has been widely researched. Though many researches proposed some general solution for PiH, most of them only rely on accurate force control can be achieved or the environment is structured. In the unstructured environment, it is still a huge challenge. And different sizes and shapes of pegs will significantly increase the difficulty of operation even human-in-loop method because of force and torque introduced from the contact environment and uncertainty from vision, many previous strategies cannot be adapted to these situations. Recently, machine learning method has been achieved many successful applications on robotics which can adapt on different situations with many uncertainties, but making robots learning in the real world still needs more setup, and it also may destroy the robots. Our simulation platform which based on state of art ROS and Gazebo and shipped with Docker and Weave virtual network provides a reproducible and easily deployable platform for robot learning the PiH task. And we also include a tele-operation method for the human operator to tele-operate the simulation robot with force feedback during the peg is approaching to the hole which will enable robot learning trajectory execution from human demonstrations.",2018 13th IEEE Conference on Industrial Electronics and Applications (ICIEA),2018,10.1109/ICIEA.2018.8397908,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e1c148b1ef2859e70bdbb65b18092b0335879127,https://www.semanticscholar.org/paper/e1c148b1ef2859e70bdbb65b18092b0335879127,Editorial: Artificial Intelligence and Human Movement in Industries and Creation,"Recent advances in human motion sensing technologies and machine learning have enhanced the potential of Artificial Intelligence to improve our quality of life, increase productivity and reshapemultiple industries, including cultural and creative industries. In order to achieve this goal, humansmust remain at the center of Artificial Intelligence and AI should learn from humans and collaborate effectively with them.Human-CentredArtificial Intelligence (HAI) is expected to create new opportunities and challenges in the future, which cannot yet be foreseen. Any type of programmable entity (e.g., robots, computers, autonomous vehicles, drones, Internet of Things, etc.) will have different layers of perception and sophisticated HAI algorithms that will detect human intentions and behaviors (Psaltis et al., 2017) and learn continuously from them. Thus, every single intelligent system will be able to capture human motions, analyze them (Zhang et al., 2019), detect poses and recognize gestures (Chatzis et al., 2020; Stergioulas et al., 2021) and activities (Papastratis et al., 2020; Papastratis et al., 2021; Konstantinidis et al., 2021), including facial expressions and gaze (Bek et al., 2020), enabling natural collaborationwith humans. Different sensing technologies, such as opticalMocap systems, wearable inertial sensors, RGB or depth cameras and other modality type sensors, are employed for capturing humanmovement in the scene and transforming this information into a digital representation. Most of the researchers usually focus on the use of a single modality sensor due to the simplicity and low cost of the final system and the design of either conventional machine learning algorithms or complex deep learning network architectures for analyzing humanmotion data (Konstantinidis et al., 2018; Konstantinidis et al., 2020). Such cost-effective approaches have been applied to a wide range of application domains, including entertainment (Kaza et al., 2016; Baker, 2020), health (Dias et al.; Konstantinidis et al., 2021), education (Psaltis et al., 2017; Stefanidis et al., 2019), sports (Tisserand et al., 2017), robotics (Jaquier et al., 2020; Gao et al., 2021), art and cultural heritage (Dimitropoulos et al., 2018), showing the great potential of AI technology. Based on the aforementioned, it is evident that HAI is currently at the center of scientific debates and technological exhibitions. Developing and deploying intelligent machines is definitely both an economic challenge (e.g., flexibility, simplification, ergonomy) as well as a societal challenge (e.g., safety, transparency), not only from a factory perspective, but also for the real-world in general. The papers in this Research Topic adopt different sensing technologies, such as depth sensors, inertial suits, IMU sensors and force-sensing resistors (FSRs) to capture human movement, while they present diverse approaches for modeling the temporal data. More specifically, Sakr et al. investigate the feasibility of employing FSRs worn on the arm to measure the Force Myography (FMG) signals for isometric force/torque estimation. A two-stage Edited and reviewed by: Astrid Marieke Rosenthal-von Der Pütten, RWTH Aachen University, Germany",Frontiers in Robotics and AI,2021,10.3389/frobt.2021.712521,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
630feefad9164e33cef0e4e51b56ca79f238a2ee,https://www.semanticscholar.org/paper/630feefad9164e33cef0e4e51b56ca79f238a2ee,"Swarm robotics: Robustness, scalability, and self-X features in industrial applications","Abstract Applying principles of swarm intelligence to the control of autonomous systems in industry can advance our ability to manage complexity in prominent and high-cost sectors—such as transportation, logistics, and construction. In swarm robotics, the exclusive use of decentralized control relying on local communication and information provides the key advantage first of scalability, and second of robustness against failure points. These are directly useful in certain applied tasks that can be studied in laboratory environments, such as self-assembly and self-organized construction. In this article, we give a brief introduction to swarm robotics for a broad audience, with the intention of targeting future industrial applications. We then present a summary of four examples of our recently published research results with simple models. First, we present our approach to self-reconfiguration, which uses collective adjustment of swarm density in a dynamic setting. Second, we describe our robot experiments for self-organized material deployment in structured and semi-structured environments, applicable to braided composites. Third, we present our machine learning approach for self-assembly, motivated as a simple model developing foundational methods, which generates self-organizing robot behaviors to form emergent patterns. Fourth, we describe our experiments implementing a bioinspired model in a robot swarm, where we show self-healing of damage as the robots collectively locate a resource. Overall, the four examples we present concern robustness, scalability, and self-X features, which we propose as potentially relevant to future research in swarm robotics applied to industry sectors. We summarize these approaches as an introduction to our recent research, targeting the broad audience of this journal.",it Inf. Technol.,2019,10.1515/itit-2019-0003,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8138945daadfa4fe36c343ec8755d1ccd8d70e0f,https://www.semanticscholar.org/paper/8138945daadfa4fe36c343ec8755d1ccd8d70e0f,L3E-HD: A Framework Enabling Efficient Ensemble in High-Dimensional Space for Language Tasks,"Brain-inspired hyperdimensional computing (HDC) has been introduced as an alternative computing paradigm to achieve efficient and robust learning. HDC simulates cognitive tasks by mapping all data points to patterns of neural activity in the high-dimensional space, which has demonstrated promising performances in a wide range of applications such as robotics, biomedical signal processing, and genome sequencing. Language tasks, generally solved using machine learning methods, are widely deployed on low-power embedded devices. However, existing HDC solutions suffer from major challenges that impede the deployment of low-power embedded devices: the storage and computation overhead of HDC models grows dramatically with (i) the number of dimensions and (ii) the complex similarity metric during the inference. In this paper, we proposed a novel ensemble framework for the language task, termed L3E-HD, which enables efficient HDC on low-power edge devices. L3E-HD accelerates the inference by mapping data points to a high-dimensional binary space to simplify similarity search, which dominates costly and frequent operation in HDC. Through marrying HDC with the ensemble technique, L3E-HD also addresses the severe accuracy degradation induced by the compression of the dimension and precision of the model. Our experiments show that the ensemble technique is naturally a perfect fit to boost HDCs. We find that our L3E-HD, which is faster, more efficient, and more accurate than conventional machine learning methods, can even surpass the accuracy of the full-precision model at a smaller model size. Code is released at: https://github.com/MXHX7199/SIGIR22-EnsembleHDC.",SIGIR,2022,10.1145/3477495.3531761,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
be88daa1e8e95877797468246f8ac16c07670a6d,https://www.semanticscholar.org/paper/be88daa1e8e95877797468246f8ac16c07670a6d,A Novel Radar Signal Recognition Method Based On Deep Learning Doc Download,"Micro-Doppler Characteristics of Radar Targets is a monograph on radar target’s micro-Doppler effect theory and micro-Doppler feature extraction techniques. The micro-Doppler effect is presented from two aspects, including micro-Doppler effect analysis and micro-Doppler feature extraction, with micro-Doppler effects induced by different micro-motional targets in different radar systems analyzed and several methods of micro-Doppler feature extraction and three-dimensional micro-motion feature reconstruction presented. The main contents of this book include micro-Doppler effect in narrowband radar, micro-Doppler effect in wideband radar, micro-Doppler effect in bistatic radar, microDoppler feature analysis and extraction, and three-dimensional micro-motion feature reconstruction, etc. This book can be used as a reference for scientific and technical personnel engaged in radar signal processing and automatic target recognition, etc. It is especially suitable for beginners who are interested in research on micro-Doppler effect in radar. Presents new views on micro-Doppler effects, analyzing and discussing micro-Doppler effect in wideband radar rather than focusing on narrowband Provides several new methods for micro-Doppler feature extraction which are very helpful and practical for readers Includes practical cases that align with main MATLAB codes in each chapter, with detailed program annotations Wavelet analysis and its applications have been one of the fastest-growing research areas in the past several years. Wavelet theory has been employed in numerous fields and applications, such as signal and image processing, communication systems, biomedical imaging, radar, and air acoustics. Active media technology is concerned with the development of autonomous computational or physical entities capable of perceiving, reasoning, adapting, learning, cooperating, and delegating in a dynamic environment.This book captures the essence of the state of the art in wavelet analysis and its applications and active media technology. At the Congress, invited talks were delivered by distinguished researchers, namely Prof John Daugman of Cambridge University, UK; Prof Bruno Torresani of INRIA, France; Prof Victor Wickerhauser of Washington University, USA, Prof Ning Zhong of the Maebashi Institute of Technology, Japan; Prof John Yen of Pennsylvania State University, USA; and Prof Sankar K Pal of the Indian Statistical Institute, India. This volume is an initiative undertaken by the IEEE Computational Intelligence Society’s Task Force on Security, Surveillance and Defense to consolidate and disseminate the role of CI techniques in the design, development and deployment of security and defense solutions. Applications range from the detection of buried explosive hazards in a battlefield to the control of unmanned underwater vehicles, the delivery of superior video analytics for protecting critical infrastructures or the development of stronger intrusion detection systems and the design of military surveillance networks. Defense scientists, industry experts, academicians and practitioners alike will all benefit from the wide spectrum of successful applications compiled in this volume. Senior undergraduate or graduate students may also discover uncharted territory for their own research endeavors. By studying applications in radar, telecommunications and digital image restoration, this monograph discusses signal processing techniques based on bispectral methods. Improved robustness against different forms of noise as well as preservation of phase information render this method a valuable alternative to common power-spectrum analysis used in radar object recognition, digital wireless communications, and jitter removal in images. The first book to present a systematic and coherent picture of MIMO radars Due to its potential to improve target detection and discrimination capability, Multiple-Input and Multiple-Output (MIMO) radar has generated significant attention and widespread interest in academia, industry, government labs, and funding agencies. This important new work fills the need for a comprehensive treatment of this emerging field. Edited and authored by leading researchers in the field of MIMO radar research, this book introduces recent developments in the area of MIMO radar to stimulate new concepts, theories, and applications of the topic, and to foster further cross-fertilization of ideas with MIMO communications. Topical coverage includes: Adaptive MIMO radar Beampattern analysis and optimization for MIMO radar MIMO radar for target detection, parameter estimation, tracking,association, and recognition MIMO radar prototypes and measurements Space-time codes for MIMO radar Statistical MIMO radar Waveform design for MIMO radar Written in an easy-to-follow tutorial style, MIMO Radar Signal Processing serves as an excellent course book for graduate students and a valuable reference for researchers in academia and industry. This 1179-page book assembles the complete contributions to the International Conference on Intelligent Computing, ICIC 2006: one volume of Lecture Notes in Computer Science (LNCS); one of Lecture Notes in Artificial Intelligence (LNAI); one of Lecture Notes in Bioinformatics (LNBI); and two volumes of Lecture Notes in Control and Information Sciences (LNCIS). Include are 149 revised full papers, and a Special Session on Computing for Searching Strategies to Control Dynamic Processes. Novel deep learning approaches are achieving state-of-the-art accuracy in the area of radar target recognition, enabling applications beyond the scope of human-level performance. This book provides an introduction to the unique aspects of machine learning for radar signal processing that any scientist or engineer seeking to apply these technologies ought to be aware of. Collects the revised and updated versions of lectures presented at an advanced course on [title] held at the Accademia dei Lincei, Rome, 1988, as well as some additional chapters. The 13 chapters address basic concepts on detection, estimation, and optimum filtering; models of clutter; CFAR techniques in clutter; pulse compression and equivalent technologies; pulse doppler radar; MTI, MTD, and adaptive clutter cancellation; rejection of active interference; architecture and implementation of radar signal processors; identification of radar targets; phased arrays; bistatic radars; space-based radar; and evolution and future trends of radar. Primarily for radar engineers and researchers, as well as advanced students. Distributed by INSPEC. Annotation copyright by Book News, Inc., Portland, OR This definitive volume covers state-of-the-art over-the-horizon radar systems, with emphasis on the practical application of advanced signal processing techniques. In recent years rough set theory has attracted the attention of many researchers and practitioners all over the world, who have contributed essentially to its development and applications. Weareobservingagrowingresearchinterestinthefoundationsofroughsets, including the various logical, mathematical and philosophical aspects of rough sets. Some relationships have already been established between rough sets and other approaches, and also with a wide range of hybrid systems. As a result, rough sets are linked with decision system modeling and analysis of complex systems, fuzzy sets, neural networks, evolutionary computing, data mining and knowledge discovery, pattern recognition, machine learning, and approximate reasoning. In particular, rough sets are used in probabilistic reasoning, granular computing (including information granule calculi based on rough mereology), intelligent control, intelligent agent modeling, identi?cation of autonomous stems, and process speci?cation. Methods based on rough set theory alone or in combination with other proacheshavebeendiscoveredwith awide rangeofapplicationsinsuchareasas: acoustics, bioinformatics, business and ?nance, chemistry, computer engineering (e.g., data compression, digital image processing, digital signal processing, pallel and distributed computer systems, sensor fusion, fractal engineering), desion analysis and systems, economics, electrical engineering (e.g., control, signal analysis, power systems), environmental studies, informatics, medicine, molelar biology, musicology, neurology, robotics, social science, software engineering, spatial visualization, Web engineering, and Web mining. These proceedings present technical papers selected from the 2012 International Conference on Intelligent Systems and Knowledge Engineering (ISKE 2012), held on December 15-17 in Beijing. The aim of this conference is to bring together experts from different fields of expertise to discuss the state-of-the-art in Intelligent Systems and Knowledge Engineering, and to present new findings and perspectives on future developments. The proceedings introduce current scientific and technical advances in the fields of artificial intelligence, machine",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cef345d252503559472ba128aaf95ba5e23476ec,https://www.semanticscholar.org/paper/cef345d252503559472ba128aaf95ba5e23476ec,Artificial Intelligence in the Hospitality Sector,"Hospitality is one of the most important sectors of the economy and offers employment to thousands of people. The recent advances in technology has seen that quite a few of the players in this industry have successfully deployed artificial intelligence, machine learning, and robotics. This chapter delves into the details of such deployment in the various processes in this sector and discusses the short-term, medium-term, and long-term impact of these technologies on all the major stakeholders of this industry. The author also looks at the cost benefit analysis of this technologies and concludes that most players sooner, rather than later would be forced by competition to strongly adopt them. The chapter also briefly discusses the changing roles of human employees in this scenario.",,2021,10.4018/978-1-7998-3919-4.CH013,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e3ea899d75cab6bd8297455216fa5a57a6983dfd,https://www.semanticscholar.org/paper/e3ea899d75cab6bd8297455216fa5a57a6983dfd,A Review of Verification and Validation for Space Autonomous Systems,,Current Robotics Reports,2021,10.1007/s43154-021-00058-1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a90b9c22950e03f4f38ed42d7ca2422ca565cc73,https://www.semanticscholar.org/paper/a90b9c22950e03f4f38ed42d7ca2422ca565cc73,Safe Reinforcement Learning for Legged Locomotion,"Designing control policies for legged locomotion is complex due to the under-actuated and non-continuous robot dynamics. Model-free reinforcement learning provides promising tools to tackle this challenge. However, a major bottleneck of applying model-free reinforcement learning in real world is safety. In this paper, we propose a safe reinforcement learning framework that switches between a safe recovery policy that prevents the robot from entering unsafe states, and a learner policy that is optimized to complete the task. The safe recovery policy takes over the control when the learner policy violates safety constraints, and hands over the control back when there are no future safety violations. We design the safe recovery policy so that it ensures safety of legged locomotion while minimally intervening in the learning process. Furthermore, we theoretically analyze the proposed framework and provide an upper bound on the task performance. We verify the proposed framework in four locomotion tasks on a simulated and real quadrupedal robot: efficient gait, catwalk, two-leg balance, and pacing. On average, our method achieves 48.6% fewer falls and comparable or better rewards than the baseline methods in simulation. When deployed it on realworld quadruped robot, our training pipeline enables 34% improvement in energy efficiency for the efficient gait, 40.9% narrower of the feet placement in the catwalk, and two times more jumping duration in the two-leg balance. Our method achieves less than five falls over the duration of 115 minutes of hardware time. 1",ArXiv,2022,10.48550/arXiv.2203.02638,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ec4ce9470af03e607d88b5480c32b972516c45ea,https://www.semanticscholar.org/paper/ec4ce9470af03e607d88b5480c32b972516c45ea,"Woodside Energy Ltd: pioneer in cognitive computing, artificial intelligence and robotics","Cognitive computing is a new disruptive technology with the potential to reshape the oil and gas industry across the entire value chain. For Woodside Energy Ltd (Woodside), embracing this technology is an opportunity to save time, drive efficiency and reduce costs. In 2015, Woodside collaborated with IBM and deployed a cognitive computing system (IBM’s Watson) into its business. The system focuses on capturing the vast proprietary database of knowledge on Woodside’s major capital projects. Today, the Watson proof-of-concept has been successfully deployed by the science function into the business and is now under the care of the projects function. Moreover, it is undergoing continuous advances through further machine learning, additional ingestion of documentation and features linking the cognitive computer system with existing subject matter experts. Given the success of the first pilot program, Woodside is continuing to rapidly leverage cognitive technologies in other areas of the business. In mid-2016, Woodside deployed Watson for drilling events using IBM’s Watson Explorer – Advanced Addition. This program identifies and classifies a wide variety of geological drilling events allowing Woodside’s geoscience team to provide more timely and accurate assessment of potential risks for well design. Woodside continues to develop several other business solutions using these platforms in areas as diverse as continuous improvement, business management, maintenance campaigns, legal advice, general management and robotics. This presentation shares Woodside’s lessons and insights derived from its journey across multiple forms of cognitive technology and provides insights as to the state-of-the-art and adaptation of these systems to achieve specific goals.",,2017,10.1071/AJ16142,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
def9f5e47e47e30a1fb73a4625035d3d951c1d90,https://www.semanticscholar.org/paper/def9f5e47e47e30a1fb73a4625035d3d951c1d90,Deep Learning-Based Human Emotion Detection Framework Using Facial Expressions,"Automatic recognition of facial expression is an emerging study in the recognition of emotions. Emotion plays a significant role in understanding people and is usually related to sound decisions, behaviors, human activities, and intellect. The scientific community needs accurate and deployable technologies to understand human beings’ emotional states to establish practical and emotional interactions between human beings and machines. In the paper, a deep learning-based human emotion detection framework (DL-HEDF) has been proposed to evaluate the probability of digital representation, identification, and estimation of feelings. The proposed DL-HEDF analyzes the impact of emotional models on multimodal identification. The paper introduces emerging works that use existing methods like convolutional neural networks (CNN) for human emotion identification based on language, sound, image, video, and physiological signals. The proposed emphasis on the province study illustrates the shape and display of sample size emotional stimulation. While the findings obtained are not a province, the evidence collected indicates that deep learning could be sufficient to classify face emotion. Deep learning can enhance interaction with people because it allows computers to acquire perception by learning characteristics. And by perception, robots can offer better responses, enhancing the user experience dramatically. Six basic emotional levels have been successfully classified. The suggested way of recognizing emotions has then proven effective. The output results are obtained as an analysis of the ratio of the facial expression of 87.16%, accuracy evaluation ratio being 88.7%, improving facial recognition ratio is 84.5%, and the expression intensity ratio is 82.2%. The emotional simulation ratio is 93.0%.",Journal of Interconnection Networks,2022,10.1142/s0219265921410188,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
040eb316cec08b36ae0b57fede86043ee0526686,https://www.semanticscholar.org/paper/040eb316cec08b36ae0b57fede86043ee0526686,Learning Reliable and Scalable Representations Using Multimodal Multitask Deep Learning,"Those were the words from one of the pioneers of AI when asked to comment on the progress of robotics in the twentieth century. This shows the high expectations and unforeseen challenges that we are faced with for deploying robots in complex real-world environments. One of the primary impediments has been the robustness of scene understanding models as it is a prerequisite for any action execution or planning. The tremendous progress made in machine learning in the last decade has enabled us to learn representations from raw sensor data, rather than relying on hand-engineered features. However, these models still perform inconsistently, especially in challenging weather conditions. The current dominant paradigms rely on camera images or depth data. However, alternate modalities such as infrared [1] and sound [2] need to be exploited for learning the most comprehensive information about the scene that will enable us to reduce perceptual ambiguity in challenging conditions. State-of-art deep learning models rely on thousands to millions of annotated training data and acquiring this data is an arduous task, if not impossible for every foreseeable scenario and for every task. Moreover, learning task-specific models on task-specific datasets, limits the overall learning ability of the robot as most models are trained in a supervised fashion and independently, therefore they have no ability to share cross-domain information and exploit training signals from complementary tasks. In order to address this limitation, our models should be able to learn representations across different modalities as well as reuse and share the learned representations across different tasks. My work enables models to effectively learn fused representations from multiple modalities and across tasks, exploiting complementary features and cross-modal interdependencies. Despite that fact that we are still far away from creating robots with human-level intelligence, equipping them with these basic capabilities will enable robots to learn new tasks from limited amount of data by leveraging transfer learning which facilitates self-supervised model adaptation. Advancing self-supervised learning techniques to autonomous learning is a strong starting point that will enable robots to continuously learn from what it experiences and perceives in the real-world. Modality 1 Modality 2 Unimodal Seg. Multimodal Seg.",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a5793116b8d0c9749afa91df50b10de5487880e0,https://www.semanticscholar.org/paper/a5793116b8d0c9749afa91df50b10de5487880e0,Spacecraft Informatics,"Spacecraft informatics is one of the most exciting and contemporary research topics in recent years. Many countries are deploying related technologies such as AI, robotics, machine learning, etc., in the deep-space explorations. Moreover, considering the high-complexity, high cost and high risk involved in spacecraft, advanced technologies in information modelling, simulation, optimisation and decision support methods are required to improve the effectiveness, efficiencies, reliabilities and safety of the space operations (Du et al. 2017; Rui et al. 2014). The emerging informatics approach offers the benefit to the area of spacecraft regarding in-orbit spacecraft, satellites, space-stations of any types in deep-space exploration missions from ground control, user payload, space weather and conditions, remote sensing and telemetry, and many more spaceflight missions and activities of designing, forecasting, planning and control. To contribute the present and future space exploration and spacecraft development, in this special issue, we have collected excellent papers of research in spacecraft informatics. Each paper underwent a double-blind peer review by independent, anonymous expert referees. After the reviewing processes, eight highquality papers were accepted and are published in this issue. The first paper is ‘Optimisation problems and resolution methods in satellite scheduling and spacecraft operation: a survey’ by Xhafa and Ip (2019). This paper aims to study the state of the art in the satellite scheduling regarding the spacecraft design, operation and satellite deployment system. With heuristics methods, the constraint features in satellite mission planning, including window accessibility and visibility requirements can be addressed for producing smalland low-cost satellites. The second paper, entitled ‘Moon image segmentation with a new mixture histogram model’ by Hsu et al. (2019) is related to an application of image processing technology in the spacecraft. This paper aims to develop a histogram mixture model with genetic algorithm for improving the effectiveness in segmenting the moon surface image. Instead of the manual parameters measurement, the parameters can be obtained by a genetic algorithm. The results show that the proposed algorithm improved the drawbacks of previous non-parametric methods for moon image segmentation. In the papers, entitled ‘Blockchain adoption for information sharing: risk decisionmaking in spacecraft supply chain’ by Zheng et al. (2019) and ‘A framework for rocket and satellite launch information management systems based on blockchain technology’ by Li, Wang, and Zhang (2019), they employed blockchain technology for information management and sharing in the spacecraft supply chain. The use of blockchain technology allows the stakeholders in the spacecraft to (i) reduce transaction cost and risks, and (ii) improve the reliability and traceability of the spacecraft information to enhance the overall effectiveness and efficiency of the supply chain. The paper ‘Health condition estimation of spacecraft key components using belief rule base’ by Tang et al. (2019) developed a semi-quantitative method to examine the ENTERPRISE INFORMATION SYSTEMS 2021, VOL. 15, NO. 8, 1019–1021 https://doi.org/10.1080/17517575.2021.1886331",Enterp. Inf. Syst.,2021,10.1080/17517575.2021.1886331,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b9c8313856b90f457a4f71cef684f1831f792017,https://www.semanticscholar.org/paper/b9c8313856b90f457a4f71cef684f1831f792017,Composable Energy Modeling for ML-Driven Drone Applications,"We are now witnessing the extensive deployment of drones in a diverse set of applications with Machine Learning (ML) constituting a key enabler empowering the uptake of drone technology. With the advancements of robotics and edge computing, on-board ML is on the uprise. However, testing ML solutions for drones before release to production is a daunting task for ML practitioners. This usually involves the testing on a robotics emulator to collect various key performance indicators ranging from algorithm correctness to resource utilization. Thus, to thoroughly evaluate performance, a true understanding of the ML algorithm impact on the drones most scarce resource is required. Without a doubt, this is the drones battery, which entails continuously monitoring energy consumption. In this paper we introduce HornEt, a modular framework enabling the customization and composition of various monitorable components to produce realistic energy models that can be used during the testing of ML-driven drone applications. To show the wide applicability of our framework, we introduce a proof-of-concept use-case illustrating the energy profiling of a drone application at different levels of granularity.",2021 IEEE International Conference on Cloud Engineering (IC2E),2021,10.1109/ic2e52221.2021.00039,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cffb0a0ad3738aa06511a9ba484d9911ac4793a6,https://www.semanticscholar.org/paper/cffb0a0ad3738aa06511a9ba484d9911ac4793a6,Digitization of Drill Bit Inspections; User-Centered Design Methods to Automate Robotic Inspections,"
 One unique facet of digital technology is the merging of separate technologies for new workflows and products. Like other industries, energy is also doing this. This project will automate the bit inspection process and this system will reduce labor costs, increase product quality, and improve bit performance. The innovation center is working on various aspects of the project, which aims to join automation technologies with robotic capabilities.
 Industrial robots are used extensively in traditional high-volume manufacturing applications. The high-mix, low-volume nature of oil and gas manufacturing operations has impeded deployment of automation solutions. Recent advances in sensors, computers, and machine learning now enable integrating robotics and automation technologies into these flexible manufacturing workflows. Driven by digital transformation, an automated inspection system for polycrystalline diamond compact (PDC) drill bits has been developed. The system uses high-resolution robotic 3D scanning, 2D imaging, and artificial intelligence to improve inspection efficiency and product quality. In our user-experience- (UX-) focused approach, we streamlined the user interface (UI) research methods to develop the robotic inspection UI and successfully tested the design with end users. This paper introduces the inspection system and improved workflows for the PDC bits, illustrates the innovative UX/UI development process, and targeted evaluation with the end users, which is crucial before deploying the system in production. We also concluded with some recommended improvements to guide future work.","Day 2 Wed, September 22, 2021",2021,10.2118/206261-ms,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1da1f40e379c4dd4303f0b769264191339afafea,https://www.semanticscholar.org/paper/1da1f40e379c4dd4303f0b769264191339afafea,"Building Efficient, Reliable, and Ethical Autonomous Systems","As autonomous systems rapidly grow in adaptability, effectiveness, and sophistication, their deployment has been accelerating in complex real-world domains that range from elder care robotics and autonomous driving to smart city management and military technology. However, while our ability to build efficient and reliable autonomous systems that integrate into our daily lives has expanded over the years, it has inevitably outstripped our ability to build ethical autonomous systems. Therefore, the goal of my research is to build autonomous systems that operate in natural, partially observable, stochastic domains for long durations in not only an efficient and reliable but also ethical way. Given the goal of my research, I have developed a range of approaches based on MDPs, POMDPs, and Dec-POMDPs along with their solution methods by using dynamic programming, mathematical programming, reinforcement learning, machine learning, deep learning, abstractions, heuristic search, and probabilistic graphical models. As a demonstration of my research, I have applied these approaches to autonomous vehicles (route navigation, obstacle handling, lane merging, and intersection negotiation), planetary exploration rovers, earth observation satellites, and standard general-purpose mobile robots. My research has fortunately led to a distinguished paper award (AAAI), an NSF Graduate Research Fellowship, and top publications (AAAI/IJCAI/ECAI/ICRA/IROS/AAMAS/AIES/SoCS). Moreover, I have had the opportunity to mentor students, write a grant proposal, and collaborate with industry. First, I have been a mentor for 6 BS, MS, and PhD students who have all had the experience as an author on multiple papers for top conferences. In fact, most recently, I am excited to say that one of my BS students will start a PhD in artificial intelligence at Brown University in the fall. Next, I was a main author on an NSF grant proposal on adaptive metareasoning for bounded rational agents that was awarded over $400,000. Finally, I am a key contributor to an industry collaboration with Nissan Research Center that has led to top publications and patents for over 3 years.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2a1208fd6b4a7a314c90c2b4bf103e5993134e52,https://www.semanticscholar.org/paper/2a1208fd6b4a7a314c90c2b4bf103e5993134e52,Connecting Health Immersion of Digital into eHealth,,Introduction to Nursing Informatics,2021,10.1007/978-3-030-58740-6_2,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e9a750ff919abc5625f57ecfb52bd804c9320ef9,https://www.semanticscholar.org/paper/e9a750ff919abc5625f57ecfb52bd804c9320ef9,A Survey: Robot Grasping,"The ﬁeld of autonomous robotics has made signiﬁcant progress with the advent of learning methods that have been successfully applied in robotics and have achieved tremendous accuracy. Today, we can observe the successful application of classical machine learning, computer vision, and reinforcement learning in various robotic tasks like path planning, perception, locomotion, grasping, manipulation etc. But the big question remains, ""Is robotics ready for the real world?"" While it is true that we have now successfully deployed some robots in the real world, with some even interacting and collaborating with humans, many tasks remain difﬁcult for robots to accomplish. In this survey paper, we focus on robot grasping, which is a signiﬁcant challenge for robots and hinders their successful deployment in the real world. Our paper aims to review, categorize, and describe research in robotics focusing on robot grasping, the role of robot grippers and learning methods explored towards achieving intelligent control of robots when executing a grasping task.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
574cd007a95f7fc8f88db63e46b16ca2896d503c,https://www.semanticscholar.org/paper/574cd007a95f7fc8f88db63e46b16ca2896d503c,Artificial Intelligence in News Media: Current Perceptions and Future Outlook,"In recent years, news media have been hugely disrupted by the potential of technological-driven approaches in the creation, production, and distribution of news products and services. Artificial intelligence (AI) has emerged from the realm of science fiction and has become a very real tool that can aid society in addressing many issues, including the challenges faced by the news industry. The ubiquity of computing has become apparent and has shown the different approaches that can be achieved using AI. We analyzed the news industry AI adoption based on the seven subfields emanated from AI: (i) machine learning; (ii) computer vision (CV); (iii) speech recognition; (iv) natural language processing (NLP); (v) planning, scheduling, and optimization; (vi) expert systems; and (vii) robotics. Our findings suggest that three subfields are being more developed in the news media: machine learning, planning, scheduling &amp; optimization, and computer vision. Other areas are still not fully deployed in the journalistic field. Most of the AI news projects rely on funds from tech companies, such as Google. This limits the potential of AI in the news industry to a small number of players. We conclude by providing examples of how these subfields are being developed in journalism and present an agenda for future research.",Journalism and Media,2021,10.20944/preprints202110.0020.v1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1275f4e4678e6cba3ef71d6139d357ab7ea0297a,https://www.semanticscholar.org/paper/1275f4e4678e6cba3ef71d6139d357ab7ea0297a,Triple Detector based on Feature Pyramid Network for License Plate Detection and Recognition System in Unusual Conditions,"Vehicle license plate detection and recognition is one application of computer vision which was widely deployed in traffic monitoring, road toll, and parking lot system. Its benefits are reducing service labor and increase processing accuracy. There are many studies in this area such as robotics and machine learning. With the strong development of neural networks and deep learning, application deployment becomes easier and more accurate. This paper focuses on the development of a license plate detector that supports the license plate detection and recognition system in unusual conditions under the influence of light, weather, and camera placement. The license plate detector is built based on the backbone, Feature Pyramid Network (FPN), and triple detector. The network was trained and tested on the CCPD (Chinese City Parking Dataset) dataset and achieved 96.1% of AP (Average Precision) in total, the network can outperform the state-of-the-art detection network in this field. On the other hand, the license plate detection network combined with the LPRNet network (License Plate Recognition via Deep Neural Networks) embedded by the Spatial Transformer to recognize the numbers and characters in the license plate with 98.8% of AP on CCPD-Base subset. As a result, the number plate detector exhibits superior capabilities compared to previous methodologies under unusual conditions, thereby enhancing the overall processing visibility of the system.",2021 IEEE 30th International Symposium on Industrial Electronics (ISIE),2021,10.1109/isie45552.2021.9576487,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
11c4cac8e7a373beace359eed791eb67efd98b7f,https://www.semanticscholar.org/paper/11c4cac8e7a373beace359eed791eb67efd98b7f,Compressed MobileNet V3: An efficient CNN for resource constrained platforms,"Computer Vision is a mathematical
tool formulated to extend human vision to machines. This tool can perform
various tasks such as object classification, object tracking, motion
estimation, and image segmentation. These tasks find their use in many applications,
namely robotics, self-driving cars, augmented reality, and mobile applications.
However, opposed to the traditional technique of incorporating handcrafted
features to understand images, convolution neural networks are being used to
perform the same function. Computer vision applications widely use CNNs due to
their stellar performance in interpreting images. Over the years, there have
been numerous advancements in machine learning, particularly to CNNs. However,
the need to improve their accuracy, model size and complexity increased, making
their deployment in restricted environments a challenge. Many researchers
proposed techniques to reduce the size of CNN while still retaining its
accuracy. Few of these include network quantization, pruning, low rank, and
sparse decomposition and knowledge distillation. Some methods developed
efficient models from scratch. This thesis achieves a similar goal using design
space exploration techniques on the latest variant of MobileNets, MobileNet V3.
Using Depthwise Pointwise Depthwise (DPD) blocks, escalation in the number of
expansion filters in some layers and mish activation function MobileNet V3 is
reduced to 84.96% in size and made 0.2% more accurate. Furthermore, it is
deployed in NXP i.MX RT1060 for image classification on CIFAR-10 dataset.",,2021,10.25394/PGS.14442710.V1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c6e56e450c2a081c85f7c31896c46ec03cb0ed2b,https://www.semanticscholar.org/paper/c6e56e450c2a081c85f7c31896c46ec03cb0ed2b,Scalable sim-to-real transfer of soft robot designs,"The manual design of soft robots and their controllers is notoriously challenging, but it could be augmented—or, in some cases, entirely replaced—by automated design tools. Machine learning algorithms can automatically propose, test, and refine designs in simulation, and the most promising ones can then be manufactured in reality (sim2real). However, it is currently not known how to guarantee that behavior generated in simulation can be preserved when deployed in reality. Although many previous studies have devised training protocols that facilitate sim2real transfer of control polices, little to no work has investigated the simulation-reality gap as a function of morphology. This is due in part to an overall lack of tools capable of systematically designing and rapidly manufacturing robots. Here we introduce a low cost, open source, and modular soft robot design and construction kit, and use it to simulate, fabricate, and measure the simulation-reality gap of minimally complex yet soft, locomoting machines. We prove the scalability of this approach by transferring an order of magnitude more robot designs from simulation to reality than any other method. The kit and its instructions can be found here: github.com/skriegman/sim2real4designs",2020 3rd IEEE International Conference on Soft Robotics (RoboSoft),2019,10.1109/RoboSoft48309.2020.9116004,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e1a5ff307028e48b4a38f751135cdb5e5c4952cd,https://www.semanticscholar.org/paper/e1a5ff307028e48b4a38f751135cdb5e5c4952cd,Coffee With a Hint of Data: Towards Using Data-Driven Approaches in Personalised Long-Term Interactions,"While earlier research in human-robot interaction pre-dominantly uses rule-based architectures for natural language interaction, these approaches are not flexible enough for long-term interactions in the real world due to the large variation in user utterances. In contrast, data-driven approaches map the user input to the agent output directly, hence, provide more flexibility with these variations without requiring any set of rules. However, data-driven approaches are generally applied to single dialogue exchanges with a user and do not build up a memory over long-term conversation with different users, whereas long-term interactions require remembering users and their preferences incrementally and continuously and recalling previous interactions with users to adapt and personalise the interactions, known as the lifelong learning problem. In addition, it is desirable to learn user preferences from a few samples of interactions (i.e., few-shot learning). These are known to be challenging problems in machine learning, while they are trivial for rule-based approaches, creating a trade-off between flexibility and robustness. Correspondingly, in this work, we present the text-based Barista Datasets generated to evaluate the potential of data-driven approaches in generic and personalised long-term human-robot interactions with simulated real-world problems, such as recognition errors, incorrect recalls and changes to the user preferences. Based on these datasets, we explore the performance and the underlying inaccuracies of the state-of-the-art data-driven dialogue models that are strong baselines in other domains of personalisation in single interactions, namely Supervised Embeddings, Sequence-to-Sequence, End-to-End Memory Network, Key-Value Memory Network, and Generative Profile Memory Network. The experiments show that while data-driven approaches are suitable for generic task-oriented dialogue and real-time interactions, no model performs sufficiently well to be deployed in personalised long-term interactions in the real world, because of their inability to learn and use new identities, and their poor performance in recalling user-related data.",Frontiers in Robotics and AI,2021,10.3389/frobt.2021.676814,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f42251abafb145714a9f46eee09341c0855a6f7f,https://www.semanticscholar.org/paper/f42251abafb145714a9f46eee09341c0855a6f7f,Coinbot: Intelligent Robotic Coin Bag Manipulation Using Artificial Brain,"Given the laborious difficulty of moving heavy bags of physical currency in the cash center of the bank, there is a large demand for training and deploying safe autonomous systems capable of conducting such tasks in a collaborative workspace. In this paper, we apply deep reinforcement learning and machine learning techniques to the task of controlling a collaborative robot to automate the unloading of coin bags from a trolley. To accomplish the task-specific process of gripping coin bags where the center of the mass changes during manipulation, a special gripper was designed in physical hardware. Leveraging a depth camera and deep learning, a bag detection and pose estimation has been done for choosing the optimal point of grasping. An intelligent approach based on deep reinforcement learning has been introduced to propose the best configuration of the robot end-effector to maximize successful grasping. A boosted motion planning is utilized to speed up the robot operation. Real-world trials with the proposed pipeline have demonstrated success rates over 96% in a real-world setting.","2021 7th International Conference on Automation, Robotics and Applications (ICARA)",2021,10.1109/ICARA51699.2021.9376455,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5dcfeaad7ac2a8b484aced2d14c557d91340deda,https://www.semanticscholar.org/paper/5dcfeaad7ac2a8b484aced2d14c557d91340deda,SQRP: Sensing Quality-aware Robot Programming System for Non-expert Programmers,"Robot programming typically makes use of a set of mechanical skills that is acquired by machine learning. Because there is in general no guarantee that machine learning produces robot programs that are free of surprising behavior, the safe execution of a robot program must utilize monitoring modules that take sensor data as inputs in real time to ensure the correctness of the skill execution. Owing to the fact that sensors and monitoring algorithms are usually subject to physical restrictions and that effective robot programming is sensitive to the selection of skill parameters, these considerations may lead to different sensor input qualities such as the view coverage of a vision system that determines whether a skill can be successfully deployed in performing a task. Choosing improper skill parameters may cause the monitoring modules to delay or miss the detection of important events such as a mechanical failure. These failures may reduce the throughput in robotic manufacturing and could even cause a destructive system crash. To address above issues, we propose a sensing quality-aware robot programming system that automatically computes the sensing qualities as a function of the robot’s environment and uses the information to guide non-expert users to select proper skill parameters in the programming phase. We demonstrate our system framework on a 6DOF robot arm for an object pick-up task.",2021 IEEE International Conference on Robotics and Automation (ICRA),2021,10.1109/ICRA48506.2021.9561020,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d8ffef877579dffb1d3de19741e58774f65cf8df,https://www.semanticscholar.org/paper/d8ffef877579dffb1d3de19741e58774f65cf8df,"The Pransky interview: Harry Kloor, PhD, PhD – CEO and Co-Founder, Beyond Imagination Inc.; scientist; entrepreneur; inventor; filmmaker","
Purpose
The following article is a “Q&A interview” conducted by Joanne Pransky of Industrial Robot Journal as a method to impart the combined technological, business and personal experience of a prominent, robotic industry PhD-turned successful innovator and entrepreneur regarding turning his lifelong dream into an invention and commercialized product. This paper aims to discuss these issues.


Design/methodology/approach
Harry Kloor is a successful serial entrepreneur, scientist, technologist, educator, policy advisor, author and Hollywood filmmaker. He is the CEO and co-founder of Beyond Imagination, a company that has developed a suite of exponential technology solutions that deploys artificial intelligence (AI), AR, robotics, machine learning and human–computer interaction technology to enhance and revolutionize the world’s workforce. The company early in 2021 completed BEOMNI 1.0, the world’s first fully functional humanoid robotic system with an AI evolving brain, enabling remote work at a high level of fidelity to be done from around the globe. Kloor describes how he transformed his childhood dream into his brainchild and tangible reality.


Findings
Kloor was born a groundbreaker who did not take no for an answer. He was born partially crippled with his legs facing backwards. The doctors said that he would spend his life in braces and would never be able to run. His parents told him not to let those ideas limit him and by the age of seven he ran for the first time and went on to become a martial arts master. Kloor’s childhood dream was to create ways to leave his body and inhabit a robotic body so that he could physically be free from his limited mobility. Kloor built his first computer at the age of seven and invented his first product at the age of eight. Kloor's inspiration to study science came largely from science fiction and his 20,000-plus collection of comic books. Knowing the nature of exponential growth, he spent the next 40 years building the expertise, relationships, networks and experience in all areas of exponential technology. Kloor obtained a BA from Southern Oregon State College, an MEd from Southern Oregon University and two simultaneous PhDs, one in chemistry and one in physics, from Purdue University. Kloor co-founded the company Universal Consultants, where he served as chief science consultant, providing guidance to clients in the development of new technological products, patents and policy positions. Kloor was the founder of Stem CC Inc. – a stem cell company that was sold in 2018 to Celularity, one of the world’s most cutting edge clinical-stage cell therapeutics company. Kloor is also the founder and president of Jupiter 9 Productions and is a credited film writer, director and producer. Since his graduation from Purdue University, he has written for Star Trek: Voyager and was the story editor for Gene Roddenberry’s Earth: Final Conflict, a series he co-created/developed. Kloor helped create Taiwan’s animation industry, bringing Quantum Quest: A Cassini Space Odyssey, the first big animation film that starred major Hollywood stars, to Taiwan. Kloor also sits on the board of Brain Mapping and Therapeutics Society and serves as their Chief Scientific Advisor and Educational Outreach Coordinator.


Originality/value
Kloor is a “creative consultant and universal problem solver, with an emphasis in technology and education.” Kloor has worked with Dr Peter Diamandis since the first class of the International Space University in 1988. Kloor was one of the five founding team members of XPRIZE serving as its CSO until 2005 and was one of the founders of the Rocket Racing League. He was on the founding team of Singularity University and taught at Singularity’s first summer program. In 2016 he created the $10m Avatar XPRIZE, and in 2018 he co-created the Carbon Extraction XPRIZE which obtained the largest incentive prize in history, a $100m, funded by Elon Musk and the Musk Foundation. Kloor is the only person in world history to earn two PhDs simultaneously in two distinct academic disciplines. In recognition of this achievement, he was named ABC World News’ Person of the Week in August 1994. Kloor has received numerous awards, including The Golden Axon Award from the Society for Brain Mapping & Therapeutics. He has recently created the Kloor Cycle, a four-stage experiential autonomous learning process within Beomni’s “AI Brain,” adapted from Kolb’s Learning Cycles.
",Industrial Robot: the international journal of robotics research and application,2022,10.1108/ir-06-2022-0148,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1f2e024d9fc65a663ebf2203cdc1042371883bcc,https://www.semanticscholar.org/paper/1f2e024d9fc65a663ebf2203cdc1042371883bcc,Composing Complex and Hybrid AI Solutions,"Progress in several areas of computer science has been enabled by comfortable and efficient means of experimentation, clear interfaces, and interchangable components, for example using OpenCV for computer vision or ROS for robotics. We describe an extension of the Acumos system towards enabling the above features for general AI applications. Originally, Acumos was created for telecommunication purposes, mainly for creating linear pipelines of machine learning components. Our extensions include support for more generic components with gRPC/Protobuf interfaces, automatic orchestration of graphically assembled solutions including control loops, sub-component topologies, and event-based communication, and provisions for assembling solutions which contain user interfaces and shared storage areas. We provide examples of deployable solutions and their interfaces. The framework is deployed at http://aiexp.ai4europe.eu/ and its source code is managed as an open source Eclipse project.",ArXiv,2022,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
468bf4c83e5fc1b36e4003a4881b5c5d5931c96b,https://www.semanticscholar.org/paper/468bf4c83e5fc1b36e4003a4881b5c5d5931c96b,PoisonHD: Poison Attack on Brain-Inspired Hyperdimensional Computing,"While machine learning (ML) methods especially deep neural networks (DNNs) promise enormous societal and economic benefits, their deployments present daunting challenges due to intensive computational demands and high storage requirements. Brain-inspired hyperdimensional computing (HDC) has recently been introduced as an alternative computational model that mimics the “human brain” at the functionality level. HDC has already demonstrated promising accuracy and efficiency in multiple application domains including healthcare and robotics. However, the robustness and security aspects of HDC has not been systematically investigated and sufficiently examined. Poison attack is a commonly-seen attack on various ML models including DNNs. It injects noises to labels of training data to introduce classification error of ML models. This paper presents PoisonHD, an HDC-specific poison attack framework that maximizes its effectiveness in degrading the classification accuracy by leveraging the internal structural information of HDC models. By applying PoisonHD on three datasets, we show that PoisonHD can cause significantly greater accuracy drop on HDC model than a random label-flipping approach. We further develop a defense mechanism by designing an HDC-based data sanitization that can significantly recover the accuracy loss caused by poison attack. To the best of our knowledge, this is the first paper that studies the poison attack on HDC models.","2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)",2022,10.23919/DATE54114.2022.9774641,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
855f4d3a07e27a44b86f624fd32724d94b5f1a41,https://www.semanticscholar.org/paper/855f4d3a07e27a44b86f624fd32724d94b5f1a41,The Impact of Human–Robot Interfaces on the Learning of Visual Objects,"This paper studies the impact of interfaces, allowing nonexpert users to efficiently and intuitively teach a robot to recognize new visual objects. We present challenges that need to be addressed for real-world deployment of robots capable of learning new visual objects in interaction with everyday users. We argue that in addition to robust machine learning and computer vision methods, well-designed interfaces are crucial for learning efficiency. In particular, we argue that interfaces can be key in helping nonexpert users to collect good learning examples and, thus, improve the performance of the overall learning system. Then, we present four alternative human–robot interfaces: Three are based on the use of a mediating artifact (smartphone, wiimote, wiimote and laser), and one is based on natural human gestures (with a Wizard-of-Oz recognition system). These interfaces mainly vary in the kind of feedback provided to the user, allowing him to understand more or less easily what the robot is perceiving and, thus, guide his way of providing training examples differently. We then evaluate the impact of these interfaces, in terms of learning efficiency, usability, and user’s experience, through a real world and large-scale user study. In this experiment, we asked participants to teach a robot 12 different new visual objects in the context of a robotic game. This game happens in a home-like environment and was designed to motivate and engage users in an interaction where using the system was meaningful. We then discuss results that show significant differences among interfaces. In particular, we show that interfaces such as the smartphone interface allows nonexpert users to intuitively provide much better training examples to the robot, which is almost as good as expert users who are trained for this task and are aware of the different visual perception and machine learning issues. We also show that artifact-mediated teaching is significantly more efficient for robot learning, and equally good in terms of usability and user’s experience, than teaching thanks to a gesture-based human-like interaction.",IEEE Transactions on Robotics,2013,10.1109/TRO.2012.2228134,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d315b2db6a5223dc817a4fdfc9198dcc06ec2358,https://www.semanticscholar.org/paper/d315b2db6a5223dc817a4fdfc9198dcc06ec2358,An affective computational model for machine consciousness,"In the past, several models of consciousness have become popular and have led to the development of models for machine consciousness with varying degrees of success and challenges for simulation and implementations. Moreover, affective computing attributes that involve emotions, behavior and personality have not been the focus of models of consciousness as they lacked motivation for deployment in software applications and robots. The affective attributes are important factors for the future of machine consciousness with the rise of technologies that can assist humans. Personality and affection hence can give an additional flavor for the computational model of consciousness in humanoid robotics. Recent advances in areas of machine learning with a focus on deep learning can further help in developing aspects of machine consciousness in areas that can better replicate human sensory perceptions such as speech recognition and vision. With such advancements, one encounters further challenges in developing models that can synchronize different aspects of affective computing. In this paper, we review some existing models of consciousnesses and present an affective computational model that would enable the human touch and feel for robotic systems.",ArXiv,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e7c7f0dba3ab77ca5a99f8adbaa583ce7ca2be15,https://www.semanticscholar.org/paper/e7c7f0dba3ab77ca5a99f8adbaa583ce7ca2be15,Intelligent welding robot system based on deep learning,"An intelligent welding robot system based on deep learning is designed. Firstly, the binocular vision system needs to be fixed and calibrated, and correct the left and right cameras to make the polar lines of the left and right images collinear. Then, the left camera is used to take 1000 photos of arbitrary arranging fence. The position of the weld is marked by LabelImg software, and the training set and validation set are determined in a ratio of 3:1, and then used to train the convolutional neural network. After the loss function tends to converge, the training stops. The model optimizer converts the TensorFlow model into an IR model through the model optimizer in openvino, which is then deployed to the Raspberry Pi 3B+, which performs forward propagation through Intel’s NCS2 edge computing stick to identify the location of the weld in the image. The exact position of the weld is found by machine vision, and the parallax of all the welds is calculated by matching with the right image, and the depth of the weld is obtained by calculation. Finally, transfer the UR3 robot through Ethernet and control it for automatic welding.",2019 Chinese Automation Congress (CAC),2019,10.1109/CAC48633.2019.8997310,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e53a11346a7264cce3dc2d889e1c46ea18837535,https://www.semanticscholar.org/paper/e53a11346a7264cce3dc2d889e1c46ea18837535,Cross-Modal Learning Filters for RGB-Neuromorphic Wormhole Learning,"Robots that need to act in an uncertain, populated, and varied world need heterogeneous sensors to be able to perceive and act robustly. For example, self-driving cars currently on the road are equipped with dozens of sensors of several types (lidar, radar, sonar, cameras, . . . ). All of this existing and emerging complexity opens up many interesting questions regarding how to deal with multi-modal perception and learning. The recently developed technique of “wormhole learning” shows that even temporary access to a different sensor with complementary invariance characteristics can be used to enlarge the operating domain of an existing object detector without the use of additional training data. For example, an RGB object detector trained with daytime data can be updated to function at night time by using a “wormhole” jump through a different modality that is more illumination invariant, such as an IR camera. It turns out that having an additional sensor improves performance, even if you subsequently lose it. In this work we extend wormhole learning to allow it to cope with sensors that are radically different, such as RGB cameras and event-based neuromorphic sensors. Their profound differences imply that we need a more careful selection of which samples to transfer, thus we design “cross-modal learning filters”. We will walk in a relatively unexplored territory of multi-modal observability that is not usually considered in machine learning. We show that wormhole learning increases performance even though the intermediate neuromorphic modality is on average much worse at the task. These results suggest that multi-modal learning for perception is still an early field and there might be many opportunities to improve the perception performance by accessing a rich set of heterogeneous sensors (even if some are not actually deployed on the robot).",Robotics: Science and Systems,2019,10.15607/RSS.2019.XV.045,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3f3ec2d68dbacd84265986d00673c52a30c58f31,https://www.semanticscholar.org/paper/3f3ec2d68dbacd84265986d00673c52a30c58f31,Acquiring Human-Robot Interaction skills with Transfer Learning Techniques,"Robotics is a reasonably mature technology when robots are restricted to operating with well-known and well-engineered environments, e.g. in manufacturing robotics or domestic applications such as vacuum cleaning or lawn mowing. For more diverse tasks and open-ended environments, robotic behaviours are mainly hand-tuned: for most of the one million robots deployed today basically, the environment should be modeled and the robot's skills and strategies programmed by hand. This is a perfectly feasible approach as long as this hand-coding is inexpensive and reliable enough. This will be the case if the environment is well structured and stable and if the robot's tasks are restricted in scope and diversity, with only limited Human-Robot Interaction (HRI). However, if the robot has to face a diversity of tasks and/or a variety of environment, then the robot has to use more complex approaches based on machine learning and planning techniques in order to decide how to act in the environment. Recently, an increasing interest in the research community is how to enable robots to interact and communicate with humans, which is a major step towards integrating robots in our daily life. Within the large domain of HRI, this research area is called Social robotics. Communicating with humans is a complex process, involving several modalities such as speech, facial expressions, gaze, head movements, hand gestures, etc. Social robotics focuses on modeling how humans use these communication channels in order to smoothly interact with each other, and how to endow a robot with similar skills. Modeling such complex multimodal interactions has proven to be a challenge. Machine learning is invading this domain. Recent research performed on interactive data (i.e. challenging the problem of generating co-verbal gestures of one participant given the co-verbal gestures of the other and verbal activities of both participants) showed a great potential for machine learning in learning complex HRI behaviors. However, the behavioural models are task- and situation-specific: if anything changes in the learning problem (the input distribution or shape, the interaction scenario, etc), we need to learn everything again from scratch. This means that policies learnt in one environment might not work -- or work sub-optimally -- in other environments and conditions. A growing interest of research in machine learning is on how to transfer knowledge learned from one task (source task) to a new task (target task). The idea is to capture the similarities between the source and the target tasks, and exploit them in order to learn the target task faster. The shared knowledge can be implicit, like finding latent variables for the input distributions of both tasks. It can also be explicit, like representing robotic knowledge as skills. Our challenge is thus to develop methods to transfer the knowledge model the robot learns on one interactive situation with humans to new tasks/situations -~notably to new interactive tasks~-, in order to enable rapid learning and adaptation to these new tasks.",HRI,2017,10.1145/3029798.3034823,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8206694e424e03fde268759de2b28974e6337352,https://www.semanticscholar.org/paper/8206694e424e03fde268759de2b28974e6337352,Review Based on Different Deep Learning Architecture and their Application,"Deep neural networks (DNNs) are currently widely used for much artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI system. An up-to-date overview is provided on four deep learning architectures, namely, Artificial Neural Networks, Auto encoder, convolutional neural network, deep belief network, and restricted Boltzmann machine, Recurrent Neural Network. Different types of deep neural networks are surveyed and recent progress is summarized.",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
46312d517f4bae41d5bae0502977e4791121e21e,https://www.semanticscholar.org/paper/46312d517f4bae41d5bae0502977e4791121e21e,Decision Support System to Optimize Cloud Service Prioritization for Model Deployment,"The bio-inspired concept of deep learning has brought a revolution in artificial intelligence. It has challenged several areas including computer vision, signal processing, healthcare, transportation, security, robotics and machine translation. The core idea is to make learning algorithms efficient and convenient to use for solving daily life problems. This technology is still naive and facing multiple challenges like massive data availability, computation and infrastructural cost, resource dependency, efficient resource utilization, model production and platform procurement. Also it is found that most of the structured and unstructured data comes in the form of images, captured through different types of sensors. These images if utilized efficiently, can serve as an effective tool to solve numerous problems. To target above, a resource independent deep learning framework is proposed in this article. This work is an effort towards deploying deep neural network off-premises for medical image analysis to eliminate on-premises resource dependency and making efficient use of pay-as-per-demand paradigm offered by cloud services. This approach not only reduces the overall infrastructural cost but also enables a diverse range of need-based computational resource selection. The proposed work has shown promising results and considered as an effort to promote cloud-based resource independent machine intelligence.",2021 4th International Conference on Information and Computer Technologies (ICICT),2021,10.1109/ICICT52872.2021.00033,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
44cdb43bba6bb68b68dbb798a45daefec8132182,https://www.semanticscholar.org/paper/44cdb43bba6bb68b68dbb798a45daefec8132182,Can Robotic AI Systems Be Virtuous and Why Does This Matter?,,International journal of social robotics,2022,10.1007/s12369-022-00887-w,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a37ef52ff3837fd7f749265bb8b07fb1c903fb0f,https://www.semanticscholar.org/paper/a37ef52ff3837fd7f749265bb8b07fb1c903fb0f,"Project work on the model work-based training PUD-BJ: from idea to product: early deployment technique using learning models that show how the tires fitted into a simple toys, learning tools primary and secondary schools","The purpose of our project is to present invention as a teaching aid in teaching techniques and technology to show the operation of the tire models, through experiential learning. Our work by model work-based training PUD-BJ – from concept to product is useful and dedicated to the general public in the context of early introduction of technology and vocational guidance, as a child in kindergarten, primary school pupils and future students at information days and technical days that take place in schools . The invention represents a simple way to know the model PUD-BJ – from idea to product, where you can learn about different models and techniques of machines with pneumatic operation and behavior of the basic physical and chemical and mechanical laws built with industrial elements. Such innovative offering designed model that we know better understanding of the subject matter with the scope of the teaching models of mechanics and mechatronics courses in engineering and technology and robotics modern times, the entire vertical of education – from kindergarten to university technical studies. Designed invention has great didactic value, as confirmed by a study group of teachers presented for illustrative way of effective presentation and interpretation and performance in the learning environment of basic physical, chemical, electrical and mechanical laws, on which work has already too many the toy makers and devices, to be used along the entire vertical of education – from kindergarten to university teaching both as a technical study. The project approach work models PUD-BJ – from idea to product, encouraging creative ideas in the early deployment of the technology, especially the mental development of children already in kindergarten, for pupils in elementary school and in high school students in choosing their career.",,2019,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
28689043f11f4d982c4b4f1cfd5a2ff1487ba3f8,https://www.semanticscholar.org/paper/28689043f11f4d982c4b4f1cfd5a2ff1487ba3f8,Core microbiomes for sustainable agroecosystems,,Nature Plants,2018,10.1038/s41477-018-0139-4,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
64dfb21fc22523f2fbb1128cc9bd32a904df5e92,https://www.semanticscholar.org/paper/64dfb21fc22523f2fbb1128cc9bd32a904df5e92,Accelerating the discovery of materials for clean energy in the era of smart automation,,Nature Reviews Materials,2018,10.1038/s41578-018-0005-z,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c21d84ac9c329819f4ddf3ffc98aa23cf6441082,https://www.semanticscholar.org/paper/c21d84ac9c329819f4ddf3ffc98aa23cf6441082,Learning How to Behave,,Handbuch Maschinenethik,2018,10.1007/978-3-658-17484-2_17-1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4a2095a7b63390db1711de0ae91134a986b3b8c2,https://www.semanticscholar.org/paper/4a2095a7b63390db1711de0ae91134a986b3b8c2,MAS collaboration and machine learning method for robot soccer,"This paper analyses the distributed artificial intelligent system and designs an offline learning method. We also develop a multi-agent collaboration protocol, which is deployed in the robot soccer environment. We attended the China Robot Competition 2002 and proved that the method is efficient.",Proceedings. International Conference on Machine Learning and Cybernetics,2002,10.1109/ICMLC.2002.1174540,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2efecdfa2c6d7459b2b941d714e24bd3e64e6bc5,https://www.semanticscholar.org/paper/2efecdfa2c6d7459b2b941d714e24bd3e64e6bc5,Automated Experimentation Powers Data Science in Chemistry.,"ConspectusData science has revolutionized chemical research and continues to break down barriers with new interdisciplinary studies. The introduction of computational models and machine learning (ML) algorithms in combination with automation and traditional experimental techniques has enabled scientific advancement across nearly every discipline of chemistry, from materials discovery, to process optimization, to synthesis planning. However, predictive tools powered by data science are only as good as their data sets and, currently, many of the data sets used to train models suffer from several limitations, including being sparse, limited in scope and requiring human curation. Likewise, computational data faces limitations in terms of accurate modeling of nonideal systems and can suffer from low translation fidelity from simulation to real conditions. The lack of diverse data and the need to be able to test it experimentally reduces both the accuracy and scope of the predictive models derived from data science. This Account contextualizes the need for more complex and diverse experimental data and highlights how the seamless integration of robotics, machine learning, and data-rich monitoring techniques can be used to access it with minimal human labor.We propose three broad categories of data in chemistry: data on fundamental properties, data on reaction outcomes, and data on reaction mechanics. We highlight flexible, automated platforms that can be deployed to acquire and leverage these data. The first platform combines solid- and liquid-dosing modules with computer vision to automate solubility screening, thereby gathering fundamental data that are necessary for almost every experimental design. Using computer vision offers the additional benefit of creating a visual record, which can be referenced and used to further interrogate and gain insight on the data collected. The second platform iteratively tests reaction variables proposed by a ML algorithm in a closed-loop fashion. Experimental data related to reaction outcomes are fed back into the algorithm to drive the discovery and optimization of new materials and chemical processes. The third platform uses automated process analytical technology to gather real-time data related to reaction kinetics. This system allows the researcher to directly interrogate the reaction mechanisms in granular detail to determine exactly how and why a reaction proceeds, thereby enabling reaction optimization and deployment.",Accounts of chemical research,2021,10.1021/acs.accounts.0c00736,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
320e3a66dbc51a1331f65b5ac42b937a39d5e051,https://www.semanticscholar.org/paper/320e3a66dbc51a1331f65b5ac42b937a39d5e051,Physical Activity Recognition With Statistical-Deep Fusion Model Using Multiple Sensory Data for Smart Health,"Nowadays, enhancing the living standard with smart healthcare via the Internet of Things is one of the most critical goals of smart cities, in which artificial intelligence plays as the core technology. Many smart services, deployed according to wearable sensor-based physical activity recognition, have been able to early detect unhealthy daily behaviors and further medical risks. Numerous approaches have studied shallow handcrafted features coupled with traditional machine learning (ML) techniques, which find it difficult to model real-world activities. In this work, by revealing deep features from deep convolutional neural networks (DCNNs) in fusion with conventional handcrafted features, we learn an intermediate fusion framework of human activity recognition (HAR). According to transforming the raw signal value to pixel intensity value, segmentation data acquired from a multisensor system are encoded to an activity image for deep model learning. Formulated by several novel residual triple convolutional blocks, the proposed DCNN allows extracting multiscale spatiotemporal signal-level and sensor-level correlations simultaneously from the activity image. In the fusion model, the hybrid feature merged from the handcrafted and deep features is learned by a multiclass support vector machine (SVM) classifier. Based on several experiments of performance evaluation, our fusion approach for activity recognition has achieved the accuracy over 96.0% on three public benchmark data sets, including Daily and Sport Activities, Daily Life Activities, and RealWorld. Furthermore, the method outperforms several state-of-the-art HAR approaches and demonstrates the superiority of the proposed intermediate fusion model in multisensor systems.",IEEE Internet of Things Journal,2021,10.1109/JIOT.2020.3013272,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2cb9bfe3ff61d006ceeb995c507fa6df96af86a7,https://www.semanticscholar.org/paper/2cb9bfe3ff61d006ceeb995c507fa6df96af86a7,Multi-Agent Systems for Search and Rescue Applications,,,2021,10.1007/S43154-021-00048-3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3bbda6495017542d8470ccee1db02180bd206e8c,https://www.semanticscholar.org/paper/3bbda6495017542d8470ccee1db02180bd206e8c,Enhancing Security Measures of AI Applications,"Artificial Intelligence also often referred to as machine learning is being labelled to as the future has been into light since more than a decade. Artificial Intelligence designated by the acronym AI has a vast scope of development and the developers have been working on with it constantly. AI is being associated with the existing objects in the world as well as with the ones that are about to arrive to improve them and make them more reliable. AI as it states in its name is intelligence, intelligence shown by the machines to work similar to humans and work on achieving the goals they are being provided with. Another application of AI could be to provide defenses against the present cyber threats, vehicle overrides etc. Also, AI might be intelligence but, in the end, it’s still a bunch of codes, hence it is prone to be corrupted or misused by the world. To prevent the misuse of the technologies, it is necessary to deploy them with a sustainable defensive system as well. Obviously, there is going to be a default defense system but it is prone to be corrupted by the hackers or malfunctioning of the intelligence in certain scenarios which can result disastrous especially in case of Robotics. A proposal referred to as the “Guard Masking” has been offered in the following paper, to provide an alternative for securing Artificial Intelligence.","2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",2020,10.1109/ICRITO48877.2020.9197790,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
db763e5e61c75b6242b5786ac592eb9109af7a91,https://www.semanticscholar.org/paper/db763e5e61c75b6242b5786ac592eb9109af7a91,Balancing Communication and Computation in Distributed Optimization,"Methods for distributed optimization have received significant attention in recent years owing to their wide applicability in various domains including machine learning, robotics, and sensor networks. A distributed optimization method typically consists of two key components: communication and computation. More specifically, at every iteration (or every several iterations) of a distributed algorithm, each node in the network requires some form of information exchange with its neighboring nodes (communication) and the computation step related to a (sub)-gradient (computation). The standard way of judging an algorithm via only the number of iterations overlooks the complexity associated with each iteration. Moreover, various applications deploying distributed methods may prefer a different composition of communication and computation. Motivated by this discrepancy, in this paper, we propose an adaptive cost framework that adjusts the cost measure depending on the features of various applications. We present a flexible algorithmic framework, where communication and computation steps are explicitly decomposed to enable algorithm customization for various applications. We apply this framework to the well-known distributed gradient descent (DGD) method, and show that the resulting customized algorithms, which we call DGD<inline-formula><tex-math notation=""LaTeX"">$^t$</tex-math></inline-formula>, NEAR-DGD<inline-formula><tex-math notation=""LaTeX"">$^t$</tex-math></inline-formula>, and NEAR-DGD<inline-formula><tex-math notation=""LaTeX"">$^+$</tex-math></inline-formula>, compare favorably to their base algorithms, both theoretically and empirically. The proposed NEAR-DGD<inline-formula><tex-math notation=""LaTeX"">$^+$</tex-math></inline-formula> algorithm is an exact first-order method where the communication and computation steps are nested, and when the number of communication steps is adaptively increased, the method converges to the optimal solution. We test the performance and illustrate the flexibility of the methods, as well as practical variants, on quadratic functions and classification problems that arise in machine learning, in terms of iterations, gradient evaluations, communications, and the proposed cost framework.",IEEE Transactions on Automatic Control,2017,10.1109/TAC.2018.2880407,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2a1774a1b3b0b78cbb9b298030ac45a4d66125ef,https://www.semanticscholar.org/paper/2a1774a1b3b0b78cbb9b298030ac45a4d66125ef,Towards a whole body brain-machine interface system for decoding expressive movement intent Challenges and Opportunities,"The restoration and rehabilitation of human bipedal locomotion represent major goals for brain machine interfaces (BMIs), i.e., devices that translate neural activity into motor commands to control wearable robots to enable locomotive and non-locomotive tasks by individuals with gait disabilities. Prior BMI efforts based on scalp electroencephalography (EEG) have revealed that fluctuations in the amplitude of slow cortical potentials in the delta band contain information that can be used to infer motor intent, and more specifically, the kinematics of walking and non-locomotive tasks such as sitting and standing. However, little is known about the extent to which EEG can be used to discern the expressive qualities that influence such functional movements. Here, we discuss how novel experimental approaches integrated with machine learning techniques can deployed to decode expressive qualities of movement. Applications to artistic brain-computer interfaces (BCIs), movement aesthetics, and gait neuroprostheses endowed with expressive qualities are discussed.",2017 5th International Winter Conference on Brain-Computer Interface (BCI),2017,10.1109/IWW-BCI.2017.7858142,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
829ae84db1f604eef173a0eadfeef5925c0cb487,https://www.semanticscholar.org/paper/829ae84db1f604eef173a0eadfeef5925c0cb487,How to Train Your Robot: Project-Based AI and Ethics Education for Middle School Classrooms,"We developed the How to Train Your Robot curriculum to empower middle school students to become conscientious users and creators of Artificial Intelligence (AI). As AI becomes more embedded in our daily lives, all members of society should have the opportunity to become AI literate. Today, most deployed work in K-12 AI education takes place at strong STEM schools or during extracurricular clubs. But, to promote equity in the field of AI, we must also design curricula for classroom use at schools with limited resources. How to Train Your Robot leverages a low-cost ($40) robot, a block-based programming platform, novice-friendly model creation tools, and hands-on activities to introduce students to machine learning. During the summer of 2020, we trained in-service teachers, primarily from Title 1 public schools, to deliver a five-day, online version of the curriculum to their students. In this work, we describe how students' self-directed final projects demonstrate their understanding of technical and ethical AI concepts. Students successfully selected project ideas, taking the strengths and weaknesses of machine learning into account, and implemented an array of projects about everything from entertainment to science. We saw that students had the most difficulty designing mechanisms to respond to user feedback after deployment. We hope this work inspires future AI curricula that can be used in middle school classrooms.",SIGCSE,2021,10.1145/3408877.3439690,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
23838783bfb72a364f672084f2ccd178d13f630a,https://www.semanticscholar.org/paper/23838783bfb72a364f672084f2ccd178d13f630a,Assessing Wireless Sensing Potential With Large Intelligent Surfaces,"Sensing capability is one of the most highlighted new feature of future 6G wireless networks. This paper addresses the sensing potential of Large Intelligent Surfaces (LIS) in an exemplary Industry 4.0 scenario. Besides the attention received by LIS in terms of communication aspects, it can offer a high-resolution rendering of the propagation environment. This is because, in an indoor setting, it can be placed in proximity to the sensed phenomena, while the high resolution is offered by densely spaced tiny antennas deployed over a large area. By treating an LIS as a radio image of the environment relying on the received signal power, we develop techniques to sense the environment, by leveraging the tools of image processing and machine learning. Once a radio image is obtained, a Denoising Autoencoder (DAE) network can be used for constructing a super-resolution image leading to sensing advantages not available in traditional sensing systems. Also, we derive a statistical test based on the Generalized Likelihood Ratio (GLRT) as a benchmark for the machine learning solution. We test these methods for a scenario where we need to detect whether an industrial robot deviates from a predefined route. The results show that the LIS-based sensing offers high precision and has a high application potential in indoor industrial environments.",IEEE Open Journal of the Communications Society,2020,10.1109/OJCOMS.2021.3073467,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d8e75f3c6828639e6095eae5e23969008e957733,https://www.semanticscholar.org/paper/d8e75f3c6828639e6095eae5e23969008e957733,Reinforcement Learning Concepts Ministering Smart City Applications Using IoT,,,2020,10.1007/978-3-030-34328-6_2,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3c67114d32bcc530b15c308480440a3b2b84eb43,https://www.semanticscholar.org/paper/3c67114d32bcc530b15c308480440a3b2b84eb43,"Data Transfer via UAV Swarm Behaviours Rule Generation, Evolution and Learning","This paper presents an adaptive robotic swarm of Unmanned Aerial Vehicles (UAVs) enabling communications between separated non-swarm devices. The swarm nodes utilise machine learning and hy per-heuristic rule evolution to enable each swarm member to act appropriately for the given environment. The contribution of the mac hine learning is verified with an exploration of swarms with and without this module. The exploration finds that in challenging environments the learning greatly improves the swarm’s ability to complete the task. The swarm evolution process of this study is found to successfully create different data transfer methods depending on the separation of non-swarm devices and the communication range of the swarm members. This paper also explores the resilience of the swarm to agent loss, and the scalability of the swarm in a range of environment sizes. In regard to resilience, the swarm is capable of recovering from agent loss and is found to have improved evolution. In regard to scalability, the swarm is observed to have no upper limit to the number of agents deployed in an environment. However, the size of the environment is seen to be a limit for optimal swarm performance.",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
005f66c6330a70545a70518ad9d753d4a8db04cd,https://www.semanticscholar.org/paper/005f66c6330a70545a70518ad9d753d4a8db04cd,Skeleton-based Human Action Recognition,"With the continuous advance of the technology and the rather growing demands in fields such as the industrial, or the medical concerning the automatization of processes and the support of the professional workers, the use of Artificial Intelligence is vastly deployed. Professional workers have to co-exist and/or collaborate with robotic agents such as Automated Guided Vehicles and robotic arms, to enhance their effectiveness and productivity. Those robotic agents have machine learning algorithms embodied to them, for the proper support of the humans. This task is also the main goal of this thesis. A gesture recognition engine is developed, using the method of a Multilayer perceptron (MLP), for the recognition of the gestures of two different datasets with a similar structure. The results of this method are also compared to those, with the use of the methods of Hidden Markov Models (HMMs) and Long-Short TermMemory (LSTM), in order to the reach to the optimal classification architecture. Apart from the methods themselves, different ways for data handling were also tested. The aim is to examine whether in the specific case and with the used data, the resampling method was more useful in terms of training or testing an algorithm, compared to training or testing with the whole amount of data. This led to observations concerning the computational cost of some experiments and the robustness of the used algorithms. As such, the scientific contribution of this thesis relies on the implementation of a robust, accurate, as well as fast computationally gesture recognition engine.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9ea7cbaa26f83a2c7516a7b81288e6150e41c98c,https://www.semanticscholar.org/paper/9ea7cbaa26f83a2c7516a7b81288e6150e41c98c,Biometric Physiological Responses from Dairy Cows Measured by Visible Remote Sensing Are Good Predictors of Milk Productivity and Quality through Artificial Intelligence,"New and emerging technologies, especially those based on non-invasive video and thermal infrared cameras, can be readily tested on robotic milking facilities. In this research, implemented non-invasive computer vision methods to estimate cow’s heart rate, respiration rate, and abrupt movements captured using RGB cameras and machine learning modelling to predict eye temperature, milk production and quality are presented. RGB and infrared thermal videos (IRTV) were acquired from cows using a robotic milking facility. Results from 102 different cows with replicates (n = 150) showed that an artificial neural network (ANN) model using only inputs from RGB cameras presented high accuracy (R = 0.96) in predicting eye temperature (°C), using IRTV as ground truth, daily milk productivity (kg-milk-day−1), cow milk productivity (kg-milk-cow−1), milk fat (%) and milk protein (%) with no signs of overfitting. The ANN model developed was deployed using an independent 132 cow samples obtained on different days, which also rendered high accuracy and was similar to the model development (R = 0.93). This model can be easily applied using affordable RGB camera systems to obtain all the proposed targets, including eye temperature, which can also be used to model animal welfare and biotic/abiotic stress. Furthermore, these models can be readily deployed in conventional dairy farms.",Sensors,2021,10.3390/s21206844,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
eb7678504e59afa2df40bcf59c3a2421c8102f5a,https://www.semanticscholar.org/paper/eb7678504e59afa2df40bcf59c3a2421c8102f5a,Permanent Magnet-Based Localization for Growing Robots in Medical Applications,"Growing robots that achieve locomotion by extending from their tip, are inherently compliant and can safely navigate through constrained environments that prove challenging for traditional robots. However, the same compliance and tip-extension mechanism that enables this ability, also leads directly to challenges in their shape estimation and control. In this letter, we present a low-cost, wireless, permanent magnet-based method for localizing the tip of these robots. A permanent magnet is placed at the robot tip, and an array of magneto-inductive sensors is used to measure the change in magnetic field as the robot moves through its workspace. We develop an approach to localization that combines analytical and machine learning techniques and show that it outperforms existing methods. We also measure the position error over a 500 mm × 500 mm workspace with different magnet sizes to show that this approach can accommodate growing robots of different scales. Lastly, we show that our localization method is suitable for tracking the tip of a growing robot by deploying a 12 mm robot through different, constrained environments. Our method achieves position and orientation errors of 3.0 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 1.1 mm and 6.5 <inline-formula><tex-math notation=""LaTeX"">$\pm 5.4^\circ$</tex-math></inline-formula> in the planar case and 4.3 <inline-formula><tex-math notation=""LaTeX"">$\pm$</tex-math></inline-formula> 2.3 mm, 3.9 <inline-formula><tex-math notation=""LaTeX"">$\pm 3.0^\circ$</tex-math></inline-formula>, and 3.8 <inline-formula><tex-math notation=""LaTeX"">$\pm 3.5^\circ$</tex-math></inline-formula> in the 5-DOF setting.",IEEE Robotics and Automation Letters,2020,10.1109/LRA.2020.2972890,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
af26348693321847450d78f862f5ddd8d137af3e,https://www.semanticscholar.org/paper/af26348693321847450d78f862f5ddd8d137af3e,Special issue on Artificial Intelligence in Engineering Education,"Artificial intelligence (AI) can be defined as the intelligence exhibited by machines and computers in accomplishing desired tasks in a similar way to how normal human beings think and act. Hence AI is also termed machine intelligence. For a computational system to be artificially intelligent, the system should possess the ability to understand the surrounding environment, make proper assumptions, and based on the circumstances make judicious decisions that maximize the possibilities of accomplishing goals most of the time. These AI‐enabled devices are also called Intelligent Agents. These intelligent agents use some mapping functions also termed cognitive functions, which take these environmental parameters and contextual information as inputs along with the goal to be accomplished and manipulate the right means to accomplish the targeted goal. AI can also be considered inter‐ disciplinary as it involves several other disciplines such as Machine Learning, Computer Vision, Cognitive Science, Neural Networks, Data Mining, Natural Language Processing (NLP), robotics, and mathematics. All these disciplines are related, and thereby intelligent agents are trained to understand and adapt to the surrounding environment according to the context. The use of AI spans across several applications such as Human–Computer Interaction (HCI) based smart agent development, devising smart surveillance solutions using computer vision, creating robust and stable decision making systems that can understand, evaluate, manipulate, analyze, and predict several novel patterns by processing large volumes of application data, development of multilingual systems that uses NLP to understand the language features used across the context and aid decision making and so on. Also, since its inception as an academic discipline in the 1950s, AI has grown leaps and bounds as a discipline, and its applications have stretched across several domains such as Retail and Business solutions, Manufacturing and Logistics, Automobiles, Business Analytics and Market predictions, Healthcare, Security Systems, and Education. One of the key emerging areas where extensive efforts are spent towards developing smart applications and agents is the educational domain. Gone are the days where the educational system was completely driven by humans, and the growth of AI‐enabled intelligent agents has set the tone by replacing most human work with that of smart agents. Educational systems use AI‐based agents to study the behavior of students and suggest suitable courses for them. Smart agents are nowadays deployed in classrooms for complete classroom monitoring that includes tracking attendance, monitoring classroom activities, student and staff behavior monitoring, and so on. Similarly, smart agents are deployed to scan through the contents available online and suggest suitable content to students according to the course and also according to the different levels of understanding of student fraternity. Also, computer vision‐based smart agents are deployed to study the state of mind of students when they undergo different courses and provide insightful information about their likeness towards a subject or course. This agent‐based information serves as useful information in deciding the teaching methodology and also framing of course contents. Also, smart systems play a vital role in analyzing student results and providing insightful information about student performance. Thus, it is imperative that AI has become an indispensable force to reckon with in the future forward across the educational domain. However, the major drawback in these artificially intelligent systems is that they are not always accurate with decision making and at times predict otherwise. Also, training the AI‐based agent to understand the contextual paradigm and surrounding environment is a challenge. This special issue on “Artificial Intelligence In Education” is focused on drawing original studies related to the development and refinement of smart agents that can be applied across the educational domain.",Comput. Appl. Eng. Educ.,2021,10.1002/cae.22398,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2ec04a5603741df0d13fba547157b9c1d63f8027,https://www.semanticscholar.org/paper/2ec04a5603741df0d13fba547157b9c1d63f8027,Initial Development of the Hybrid Aerial Underwater Robotic System (HAUCS): Internet of Things (IoT) for Aquaculture Farms,"Aquaculture, especially fish farming, plays a vital role in ensuring food security in the United States and worldwide. However, for fish farming to be sustainable and economically viable, drastic improvements to the current labor-intensive and resource-inefficient operations are required. The hybrid aerial/underwater robotic system (HAUCS) aims to bring fundamental innovations to how pond-based farms operate. HAUCS is an end-to-end framework that consists of three principal subsystems: 1) a team of collaborative aero-amphibious robotic sensing platforms integrated with water quality sensors; 2) a land-based home station that can provide automated charging and sensor cleaning; and 3) a backend processing center that includes a machine-learning-based water quality prediction model and farm control center. HAUCS will be capable of collaborative monitoring and decision-making on farms of varying scales. The HAUCS platform, payload, and prediction model are discussed. The initial deployment of the HAUCS framework at a land-based aquaculture fish farm is presented.",IEEE Internet of Things Journal,2021,10.1109/JIOT.2021.3068571,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cf42333ca49dc4b02b8ee384ff6f04fe55610b81,https://www.semanticscholar.org/paper/cf42333ca49dc4b02b8ee384ff6f04fe55610b81,Automatic Traffic Sign Detection System With Voice Assistant,"In the arena of artificial intelligence, the world is revolutionizing with many technological applications being incorporated with Artificial Intelligence due to improved efficiency and performance. AI has penetrated drastically, delving deep into locker room decisions in many fields like agriculture, healthcare, military, manufacturing, robotics, transportation and so on. AI does a lot more than improving our lives, in most cases, it saves our lives too. Autonomous vehicles, the so-called self-driving cars, are one of the greatest applications of AI and are very instrumental in making the machine work autonomously by observing and interpreting the real-life scenario of the environment. This paper deals with the deployment of an Automatic Traffic sign detection System with voice assistant, which is one of the applications of autonomous vehicles, which can tone down the driver from puzzling traffic conditions significantly increasing driving safety and comfort. This will require an appropriate database and algorithm for improved accuracy in performance. This paper, therefore, compares the features, accuracy, and efficiency of various deep learning algorithms and comes up with a varied model thus saving computational resources.",2022 International Conference on Advanced Computing Technologies and Applications (ICACTA),2022,10.1109/ICACTA54488.2022.9753620,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
edf954eab21423a9072cbd24a53f69de88841068,https://www.semanticscholar.org/paper/edf954eab21423a9072cbd24a53f69de88841068,Shaping the Future of Smart Dentistry: From Artificial Intelligence (AI) to Intelligence Augmentation (IA),"Digitization is the emerging process in the current transformation of industry. Understanding the role and socio-economic consequences of digitalization is crucial for the way technology is being deployed in each sector. One of the affected sectors is dentistry. This study highlights the current advances and challenges in integrating and merging artificial intelligence (AI), intelligence augmentation (IA), and machine learning (ML) in dentistry. We conduct a comparative analysis to give an overview of which technology is being currently deployed and what role IA and AI will play in dentistry, as AI plays an assistive role in advancing human capabilities. We find that challenges range from AI finding its way into routine medical practice to qualitative challenges of retrieving adequate data input. Other challenges lie in the yet unanswered questions of liability in how to reduce deployment costs of new technology. Given these challenges, we provide an outlook of how future technology can be deployed in daily-life dentistry and how robots and humans will interact, given the current technology developments. The aim of this paper is to discuss the future of dentistry and whether it is AI or IA conquering the modern dentistry era.",IoT,2021,10.3390/iot2030026,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2341da561c2210a5f287a9761a9415ccf6aa853a,https://www.semanticscholar.org/paper/2341da561c2210a5f287a9761a9415ccf6aa853a,Laser-Based People Detection and Obstacle Avoidance for a Hospital Transport Robot,"This paper describes the development of a laser-based people detection and obstacle avoidance algorithm for a differential-drive robot, which is used for transporting materials along a reference path in hospital domains. Detecting humans from laser data is an important functionality for the safety of navigation in the shared workspace with people. Nevertheless, traditional methods normally utilize machine learning techniques on hand-crafted geometrical features extracted from individual clusters. Moreover, the datasets used to train the models are usually small and need to manually label every laser scan, increasing the difficulty and cost of deploying people detection algorithms in new environments. To tackle these problems, (1) we propose a novel deep learning-based method, which uses the deep neural network in a sliding window fashion to effectively classify every single point of a laser scan. (2) To increase the speed of inference without losing performance, we use a jump distance clustering method to decrease the number of points needed to be evaluated. (3) To reduce the workload of labeling data, we also propose an approach to automatically annotate datasets collected in real scenarios. In general, the proposed approach runs in real-time and performs much better than traditional methods. Secondly, conventional pure reactive obstacle avoidance algorithms can produce inefficient and oscillatory behaviors in dynamic environments, making pedestrians confused and possibly leading to dangerous reactions. To improve the legibility and naturalness of obstacle avoidance in human crowded environments, we introduce a sampling-based local path planner, similar to the method used in autonomous driving cars. The key idea is to avoid obstacles by switching lanes. We also adopt a simple rule to decrease the number of unnecessary deviations from the reference path. Experiments carried out in real-world environments confirmed the effectiveness of the proposed algorithms.",Sensors,2020,10.3390/s21030961,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bc0c11e3a101dcc64945390bf70fb07742143d7a,https://www.semanticscholar.org/paper/bc0c11e3a101dcc64945390bf70fb07742143d7a,"CyPhyHouse: A programming, simulation, and deployment toolchain for heterogeneous distributed coordination","Programming languages, libraries, and development tools have transformed the application development processes for mobile computing and machine learning. This paper introduces CyPhyHouse—a toolchain that aims to provide similar programming, debugging, and deployment benefits for distributed mobile robotic applications. Users can develop hardware-agnostic, distributed applications using the high-level, event driven Koord programming language, without requiring expertise in controller design or distributed network protocols. The modular, platform-independent middleware of CyPhyHouse implements these functionalities using standard algorithms for path planning (RRT), control (MPC), mutual exclusion, etc. A high-fidelity, scalable, multi-threaded simulator for Koord applications is developed to simulate the same application code for dozens of heterogeneous agents. The same compiled code can also be deployed on heterogeneous mobile platforms. The effectiveness of CyPhyHouse in improving the design cycles is explicitly illustrated in a robotic testbed through development, simulation, and deployment of a distributed task allocation application on in-house ground and aerial vehicles.",2020 IEEE International Conference on Robotics and Automation (ICRA),2019,10.1109/ICRA40945.2020.9196513,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
315acb4dece97c44e54e5ada981f68ed4e780d90,https://www.semanticscholar.org/paper/315acb4dece97c44e54e5ada981f68ed4e780d90,Digital Transformation in Smart Farm and Forest Operations Needs Human-Centered AI: Challenges and Future Directions,"The main impetus for the global efforts toward the current digital transformation in almost all areas of our daily lives is due to the great successes of artificial intelligence (AI), and in particular, the workhorse of AI, statistical machine learning (ML). The intelligent analysis, modeling, and management of agricultural and forest ecosystems, and of the use and protection of soils, already play important roles in securing our planet for future generations and will become irreplaceable in the future. Technical solutions must encompass the entire agricultural and forestry value chain. The process of digital transformation is supported by cyber-physical systems enabled by advances in ML, the availability of big data and increasing computing power. For certain tasks, algorithms today achieve performances that exceed human levels. The challenge is to use multimodal information fusion, i.e., to integrate data from different sources (sensor data, images, *omics), and explain to an expert why a certain result was achieved. However, ML models often react to even small changes, and disturbances can have dramatic effects on their results. Therefore, the use of AI in areas that matter to human life (agriculture, forestry, climate, health, etc.) has led to an increased need for trustworthy AI with two main components: explainability and robustness. One step toward making AI more robust is to leverage expert knowledge. For example, a farmer/forester in the loop can often bring in experience and conceptual understanding to the AI pipeline—no AI can do this. Consequently, human-centered AI (HCAI) is a combination of “artificial intelligence” and “natural intelligence” to empower, amplify, and augment human performance, rather than replace people. To achieve practical success of HCAI in agriculture and forestry, this article identifies three important frontier research areas: (1) intelligent information fusion; (2) robotics and embodied intelligence; and (3) augmentation, explanation, and verification for trusted decision support. This goal will also require an agile, human-centered design approach for three generations (G). G1: Enabling easily realizable applications through immediate deployment of existing technology. G2: Medium-term modification of existing technology. G3: Advanced adaptation and evolution beyond state-of-the-art.",Sensors,2022,10.3390/s22083043,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8218bcce2eab827d7c83456cb33e08112e498082,https://www.semanticscholar.org/paper/8218bcce2eab827d7c83456cb33e08112e498082,Generic distributed polymorphic learning model for a community of heterogeneous cyber physical social robots in MAS Environment and GPU Architecture,"This paper presents a new distributed polymorphic learning model for a community of heterogeneous cyber physical robots operating in a multi agent environment. This model allows a community of intelligent physical agents to exchange their minds represented by configured and trained neural net-works. The training operation of the neural networks is performed, using machines and deep learning techniques, in a distributed way based on special agents deployed in machines having high-performance computing resources based on GPUs. Each mind, specialized in a specific field, is initially affected to an agent. Depending on the event context, robots can automatically select the trained and appropriate trained network to resolve the situation either by using their own training models, or by collaborating with other agents specialized to perform the context event. In this article, we present results of a model implementation based on DeepLearning4J Framework and a multi-agent system middleware",2020 International Conference on Intelligent Systems and Computer Vision (ISCV),2020,10.1109/ISCV49265.2020.9204226,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,https://www.semanticscholar.org/paper/6ea59f72f3239e524b7fb5d09eaf2fc9638cb018,Ai Technology Achieving General Purpose Ai That Can,"Artificial IntelligenceArtificial Intelligence in Cyber Security: Impact and ImplicationsTechnological Progress, Artificial Intelligence, and Inclusive GrowthEnergy Research AbstractsCan Artificial Intelligence ImproveHandbook of Pharmaceutical Granulation TechnologyArtificial Intelligence for BusinessArtificial Intelligence in EducationThe Democratization of Artificial IntelligenceAdvances in Artificial Intelligence, Software and Systems EngineeringFinancing Our FutureProceedings of the Future Technologies Conference (FTC) 2021, Volume 1Artificial IntelligenceNew Technologies in Dermatological Science and PracticeArtificial IntelligenceGame Theory and Machine Learning for Cyber SecurityRegulatory Aspects of Artificial Intelligence on BlockchainConcise Encyclopedia of Software EngineeringEuropean Artificial Intelligence (AI) Leadership, the Path for an Integrated VisionAI In The Age Of Cyber-DisorderReadings in Artificial Intelligence and DatabasesHuman decisionsConstitution 3.0Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart AgricultureArtificial Intelligence and Deep Learning for Decision MakersThe Regional Economics of Technological TransformationsArchitects of IntelligenceArtificial IntelligenceArtificial Intelligence and Integrated Intelligent Information SystemsECIAIR 2019 European Conference on the Impact of Artificial Intelligence and Robotics Intelligence UnboundHow to Achieve Inclusive GrowthArtificial Intelligence for Business OptimizationArtificial Intelligence in SocietyThe Myth of Artificial IntelligenceThe Digital Innovation RaceAI-First HealthcareProject Management Best Practices: Achieving Global ExcellenceConnected WorldBiotechnology: Concepts, Methodologies, Tools, and Applications The interaction of database and AI technologies is crucial to such applications as data mining, active databases, and knowledge-based expert systems. This volume collects the primary readings on the interactions, actual and potential, between these two fields. The editors have chosen articles to balance significant early research and the best and most comprehensive articles from the 1980s. An in-depth introduction discusses basic research motivations, giving a survey of the history, concepts, and terminology of the interaction. Major themes, approaches and results, open issues and future directions are all discussed, including the results of a major survey conducted by the editors of current work in industry and research labs. Thirteen sections follow, each with a short introduction. Topics examined include semantic data models with emphasis on conceptual modeling techniques for databases and information systems and the integration of data model concepts in high-level data languages, definition and maintenance of integrity constraints in databases and knowledge bases, natural language front ends, object-oriented database management systems, implementation issues such as concurrency control and error recovery, and representation of time and knowledge incompleteness from the viewpoints of databases, logic programming, and AI.The artificial intelligence (AI) landscape has evolved significantly from 1950 when Alan Turing first posed the question of whether machines can think. Today, AI is transforming societies and economies. It promises to generate productivity gains, improve well-being and help address global challenges, such as climate change, resource scarcity and health crises.As technology continues to saturate modern society, agriculture has started to adopt digital computing and data-driven innovations. This emergence of “smart” farming has led to various advancements in the field, including autonomous equipment and the collection of climate, livestock, and plant data. As connectivity and data management continue to revolutionize the farming industry, empirical research is a necessity for understanding these technological developments. Artificial Intelligence and IoT-Based Technologies for Sustainable Farming and Smart Agriculture provides emerging research exploring the theoretical and practical aspects of critical technological solutions within the farming industry. Featuring coverage on a broad range of topics such as crop monitoring, precision livestock farming, and agronomic data processing, this book is ideally designed for farmers, agriculturalists, product managers, farm holders, manufacturers, equipment suppliers, industrialists, governmental professionals, researchers, academicians, and students seeking current research on technological applications within agriculture and farming.This book constitutes the refereed proceedings of the Second International Conference, SLAAI-ICAI 2018, held in Moratuwa, Sri Lanka, in December 2018. The 32 revised full papers presented were carefully reviewed and selected from numerous submissions. The papers are organized in the following topical sections: ?intelligence systems; neural networks; game theory; ontology engineering; natural language processing; agent based system; signal and image processing.After a long time of neglect, Artificial Intelligence is once again at the center of most of our political, economic, and socio-cultural debates. Recent advances in the field of Artifical Neural Networks have led to a renaissance of dystopian and utopian speculations on an AI-rendered future. Algorithmic technologies are deployed for identifying potential terrorists through vast surveillance networks, for producing sentencing guidelines and recidivism risk profiles in criminal justice systems, for demographic and psychographic targeting of bodies for advertising or propaganda, and more generally for automating the analysis of language, text, and images. Against this background, the aim of this book is to discuss the heterogenous conditions, implications, and effects of modern AI and Internet technologies in terms of their political dimension: What does it mean to critically investigate efforts of net politics in the age of machine learning algorithms?Intelligence Unbound explores the prospects, promises, and potential dangers of machine intelligence and uploaded minds in a collection of stateof-the-art essays from internationally recognized philosophers, AI researchers, science fiction authors, and theorists. Compelling and intellectually sophisticated exploration of the latest thinking on Artificial Intelligence and machine minds Features contributions from an international cast of philosophers, Artificial Intelligence researchers, science fiction authors, and more Offers current, diverse perspectives on machine intelligence and uploaded minds, emerging topics of tremendous interest Illuminates the nature and ethics of tomorrow’s machine minds—and of the convergence of humans and machines—to consider the pros and cons of a variety of intriguing possibilities Considers classic philosophical puzzles as well as the latest topics debated by scholars Covers a wide range of viewpoints and arguments regarding the prospects of uploading and machine intelligence, including proponents and skeptics, pros and consCompanies that don't use AI to their advantage will soon be left behind. Artificial intelligence and machine learning will drive a massive reshaping of the economy and society. What should you and your company be doing right now to ensure that your business is poised for success? These articles by AI experts and consultants will help you understand today's essential thinking on what AI is capable of now, how to adopt it in your organization, and how the technology is likely to evolve in the near future. Artificial Intelligence: The Insights You Need from Harvard Business Review will help you spearhead important conversations, get going on the right AI initiatives for your company, and capitalize on the opportunity of the machine intelligence revolution. Catch up on current topics and deepen your",,2022,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c2eaae510b82577c8ee0802138b813e3f974bf25,https://www.semanticscholar.org/paper/c2eaae510b82577c8ee0802138b813e3f974bf25,Automated Detection of Side Channels in Cryptographic Protocols: DROWN the ROBOTs!,"Currently most practical attacks on cryptographic protocols like TLS are based on side channels, such as padding oracles. Some well-known recent examples are DROWN, ROBOT and Raccoon (USENIX Security 2016, 2018, 2021). Such attacks are usually found by careful and time-consuming manual analysis by specialists. In this paper, we consider the question of how such attacks can be systematically detected and prevented before (large-scale) deployment. We propose a new, fully automated approach, which uses supervised learning to identify arbitrary patterns in network protocol traffic. In contrast to classical scanners, which search for known side channels, the detection of general patterns might detect new side channels, even unexpected ones, such as those from the ROBOT attack. To analyze this approach, we develop a tool to detect Bleichenbacher-like padding oracles in TLS server implementations, based on an ensemble of machine learning algorithms. We verify that the approach indeed detects known vulnerabilities successfully and reliably. The tool also provides detailed information about detected patterns to developers, to assist in removing a potential padding oracle. Due to the automation, the approach scales much better than manual analysis and could even be integrated with a CI/CD pipeline of a development environment, for example.",IACR Cryptol. ePrint Arch.,2021,10.1145/3474369.3486868,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7232dd331d43ed5d314eb32a7a4298dc8404034f,https://www.semanticscholar.org/paper/7232dd331d43ed5d314eb32a7a4298dc8404034f,Emotion based Media Playback System using PPG Signal,"The study involved identifying human emotions and integrates the identified emotion with the music system. The idea is to develop a complete product to utilize the detected emotion in a real-time application and also to achieve more accuracy and less memory. Human emotions are identified using physiological signals such as electrocardiography, electromyography, photoplethysmography, respiration, skin temperature, etc. Obtaining photoplethysmography (PPG) from the sensor is a simple, cost-effective, and non-invasive method. PPG sensors are capable of providing accurate heart-rate (HR) by detecting the variations in the blood flow. Signals are acquired using wearable technology from a personal whereas not compromising comfort and privacy. The attributes of Heart Rate Variability are analyzed to describe emotions, namely happy, calm, unhappy (sad), and fear using Machine learning technology. We deployed this recognized emotion to automate the music system associated with its emotion. To bring this, we built an Android app to communicate with the smart wearable utilized. Totally 150 members from both genders have participated. The accuracy of 91.81% is achieved. This emotion recognition system can be used in various fields like robotics, medicine, virtual reality, and gaming, advertising, education, automotive working conditions and safety, home appliances.","2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",2021,10.1109/WiSPNET51692.2021.9419475,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4de9fa0ecb7c10e9a3b914ffaa43466c726f9c21,https://www.semanticscholar.org/paper/4de9fa0ecb7c10e9a3b914ffaa43466c726f9c21,A Smart Capacitive Sensor Skin with Embedded Data Quality Indication for Enhanced Safety in Human–Robot Interaction,"Smart sensors are an integral part of the Fourth Industrial Revolution and are widely used to add safety measures to human–robot interaction applications. With the advancement of machine learning methods in resource-constrained environments, smart sensor systems have become increasingly powerful. As more data-driven approaches are deployed on the sensors, it is of growing importance to monitor data quality at all times of system operation. We introduce a smart capacitive sensor system with an embedded data quality monitoring algorithm to enhance the safety of human–robot interaction scenarios. The smart capacitive skin sensor is capable of detecting the distance and angle of objects nearby by utilizing consumer-grade sensor electronics. To further acknowledge the safety aspect of the sensor, a dedicated layer to monitor data quality in real-time is added to the embedded software of the sensor. Two learning algorithms are used to implement the sensor functionality: (1) a fully connected neural network to infer the position and angle of objects nearby and (2) a one-class SVM to account for the data quality assessment based on out-of-distribution detection. We show that the sensor performs well under normal operating conditions within a range of 200 mm and also detects abnormal operating conditions in terms of poor data quality successfully. A mean absolute distance error of 11.6 mm was achieved without data quality indication. The overall performance of the sensor system could be further improved to 7.5 mm by monitoring the data quality, adding an additional layer of safety for human–robot interaction.",Sensors,2021,10.3390/s21217210,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c76a3fb1a15007031fbb47e16ba8dff98d02a9df,https://www.semanticscholar.org/paper/c76a3fb1a15007031fbb47e16ba8dff98d02a9df,Single RGB-D Camera Teleoperation for General Robotic Manipulation,"We propose a teleoperation system that uses a single RGB-D camera as the human motion capture device. Our system can perform general manipulation tasks such as cloth folding, hammering and 3mm clearance peg in hole. We propose the use of non-Cartesian oblique coordinate frame, dynamic motion scaling and reposition of operator frames to increase the flexibility of our teleoperation system. We hypothesize that lowering the barrier of entry to teleoperation will allow for wider deployment of supervised autonomy system, which will in turn generates realistic datasets that unlock the potential of machine learning for robotic manipulation.",ArXiv,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
95beb6c2a030929486c91d101f05caf8622c1527,https://www.semanticscholar.org/paper/95beb6c2a030929486c91d101f05caf8622c1527,Deploying the Big Data Science Center at the Shanghai Synchrotron Radiation Facility: the first superfacility platform in China,"With recent technological advances, large-scale experimental facilities generate huge datasets, into the petabyte range, every year, thereby creating the Big Data deluge effect. Data management, including the collection, management, and curation of these large datasets, is a significantly intensive precursor step in relation to the data analysis that underpins scientific investigations. The rise of artificial intelligence (AI), machine learning (ML), and robotic automation has changed the landscape for experimental facilities, producing a paradigm shift in how different datasets are leveraged for improved intelligence, operation, and data analysis. Therefore, such facilities, known as superfacilities, which fully enable user science while addressing the challenges of the Big Data deluge, are critical for the scientific community. In this work, we discuss the process of setting up the Big Data Science Center within the Shanghai Synchrotron Radiation Facility (SSRF), China’s first superfacility. We provide details of our initiatives for enabling user science at SSRF, with particular consideration given to recent developments in AI, ML, and robotic automation.",Mach. Learn. Sci. Technol.,2021,10.1088/2632-2153/abe193,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7162a6153ce1a8b3e768d5b96511d11912753e1d,https://www.semanticscholar.org/paper/7162a6153ce1a8b3e768d5b96511d11912753e1d,FlockAI: A Testing Suite for ML-Driven Drone Applications,"Machine Learning (ML) is now becoming a key driver empowering the next generation of drone technology and extending its reach to applications never envisioned before. Examples include precision agriculture, crowd detection, and even aerial supply transportation. Testing drone projects before actual deployment is usually performed via robotic simulators. However, extending testing to include the assessment of on-board ML algorithms is a daunting task. ML practitioners are now required to dedicate vast amounts of time for the development and configuration of the benchmarking infrastructure through a mixture of use-cases coded over the simulator to evaluate various key performance indicators. These indicators extend well beyond the accuracy of the ML algorithm and must capture drone-relevant data including flight performance, resource utilization, communication overhead and energy consumption. As most ML practitioners are not accustomed with all these demanding requirements, the evaluation of ML-driven drone applications can lead to sub-optimal, costly, and error-prone deployments. In this article we introduce FlockAI, an open and modular by design framework supporting ML practitioners with the rapid deployment and repeatable testing of ML-driven drone applications over the Webots simulator. To show the wide applicability of rapid testing with FlockAI, we introduce a proof-of-concept use-case encompassing different scenarios, ML algorithms and KPIs for pinpointing crowded areas in an urban environment.",Future Internet,2021,10.3390/fi13120317,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b0982a1895f2a792c8b0c2dd691533012ef3f1b5,https://www.semanticscholar.org/paper/b0982a1895f2a792c8b0c2dd691533012ef3f1b5,Interactive Learning of Temporal Features for Control,"The ongoing industry revolution is demanding more flexible products, including robots in household environments or medium scale factories. Such robots should be able to adapt to new conditions and environments, and to be programmed with ease. As an example, let us suppose that there are robot manipulators working in an industrial production line that need to perform a new task. If these robots were hard coded, it could take days to adapt them to the new settings, which would stop the production of the factory. Easily programmable robots by non-expert humans would speed up this process considerably. In this regard, we present a framework in which robots are capable to quickly learn new control policies and state representations, by using occasional corrective human feedback. To achieve this, we focus on interactively learning these policies from non-expert humans that act as teachers. We present a Neural Network (NN) architecture, along with an Interactive Imitation Learning (IIL) method, which efficiently learns spatiotemporal features and policies from raw high dimensional observations (raw pixels from an image), for tasks in environments not fully temporally observable. We denominate IIL as a branch of Imitation Learning (IL) where human teachers provide different kinds of feedback to the robots, like new demonstrations triggered by robot queries [1], corrections [2], preferences [3], reinforcements [4], etc. Most IL methods work under the assumption of learning from perfect demonstrations; therefore, they fail when teachers only have partial insights in the task execution. Non-expert teachers could be considered all the users who are neither Machine Learning (ML)/control experts, nor skilled to fully show the desired behavior of the policy. Interactive approaches like COACH [5], and some Interactive Reinforcement Learning (IRL) approaches [4], [6], are intended for non-expert teachers, but are not completely deployable for end-users. Sequential decision-making learning methods (IL, IIL, IRL, etc.) rely on good state representations, which make the shaping of the policy landscape simple, and provide good generalization properties. However, this requirement brings the need of experts on feature engineering to pre-process the states properly, before running the learning algorithms.",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a979f721160e23a724a279178f506a6982db5067,https://www.semanticscholar.org/paper/a979f721160e23a724a279178f506a6982db5067,Multi-Robot Deep Reinforcement Learning via Hierarchically Integrated Models,"Deep reinforcement learning algorithms require large and diverse datasets in order to learn successful perception-based control policies. However, gathering such datasets with a single robot can be prohibitively expensive. In contrast, collecting data with multiple platforms with possibly different dynamics is a more scalable approach to large-scale data collection. But how can deep reinforcement learning algorithms leverage these dynamically heterogeneous datasets? In this work, we propose a deep reinforcement learning algorithm with hierarchically integrated models (HInt). At training time, HInt learns separate perception and dynamics models, and at test time, HInt integrates the two models in a hierarchical manner and plans actions with the integrated model. This method of planning with hierarchically integrated models allows the algorithm to train on datasets gathered by a variety of different platforms, while respecting the physical capabilities of the deployment robot at test time. Our simulated and real world navigation experiments show that HInt outperforms conventional hierarchical policies and single-source approaches. Figure 1: Overview of our hierarchically integrated models (HInt) algorithm. At training time, HInt separately trains a perception model and a dynamics model, then at test time, HInt combines the perception and dynamics model into a single model for integrated planning and execution. Our modular training procedure enables HInt to train the perception model using data gathered by multiple platforms, such as ground robots and even people recording video with a hand-held camera, while our integrated model at test time ensures the perception model only considers trajectories which are dynamically feasible. NeurIPS 2020 3rd Robot Learning Workshop: Grounding Machine Learning Development in the Real World.",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3894983b1d61413fd98ef4e9bcc8ab892aa8a0cb,https://www.semanticscholar.org/paper/3894983b1d61413fd98ef4e9bcc8ab892aa8a0cb,Urban Robotics: Achieving Autonomy in Design and Regulation of Robots and Cities,"Are cities ready for self-driving, artificially intelligent, vehicles and robotics? The urban marketplace is increasingly filled with products emblematic of “smart” cities, from widely discussed autonomous vehicles to smaller variations on the theme, such as robotics for delivery, security, and entertainment. Altogether, such urban robotics represent a new wave of technology in which digital sensors, networked devices, and their associated data stores are given the algorithmic, physical, and legal means to move in public space. As time goes on, the public is increasingly likely to encounter self-driving vehicles, robots, and drones on city streets, sidewalks, and in urban airspace. How should cities respond to these new and impending technologies? 
 
Firms have technological, market, and financial interest in testing and deploying their products in public space, but the implications for cities span a broad array of intended and unintended consequences. Cities are natural sites of experimentation for firms interested in bringing these products to market, and the perception of economic opportunity associated with tech firms is leading city representatives to reclaim public spaces, transforming them into testbeds for product development. It is worth noting, however, that experimentation involves trial and error, and there are limitations to the ability of artificial intelligence to navigate the wide range of conditions and events that comprise the urban environment. Ultimately, the design of the environment may be as important to the success of urban robotics as the design of the hardware and software that comprise these products. These are matters that city officials care about; the design, function, and finance of urban built environments is generally the purview of local government. Additionally, artificial intelligence imbues devices with the purpose of replacing as well as augmenting the roles and responsibilities of persons, and this tension exists at the local level. Significant new questions in law, such as legal liability for the performance of artificially intelligent devices, are being addressed as autonomous vehicles and devices enter public rights-of-way. Lastly, these products generate rich data stores about the public, bringing market potential along with the coupled moral hazard of data monetization and loss of privacy, including surveillance. Which parties are positioned to benefit from this experimentation, and which will absorb the costs? In the face of these potentially widespread and enduring industrial and technological changes, how might cities act in the public interest? 
 
The answers to these questions lie as much in the institutional arrangements designed to govern this new wave of technologies as it does in the intrinsic capabilities of these products. Anyone evaluating the existing policy environment for artificially intelligent devices today would find technological optimism as well as pessimism, conflicting perspectives of the public interest, and preemptive acts at the state and federal levels. In particular, preemption in current policy-making raises issues, because the consequences and cost of product design, including safety and surveillance as well as convenience and expense, play out at the local level. As city officials ask their residents to co-exist with robots and negotiate with firms over the transaction costs that accompany these products, they need the flexibility and funding necessary to adapt to market conditions and the authority to act as market makers. In the best of circumstances, federal agencies provide guidance and domain expertise, while states provide a supportive framework for cities to operate in, with a backstop against the expansive possibility of harm. In the most egregious cases, preemption threatens to revoke the rights of the persons who, at the local level, are asked to bear the risk and cost of residing with robots, and to prevent the resolution of conflicts through local levels of government. Preemption debates in technology law have already arisen around net neutrality, sharing economy platforms, and municipal broadband, with important consequences. Some proposed federal and state laws and existing state statutes already preempt cities on robotics in several important ways. 
 
The purpose of this article is to provide a framework for public decision-makers to engage effectively with the firms that are bringing artificially intelligent robotics to market in public space. With an institutional economic perspective, this article suggests a means for evidence-based policymaking by breaking down design and its evaluation into constituent sequential components, recognizing the private and social costs of experimentation in cities, and recommending a limited scope for state and federal intervention. Part II begins by defining the characteristics of the current wave of robotics entering public space, placing public-facing robotics within the theory of the nature of technology, and elaborating on the process of product design with algorithmic feedback for machine learning in complex urban environments. Part III explores the opportunities and hazards that await cities as sites of experimentation, and introduces a comparative approach to policy-making to forestall social externalities while permitting technological change. Part IV explores the policy environment that is already taking shape for governing artificially intelligent robotics in public space. Part V draws on the arguments from the preceding parts of the paper to recommends against broad express preemption or field preemption at the state and federal level of local governments in robotics law. Part VI addresses possible counter arguments. Part VII concludes with a research agenda for urban robotics going forward.",,2018,10.2139/SSRN.3145460,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4cb659cf5a8dea8b86263c65ec613f48c2426c51,https://www.semanticscholar.org/paper/4cb659cf5a8dea8b86263c65ec613f48c2426c51,"Analyzing Dilemmas Posed by Artificial Intelligence and 4IR Technologies Requires using all Available Models, Including the Existing International Human Rights Framework and Principles of AI Ethics","We are living in the epoch referred to as the ‘4th industrial revolution'. The 4th industrial Revolution (4IR) is a development characterized by a fusion of technologies that blur the digital, physical, and biological spheres (e.g., cyberspace, virtual and augmented reality, body-machine interface and robotics). 
 
Certain is the guaranteed ubiquitous adoption of these technologies, and futurism. Where the former is a reference to the increasing use and normalization of such technologies in everyday life, government service provision and industry. The latter is a reference to the philosophical/science fiction discussions that are emerging as a result of these changes (e.g. debates around the ‘singularity’, transhumanism, and posthumanism – often presented in utopian/dystopia terms). As such, the definition of digital ethics can be expanded and expressed in terms of the impacts of new digital technologies, through analysis of potential opportunities and risks in contemporary and future contexts. 
 
Many are working on forward‑looking policy frameworks and governance protocols, with broad multistakeholder engagement and buy‑in, to accelerate the adoption of emerging technologies in the global public interest, such as artificial intelligence (AI) and machine learning (ML) blockchain, 5G, data analytics, quantum computing, autonomous vehicles, synthetic biology, the internet of things (IoT), and killer robots or autonomous weapons systems (AWS). We have gained insight into the unequal distribution of the positive and negative impacts of AI on human rights throughout society, and have begun to explore the power of the human rights framework to address these disparate impacts. 
 
Although internationally recognized laws and standards on human rights provide a common standard of achievement for all people in all countries, more work is needed to understand how they can be best applied in the context of disruptive technology. 
 
AI systems raise myriad questions for society and democracy, only some of which are covered or addressed by existing laws. In order to fill these perceived gaps, a vocal group of governments, industry players, academics, and civil society actors have been promoting principles or frameworks for ethical AI. 
 
COVID-19 accelerated the use of AI in all countries and all fields. The pandemic accelerated the transition to a society that is increasingly based on the use of AI. This also increased the threats new risks related to human rights in the context of AI deployment. The human rights implications of governments' aggressive measures targeting the spread of COVID-19-related misinforation is also discussed. 
 
The question of whether corporations can act ethically is particularly relevant for Big Tech. Many of these firms are oligopolies that individuals and governments alike depend on completely, though they have little to no capacity to independently remedy issues when they arise, as Project Maven showed. Artificial intelligence and automated decision-making tools are increasing in power and centrality, and technology companies retain large troves of private data that it sells. These companies are at the forefront of technological innovation and may be caught up with the factual question of what can be done rather than the normative question of whether it should be done. All these issues arise in a field where there is little to no government regulation or intervention. The threats AI poses to society are so new, that the legal system is struggling to impose sufficient values and restrictions. Thus, a coherent approach to addressing AI ethics, values and consequences is, indeed, urgently needed. 
 
In May 2019, 42 countries adopted the Organization for Economic Co-operation and Development (OECD) AI Principles, a legal recommendation that includes five principles and five recommendations related to the use of AI. To ensure the successful implementation of the Principles, the OECD launched the AI Policy Observatory in February 2020. The Observatory publishes practical guidance about how to implement the AI Principles, and supports a live database of AI policies and initiatives globally. It also compiles metrics and measurement of global AI development and uses its convening power to bring together the private sector, governments, academia, and civil society. 
 
The AI ethics and governance initiatives discussed are cause for optimism that the global community will use all available models and brainpower for analysis and ultimately global governance of AI.",SSRN Electronic Journal,2021,10.2139/ssrn.3874279,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d30c40f372976914f6a433d9bd6c70a2eea54832,https://www.semanticscholar.org/paper/d30c40f372976914f6a433d9bd6c70a2eea54832,Factor Graphs for Robot Perception,"We review the use of factor graphs for the modeling and solving of large-scale inference problems in robotics. Factor graphs are a family of probabilistic graphical models, other examples of which are Bayesian networks and Markov random fields, well known from the statistical modeling and machine learning literature. They provide a powerful abstraction that gives insight into particular inference problems, making it easier to think about and design solutions, and write modular software to perform the actual inference. We illustrate their use in the simultaneous localization and mapping problem and other important problems associated with deploying robots in the real world. We introduce factor graphs as an economical representation within which to formulate the di erent inference problems, setting the stage for the subsequent sections on practical methods to solve them. We explain the nonlinear optimization techniques for solving arbitrary nonlinear factor graphs, which requires repeatedly solving large sparse linear systems. The sparse structure of the factor graph is the key to understanding this more general algorithm, and hence also understanding (and improving) sparse factorization methods. We provide insight into the graphs underlying robotics inference, and how their sparsity is a ected by the implementation choices we make, crucial for achieving highly performant algorithms. As many inference problems in robotics are incremental, we also discuss the iSAM class of algorithms that can reuse previous computations, re-interpreting incremental matrix factorization methods as operations on graphical models, introducing the Bayes tree in the process. Because in most practical situations we will have to deal with 3D rotations and other nonlinear manifolds, we also introduce the more sophisticated machinery to perform optimization on nonlinear manifolds. Finally, we provide an overview of applications of factor graphs for robot perception, showing the broad impact factor graphs had in robot perception. F. Dellaert and M. Kaess. Factor Graphs for Robot Perception. Foundations and Trends® in Robotics, vol. 6, no. 1-2, pp. 1–139, 2017. DOI: 10.1561/2300000043.",Found. Trends Robotics,2017,10.1561/2300000043,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d0c62486d25939add3025bf2c4739b97ed05760b,https://www.semanticscholar.org/paper/d0c62486d25939add3025bf2c4739b97ed05760b,Performance Evaluation of YOLOv3 and YOLOv4 Detectors on Elevator Button Dataset for Mobile Robot,"The performance evaluation of an AI network model is the important part for building an effective solution before its deployment in real-world on the robot. In our study, we have implemented YOLOv3-tiny and YOLOv4-tiny darknet based frameworks for performance evaluation of the elevator button recognition task and tested both variants on image and video datasets. The objective of our study is two-fold: First, to overcome the limitation of elevator buttons dataset by creating new dataset and increasing its quantity without compromising the quality; Second, to provide a comparative analysis through experimental results and the performance evaluation of both detectors using four machine learning metrics. The purpose of our work is to assist the researchers and developers in decision making of suitable detector selection for deployment in the elevator robot towards button recognition application. The results show that YOLOv4-tiny outperforms YOLOv3-tiny with an overall accuracy of 98.60% compared to 97.91% at 0.5 IoU.","2021 21st International Conference on Control, Automation and Systems (ICCAS)",2021,10.23919/ICCAS52745.2021.9649792,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
16b66e9be116f04a4898ccb28d44c89ecdbde18c,https://www.semanticscholar.org/paper/16b66e9be116f04a4898ccb28d44c89ecdbde18c,Optimization of Transmission Strategy for Wireless Power Transfer Using Multi-Armed Bandit Algorithm,"This paper aims to solve the optimization problems in far-field wireless power transfer systems using machine learning techniques. We assembled the RF power transfer robot, which can emit the electromagnetic wave to charge the energy harvesters that are deployed in the experimental field. The wireless transmitter intends to charge all the energy harvesters in a fair manner. Since the energy harvesters can be either stationary or mobile, a multi-armed bandit(MAB) problem is formulated and we use Upper Confidence Bound(UCB) algorithm to determine the optimal transmission strategy. As the number of the transmitters is increased, multiple wireless transmitters coordinate with each other to boost the levels of energy harvesting at all energy harvesters. Correspondingly, we formulate a combinational MAB problem and UCB algorithm is applied to determine the optimal transmission strategy for each transmitter. The simulation results prove the superiority of the Multi-armed bandit approach in solving the proposed optimization problems.","2021 IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",2021,10.1109/iemcon53756.2021.9623190,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9d97175ec8898b5ed42b2f5613a0e9f8e3a212b1,https://www.semanticscholar.org/paper/9d97175ec8898b5ed42b2f5613a0e9f8e3a212b1,Ocean Surface Cleaning Autonomous Robot (OSCAR) using Object Classification Technique and Path Planning Algorithm,"Increasing water pollution is one of the biggest concerns in today’s world. It leads to a variety of problems including an increase in the level of toxic concentration in the water. This paper aims to introduce a concept of an ocean/water body cleaning robot that attempts to classify the wastes using a camera with a custom machine learning model and segregate accordingly using separators while collecting them on the basket attached, that can be recycled on the base station. The robot can be deployed on any water surface thus making it more effective than a largescale ocean pollution cleaning technique. It can be used to clean up oil spills from shipwreck and pipeline leakage and can monitor the water quality of the particular location and send a distress signal to the base station if the readings are abnormal. The water quality data and the information about the type of pollutants from the machine learning model can be used to formulate local laws to reduce pollution and create awareness about the type of material that ends up at the ocean/water body.",Journal of Physics: Conference Series,2021,10.1088/1742-6596/2115/1/012021,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
625b0bdaea75d2e2a224b1a3a032748be500c5b1,https://www.semanticscholar.org/paper/625b0bdaea75d2e2a224b1a3a032748be500c5b1,State Evaluation Method of Robot Lubricating Oil Based on Support Vector Regression,"Recently, the development of the Industrial Internet of Things (IIoT) has led enterprises to re-examine the research of the equipment-state-prediction models and intelligent manufacturing applications. Take industrial robots as typical example. Under the effect of scale, robot maintenance decision seriously affects the cost of spare parts and labor deployment. In this paper, an evaluation method is proposed to predict the state of robot lubricating oil based on support vector regression (SVR). It would be the proper model to avoid the structural risks and minimize the effect of small sample volume. IIoT technology is used to collect and store the valuable robot running data. The key features of the running state of the robot are extracted, and the machine learning model is applied according to the measured element contents of the lubricating oil. As a result, the cost of spare parts consumption can be saved for more than two million CNY per year.",Comput. Intell. Neurosci.,2021,10.1155/2021/9441649,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c1e231a7694f0c51cbf15924aed10fad578358e6,https://www.semanticscholar.org/paper/c1e231a7694f0c51cbf15924aed10fad578358e6,Novel applications of soft computing techniques for industrial and environmental enterprises,"This special issue compiles recent applications of soft-computing techniques for managing enterprises within industrial and environmental sectors. It is aimed at both researchers and practitioners from academia and industry who are engaged in deploying soft-computing solutions for real-life enterprise problems. Six papers are included in this special issue, covering a wide variety of case studies, ranging from conversational systems to vehicle routing problems. Additionally, a variety of soft-computing techniques have been applied, such as deep learning, random forest, particle swarm optimization, clustering, and metaheuristics among others. Thanks to the wide-range panoramic view presented by these complementary works, readers can get a feel for developing up-to-date soft-computing systems to solve present problems in enterprise contexts. In the first contribution by Cárdenas-Montes, recent machine learning models are applied for finding anomalies in Ozone-urban images generated from an Air Quality Monitoring Network. To do so, deep learning with maps-based data coming from an air-quality sensor network is applied. Differentiating from previous work, time series are not individually handled, but they are used for generating maps. This is carried out thanks to advances in computer vision to process the data. Besides, the manuscript contributes to the outlier detection problem, investigating the most appropriate map-generation technique for different scientific targets. The author's proposal is compared to the direct use of DBSCAN on the intensity of the pixels, as well as other approaches for learning latent variables, such as autoencoders based on Convolutional Neural Networks (undercomplete and denoising). The proposed approach achieves high performance when labelling maps with anomalies at a local and a global scale, including the outliers identified by the direct use of DBSCAN on the intensity of the pixels. By using the author's approach, anomalous measurements in a few monitoring stations are also detected. López-Sánchez et al. propose a new method to solve a bi-objective variant of the Vehicle Routing Problem, taking into account its industrial and environmental implications. More precisely, the transportation of products by industrial enterprises to their customers is addressed, in which has been named the Periodic Vehicle Routing Problem with Service Choice (PVRP-SC) problem. It is defined as the problem of finding a set of routes for each vehicle and each day over a planning horizon in order to minimize total travel cost minus service benefit while satisfying operational constraints. The authors approach PVRP-SC from a novel perspective; instead of considering the problem as a single-objective optimization problem, they have solved the problem as a bi-objective optimization one, that minimizes the total emissions produced by all vehicles and maximizes the service quality measured as the number of times that a customer is visited by a vehicle in order to be served. To do that, a multi-objective approach based on Multi-Start Local Searches is applied. According to the experimental setup that has been carried out, the proposed algorithm outperforms other standard alternatives. The next contribution by Burkart et al. aims to examine how participants, supported by a Decision Support System, revise their initial prediction by four different treatments in a between-subject design study. This research work examines how prediction revision (adjusting an initial prediction on the basis of new information) is affected by the provided explanations. The four treatments (interpretable regression model, Random Forest, black box with a local explanation, and black box with a global explanation) differ in the degree of explainability in order to understand the system decisions. Participants in each treatment were told basic facts about their corresponding model and how each is used for producing a response when presented with a set of predictor values. Amazon Mechanical Turk with Sophie Labs was used as a platform for designing online experiments, in which 80 participants were recruited and randomly assigned to the experiments. The obtained results show that all participants improved their predictions after receiving advice whether it was a complete black box or a black box with an explanation. Authors conclude that naturally interpretable models were not incorporated more heavily in the decision process than black box models and explanations from them. Bayram et al. propose an anomaly detection system based on acoustic signals, especially required to quickly detect and interfere with the abnormal events during the industrial processes. More precisely, the authors propose a real time Acoustic Anomaly Detection System employing sequence-to-sequence Autoencoder models. The proposed processing pipeline uses the audio features extracted from the streaming audio signal captured by a single-channel microphone. The reconstruction error generated by the Autoencoder models is calculated to measure the degree of abnormality of the sound event. The performance of Convolutional Long Short-Term Memory Autoencoder is evaluated and compared with sequential Convolutional Autoencoder using sounds captured from various industrial manufacturing processes. The experimental setup comprises different Signal-to-Noise Ratio (SNR) conditions of sound events such as an explosion, fire, and glass breaking. Four acoustic datasets have been analyzed, containing sounds associated to the following industrial activities: painting, cutting, welding, and robotic arm operation. In the validation that was carried out with the real time system, it is shown that the Long Short-Term Memory version of the Autoencoder outperforms the other version of such model according to the AUC (Area Under the ROC Curve) metric. Received: 5 October 2020 Accepted: 7 October 2020",Expert Syst. J. Knowl. Eng.,2020,10.1111/exsy.12654,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1850ddf6523f429961c49ed7d0d701e14c706964,https://www.semanticscholar.org/paper/1850ddf6523f429961c49ed7d0d701e14c706964,A platform for robot-assisted intraoperative imaging in breast conserving surgery,"INTRODUCTION: Breast conserving surgery is a procedure that involves removing a cancerous lesion from the breast while preserving the surrounding healthy tissue. The outcomes of these surgeries are more psychologically and cosmetically favourable when compared to treatment options like partial or full mastectomy where the entire breast is removed. A main shortcoming of these procedures is that precise tumor delineation is difficult whenever a tumor is irregularly shaped, non-palpable or if the surgeon has restricted visual access to the incision site. Recent advancements in optical, acoustic and biochemical imaging approaches such as spectroscopy, ultrasound and mass spectrometry, enable intraoperative tissue assessment as a potential solution to this problem [1]. With the assistance of machine learning, imaging data can be used to identify a cross section of tissue as either cancerous or healthy. However, effective use of these imaging tools is limited by the surgeon’s ability to thoroughly scan the resection cavity by hand and retrace their motion to perform additional resection if necessary. Therefore, in this work we propose a robotic framework that can be used to deploy these imaging tools in breast cancer surgery. The system itself is unique when compared to existing surgical robotic devices that are designed for robust control and force feedback. As the breast itself poses practically no resistant force on the surgeon’s tools, we made use of an inexpensive haptic manipulator and began the integration of this device with navigation technology.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7902570bbee92baecd98e1cc4e94d00fb7e66dcb,https://www.semanticscholar.org/paper/7902570bbee92baecd98e1cc4e94d00fb7e66dcb,How to Train Your Robot,"We developed the How to Train Your Robot curriculum to empower middle school students to become conscientious users and creators of Artificial Intelligence (AI). As AI becomes more embedded in our daily lives, all members of society should have the opportunity to become AI literate. Today, most deployed work in K-12 AI education takes place at strong STEM schools or during extracurricular clubs. But, to promote equity in the field of AI, we must also design curricula for classroom use at schools with limited resources. How to Train Your Robot leverages a low-cost ($40) robot, a block-based programming platform, novice-friendly model creation tools, and hands-on activities to introduce students to machine learning. During the summer of 2020, we trained in-service teachers, primarily from Title 1 public schools, to deliver a five-day, online version of the curriculum to their students. In this work, we describe how students’ self-directed final projects demonstrate their understanding of technical and ethical AI concepts. Students successfully selected project ideas, taking the strengths and weaknesses of machine learning into account, and implemented an array of projects about everything from entertainment to science. We saw that students had the most difficulty designing mechanisms to respond to user feedback after deployment. We hope this work inspires future AI curricula that can be used in middle school classrooms.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7a49c9f6907178a8bdedfe0fb1040d3db0e1fc57,https://www.semanticscholar.org/paper/7a49c9f6907178a8bdedfe0fb1040d3db0e1fc57,Energy Efficiency and Security for Embedded AI: Challenges and Opportunitissses,"Gigantic rates of data production in the era of Big Data, Internet of Thing (IoT), and Smart Cyber Physical Systems (CPS) pose incessantly escalating demands for massive data processing, storage, and transmission while continuously interacting with the physical world under unpredictable, harsh, and energy-/power-constrained scenarios. Therefore, such systems need to support not only the high-performance capabilities under tight power/energy envelop, but also need to be intelligent/cognitive and robust. This has given rise to a new age of Machine Learning (and, in general Artificial Intelligence) at different levels of the computing stack, ranging from Edge and Fog to the Cloud. In particular, Deep Neural Networks (DNNs) have shown tremendous improvement over the past 6-8 years to achieve a significantly high accuracy for a certain set of tasks, like image classification, object detection, natural language processing, and medical data analytics. However, these DNN require highly complex computations, incurring huge processing, memory, and energy costs. To some extent, Moore’s Law help by packing more transistors in the chip. However, at the same time, every new generation of device technology faces new issues and challenges in terms of energy efficiency, power density, and diverse reliability threats. These technological issues and the escalating challenges posed by the new generation of IoT and CPS systems force to rethink the computing foundations, architectures and the system software for embedded intelligence. Moreover, in the era of growing cyber-security threats, the intelligent features of a smart CPS and IoT system face new type of attacks, requiring novel design principles for enabling Robust Machine Learning. In my research group, we have been extensively investigating the foundations for the next-generation energy-efficient and robust AI computing systems while addressing the above-mentioned challenges across the hardware and software stacks. In this talk, I will present different design challenges for building highly energy-efficient and robust machine learning systems for the Edge, covering both the efficient software and hardware designs. After presenting a quick overview of the design challenges, I will present the research roadmap and results from our Brain-Inspired Computing (BrISC) project, ranging from neural processing with specialized machine learning hardware to efficient neural architecture search algorithms, covering both fundamental and technological challenges, which enable new opportunities for improving the area, power/energy, and performance efficiency of systems by orders of magnitude. Towards the end, I will provide a quick overview of different reliability and security aspects of the machine learning systems deployed in Smart CPS and IoT, specifically at the Edge. This talk will pitch that a cross-layer design flow for machine learning/AI, that jointly leverages efficient optimizations at different software and hardware layers, is a crucial step towards enabling the wide-scale deployment of resource-constrained embedded AI systems like UAVs, autonomous vehicles, Robotics, IoT-Healthcare / Wearables, Industrial-IoT, etc.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
eff83f213e3355e5d8ed0dc50285c3c487dfb2c3,https://www.semanticscholar.org/paper/eff83f213e3355e5d8ed0dc50285c3c487dfb2c3,Automated weed detection system in smart farming for developing sustainable agriculture,,International Journal of Environmental Science and Technology,2021,10.1007/s13762-021-03606-6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
487a2f0fda790ff61c69331a3d69c8d1f3a80488,https://www.semanticscholar.org/paper/487a2f0fda790ff61c69331a3d69c8d1f3a80488,Recent Scope for AI in the Food Production Industry Leading to the Fourth Industrial Revolution,"In today's situation, Artificial intelligence and computer vision collectively join together to analyze the big data obtained from predicted models. The role of AI in the agri-based food industry helps the stakeholders to access and monitor the supply chain. The phenomenon of applying AI and computer vision in the food industry would improve the entire operations. This research paper tries to provide an assisting model for farmers in food-processing and agriculture through the state-of-the-art method. Several concepts related to sustainability in food processing have been estimated through machine learning, and the deep learning model as a worldwide concept. The demand for the usage of AI and computer vision in the Ag-TECH industry has increased which impacts sustainable food production to feed the future. Certain implications have been suggested for real-time monitoring of the farming process, politics behind sustainable food production, and investment which is the main game-player in the present situation. The 4th Industrial Revolution [IR-4.0] was ushered in by the deployment of computer vision and AI in the food business, with computer vision robotics playing a crucial role in ensuring sustainable food production.",Webology,2021,10.14704/web/v18i2/web18375,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
47d13e0c817235413462b922f5f14e76d3b14bf5,https://www.semanticscholar.org/paper/47d13e0c817235413462b922f5f14e76d3b14bf5,Model Diet: A Simple yet Effective Model Compression for Vision Tasks,"Computer vision coupled with machine learning algorithms has greatly helped mobile robotic platforms become more intelligent and capable of performing in the real world. Specifically, Convolutional Neural Networks (CNNs) have achieved a high accuracy on a range of visual perception tasks (e.g., object detection, classification, segmentation, and similar others). One of the bottlenecks in CNNs is their high computational requirement. This makes most of them not easily deployable on robotic platforms, since their on-board computational power is limited. Recently, Involution successfully reduced the number of parameters of CNNs by replacing all the 3 × 3 convolution kernels with involution kernels, which use 1 × 1 convolution for the kernel generation. Filter pruning methods have also successively reduced the number of parameters in CNNs. Notably, however, Involution has reshaping layers and the kernel size is unknown when loading the pre-trained model. In this paper, we propose a pruning method named Model Diet that can be applied to Involution and other CNNs. We present experimental results showing that it has better results compared with randomly initialized weights.","2021 21st International Conference on Control, Automation and Systems (ICCAS)",2021,10.23919/ICCAS52745.2021.9649988,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fd94e07e5f9c5e255797344be4f08f826f3f44ce,https://www.semanticscholar.org/paper/fd94e07e5f9c5e255797344be4f08f826f3f44ce,Ridon Vehicle: Drive-by-Wire System for Scaled Vehicle Platform and Its Application on Behavior Cloning,"For autonomous driving research, using a scaled vehicle platform is a viable alternative compared to a full-scale vehicle. However, using embedded solutions such as small robotic platforms with differential driving or radio-controlled (RC) car-based platforms can be limiting on, for example, sensor package restrictions or computing challenges. Furthermore, for a given controller, specialized expertise and abilities are necessary. To address such problems, this paper proposes a feasible solution, the Ridon vehicle, which is a spacious ride-on automobile with high-driving electric power and a custom-designed drive-by-wire system powered by a full-scale machine-learning-ready computer. The major objective of this paper is to provide a thorough and appropriate method for constructing a cost-effective platform with a drive-by-wire system and sensor packages so that machine-learning-based algorithms can be tested and deployed on a scaled vehicle. The proposed platform employs a modular and hierarchical software architecture, with microcontroller programs handling the low-level motor controls and a graphics processing unit (GPU)-powered laptop computer processing the higher and more sophisticated algorithms. The Ridon vehicle platform is validated by employing it in a deep-learning-based behavioral cloning study. The suggested platform’s affordability and adaptability would benefit broader research and the education community.",Energies,2021,10.3390/en14238039,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8e881a01c3b7c698bc29fc28af5b8e919f5c3038,https://www.semanticscholar.org/paper/8e881a01c3b7c698bc29fc28af5b8e919f5c3038,RF Controlled Solar Based Robotic Drone,"In this paper, we present the recent research in the development of AL drones by using an algorithm of deep learning (A subset of machine learning algorithms) and computer vision (CV) based on a drone camera, the study explores the potential use of drones not only in searching the victim person, animals, important properties during natural calamities like flood, volcanoes eruption, forest fire, cyclones, and earthquake, etc. but also it insights into much useful information to collect to ensure the future safety by using a machine learning algorithm. Preparing for and responding to disasters is a major logistical challenge. ""Actionable Data"" is the next powerful approach and reliable to the Al drones. As we know that drones often generate a huge amount of data – sometimes it produces a large amount of data that we can’t handle at a time. Unmanned Aerial Vehicles (UAVs) are the only possible approach that can add value to our AI-based drone without putting any additional efforts to make UAVs more frequent automation. During search and rescue operations, UAVs played a crucial role over humans. As we know that UAVs can be sent to any desired location without any prior knowledge about the exact conditions in the target area. In this paper, we worked to reduce the number of factors that cause hindrance to the effective deployment of UAVs and increasing the duration of flight duration for the rescue operation. We also use the first Indian microprocessor Shakti class c which is installed in the board Artix-7.",2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),2021,10.1109/icacite51222.2021.9404629,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c9d1eec42696e977e229058135886449961db4df,https://www.semanticscholar.org/paper/c9d1eec42696e977e229058135886449961db4df,Prescient teleoperation of humanoid robots,"Humanoid robots could be versatile and intuitive human avatars that operate remotely in inaccessible places: the robot could reproduce in the remote location the movements of an operator equipped with a wearable motion capture device while sending visual feedback to the operator. While substantial progress has been made on transferring (“retargeting”) human motions to humanoid robots, a major problem preventing the deployment of such systems in real applications is the presence of communication delays between the human input and the feedback from the robot: even a few hundred milliseconds of delay can irreversibly disturb the operator, let alone a few seconds. To overcome these delays, we introduce a system in which a humanoid robot executes commands before it actually receives them, so that the visual feedback appears to be synchronized to the operator, whereas the robot executed the commands in the past. To do so, the robot continuously predicts future commands by querying a machine learning model that is trained on past trajectories and conditioned on the last received commands. In our experiments, an operator was able to successfully control a humanoid robot (32 degrees of freedom) with stochastic delays up to 2 seconds in several whole-body manipulation tasks, including reaching different targets, picking up, and placing a box at distinct locations.",ArXiv,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
482eb4597d6580afbac02458e4f89adbbc31907f,https://www.semanticscholar.org/paper/482eb4597d6580afbac02458e4f89adbbc31907f,Improving Users Engagement Detection using End-to-End Spatio-Temporal Convolutional Neural Networks,"The ability to infer latent behaviours such as the degree of engagement of humans interacting with social robots is still considered one challenging task in the human-robot interaction (HRI) field. Data-driven techniques based on machine learning were recently shown to be a promising approach for tackling the users' engagement detection problem, however, the resolution often involves multiple consecutive stages. This in return makes these techniques either incapable of capturing the users' engagement especially in a dynamic environment or un-deployable because of their inability to track engagement in real-time. This study is based on a data-driven framework, and we propose an end-to-end technique based on a unique 3D convolutional neural network architecture. Our proposed framework was trained and evaluated using a real-life dataset of users interacting spontaneously with a social robot in a dynamic environment. The framework has shown promising results over three different evaluation metrics when compared against three baseline approaches from the literature with an F1-score of 76.72. Additionally, our framework has achieved a resilient real-time performance of 25 Hz.",HRI,2021,10.1145/3434074.3447157,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f093e35c7bbc708fe04fc5d87844013f2790d47e,https://www.semanticscholar.org/paper/f093e35c7bbc708fe04fc5d87844013f2790d47e,Digital surgery for gastroenterological diseases,"Advances in machine learning, computer vision and artificial intelligence methods, in combination with those in processing and cloud computing capability, portend the advent of true decision support during interventions in real-time and soon perhaps in automated surgical steps. Such capability, deployed alongside technology intraoperatively, is termed digital surgery and can be delivered without the need for high-end capital robotic investment. An area close to clinical usefulness right now harnesses advances in near infrared endolaparoscopy and fluorescence guidance for tissue characterisation through the use of biophysics-inspired algorithms. This represents a potential synergistic methodology for the deep learning methods currently advancing in ophthalmology, radiology, and recently gastroenterology via colonoscopy. As databanks of more general surgical videos are created, greater analytic insights can be derived across the operative spectrum of gastroenterological disease and operations (including instrumentation and operative step sequencing and recognition, followed over time by surgeon and instrument performance assessment) and linked to value-based outcomes. However, issues of legality, ethics and even morality need consideration, as do the limiting effects of monopolies, cartels and isolated data silos. Furthermore, the role of the surgeon, surgical societies and healthcare institutions in this evolving field needs active deliberation, as the default risks relegation to bystander or passive recipient. This editorial provides insight into this accelerating field by illuminating the near-future and next decade evolutionary steps towards widespread clinical integration for patient and societal benefit.",World journal of gastroenterology,2021,10.3748/wjg.v27.i42.7240,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6c96b42d932ba87490327b4613246259dae21638,https://www.semanticscholar.org/paper/6c96b42d932ba87490327b4613246259dae21638,Learnable state estimator for multi-legged robot,"The enhanced rough terrain traversability of the multilegged robots is directly connected to their relative complex morphology in comparison to the wheeled or tracked robots. However, the efficient rough terrain locomotion requires reliable contact sensing necessary to adapt robot behaviour and cope with the terrain irregularities. In this thesis, the learnable leg contact estimator for an affordable hexapod robot with positional feedback-only has been developed and experimentally evaluated. In particular, three light-weight machine learning approaches, namely Ordinary Least Squares regression, Ordinary Least Squares regression with second-order polynomial features and three-layer feed-forward neural network with the Rectified Linear Unit activation function, are used to predict leg position based on the sequence of the measured and set positions of a particular leg. The proposed methods are investigated experimentally w.r.t. the model precision, robustness to the parameter changes, size of the training set and computational requirements and experimentally deployed to the SCARAB hexapod platform to detect foot-contact. Additionally, the statistical properties of training datasets, the regressors sensitivity to the leg movement speed, the effect of the measured and the set positions on the prediction are examined to explain the unsuccessful deployment. Finally, the artificial foot contact scenarios have been designed to examine how the regressors react to arbitrary collisions. The collected results show that despite the promising performance in the initial scenarios, proposed regressors are not suitable for the contact detections since the regressors either overfit or provide unreliable predictions.",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6c96b42d932ba87490327b4613246259dae21638,https://www.semanticscholar.org/paper/6c96b42d932ba87490327b4613246259dae21638,Learnable state estimator for multi-legged robot,"The enhanced rough terrain traversability of the multilegged robots is directly connected to their relative complex morphology in comparison to the wheeled or tracked robots. However, the efficient rough terrain locomotion requires reliable contact sensing necessary to adapt robot behaviour and cope with the terrain irregularities. In this thesis, the learnable leg contact estimator for an affordable hexapod robot with positional feedback-only has been developed and experimentally evaluated. In particular, three light-weight machine learning approaches, namely Ordinary Least Squares regression, Ordinary Least Squares regression with second-order polynomial features and three-layer feed-forward neural network with the Rectified Linear Unit activation function, are used to predict leg position based on the sequence of the measured and set positions of a particular leg. The proposed methods are investigated experimentally w.r.t. the model precision, robustness to the parameter changes, size of the training set and computational requirements and experimentally deployed to the SCARAB hexapod platform to detect foot-contact. Additionally, the statistical properties of training datasets, the regressors sensitivity to the leg movement speed, the effect of the measured and the set positions on the prediction are examined to explain the unsuccessful deployment. Finally, the artificial foot contact scenarios have been designed to examine how the regressors react to arbitrary collisions. The collected results show that despite the promising performance in the initial scenarios, proposed regressors are not suitable for the contact detections since the regressors either overfit or provide unreliable predictions.",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
137873d22d49760b9469c63dad4211f3083653b8,https://www.semanticscholar.org/paper/137873d22d49760b9469c63dad4211f3083653b8,"Real Time Cloud-Based Automation for Formation Evaluation Optimization, Risk Mitigation and Decarbonization","
 Recent developments in artificial intelligence (AI) have enabled upstream exploration and production companies to make better, faster and accurate decisions at any stage of well construction, while reducing operational expenditure and risk, increasing logistic efficiencies. The achieved optimization through digitization at the wellsite will significantly reduce the carbon emissions per well drilled when fully embraced by the industry. In addition, an industry pushed to drill in more challenging environments, they must embrace safer and more practical methods.
 An increase in prediction techniques, to generate synthetic formation evaluation wellbore logs, has unlocked the ability to implement a combination of predictive and prescriptive analytics with petrophysical and geochemical workflows in real time. The foundation of the real time automation is based on advanced machine learning (ML) techniques that are deployed via cloud connectivity.
 Three levels of logging precision are defined in the automated workflow based on the data inputs and machine learning models. The first level is the forecasting ahead of the bit that implements advanced machine learning using historical data, aiding proactive operational decisions. The second level has improved precision by incorporating real time drilling measurements and providing a credible contingency to for wellbore logging program. The last level incorporates petrophysical workflows and geochemical measurements to achieve the highest precision for logging prediction in the industry. Supervised and unsupervised machine learning models are presented to demonstrate the path for automation.
 Precision above 95% in the real time automated workflows was achieved with a combination of physics and advanced machine learning models. The automation of the workflow has assisted with optimization of logging programs utilizing technology with costly lost in hole charges and high rate of tool failures in offshore operations.
 The optimization has reduced the requirement for logistics associated with logging and eliminated the need for radioactive sources and lithium batteries.
 Highest precision in logging prediction has been achieved through an automated workflow for real time operations. In addition, the workflow can also be deployed with robotics technology to automate sample collection, leading to increased efficiencies.","Day 1 Tue, September 07, 2021",2021.0,10.2118/205402-ms,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
555a59cc7bfe162d01cb15a937bdafc4aa7b9551,https://www.semanticscholar.org/paper/555a59cc7bfe162d01cb15a937bdafc4aa7b9551,IoT Based on-the-fly Visual Defect Detection in Railway Tracks,"Railway transportation requires constant inspections and immediate maintenance to ensure public safety. Traditional manual inspections are not only time consuming, and expensive, but the accuracy of defect detection is also subjected to human expertise and efficiency at the time of inspection. Computing and Robotics offer automated IoT based solutions where robots could be deployed on rail-tracks and hard to reach areas, and controlled from control rooms to provide faster inspection. In this paper, a novel automated system based on robotics and visual inspection is proposed. The system provides local image processing while inspecting, cloud storage of information that consist of images of the defected railway tracks only, and robot localization within a range of 3-6 inches. The proposed system utilizes state of the art Machine Learning system and applies it on the images obtained from the tracks in order to classify them as normal or suspicious. Such locations are then marked and more careful inspection can be performed by a dedicated operator with very few locations to inspect (as opposed to the full track).","2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)",2020.0,10.1109/ICIoT48696.2020.9089560,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
404e87dfe59da0cca58db5f276360716f62f9ff0,https://www.semanticscholar.org/paper/404e87dfe59da0cca58db5f276360716f62f9ff0,Automorphing Kernels for Nonstationarity in Mapping Unstructured Environments,"In order to deploy robots in previously unseen and unstructured environments, the robots should have the capacity to learn on their own and adapt to the changes in the environments. For instance, in mobile robotics, a robot should be able to learn a map of the environment from data itself without the intervention of a human to tune the parameters of the model. To this end, leveraging the latest developments in automatic machine learning (AutoML), probabilistic programming, and statistical sampling, under the Hilbert mapping framework which can represent the occupancy of the environment as a continuous function of locations, we formulate a Bayesian framework to learn all parameters of the map. Crucially, this way, the robot is capable of learning the optimal shapes and placement of the kernels in Hilbert maps by merely embedding high-level human knowledge of the problem by means of prior probability distributions. Since the proposed framework employs stochastic variational inference, the model learns tens of thousands of parameters within minutes in both big data and data-scarce regimes. Experiments conducted on simulated and real-world datasets in static and dynamic environments indicate the proposed method significantly outperforms existing stationary occupancy mapping techniques, verifying the importance of learning the interdependent position-shape relationship of kernels alongside other model parameters.",CoRL,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
81be6cbe3c26a2e0070591fd591bcb9b4b4a4b1f,https://www.semanticscholar.org/paper/81be6cbe3c26a2e0070591fd591bcb9b4b4a4b1f,Bayesian Modeling for Optimization and Control in Robotics,"Robotics has the potential to be one of the most revolutionary technologies in human history. The impact of cheap and 
potentially limitless manpower could have a profound influence on our everyday life and overall onto our society. As 
envisioned by Iain M. Banks, Asimov and many other science fictions writers, the effects of robotics on our society might 
lead to the disappearance of physical labor and a generalized increase of the quality of life. However, the large-scale 
deployment of robots in our society is still far from reality, except perhaps in a few niche markets such as manufacturing. 
One reason for this limited deployment of robots is that, despite the tremendous advances in the capabilities of the 
robotic hardware, a similar advance on the control software is still lacking. The use of robots in our everyday life is still 
hindered by the necessary complexity to manually design and tune the controllers used to execute tasks. As a result, 
the deployment of robots often requires lengthy and extensive validations based on human expert knowledge, which 
limit their adaptation capabilities and their widespread diffusion. In the future, in order to truly achieve an ubiquitous 
robotization of our society, it is necessary to reduce the complexity of deploying new robots in new environments and 
tasks. 
The goal of this dissertation is to provide automatic tools based on Machine Learning techniques to simplify and 
streamline the design of controllers for new tasks. In particular, we here argue that Bayesian modeling is an important tool 
for automatically learning models from raw data and properly capture the uncertainty of the such models. Automatically 
learning models however requires the definition of appropriate features used as input for the model. Hence, we present 
an approach that extend traditional Gaussian process models by jointly learning an appropriate feature representation 
and the subsequent model. By doing so, we can strongly guide the features representation to be useful for the subsequent 
prediction task. 
A first robotics application where the use of Bayesian modeling is beneficial is the accurate learning of complex dynamics models. For highly non-linear robotic systems, such as in presence of contacts, the use of analytical system 
identification techniques can be challenging and time-consuming, or even intractable. We introduce a new approach for 
learning inverse dynamics models exploiting artificial tactile sensors. This approach allows to recognize and compensate 
for the presence of unknown contacts, without requiring a spatial calibration of the tactile sensors. We demonstrate 
on the humanoid robot iCub that our approach outperforms state-of-the-art analytical models, and when employed in 
control tasks significantly improves the tracking accuracy. 
A second robotics application of Bayesian modeling is automatic black-box optimization of the parameters of a controller. When the dynamics of a system cannot be modeled (either out of complexity or due to the lack of a full state 
representation), it is still possible to solve a task by adapting an existing controller. The approach used in this thesis is 
Bayesian optimization, which allows to automatically optimize the parameters of the controller for a specific task. We 
evaluate and compare the performance of Bayesian optimization on a gait optimization task on the dynamic bipedal 
walker Fox. Our experiments highlight the benefit of this approach by reducing the parameters tuning time from weeks 
to a single day. 
In many robotic application, it is however not possible to always define a single straightforward desired objective. 
More often, multiple conflicting objectives are desirable at the same time, and thus the designer needs to take a decision 
about the desired trade-off between such objectives (e.g., velocity vs. energy consumption). One framework that is 
useful to assist in this decision making is the multi-objective optimization framework, and in particular the definition of 
Pareto optimality. We propose a novel framework that leverages the use of Bayesian modeling to improve the quality 
of traditional multi-objective optimization approaches, even in low-data regimes. By removing the misleading effects 
of stochastic noise, the designer is presented with an accurate and continuous Pareto front from which to choose the 
desired trade-off. Additionally, our framework allows the seamless introduction of multiple robustness metrics which can 
be considered during the design phase. These contributions allow an unprecedented support to the design process of 
complex robotic systems in presence of multiple objective, and in particular with regards to robustness. 
The overall work in this thesis successfully demonstrates on real robots that the complexity of deploying robots to solve 
new tasks can be greatly reduced trough automatic learning techniques. We believe this is a first step towards a future 
where robots can be used outside of closely supervised environments, and where a newly deployed robot could quickly 
and automatically adapt to accomplish the desired tasks.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f271590c8c74353967d66333c03e2851207915d5,https://www.semanticscholar.org/paper/f271590c8c74353967d66333c03e2851207915d5,Artificial Intelligence in Pharmacovigilance: Do We Need Explainability?,"With increasing deployment of complex and opaque machine learning algorithms (black boxes) to make decisions in areas that profoundly affect individuals such as underwriting, judicial sentencing, and robotic driving, are increasing calls for explanations of how they make these decisions to assure that they are accurate, objective, and fair. Black boxes are also being increasingly explored and deployed in real world pharmacovigilance, so an understandable question is whether such explainability is important in this domain as well. ""Explainable artificial intelligence (AI)"" refers to a set of tools that aim to provide understandable approximations, hypothesis, or more precise traces of the ""inner thoughts"" of black boxes. We consider whether and how general arguments made for explainable AI, such as building trust and gaining scientific insights, apply to pharmacovigilance including an explication of the limitations in these arguments and the methods themselves. Given the multiple application domains within pharmacovigilance, the answer to the question is situation dependent. If the field of explainable AI advances to the point of consistently providing high quality explanations as testable hypothesis, explainable artificial intelligence should be a credible addition to the pharmacovigilance toolkit for model development, signal management and clinical pharmacovigilance. Its incremental contribution to broad trust building per se, though widely touted in general, is a dubious argument for pharmacovigilance. This article is protected by copyright. All rights reserved.",Pharmacoepidemiology and drug safety,2022.0,10.1002/pds.5501,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b093b90c96d4d554acd9e648c3f34b8634d4a176,https://www.semanticscholar.org/paper/b093b90c96d4d554acd9e648c3f34b8634d4a176,"Design, Development, and a Pilot Study of a Low-Cost Robot for Child–Robot Interaction in Autism Interventions","Socially assistive robots are widely deployed in interventions with children on the autism spectrum, exploiting the benefits of this technology in social behavior intervention plans, while reducing their autistic behavior. Furthermore, innovations in modern technologies such as machine learning enhance these robots with great capabilities. Since the results of this implementation are promising, their total cost makes them unaffordable for some organizations while the needs are growing progressively. In this paper, a low-cost robot for autism interventions is proposed, benefiting from the advantages of machine learning and low-cost hardware. The mechanical design of the robot and the development of machine learning models are presented. The robot was evaluated by a small group of educators for children with ASD. The results of various model implementations, together with the design evaluation of the robot, are encouraging and indicate that this technology would be advantageous for deployment in child–robot interaction scenarios.",Multimodal Technologies and Interaction,2022.0,10.3390/mti6060043,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9d0aa40f942026cb9542db9ea27678e4b0fe269f,https://www.semanticscholar.org/paper/9d0aa40f942026cb9542db9ea27678e4b0fe269f,Leaf Disease Detection and Spray Pesticide Robot using Image Processing,"Abstract: India is a land of agriculture and mainly known for growing variety of crops. Around half of the population in India depend on agriculture. Diseases to the crops may affect the livelihood of the farmers. In order to overcome this major problem, a robot that detects the leaf disease using image processing and Machine learning is deployed. This robot also monitors the field condition such as quality of crops and sprays the required amount pesticides for achieving the good yield in agriculture. The robot is designed using an advanced processor known as yolov5 which is integrated with machine learning model. The machine learning model with Image processing is trained with feature extraction, Segmentation and classification of disease using image processing.",International Journal for Research in Applied Science and Engineering Technology,2022.0,10.22214/ijraset.2022.45486,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a838297885376346db0603db8b929f6ea12c2e8c,https://www.semanticscholar.org/paper/a838297885376346db0603db8b929f6ea12c2e8c,Morse Graphs: Topological Tools for Analyzing the Global Dynamics of Robot Controllers,"Understanding the global dynamics of a robot controller, such as identifying attractors and their regions of attraction (RoA), is important for safe deployment and synthesizing more effective hybrid controllers. This paper proposes a topological framework to analyze the global dynamics of robot controllers, even data-driven ones, in an effective and explainable way. It builds a combinatorial representation representing the underlying system’s state space and non-linear dynamics, which is summarized in a directed acyclic graph, the Morse graph. The approach only probes the dynamics locally by forward propagating short trajectories over a state-space discretization, which needs to be a Lipschitz-continuous function. The framework is evaluated given either numerical or data-driven controllers for classical robotic benchmarks. It is compared against established analytical and recent machine learning alternatives for estimating the RoAs of such controllers. It is shown to outperform them in accuracy and efficiency. It also provides deeper insights as it describes the global dynamics up to the discretization’s resolution. This allows to use the Morse graph to identify how to synthesize controllers to form improved hybrid solutions or how to identify the physical limitations of a robotic system.",ArXiv,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
16437fd197d5dc00ea605220f3c71c612cd780e0,https://www.semanticscholar.org/paper/16437fd197d5dc00ea605220f3c71c612cd780e0,AI as a leader - what individual factors influence the acceptance of AI applications that take on leadership tasks?,"In times of digital transformation and the rise of Artificial Intelligence (AI), there is a constant power struggle between technology and humans. Due to the advancing development of digitalization, Artificial Intelligence (AI) is no longer a future version. Through various methods, such as machine learning, it is now already possible to work with a large amount of data. The goal of AI development is to support people in the best possible way in both professional and private contexts (Buxmann & Schmidt, 2018). It is already capable of relieving leaders in a company, for example, by allowing routine, steering and/or deployment tasks to be taken over by AI applications, so that leaders have more time for their employees and can focus on the strategic development of their own area of responsibility (Manyika et al., 2017; Offensive Mittelstand, 2018). In the case of an AI manager, his or her successful integration will ultimately depend on whether employees and even other human managers will accept an algorithm's instructions (Sahota & Ashley, 2019). It will be critical to the subsequent successful implementation of AI as a leader to determine what application-specific concerns exist and what specific expectations are placed on the design. Therefore the research question is asked: What individual factors of human leaders and their employees influence the acceptance of AI as a leader? To answer the question, four hypotheses are operationalized in an online survey with N=74 that collects data on leaders' and employees´ acceptance and expectations of AI as a leader. The questionnaire is based on literature and already established instruments. To survey the acceptance of the subjects, the technology acceptance model (TAM) proposed by Davis (1985) is followed by asking the perceived usefulness (PU) and the perceived ease of use (PEU). In the absence of concrete AI applications that embody the identity of an executive, three use cases from the corporate landscape are used as templates for three scenarios (digital cognitive assistant in staff recruitment, in supervision in form of a smart screen and a physical autonomous system in a form of a robot). It is found that technology affinity as well as commitment have an impact on the acceptance of AI leaders. Technology-related factors predicted higher acceptance for an AI leader that is a cognitive assistance in supervision. In this case, participants who indicated more technological expertise or involvement in AI activities perceived AI leaders as easier to use. As expected, the effect of age on perceived ease of use was mediated by technology affinity (for all scenarios and aggregated), such that older respondents had lower technology affinity and thus lower perceptions of the ease of use of AI leaders. In addition, whether the user had managerial responsibilities or not did not matter for acceptance. Most respondents were convinced that AI-powered leadership will change organizations in terms of new job profiles and new skills, however, they did not believe in a radical transformation any time soon. The obligatory requirements are to work as transparently as possible. The first step has been taken, which now needs to be confirmed in a broad-based study.",AHFE International,2022.0,10.54941/ahfe1002233,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
08e9269bc45884ae6254ad0547ce6d66698add51,https://www.semanticscholar.org/paper/08e9269bc45884ae6254ad0547ce6d66698add51,FoReCo: a forecast-based recovery mechanism for real-time remote control of robotic manipulators,"—Wireless communications represent a game changer for future manufacturing plants, enabling ﬂexible production chains as machinery and other components are not restricted to a location by the rigid wired connections on the factory ﬂoor. However, the presence of electromagnetic interference in the wireless spectrum may result in packet loss and delay, making it a challenging environment to meet the extreme relia- bility requirements of industrial applications. In such conditions, achieving real-time remote control, either from the Edge or Cloud, becomes complex. In this paper, we investigate a forecast-based recovery mechanism for real-time remote control of robotic manipulators (FoReCo) that uses Machine Learning (ML) to infer lost commands caused by interference in the wireless channel. FoReCo is evaluated through both simulation and experimentation in interference prone IEEE 802.11 wireless links, and using a commercial research robot that performs pick-and-place tasks. Results show that in case of interference, FoReCo trajectory error is decreased by x18 and x2 times in simulation and experimentation, and that FoReCo is sufﬁciently lightweight to be deployed in the hardware of already used in existing solutions.",IEEE Transactions on Network and Service Management,2022.0,10.1109/tnsm.2022.3173436,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
81ce275dc689b345f002bd71cc9b297eeb2ed142,https://www.semanticscholar.org/paper/81ce275dc689b345f002bd71cc9b297eeb2ed142,R-STDP Spiking Neural Network Architecture for Motion Control on a Changing Friction Joint Robotic Arm,"Neuromorphic computing is a recent class of brain-inspired high-performance computer platforms and algorithms involving biologically-inspired models adopting hardware implementation in integrated circuits. The neuromorphic computing applications have provoked the rise of highly connected neurons and synapses in analog circuit systems that can be used to solve today's challenging machine learning problems. In conjunction with biologically plausible learning rules, such as the Hebbian learning and memristive devices, biologically-inspired spiking neural networks are considered the next-generation neuromorphic hardware construction blocks that will enable the deployment of new analog in situ learning capable and energetic efficient brain-like devices. These features are envisioned for modern mobile robotic implementations, currently challenging to overcome the pervasive von Neumann computer architecture. This study proposes a new neural architecture using the spike-time-dependent plasticity learning method and step-forward encoding algorithm for a self tuning neural control of motion in a joint robotic arm subjected to dynamic modifications. Simulations were conducted to demonstrate the proposed neural architecture's feasibility as the network successfully compensates for changing dynamics at each simulation run.",Frontiers in Neurorobotics,2022.0,10.3389/fnbot.2022.904017,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
864b36489b40f1081e4eab0ec1aa52490acf93f7,https://www.semanticscholar.org/paper/864b36489b40f1081e4eab0ec1aa52490acf93f7,Unreal engine-based photorealistic aerial data generation and unit testing of artificial intelligence algorithms,"A number of real-time object detection, tracking, and autonomy artificial intelligence (AI) and machine learning (ML) algorithms are being proposed for unmanned aerial vehicles (UAVs). A big challenge is can we stress test these algorithms, identify their strengths and weaknesses, and assess if the UAV is safe and trustworthy? The process of collecting real-world UAV data is costly, time consuming, and riddled by lack of quality geospatial ground truth and metadata. Herein, we outline a fully automated framework and work ow to address the above challenges using free or low-cost assets, the photorealistic Unreal Engine (UE), and AirSim aerial platform simulator. Specifically, we discuss the rapid prototyping of an outdoor environment combined with the robotic operating system (ROS) for abstracting UAV data collection, control, and processing. Real and accurate ground truth is collected and metrics are presented for individual frame and entire flight collection evaluation. Metrics recorded and analyzed include percentage of scene mapped, 3D mapping accuracy, time to complete task, object detection and tracking statistics, battery usage, altitude (from ground), collisions, and other statistics. These metrics are computed in general and with respect to context, e.g., clutter, view angle, etc. Overall, the proposed work is an automated way to explore UAV operation before real-world testing or deployment. Promising preliminary results are discussed for an outdoor environment with vegetation, short and long range objects, buildings, people, vehicles, and other features for a UAV performing loitering and interrogation.",Defense + Commercial Sensing,2022.0,10.1117/12.2618812,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4b1489711d6fbbd16c25ec5d6ac4d34df4003464,https://www.semanticscholar.org/paper/4b1489711d6fbbd16c25ec5d6ac4d34df4003464,Distributional Actor-Critic Ensemble for Uncertainty-Aware Continuous Control,"—Uncertainty quantiﬁcation is one of the central challenges for machine learning in real-world applications. In reinforcement learning, an agent confronts two kinds of un- certainty, called epistemic uncertainty and aleatoric uncertainty. Disentangling and evaluating these uncertainties simultaneously stands a chance of improving the agent’s ﬁnal performance, accelerating training, and facilitating quality assurance after deployment. In this work, we propose an uncertainty-aware reinforcement learning algorithm for continuous control tasks that extends the Deep Deterministic Policy Gradient algorithm (DDPG). It exploits epistemic uncertainty to accelerate explo- ration and aleatoric uncertainty to learn a risk-sensitive policy. We conduct numerical experiments showing that our variant of DDPG outperforms vanilla DDPG without uncertainty estimation in benchmark tasks on robotic control and power-grid optimiza- tion.",ArXiv,2022.0,10.48550/arXiv.2207.13730,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0e28b14f57d90fd320909bb3add1ad05cd57d745,https://www.semanticscholar.org/paper/0e28b14f57d90fd320909bb3add1ad05cd57d745,A Resource Recommendation Model for Heterogeneous Workloads in Fog-Based Smart Factory Environment,"The wide deployment of advanced robots with industrial IoT (IIoT) technologies in smart factories generates a large volume of data during production and a wide variety of data processing workloads are launched to maintain productivity and safety of smart manufacture. The emerging fog computing paradigm offers a promising solution to enhancing data processing performance in a smart factory environment while on the other hand brings in new challenges to resource management, which call for a more effective approach for recommending resource configurations to heterogeneous workloads. In this paper, we propose an Optimized Recommendations of Heterogeneous Resource Configurations (ORHRC) model that employs machine learning techniques to provide resource configuration recommendations for the heterogeneous workloads in a fog computing-based smart factory environment. ORHRC learns a recommendation model by leveraging the operating characteristics and execution time of workloads on fog servers with different configurations. We also design a decision model in ORHRC to further improve prediction accuracy and reduce operational overheads. Experiment results show that ORHRC outperforms the state of art configuration recommendation methods in terms of average prediction accuracy. Note to Practitioners—The various data processing workloads in a smart factory environment need to be processed by the computational resources with optimal configurations for meeting their performance requirements. In this paper, we employ machine learning technologies for enabling automatic recommendation of resource configurations to heterogeneous workloads. Specifically, we develop an Optimized Recommendations of Heterogeneous Resource Configurations (ORHRC) model that can identify the optimal resource configurations for various workloads. We also conducted extensive experiments that verify the effectiveness of the proposed ORHRC model.",IEEE Transactions on Automation Science and Engineering,2022.0,10.1109/TASE.2022.3141590,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
79c2fd00f5ec647b5333c4077b9cb5b36a00e7d3,https://www.semanticscholar.org/paper/79c2fd00f5ec647b5333c4077b9cb5b36a00e7d3,Face recognition and real-time tracking system based on convolutional neural network and parallel-cascade PID controller,"The purpose of this research is to develop a high-efficiency, low-cost, and easy-to-use tracking system for vehicles, and it is expected that the system can be extended to areas such as service robots, autonomous driving, and manufacturing. In this paper, we introduced an object detection algorithm based on convolutional neural networks to realize face recognition, which has better efficiency and robustness than traditional machine learning methods. With the concept of edge computing, we deployed the model on the local embedded system to improve the information transmission and security issues of cloud computing. In order to realize the tracking system, this paper builds a mecanum-wheel vehicle with omnidirectional mobility, and proposes a parallel-cascade PID controller architecture based on the mecanum-wheel vehicle. The fixed distance linear tracking control can be realized through the dual-loop feedback control of distance and yaw angle; moreover, the vehicle slipping which is caused by difference rotation speed can be improved. Finally, through algorithm optimization, controller parameter adjustment, and system integration, an omnidirectional mobile vehicle with recognition and tracking functions is realized. The experiment results indicate that the system is stable and robust during actual operation.",Measurement and Control,2022.0,10.1177/00202940221089237,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
21e4e1fbe2305c41661aec768229f5c976cc0252,https://www.semanticscholar.org/paper/21e4e1fbe2305c41661aec768229f5c976cc0252,Bayesian Networks for Brain-Computer Interfaces: A Survey,"Brain-Computer Interface (BCI) is a rapidly developing technology that allows direct communications between the human brain and external devices, such as robotic arms and computers. Bayesian Networks is a powerful tool in machine learning for tackling with problems that requires understanding and modelling the uncertainty and complexity within complex system built by sub-modular components. Therefore, deploying Bayesian Networks in the application of Brain-Computer Interfaces becomes an increasingly popular approach in BCI research. This survey covers related existing works in relatively high-level perspectives, classifies the models and algorithms involved, and also summarizes the application of Bayesian Networks or its variants in the context of Brain-Computer Interfaces.",,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b1740be13a0d3c9c05873dab4f07e5688eb39a98,https://www.semanticscholar.org/paper/b1740be13a0d3c9c05873dab4f07e5688eb39a98,The Role of the Arts and Innovation in Revitalizing Downtown,"Jim has over 20 years’ experience in the development and global deployment of advanced technology products with an emphasis on great customer experiences. Career highlights include holding executive roles in strategic marketing, global product management/development, M&A evaluation/integration, and customer experience oversight for global communication and technology leaders, including Level 3 Communications (now Lumen), Global Crossing Ltd., Silicon Graphics/Cray Research and Soleo Communications. Jim is excited to highlight Immersitech, a local technology start-up focused on machine learning based audio software tools designed to improve the overall quality and engagement levels for providers in the business communications, distance learning and social entertainment markets. the topics of Journal of Manufacturing and Materials Processing. His research is at the intersection of mechanical/industrial engineering and computer science, driven by engineering design, geometric modeling, computer vision, artificial intelligence, and human-computer interaction. His research interests include augmented reality-based human-robot interaction, virtual reality-based manufacturing training, machine learning for understanding manufacturing knowledge and skills, and gamification for training. for more This poster is based on the author's research over the last two years, with a particular emphasis on the application of augmented reality and virtual reality to novel users and K-12 education. The author's various research innovations and attempts to develop systematic STEM curricula cover systematic STEM curricula that incorporate emerging techniques such as augmented/virtual reality-assisted education, occupational training that incorporates cognition awareness, and AI-assisted data science in elementary and secondary education. Three specific research were conducted: Immersive Virtual Reality Training with Error Management for CNC Milling Set-up, System Design of A Human-centered Augmented Reality Robot Programming Interface with Cognition Awareness, and Data Analysis in Machine Learning for K-12 Students. To solve the problems of complex robot programming tasks, we propose an Augmented Reality (AR) based human-robot interface for planning a collision-free path in a complex environment. Current robot programming methods usually require a high level of experience in robot programming, the time-consuming 3D modeling of the working environment for collision detection, and a tedious and inefficient re-planning to adapt to the environment or task changes. In order to address these problems, an end-to-end AR human-robot interface is proposed, which provides a new affordance to users by enabling them to plan the path in the AR environment. A set of user-interactive tools allow users to define and edit waypoints as the high-level guidance and the direct inputs for the toolpath planning package, Kinematics and Dynamics Library (KDL). With the fast sensing of the workspace and accurate rendering, an in-situ simulation module is utilized for collision check and verification by the users’ perception. Users will repeat the process of 1) waypoints definition and editing, and 2) the collision checking and path feasibility verification until a satisfactory path is obtained. Preliminary testing is conducted in a use case with complex obstacles to verify the effectiveness and the efficiency of the proposed interface. Background: Anxiety and distress have shown to exacerbate the experience of pain (Ploghaus et al., 2001) and increase sedatives and opiates dosages required before, during, and after surgical procedures (Ina & Zeev, 1999). Virtual reality (VR) is a relatively new intervention that has been used to promote relaxation and manage perioperative stress by using the principle of distraction (Eijlers et al., 2019; Ganry et al., 2018). Primary objective: To investigate if VR reduces perioperative anxiety levels in children and adults across the departments of Oral and Maxillofacial Surgery and Pediatric Procedural Care Ambulatory Centers at the University of Rochester Medical Center/ Strong Memorial Hospital in Rochester, NY. Method: We will perform a single-center feasibility and acceptability pilot study examining the impact of VR on perioperative distress in pediatric and adult patients by collecting data from 60 children and 30 adults in the span of six months. We will measure pre-operative anxiety using a verbal rating of anxiety for adults during three different time points. For children, we will use the mYPAS Form as a reliable observational measure with four behavior domains. Furthermore, we plan to assess patients’ perception of the VR intervention at the end of the procedure using the Acceptability Intervention Measure (AIM). At the end of the study, we will use a Feasibility of Intervention Measure (FIM) for medical staff at each department to assess their perception of the VR intervention and its ability to be successfully implemented in the hospital workflow. This project develops a noninvasive ultrasound-based imaging modality, which will be applied to assess the viscoelastic properties of hepatic fibrosis. It will be used to monitor in situ changes in biomechanical properties associated with chronic liver injury. Ultrasound being a preferred choice in clinical assessments has been established as an effective modality for monitoring fibrosis progression in patients. Biological tissues being predominantly viscoelastic exhibit frequency dependent shearwave speed and attenuation. Most elastography methodologies estimate the group speed of shear waves but refrain from estimation or measurement of the frequency dependence of shear wave speed or attenuation. Previous studies in our lab have shown, that shear wave propagation in a medium is itself a function of the Acoustic Radiation Force (ARF) push beam geometry and duration. We propose to develop a robust viscoelastic estimator that accounts for variation We develop a pulsed terahertz spectroscopy-based imaging technique to study paraffin-embedded murine pancreatic ductal adenocarcinoma (PDAC) tissues. We employ a novel maximum-likelihood estimation (MLE)-based parameter extraction method to map terahertz markers namely refractive index and absorption coefficient which can reflect the tissue characteristics enabling unbiased and reproducible THz measurements. We report a well-resolved differences between the tumor and healthy pancreas along with an enhanced absorbance in tumor tissue compared to its healthy counterpart. Additionally, we probe untreated and stereotactic body radiotherapy (SBRT) treated PDAC tissue to measure degree of cytotoxic responsivity to such therapies. The research goal of this collaborative work is to explore and demonstrate the use of sub-MHz ultrasound frequencies in combination with super-resolution image reconstruction methods for transcranial brain imaging with sub-millimeter resolution capability. Low-frequency ultrasound allows higher penetration with a possibility of traversing skull layers with reduced losses. Before tissue imaging is commenced, the proposed method first determines an optimal excitation frequency by measuring echoes from skull layers in response to a range of frequencies (e.g., 300 kHz to 800 kHz). During the subsequent imaging procedure, the received echoes from brain tissue are then fit to the imaging model in a least-square sense penalized by a mixed L1 and L2-norm to estimate reflectance coefficients from the brain tissue and recover resolution losses due to the use of frequencies lower than those traditionally used in standard medical ultrasound. The proposed method can increase the imaging resolution up to an order of magnitude as compared to traditional B-mode ultrasound. Preliminary results show that Guitar tablature transcription is an important but understudied problem within the field of music information retrieval. Traditional signal processing approaches offer only limited performance on the task, and there is little acoustic data with transcription labels for training machine-learning models. However, guitar transcription labels alone are more widely available in the form of tablature, which is commonly shared among guitarists online. In this work, a collection of symbolic tablature is leveraged to estimate the pairwise likelihood of notes on the guitar. The output layer of a baseline tablature transcription model is reformulated, such that an inhibition loss can be incorporated to discourage the co-activation of unlikely note pairs. This naturally enforces playability constraints for guitar, and yields tablature which is more consistent with the symbolic data used to estimate pairwise likelihoods. With this methodology, we show that symbolic tablature can be used to shape the distribution of a tablature transcription model’s predictions, even when little acoustic data is available. Automatic Speaker Verification (ASV) systems aim to verify a speaker’s claimed identity through voice. However, voice can be easily forged with replay, text-to-speech (TTS), and voice conversion (VC) techniques, which may compromise ASV systems. Voice anti-spoofing is developed to improve the reliability of speaker verification systems against such spoofing attacks. One main issue of voice anti-spoofing systems is its generalization ability to unseen synthetic attacks, i.e., synthesis methods that are not seen during training of the anti-spoofing models. We propose one-class learning, where the model compacts the distribution of learned representations of bona fide speech while pushing away spoofing attacks to improve the results. Another issue is the robustness to variations of acoustic and telecommunication channels. To alleviate this issue, AI algorithms have been found to learn biases from data. Therefore, it is urgent and vital to identify biases in AI algorithms. However, the previous bias identification pipeline overly relies o",,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a82fab286b9426c591337b7c438c023658ce2d5e,https://www.semanticscholar.org/paper/a82fab286b9426c591337b7c438c023658ce2d5e,A Bayesian Approach to Context-based Recognition of Human Intention for Context-Adaptive Robot Assistance in Space Missions,"Long-duration space exploration (LDSE) missions with a high de-mand for intelligent and adaptable robotic assistance are expected to start in the next decades. Robot assistants deployed on these missions should be able to interpret the intentions of astronaut(s) and provide assistance accordingly, under continuously changing interaction context. This necessitates the development of robust and context-adaptive models for inferring human intentions. However, current state-of-the-art artificial intelligence (AI) algorithms rely on expensive data-driven machine learning (ML) models that provide inferences assuming a set of known contexts. In this paper, we present a context-adaptive hybrid architecture for intelligent and adaptable Human Computer Interaction (HCI). This architecture uses state-of-the-art machine learning models for the recognition of specific contexts within the robot’s environment, and a two-layer Bayesian network to infer human intentions based on the current context. The Bayesian network enables the integration of human expert knowledge about the mission goals within the inference process and can also deal with missing or uncertain information about context. Thus, the proposed framework increases robustness and reduces the need for large and complex (high dimensional) human-robot interaction data, which is unavailable in space appli-cations. Context-adaptive interpretation of human intention would promote an intuitive interaction with robots, which in turn could reduce the overall stress in astronauts while interacting with robot assistants in isolated, confined or extreme (ICE) conditions. of",,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7efc7f1cf2fedde7f3382268482882531b8f2b66,https://www.semanticscholar.org/paper/7efc7f1cf2fedde7f3382268482882531b8f2b66,Robotic Tele-operation Performance Analysis via Digital Twin Simulations,"Tele-operated robots are gaining industrial traction for remote inspection and operation in hazardous environments. While the tele-operated robots may have autonomous loops, they must support closed-loop control commands from human operators. This research demonstration aims to develop a realistic digital twin of the remote inspection environment capturing robots, network transmission, and machine learning models in order to study the complex interactions between these components. Along with this, it provides guidelines for performance tuning over the digital twin in order to optimally use robotic, compute, and network resources. The study of the performance of this digital twin relies on various network parameters, size of data transferred, robot speed and other hyperparameters of machine learning model. This could be used to derive specific values for each of them in order to trade-off deployment accuracy and efficiency.",2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),2022.0,10.1109/COMSNETS53615.2022.9668555,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5a5c88eda3141cb068346c9c321b2ced0957f959,https://www.semanticscholar.org/paper/5a5c88eda3141cb068346c9c321b2ced0957f959,Scene setter presentation: Why does innovation and technology matter to the industry’s future?,"Technology is an enabler, not a metaphysical abstraction that will resolve the energy transition. Many of the technologies our industry is working on could help us reduce potential environmental impacts, advance decarbonisation, and improve efficiency. Some cost-saving examples of innovation include centralising and automating work planning and monitoring in well operations, oil and gas production, and refining through to using Robotic Process Automation to automate manual processes. Revenues can be enhanced by using machine learning and advanced analytics to optimise entire systems from the reservoir through to refineries, enabling intervention ahead of predicted equipment failure, optimising supply and demand functions through trading and shipping. Huge investment is also being placed in carbon reduction technology like satellite imaging and drones. We are finally seeing some momentum in CCUS and hydrogen. Digital technologies enable everything we do, particularly because they can be deployed relatively quickly and low cost. But we need to consider a few things when we consider technology. First, we cannot afford to wait for game-changing technologies to solve climate change. Second, the Paris goals can be met with technologies that exist today, albeit not at the price point we would ideally like. The challenge is pace of deployment, supported by customer demand and regulation. A technology-inclusive approach to policymaking can ensure they are able to use the broadest possible range of emission mitigation solutions possible. By picking winners early in the race and focusing public support on a small number of technologies, we are concentrating risk and increasing chances of failure. To view the video, click on the link to the right.",The APPEA Journal,2022.0,10.1071/aj21456,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2d2480afbd7cddb9a27a0fc34b951ebb679d05a9,https://www.semanticscholar.org/paper/2d2480afbd7cddb9a27a0fc34b951ebb679d05a9,Design Methodology for Deep Out-of-Distribution Detectors in Real-Time Cyber-Physical Systems,"—When machine learning (ML) models are supplied with data outside their training distribution, they are more likely to make inaccurate predictions; in a cyber-physical system (CPS), this could lead to catastrophic system failure. To mitigate this risk, an out-of-distribution (OOD) detector can run in parallel with an ML model and ﬂag inputs that could lead to undesirable outcomes. Although OOD detectors have been well studied in terms of accuracy, there has been less focus on deployment to resource constrained CPSs. In this study, a design methodology is proposed to tune deep OOD detectors to meet the accuracy and response time requirements of embedded applications. The methodology uses genetic algorithms to optimize the detector’s preprocessing pipeline and selects a quantization method that balances robustness and response time. It also identiﬁes several candidate task graphs under the Robot Operating System (ROS) for deployment of the selected design. The methodology is demonstrated on two variational autoencoder based OOD detectors from the literature on two embedded platforms. Insights into the trade-offs that occur during the design process are provided, and it is shown that this design methodology can lead to a drastic reduction in response time in relation to an unoptimized OOD detector while maintaining comparable accuracy.",ArXiv,2022.0,10.48550/arXiv.2207.14694,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d5068fccb87cb3f38d82bd2e978c9ba9d65dc3d0,https://www.semanticscholar.org/paper/d5068fccb87cb3f38d82bd2e978c9ba9d65dc3d0,Research Statement by Hussein Sibai,"Motivation: Autonomy research has made significant progress in solving the most challenging perception, planning, and control problems. Such progress is advancing exciting applications like autonomous driving and robotic surgery. Unfortunately, the resulting systems are also known to be expensive and to have serious faults that led to fatalities. Rigorous and efficient methods for their design, evaluation, and deployment are thus in urgent need. Carefully applied formal approaches that exploit the structure inherent in these systems can transform practice and provide insights for effective fulfillment of that need. I create such approaches while drawing ideas from control theory, formal methods, testing, and machine learning.",,2022.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1733609464826930d98d67e8015edaf0ad8c1d75,https://www.semanticscholar.org/paper/1733609464826930d98d67e8015edaf0ad8c1d75,Pizza Ordering Chatbot Using Amazon Lex,"Abstract: Because of breakthroughs in machine learning and deep learning, which are causing a change in every industry area and managing various types of activities better than people. The majority of monotonous jobs that were formerly performed by humans are now replaced by AI. Every firm is aiming to replace the least skilled labour with AI robots that can do comparable tasks more efficiently, especially when it comes to chatbots. A chatbot is computer software that mimics human interaction by using voice instructions, text dialogues, or both. Chatbots are being employed to address consumer concerns or problems in food delivery app businesses such as Zomato and Swiggy, but are chatbots truly useful in that business model? This business model's target customers are those who don't have time to go outside to obtain food, prefer convenience at home, or are unwilling to endure discomfort, thus their concerns should be resolved in the most convenient way possible. To fulfil the user's request, a chatbot is employed. It is critical for the chatbot to plan how to carry out the task that the user has asked. New tools are available now to create and deploy chatbots; Amazon Lex by AWS is one of them. This project focuses on creating a Pizza Ordering Chatbot using Amazon Lex to help the user order pizza. Keywords: Amazon Lex, Amazon Web Services (AWS), Chatbot",International Journal for Research in Applied Science and Engineering Technology,2022.0,10.22214/ijraset.2022.40861,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
21d7c4a7fc70a18624d653f498891d187b41d7e1,https://www.semanticscholar.org/paper/21d7c4a7fc70a18624d653f498891d187b41d7e1,An Agile Ethical/Legal Model for the International and National Governance of AI and Robotics,"The accelerating pace of emerging technologies such as AI has revealed a total mismatch between existing governmental approaches and what is needed for effective ethical/legal oversight. To address this “pacing gap” the authors proposed governance coordinating committees (GCCs) in 2015 as a new more agile approach for the coordinated oversight of emerging technologies. In this paper, we quickly reintroduce the reasons why AI and robotics require more agile governance, and the potential role of the GCC model for meeting that need. Secondly, we flesh out the roles for government, engineering, and ethics in forcing a comprehensive approach to the oversight of AI/robotics mediated by a GCC. We argue for an international GCC with complementary regional bodies in light of the transnational nature of AI concerns and risks. We also propose a series of new mechanisms for enforcing (directly or indirectly) “soft law” approaches for AI through coordinated institutional controls by insurers, journal publishers, grant funding agencies, courts and governments. The GCC is particularly well-adapted and situated for coordinating this type of enforcement of soft law requirements. Finally, we show how a GCC can support and reinforce the governance initiatives of organizations such as the IEEE, WEF, the Partnership in AI, and various AI research centers. Introduction: The Need for Agile Governance The accelerating pace of emerging technologies such as AI, and the onset of a Fourth Industrial Revolution, has revealed a total mismatch with existing governmental approaches and what is needed for effective ethical/legal oversight (Marchant and Wallach 2016). These emerging technologies exceed the regulatory scope, capabilities and jurisdiction of any one agency or nation. For example, AI raises ethical, legal and social concerns in need of governance relating to military use, safety, privacy, transparency, bias, unCopyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. fair business practices, antitrust, human enhancement, criminal justice, impacts on personal, family and societal relationships, economic equality, technological unemployment, existential risk, and no doubt many others. These diverse issues span many different industries, regulatory authorities, non-governmental organization, experts and other stakeholders. While these concerns raise distinct issues that often must be addressed in their own way, they are also connected in that they relate to the same underlying technologies and therefore necessitate a more holistic approach. In addition to the complexity of emerging technologies such as AI, the pace at which they are being developed also presents a major obstacle to traditional government regulation. AI is developing at an accelerating trajectory, surprising even many AI experts about its recent speed and impact (Executive Office of the President, 2016). At the same time, our traditional governmental institutions of legislation, regulation and judicial review are slowing down rather than speeding up, creating the “pacing problem” (Marchant 2011). To address these governance challenges, the authors proposed governance coordinating committees (GCCs) in 2015 as a new more agile approach for the coordinated oversight of emerging technologies such as AI (Marchant and Wallach 2015). In addition, we further proposed that pilot projects be started for AI/robotics and also for synthetic biology. The selection of these fields for pilot projects was occasioned by the fact that AI/robotics and synthetic biology are relatively new fields of research, largely unencumbered by rules and regulations. Much has happened since our initial proposal. Machine learning approaches have led to breakthroughs in AI, and CRISPR/Cas9 has speeded up gene editing and in turn the development of genomic products and synthetic organisms. Governments have taken notice and are studying ways to regulate AI and genomics. In addition, many existing and new non-governmental organizations (NGOs or Civil Society), alliances, and research centers have sprung up to address potential benefits, societal impacts, risks and dangers posed by the deployment of AI and gene editing. In this paper, we expand on our initial proposal to describe how a GCC, perhaps at an international level, could help to break the current logjam with respect to agile and effective governance of AI.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ebd8164077eaea87e92fb7b8870a0b7fc99f5a62,https://www.semanticscholar.org/paper/ebd8164077eaea87e92fb7b8870a0b7fc99f5a62,Convolutional Autoencoder for Feature Extraction in Tactile Sensing,"A common approach in the field of tactile robotics is the development of a new perception algorithm for each new application of existing hardware solutions. In this letter, we present a method of dimensionality reduction of an optical-based tactile sensor image output using a convolutional neural network encoder structure. Instead of using various complex perception algorithms, and/or manually choosing task-specific data features, this unsupervised feature extraction method allows simultaneous online deployment of multiple simple perception algorithms on a common set of black-box features. The method is validated on a set of benchmarking use cases. Contact object shape, edge position, orientation, and indentation depth are estimated using shallow neural networks and machine learning models. Furthermore, a contact force estimator is trained, affirming that the extracted features contain sufficient information on both spatial and mechanical characteristics of the manipulated object.",IEEE Robotics and Automation Letters,2019.0,10.1109/LRA.2019.2927950,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6a76c06c8c53b65feaa7c7f747dbe6972dae1683,https://www.semanticscholar.org/paper/6a76c06c8c53b65feaa7c7f747dbe6972dae1683,CNN Architectures for Geometric Transformation-Invariant Feature Representation in Computer Vision: A Review,,SN Comput. Sci.,2021.0,10.1007/S42979-021-00735-0,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0521ba1df2d3d11acec9b55404a3667874294940,https://www.semanticscholar.org/paper/0521ba1df2d3d11acec9b55404a3667874294940,Verifying Controllers Against Adversarial Examples with Bayesian Optimization,"Recent successes in reinforcement learning have lead to the development of complex controllers for realworld robots. As these robots are deployed in safety-critical applications and interact with humans, it becomes critical to ensure safety in order to avoid causing harm. A first step in this direction is to test the controllers in simulation. To be able to do this, we need to capture what we mean by safety and then efficiently search the space of all behaviors to see if they are safe. In this paper, we present an active-testing framework based on Bayesian Optimization. We specify safety constraints using logic and exploit structure in the problem in order to test the system for adversarial counter examples that violate the safety specifications. These specifications are defined as complex boolean combinations of smooth functions on the trajectories and, unlike reward functions in reinforcement learning, are expressive and impose hard constraints on the system. In our framework, we exploit regularity assumptions on individual functions in form of a Gaussian Process (GP) prior. We combine these into a coherent optimization framework using problem structure. The resulting algorithm is able to provably verify complex safety specifications or alternatively find counter examples. Experimental results show that the proposed method is able to find adversarial examples quickly.",2018 IEEE International Conference on Robotics and Automation (ICRA),2018.0,10.1109/ICRA.2018.8460635,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d077a47f2ed077f294f50ec5e990939c2ea8fe6f,https://www.semanticscholar.org/paper/d077a47f2ed077f294f50ec5e990939c2ea8fe6f,"Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable","The history of AI has included several ""waves"" of ideas. The first wave, from the mid-1950s to the 1980s, focused on logic and symbolic hand-encoded representations of knowledge, the foundations of so-called ""expert systems"". The second wave, starting in the 1990s, focused on statistics and machine learning, in which, instead of hand-programming rules for behavior, programmers constructed ""statistical learning algorithms"" that could be trained on large datasets. In the most recent wave research in AI has largely focused on deep (i.e., many-layered) neural networks, which are loosely inspired by the brain and trained by ""deep learning"" methods. However, while deep neural networks have led to many successes and new capabilities in computer vision, speech recognition, language processing, game-playing, and robotics, their potential for broad application remains limited by several factors. 
A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on. This lack of robustness also appears in the vulnerability of AI systems to adversarial attacks, in which an adversary can subtly manipulate data in a way to guarantee a specific wrong answer or action from an AI system. AI systems also can absorb biases-based on gender, race, or other factors-from their training data and further magnify these biases in their subsequent decision-making. Taken together, these various limitations have prevented AI systems such as automatic medical diagnosis or autonomous vehicles from being sufficiently trustworthy for wide deployment. The massive proliferation of AI across society will require radically new ideas to yield technology that will not sacrifice our productivity, our quality of life, or our values.",ArXiv,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
888b7ccfe97408434b3552229cdbffcba25d5654,https://www.semanticscholar.org/paper/888b7ccfe97408434b3552229cdbffcba25d5654,New Industry 4.0 Advances in Industrial IoT and Visual Computing for Manufacturing Processes,"Modern factories are experiencing rapid digital transformation supported by emerging technologies, such as the Industrial Internet of things (IIOT), industrial big data and cloud technologies, deep learning and deep analytics, AI, intelligent robotics, cyber-physical systems and digital twins, complemented by visual computing (including new forms of artificial vision with machine learning, novel HMI, simulation, and visualization). This is evident in the global trend of Industry 4.0. The impact of these technologies is clear in the context of high-performance manufacturing. Important improvements can be achieved in productivity, systems reliability, quality verification, etc. Manufacturing processes, based on advanced mechanical principles, are enhanced by big data analytics on industrial sensor data. In current machine tools and systems, complex sensors gather useful data, which is captured, stored, and processed with edge, fog, or cloud computing. These processes improve with digital monitoring, visual data analytics, AI, and computer vision to achieve a more productive and reliable smart factory. New value chains are also emerging from these technological changes. This book addresses these topics, including contributions deployed in production, as well as general aspects of Industry 4.0.",,2020.0,10.3390/books978-3-03928-291-3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
37c7b64af0172e21d7dff4b919cea9707d219b8a,https://www.semanticscholar.org/paper/37c7b64af0172e21d7dff4b919cea9707d219b8a,Neurobiologically Inspired Self-Monitoring Systems,"In this article, we explore neurobiological principles that could be deployed in systems requiring self-preservation, adaptive control, and contextual awareness. We start with low-level control for sensor processing and motor reflexes. We then discuss how critical it is at an intermediate level to maintain homeostasis and predict system set points. We end with a discussion at a high level, or cognitive level, where planning and prediction can further monitor the system and optimize performance. We emphasize the information flow between these levels both from a systems neuroscience and an engineering point of view. Throughout the article, we describe the brain systems that carry out these functions and provide examples from artificial intelligence, machine learning, and robotics which include these features. Our goal is to show how biological organisms performing self-monitoring can inspire the design of autonomous and embedded systems.",Proceedings of the IEEE,2020.0,10.1109/JPROC.2020.2979233,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
53f16cd9c64248aea915c56390815f1652ea7da9,https://www.semanticscholar.org/paper/53f16cd9c64248aea915c56390815f1652ea7da9,Toward Smart Urban Development Through Intelligent Edge Analytics,,Integration of WSN and IoT for Smart Cities,2020.0,10.1007/978-3-030-38516-3_8,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
757e6216cc8a60a127611a32295a9317b22d75be,https://www.semanticscholar.org/paper/757e6216cc8a60a127611a32295a9317b22d75be,Combining Model-Based and Model-Free Updates for Deep Reinforcement Learning,"The ability to learn motor skills autonomously is one of the main requirements for deploying robots in unstructured realworld environments. The goal of reinforcement learning (RL) is to learn such skills through trial and error, thus avoiding tedious manual engineering. However, real-world applications of RL have to contend with two often opposing requirements: data-efficient learning and the ability to handle complex, unknown dynamical systems that might be difficult to model explicitly. Real-world physical systems, such as robots, are typically costly and time consuming to run, making it highly desirable to learn using the lowest possible number of realworld trials. Model-based methods tend to excel at this [5], but suffer from significant bias, since complex unknown dynamics cannot always be modeled accurately enough to produce effective policies. Model-free methods have the advantage of handling arbitrary dynamical systems with minimal bias, but tend to be substantially less sample-efficient [9, 17]. Can we combine the efficiency of model-based algorithms with the final performance of model-free algorithms in a method that we can practically use on real-world physical systems? Many prior methods that combine model-free and modelbased techniques achieve only modest gains in efficiency or performance [6, 7]. In this work, we aim to develop a method in the context of a specific policy representation: time-varying linear-Gaussian controllers. The structure of these policies provides us with an effective option for model-based updates via iterative linear-Gaussian dynamics fitting [10], as well as a simple option for model-free updates via the path integral policy improvement (PI) algorithm [19]. Although time-varying linear-Gaussian (TVLG) policies are not as powerful as representations such as deep neural networks [13, 14] or RBF networks [4], they can represent arbitrary trajectories in continuous state-action spaces. Furthermore, prior work on guided policy search (GPS) has shown that TVLG policies can be used to train generalpurpose parameterized policies, including deep neural network policies, for tasks involving complex sensory inputs such as vision [10, 12]. This yields a general-purpose RL procedure with favorable stability and sample complexity compared to fully model-free deep RL methods [16]. The main contribution of this paper is a procedure for optimizing TVLG policies that integrates both fast model-",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3a31b3354f9d5bc18dd0e3abf23dc9aedc5b8d1a,https://www.semanticscholar.org/paper/3a31b3354f9d5bc18dd0e3abf23dc9aedc5b8d1a,Is it a great Autonomous FX Trading Strategy or you are just fooling yourself,"There are many practitioners that create software to buy and sell financial assets in an autonomous way. There are some digital platforms that allow the development, test and deployment of trading agents (or robots) in simulated or real markets. In fact, autonomous trading robots have been studied in artificial intelligence area for some time. Some of these work focus on very short horizons of investment, while others deal with longer periods. The spectrum of used AI techniques in finance field is wide and it includes more recent approaches like convolutional neural networks and deep reinforcement learning. There are many cases, where the developers are successful in creating robots with great performance when executing with historical price series (so called backtesting). Furthermore, some electronic platforms make available thousands of robots that [allegedly] are able to be profitable in real markets. These autonomous strategies may be created with some simple idea (moving averages, for instance) or using complex machine learning schemes. Nevertheless, when these robots are used in real markets (or data not used in their training or evaluation) frequently they present very poor performance and high variance of returns. In this paper, we propose a method for testing Foreign Exchange (FX) trading strategies that can provide realistic expectations about strategy’s performance. This method addresses many pitfalls that can fool even experience practitioners and researchers. We present the results of applying such method in several famous autonomous strategies in many different financial assets. By the analysis of such results, we can realize that it is very hard to build a reliable strategy and many published strategies are far from being reliable vehicles of investment. Some periods of good performance can be observed, but they are not predictable and probably they happen by pure chance, rather than by the identification of real pattern in the input data. These facts can be maliciously used by those who try to sell such robots, by advertising such great (and non repetitive) results, while hiding the bad but meaningful results. The proposed method can be used to select among potential robots, establishes minimal periods and requirements for the test executions. In this way, the method helps to tell if you really have a great trading strategy or you are just fooling yourself.",ArXiv,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0950cf9fd7356a2192aa5331072a15a3b6676677,https://www.semanticscholar.org/paper/0950cf9fd7356a2192aa5331072a15a3b6676677,Classification of left and right foot kinaesthetic motor imagery using common spatial pattern,"Background and objectives: Brain-computer interface (BCI) systems typically deploy common spatial pattern (CSP) for feature extraction of mu and beta rhythms based on upper-limbs kinaesthetic motor imageries (KMI). However, it was not used to classify the left versus right foot KMI, due to its location inside the mesial wall of sensorimotor cortex, which makes it difficult to be detected. We report novel classification of mu and beta EEG features, during left and right foot KMI cognitive task, using CSP, and filter bank common spatial pattern (FBCSP) method, to optimize the subject-specific band selection. We initially proposed CSP method, followed by the implementation of FBCSP for optimization of individual spatial patterns, wherein a set of CSP filters was learned, for each of the time/frequency filters in a supervised way. This was followed by the log-variance feature extraction and concatenation of all features (over all chosen spectral-filters). Subsequently, supervised machine learning was implemented, i.e. logistic regression (Logreg) and linear discriminant analysis (LDA), in order to compare the respective foot KMI classification rates. Training and testing data, used in the model, was validated using 10-fold cross validation. Four methodology paradigms are reported, i.e. CSP LDA, CSP Logreg, and FBCSP LDA, FBCSP Logreg. All paradigms resulted in an average classification accuracy rate above the statistical chance level of 60.0% (P < 0.01). On average, FBCSP LDA outperformed remaining paradigms with kappa score of 0.41 and classification accuracy of 70.28% ± 4.23. Similarly, this paradigm enabled discrimination between right and left foot KMI cognitive task at highest accuracy rate i.e. maximum 77.5% with kappa = 0.55 and the area under ROC curve as 0.70 (in single-trial analysis). The proposed novel paradigms, using CSP and FBCSP, established a potential to exploit the left versus right foot imagery classification, in synchronous 2-class BCI for controlling robotic foot, or foot neuroprosthesis.",Biomedical physics & engineering express,2019.0,10.1088/2057-1976/ab54ad,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9c5f0f80c7dd21463177b7329d748b785af3c516,https://www.semanticscholar.org/paper/9c5f0f80c7dd21463177b7329d748b785af3c516,Shifting Perspectives on AI Evaluation: The Increasing Role of Ethics in Cooperation,"Evaluating AI is a challenging task, as it requires an operative definition of intelligence and the metrics to quantify it, including amongst other factors economic drivers, depending on specific domains. From the viewpoint of AI basic research, the ability to play a game against a human has historically been adopted as a criterion of evaluation, as competition can be characterized by an algorithmic approach. Starting from the end of the 1990s, the deployment of sophisticated hardware identified a significant improvement in the ability of a machine to play and win popular games. In spite of the spectacular victory of IBM’s Deep Blue over Garry Kasparov, many objections still remain. This is due to the fact that it is not clear how this result can be applied to solve real-world problems or simulate human abilities, e.g., common sense, and also exhibit a form of generalized AI. An evaluation based uniquely on the capacity of playing games, even when enriched by the capability of learning complex rules without any human supervision, is bound to be unsatisfactory. As the internet has dramatically changed the cultural habits and social interaction of users, who continuously exchange information with intelligent agents, it is quite natural to consider cooperation as the next step in AI software evaluation. Although this concept has already been explored in the scientific literature in the fields of economics and mathematics, its consideration in AI is relatively recent and generally covers the study of cooperation between agents. This paper focuses on more complex problems involving heterogeneity (specifically, the cooperation between humans and software agents, or even robots), which are investigated by taking into account ethical issues occurring during attempts to achieve a common goal shared by both parties, with a possible result of either conflict or stalemate. The contribution of this research consists in identifying those factors (trust, autonomy, and cooperative learning) on which to base ethical guidelines in agent software programming, making cooperation a more suitable benchmark for AI applications.",AI,2022.0,10.3390/ai3020021,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1f6e58ae510b9555a9ad8a27547ab8a36f31076f,https://www.semanticscholar.org/paper/1f6e58ae510b9555a9ad8a27547ab8a36f31076f,Valve Detection for Autonomous Water Pipeline Inspection Platform,"Water distribution and transmission lines are indispensable to urban infrastructure. The water pipelines are subject to both structural and functional deterioration due to various reasons including aging, negligence, and high demand for water supply. Hence, to ensure a safe and reliable water supply, the water utilities need to perform routine pipe condition assessment. The condition assessment is usually carried out by visual inspection with the machine vision system carried by a robotic platform. The inspection platforms will capture the internal condition of the water pipelines in a video stream. However, the robotic platform frequently experiences difficulties while traversing through the valves installed along the pipeline. This inhibits and disrupts the inspection process of the water pipelines. Therefore, this article proposes a deep learning-based automatic valve detection framework to facilitate the robot’s navigation and ensure continuous inspection without any interruptions. The valve detection model is developed by combining MobileNet-160 and Feature Pyramid Network and is named as MFPN. The developed framework also employs a generative adversarial network to solve the sparse dataset issues and improve the generality of the framework. The comparative study and ablation analyses demonstrate that the proposed framework can achieve a higher mAP value of 89.11% in comparison with the state of the art. Hence, this light weight and efficient solution can be deployed to the robotic platform for real-time valve detection and enable autonomous navigation of the robotic platform for condition assessment of water pipelines.",IEEE/ASME Transactions on Mechatronics,2021.0,10.1109/TMECH.2021.3079409,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0d732f8f8e60a40fa55de483f0cf3b5a93e220cf,https://www.semanticscholar.org/paper/0d732f8f8e60a40fa55de483f0cf3b5a93e220cf,Fast and Efficient Non-Contact Ball Detector for Picking Robots,"Object detectors based on deep learning requires high-performance computing and large run-time memory footprint to maintain good detection performance. They bring high computation overhead and power consumption to on-board embedded devices of non-contact ball picking robot. Furthermore, it is difficult to deploy on the machine because the model size is so big. The accuracy of the existing simplified detectors deployed on embedded devices cannot meet the requirements of practical applications. Therefore, how to reduce floating point operations (FLOPs) and the size of model without notably sacrificing detection precision becomes an urgent problem to be solved. To solve this problem, a shuttle residual block which is more efficient network unit based on depthwise separable convolution was proposed. And we designed a non-contact ball object detector for picking robots, which is shallower than YOLOv3 and has narrower structure. We evaluate the proposed method on non-contact Ball dataset and compelling results are achieved by the proposed method. Compared with YOLOv3, the proposed method reduces FLOPs by 86.2%, declines parameter size by 89.5%. Overall, the proposed method achieves comparable detection accuracy than YOLOv3, and its speed is 2.2 times faster than YOLOv3.",IEEE Access,2019.0,10.1109/ACCESS.2019.2955834,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
448af8143f2baa28ca596c4370c3e16f65ee7c99,https://www.semanticscholar.org/paper/448af8143f2baa28ca596c4370c3e16f65ee7c99,Industrial Priorities for Cognitive Robotics,"We present the results of a survey of industrial developers to determine what they and their customers require from a cognitive robot. These are cast as a series of eleven functional abilities: 1) Safe, reliable, transparent operation. 2) High-level instruction and context-aware task execution. 3) Knowledge acquisition and generalization. 4) Adaptive planning. 5) Personalized interaction. 6) Self-assessment. 7) Learning from demonstration. 8) Evaluating the safety of actions. 9) Development and self-optimization. 10) Knowledge transfer. 11) Communicating intentions and collaborative action. I. INDUSTRIAL REQUIREMENTS While cognitive robotics is still an evolving discipline and much research remains to be done, we nevertheless need to have a clear idea of what cognitive robots will be able to do if they are to be useful to industrial developers and end users. The RockEU2 project canvassed the views of thirteen developers to find out what they and their customers want. The results of this survey follow, cast as a series of eleven functional abilities. A. Safe, reliable, transparent operation Cognitive robots will be able to operate reliably and safely around humans and they will be able to explain the decisions they make, the actions they have taken, and the actions they are about to take. A cognitive robot will help people and prioritize their safety. Only reliable behaviour will build trust. It will explain decisions, i.e. why it acted the way it did. This is essential if the human is to develop a sense of trust in the robot. A cognitive robot will have limited autonomy to set intermediate goals to when carrying out tasks set by users. However, in all cases it defers to the users preferences, apart from some exceptional circumstances, e.g. people with dementia can interact in unpredictable ways and the robot will be able to recognize these situations and adapt in some appropriate manner. The freedom to act autonomously will have formal boundaries and the rules of engagement will be set on the basis of †Much of the work described in this paper was conducted while the author was at the University of Skövde, Sweden. This research was funded by the European Commission under grant agreement No: 688441, RockEU2. three parameters: safety for people, safety for equipment, and safety of the robot system. The rules may change depending on the environment and a cognitive robot will not exceed the limits of safe operation. The limits may be application specific, e.g., the robot should not deviate further than a given specification/distance/etc. A cognitive robot will use this type of knowledge to act responsibly and will ask for assistance when necessary (e.g. before it encounters difficulties). In particular, in emergency situations, the robot will stop all tasks to follow some emergency procedure. Ideally, if the user is deliberately trying to misuse the robot, e.g. programming it to assist with some unethical task, a cognitive robot will cease operation. B. High-level instruction and context-aware task execution Cognitive robots will be given tasks using high-level instructions and they will factor in contextual constraints that are specific to the application scenario when carrying out these tasks, determining for themselves the priority of possible actions in case of competing or conflicting requirements. Goals and tasks will be expressed using high-level instructions that will exploit the robots contextual knowledge of the task. This will allow the robot to pre-select the information that is important to effectively carry out the task. The goals will reflect the users perspective. This means that all skills which implicitly define the goals are tightly linked to realworld needs and to the solution of specific problems, e.g., “get me a hammer”. The following guidelines will apply. • Instructions will use natural language and gestures to specify the goals. • Natural language will be relatively abstract but will be grounded in the codified organisational rules, regulations, and behavioural guidelines that apply to a given application environment. This grounding means that each abstract instruction is heavily loaded with constraints which should make it easier for the robot to understand and perform the task effectively. • The goals should be specified in a formalised and structured way, where the designer defines them well and can verify them. For example, teach the robot the environment it is working in, follow a described route to reach each of the target locations and reach these positions to carry out the task. These clearly-specified tasks are tightly coupled with risks and costs, e.g. of incorrect execution. Proceedings of EUCognition 2016 ""Cognitive Robot Architectures"" CEUR-WS 6 • It should be possible for the robot to be given goals in non-specific terms (e.g. assist in alleviating the symptoms of dementia), guidelines on acceptable behaviour (or action policies), and relevant constraints, leaving it to the robot to identify the sub-goals that are needed to achieve these ultimate goals. • A cognitive robot will learn ways of measuring the success of outcomes for the objectives that have been set, e.g., creating a metric such as the owners satisfaction related not only to the directly specified objective but also the manner in which the job was done). It should be learn from these metrics. A cognitive robot will consider the contextual constraints that are specific to the application scenario. It will determine the priority of potential actions, e.g., in case of competing or conflicting needs. For example, the robot might know the procedure to be followed but the locations to be visited or the objects to be manipulated need to be specified (or vice versa). For example, when an automated harvester encounters a bale of straw, it can deal with it as an obstacle or something to be harvested, depending on the current task. For example, the robot might engage in spoken interaction with older adults until the goal is communicated unambiguously, using context to disambiguate the message and allow for the difficulties in dealing with different accents, imprecise speech, and poor articulation. A cognitive robot will know what is normal, i.e. expected, behaviour (possibly based on documented rules or practices) and it will be able to detect anomalous behaviour and then take appropriate action. The following guidelines will apply. • It will be possible to pre-load knowledge about the robots purpose and its operating environment, including any rules or constraints that apply to behaviour in that environment. • It will be possible to utilize domain-specific skill pools (e.g. from shared databases) so that the robot is preconfigured to accomplish basic tasks without having to resort to learning or development. • The robot will continually improve its skills (within limits of the goals and safety, see above) and share these with other robots. • The robot might assist the user by proposing goals from what it understood and the user makes the final selection. The level of detail in the description required by a cognitive robot will decrease over time as the robot gains experience, in the same way as someone new on the job is given very explicit instructions at first and less explicit instructions later on. One should need to demonstrate only the novel parts of the task, e.g., pouring liquid in a container, but not the entire process. It will be possible to instruct the robot off-line if there is no access to the physical site; e.g., using a simulation tool, with the robot then being deployed in the real scenario. C. Knowledge acquisition and generalization Cognitive robots will continuously acquire new knowledge and generalize that knowledge so that they can undertake new tasks by generating novel action policies based on their history of decisions. This will allow the rigor and level of detail with which a human expresses the task specification to be relaxed on future occasions. A cognitive robot will build and exploit experience so that its decisions incorporate current and long term data. For example, route planning in a factory, hospital, or hotel should take into account the history of rooms and previous paths taken, or it might take another look to overcome high uncertainty. In general, the robot will overcome uncertainty in a principled manner. A cognitive robot will generalize knowledge to new task by understanding the context of a novel task and extrapolating from previous experience. For example, a care-giving robot will reuse knowledge of a rehabilitation exercise, customizing it to another person. A welding robot will weld a new instance of a family of parts. In general, a cognitive robot will extract useful meaning from an interaction for a future and more general use, with the same or another user. This may extend to learn cultural preferences and social norms. For example, in a domestic environment, a cognitive robot will learn how to do simple household tasks, e.g. how to grasp different objects and them bring to a person that wants them. This will be continuously extended, allowing the robot to do more complex things, including cooking. D. Adaptive planning Cognitive robots will be able to anticipate events and prepare for them in advance. They will be able to cope with unforeseen situations, recognizing and handling errors, gracefully and effectively. This will also allow them to handle flexible objects or living creatures. A cognitive robot will be able to recognize that circumstances have changed to avoid situations where progress is impossible. It will also be able to recognize errors and recover. This may include retrying with a slightly different strategy. The learning process will be fast, ideally learning from each error. A cognitive robot will be able to learn how to handle errors, how to react to situations where, e.g., a human is doing something unexpected or parts are located in an unexpected place.",EUCognition,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d2a2de8548e88f420ae5542dcf495dac92b59e5a,https://www.semanticscholar.org/paper/d2a2de8548e88f420ae5542dcf495dac92b59e5a,Incremental Adversarial Domain Adaptation for Continually Changing Environments,"Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. While unsupervised domain adaptation aims to address this challenge, current approaches do not utilise the continuity of the occurring shifts. In particular, many robotics applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of intermediate domains which successively diverge from the labelled source domain. We empirically demonstrate that our incremental approach improves handling of large appearance changes, e.g. day to night, on a traversable-path segmentation task compared with a direct, single alignment step approach. Furthermore, by approximating the feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.",2018 IEEE International Conference on Robotics and Automation (ICRA),2017.0,10.1109/ICRA.2018.8460982,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1c3acda43ff338a7307ff1a68367f976f3c1d09c,https://www.semanticscholar.org/paper/1c3acda43ff338a7307ff1a68367f976f3c1d09c,A Robotic Architecture for the Autonomous Open-Ended Learning of Interdependent Tasks,"Autonomous acquisition of many different skills is necessary to foster behavioural versatility in artificial agents and robots. While the learning of multiple skills per se can be addressed through different machine learning techniques sequentially assigning a series of N tasks to the agent, autonomy implies that the agent itself has the capacity to select on which task to focus at each moment and to shift between them in a smart way. Intrinsic motivations (IMs) have been used in the field of machine learning and developmental robotics [1], [2] as a motivational signal for the autonomous selection of tasks (often called “goals”): the learning progress in accomplishing the tasks is used as a transient reward to select goals in which the system is making the most learning progress [3]. In realworld scenarios, tasks may require specific initial conditions to be performed or may be interdependent, so that to achieve a task the agent needs first to learn and accomplish other tasks. This latter case is of particular interest and it is still an open question from an autonomous open-ended learning perspective. We propose a reinforcement learning (RL) system for robot control that is capable of learning multiple interdependent tasks by treating the selection of tasks/goals as a Markov Decision Process (MDP) where the agent selects goals to maximise its overall competence.",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ea975ad98dfc1589d8b4cb6d7cfdcc64c64f7223,https://www.semanticscholar.org/paper/ea975ad98dfc1589d8b4cb6d7cfdcc64c64f7223,Using sensor fusion for parallel image classification,"With the rise of deep learning in recent years, image pattern recognition tasks have received much attention and have seen many proposed solutions that achieve near-perfect accuracy on public datasets. Combined with hardware acceleration, these solutions may be deployed in portable devices for robotics or Internet of Things applications. By the nature of deep learning, resulting models have highly interconnected layers, which poses a challenge when trying to deploy the model on a distributed system. We study an alternative approach that uses conventional machine learning algorithms and allows for parallel and distributed image classification. Each unit that participates in recognition gathers image data independently of others and operates only on a subset of classes. The result of each unit’s work is then combined with results of other units to produce the final classification. Use of random sampling during learning and classification allows for some resistance to noise and affine transformations. Using Flowers Recognition dataset and applying multivariate normal distribution for image sampling during learning and classification, with 1,024 features per class and with each feature containing 256 datapoints, applying linear-distance k-Nearest Neighbours algorithm with k = 4, training set size of 20, classification accuracy of 79.86% was achieved.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1a0cf3eba22c053ad21503e034baf3178899258a,https://www.semanticscholar.org/paper/1a0cf3eba22c053ad21503e034baf3178899258a,Design and Deployment of a Data Lake at a Pilot Plant Scale for a Smart Electropolishing Process,"In order to remain competitive and satisfy the demands of today’s customers in a timely manner, manufacturing industries are embracing the Industry 4.0 philosophy where automation is pushed beyond robotics to new technologies emerging from data science and artificial intelligence. The aim is to reduce time spent on none added value tasks and help learning from past experience in order to enhance efficiency and quality of manufacturing processes.
 Traditional industries, such as electropolishing, need to find ways to automate their, often heavily artisanal-based techniques and develop an intelligent network of machines and processes taking advantage of information and communication technology such as Big Data, IoT (Internet of Things), or Artificial Intelligence (AI). This digital transition can be realized through the application of an IIoT (Industrial Internet of Things) platform that constructs a massive, sophisticated information network of interconnected sensors, equipment, and processes known as cyber-physical systems.
 Within this network, large amounts of data (for example process bath attributes such as temperature or viscosity and part characteristics such as roughness or brightness) can be collected automatically via sensors and through user-friendly applications from manual measurements and observations. All data are uploaded automatically into a cloud-based data storage system. In order for this collected information to be useful, the data needs to be processed to allow pattern discovery and extraction of useful information regarding the system performance, probable faults in the process, and product quality. Besides others, machine learning algorithms play a key role in extracting useful information.
 Classification and processing of such massive, diverse, and rapidly arriving data sets are known to be challenging. As a result, the concept of data lake has arisen in the last decade as an appealing and cost-effective approach for companies to manage large amounts of data. It consists of a large repository of datasets designed to transform raw and unstructured data into structured, usable information to allow further processing. A data lake, organized typically in four layers (ingestion, distillation, processing, and insights layers), stores both old and near real-time data in one location for initial assessment, with comprehensive data organization, analysis, and visualization being performed only when necessary 1,2. This promotes agility by allowing data to be accessed by everyone in the company. 2
 
 In this work, a data lake is designed and implemented in conjunction with a pilot plant to demonstrate how in the electropolishing process of stainless-steel samples in an aging electrolyte, data can be collected and organized for further processing using machine learning techniques in order to optimize the process and part quality based on the data analysis results.
 References: 
 
 N. Miloslavskaya and A. Tolstoy, Procedia Comput. Sci., 88, 300–305 (2016).
 
 
 H. Fang, in 2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER),, p. 820–824 (2015).
 
 
 
 
 
 
 
 
 Figure 1
",ECS Meeting Abstracts,2022.0,10.1149/ma2022-01251219mtgabs,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
24a93a51eea700d779b7c1e7a3005288d5071565,https://www.semanticscholar.org/paper/24a93a51eea700d779b7c1e7a3005288d5071565,VLSI Hardware Architecture for Gaussian Process,"Gaussian process (GP) is a popular machine learning technique that is widely used in many application domains, especially in robotics. However, GP is very computation intensive and time consuming during the inference phase, thereby bringing severe challenges for its large-scale deployment in real-time applications. In this paper, we propose two efficient hardware architecture for GP accelerator. One architecture targets for general GP inference, and the other architecture is specifically optimized for the scenario when the data point is gradually observed. Evaluation results show that the proposed hardware accelerator provides significant hardware performance improvement than the general-purpose computing platform.","2020 54th Asilomar Conference on Signals, Systems, and Computers",2020.0,10.1109/IEEECONF51394.2020.9443272,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7b4fd6bbd3933704f5f3c8afcea3950866d57a2d,https://www.semanticscholar.org/paper/7b4fd6bbd3933704f5f3c8afcea3950866d57a2d,What Can Digitization Do For Formulated Product Innovation and Development,"Digitization oﬀers signiﬁcant opportunities for the formulated product industry to transform the way it works and develop new methods of business. R&D is one area of operation that is challenging to take advantage of these technologies due to its high level of domain specialisation and creativity but the beneﬁts could be signiﬁcant. Recent developments of base level technologies such as artiﬁcial intelligence (AI)/machine learning (ML), robotics and high performance computing (HPC), to name a few, present disruptive and transformative technologies which could oﬀer new insights, discovery methods and enhanced chemical control when combined in a digital ecosystem of connectivity, distributive services and decentralisation. At the fundamental level, research in these technologies has shown that new physical and chemical insights can be gained, which in turn can augment experimental R&D approaches through physics-based chemical simulation, data driven models and hybrid approaches. In all of these cases, high quality data is required to build and validate models in addition to the skills and expertise to exploit such methods. In this article we give an overview of some of the digital technology demonstrators we have developed for formulated product R&D. We discuss the challenges in building and deploying these demonstrators.",,2020.0,10.26434/chemrxiv.11763864.v2,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0a8a8ade1539f1ecc48c605cadc52a1f22d3ff3f,https://www.semanticscholar.org/paper/0a8a8ade1539f1ecc48c605cadc52a1f22d3ff3f,Smart AI Assistant,"The system also provides the user with the facility to teach the bot himself. This allows user to adjust the bot according to himself and also correct the bot when it’s wrong. The technologies being used in the system are Machine-Learning (ML), Chatbot, Image processing and Neural Network. The proposed system can de deployed on personal computers, laptops, it can also be used in robotics and simulations.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c5b47ac47f4a44395ce491f5026b083527a8e43f,https://www.semanticscholar.org/paper/c5b47ac47f4a44395ce491f5026b083527a8e43f,AI3SD Video: Smart Cleaning & COVID-19,"Industrial Digital Technologies (IDTs) such as robotics, AI and IoT are transforming manufacturing worldwide with significant productivity, efficiency and environmental sustainability benefits. This digital revolution is often labelled Industry 4.0 and at its heart is the enhanced collection and use of data. The food and drink sector has been slow to adopt IDT’s for a variety of reasons including the availability of cost effective sensing technologies, capable of operating in production environments. This presentation will discuss the use of IDTs within the important task of food factory cleaning. It will cover the benefits and challenges of deploying robots, sensors and machine learning technologies for factory cleaning tasks in addition to the ever growing importance of effective factory cleaning during a global pandemic.",,2020.0,10.5258/SOTON/P0057,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ba2a844e2ca9e53f343adaab2316245b4be21934,https://www.semanticscholar.org/paper/ba2a844e2ca9e53f343adaab2316245b4be21934,AUTONOMOUS FLYING ROBOTS for INTELLIGENT IOT DATA HARVESTING,"This presentation reports on fascinating results obtained within the PERFUME project, which is an “Advanced ERC” (European Research Council) funded project that develops algorithms and prototypes for autonomous aerial robots (drones or “UAV – Unmanned Aerial Vehicles”) that provide wireless connectivity to users and devices. Our work finds applications in intelligent data harvesting in future IoT networks and ultra-flexible deployment radio access networks to provide on-demand connectivity in post-disaster wireless networks. The uniqueness of PERFUME lies in the requirement of a cross-disciplinary research among three important fields: (1) Future wireless networks and IoT, (2) Artificial intelligence, and (3) Robotics. The key idea is to insert autonomous robots at the edge of the wireless networks as flying base stations (BSs) in order to make network deployment ultra-flexible. The drone’s trajectory is optimized using machine learning algorithms as to maximize the IoT network’s performance under navigation and limited on-board energy constraints of the drone. Practical prototypes of our autonomous flying robots have been designed, tested, and recorded on videos.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
59b58d67815f8a73d36322809940d12c754cbeb9,https://www.semanticscholar.org/paper/59b58d67815f8a73d36322809940d12c754cbeb9,Comparative Evaluation of Feature Descriptors Through Bag of Visual Features with Multilayer Perceptron on Embedded GPU System,"Loop Closure Detection (LCD) is one of the most significant steps for a Simultaneous Localization and Mapping (SLAM) system of an autonomous robot. Many of the state-of-the-art techniques for the LCD problem are based on handcrafted resources and Bag of Words (BoW). Even with advances in Machine Learning (ML) area, Deep Learning (DL) methods and Convolutional Neural Network (CNN) are not fully explored in the LCD problem context. We addressed the LCD problem proposing a hybrid Deep Neural Network (DNN) architecture for deployment on an embedded GPU system: NVIDIA's Jetson Nano. The main idea of this work consists of reformulating convolutional filters of a CNN through Local Binary Descriptors to produce a sparse model as an efficient alternative to traditional CNNs. This paper discusses the evaluation of the Bag of Visual Features (BoVF) approach, extracting features through Local Feature Descriptors such as SIFT, SURF, and KAZE and Local Binary Descriptors such as BRIEF, ORB, BRISK, AKAZE, and FREAK were evaluated for the recognition and classification steps. For this task, we used six visual datasets (MNIST, JAFFE, Extended CK+, FEI, CIFAR-10, and FER-2013) through Multilayer Perceptron (MLP) classifier. Experimentally, we demonstrated the feasibility to produce promising results combining BoVf with MLP classifier. Therefore, empirically, we can assume that the computed descriptors generated by a Local Binary Descriptor alongside a hybrid DNN architecture, can accomplish satisfactory results addressed in future work in the Feature Detection step, and reformulated in the Feature Extraction step into convolutional filters of CNN architecture of the proposed system.","2020 Latin American Robotics Symposium (LARS), 2020 Brazilian Symposium on Robotics (SBR) and 2020 Workshop on Robotics in Education (WRE)",2020.0,10.1109/LARS/SBR/WRE51543.2020.9306931,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6af4abe9f3360b2817db4a655e1d3486754ba6b0,https://www.semanticscholar.org/paper/6af4abe9f3360b2817db4a655e1d3486754ba6b0,Large-scale learning for media understanding,,EURASIP J. Image Video Process.,2015.0,10.1186/S13640-015-0080-7,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
69f91edaf5abc963a9b9b121d1f8bc851333e133,https://www.semanticscholar.org/paper/69f91edaf5abc963a9b9b121d1f8bc851333e133,Active Learning in the Era of Big Data,"Active learning methods automatically adapt data collection by selecting the most informative samples in order to accelerate machine learning. Because of this, real-world testing and comparing active learning algorithms requires collecting new datasets (adaptively), rather than simply applying algorithms to benchmark datasets, as is the norm in (passive) machine learning research. To facilitate the development, testing and deployment of active learning for real applications, we have built an open-source software system for large-scale active learning research and experimentation. The system, called NEXT, provides a unique platform for realworld, reproducible active learning research. This paper details the challenges of building the system and demonstrates its capabilities with several experiments. The results show how experimentation can help expose strengths and weaknesses of active learning algorithms, in sometimes unexpected and enlightening ways.",,2015.0,10.2172/1225849,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
07ac515f443f59aaeb36b6a8e0f91d9b7ea8f9e3,https://www.semanticscholar.org/paper/07ac515f443f59aaeb36b6a8e0f91d9b7ea8f9e3,Robots in the Wild,"Human-robot interaction (HRI) research to date has been dominated by laboratory studies, largely examining a single human interacting with a single robot. This research has helped establish a fundamental understanding about people as they interact with robots, how specific design choices affect interactions with robots, and how novel mechanisms or computational tools can be used to improve HRI. The predominant focus of this growing body of work, however, stands in stark contrast to the complex social contexts in which robots are increasingly placed. Developments in machine learning and tele-robotics, as well as compliant and social robotics, have occasioned more robots in closer proximity and even direct contact with people. Robots, especially mobile autonomous robots, are now deployed across work contexts and “sociable robots” such as Jibo, Cozmo, Kuri, and M.A.X. are increasingly becoming staples of household technology. These robots interact with people in everyday contexts across a wide range of tasks and situations, yet our research reflects a time when studies of HRI were possible almost exclusively only in laboratory settings. As a result, we have a limited understanding of how people will respond to robots in complex social settings and how robots will affect social dynamics in situ. In particular, our theories reflect an oversimplified view of HRI. The time is ripe for studies that tackle HRI in these complex settings and build generalizable theories about what to expect of HRI in the wild. Although there is much to be learned in controlled laboratory settings and they should continue to be a mainstay of HRI studies, we argue that more HRI researchers need to venture into the",HRI 2018,2018.0,10.1145/3208975,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
508ecc80dc2c74da8a1c61006a53bfde6d88d00e,https://www.semanticscholar.org/paper/508ecc80dc2c74da8a1c61006a53bfde6d88d00e,Making humanoid robots teaching assistants by using natural language processing (NLP) cloud-based services,"Humanoid robots have a substantial potential to serve as teaching and social assistants. However, the expectations of the children from robots to interact like humans are huge. This study presents a general model for understanding the natural language in human-robot interaction by applying Generative Pre-trained Transformer (GPT) language models as a service in the Internet of Things. Thus, the physical presence of the robot can help in fine-tuning the GPT model by prompts derived from the environmental context and subsequent robot actions for embodiment understanding of the GPT outputs. The model uses web or cloud services for Natural Language Processing (NLP) to produce and play human-like text, question answering or text generation. Verbal questions are processed either via a local speech recognition software or via a Speech-to-Text (STT) cloud service. The converted question into machine-readable code is sent to one of the GPT language models with zero- or few-shot learning prompts. GPT-J model has been tested and deployed either in the web or cloud with options for varying the parameters for controlling the haphazardness of the generated text. The robot produces human-like text by using Text-to-Speech (TTS) cloud services that convert the response into audio format rendered on the robot to be played. Useful requirements how the model to be used in order to be feasible were determined based on the evaluation of the outputs given from the different NLP and GPT-J web or cloud-services. We designed and implemented the model in order to be used by a humanoid NAO-type robot in the speech language therapy practice, however it can be used for other open-source and programmable robots and in different contexts.",Journal of Mechatronics and Artificial Intelligence in Engineering,2022.0,10.21595/jmai.2022.22720,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
41f7d19e4096da4d29602baa1b52df31705368b7,https://www.semanticscholar.org/paper/41f7d19e4096da4d29602baa1b52df31705368b7,A Cognitive Architecture for Self Learning in Humanoid Robots,"Cognitive is the mental process of knowing, including characteristics such as perception, awareness, judgment, and reasoning. Today humanoid robots need to become self-learner like humans, in this way they can be able to experience different things and learn from their experience, relating to, being, or involving conscious intellectual capable of being reduced to empirical factual knowledge. Considering the advantages of humanoid robots, in this study we propose a novel framework called Cognitive Architecture for Self Learning in Humanoid Robots (CASLHR). It combines the active memory, action schematical engine and sensor listener layers which try to produce human-like intelligence by analyzing the internal processes and the architecture of the human brain. The proposed CASLHR architecture may result in robust, safe, reliable, and flexible machines that can substitute humans in multiple tasks. This architecture is illustrated through case studies about firefighting task in the building and communication with the realworld. It can feel and perceive similar to a human being and will be able to learn from its experience and simultaneously updates its actions based on the success rate of its attempts to achieve a goal.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4ec29250a8a12d91016876caf56004483699f3c1,https://www.semanticscholar.org/paper/4ec29250a8a12d91016876caf56004483699f3c1,Deep Q-Learning for Navigation of Robotic Arm for Tokamak Inspection,,ICA3PP,2018.0,10.1007/978-3-030-05063-4_6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0f8e0eaa748b624ddc888e40c906cf316c9610ee,https://www.semanticscholar.org/paper/0f8e0eaa748b624ddc888e40c906cf316c9610ee,Mobile Image Multi-label Recognition Algorithm Based on PaddlePaddle Platform,"As one of the core algorithms of machine vision, the mobile image multi-label recognition algorithm has received extensive attention from researchers in recent years and has been widely used in cutting-edge fields such as deep learning framework paddlepaddle platform, video surveillance, intelligent robots, and unmanned aerial vehicles. However, the existing recognition algorithms are not completely satisfied with the practical application in life and production. Due to the complexity of the platform environment, they can often only propose specific solutions based on existing problems, and there is no universal algorithm that is suitable for all kinds of Complex environment. The purpose of this paper is to study the multi-label recognition algorithm of moving images based on PaddlePaddle platform. This research mainly analyzes and researches the mobile image multi-tag space deployment plan and the multi-tag recognition algorithm, and further improves the tag reading rate and recognition reliability of the mobile image on the PaddlePaddle platform. This research first analyzes several key factors that affect the performance of UHF recognition system, considers the improvement plan of PaddlePaddle platform’s mobile image multi-tag recognition algorithm from the two aspects of space diversity and frequency diversity, and finally determines the multiple The label space diversity scheme, and the introduction of a multi-label optimization recognition algorithm to improve the recognition efficiency of the PaddlePaddle platform’s mobile image multi-label. Experimental data shows that the reading rate can reach 0.907 when identifying 300 tags in the experiment, and when the number of tags is greater than 300, the reading rate is close to 1, which verifies that the algorithm proposed in this paper is used in the multi-tag recognition of moving images on the PaddlePaddle platform.",Journal of Physics: Conference Series,2021.0,10.1088/1742-6596/2066/1/012046,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cc9083c4482cae00aa5ae72b136d66dda1a46fc1,https://www.semanticscholar.org/paper/cc9083c4482cae00aa5ae72b136d66dda1a46fc1,Future Micromobility Concepts '“ Low-Speed Autonomous Mobility Platform,"""Research and/or Engineering Questions/Objective With the artificial intelligence as a foundation, this research aims to develop a low-speed autonomous mobility platform with a sensor suite that gives the mobility the eyes for sensing and perception, the brain for computing and planning and the feet for moving around. It can plan an efficient and suitable path with collision avoidance via the A.I. technologies of deep learning and sensor fusion autonomously after receiving commands. Adapting to the shift of the modern world with a severe labour shortage, the low-speed autonomous mobility platform can offer flexible and versatile alternatives and extend to a wide range of applications, including last-mile delivery, inspection, roaming vending machine, pop-up store, battery swapping, autonomous micro-mobilities (scooter, wheelchairs). Methodology Unlike the industrial robots bolted firmly to the floor which blindly perform the same movement, the low-speed autonomous mobility is equipped with a sensor suite comprising 3D LiDAR, cameras, GPS, inertial measurement unit (IMU) and ultrasonic sensors. Through the technologies of deep learning with sensor fusion (a combination of sensor and vision technology), an advanced form of artificial intelligence and dynamic way of computerised decision, the low-speed autonomous mobility is capable of planning an efficient and suitable path with collision avoidance of stationary or moving obstacles in crowded and dynamic environment, not only limited to apartment complexes, parks or warehouses, but also some shared and public space. Results A prototype of low-speed autonomous mobility with a storage cabinet as last-mile delivery mobility is developed to validate the performance of the sensor suite. The prototype has been tested in our R&D building in Hong Kong for various delivery tasks with collision avoidance. Such testing allows us to enhance the system design even under an in-house condition and during the development phase. In 2020, the system will be equipped with a robotic arm to perform more versatile work. Limitations of this study Although there is no infrastructure and investment required for the mobility platform, for the target areas to deploy this autonomous mobility, the human intervention is required in the first time to build a map for the localisation. Besides, the mobility platform cannot climb up stairs due to the selection of wheel type. What does the paper offer that is new in the field including in comparison to other work by the authors? The mobility equipped with a sensor suite and software platform can be extended to various application scenarios, making it more flexible and versatile, rather than serving only one purpose. The idea of extension shortens the development time and resource. Conclusions The capability of planning an efficient and suitable path, traversing to assigned destination autonomously and collision avoidance unlock the possibilities of the low-speed mobility to perform assigned tasks autonomously in a less labour intensive, yet more effective and efficient way.""",FISITA World Congress 2021 - Technical Programme,2021.0,10.46720/f2020-dgt-015,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
504a77040e983f31283aa6ca12d9dc60233b1c07,https://www.semanticscholar.org/paper/504a77040e983f31283aa6ca12d9dc60233b1c07,sEMG-based Regression of Hand Kinematics with Temporal Convolutional Networks on a Low-Power Edge Microcontroller,"Human-Machine Interfaces based on gesture control are a very active field of research, aiming to enable natural interaction with objects. Nowadays, one of the most promising State-of-the-Art (SoA) methodology for robotic hand control relies on the surface electromyographic (sEMG) signal, a non-invasive approach that can provide accurate and intuitive control when coupled with decoding algorithms based on Deep Learning (DL). However, the vast majority of the approaches so far have focused on sEMG classification, producing control systems that limit gestures to a predefined set of positions. In contrast, sEMG regression is still a new field, providing a more natural and complete control method that returns the complete hand kinematics. This work proposes a regression framework based on TEMPONet, a SoA Temporal Convolutional Network (TCN) for sEMG decoding, which we further optimize for deployment. We test our approach on the NinaPro DB8 dataset, targeting the estimation of 5 continuous degrees of freedom for 12 subjects (10 able-bodied and 2 trans-radial amputees) performing a set of 9 contralateral movements. Our model achieves a Mean Absolute Error of 6.89°, which is 0.15° better than the SoA. Our TCN reaches this accuracy with a memory footprint of only 70.9 kB, thanks to int8 quantization. This is remarkable since high-accuracy SoA neural networks for sEMG can reach sizes up to tens of MB, if deployment-oriented reductions like quantization or pruning are not applied. We deploy our model on the GAP8 edge microcontroller, obtaining 4.76 ms execution latency and an energy cost per inference of 0.243 mJ, showing that our solution is suitable for implementation on resource-constrained devices for real-time control.",2021 IEEE International Conference on Omni-Layer Intelligent Systems (COINS),2021.0,10.1109/COINS51742.2021.9524188,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
952ec800bb69063afb0abba7c1f446d21a9475d1,https://www.semanticscholar.org/paper/952ec800bb69063afb0abba7c1f446d21a9475d1,Modular approach to data preprocessing in ALOHA and application to a smart industry use case,"Applications in the smart industry domain, such as interaction with collaborative robots using vocal commands or machine vision systems often requires the deployment of deep learning algorithms on heterogeneous low power computing platforms. The availability of software tools and frameworks to automatize different design steps can support the effective implementation of DL algorithms on embedded systems, reducing related effort and costs. One very important aspect for the acceptance of the framework, is its extensibility, i.e. the capability to accommodate different datasets and define customized preprocessing, without requiring advanced skills. The paper addresses a modular approach, integrated into the ALOHA 1 tool flow, to support the data preprocessing and transformation pipeline. This is realized through customizable plugins and allows the easy extension of the tool flow to encompass new use cases. To demonstrate the effectiveness of the approach, we present some experimental results related to a keyword spotting use case and we outline possible extensions to different use cases. Keywords— Deep Learning, flows and tools, computer-aided design, edge computing.",ArXiv,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
77cb7732ab66d79d2999fc13896e90d9c2d58c39,https://www.semanticscholar.org/paper/77cb7732ab66d79d2999fc13896e90d9c2d58c39,Real-Time Pressure Estimation and Localisation with Optical Tomography-inspired Soft Skin Sensors,"Sensing and localising pressure resulting from physical interaction between a robot and its environment is a key requirement in the deployment of soft robots in real-life scenarios. In order to adapt the robot's behaviour in real-time, we argue that sensors must have a high sampling rate. In this paper, we present a novel tactile sensing strategy for soft sensors, based on an imaging technique known as optical tomography. Instead of transmitting light through the soft sensor in a sequential way (as commonly done in tomography systems), we demonstrate that concurrently illuminating the sensor with multiple light sources and reading out the sensor response has several advantages. Firstly, it drastically increases the sampling rate of the sensor when compared to standard tomography approaches, making it more suitable to sense sudden and short-lived contacts. Secondly, by concurrently switching on the light sources, we increase performance in terms of pressure localisation and pressure estimation achieved through Machine Learning techniques. We carry out experiments demonstrating that our approach allows for a robust pressure estimation and contact point localisation with an accuracy up to 91.1 % (vs 70.3%) at a higher sampling rate.",2022 IEEE 5th International Conference on Soft Robotics (RoboSoft),2022.0,10.1109/RoboSoft54090.2022.9762066,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9deb6c8258b6599331cf6547307bbd63f460f33b,https://www.semanticscholar.org/paper/9deb6c8258b6599331cf6547307bbd63f460f33b,Learning for Autonomous Navigation,"Autonomous navigation by a mobile robot through L natural, unstructured terrain is one of the premier k challenges in field robotics. Tremendous advances V in autonomous navigation have been made recently in field robotics. Machine learning has played an increasingly important role in these advances. The Defense Advanced Research Projects Agency (DARPA) UGCV-Perceptor Integration (UPI) program was conceived to take a fresh approach to all aspects of autonomous outdoor mobile robot design, from vehicle design to the design of perception and control systems with the goal of achieving a leap in performance to enable the next generation of robotic applications in commercial, industrial, and military applications. The essential problem addressed by the UPI program is to enable safe autonomous traverse of a robot from Point A to Point B in the least time possible given a series of waypoints in complex, unstructured terrain separated by 0.2-2 km. To accomplish this goal, machine learning techniques were heavily used to provide robust and adaptive performance, while simultaneously reducing the required development and deployment time. This article describes the autonomous system, Crusher, developed for the UPI program and the learning approaches that aided in its successful performance.",IEEE Robotics & Automation Magazine,2010.0,10.1109/MRA.2010.936946,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
93fdda0b8b2ef16d747a0aa955b839173fcb6874,https://www.semanticscholar.org/paper/93fdda0b8b2ef16d747a0aa955b839173fcb6874,Learning-Based Algal Bloom Event Recognition for Oceanographic Decision Support System Using Remote Sensing Data,"This paper describes the use of machine learning methods to build a decision support system for predicting the distribution of coastal ocean algal blooms based on remote sensing data in Monterey Bay. This system can help scientists obtain prior information in a large ocean region and formulate strategies for deploying robots in the coastal ocean for more detailed in situ exploration. The difficulty is that there are insufficient in situ data to create a direct statistical machine learning model with satellite data inputs. To solve this problem, we built a Random Forest model using MODIS and MERIS satellite data and applied a threshold filter to balance the training inputs and labels. To build this model, several features of remote sensing satellites were tested to obtain the most suitable features for the system. After building the model, we compared our random forest model with previous trials based on a Support Vector Machine (SVM) using satellite data from 221 days, and our approach performed significantly better. Finally, we used the latest in situ data from a September 2014 field experiment to validate our model.",Remote. Sens.,2015.0,10.3390/rs71013564,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
96b2a1f6e58203de563bb7259ddc487394d71ec5,https://www.semanticscholar.org/paper/96b2a1f6e58203de563bb7259ddc487394d71ec5,Conversational Interfaces for Information Search,,,2020.0,10.1007/978-3-030-38825-6_13,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
86d806fe28b6ffc4ce11098d7b1c1dbea40d93e7,https://www.semanticscholar.org/paper/86d806fe28b6ffc4ce11098d7b1c1dbea40d93e7,Towards Controlling Bucket Fill Factor in Robotic Excavation by Learning Admittance Control Setpoints,,FSR,2017.0,10.1007/978-3-319-67361-5_3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10bbc5556fe6b79b18bb986af5e601b7ac7ab114,https://www.semanticscholar.org/paper/10bbc5556fe6b79b18bb986af5e601b7ac7ab114,LTF: A Label Transformation Framework for Correcting Target Shift,"Distribution shift is a major obstacle to the deployment of current deep learning models on realworld problems. Let Y be the target (label) and X the predictors (features). We focus on one type of distribution shift, target shift, where the marginal distribution of the target variable PY changes but the conditional distribution PX|Y does not. Existing methods estimate the density ratio between the sourceand target-domain label distributions by density matching. However, these methods are either computationally infeasible for large-scale data or restricted to shift correction for discrete labels. In this paper, we propose an end-to-end Label Transformation Framework (LTF) for correcting target shift, which implicitly models the shift of PY and the conditional distribution PX|Y using neural networks. Thanks to the flexibility of deep networks, our framework can handle continuous, discrete, and even multidimensional labels in a unified way and is scalable to large data. Moreover, for high dimensional X , such as images, we find that the redundant information in X severely degrades the estimation accuracy. To remedy this issue, we propose to match the distribution implied by our generative model and the target-domain distribution in a low-dimensional feature space that discards information irrelevant to Y . Both theoretical and empirical studies demonstrate the superiority of our method over previous approaches. UBTECH Sydney AI Centre, School of Computer Science, Faculty of Engineering, The University of Sydney, Darlington, NSW 2008, Australia School of Mathematics and Statistics, The University of Melbourne Department of Philosophy, Carnegie Mellon University. Correspondence to: Jiaxian Guo <jguo5934@uni.sydney.edu.au>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
87ad56e06d48fa9b30e2915473c488c1b4b7e6ae,https://www.semanticscholar.org/paper/87ad56e06d48fa9b30e2915473c488c1b4b7e6ae,Learn from experience: Probabilistic prediction of perception performance to avoid failure,"Despite significant advances in machine learning and perception over the past few decades, perception algorithms can still be unreliable when deployed in challenging time-varying environments. When these systems are used for autonomous decision-making, such as in self-driving vehicles, the impact of their mistakes can be catastrophic. As such, it is important to characterize the performance of the system and predict when and where it may fail in order to take appropriate action. While similar in spirit to the idea of introspection, this work introduces a new paradigm for predicting the likely performance of a robot’s perception system based on past experience in the same workspace. In particular, we propose two models that probabilistically predict perception performance from observations gathered over time. While both approaches are place-specific, the second approach additionally considers appearance similarity when incorporating past observations. We evaluate our method in a classical decision-making scenario in which the robot must choose when and where to drive autonomously in 60km of driving data from an urban environment. Results demonstrate that both approaches lead to fewer false decisions (in terms of incorrectly offering or denying autonomy) for two different detector models, and show that leveraging visual appearance within a state-of-the-art navigation framework increases the accuracy of our performance predictions.",Int. J. Robotics Res.,2018.0,10.1177/0278364917730603,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a78a795f6b2916e93e7c5df147a3aea76a1752cb,https://www.semanticscholar.org/paper/a78a795f6b2916e93e7c5df147a3aea76a1752cb,"Jellyfish encounters: science, technology and security in the Anthropocene ocean","ABSTRACT The article furthers the debate on security in the Anthropocene by scrutinizing one crucial aspect of it – that is: the relation between knowledge, technology and security. For this, it takes the emerging debate in International Relations and Critical Security Studies to a space that it has so far neglected: the global ocean. Focusing on countermeasures against rising jellyfish blooms – from early-warning systems to autonomous killer robots – the article studies how digital technologies are increasingly being deployed to cope with anthropogenic environmental risks. Conceptually, the article develops the notion of the Anthropocene ocean. This concept is used to show how complex and entangled phenomena – such as jellyfish blooms – challenge existing regimes of security as well as underlying forms of knowledge production. The analysis shows how attempts to control and manage risks in the ocean space are replaced by experimental forms of governance that are enabled by digital technologies including big data, machine learning and sensors of different kinds. The final part of the article argues that such forms of technological experimentation and related knowledge practices are underwritten by a logic of geopolitics and war.",,2020.0,10.1080/21624887.2020.1815478,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
da18b8acd389676bfd2db4893a7e151485c148ce,https://www.semanticscholar.org/paper/da18b8acd389676bfd2db4893a7e151485c148ce,Object and Gas Source Detection with Robotic Platforms in Perceptually-Degraded Environments,"In exploration-oriented robotic missions for disaster relief in unknown environments, it is of prime importance for a human supervisor to rapidly gain situational awareness of salient objects within the environment. In this paper, we present a cascaded and modular pipeline for the detection, localization and visualization of objects of interest that is adaptable to heterogeneous robots with arbitrary sensor configurations. We apply this pipeline for static objects which may be seen with the human eye as well as invisible spatially-diffuse phenomena such as gas and WiFi signals. We present results from its deployment in time-critical scenarios with multiple collaborative robots in a variety of harsh underground environments. For visually-observable objects, detections are made in both the color and thermal spectra using a state-of-the-art machine learning framework for object detection. We fine-tune a pretrained convolutional neural network for a specific set of objects; in this case we use around 1000 images covering a range of poses and contexts for each object class. Once detected, a robust architecture for localization is used with outlier rejection and a hierarchy of fall-back distance measurement methods. Spatially-diffuse phenomena are registered by tracking signal strength over time. A human supervisor can optionally provide input to refine the automatically-generated detections with the help of intuitive map-based visualizations and object thumbnails ranked by classification confidence.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0799b77e487b8cdffabf2a3625750e4ac1e0d2ad,https://www.semanticscholar.org/paper/0799b77e487b8cdffabf2a3625750e4ac1e0d2ad,A Plausibility-Based Fault Detection Method for High-Level Fusion Perception Systems,"Trustworthy environment perception is the fundamental basis for the safe deployment of automated agents such as self-driving vehicles or intelligent robots. The problem remains that such trust is notoriously difficult to guarantee in the presence of systematic faults, e.g., non-traceable errors caused by machine learning functions. One way to tackle this issue without making rather specific assumptions about the perception process is plausibility checking. Similar to the reasoning of human intuition, the final outcome of a complex black-box procedure is verified against given expectations of an object’s behavior. In this article, we apply and evaluate collaborative, sensor-generic plausibility checking as a mean to detect empirical perception faults from their statistical fingerprints. Our real use case is next-generation automated driving that uses a roadside sensor infrastructure for perception augmentation, represented here by test scenarios at a German highway and a city intersection. The plausibilization analysis is integrated naturally in the object fusion process, and helps to diagnose known and possibly yet unknown faults in distributed sensing systems.",IEEE Open Journal of Intelligent Transportation Systems,2020.0,10.1109/OJITS.2020.3027146,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d9753986e9e48fed3d46ca4834f7b71341430832,https://www.semanticscholar.org/paper/d9753986e9e48fed3d46ca4834f7b71341430832,Fusion of Gesture and Speech for Increased Accuracy in Human Robot Interaction,"An approach for decision-level fusion for gesture and speech based human-robot interaction (HRI) is proposed. A rule-based method is compared with several machine learning approaches. Gestures and speech signals are initially classified using hidden Markov models, reaching accuracies of 89.6% and 84% respectively. The rule-based approach reached 91.6% while SVM, which was the best of all evaluated machine learning algorithms, reached an accuracy of 98.2% on the test data. A complete framework is deployed in real time humanoid robot (NAO) which proves the efficacy of the system.",2019 24th International Conference on Methods and Models in Automation and Robotics (MMAR),2019.0,10.1109/MMAR.2019.8864671,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b064a078ccbd13b167d3d6c9de183a9cc08fc4eb,https://www.semanticscholar.org/paper/b064a078ccbd13b167d3d6c9de183a9cc08fc4eb,At Your Service: Coffee Beans Recommendation From a Robot Assistant,"With advances in the field of machine learning, service robots are envisioned to become more present. The COVID-19 pandemic has accelerated this need. One such example would be coffee shops, which have become intrinsic to our everyday lives. Yet, serving an excellent cup of coffee is not trivial as a coffee blend typically comprises rich aromas, indulgent and unique flavours. Our work addresses this by proposing a computational model which recommends optimal coffee beans resulting from users' preferences. Given coffee properties (objective features), we apply different supervised learning techniques to predict coffee qualities (subjective features). We then consider an unsupervised learning method to analyse the relationship between coffee beans in the subjective feature space. Evaluated on a real coffee beans dataset based on digitised reviews, our results illustrate that the proposed computational model gives up to 92.7 percent recommendation accuracy for coffee prediction. From this, we propose how it can be deployed on a robot.",HAI,2020.0,10.1145/3406499.3418765,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1fbbf88cad2535ebc711ab9dec3b4a6f275b0e84,https://www.semanticscholar.org/paper/1fbbf88cad2535ebc711ab9dec3b4a6f275b0e84,Towards Transparency of TD-RL Robotic Systems with a Human Teacher,"The high request for autonomous and flexible HRI implies the necessity of deploying Machine Learning (ML) mechanisms in the robot control. Indeed, the use of ML techniques, such as Reinforcement Learning (RL), makes the robot behaviour, during the learning process, not transparent to the observing user. In this work, we proposed an emotional model to improve the transparency in RL tasks for human-robot collaborative scenarios. The architecture we propose supports the RL algorithm with an emotional model able to both receive human feedback and exhibit emotional responses based on the learning process. The model is entirely based on the Temporal Difference (TD) error. The architecture was tested in an isolated laboratory with a simple setup. The results highlight that showing its internal state through an emotional response is enough to make a robot transparent to its human teacher. People also prefer to interact with a responsive robot because they are used to understand their intentions via emotions and social signals.",ArXiv,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1a51198caa98796094b3c472005ac91641cee7a5,https://www.semanticscholar.org/paper/1a51198caa98796094b3c472005ac91641cee7a5,A Primer on Large Intelligent Surface (LIS) for Wireless Sensing in an Industrial Setting,,CrownCom,2020.0,10.1007/978-3-030-73423-7_10,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6a2a534a9d01b76ecb5caba20032022a904cb50d,https://www.semanticscholar.org/paper/6a2a534a9d01b76ecb5caba20032022a904cb50d,COSMIC Semantic Segmentation Framework,"Deep space missions such as the Mars Reconnaissance Orbiter collect more data than can be sent back to Earth due to limited communications bandwidth. Machine learning algorithms can be deployed on board orbiters to prioritize the downlink of scientifically interesting images, such as those including fresh impact craters, recurring slope lineae, or dust devils. However, basic machine learning research is necessary to boost realworld performance, and numerous possible convolutional neural network architectures must be evaluated in terms of accuracy and compute requirements. A framework is designed to reduce redundant development, to standardize the algorithm testing process, and to allow developers to focus on the implementation details of novel machine learning algorithms. Three convolutional neural network implementations are included with the framework, pending use in future research. 1Content-based Onboard Summarization to Monitor Infrequent Change 2CL #18-465",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
34a2aa8fc1d92f9d38c63e5144524bbcffa4c1a3,https://www.semanticscholar.org/paper/34a2aa8fc1d92f9d38c63e5144524bbcffa4c1a3,Plant Phenotyping by Deep-Learning-Based Planner for Multi-Robots,"Plant Phenotyping by Deep-Learning-Based Planner for Multi-Robots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C. Wu, R. Zeng, J. Pan, C. C. L. Wang, and Y.-J Liu 3113 Hand-Eye Calibration With a Remote Centre of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K. Pachtrachai, F. Vasconcelos, G. Dwyer, S. Hailes, and D. Stoyanov 3121 Robust and Adaptive Lower Limb Prosthesis Stance Control via Extended Kalman Filter-Based Gait Phase Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . N. Thatte, T. Shah, and H. Geyer 3129 A Communication-Aware Mutual Information Measure for Distributed Autonomous Robotic Information Gathering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .S. Moon and E. W. Frew 3137 Monocular Object and Plane SLAM in Structured Environments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . S. Yang and S. Scherer 3145 Toward Ergonomic Risk Prediction via Segmentation of Indoor Object Manipulation Actions Using Spatiotemporal Convolutional Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Parsa, E. U. Samani, R. Hendrix, C. Devine, Shashi M. Singh, S. Devasia, and A. G. Banerjee 3153 A Compact Soft Articulated Parallel Wrist for Grasping in Narrow Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . F. Negrello, S. Mghames, G. Grioli, M. Garabini, and Manuel Giuseppe Catalano 3161 Submodular Optimization for Coupled Task Allocation and Intermittent Deployment Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Jun Liu and R. K. Williams 3169 A Testbed for Haptic and Magnetic Resonance Imaging-Guided Percutaneous Needle Biopsy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E. Mendoza and J. P. Whitney 3177 Deep Visual MPC-Policy Learning for Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .N. Hirose, F. Xia, R. Martı́n-Martı́n, A. Sadeghian, and S. Savarese 3184 Robust Moving Path Following Control for Robotic Vehicles: Theory and Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M. F. Reis, R. P. Jain, A. P. Aguiar, and J. B. de Sousa 3192 A Neurologically Inspired Sequence Processing Model for Mobile Robot Place Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . P. Neubert, S. Schubert, and P. Protzel 3200 Autonomous Free-Form Trenching Using a Walking Excavator . . . . . . . . D. Jud, P. Leemann, S. Kerscher, and M. Hutter 3208 Fast Classification of Large Germinated Fields Via High-Resolution UAV Imagery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J. Valente, L. Kooistra, and S. Mücher 3216 Efficiency and Reliability Joint Optimization of Chiller Plants Based on a Hybrid Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. Zhang, K. Mittal, J. Wilson, P. B. Luh, J. Fan, and S. Gupta 3224 Provably Safe Multi-Robot Coordination With Unreliable Communication . . . A. Mannucci, L. Pallottino, and F. Pecora 3232 A Soft Exoglove Equipped With a Wearable Muscle-Machine Interface Based on Forcemyography and Electromyography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .A. Dwivedi, L. Gerez, W. Hasan, C.-H. Yang, and M. Liarokapis 3240 Balancing Using Vertical Center-of-Mass Motion: A 2-D Analysis From Model to Robot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. J. van Hofslot, R. Griffin, S. Bertrand, and J. Pratt 3247 Multiple Hypothesis Semantic Mapping for Robust Data Association . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . L. Bernreiter, A. Gawel, H. Sommer, J. Nieto, R. Siegwart, and C. C. Lerma 3255",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
99316df3b069d75f5c59afd5f4b0e88e92ea6360,https://www.semanticscholar.org/paper/99316df3b069d75f5c59afd5f4b0e88e92ea6360,Recurrent and Spiking Modeling of Sparse Surgical Kinematics,"Robot-assisted minimally invasive surgery is improving surgeon performance and patient outcomes. This innovation is also turning what has been a subjective practice into motion sequences that can be precisely measured. A growing number of studies have used machine learning to analyze video and kinematic data captured from surgical robots. In these studies, models are typically trained on benchmark datasets for representative surgical tasks to assess surgeon skill levels. While they have shown that novices and experts can be accurately classified, it is not clear whether machine learning can separate highly proficient surgeons from one another, especially without video data. In this study, we explore the possibility of using only kinematic data to predict surgeons of similar skill levels. We focus on a new dataset created from surgical exercises on a simulation device for skill training. A simple, efficient encoding scheme was devised to encode kinematic sequences so that they were amenable to edge learning. We report that it is possible to identify surgical fellows receiving near perfect scores in the simulation exercises based on their motion characteristics alone. Further, our model could be converted to a spiking neural network to train and infer on the Nengo simulation framework with no loss in accuracy. Overall, this study suggests that building neuromorphic models from sparse motion features may be a potentially useful strategy for identifying surgeons and gestures with chips deployed on robotic systems to offer adaptive assistance during surgery and training with additional latency and privacy benefits.",ICONS,2020.0,10.1145/3407197.3407210,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ca05b55d9ea2a16169a434b3ed9bc1a03de1bac0,https://www.semanticscholar.org/paper/ca05b55d9ea2a16169a434b3ed9bc1a03de1bac0,Learning to Schedule Deadline- and Operator-Sensitive Tasks,"The use of semi-autonomous and autonomous robotic assistants to aid in care of the elderly is expected to ease the burden on human caretakers, with small-stage testing already occurring in a variety of countries. Yet, it is likely that these robots will need to request human assistance via teleoperation when domain expertise is needed for a specific task. As deployment of robotic assistants moves to scale, mapping these requests for human aid to the teleoperators themselves will be a difficult online optimization problem. In this paper, we design a system that allocates requests to a limited number of teleoperators, each with different specialities, in an online fashion. We generalize a recent model of online job scheduling with a worst-case competitive-ratio bound to our setting. Next, we design a scalable machine-learning-based teleoperator-aware task scheduling algorithm and show, experimentally, that it performs well when compared to an omniscient optimal scheduling algorithm.",ArXiv,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a6c7b3c141f6c604e08fee902fa268665b8a4221,https://www.semanticscholar.org/paper/a6c7b3c141f6c604e08fee902fa268665b8a4221,Simple linear classifiers via discrete optimization: learning certifiably optimal scoring systems for decision-making and risk assessment,"Scoring systems are linear classification models that let users make quick predictions by adding, subtracting, and multiplying a few small numbers. These models are widely used in applications where humans have traditionally made decisions because they are easy to understand and validate. In spite of extensive deployment, many scoring systems are still built using ad hoc approaches that combine statistical techniques, heuristics, and expert judgement. Such approaches impose steep trade-offs with performance, making it difficult for practitioners to build scoring systems that will be used and accepted. In this dissertation, we present two new machine learning methods to learn scoring systems from data: Supersparse Linear Integer Models (SLIM) for decision-making applications; and Risk-calibrated Supersparse Linear Integer Models (RiskSLIM) for risk assessment applications. Both SLIM and RiskSLIM solve discrete optimization problems to learn scoring systems that are fully optimized for feature selection, small integer coefficients, and operational constraints. We formulate these problems as integer programming problems and develop specialized algorithms to recover certifiably optimal solutions with an integer programming solver. We illustrate the benefits of this approach by building scoring systems for realworld problems such as recidivism prediction, sleep apnea screening, ICU seizure prediction, and adult ADHD diagnosis. Our results show that a discrete optimization approach can learn simple models that perform well in comparison to the state-ofthe-art, but that are far easier to customize, understand, and validate. Thesis Supervisor: Cynthia Rudin Title: Associate Professor of Computer Science Duke University",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9a92315f9cb46422655a44d36771dc6afe6f2307,https://www.semanticscholar.org/paper/9a92315f9cb46422655a44d36771dc6afe6f2307,An Intelligent and Automated Approach for Smart Minimarkets,"This paper presents the design and implementation of a smart and safe minimarket prototype for deployment in busy smart cities to mitigate the overhead of shopping experience The prototype allows customers to remotely access and browse the available products at the minimarket using a special smart-phone application The system can intelligently detect the nearby location of customers and subsequently provide location-dependent services such as allowing orders to be placed using the application, predicting weekly customer expenditures based on artificial-neural-network machine-learning approach, and automatically delivering purchased products using a robotic shopping cart This proposal is believed to support safe shopping which became a critical issue after COVID-19 pandemic From a service provider view point, the application allows the provider to remotely manage the minimarket by adding/removing product items, keeping track of shortage in products, and getting revenue information Empirical results show that the average service time of the minimarket is approximate to 60 seconds per customer However, an analytical model based on queueing theory was used to analyze the performance of the system when customers arrive according to a Poisson random process and get served according to a general-service-time distribution (M/G/1) The case of batch customer arrivals (M-[H]/G/1) was also analyzed, where batch size is also assumed to be random Various traffic intensities and the effect of variable service times were studied and cross-validated with simulation results Worst-case scenario shows that under heavy load of 95%, when customers arrive at the minimarket every 63 seconds on average, the average response time for each customer is minutes (C) 2020 The Authors Published by Atlantis Press SARL",Int. J. Comput. Intell. Syst.,2020.0,10.2991/ijcis.d.200611.001,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
16c6872ef3d9959a47f56e2f2ccb97b906d109e7,https://www.semanticscholar.org/paper/16c6872ef3d9959a47f56e2f2ccb97b906d109e7,The Laws and Regulation of AI and Autonomous Systems,,Unimagined Futures – ICT Opportunities and Challenges,2020.0,10.1007/978-3-030-64246-4_4,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a5ee3107d80c4d6b0c50861e30941550192d151b,https://www.semanticscholar.org/paper/a5ee3107d80c4d6b0c50861e30941550192d151b,A Low-Cost Autonomous Surface Vehicle for Multi-Vehicle Operations,"To provide a low-cost modular easily deployable platform for advancing marine autonomy and supporting multi-vehicle missions, an Autonomous Surface Vehicles (ASV) called BREAM (Boat for Robotic Engineering and Applied Machine-Learning) is de veloped by Purdue University. BREAM is equipped with a Ultra Short Baseline (USBL) system for underwater localization, an acoustic modem for underwater communication, and a portable and adaptable docking station for underwater power/data transfer. This paper discusses the design, construction, control and experimental validation of the vehicle's autonomous capabilities.",Global Oceans 2020: Singapore – U.S. Gulf Coast,2020.0,10.1109/IEEECONF38699.2020.9389236,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8dc6411e55cab57dbad552f524e17e44d2172bcd,https://www.semanticscholar.org/paper/8dc6411e55cab57dbad552f524e17e44d2172bcd,"Technology Driven Disruption in Global Retailing: Implications for Strategy, Execution and Economic Policy","Global retailing is expected to be a 28 Trillion USD industry by 2020 and currently represents 31% of global GDP. (Berkshire Hathaway 2016) Currently, across a global landscape the retail sector is experiencing the effects of technological disruption, perhaps the fourth major retail disruption in the last 150 years. Some analysts have stated that retail may change more in the next five years than in the last fifty. 
 
Projected impacts will ripple across not only the retail industry but across consumers, suppliers and impact the workforce of nations. In the USA some estimates have up to 25% of the workforce in retail or supporting firms. Today many of those jobs are in jeopardy as firms deploy new technology such as robots, AI, machine learning, digital, VR and new business models using these technologies. Globally these changes will affect both developed nations as well as the developing economies in fundamental ways. Along with disrupted jobs new opportunities are expected to be created that require skill sets that many workers are not currently prepared. Currently there are differing opinions by experts regarding the net, net employment impact of these new technologies. 
 
Research Purpose: This research will review the academic and practitioner literature on current and future projected technological trends in retail and discuss implications for retail strategy and execution, and potential governmental policy implications. In addition to examining the literature we will conduct exploratory interviews with retail leaders, leading suppliers, retail trade associations and technology service providers about their thoughts on the speed and impact of technology on retail evolution/revolution. We will focus our work mostly in the food/CPG and apparel industries given past research and access opportunities with industry leaders. The primary purpose of this research is to review the state of technologies impact on this key global industry and to summarize implications for private and public policy strategies. We will be looking in particular where there are areas of agreement/disagreement in the literature and among experts. 
 
Research Methodology: This research will conduct secondary analysis of the business literature on retail technology and competitive trends and supplement using an exploratory qualitative interviews of key informants from industry. Thus this research is largely exploratory. 
 
Analysis will use existing models of technology and business model diffusion, together a discussion of how the current disrupted environment is different and/or similar to past technology disruptions. The expected impacts of retail disruption will also be examined using Christiansen’s model of industry disruption and impacts on industry value-chain configuration. 
 
Finding/Results: The findings of this research will be presented in the form of a summary of retail trends and anticipated impacts across the industry value chain and on employment. Areas of agreement / disagreement in the literature and among experts will be highlighted. In particular this research will describe the key technology trends affecting retailers, overview the expected impacts of those trends and then discuss implications for retail strategies, such as Omni-channel and business model. In addition policy issues regarding employment impact and future skill level training will be discussed. Examples depicting trends and impacts will be described in the discussion. 
 
Conclusions: In summary this research reviews technology trends in retailing and then discusses implications of these trends for retail practitioners and for public policy makers. Managerial and competitive issues such as strategic flexibility/agility, scale, retail branding, productivity and implications of evolving GMROI on retail investment. Limitations of the research and exploratory methodology will be reviewed and opportunities for future research will be presented in the context of the literature and managerial decision making.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0d63016877577f8072725e8370d5c084f18a313b,https://www.semanticscholar.org/paper/0d63016877577f8072725e8370d5c084f18a313b,Artificially intelligent systems are learning how to develop new products and designs . What does that leave engineers to do ?,"Not all that long ago, engineering was a profession conducted with pencils and paper. Calculations were done by hand and designs were sketched out on large sheets. From actual blueprints, physical models would be made to work out how the final product should look and be made. Today, of course, engineering is a discipline intensely involved with computational and software tools. Computer-assisted design, computational fluid dynamics, and finite-element analysis applications are some of the basic tools that engineers deploy when creating new product designs. When physical models must be tested, prototypes can be printed directly from the computer files. Although these tools have enhanced the powers of engineers, the engineer is still clearly in control of the design process. But that control is now in question. There is increasing interest in using new artificial intelligence and other technologies to reach higher levels of product automation and accelerate innovation of new products. Advances in AI, combined synergistically with other technologies such as cognitive computing, Internet of Things, 3-D (or even 4-D) printing, advanced robotics, virtual and mixed reality, and human machine interfaces, are transforming what, where, and how products are designed, manufactured, assembled, distributed, serviced, and upgraded.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
586d40cc9f8a88e469bf67d8ac29ce1706e212aa,https://www.semanticscholar.org/paper/586d40cc9f8a88e469bf67d8ac29ce1706e212aa,Automatic Speaker Recognition System Pdf Read,"This book constitutes the thoroughly refereed post-conference proceedings of the Second International Symposium on Intelligent Informatics (ISI 2013) held in Mysore, India during August 23-24, 2013. The 47 revised papers presented were carefully reviewed and selected from 126 initial submissions. The papers are organized in topical sections on pattern recognition, signal and image processing; data mining, clustering and intelligent information systems; multi agent systems; and computer networks and distributed systems. The book is directed to the researchers and scientists engaged in various fields of intelligent informatics. The conference will consist of various topics relating to power systems, robotics, mechatronics and pattern recognition Automatic Speech and Speaker RecognitionAdvanced TopicsSpringer Science & Business Media This book presents an overview of speaker recognition technologies with an emphasis on dealing with robustness issues. Firstly, the book gives an overview of speaker recognition, such as the basic system framework, categories under different criteria, performance evaluation and its development history. Secondly, with regard to robustness issues, the book presents three categories, including environment-related issues, speaker-related issues and application-oriented issues. For each category, the book describes the current hot topics, existing technologies, and potential research focuses in the future. The book is a useful reference book and self-learning guide for early researchers working in the field of robust speech recognition. Speech Recognition has a long history of being one of the difficult problems in Artificial Intelligence and Computer Science. As one goes from problem solving tasks such as puzzles and chess to perceptual tasks such as speech and vision, the problem characteristics change dramatically: knowledge poor to knowledge rich; low data rates to high data rates; slow response time (minutes to hours) to instantaneous response time. These characteristics taken together increase the computational complexity of the problem by several orders of magnitude. Further, speech provides a challenging task domain which embodies many of the requirements of intelligent behavior: operate in real time; exploit vast amounts of knowledge, tolerate errorful, unexpected unknown input; use symbols and abstractions; communicate in natural language and learn from the environment. Voice input to computers offers a number of advantages. It provides a natural, fast, hands free, eyes free, location free input medium. However, there are many as yet unsolved problems that prevent routine use of speech as an input device by non-experts. These include cost, real time response, speaker independence, robustness to variations such as noise, microphone, speech rate and loudness, and the ability to handle non-grammatical speech. Satisfactory solutions to each of these problems can be expected within the next decade. Recognition of unrestricted spontaneous continuous speech appears unsolvable at present. However, by the addition of simple constraints, such as clarification dialog to resolve ambiguity, we believe it will be possible to develop systems capable of accepting very large vocabulary continuous speechdictation. rd It is a pleasure and an honour both to organize ICB 2009, the 3 IAPR/IEEE Intertional Conference on Biometrics. This will be held 2–5 June in Alghero, Italy, hosted by the Computer Vision Laboratory, University of Sassari. The conference series is the premier forum for presenting research in biometrics and its allied technologies: the generation of new ideas, new approaches, new techniques and new evaluations. The ICB series originated in 2006 from joining two highly reputed conferences: Audio and Video Based Personal Authentication (AVBPA) and the International Conference on Biometric Authentication (ICBA). Previous conferences were held in Hong Kong and in Korea. This is the first time the ICB conference has been held in Europe, and by Programme Committee, arrangements and by the quality of the papers, ICB 2009 will continue to maintain the high standards set by its predecessors. In total we received around 250 papers for review. Of these, 36 were selected for oral presentation and 93 for poster presentation. These papers are accompanied by the invited speakers: Heinrich H. Bülthoff (Max Planck Institute for Biological Cybernetics, Tübgen, Germany) on “What Can Machine Vision Learn from Human Perception?”, daoki Furui (Department of Computer Science, Tokyo Institute of Technology) on “40 Years of Progress in Automatic Speaker Recognition Technology” and Jean-Christophe Fondeur (SAGEM Security and Morpho, USA) on “Large Scale Deployment of Biomrics and Border Control”. Automatic speech recognition and speaker recognition have a lot of applications in personal identification, access control and in the new manmachine-interface paradigm. The existing applications in voice-activated embedded systems solve the problem of recognition of the spoken words only or the problem of recognition of a speaker through the words uttered only. The goal of this project, therefore, is the development of a robust algorithm for both speech recognition and speaker verification. An example of a target application of this work is speech dialing of mobile phones with a speaker verification front-end in order to effect access control. In view of the memory and computational constraints of embedded systems, the dynamic time warping algorithm is used. This project only considers isolated spoken digits. The developed algorithm is coded in C language and can be ported to firmware for Arabic numeral digit recognition with a speaker verification front end for an embedded system like mobile phones. The system produced a FAR of 13.33% and a FRR of 24.3% for a total of 70 true claims and 30 false claims. It also had a word accuracy of 96.7%. The three volume set LNCS 7062, LNCS 7063, and LNCS 7064 constitutes the proceedings of the 18th International Conference on Neural Information Processing, ICONIP 2011, held in Shanghai, China, in November 2011. The 262 regular session papers presented were carefully reviewed and selected from numerous submissions. The papers of part I are organized in topical sections on perception, emotion and development, bioinformatics, biologically inspired vision and recognition, bio-medical data analysis, brain signal processing, brain-computer interfaces, brain-like systems, brain-realistic models for learning, memory and embodied cognition, Clifford algebraic neural networks, combining multiple learners, computational advances in bioinformatics, and computational-intelligent human computer interaction. The second volume is structured in topical sections on cybersecurity and data mining workshop, data mining and knowledge doscovery, evolutionary design and optimisation, graphical models, humanoriginated data analysis and implementation, information retrieval, integrating multiple nature-inspired approaches, Kernel methods and support vector machines, and learning and memory. The third volume contains all the contributions connected with multi-agent systems, natural language processing and intelligent Web information processing, neural encoding and decoding, neural network models, neuromorphic hardware and implementations, object recognition, visual perception modelling, and advances in computational intelligence methods based pattern recognition. The Defense Communications Division of ITT (ITTDCD) has developed an automatic speaker recognition (ASR) system that meets the functional requirements defined in NRL's Statement of Work. This report is organized as follows. Chapter 2 is a short history of the development of the ASR system, both the algorithm and the implementation. Chapter 3 describes the methodology of the system testing, while Chapter 4 summarizes the test results. In Chapter 5, we discuss some further testing that was performed using the GFM test material. Conclusions derived from the contract work are given Chapter 6. Speech recognition. (JES). This book is a collection research papers and articles from the 2nd International Conference on Communications and CyberPhysical Engineering (ICCCE – 2019), held in Pune, India in Feb 2019. Discussing the latest developments in voice and data communication engineering, cyber-physical systems, network science, communication software, imageand multimedia",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2facfef1eb48674ba6de26ff50dedd3bfcd4a37a,https://www.semanticscholar.org/paper/2facfef1eb48674ba6de26ff50dedd3bfcd4a37a,Augmented Neural Prostheses,"Motor neuroprosthetics have the potential to oneday restore fine movement of the hand and fingers to people with upper limb amputations or paralysis. There have been promising proof of concept demonstrations, but performance has so far been far below able-bodied hand movement. In our group we have explored controlling individuated finger movements with muscle-amplified nerve signals from amputated nerves in humans, as well as with neural activity from primary motor cortex in nonhuman primates. Most motor decoders in the literature would naturally improve with the quality and quantity of neural signals recorded, but these types of hardware improvements translate very slowly to human use. Fortunately, machine learning has provided a range of methods for augmenting a simple underlying linear model linking neural activity to movement. These include Kalman filter trajectory models, enforced smoothing, mode selection schemes, and recurrent neural networks. In the future, neural networks have the potential to further embed intelligence about what movements should look like directly into our decoding algorithms. Within the past decade, there have been numerous demonstration of cortical brain-machine interfaces (BMIs) controlling upper limb movement in people with spinal cord injuries. [1], [3], [4], [6]. While this is promising preliminary work, performance is not yet sufficient for a disabled patient to be without a caregiver for long periods of time. Increased independence would likely be a prerequisite for wide-scale clinical BMI deployment. There is strong interest in the spinal cord injury (SCI) community to have cortical neuroprostheses, particularly to regain control of their native arm [2]. However, BMI performance must be high enough to produce meaningful life improvements, for example through increased autonomy or restoration of the ability to work. Intracortical BMI performance has increased dramatically since the early 2000s. Interestingly, the underlying map between neural firing rates has remained basically the same since early work. Most human and monkey brain machine interface experiments today still use linear regression often augmented by regularization (e.g. [4]. This is simple to use and generalizes well to movements not specifically in the training dataset. Nonetheless, the linear prediction itself is actually very poor, requiring heavy “fixing” to produce a smooth movement. Also, the problem becomes harder as we add degrees of freedom or move to the fingers, where movements span the full range of highly coupled joints rather than staying within a local linear approximation in a limited working volume. As a result, the large improvements over the past twenty years have come about by augmenting this basic underlying linear fit with additional information. The Kalman filter, and its many variants add an underlying physical model to neural prediction [5]. Similarly, recurrent neural networks have begun to show promise in brain machine interfaces and have their own underlying dynamics that could theoretically infer missing information from a weak neural signal [7],[8]. While we routinely augment our algorithms with information beyond what we get directly from the neurons, augmentation in terms of going beyond what could be accomplished with able-bodied hands is far more challenging. All of the high performance BMI systems currently require a brain surgery. Also, in terms of end effector, it is still extremely difficult for even a state of the art robotic manipulator to fold a shirt. However, to quote Bill Gates, “we always overestimate the change that will occur in the next two years and underestimate the change that will occur in the next ten.” As intracortical high channel count BMIs enter their third decade with promising paths to further improvement, things may progress faster than expected. 1. Ajiboye, A. Bolu, et al. ""Restoration of reaching and grasping movements through brain-controlled muscle stimulation in a person with tetraplegia: a proof-of-concept demonstration."" The Lancet 389.10081 (2017): 1821-1830. 2. Blabe, Christine H., et al. ""Assessment of brain–machine interfaces from the perspective of people with paralysis."" Journal of neural engineering 12.4 (2015): 043002. 3. Bouton, Chad E., et al. ""Restoring cortical control of functional movement in a human with quadriplegia."" Nature 533.7602 (2016): 247. 4. Collinger, Jennifer L., et al. ""High-performance neuroprosthetic control by an individual with tetraplegia."" The Lancet 381.9866 (2013): 557-564. 5. Gilja, Vikash, et al. ""Clinical translation of a high-performance neural prosthesis."" Nature medicine 21.10 (2015): 1142. 6. Hochberg, Leigh R., et al. ""Reach and grasp by people with tetraplegia using a neurally controlled robotic arm."" Nature 485.7398 (2012): 372. 7. Pandarinath, Chethan, et al. ""Inferring single-trial neural population dynamics using sequential auto-encoders."" Nature methods (2018): 1. 8. Sussillo, David, et al. ""A recurrent neural network for closed-loop intracortical brain–machine interface decoders."" Journal of neural engineering 9.2 (2012): 026027. Augmenting neuroprosthetic control for individuated finger movements Cynthia Chestek, Associate Professor of Biomedical Engineering University of Michigan, Ann Arbor",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4b8ab3bb19e64a931209e02b7449cec03da68fa2,https://www.semanticscholar.org/paper/4b8ab3bb19e64a931209e02b7449cec03da68fa2,Knowledge Organization Seventh Annual “Best Paper in KO Award” for Volume 46 (2019),"The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. Vanda Broughton is Emeritus Professor of library and information studies at University College London. Her principal research interest is in the development of faceted classification, particularly as it affects different disciplines. She is editor of the second edition of the Bliss Bibliographic Classification (BC2), and an associate editor of the Universal Decimal Classification. In addition to the published volumes of BC2, she is author of several books on knowledge organization systems and numerous articles and conference papers. Smiraglia, Richard P. 2019. “Work.” Knowledge Organization 46, no. 4: 308-19. Abstract: A work is a deliberately created informing entity intended for communication. A work consists of abstract intellectual content that is distinct from any object that is its carrier. In library and information science, the importance of the work lies squarely with the problem of information retrieval. Works are mentefacts—intellectual (or mental) constructs that serve as artifacts of the cultures in which they arise. The meaning of a work is abstract at every level, from its creator’s conception of it, to its reception and inherence by its consumers. Works are a kind of informing object and are subject to the phenomenon of instantiation, or realization over time. Research has indicated a base typology of instantiation. The problem for information retrieval is to simultaneously collocate and disambiguate large sets of instantiations. Cataloging and bibliographc tradition stipulate an alphabetico-classed arrangement of works based on an authorship principle. FRBR provided an entity-relationship schema for enhanced control of works in future catalogs, which has been incorporated into RDA. FRBRoo provides an empirically more precise model of work entities as informing objects and a schema for their representation in knowledge organization systems. A work is a deliberately created informing entity intended for communication. A work consists of abstract intellectual content that is distinct from any object that is its carrier. In library and information science, the importance of the work lies squarely with the problem of information retrieval. Works are mentefacts—intellectual (or mental) constructs that serve as artifacts of the cultures in which they arise. The meaning of a work is abstract at every level, from its creator’s conception of it, to its reception and inherence by its consumers. Works are a kind of informing object and are subject to the phenomenon of instantiation, or realization over time. Research has indicated a base typology of instantiation. The problem for information retrieval is to simultaneously collocate and disambiguate large sets of instantiations. Cataloging and bibliographc tradition stipulate an alphabetico-classed arrangement of works based on an authorship principle. FRBR provided an entity-relationship schema for enhanced control of works in future catalogs, which has been incorporated into RDA. FRBRoo provides an empirically more precise model of work entities as informing objects and a schema for their representation in knowledge organization systems. Richard P. Smiraglia holds a PhD in information from the University of Chicago. He is Senior Fellow and Executive Director of the Institute for Knowledge Organization and Structure, Inc. and is Editor-in-Chief of this journal. He also is Professor Emeritus of the iSchool at the University of Wisconsin-Milwaukee. He was 2017-2018 KNAW Visiting Professor at DANS (Data Archiving and Networked Services division of the Royal Netherlands Academy of the Arts and Sciences), The Hague, The Netherlands, where he remains visiting fellow and was the 2018 recipient of the 2018 Frederick G. Kilgour Award for Research in Library and Information Technology. * Awards Committee for Volume 45 (2018): Ann Graf. chair; Jane Greenberg, Joseph Tennis, Daniel Martínez-Ávila and Yejun Wu",KNOWLEDGE ORGANIZATION,2020.0,10.5771/0943-7444-2020-8-714,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4f2847a25e1d70abf82dd761eb20e1fbb3f43fd6,https://www.semanticscholar.org/paper/4f2847a25e1d70abf82dd761eb20e1fbb3f43fd6,"Design, deployment, and algorithmic optimization of zoomorphic, in- teractive robot companions","Pets provide companionship, stress reduction, and even performance enhancement [28]. However, pets cannot be adapted sufficiently to many people’s needs, such as frequent travel, living in residences where pets are not permitted, and physical or cognitive disability. Through zoomorphic, nonverbal robots, Emoters seeks to make those benefits more widely available and less costly, including to those who cannot have pets: the disabled elderly, young professionals with little time and much stress, remotely stationed military personnel, and many others. This proposal seeks funding to design (1) simple yet believably lifelike robots; (2) a puppeteer platform that serves as the robots’ eyes, ears, and brains; and (3) a cloud-based system that connects to the platform and permits the rapid improvement of these robots through data collection and experimentation, swiftly increasing their value to users. To achieve these objectives, the Emoters team will apply their expertise in machine learning for human-robot interaction, computer vision, mechatronics engineering, and secure data collection and analytics.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ab790b32c23ad4921264756dbe69f630bc58c064,https://www.semanticscholar.org/paper/ab790b32c23ad4921264756dbe69f630bc58c064,Data Transfer via UAV Swarm Behaviours,"This paper presents an adaptive robotic swarm of Unmanned Aerial Vehicles (UAVs) enabling communications between separated non-swarm devices. The swarm nodes utilise machine learning and hyper-heuristic rule evolution to enable each swarm member to act appropriately for the given environment. The contribution of the machine learning is verified with an exploration of swarms with and without this module. The exploration finds that in challenging environments the learning greatly improves the swarm’s ability to complete the task. The swarm evolution process of this study is found to successfully create different data transfer methods depending on the separation of non-swarm devices and the communication range of the swarm members. This paper also explores the resilience of the swarm to agent loss, and the scalability of the swarm in a range of environment sizes. In regard to resilience, the swarm is capable of recovering from agent loss and is found to have improved evolution. In regard to scalability, the swarm is observed to have no upper limit to the number of agents deployed in an environment. However, the size of the environment is seen to be a limit for optimal swarm performance. Introduction Phillip C. Smith [1] Monash University Robert Hunjet [2] Defence Science and Technology Group, Edinburgh, South Australia Aldeida Aleti [3] Monash Swarm Robotics Laboratory Jan Carlo Barca [4] Deakin University AJTDE Vol 6, No 2 May 2018 [5]",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8a0068d470744acdbb2c073e716e74713dc98a8a,https://www.semanticscholar.org/paper/8a0068d470744acdbb2c073e716e74713dc98a8a,AI Therapist Realizing Expert Verbal Cues for Effective Robot-Assisted Gait Training,"Repetitive and specific verbal cues by a therapist are essential in aiding a patient’s motivation and improving the motor learning process. The verbal cues comprise various expressions, sentences, volumes, and timings, depending on the therapist’s proficiency. This paper proposes an AI therapist (AI-T) that implements the verbal cues of professional therapists having extensive experience with robot-assisted gait training using the SUBAR for stroke patients. The AI-T was developed using a neuro-fuzzy system, a machine learning technique leveraging the benefits of fuzzy logic and artificial neural networks. The AI-T was trained with the professional therapist’s verbal cue data, as well as clinical and robotic data collected from robot-assisted gait training with real stroke patients. Ten clinical data and 16 robotic data are input variables, and six verbal cues are output variables. Fifty-eight stroke patients wore the SUBAR, a gait training robot, and participated in the robot-assisted gait training. A total of 9059 verbal cue data, 580 clinical data of stroke patients, and 144 944 robotic data were collected from 693 training sessions. Test results show that the trained AI-T can implement six types of verbal cues with 93.7% accuracy for the 1812 verbal cue data of the professional therapist. Currently, the trained AI-T is deployed in the SUBAR and provides six verbal cues to stroke patients in robot-assisted gait training.",IEEE Transactions on Neural Systems and Rehabilitation Engineering,2020.0,10.1109/TNSRE.2020.3038175,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7ec15731613ac0dc5d09125d325e7970a5a6a1ea,https://www.semanticscholar.org/paper/7ec15731613ac0dc5d09125d325e7970a5a6a1ea,Communications for IoT: Connectivity and Networking,"The Internet of Things (IoT) is revolutionizing many industries by enabling machines to directly work with each other without human intervention, and communication and networking technologies are fundamental to enabling it. IoT systems are also creating a significant amount of new data which can now be processed to analyze and learn from using novel machine-learning techniques. However, the requirements for communications can vary significantly depending on the target applications, and they can range from ultra-low power for enabling a vast deployment of sensors with multi-year battery life to ultra-reliable low latency communications (URLLC) for smart factories and remote robot control. 5G, as an example, is expected to deliver a number of enhancements to support massive IoT deployments as well as URLLC capabilities for new verticals such as Smart Cities, Industrial IoT, E-Health, Public Safety, and Autonomous Vehicles (V2V, V2X). The purpose of this Special Issue is to provide researchers and practitioners working on Connectivity and Networking technologies for IoT systems a means to share experiences and disseminate successful applications of IoT technologies and identify recent trends and opportunities poised to revolutionize the IoT industry.",IEEE Internet of Things Magazine,2020.0,10.1109/MIOT.2020.9063399,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4679ace4728f5ecb8a12e98ce40a3972bd78db2a,https://www.semanticscholar.org/paper/4679ace4728f5ecb8a12e98ce40a3972bd78db2a,Analysis of Computer Vision Methods for Counting Surgical Instruments,"Dear Editor, The surgical count is the primary method to account for and manage surgical instruments, needles, and sponges during operative procedures. However, a miscount, when there is a discrepancy between counted and deployed instruments, is estimated to occur in roughly 1 in 140 cases. These events often result in significant costs to hospitals and patients per year due to lost operating room (OR) time and secondary imaging procedures. An estimated 1 in 70 miscounts result in a retained surgical instrument, resulting in patient harm, as well costly reoperation and litigation. Given the high cost of OR time and the current burden of counting procedures, it is clear that a more accurate and less labor-intensive counting system is needed. In recent years, there is tremendous growth in machine learning technology applied to healthcare. Computer vision techniques are now applied across many medical domains and are most visible in the context of minimally invasive surgery and endoscopic surgery. We conducted a literature search of computer vision studies on the detection or localization of surgical instruments outside of the surgical field. We are highlighting 4 studies that provide insight into both the feasibility and challenges of utilizing existing computer vision techniques to build a system that can perform the surgical count. The 4 studies, summarized in Table 1, implemented a wide range of computer vision techniques to localize different types of surgical items, with relatively high detection accuracies ranging from 89% to 95%. Various algorithms were tested, including: instrument barcoding with template matching, random forest, and convolutional neural networks. Three studies presented their object detection models in the context of a robot manipulator which could pick up the detected instrument. If computer vision is to have widespread adoption as a modality to perform the surgical count, the underlying object detection and tracking algorithms must be robust to the large number and types of surgical objects. The categories of objects present in an operative setting include soft disposables, such as laparotomy sponges, hard disposables, such as surgical needles, and instruments, such as hemostats. However, all 4 of the studies only considered object detection on a limited set of instruments and hard disposables. In particular, none of the studies attempted detection of surgical sponges and needles, 2 of the most commonly miscounted items in the OR. Data standardization and algorithmic benchmarking is another concern, as only 2 studies released their datasets publicly. Both datasets are limited in size (3200 and 3009 images, respectively), number of object categories, and types of objects, as Zhou and Wachs considered 5 instrument categories (scalpel, retractor, hemostat, scissors and babcock forceps), while Lavado considered 4 instrument categories (scalpel, straight dissection clamp, straight mayo scissor and curved mayo scissor). Furthermore, neither dataset contains annotations with object identifiers for tracking objects across a sequence of images. A collaborative effort between research groups should be established to create and publicly release a sufficiently large and varied dataset for detection and tracking. This would allow for a standardized process to evaluate the performance of computer vision algorithms at the surgical counting task.",Surgical innovation,2020.0,10.1177/1553350620956425,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e2ae8e565a6cc5f1cb206632bc837402a059be32,https://www.semanticscholar.org/paper/e2ae8e565a6cc5f1cb206632bc837402a059be32,Bail or Jail? Judicial Versus Algorithmic Decision-Making in the Pretrial System,"To date, there are approximately sixty risk assessment tools deployed in the criminal justice system. These tools aim to differentiate between low-, medium-, and high-risk defendants and to increase the likelihood that only those who pose a risk to public safety or who are likely to flee are detained. Proponents of actuarial tools claim that these tools are meant to eliminate human biases and to rationalize the decision-making process by summarizing all relevant information in a more efficient way than can the human brain. Opponents of such tools fear that in the name of science, actuarial tools reinforce human biases, harm defendants’ rights, and increase racial disparities in the system. The gap between the two camps has widened in the last few years. Policymakers are torn between the promise of technology to contribute to a more just system and a growing movement that calls for the abolishment of the use of actuarial risk assessment tools in general and the use of machine learning-based tools in particular. 
This paper examines the role that technology plays in this debate and examines whether deploying artificial intelligence (“AI”) in existing risk assessment tools realizes the fears emphasized by opponents of automation or improves our criminal justice system. It focuses on the pretrial stage and examines in depth the seven most commonly used tools. Five of these tools are based on traditional regression analysis, and two have a machine-learning component. This paper concludes that classifying pretrial risk assessment tools as AI-based tools creates the impression that sophisticated robots are taking over the courts and pushing judges from their jobs, but that impression is far from reality. Despite the hype, there are more similarities than differences between tools based on traditional regression analysis and tools based on machine learning. Robots have a long way to go before they can replace judges, and this paper does not argue for replacement. The long list of policy recommendations discussed in the last chapter highlights the extensive work that needs to be done to ensure that risk assessment tools are both accurate and fair toward all members of society. These recommendations apply regardless of whether machine learning or regression analysis is used. Special attention is paid to assessing how machine learning would impact those recommendations. For example, this paper argues that carefully detailing each of the factors used in the tools and including multiple options to choose from (i.e., not just binary “yes-or-no” questions) will be useful for both regression analysis and machine learning. However, machine learning would likely lead to more personalized and meaningful scoring of criminal defendants because of the ability of machine learning techniques to “zoom in” on the unique details of each individual case.",,2020.0,10.7916/STLR.V21I2.6838,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
71104a613283ae095345cdb4becc18303fcecff3,https://www.semanticscholar.org/paper/71104a613283ae095345cdb4becc18303fcecff3,Intelligent Data Engineering and Automated Learning – IDEAL 2013,,Lecture Notes in Computer Science,2013.0,10.1007/978-3-642-41278-3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9581fb2c51197911e3e185cb61e632dca323c165,https://www.semanticscholar.org/paper/9581fb2c51197911e3e185cb61e632dca323c165,Artificial intelligence in nuclear cardiology: Preparing for the fifth industrial revolution,,Journal of Nuclear Cardiology,2021.0,10.1007/s12350-021-02671-1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fd59f3c4054f09326414f4cd0841f72e6d37079f,https://www.semanticscholar.org/paper/fd59f3c4054f09326414f4cd0841f72e6d37079f,Face Recognition and Classification Using GoogleNET Architecture,,SocProS,2019.0,10.1007/978-981-15-0035-0_20,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d218d20654670759e506580b535c3e9eb1b3dbc6,https://www.semanticscholar.org/paper/d218d20654670759e506580b535c3e9eb1b3dbc6,Self-Supervised Damage-Avoiding Manipulation Strategy Optimization via Mental Simulation,,Intelligent Service Robotics,2017.0,10.1007/s11370-019-00286-7,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fdfe6f06aad5bec0bd816cf90f7734bc0dcc7835,https://www.semanticscholar.org/paper/fdfe6f06aad5bec0bd816cf90f7734bc0dcc7835,Towards Automated Planning for Enterprise Services: Opportunities and Challenges,,ICSOC,2019.0,10.1007/978-3-030-33702-5_6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
43f1a2d83868c882a32cf148950e1ee00eb81977,https://www.semanticscholar.org/paper/43f1a2d83868c882a32cf148950e1ee00eb81977,YOLOpeds: Efficient Real-Time Single-Shot Pedestrian Detection for Smart Camera Applications,"Deep Learning-based object detectors can enhance the capabilities of smart camera systems in a wide spectrum of machine vision applications including video surveillance, autonomous driving, robots and drones, smart factory, and health monitoring. Pedestrian detection plays a key role in all these applications and deep learning can be used to construct accurate state-of-the-art detectors. However, such complex paradigms do not scale easily and are not traditionally implemented in resource-constrained smart cameras for on-device processing which offers significant advantages in situations when real-time monitoring and robustness are vital. Efficient neural networks can not only enable mobile applications and on-device experiences but can also be a key enabler of privacy and security allowing a user to gain the benefits of neural networks without needing to send their data to the server to be evaluated. This work addresses the challenge of achieving a good trade-off between accuracy and speed for efficient deployment of deep-learning-based pedestrian detection in smart camera applications. A computationally efficient architecture is introduced based on separable convolutions and proposes integrating dense connections across layers and multi-scale feature fusion to improve representational capacity while decreasing the number of parameters and operations. In particular, the contributions of this work are the following: 1) An efficient backbone combining multi-scale feature operations, 2) a more elaborate loss function for improved localization, 3) an anchor-less approach for detection, The proposed approach called YOLOpeds is evaluated using the PETS2009 surveillance dataset on 320x320 images. Overall, YOLOpeds provides real-time sustained operation of over 30 frames per second with detection rates in the range of 86% outperforming existing deep learning models.",IET Comput. Vis.,2020.0,10.1049/iet-cvi.2019.0897,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2fd1ba265854ac769d6a0810472962b8312fe7e3,https://www.semanticscholar.org/paper/2fd1ba265854ac769d6a0810472962b8312fe7e3,A Comprehensive Review of the Fourth Industrial Revolution IR 4.0 in Oil and Gas Industry,"
 Digital transformation is one of the most discussed themes across the globe. The disruptive potential arising from the joint deployment of IoT, robotics, AI and other advanced technologies is projected to be over $300 trillion over the next decade. With the advances and implementation of these technologies, they have become more widely-used in all aspects of oil and gas industry in several processes. Yet, as it is a relatively new area in petroleum industry with promising features, the industry overall is still trying to adapt to IR 4.0. This paper examines the value that Industry 4.0 brings to the oil and gas upstream industry. It delineates key Industry 4.0 solutions and analyzes their impact within this segment. A comprehensive literature review has been carried out to investigate the IR 4.0 concept's development from the beginning, the technologies it utilizes, types of technologies transferred from other industries with a longer history of use, robustness and applicability of these methods in oil and gas industry under current conditions and the incremental benefits they provide depending on the type of the field are addressed. Real field applications are illustrated with applications indifferent parts of the world with challenges, advantages and drawbacks discussed and summarized that lead to conclusions on the criteria of application of machine learning technologies.","Day 1 Tue, October 12, 2021",2021.0,10.2118/205772-ms,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6f3811bc5f307336fcdcb46a7b61f1602920ae77,https://www.semanticscholar.org/paper/6f3811bc5f307336fcdcb46a7b61f1602920ae77,Dynamic Voltage and Frequency Scaling to Improve Energy-Efficiency of Hardware Accelerators,"Neural networks (NNs) have been used in a wide variety of artificial intelligence (AI) applications, including speech recognition, image recognition, automatic robotics, and games. State-of-the-art NNs provide high prediction accuracy at the expense of massive computation that involves large model parameters which consume substantial energy. Though sparse NN s have emerged to reduce the computation and storage overhead, existing specialized DNN accelerators cannot maximize the energy savings when exploiting both dynamic and static sparsity, especially for irregular NNs. In this paper, we propose a dynamic voltage and frequency scaling (DVFS) based hardware accelerator that effectively exploits the dynamic and static sparsity of NNs with dynamic voltage/frequency (V/F) scaling and power gating techniques to reduce both static and dynamic power. To explore the efficiency of DVFS implementation at different granularities, we evaluate both coarse-grained and fine-grained DVFS implementation with different design trade-offs. Further, our proposed DVFS model predicts the dynamic computation workloads as well as V/ F pairs to be supplied to processing elements (PEs) in the hardware intelligently through pre-trained weight vectors. The machine learning based prediction algorithm is deployed to improve the DVFS mode selection accuracy. Our simulation results on AlextNet, VGG16, and ResNet50 show that we can achieve an average dynamic energy savings of 59–66 % and an average static power reduction of 69–80 % compared to the baseline.","2021 IEEE 28th International Conference on High Performance Computing, Data, and Analytics (HiPC)",2021.0,10.1109/HiPC53243.2021.00037,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6f3811bc5f307336fcdcb46a7b61f1602920ae77,https://www.semanticscholar.org/paper/6f3811bc5f307336fcdcb46a7b61f1602920ae77,Dynamic Voltage and Frequency Scaling to Improve Energy-Efficiency of Hardware Accelerators,"Neural networks (NNs) have been used in a wide variety of artificial intelligence (AI) applications, including speech recognition, image recognition, automatic robotics, and games. State-of-the-art NNs provide high prediction accuracy at the expense of massive computation that involves large model parameters which consume substantial energy. Though sparse NN s have emerged to reduce the computation and storage overhead, existing specialized DNN accelerators cannot maximize the energy savings when exploiting both dynamic and static sparsity, especially for irregular NNs. In this paper, we propose a dynamic voltage and frequency scaling (DVFS) based hardware accelerator that effectively exploits the dynamic and static sparsity of NNs with dynamic voltage/frequency (V/F) scaling and power gating techniques to reduce both static and dynamic power. To explore the efficiency of DVFS implementation at different granularities, we evaluate both coarse-grained and fine-grained DVFS implementation with different design trade-offs. Further, our proposed DVFS model predicts the dynamic computation workloads as well as V/ F pairs to be supplied to processing elements (PEs) in the hardware intelligently through pre-trained weight vectors. The machine learning based prediction algorithm is deployed to improve the DVFS mode selection accuracy. Our simulation results on AlextNet, VGG16, and ResNet50 show that we can achieve an average dynamic energy savings of 59–66 % and an average static power reduction of 69–80 % compared to the baseline.","2021 IEEE 28th International Conference on High Performance Computing, Data, and Analytics (HiPC)",2021.0,10.1109/HiPC53243.2021.00037,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c83414b4deba5065606edb8a9ad2c4bd3aaa4fb2,https://www.semanticscholar.org/paper/c83414b4deba5065606edb8a9ad2c4bd3aaa4fb2,Application of Bellman's Equation in Ant-Like Robotic Device Path Decisions,"Swarm Intelligence is about emergency of collective intelligence from groups of homogeneous robotic devices deployed for a purpose. Ant Colony Systems, in particular, are inspiring. They commonly have drawn inspiration from the behaviors of real ants in nature in order to construct routes between the food sources and the nest. There are still gaps in alternative options for path decision in ant agents. Bellman's equation has been successfully used to solve path decision problems in machine learning. We proposed to investigate impact of a Bellman's equation inspired algorithm for path decision on stigmergic ant agent robotic devices. A design science research paradigm was used to design our research experiment in which a simulated environment was designed to simulate the behavior of ant agents when using a Bellman's equation inspired algorithm for path decision. We introduced a reward function to the orientation process of ant agents. Reward function rewards a decision made when an ant moves from one point to an adjacent cell. The Bellman's inspired algorithm for ant orientation led to convergence of ant agents even though there was reduced quality of convergence. Evaluation of results show that Bellman's equation can be used in path decision processes for ant agent robotic devices. Our results contributed to adding an alternative way of implementing path decision for ant agents. This will help in growing the knowledge around ant agents and finding better ways to implementing path decisions for ant agents.",2020 2nd International Multidisciplinary Information Technology and Engineering Conference (IMITEC),2020.0,10.1109/IMITEC50163.2020.9334099,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6dadcf58327c69d94f4dcc87e7cfc2b2a4047c6b,https://www.semanticscholar.org/paper/6dadcf58327c69d94f4dcc87e7cfc2b2a4047c6b,Semi-Automatic Generation of Training Data for Neural Networks for 6D Pose Estimation and Robotic Grasping,". Machine-learning-based approaches for pose estimation are trained using annotated ground-truth data – images showing the object and information of its pose. In this work an approach to semi-automatically generate 6D pose-annotated data, using a movable marker and an articulated robot, is presented. A neural network for pose estimation is trained using datasets varying in size and type. The evaluation shows that small datasets recorded in the target domain and supplemented with augmented images lead to more robust results than larger synthetic datasets. The results demonstrate that a mobile ma-nipulator using the proposed pose-estimation system could be deployed in real-life logistics applications to the level of automation.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9832c224be912307ee934e17b40ea1b7f30a2172,https://www.semanticscholar.org/paper/9832c224be912307ee934e17b40ea1b7f30a2172,Editorial for special section “CACS18: Modelling and control for practical systems”,"Application of control or modelling technology covers many diverse fields of practical systems. However, many related methods for modelling and control need massive computation and then are not realizable in the previous ages. Due to the massive progress in the computation ability of computer recently, many algorithms for control and modelling of practical systems, which are hard to be implemented in the previous ages, are realizable now. Especially, some artificial intelligent methods, for example, soft computing and machine learning, are developed and performed well based on the powerful hardware of the computer nowadays. These AI-related techniques are helpful to improve the control and modelling of practical systems. This special issue aims to collect original papers that address emerging issues in control and modelling of practical systems based on artificial intelligent method and to encourage readers to explore this promising and challenging research area. We place particular emphasis on contributions dealing with practical applications, including the use of advanced methods and devices to show recent research in control and modelling. The application area of the papers involved in this special issue includes automotive engine control systems, chemical processes, power systems and power electronics, robot manufacture, water quality control systems, robot navigation, multiaxis CNC machine, security of communication systems, and emotion detection of people, and so forth. In this special issue, the paper by Chang-Hua Lien, Ker-Wei Yu, and Hao-Chin Chang analyzes the asymptotic mean-square stabilizability and mixed performance of switched time-delay systems with time-varying random delay via a switching signal selection. The switched time-delay systems can be modelled from some practical systems, such as automotive engine control systems, chemical processes, power systems, and power electronics. The paper by Jyun-Yu Jhang, Chin-Ling Lee, ChengJian Lin, and Kuu-Young Young proposes a novel method for multi-robot deployment and navigation under dynamic environments. This paper adopts an interval type-2 fuzzy controller (IT2FC). The parameters of the IT2FC are adjusted by using the dynamic group whale optimization algorithm (DGWOA). The paper by Ko-Fei Lee and Ching-Hung Lee proposes a servo-tuning system for five-axis machine tools using a virtual feed drive system in which the particle swarm optimization (PSO) algorithm was adopted to identify and estimate the optimal system parameters. The paper by Feng-Hsiag Hsiao presents a methodology of systematic design of an improved genetic algorithm (IGA)-based secure communication in multiple time-delay chaotic (MTDC) systems with advanced encryption standard (AES) and optimal H∞ performance. Finally, in the paper by Shing-Tai Pan and Wei-Ching Li, a Fuzzy Hidden Markov Model (FHMM) for electrocardiogram (ECG)-based emotion recognition is proposed. The FHMM model proposed in this paper can improve the precision of traditional discrete HMM modeling. And the FHMM is then used for recognition of emotion based on ECG signals. The readers of this special issue can have a closer exploration to these papers and find more details from these papers. It is our wish that the reader will benefit and be motivated after reading this special issue. Finally, the guest editors wish to thank Editor-in-Chief of Asian Journal of Control, Professor Li-Chen Fu, for providing this valuable opportunity. The guest editors also wish to thank all the authors and the reviewers for their contributions to this special issue.",Asian Journal of Control,2020.0,10.1002/asjc.2488,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0a089212d4b2452426ae22de1ddd17cee1802c18,https://www.semanticscholar.org/paper/0a089212d4b2452426ae22de1ddd17cee1802c18,Adithri – (F2) The Farmer’s Friend,,Lecture Notes in Electrical Engineering,2020.0,10.1007/978-981-15-7961-5_22,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e600dfa52ba523f8f712af7039dfe14ac3e82283,https://www.semanticscholar.org/paper/e600dfa52ba523f8f712af7039dfe14ac3e82283,Development of a Neural Network Library for Resource Constrained Speech Synthesis,"Machine learning frameworks, like Tensorflow and PyTorch, use GPU hardware acceleration to deliver the needed performance. Since GPUs require a lot of power (and space) to operate, typical use cases involve high-performance servers, with the final deployment available as a cloud service. To address limitations of this approach, AI Accelerators have been proposed. In this context, we have designed and implemented a library of neural network algorithms, to efficiently run on “edge devices”, with AI Accelerators. Moreover, a unified interface has been provided, to allow easy experimentation with various neural networks applied to the same dataset. Here, let us stress that we do not propose new algorithms, but port known ones to, resource restricted, edge devices. The context is provided by a speech synthesis application for edge devices that is deployed on an NVIDIA Jetson Nano. This application is to be used by social robots for real-time off-cloud text-to-speech processing.",2020 5th IEEE International Conference on Recent Advances and Innovations in Engineering (ICRAIE),2020.0,10.1109/ICRAIE51050.2020.9358310,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b8317ef9bcf24c688c67e5062cb388e53b2dd150,https://www.semanticscholar.org/paper/b8317ef9bcf24c688c67e5062cb388e53b2dd150,Decoding Motor Skills of AI and Human Policies: A Study on Humanoid and Human Balance Control,"In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of utilising the power of machine learning in addition to other options that intent to use learning directly in real-world applications. We first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots that exhibit human-like push recovery behaviour. Next, we propose to take advantage of DRL to quickly discover solutions for very difficult problems, and then extract the principles of those policies as guidelines for developing engineered controllers. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms can learn a good policy with short development and training time that may require humans years to learn. We analyse input-output data collected from humanoid and human policies and postulate a Minimum-Jerk ModelPredictive Control (MJMPC) Framework that quantitatively reflects both AI and human push recovery policies. I. SCIENTIFIC MOTIVATION From the advancement in computers, computer-aided design for mechanical and electronic engineering, architecture and many other engineering fields emerged. Foreseeing a similar development curve and technology wave, we forecast a new emerging discipline in the near future that uses learning-aided approaches for catalysing control development, alongside other similar applications such as in medicine discovery. In this study, we propose a new paradigm of using a machine learning approach to facilitate a quicker, more efficient and effective control development, as a different approach of leveraging the power of machine learning in addition to other options that intent to use learning directly in real-world applications. Machine Learning and Deep Reinforcement Learning (DRL) in particular have reached an advanced stage to produce powerful policies with better autonomous performances than many state-of-the-art control and planning approaches in robot locomotion [1], robotic manipulation [2], and even the control of complex morphological machines [3]. Notably, DRL’s ability to solve complex problems with a relatively short development time is especially attractive, which is empowered by training policies that maximise the cumulative reward through the exploration of the action and state space, rather than using prior knowledge of the models about the robot, the world, and their interactions. To leverage the capabilities of DRL, we first develop a DRL-based control framework to learn rich motor skills of push recovery for humanoid robots. The complexity in whole-body balancing arises in challenges such as multi(a) Ankle Strategy (b) Hip Strategy (c) Toe Strategy (d) Step Strategy Fig. 1: Human-like Push Recovery strategies emerging from Deep Reinforcement Learning. The discovered behaviours serve as a guideline for the design of certifiable and safe controllers that replicate advantageous strategies from AI policies. contact coordination based on multi-sensory inputs, state transitions between fullyand under-actuated situations, switching policies, and generalising to external disturbances on any body parts, while accounting for all edge cases that a designer has difficulty to consider beforehand. In such a setting, manually designing the individual control strategies and finding a reliable switching mechanism requires both substantial development time, mathematical rigour, and code implementation. On the other hand, through a well-designed DRL framework and task-specific training procedures, a robust policy can be learned automatically by interacting with the environment, requiring only computational power. In particular, as shown in Fig. 1, our learned policy exhibits human-like push recovery behaviour with four typical push recovery strategies emerging naturally: ankle, hip, toe, and IEEE Robotics and Automation Magazine (RAM) paper, presented at IROS 2020. It should be cited as a RAM paper. stepping strategy. Though the learned control policy could possibly be deployed on the real robotic system, the lack of explainability and analytical reasoning of the Neural Network makes it unsuitable for safety-critical applications in real world. Furthermore, due to the demand of large data and sampleinefficient nature of DRL algorithms, complex policies are typically trained in simulation, which cannot guarantee the same performance while transferred directly to the real system [1], and the challenge of reality gap raises concerning about both the safety and performance. To benefit from both the safety and interpretability for the control policy and the versatility and adaptability from learning, we propose to take advantage of DRL to quickly discover versatile, deployable policies and solutions for very difficult problems, and then study, analyse and extract the principles of those policies as guidelines for developing engineered controllers in a reliable manner. By doing so, we utilise the AI-solutions for rapid control development (Fig. 3) to design safe and certifiable controllers which can be verified and deployed on real-world robots (Fig. 2). While classical control development is based on gradually building knowledge that increases the performance incrementally, using a template policy will provide disruptive, innovative solutions that will escalate performance (green line, Fig. 3). DRL is able to achieve good performance by a number of iterations in the DRL learning framework. However, the achieved performance is still comparatively low to what tuning in control can do. Combining both approaches to “kick-start” the iteration process helps to design good controllers. After knowing the system and the controller, it is straightforward to improve upon due to the fact that we are then able to understand why the performance is lower than the optimum, whereas in the case of DRL, there is little influence from human engineers to improve the performance but reshaping the reward and/or altering the learning framework, and relying on the exploration being sufficiently large to achieve high performance. In this paper, we are motivated to study a viable approach to infer underlying principles of an AI policy by studying its perception-action relation, i.e., to some extent, reverseengineer an equivalent controller in terms of functionality based on a black-box policy. This methodology is not only applicable to AI policies, but also to any black-box policies, such as a human policy. Without knowing exactly how push recovery policies are realised by Artificial Neural Network (ANN) or biological human Neural Network, we can still analyse the behaviour at the functionality level by studying their input-output relationship. Based on evidence of optimality in human manipulation tasks [4], we hypothesise that policies for push recovery in humans and humanoid are both optimal control process that follows certain optimal criteria that can be quantified. Following this hypothesis, we analyse and utilise inputoutput data collected from both humanoid and human policies, and propose a Minimum-Jerk Model-Predictive Control (MJMPC) Framework that is able to quantitatively reflect both the AI and human push recovery policies. The engineered controller has high similarity (Coefficient of Determination more than 90%) with the collected data, and also exhibits the same human-like push recovery strategies, which emerge from the proposed MJMPC without the need of manual switching between the strategies. Furthermore, a comparison between humanoid and human balancing is conducted to show the characteristics of the learned humanoid behaviour. This comparison will show that DRL algorithms are very powerful to learn a policy (e.g., balancing) within a short development and training time that may require humans years to learn. In contrast, in order to design an engineered controller from scratch with similar performance, months or even years are needed for developmental iterations, mainly because of the high-redundancy and a diversity of control actions, which are yet challenging to resolve the physical optimality on a high Degree of Freedom (DoF) robot. In this regard, the learning approach is very attractive because of the significant reduction of manual effort, and the learning architecture requires only the design of input-output and rewards. This article shed some light on a new paradigm: the recent high-profile successes in DRL suggest new high-quality value in learning methods that the discovered policies can be used as a basis for speeding up the development of robotic controllers (Fig. 2). As an outcome in this push recovery study, we obtain a certifiable, analysable optimal controller that does not require any state machine or switching mechanism, while exhibiting human-like push recovery strategies, such as ankle, hip, toe, and stepping strategy all in a coherent optimisation process. II. GENERATING COMPLEX MOTIONS FOR HUMANOID ROBOTS THROUGH DEEP REINFORCEMENT LEARNING To use DRL-policies as a basis for analysis, these policies must reach a certain performance threshold that ideally surpasses traditional control approaches both in the types of motions it can generate and the amount of disturbances that it can withstand. DRL has been shown to be capable of learning locomotion and fall recovery policies that surpasses traditional control approaches for quadruped robots in terms of power efficiency, and versatility of motion [1]. In this section, we present a hierarchical learning framework for achieving versatile behaviours during push recovery for humanoid robots as proposed in [5]. The learned policy exhibits a wide range of balancing strategies that are comparable to human push recovery. In particular, the learned policy is able to withstand external distu",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b01665121efe7b61bf8a5180cdb0decaf874f891,https://www.semanticscholar.org/paper/b01665121efe7b61bf8a5180cdb0decaf874f891,Adaptive Terminal Sliding Mode Control For Nonlinear,"Modelling and Control of Mechatronic and Robotic SystemsAdvanced Machine Learning Technologies and ApplicationsUnmanned Driving Systems for Smart TrainsInformatics in Control, Automation and RoboticsCyberspace Safety and SecurityAdaptive Identification and Control of Uncertain Systems with Non-smooth DynamicsElectrical Engineering and ControlSoft Computing: Theories and ApplicationsElectronics and Signal ProcessingAdvances in Reconfigurable Mechanisms and Robots IControl of Chaos in Nonlinear Circuits and SystemsSliding Modes after the first Decade of the 21st CenturyModeling and Control of Static Converters for Hybrid Storage SystemsInternational Conference on Electrical, Control and Automation (ICECA 2014)Sliding Mode Controllers for Power Electronic ConvertersIntelligent Computing Theories and ApplicationCognitive Systems and Signal ProcessingInertial Quasi-Velocity Based Controllers for a Class of VehiclesApplied Methods and Techniques for Mechatronic SystemsUnmanned Aerial SystemsSliding Mode Control (SMC)Variable Structure Systems: Towards the 21st CenturyControl and AutomationA Robust Adaptive Terminal Sliding Mode Control for Rigid Robotic ManipulatorsWind Turbine Control and MonitoringAdvances in Variable Structure Systems and Sliding Mode Control—Theory and ApplicationsDynamical SystemsMICAI 2004: Advances in Artificial IntelligenceControl of Power Electronic Converters and SystemsCommunications in MicrogridsElectrical, Information Engineering and Mechatronics 2011Advanced Robust Nonlinear Control Approaches for Quadrotor Unmanned Aerial VehicleProceedings of 2016 Chinese Intelligent Systems ConferenceStudy on Adaptive Fuzzy Terminal Sliding-Mode Control and Adaptive Terminal Sliding-Function Control2011 International Conference in Electrics, Communication and Automatic Control ProceedingsAdaptive Sliding Mode Neural Network Control for Nonlinear SystemsMem-elements for Neuromorphic Circuits with Artificial Intelligence ApplicationsProceedings of the International Conference on Advanced Intelligent Systems and Informatics 2019Mathematical Techniques of Fractional Order SystemsIntelligent Computing Methodologies Adaptive Identification and Control of Uncertain Systems with Nonsmooth Dynamics reports some of the latest research on modeling, identification and adaptive control for systems with nonsmooth dynamics (e.g., backlash, dead zone, friction, saturation, etc). The authors present recent research results for the modelling and control designs of uncertain systems with nonsmooth dynamics, such as friction, dead-zone, saturation and hysteresis, etc., with particular applications in servo systems. The book is organized into 19 chapters, distributed in five parts concerning the four types of nonsmooth characteristics, namely friction, dead-zone, saturation and hysteresis, respectively. Practical experiments are also included to validate and exemplify the proposed approaches. This valuable resource can help both researchers and practitioners to learn and understand nonlinear adaptive control designs. Academics, engineers and graduate students in the fields of electrical engineering, control systems, mechanical engineering, applied mathematics and computer science can benefit from the book. It can be also used as a reference book on adaptive control for servo systems for students with some background in control engineering. Explains the latest research outputs on modeling, identification and adaptive control for systems with nonsmooth dynamics Provides practical application and experimental results for robotic systems, and servo motorsAdvances in Reconfigurable Mechanisms and Robots I provides a selection of key papers presented in The Second ASME/IFToMM International Conference on Reconfigurable Mechanisms and Robots (ReMAR 2012) held on 9th -11th July 2012 in Tianjin, China. This ongoing series of conferences will be covered in this ongoing collection of books. A total of seventy-eight papers are divided into seven parts to cover the topology, kinematics and design of reconfigurable mechanisms with the reconfiguration theory, analysis and synthesis, and present the current research and development in the field of reconfigurable mechanisms including reconfigurable parallel mechanisms. In this aspect, the recent study and development of reconfigurable robots are further presented with the analysis and design and with their control and development. The bio-inspired mechanisms and subsequent reconfiguration are explored in the challenging fields of rehabilitation and minimally invasive surgery. Advances in Reconfigurable Mechanisms and Robots I further extends the study to deployable mechanisms and foldable devices and introduces applications of reconfigurable mechanisms and robots. The rich-content of Advances in Reconfigurable Mechanisms and Robots I brings together new developments in reconfigurable mechanisms and robots and presents a new horizon for future development in the field of reconfigurable mechanisms and robots.Mem-elements for Neuromorphic Circuits with Artificial Intelligence Applications illustrates recent advances in the field of mem-elements (memristor, memcapacitor, meminductor) and their applications in nonlinear dynamical systems, computer science, analog and digital systems, and in neuromorphic circuits and artificial intelligence. The book is mainly devoted to recent results, critical aspects and perspectives of ongoing research on relevant topics, all involving networks of mem-elements devices in diverse applications. Sections contribute to the discussion of memristive materials and transport mechanisms, presenting various types of physical structures that can be fabricated to realize mem-elements in integrated circuits and device modeling. As the last decade has seen an increasing interest in recent advances in mem-elements and their applications in neuromorphic circuits and artificial intelligence, this book will attract researchers in various fields. Covers a broad range of interdisciplinary topics between mathematics, circuits, realizations, and practical applications related to nonlinear dynamical systems, nanotechnology, analog and digital systems, computer science and artificial intelligence Presents recent advances in the field of mem-elements (memristor, memcapacitor, meminductor) Includes interesting applications of mem-elements in nonlinear dynamical systems, analog and digital systems, neuromorphic circuits, computer science and artificial intelligenceThis volume includes extended and revised versions of a set of selected papers from the International Conference on Electric and Electronics (EEIC 2011) , held on June 20-22 , 2011, which is jointly organized by Nanchang University, Springer, and IEEE IAS Nanchang Chapter. The objective of EEIC 2011 Volume 2 is to provide a major interdisciplinary forum for the presentation of new approaches from Electrical engineering and controls, to foster integration of the latest developments in scientific research. 133 related topic papers were selected into this volume. All the papers were reviewed by 2 program committee members and selected by the volume editor Prof. Min Zhu. We hope every participant can have a good opportunity to exchange their research ideas and results and to discuss the state of the art in the areas of the Electrical engineering and controls.Adaptive Sliding Mode Neural Network Control for Nonlinear Systems introduces nonlinear systems basic knowledge, analysis and control methods, and applications in various fields. It offers instructive examples and simulations, along with the source codes, and provides the basic architecture of control science and engineering. Introduces nonlinear systems' basic knowledge, analysis and control methods, along with applications in various fields Offers instructive examples and simulations, including source codes Provides the basic architecture of control science and engineeringThis book proposes a proportional integral type sliding function, which does not facilitate the finite reaching and hence the responses of the load voltage results in an exponential steady state. To facilitate finite time reaching, it also presents the new Integral Sliding Mode Control with Finite Time Reaching (ISMCFTR). The book also extends the application of the proposed controller to another type of PEC, the DC-DC Boost converter, and also proposes the PI type sliding surface for the Zeta converter, which is non-inverting type Buck Boost converter. An important source of practical implementations, it presents practical implementations as simulation and experimental results to demonstrate the efficacy of the converter.Control of Power Electronic Converters and Systems, Volume 3, explores emerging topics in the control of power electronics and converters, including the theory behind control, and the practical operation, modeling, and control of basic power system models. This book introduces the most important controller design methods, including both analog and digital procedures. This reference explains the dynamic characterization of terminal behavior for converters, as well as preserving the stability and power quality of modern power systems. Useful for engineers in emerging applications of power electronic converters and those combining control design methods into different applications in power electronics technology. Addressing controller interactions in light of increasing renewable energy integration and related challenges with stability and power quality is becoming more frequent in power converters and passive components. Discusses different applications and their control in integrated renewable energy systems Introduces the most important controller design methods, both in analog and digital Describes different important applications to be used in future industrial products Explains the dynamic characterization of terminal behavior for convertersThis book presents the refereed proceedin",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b525e1354507298f07a51563e6f09eb0ce868a87,https://www.semanticscholar.org/paper/b525e1354507298f07a51563e6f09eb0ce868a87,IV-Posterior: Inverse Value Estimation for Interpretable Policy Certificates,"Model-free reinforcement learning (RL) is a powerful tool to learn a broad range of robot skills and policies. However, a lack of policy interpretability can inhibit their successful deployment in downstream applications, particularly when differences in environmental conditions may result in unpredictable behaviour or generalisation failures. As a result, there has been a growing emphasis in machine learning around the inclusion of stronger inductive biases in models to improve generalisation. This paper proposes an alternative strategy, inverse value estimation for interpretable policy certificates (IV-Posterior), which seeks to identify the inductive biases or idealised conditions of operation already held by pre-trained policies, and then use this information to guide their deployment. IV-Posterior uses MaskedAutoregressive Flows to fit distributions over the set of conditions or environmental parameters in which a policy is likely to be effective. This distribution can then be used as a policy certificate in downstream applications. We illustrate the use of IV-Posterior across a two environments, and show that substantial performance gains can be obtained when policy selection incorporates knowledge of the inductive biases that these policies hold.",ArXiv,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c43025c429b1fbf6f1379f61801a1b40834d62e7,https://www.semanticscholar.org/paper/c43025c429b1fbf6f1379f61801a1b40834d62e7,Convolutional networks and applications in vision,"Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or ""features"")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.",Proceedings of 2010 IEEE International Symposium on Circuits and Systems,2010.0,10.1109/ISCAS.2010.5537907,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a3b740be485b9ad2296dbdc09c874c036e5619d4,https://www.semanticscholar.org/paper/a3b740be485b9ad2296dbdc09c874c036e5619d4,"Development, applications and benefits of the network digital twin","The network digital twin is the must-have technology of any advanced DSO. One of the main challenges for DSOs is deploying the most efficient and effective solutions for operation and maintenance of electricity network and infrastructures. New technologies that are flourishing in the market show high potential for the electricity industry, too. Examples are cloud and edge-computing, artificial intelligence (AI) with machine learning, IoT sensors, wearables, augmented and virtual reality, 3D modelling, LIDAR, drones and robotics, advanced simulation and modelling software. Their adoption by DSOs and integration with existing systems (such as GIS AMI, ADMS) offer new innovative approaches to distribution systems operation, epitomized by the network digital twin. This paper describes how existing and new technologies combine to realize the network digital twin, its relevant applications and benefits. Several practical examples and lessons learned from proof of concepts, pilots and innovation projects carried out with start-ups, technology providers and data scientists are illustrated. KEYWORKDS: Network Digital Twin, artificial intelligence, 3D-modelling, IoT sensors, augmented reality.",,2019.0,10.34890/974,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
27dfc1860c14c7b7574e84c394ce0ac5c3eeaa69,https://www.semanticscholar.org/paper/27dfc1860c14c7b7574e84c394ce0ac5c3eeaa69,An IoT Reconfigurable SoC Platform for Computer Vision Applications,"The field of Internet of Things (IoT) and smart sensors has expanded rapidly in various fields of research and industrial applications. The area of IoT robotics has become a critical component in the evolution of Industry 4.0 standard. In this paper, we developed an IoT based reconfigurable System on Chip (SoC) robot that is fast and efficient for computer vision applications. It can be deployed in other IoT robotics applications and achieve its intended function. A Terasic Hexapod Spider Robot (TSR) was used with its DE0-Nano SoC board to implement our IoT robotics system. The TSR was designed to provide a competent computer vision application to recognize different shapes using a machine learning classifier. The data processing for image detection was divided into two parts, the first part involves hardware implementation on the SoC board and to provide real-time interaction of the robot with the surrounding environment. The second part of implementation is based on the cloud processing technique, where further data analysis was performed. The image detection algorithm for the computer vision component was tested and successfully implemented to recognize shapes. The TSR moves or reacts based on the detected image. The Field Programmable Gate Array (FPGA) part is programmed to handle the movement of the robot and the Hard Processor System (HPS) handles the shape recognition, Wi-Fi connectivity, and Bluetooth communication. This design is implemented, tested and can be used in real-time applications in harsh environments where movements of other robots are restricted.",2019 International Symposium on Systems Engineering (ISSE),2019.0,10.1109/ISSE46696.2019.8984462,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2b8688489e5cb0a2975e83fa121f37c62bec6d2b,https://www.semanticscholar.org/paper/2b8688489e5cb0a2975e83fa121f37c62bec6d2b,Self-supervised damage-avoiding manipulation strategy optimization via mental simulation,,Intell. Serv. Robotics,2017.0,10.1007/S11370-019-00286-7,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
20f13cfee92ad88820bec6741a5845815a6729be,https://www.semanticscholar.org/paper/20f13cfee92ad88820bec6741a5845815a6729be,Can We Stop the Academic AI Brain Drain?,,KI - Künstliche Intelligenz,2019.0,10.1007/s13218-019-00577-2,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b588930f89d5abbe9166c38715b7272f55c80598,https://www.semanticscholar.org/paper/b588930f89d5abbe9166c38715b7272f55c80598,A spatio temporal spectral framework for plant stress phenotyping,,Plant Methods,2019.0,10.1186/s13007-019-0398-8,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
22a3267240b3018fe34fb465a75f5eee6d35e51c,https://www.semanticscholar.org/paper/22a3267240b3018fe34fb465a75f5eee6d35e51c,Memory and mental time travel in humans and social robots,"From neuroscience, brain imaging and the psychology of memory, we are beginning to assemble an integrated theory of the brain subsystems and pathways that allow the compression, storage and reconstruction of memories for past events and their use in contextualizing the present and reasoning about the future—mental time travel (MTT). Using computational models, embedded in humanoid robots, we are seeking to test the sufficiency of this theoretical account and to evaluate the usefulness of brain-inspired memory systems for social robots. In this contribution, we describe the use of machine learning techniques—Gaussian process latent variable models—to build a multimodal memory system for the iCub humanoid robot and summarize results of the deployment of this system for human–robot interaction. We also outline the further steps required to create a more complete robotic implementation of human-like autobiographical memory and MTT. We propose that generative memory models, such as those that form the core of our robot memory system, can provide a solution to the symbol grounding problem in embodied artificial intelligence. This article is part of the theme issue ‘From social brains to social robots: applying neurocognitive insights to human–robot interaction’.",Philosophical Transactions of the Royal Society B,2019.0,10.1098/rstb.2018.0025,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
eecea1889746162c06de7647141202933d5c4815,https://www.semanticscholar.org/paper/eecea1889746162c06de7647141202933d5c4815,A Developmental Cognitive Architecture for Trust and Theory of Mind in Humanoid Robots,"As artificial systems are starting to be widely deployed in real-world settings, it becomes critical to provide them with the ability to discriminate between different informants and to learn from reliable sources. Moreover, equipping an artificial agent to infer beliefs may improve the collaboration between humans and machines in several ways. In this article, we propose a hybrid cognitive architecture, called Thrive, with the purpose of unifying in a computational model recent discoveries regarding the underlying mechanism involved in trust. The model is based on biological observations that confirmed the role of the midbrain in trial-and-error learning, and on developmental studies that indicate how essential is a theory of mind in order to build empathetic trust. Thrive is build on top of an actor–critic framework that is used to stabilize the weights of two self-organizing maps. A Bayesian network embeds prior knowledge into an intrinsic environment, providing a measure of cost that is used to boostrap learning without an external reward signal. Following a developmental robotics approach, we embodied the model in the iCub humanoid robot and we replicated two psychological experiments. The results are in line with real data, and shed some light on the mechanisms involved in trust-based learning in children and robots.",IEEE Transactions on Cybernetics,2020.0,10.1109/TCYB.2020.3002892,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fc08019caa925ad117673f069bc60bd91fb82435,https://www.semanticscholar.org/paper/fc08019caa925ad117673f069bc60bd91fb82435,A Robust Robot Design for Item Picking,"In order to build a stable and reliable system for the Amazon Robotics Challenge we went through a detailed study of the performance and system requirements based on the rules and our past experience of the challenge. The challenge was to build a robot that integrates grasping, vision, motion planning, among others, to be able to pick items from a shelf to specific order boxes. This paper presents the development process including component selection, module designs, and deployment. The resulting robot system has dual 6 degrees of freedom industrial arms mounted on fixed bases, which in turn are mounted on a calibrated table. The robot works with a custom-designed top-open extendable shelf. The vision system uses multiple stereo cameras mounted on a fixed calibrated frame. Feature-based comparison and machine-learning based matching are used to identify and determine item pose. The gripper system uses suction cup and the grasping strategy is pick from the top. Error recovery strategies were also implemented to ensure robust performance. During the competition, the robot was able to pick all target items with the shortest amount of time.",2018 IEEE International Conference on Robotics and Automation (ICRA),2018.0,10.1109/ICRA.2018.8461057,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3209c62295462a87106a6c843703324136b136c0,https://www.semanticscholar.org/paper/3209c62295462a87106a6c843703324136b136c0,To Study Impact of Artificial Intelligence on Human Resource Management,"In this competitive business world, for any kind of business the human resource ( HR) is an mandatory asset /input for improving the organizational performance. To be successful, organizations have to strive for higher customer – satisfaction which is vital key for their survival, and to achieve this, organizations have to adopt innovative hr – practices to improve their performance and be different among its competitors. In near future, HRM is moving away from its foundation administrative functionality like recruitment, selection, appraising to more advanced progress like Automation, Augmented Intelligence, Robotics and Artificial Intelligence that are completely re – defining and re – shaping the way of their workforce characteristics and organizations. At, present the buzz word is AI and how it is completely transforming the human resources , and how AI is seizing millions of jobs globally in each and every sector. However, hr is not traditionally associated with this emerging technology, failure to deploy AI with hr practices could prove to be a dreadful sin which makes the organizations difficult to grow big and survive in this globe driven by these advanced technologies and machines. Global workforce is completely threatened by AI and advanced self learning machines and they are wiping away the interference or involvement of humans in each and every aspect. But, in reality any advanced technology that is created and used for the benefit of humanity not for its destruction. To compete with AI and advanced machines the real challenge now lie’s within the respective hr – departments that how well they train and re-transform their workforce in understanding the AI and collaborating and working with AI & Robots.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d0a40d6c1a0a0f20996a551fb784e00e741025b9,https://www.semanticscholar.org/paper/d0a40d6c1a0a0f20996a551fb784e00e741025b9,VisDA: A Synthetic-to-Real Benchmark for Visual Domain Adaptation,"The success of machine learning methods on visual recognition tasks is highly dependent on access to large labeled datasets. However, real training images are expensive to collect and annotate for both computer vision and robotic applications. The synthetic images are easy to generate but model performance often drops significantly on data from a new deployment domain, a problem known as dataset shift, or dataset bias. Changes in the visual domain can include lighting, camera pose and background variation, as well as general changes in how the image data is collected. While this problem has been studied extensively in the domain adaptation literature, progress has been limited by the lack of large-scale challenge benchmarks.",2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),2018.0,10.1109/CVPRW.2018.00271,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f16c56967cb50320fa059f56cc55563981293bb1,https://www.semanticscholar.org/paper/f16c56967cb50320fa059f56cc55563981293bb1,Yielding Associations Network: Un sistema para mejorar la localización semántica,"In the field of the Internet of Things (IoT), Edge Computing provides an approach to localized 
computing that optimizes data storage and streamlines decision making. This work aims to 
study the performance of several artificial intelligence algorithms used in the automatic 
localization of a mobile robot. The methodology has been based on an experimental prototype, 
with which data from sensors coupled to a Raspberry Pi 3B+ have been collected. With this 
data, several machine learning models that have been trained in the cloud using Google Colab 
have been developed and tested. The operation of these models has been validated by 
deploying them in the developed prototype. The results include the comparison of the 
algorithms and a novel proposal of “Yielding Associations Network” as a system that allows to 
improve semantic localization. The conclusions highlight the great value of the application of 
Artificial Intelligence for mobile robotics and Edge Computing.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
36ea2b6b1ad1ab44593fc377da4582f961059155,https://www.semanticscholar.org/paper/36ea2b6b1ad1ab44593fc377da4582f961059155,Scope of Implementing Humanoid Robots at Educational Institutes in India’s Perspective,"The current technological advancement in the field of robotics is very vast. Emerging technologies of robots has enabled them to interact with humans. With the current pace of development in this field soon there will be many robots deployed into the market. Humanoid robot is a kind of robot that resembles human and are trained to act like same. These robots are integrated with modern developing technologies like artificial intelligence and machine learning to interact with humans. Applying this emerging technology into the field of education will generate great results. India is still a developing nation, introduction of these features into it will show great response. This will reduce the workload and consumption of resources. Not only this, but it will make error less knowledge transfer to the next generation from their own personalized robot.","2019 5th International Conference On Computing, Communication, Control And Automation (ICCUBEA)",2019.0,10.1109/ICCUBEA47591.2019.9128886,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
269c7d883b3af927e21680e59dfbec269c4190ac,https://www.semanticscholar.org/paper/269c7d883b3af927e21680e59dfbec269c4190ac,Robotic Coral Reef Health Assessment Using Automated Image Analysis,"This paper presents a system capable of autonomous surveillance and analysis of coral reef ecosystems using natural lighting. We describe our strategy to safely and effectively deploy a small marine robot to inspect a reef using its digital cameras. Image analysis using a (RBF‐SVM) radial basis function‐support vector machines in combination with (LBP) local binary pattern, Gabor and Hue descriptors developed in this work are able to analyze the resulting image data automatically and reliably by learning from the annotations of expert marine biologists. Our primary evaluation is performed on a novel coral data set that we collected during a series of robotic ocean deployments, the MRL Coral Identification Challenge. We have also applied our algorithms to a data set of coral imagery previously published by other researchers. Our algorithms recognize coral images in our own challenging data with 88.9% accuracy, while being sufficiently efficient to run online on our vehicle. This demonstrates the feasibility of such a system for practical use for the preservation of this crucial ecological resource.",J. Field Robotics,2017.0,10.1002/rob.21698,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2cc3338709ea9c14ff422025ae4a8ad09f9598ba,https://www.semanticscholar.org/paper/2cc3338709ea9c14ff422025ae4a8ad09f9598ba,Explainable AI: from black box to glass box,,Journal of the Academy of Marketing Science,2019.0,10.1007/s11747-019-00710-5,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
75b66a92e9b98655c3505ad48a98b901d3e54962,https://www.semanticscholar.org/paper/75b66a92e9b98655c3505ad48a98b901d3e54962,Intelligent Control Method of Live Working Robot Based on Cloud and Edge Computing Terminal,"The existing single machine working mode of live working robots cannot meet the computational power requirements of artificial intelligence high-precision algorithm, and it is difficult to adapt to the complex working scene, and cannot be applied to the scene where a large number of robots work at the same time to generate massive data management and control. In view of the existing shortcomings, this paper analyzes and compares the working modes of existing power grid inspection robots and live working robots, and proposes an intelligent management and control method for live working robots based on cloud and edge computing terminals. By deploying different parts of deep learning algorithm in cloud and edge computing terminal, the problems existing in the previous working mode are solved. A typical example of live working robot scene in substation is analyzed. The results show that the intelligent control method proposed in this paper can meet the computational power requirements of artificial intelligence deep learning, neural network coordinated control, recognition and control model self-learning, self-renewal and other high-precision algorithm, and can be applied to the scene of complex environment or a large number of robots working at the same time to produce massive data control.",2020 10th International Conference on Power and Energy Systems (ICPES),2020.0,10.1109/ICPES51309.2020.9349704,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
78c379f55f28a38d74955080f88b1b16c4c1d60a,https://www.semanticscholar.org/paper/78c379f55f28a38d74955080f88b1b16c4c1d60a,A Novel Approach to Evolutionary Robotics,"Evolutionary computing draws inspiration from the natural process of evolution to develop algorithms for optimisation and design. The field has developed a range of widely applicable metaheuristics that exploit the principles of evolution in software. Evolutionary robotics is a sub-field of evolutionary computing that uses the principles of evolution to design robot controllers and/or hardware, often using computer simulations to evaluate candidate solutions. Evolution typically takes place off-line: the robots are deployed after evolution has terminated with an acceptable result, and no further adaptation takes place. Over recent years, interest in on-line evolution has increased [3]. The field is about to enter a new phase with the development of algorithms where evolution takes place at the hardware level, in particular to allow ecosystems of evolving autonomous machines that can adapt to their environment. A framework for such an ecosystem in which physical robots actually reproduce was proposed as the Triangle of Life (ToL) [5]. This abstract presents the first ever implementation in hardware of an important part of that triangle, namely the autonomous mating of physical robots and conception of their offspring. The ToL framework combines two novel aspects. The first is that it captures a life cycle that does not run from birth to death, but from conception to conception, explicitly making robot birth part of the evolutionary process. Secondly, it envisions on-line evolution of both robot morphology and control in hardware, as the robots operate in their task environment. New robots are created continuously (triggered by robots meeting and autonomously deciding to mate) and is limited only by the availability of building blocks and the capacity of a ‘birth clinic’ where new robots are automatically constructed. This defines an ecosystem of continuous evolution and adaptation [5]. There have been proof-of-concept ToL implementations in simulation (e.g., [6]) that were in principle transferable to hardware, but the actual step to a hardware implementation has not yet been made. Components of a full ToL implementation in hardware do exist, e.g., the automatic assembly of evolved robots [2]. These, however, lack the aspect of autonomous mating with local mate selection (i.e., they have a centralised control loop that controls evolution) and of lifetime learning to adapt control to the new morphology. The experiments described here consider exactly these aspects and so provide an essential missing piece for a real-life ToL implementation.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4f2c4c608ebdb6264e9dcdd2004ebc1297dbb04d,https://www.semanticscholar.org/paper/4f2c4c608ebdb6264e9dcdd2004ebc1297dbb04d,Editorial: Special Issue on Space Robotics,"Space robotics continues to advance and to capture the imagination. In the past few years fictional depictions of robots – think Star Wars as well as many other popular movies, books and games – are notable for the narrowing gap with our current technical reality. Our space robotic systems are becoming more capable, intelligent, and sometimes even humanoid in form. This is the fifth special issue on space robotics published by the Journal of Field Robotics. Our previous issues in January 2007, March 2009, June 2012, and September 2013 each presented advances in robotics technologies for space applications. Each special issue has followed the biannual International Symposium on Artificial Intelligence, Robotics, and Automation in Space sponsored by the Canadian Space Agency, the German Aerospace Agency (DLR), the European Space Agency (ESA), the Japan Aerospace Exploration Agency and the U.S. National Aeronautics and Space Administration. Since our last special issue there have been many exciting developments in space exploration including a new rover, Yutu (“Jade Rabbit”), landed on the moon by the Chinese National Space Agency. The rover, part of the Chang’e 3 mission operated for one lunar day and returned video and scientific data. Robots landed on comets, Philae by the ESA/DLR made a first-ever soft landing, and flew by asteroids and planets while autonomously making observations. Spectacular images of Pluto were all targeted and acquired robotically. An exciting development in recent years is the proliferation of cubesats, which through small scale and standardized parts, dramatically lower the cost and complexity of orbital, and perhaps, surface exploration. An underlying goal of the research reported in this special issue is to improve autonomous operations in order to extend the capabilities of current and future robotic missions. They close the gap between fact and fiction. There are a number of robotic systems on the International Space Station, including SPHERES, an untethered, self-propelled platform of science and technology experiments. The creation and deployment has previously been reported in JFR. In “An Open Research Facility for VisionBased Navigation onboard the International Space Station,” Tweddle et al. report on the development and deployment of autonomous navigation by a free-flying robot in the space station. The VERTIGO Goggles for SPHERES is now operational and serving as a robotic platform for ongoing experiments. To truly automate a science-driven space mission, it is clear that we must take care of a number of engineering tasks such as state estimation and control. However, we may also need to automate aspects of the science itself. In “RealTime Orbital Image Analysis Using Decision Forests, with a Deployment Onboard the IPEX Spacecraft”, Altinok et al. describe a procedure to automatically recognize clouds that impede the ability to take atmospheric measurements for a satellite-based, Earth-observation mission. They show that their method is able to run in real-time onboard a real spacecraft, one of the first times a machine-learning approach has actually been deployed on a space mission. They provide insight lessons learned from their experience that may inform similar future implementations of science autonomy. Several future on-orbit mission concepts call for rendezvous and/or proximity operations in space between multiple spacecraft. Examples include on-orbit servicing, sample return, space station resupply, constellations of satellites, to name a few. When a human is not available to take control during rendezvous, a method of relative navigation is required during approach. In “Cooperative Relative Navigation for Space Rendezvous and Proximity Operations using Controlled Active Vision”, Zhang et al. demonstrate a new low-computational-cost relative navigation approach using a single monocular camera and a set of fiducial markers. Their experiments show that their approach achieves similar accuracy to the state of the art at a fraction of the computational cost. Their low-cost approach will hopefully find application on future space rendezvous and multi-craft missions where computational cost is a big constraint. On planetary surfaces, Bajpai et al. look at the problem of localization in feature-sparse terrain. With “Planetary Monocular Simultaneous Localization and Mapping” they detail the design and experimental verification of a method to detect salient features in the environment, localize and track them over time, and estimate rover path with higher accuracy than dead reckoning or visual odometry alone. Their approach is demonstrated in simulated lunar terrain and Mars-like imagery from the Atacama Desert in Chile. Similarly considering planetary surfaces there is the challenge of building accurate models of natural terrain. In “3D Scan Registration using Curvelet Features in Planetary Environments“Ahuja, et al. formulate a method of aligning point clouds that exceeds the accuracy of traditional algorithms by an order of magnitude. In examples from real scans of they show detailed rather than noisy reconstruction and resulting model that is useful for precision navigation. A technical journal can only equal the quality of the research reported, so we congratulate and thank all of the authors who present their work in this special issue. Our",J. Field Robotics,2016.0,10.1002/rob.21649,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b6081fc4143fde7601e486c7eea980e6c0d06abb,https://www.semanticscholar.org/paper/b6081fc4143fde7601e486c7eea980e6c0d06abb,Systematic Review of Intelligent Algorithms in Gait Analysis and Prediction for Lower Limb Robotic Systems,"The rate of development of robotic technologies has been meteoric, as a result of compounded advancements in hardware and software. Amongst these robotic technologies are active exoskeletons and orthoses, used in the assistive and rehabilitative fields. Artificial intelligence techniques are increasingly being utilised in gait analysis and prediction. This review paper systematically explores the current use of intelligent algorithms in gait analysis for robotic control, specifically the control of active lower limb exoskeletons and orthoses. Two databases, IEEE and Scopus, were screened for papers published between 1989 to May 2020. 41 papers met the eligibility criteria and were included in this review. 66.7% of the identified studies used classification models for the classification of gait phases and locomotion modes. Meanwhile, 33.3% implemented regression models for the estimation/prediction of kinematic parameters such as joint angles and trajectories, and kinetic parameters such as moments and torques. Deep learning algorithms have been deployed in ~15% of the machine learning implementations. Other methodological parameters were reviewed, such as the sensor selection and the sample sizes used for training the models.",IEEE Access,2021.0,10.1109/ACCESS.2021.3104464,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
55f3e3cc9f4af08c9d17cddf622a22134e216683,https://www.semanticscholar.org/paper/55f3e3cc9f4af08c9d17cddf622a22134e216683,The Design of Intelligent production line for Clothing Industry,"To promote the development of the textile industrial and upgrade the existing production technology. This article explored the intelligent application scenarios by analyzing the production process and technology of clothing production process. The hardware deployment and the management architecture of the production line were designed by combining the machine vision, deep learning and robot technology. And the performance of the intelligent algorithms under the production framework were also verified in this article.",2020 IEEE 6th International Conference on Computer and Communications (ICCC),2020.0,10.1109/ICCC51575.2020.9344935,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
75b3aab1efb1acc7f6f29084cbab04183237fe08,https://www.semanticscholar.org/paper/75b3aab1efb1acc7f6f29084cbab04183237fe08,Artificial Neural Network Fuzzy Inference System Anfis Epub File,"In this book a neural network learning method with type-2 fuzzy weight adjustment is proposed. The mathematical analysis of the proposed learning method architecture and the adaptation of type-2 fuzzy weights are presented. The proposed method is based on research of recent methods that handle weight adaptation and especially fuzzy weights.The internal operation of the neuron is changed to work with two internal calculations for the activation function to obtain two results as outputs of the proposed method. Simulation results and a comparative study among monolithic neural networks, neural network with type-1 fuzzy weights and neural network with type-2 fuzzy weights are presented to illustrate the advantages of the proposed method.The proposed approach is based on recent methods that handle adaptation of weights using fuzzy logic of type-1 and type-2. The proposed approach is applied to a cases of prediction for the Mackey-Glass (for ô=17) and Dow-Jones time series, and recognition of person with iris biometric measure. In some experiments, noise was applied in different levels to the test data of the Mackey-Glass time series for showing that the type-2 fuzzy backpropagation approach obtains better behavior and tolerance to noise than the other methods.The optimization algorithms that were used are the genetic algorithm and the particle swarm optimization algorithm and the purpose of applying these methods was to find the optimal type-2 fuzzy inference systems for the neural network with type-2 fuzzy weights that permit to obtain the lowest prediction error. Master's Thesis from the year 2017 in the subject Engineering Artificial Intelligence, grade: 9.00, Lovely Professional University, Punjab (Lovely professional university, Punjab), course: M.Tech, language: English, abstract: Residual life prediction is the technique which demonstrates how reliable a particular electronic system or component works under in specific operating conditions. The remaining useful life relies on the failure rate of a component and on the operating conditions of a device. This failure rate drifts for the duration of the life of the item with time. Life is an important aspect while choosing the electronic hardware. Residual life estimation and life prediction are two distinct terms. The importance of life estimation is to evaluate the remaining useful life of a specific component under the different stress parameters. As an increasing number of components are integrated on to a chip, the chances of failure increase, as the different parts have their own stress factors and different working conditions. So the condition monitoring strategies are utilized which enhances the reliability of a component and a suitable move to be made before any harmful breakdown happens. The electronic circuits need a failure estimation technique to protect the system from unavoidable failures. Residual life estimation of electronic components is an important fact these days as electronic components and devices becomes a great need of society. Residual life prediction is predicting the remaining useful life of a component or device based on various failure factors of any component and it also depends on the operating conditions. Many methods for predicting the life of electronic components have been developed. The life of electronic components can be predicted by creating an intelligent system for the failure analysis. The capability to predict the life of electronic components is a key to prevent the sudden costly failure and it will increase the overall performance and reliability of a system. So, remaining useful life prediction is an important factor for every active and passive electronic component such as resistor, capacitor and diode etc. Fuzzy Systems EngineeringTheory and PracticeSpringer Science & Business Media ""This book offers an outlook of the most recent works at the field of the Artificial Neural Networks (ANN), including theoretical developments and applications of systems using intelligent characteristics for adaptability""--Provided by publisher. This book describes hybrid intelligent systems using type-2 fuzzy logic and modular neural networks for pattern recognition applications. Hybrid intelligent systems combine several intelligent computing paradigms, including fuzzy logic, neural networks, and bio-inspired optimization algorithms, which can be used to produce powerful pattern recognition systems. Type-2 fuzzy logic is an extension of traditional type-1 fuzzy logic that enables managing higher levels of uncertainty in complex real world problems, which are of particular importance in the area of pattern recognition. The book is organized in three main parts, each containing a group of chapters built around a similar subject. The first part consists of chapters with the main theme of theory and design algorithms, which are basically chapters that propose new models and concepts, which are the basis for achieving intelligent pattern recognition. The second part contains chapters with the main theme of using type-2 fuzzy models and modular neural networks with the aim of designing intelligent systems for complex pattern recognition problems, including iris, ear, face and voice recognition. The third part contains chapters with the theme of evolutionary optimization of type-2 fuzzy systems and modular neural networks in the area of intelligent pattern recognition, which includes the application of genetic algorithms for obtaining optimal type-2 fuzzy integration systems and ideal neural network architectures for solving problems in this area. Soft computing is a consortium of computing methodologies that provide a foundation for the conception, design, and deployment of intelligent systems and aims to formalize the human ability to make rational decisions in an environment of uncertainty and imprecision. This book is based on a NATO Advanced Study Institute held in 1996 on soft computing and its applications. The distinguished contributors consider the principal constituents of soft computing, namely fuzzy logic, neurocomputing, genetic computing, and probabilistic reasoning, the relations between them, and their fusion in industrial applications. Two areas emphasized in the book are how to achieve a synergistic combination of the main constituents of soft computing and how the combination can be used to achieve a high Machine Intelligence Quotient. Over the past decades, fault diagnosis (FDI) and fault tolerant control strategies (FTC) have been proposed based on different techniques for linear and nonlinear systems. Indeed a considerable attention is deployed in order to cope with diverse damages resulting in faults occurrence. CD-ROM contains: BackProp -Data files -Display -Images -MATLAB examples. This book constitutes the refereed proceedings of the 8th International Conference on Neural Networks and Artificial Intelligence, ICNNAI 2014, held in Brest, Belarus, in June 2014. The 19 revised full papers presented were carefully reviewed and selected from 27 submissions. The papers are organized in topical sections on forest resource management; artificial intelligence by neural networks; optimization; classification; fuzzy approach; machine intelligence; analytical approach; mobile robot; real world application. Estimation of missing precipitation records is one of the most important tasks in hydro-logical and environmental study. The efficiency of hydrological and environmental models is sub-ject to the completeness of precipitation data. This study compared some basic soft computing techniques, namely, artificial neural network, fuzzy inference system and adaptive neuro-fuzzy inference system as well as the conventional methods to estimate missing monthly rainfall records in the northeast region of Thailand. Four cases studies are selected to evaluate the accuracy of the estimation models. The simultaneous rainfall data from three nearest neighbouring control stations are used to estimate missing records at the target station. The experimental results suggested that the adaptive neuro-fuzzy inference system could be considered as a recommended technique because it provided the promising estimation results, the estimation mechanism is transparent to the users, and do not need prior knowledge to create the model. The results also showed that fuzzy inference system could provide compatible accuracy to artificial neural",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
89820ce26a3b2884008754f28a02b7ab40b11650,https://www.semanticscholar.org/paper/89820ce26a3b2884008754f28a02b7ab40b11650,Learning Sequential Skills for Robot Manipulation Tasks,"Most people's imagination about robots has been shaped by Hollywood movies or novels, resulting in the dream of having robots as assistants or household helpers in our homes. However, there is still a large gap between this dream and the actual capabilities of robots. One underlying reason is that every home is unique and largely unstructured, making it impossible to pre-program a robot for all the challenges it might face in such an environment. For instance, floor plans and furniture differ from home to home. Humans and pets walk around, potentially getting in the robot's way and making the environment non-static. Hence, a pre-programmed robot deployed in such an environment will undoubtedly face problems that it cannot solve with its existing knowledge. In order to cope with this issue, researchers started to equip robots with learning capabilities. Ideally, such capabilities allow a robot to adapt skills to new or changing situations or even to learn completely new tasks. Also humans learn new skills over time and are able to adapt them if needed. Therefore, such learning capabilities seem natural to us. If we are not able to master a specific task, we usually would ask another person to demonstrate it or to give instructions on how to perform it. In robotics research, the field of ""Learning from Demonstration"" tries to mimic this behavior by learning new skills from demonstrations of a task. By applying machine learning techniques, the data perceived from a single or multiple demonstrations are exploited to learn a mapping from perception to the action of a robot. 
 
In this thesis, we concentrate on important Learning from Demonstration aspects that have not gotten so much attention in the research community so far. In particular, we focus on learning methods for robot manipulation tasks. These tasks have two important characteristics. First, they can be naturally decomposed into a set of subtasks and, therefore, can be mastered by performing the individual subtasks in the correct sequential order. Second, they involve physical contact between the robot and objects in its environment. One aim of this thesis is developing methods which allow for learning skills for robot manipulation tasks that generalize well to unknown situations. For instance, a learned skill should also be applicable if positions and orientations of objects differ from those seen in a demonstration. 
 
In the first part of the thesis, we focus on the ""sequential"" aspect of manipulation tasks. Many approaches assume that subtasks are executed in a purely sequential manner or that the human always 
demonstrates the same sequence of subtasks. We propose an approach that does not have this assumption. Based on the demonstrations, a graph is generated which connects the subtasks with each other. Each subtask is associated with a movement primitive, a basic elementary movement necessary to perform the subtask. Depending on the environmental conditions, different sequences of movement primitives are executed, allowing the robot to perform tasks which for instance require an arbitrary number of repetitions (e.g., unscrewing a light bulb). 
 
As we concentrate on the sequential aspects of a task in the first part of the thesis, we assume the demonstrations are labeled with the correct movement primitives over time. Additionally, the movement primitives are predefined. In the second part of the thesis, these two assumptions are relaxed. We first present an approach which decomposes the demonstrations into a set of meaningful movement primitives by inferring the underlying sequential structure of the task. The decomposition is based on a probability distribution we call Directional Normal Distribution. By utilizing the distribution, our method infers if a movement should be performed relative to an object in the scene and if a force should be applied in certain directions or not. Forces are especially important when interacting with the environment, for example if the robot has to manipulate objects. By defining movements relative to objects in the scene, the robot is likely to generalize better to new situations, for instance if the object positions differ from the demonstrations. 
Our task-decomposition method allows for inferring the most likely movement primitives over time and replaces the process of manually labeling the demonstrations. By combining the method with the sequencing concept presented in the first part of the thesis, complex skills can be learned from scratch without further human supervision. Such a learning scheme is an essential requirement for domestic robots, as not every human teacher might be able or willing to do the tedious labeling of the data. 
 
In both the decomposition and the sequencing part of the thesis, we assume that the teacher performs point-to-point movements and stops between two successive movements. While these assumptions lead to an approach which can learn skills for fairly complex tasks, it also restricts the class of tasks for which the approach can be used. In the third part of the thesis, we therefore introduce the Mixture of Attractors movement primitive representation. Here, a movement is modulated by continuously changing the activations of a set of simple attractors over time. We present a learning algorithm for the representation which learns both the attractors and their activations. An important property of the representation is that the attractors can be defined in different coordinate frames. The continuous activations and the attractors defined in different coordinate frames allow the system to learn movements of arbitrary shape and to generalize them to different object positions. In addition, the transitions between successive movements are smooth. 
This property reflects an important behavior of humans who often tend to co-articulate between successive movements. In contrast to many existing approaches, movements are learned by solving a convex optimization problem that does not rely on a good initial estimate of parameters. 
 
In summary, the contribution of this thesis to the state-of-the-art in Learning from Demonstration is two-fold. The first contribution is a framework which is able to learn sequential skills for robot manipulation tasks from a few demonstrations. In contrast to other approaches, our method incorporates object-relative movements and force information directly into the skill learning framework. The second contribution is the Mixture of Attractors movement primitive representation. The representation supports co-articulated movements represented in different coordinate frames and outperforms existing movement primitive representations in terms of accuracy and generalization capabilities. Both contributions are evaluated on a wide range of tasks in simulation and on a real single arm robot with seven degrees of freedom. Altogether, this thesis aims at bringing us closer to the dream of having autonomous robots in our homes.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8117f5aa289ffc05b6d10a35f5839455cb7dc25a,https://www.semanticscholar.org/paper/8117f5aa289ffc05b6d10a35f5839455cb7dc25a,Statistical natural language processing methods for intelligent process automation,"Nowadays, digitization is transforming the way businesses work. Recently, Artificial Intelligence (AI) techniques became an essential part of the automation of business processes: In addition to cost advantages, these techniques offer fast processing times and higher customer satisfaction rates, thus ultimately increasing sales. One of the intelligent approaches for accelerating digital transformation in companies is the Robotic Process Automation (RPA). 
An RPA-system is a software tool that robotizes routine and time-consuming responsibilities such as email assessment, various calculations, or creation of documents and reports (Mohanty and Vyas, 2018). Its main objective is to organize a smart workflow and therethrough to assist employees by offering them more scope for cognitively demanding and engaging work. 
 
Intelligent Process Automation (IPA) offers all these advantages as well; however, it goes beyond the RPA by adding AI components such as Machine- and Deep Learning techniques to conventional automation solutions. Previously, IPA approaches were primarily employed within the computer vision domain. However, in recent times, Natural Language Processing (NLP) became one of the potential applications for IPA as well due to its ability to understand and interpret human language. Usually, NLP methods are used to analyze large amounts of unstructured textual data and to respond to various inquiries. However, one of the central applications of NLP within the IPA domain – are conversational interfaces (e.g., chatbots, virtual agents) that are used to enable human-to-machine communication. Nowadays, conversational agents gain enormous demand due to their ability to support a large number of users simultaneously while communicating in a natural language. The implementation of a conversational agent comprises multiple stages and involves diverse types of NLP sub-tasks, starting with natural language understanding (e.g., intent recognition, named entity extraction) and going towards dialogue management (i.e., determining the next possible bots action) and response generation. Typical dialogue system for IPA purposes undertakes straightforward customer support requests (e.g., FAQs), allowing human workers to focus on more complicated inquiries. 
 
In this thesis, we are addressing two potential Intelligent Process Automation (IPA) applications and employing statistical Natural Language Processing (NLP) methods for their implementation. 
 
The first block of this thesis (Chapter 2 – Chapter 4) deals with the development of a conversational agent for IPA purposes within the e-learning domain. As already mentioned, chatbots are one of the central applications for the IPA domain since they can effectively perform time-consuming tasks while communicating in a natural language. Within this thesis, we realized the IPA conversational bot that takes care of routine and time-consuming tasks regularly performed by human tutors of an online mathematical course. This bot is deployed in a real-world setting within the OMB+ mathematical platform. Conducting experiments for this part, we observed two possibilities to build the conversational agent in industrial settings – first, with purely rule-based methods, considering the missing training data and individual aspects of the target domain (i.e., e-learning). Second, we re-implemented two of the main system components (i.e., Natural Language Understanding (NLU) and Dialogue Manager (DM) units) using the current state-of-the-art deep-learning architecture (i.e., Bidirectional Encoder Representations from Transformers (BERT)) and investigated their performance and potential use as a part of a hybrid model (i.e., containing both rule-based and machine learning methods). 
 
The second part of the thesis (Chapter 5 – Chapter 6) considers an IPA subproblem within the predictive analytics domain and addresses the task of scientific trend forecasting. Predictive analytics forecasts future outcomes based on historical and current data. Therefore, using the benefits of advanced analytics models, an organization can, for instance, reliably determine trends and emerging topics and then manipulate it while making significant business decisions (i.e., investments). In this work, we dealt with the trend detection task – specifically, we addressed the lack of publicly available benchmarks for evaluating trend detection algorithms. We assembled the benchmark for the detection of both scientific trends and downtrends (i.e., topics that become less frequent overtime). To the best of our knowledge, the task of downtrend detection has not been addressed before. The resulting benchmark is based on a collection of more than one million documents, which is among the largest that has been used for trend detection before, and therefore, offers a realistic setting for the development of trend detection algorithms.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3b1a406ff586f58e676320aea89043a62d0ec1c2,https://www.semanticscholar.org/paper/3b1a406ff586f58e676320aea89043a62d0ec1c2,Guest Editorial on “Knowledge fusion intelligent optimization for complex systems”,,Complex & Intelligent Systems,2021.0,10.1007/S40747-021-00394-X,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ccfb75c14eb42fec838d60212ca459a343df1536,https://www.semanticscholar.org/paper/ccfb75c14eb42fec838d60212ca459a343df1536,Maneuverable Autonomy of a Six-legged Walking Robot: Design and Implementation using Deep Neural Networks and Hexapod Locomotion,"Automatically real-time synthesizing behaviors for a six-legged walking robot pose several exciting challenges, which can be categorized into mechanics design, control software, and the combination of both. Due to the complexity of controlling and automation, numerous studies choose to gear their attention to a specific aspect of the whole challenge by either proposing valid and low-power assumption of mechanical parts or implementing software solutions upon sensorial capabilities and camera. Therefore, a complete solution associating both mechanical moving parts, hardware components, and software encouraging generalization should be adequately addressed. The architecture proposed in this article orchestrates (i) interlocutor face detection and recognition utilizing ensemble learning and convolutional neural networks, (ii) maneuverable automation of six-legged robot via hexapod locomotion, and (iii) deployment on a Raspberry Pi, that has not been previously reported in the literature. Not satisfying there, the authors even develop one step further by enabling real-time operation. We believe that our contributions ignite multi-research disciplines ranging from IoT, computer vision, machine learning, and robot autonomy. Keywords—Six-legged walking robot; hexapod locomotion; Raspberry Pi; deep neural networks",,2021.0,10.14569/ijacsa.2021.0120697,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
03d32f9e53c86809c0203f30ac83ede083ea2bab,https://www.semanticscholar.org/paper/03d32f9e53c86809c0203f30ac83ede083ea2bab,Smart Malware that Uses Leaked Control Data of Robotic Applications: The Case of Raven-II Surgical Robots,"In this paper, we demonstrate a new type of threat that leverages machine learning techniques to maximize its impact. We use the Raven-II surgical robot and its haptic feedback rendering algorithm as an application. We exploit ROS vulnerabilities and implement smart self-learning malware that can track the movements of the robot’s arms and trigger the attack payload when the robot is in a critical stage of a (hypothetical) surgical procedure. By keeping the learning procedure internal to the malicious node that runs outside the physical components of the robotic application, an adversary can hide most of the malicious activities from security monitors that might be deployed in the system. Also, if an attack payload mimics an accidental failure, it is likely that the system administrator will fail to identify the malicious intention and will treat the attack as an accidental failure. After demonstrating the security threats, we devise methods (i.e., a safety engine) to protect the robotic system against the identified risk.",RAID,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
fb632a7a74bfce320137904385b1c9818c2c196b,https://www.semanticscholar.org/paper/fb632a7a74bfce320137904385b1c9818c2c196b,Engaging with Robotic Swarms,"In recent years, researchers have explored human body posture and motion to control robots in more natural ways. These interfaces require the ability to track the body movements of the user in three dimensions. Deploying motion capture systems for tracking tends to be costly and intrusive and requires a clear line of sight, making them ill adapted for applications that need fast deployment. In this article, we use consumer-grade armbands, capturing orientation information and muscle activity, to interact with a robotic system through a state machine controlled by a body motion classifier. To compensate for the low quality of the information of these sensors, and to allow a wider range of dynamic control, our approach relies on machine learning. We train our classifier directly on the user to recognize (within minutes) which physiological state his or her body motion expresses. We demonstrate that on top of guaranteeing faster field deployment, our algorithm performs better than all comparable algorithms, and we detail its configuration and the most significant features extracted. As the use of large groups of robots is growing, we postulate that their interaction with humans can be eased by our approach. We identified the key factors to stimulate engagement using our system on 27 participants, each creating his or her own set of expressive motions to control a swarm of desk robots. The resulting unique dataset is available online together with the classifier and the robot control scripts.",ACM Transactions on Human-Robot Interaction,2019.0,10.1145/3323213,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
67b44dc5ad00dafc404a31f3a77ecb50adcc4b48,https://www.semanticscholar.org/paper/67b44dc5ad00dafc404a31f3a77ecb50adcc4b48,Occupancy Grid and Topological Maps Extraction from Satellite Images for Path Planning in Agricultural Robots,"Robotics will significantly impact large sectors of the economy with relatively low productivity, such as Agri-Food production. Deploying agricultural robots on the farm is still a challenging task. When it comes to localising the robot, there is a need for a preliminary map, which is obtained from a first robot visit to the farm. Mapping is a semi-autonomous task that requires a human operator to drive the robot throughout the environment using a control pad. Visual and geometric features are used by Simultaneous Localisation and Mapping (SLAM) Algorithms to model and recognise places, and track the robot’s motion. In agricultural fields, this represents a time-consuming operation. This work proposes a novel solution—called AgRoBPP-bridge—to autonomously extract Occupancy Grid and Topological maps from satellites images. These preliminary maps are used by the robot in its first visit, reducing the need of human intervention and making the path planning algorithms more efficient. AgRoBPP-bridge consists of two stages: vineyards row detection and topological map extraction. For vineyards row detection, we explored two approaches, one that is based on conventional machine learning technique, by considering Support Vector Machine with Local Binary Pattern-based features, and another one found in deep learning techniques (ResNET and DenseNET). From the vineyards row detection, we extracted an occupation grid map and, by considering advanced image processing techniques and Voronoi diagrams concept, we obtained a topological map. Our results demonstrated an overall accuracy higher than 85% for detecting vineyards and free paths for robot navigation. The Support Vector Machine (SVM)-based approach demonstrated the best performance in terms of precision and computational resources consumption. AgRoBPP-bridge shows to be a relevant contribution to simplify the deployment of robots in agriculture.",Robotics,2020.0,10.3390/ROBOTICS9040077,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4546399ebcef2da1294b07c1ce05ca7f23e6ed99,https://www.semanticscholar.org/paper/4546399ebcef2da1294b07c1ce05ca7f23e6ed99,UAV-Based Structural Damage Mapping: A Review,"Structural disaster damage detection and characterization is one of the oldest remote sensing challenges, and the utility of virtually every type of active and passive sensor deployed on various air- and spaceborne platforms has been assessed. The proliferation and growing sophistication of unmanned aerial vehicles (UAVs) in recent years has opened up many new opportunities for damage mapping, due to the high spatial resolution, the resulting stereo images and derivatives, and the flexibility of the platform. This study provides a comprehensive review of how UAV-based damage mapping has evolved from providing simple descriptive overviews of a disaster science, to more sophisticated texture and segmentation-based approaches, and finally to studies using advanced deep learning approaches, as well as multi-temporal and multi-perspective imagery to provide comprehensive damage descriptions. The paper further reviews studies on the utility of the developed mapping strategies and image processing pipelines for first responders, focusing especially on outcomes of two recent European research projects, RECONASS (Reconstruction and Recovery Planning: Rapid and Continuously Updated Construction Damage, and Related Needs Assessment) and INACHUS (Technological and Methodological Solutions for Integrated Wide Area Situation Awareness and Survivor Localization to Support Search and Rescue Teams). Finally, recent and emerging developments are reviewed, such as recent improvements in machine learning, increasing mapping autonomy, damage mapping in interior, GPS-denied environments, the utility of UAVs for infrastructure mapping and maintenance, as well as the emergence of UAVs with robotic abilities.",ISPRS Int. J. Geo Inf.,2019.0,10.3390/ijgi9010014,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c115414c1655e5a50237026cec5ebadf5447dd0f,https://www.semanticscholar.org/paper/c115414c1655e5a50237026cec5ebadf5447dd0f,Application of YOLO Object Detection Network In Weld Surface Defect Detection,"As industrial production becomes more modern and intelligent today, the inspection of product quality of the workshop is becoming more and more accustomed to replacing the old manual visual inspection methods with automated inspection systems. In the welding field, automated welding robots are not only used in traditional large-scale automobile assembly lines. In more general welding work, welding robots also plays an important role. The inspection of the welding quality of the welding robot is mainly to detect the four main types of weld defects. Compared to traditional defect classification based on support vector machines and defect detection based on template matching, this paper uses a welding surface defect detection system designed based on deep learning methods. By working with workshop welding experts, a large-scale image of nearly 5000 pictures is built. Large-scale weld defect datasets, while using the real-time and accuracy of the YOLO series of deep learning object detection frameworks, the weld defects detection model reaches 75.5% mean average precision(mAP) in constructed weld defect data set. In addition, the construction cost of the detection model and the deployment time of the detection system are greatly reduced. During the field test of the system in the workshop, among a batch of welding workpieces provided by the factory, the detection accuracy of weld defects reached 71%, which initially met the requirements of the workshop for an automated defect detection system.","2021 IEEE 11th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",2021.0,10.1109/CYBER53097.2021.9588269,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1c8acff3e9c5049144ba4c649b48c11279ad105a,https://www.semanticscholar.org/paper/1c8acff3e9c5049144ba4c649b48c11279ad105a,URoboSim — a simulation-based predictive modelling engine for cognition-enabled robot manipulation,"In a nutshell robot simulators are fully developed software systems that provide simulations as a substitute for real-world activity. They are primarily used for training modules of robot control programs, which are, after completing the learning process, deployed in real-world robots. In contrast, simulation in (artificial) cognitive systems is a core cognitive capability, which is assumed to provide a “small-scale model of external reality and of its own possible actions within its head, it is able to try out various alternatives, conclude which is the best of them, react to future situations before they arise, utilise the knowledge of past events in dealing with the present and future, and in every way to react in a much fuller, safer, and more competent manner to the emergencies which face it.” [8] This means that simulation can be considered as an embodied, online predictive modelling engine that enables robots to contextualize vague task requests such as “bring me the milk” into a concrete body motion that achieves the implicit goal and avoids unwanted side effects. In this setting a robot can run small-scale simulation and rendering processes for different reasoning tasks all the time and can continually compare simulation results with reality — it is a promising Sim2Real2Sim setup that has the potential to create much more powerful robot simulation engines. We introduce URoboSim, a robot simulation framework that is currently designed and developed with this vision in mind. 1. Simulation in engineering and cognition In recent years the disruptive progress of virtual reality simulation and rendering tools has been leveraged for realizing several high-performance robot simulators and simulation environments [24, 2, 19, 6, 25, 26, 27, 22, 20, 17]. These simulators have been used, as substitutes for the real world, to train and test the skills of robot agents [17, 1, 7, 3]. The learned and tested skills are then transferred to the real robots. The view of robot simulators as real-world substitutes is very limited compared to the role that simulation and prospection takes for cognition-enabled agency. Williams [23] has proposed a framework that views the brain as a probabilistic prospective modelling engine that can abstractly project plans and alternatives, concretely simulate the agent environment interactions, predict future situations and how they look. Schacter et al.[21] define prospection as the ability to represent what might happen in the future and propose a taxonomy of prospective capabilities distinguishing their function as well as their level of abstraction. Another example is Hesslow’s simulation theory of cognition[12, 13], which proposes that thinking is simulated interaction with the environment. The role of building, maintaining, and using models that predict and explain what is happening and include causal and intuitive physics knowledge has also been stressed in the perspective article “Building machines that learn and think like people”[16]. Figure 1. Belief-state (left) during action of real robot (right).",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,https://www.semanticscholar.org/paper/8061d521fe7c62430e1ba2cb0b8f4d19f7366e6f,Multi Uav Cooperative Surveillance With Spatio Temporal,"Deep Learning for Unmanned SystemsMultiple Heterogeneous Unmanned Aerial VehiclesAdvanced Mobile RoboticsSafe Robot Navigation Among Moving and Steady ObstaclesComputer Safety, Reliability, and SecurityAdvances in Swarm IntelligenceHolonic and Multi-Agent Systems for ManufacturingAdvances in Artificial Intelligence and Applied Cognitive ComputingUnmanned Aircraft SystemsIntelligent Computing Theories and ApplicationAutonomous Airborne Wireless NetworksAd Hoc NetworksEnabling Blockchain Technology for Secure Networking and CommunicationsUAV Sensors for Environmental MonitoringUnmanned Aerial Vehicles: Breakthroughs in Research and PracticeComputational Collective IntelligenceTime-Critical Cooperative Control of Autonomous Air VehiclesAdvances in Cooperative Control and OptimizationCooperative Robots and Sensor Networks 2015Artificial Intelligence and SecurityPRICAI 2016: Trends in Artificial IntelligenceClosing the Gap Between Research and Field Applications for Multi-UAV Cooperative MissionsMulti-rotor Platform Based UAV SystemsProceedings of the Future Technologies Conference (FTC) 2020, Volume 1Unmanned Aerial SystemsAdvanced Distributed Consensus for Multiagent SystemsCooperative Control of MultiAgent SystemsMulti-UAV Planning and Task AllocationMobile Internet SecurityCooperative Control of Multiple Unmanned Aerial Vehicles with Application to Forest Fire Detection and FightingMulti UAV Systems with Motion and Communication ConstraintsIntelligent Autonomy of UAVsIntelligent and Fuzzy Techniques in Big Data Analytics and Decision MakingIntelligent Autonomy of UAVsUAV Cooperative Decision and ControlCooperative Localization and NavigationAdvances in Guidance, Navigation and ControlMachine Learning and Intelligent CommunicationsUnmanned Aerial VehiclesThe Cognitive Approach in Cloud Computing and Internet of Things Technologies for Surveillance Tracking Systems Ad hoc networks, which include a variety of autonomous networks for specific purposes, promise a broad range of civilian, commercial, and military applications. These networks were originally envisioned as collections of autonomous mobile or stationary nodes that dynamically auto-configure themselves into a wireless network without relying on any existing network infrastructure or centralized administration. With the significant advances in the last decade, the concept of ad hoc networks now covers an even broader scope, referring to the many types of autonomous wireless networks designed and deployed for a specific task or function, such as wireless sensor networks, vehicular networks, home networks, and so on. In contrast to the traditional wireless networking paradigm, such networks are all characterized by sporadic connections, highly error-prone communications, distributed autonomous operation, and fragile multi-hop relay paths. The new wireless networking paradigm necessitates reexamination of many established concepts and protocols, and calls for developing a new understanding of fundamental problems such as interference, mobility, connectivity, capacity, and security, among others. While it is essential to advance theoretical research on fundamental and practical research on efficient policies, algorithms and protocols, it is also critical to develop useful applications, experimental prototypes, and real-world deployments to achieve an immediate impact on society for the success of this wireless networking paradigm.A comprehensive review of the state of the art in the control of multi-agent systems theory and applications The superiority of multi-agent systems over single agents for the control of unmanned air, water and ground vehicles has been clearly demonstrated in a wide range of application areas. Their large-scale spatial distribution, robustness, high scalability and low cost enable multi-agent systems to achieve tasks that could not successfully be performed by even the most sophisticated single agent systems. Cooperative Control of Multi-Agent Systems: Theory and Applications provides a wide-ranging review of the latest developments in the cooperative control of multi-agent systems theory and applications. The applications described are mainly in the areas of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Throughout, the authors link basic theory to multi-agent cooperative control practice — illustrated within the context of highly-realistic scenarios of high-level missions — without losing site of the mathematical background needed to provide performance guarantees under general working conditions. Many of the problems and solutions considered involve combinations of both types of vehicles. Topics explored include target assignment, target tracking, consensus, stochastic game theory-based framework, event-triggered control, topology design and identification, coordination under uncertainty and coverage control. Establishes a bridge between fundamental cooperative control theory and specific problems of interest in a wide range of applications areas Includes example applications from the fields of space exploration, radiation shielding, site clearance, tracking/classification, surveillance, search-and-rescue and more Features detailed presentations of specific algorithms and application frameworks with relevant commercial and military applications Provides a comprehensive look at the latest developments in this rapidly evolving field, while offering informed speculation on future directions for collective control systems The use of multi-agent system technologies in both everyday commercial use and national defense is certain to increase tremendously in the years ahead, making this book a valuable resource for researchers, engineers, and applied mathematicians working in systems and controls, as well as advanced undergraduates and graduate students interested in those areas.Time-Critical Cooperative Control of Autonomous Air Vehicles presents, in an easy-to-read style, the latest research conducted in the industry, while also introducing a set of novel ideas that illuminate a new approach to problem-solving. The book is virtually self-contained, giving the reader a complete, integrated presentation of the different concepts, mathematical tools, and control solutions needed to tackle and solve a number of problems concerning time-critical cooperative control of UAVs. By including case studies of fixed-wing and multirotor UAVs, the book effectively broadens the scope of application of the methodologies developed. This theoretical presentation is complemented with the results of flight tests with real UAVs, and is an ideal reference for researchers and practitioners from academia, research labs, commercial companies, government workers, and those in the international aerospace industry. Addresses important topics related to time-critical cooperative control of UAVs Describes solutions to the problems rooted in solid dynamical systems theory Applies the solutions developed to fixed-wing and multirotor UAVs Includes the results of field tests with both classes of UAVsThis book provides the state-of-the-art intelligent methods and techniques for solving realworld problems along with a vision of the future research. The fifth 2020 Future Technologies Conference was organized virtually and received a total of 590 submissions from academic pioneering researchers, scientists, industrial engineers, and students from all over the world. The submitted papers covered a wide range of important topics including but not limited to computing, electronics, artificial intelligence, robotics, security and communications and their applications to the real world. After a double-blind peer review process, 210 submissions (including 6 poster papers) have been selected to be included in these proceedings. One of the meaningful and valuable dimensions of this conference is the way it brings together a large group of technology geniuses in one venue to not only present breakthrough research in future technologies, but also to promote discussions and debate of relevant issues, challenges, opportunities and research findings. The authors hope that readers find the book interesting, exciting and inspiringAdvanced Distributed Consensus for Multiagent Systems contributes to the further development of advanced distributed consensus methods for different classes of multiagent methods. The book expands the field of coordinated multiagent dynamic systems, including discussions on swarms, multi-vehicle and swarm robotics. In addition, it addresses advanced distributed methods for the important topic of multiagent systems, with a goal of providing a high-level treatment of consensus to different versions while preserving systematic analysis of the material and providing an accounting to math development in a unified way. This book is suitable for graduate courses in electrical, mechanical and computer science departments. Consensus control in multiagent systems is becoming increasingly popular among researchers due to its applicability in analyzing and designing coordination behaviors among agents in multiagent frameworks. Multiagent systems have been a fascinating subject amongst researchers as their practical applications span multiple fields ranging from robotics, control theory, systems biology, evolutionary biology, power systems, social and political systems to mention a few. Gathers together the theoretical preliminaries and fundamental issues related to multiagent systems and controls Provides coherent results on adopting a multiagent framework for critically examining problems in smart microgrid systems Presents advanced analysis of multiagent systems under cyberphysical attacks and develops resilient control strategies to guarantee safe operationComplete with online files and updates, this cutting-edge text looks at the next generation of unmanned flying machines. Aerial robots can be considered as an evolution of the Unmanned Aerial Vehicl",,2021.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
42e801c089297ac900293d792648ccb08d1fac41,https://www.semanticscholar.org/paper/42e801c089297ac900293d792648ccb08d1fac41,Rescuing the Fresh Water Lakes of Africa through the Use of Drones and Underwater Robots,"In this paper, we present a conceptual system architecture for real-time monitoring, predicting and controlling of invasive water hyacinth in freshwater bodies through the use of emerging technologies. The proposed system is planned to be deployed as one of the rescue efforts to preserve the fresh water lakes of Africa. The case study and the system presented in this paper are based on the Lake Tana, situated near the city of Bahir Dar, in Ethiopia. The rescuing efforts of Lake Tana so far focused on removal of the weed by hand and using harvesting machines. With the weed invasion doubling every two weeks, the current approaches will not be able to control the rapid invasion of the weed, which is causing considerable socioeconomic losses. The proposed system architecture employs networked underwater robots, aerial drones and other environmental sensors for better mapping of the weed coverage in real-time, predicting the floating paths of the weed, and learning the favourable environmental conditions of the lake for eradicating the invasive weed. The advantages of the proposed technical intervention lie not only in accurate monitoring and fast removal of the weed, but also in facilitating data collection for better understanding of the underlying environmental and chemical conditions that facilitate the rapid infestation and growth of the invasive weed.",2021 International Conference on Information and Communication Technology for Development for Africa (ICT4DA),2021.0,10.1109/ict4da53266.2021.9672240,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aa30901b02e9923851d507e1f2726bcf9ef1b068,https://www.semanticscholar.org/paper/aa30901b02e9923851d507e1f2726bcf9ef1b068,OccuSpace: Towards a Robust Occupancy Prediction System for Activity Based Workplace,"Workplace occupancy detection is becoming increasingly important in large Activity Based Work (ABW) environments as it helps building and office management understand the utilisation and potential benefits of shared workplace. However, existing sensor-based technologies detect workstation occupancy in indoor spaces require extensive installation of hardware and maintenance incurring ongoing costs. Moreover, accuracy can depend on the specific seating styles of workers since the sensors are usually placed under the table or overhead. In this research, we provide a robust system called OccuSpace to predict occupancy of different atomic zones in large ABW environments. Unlike fixed sensors, OccuSpace uses statistical features engineered from Received Signal Strength Indicator (RSSI) of Bluetooth card beacons carried by workers while they are within the ABW environment. These features are used to train state-of-the-art machine learning algorithms for prediction task. We setup the experiment by deploying our system in a realworld open office environment. The experimental results show that OccuSpace is able to achieve a high accuracy for workplace occupancy prediction.",2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),2019.0,10.1109/PERCOMW.2019.8730762,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c69cbdc394467bc48e3dd6b9d521a31e680bf18f,https://www.semanticscholar.org/paper/c69cbdc394467bc48e3dd6b9d521a31e680bf18f,Characterising Across-Stack Optimisations for Deep Convolutional Neural Networks,"Convolutional Neural Networks (CNNs) are extremely computationally demanding, presenting a large barrier to their deployment on resource-constrained devices. Since such systems are where some of their most useful applications lie (e.g. obstacle detection for mobile robots, vision-based medical assistive technology), significant bodies of work from both machine learning and systems communities have attempted to provide optimisations that will make CNNs available to edge devices. In this paper we unify the two viewpoints in a Deep Learning Inference Stack and take an across-stack approach by implementing and evaluating the most common neural network compression techniques (weight pruning, channel pruning, and quantisation) and optimising their parallel execution with a range of programming approaches (OpenMP, OpenCL) and hardware architectures (CPU, GPU). We provide comprehensive Pareto curves to instruct trade-offs under constraints of accuracy, execution time, and memory space.",2018 IEEE International Symposium on Workload Characterization (IISWC),2018.0,10.1109/IISWC.2018.8573503,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
15151f55672b3712640b9b1ed88723f68ac01019,https://www.semanticscholar.org/paper/15151f55672b3712640b9b1ed88723f68ac01019,Deploying service integration agent for personalized smart elderly care,"In recently years, many care robots have received a lot of attention to help elderly people. However existing care robots have difficult to adopt personalization. For instance, some programmers have to customize robot program to meet needs of each elderly. Even if a care robot which has a feature of machine learning, it takes a long time to learn a preference for each elderly. In this paper, our goal is to deploy a smart care service integration agent that provides a personalization and integration for each elderly people. Our proposed service consists of three essential components, Virtual Care Giver (VCG), Virtual Care Personalizer (VCP) and Care Template. VCG is a robot agent, where executes care tasks in each home. The VCG is offered care tasks based on care template which Virtual Care Personalizer (VCP) generates. Virtual Care Personalizer (VCP) manages and generates personalization of care tasks the on cloud. Moreover, we deploy Care Template on the cloud which enables to provide the basic care tasks. To demonstrate the feasibility, we consider three kinds of usecase scenarios for two persona people.",2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS),2016.0,10.1109/ICIS.2016.7550873,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7aa4e49e05e45d994a3628efd2ac57587adf46de,https://www.semanticscholar.org/paper/7aa4e49e05e45d994a3628efd2ac57587adf46de,Visible Light Positioning for Location-Based Services in Industry 4.0,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyberphysical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",2019 16th International Symposium on Wireless Communication Systems (ISWCS),2019.0,10.1109/ISWCS.2019.8877305,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
65900342de4cc7cb6fc36af9862b99e21a98c2fb,https://www.semanticscholar.org/paper/65900342de4cc7cb6fc36af9862b99e21a98c2fb,Big tech and societal sustainability: an ethical framework,,AI & SOCIETY,2020.0,10.1007/s00146-020-00956-6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
01f3d1379049d2b1c5142dcc7ae2a7c8b79daa29,https://www.semanticscholar.org/paper/01f3d1379049d2b1c5142dcc7ae2a7c8b79daa29,Joint-based multi-task sparse learning for human action recognition,"Human action recognition from videos is a challenging computer vision task and it has wide applications, such as human robot/machine interaction, interactive entertainment, multimedia information retrieval, and surveillance. In this paper, we present a novel human action recognition method based on human joint position available video sequences. We use the joint-based covariance matrix over time as a discriminative descriptor for each human joint. Meanwhile, to encode the temporal information of the moving joints, multiple covariance matrices are deployed over sub-sequences that are different in temporal granularity. After that, we input the descriptor into multi-task sparse learning framework to obtain a more compact and discriminative action representation. The proposed method is evaluated on the sub-J-HMDB dataset. Experimental results show that the proposed method achieves a good performance and is superior to some state-of-the-art approaches.",ICIMCS '15,2015.0,10.1145/2808492.2808535,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3e030aab2643466991814bebb45701eddaf60c54,https://www.semanticscholar.org/paper/3e030aab2643466991814bebb45701eddaf60c54,Gray-box probabilistic occupancy mapping,"In order to deploy robots in previously unseen and unstructured environments, the robots should have the capacity to learn on their own and adapt to the changes in the environments. To this end, leveraging the latest developments in automatic machine learning (AutoML) and probabilistic programming, under the Hilbert mapping framework which can represent the occupancy of the environment as a continuous function of locations, we formulate a Bayesian framework to learn all parameters of the map. Crucially, this way, the robot is capable of learning the optimal shapes and placement of the kernels in Hilbert maps by merely embedding high-level human knowledge of the problem by means of prior probability distributions. Experiments conducted on simulated and real-world datasets demonstrate the importance of incorporating prior information.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d8c8e5916d5272a684bb675a43df9a4be7813fc0,https://www.semanticscholar.org/paper/d8c8e5916d5272a684bb675a43df9a4be7813fc0,Towards Non-Parametric Bayesian Learning of Robot Behaviors from Demonstration,"A large variety of mobile robots has blossomed recently, made possible by progresses in energy 
storage, electronics, processing speed, etc. We have seen these robots performing exploration and 
navigation tasks in different environments, and demonstrating impressive autonomy. However, the 
actual programming of complex behaviors, in particular when physical interaction takes place be- 
tween the robot and its environment, has remained tedious. This work often involves writing and 
tuning complicated state machines and requires a lot of engineering resources. Machine learning, 
which aims at specifying complex algorithms by leveraging data instead of just expert knowledge, 
proposes an alternative path. It appears especially useful for robotic tasks, as these often deal with 
high-dimensional input and output spaces while relying on a lot of contingent parameter values. 
Our aim is to understand the limits of capabilities that can be achieved with a simple, non-parametric 
system that learns from demonstration. We want to reduce the number of meta parameters to the 
minimum, because these ultimately have to be given to the system, while still handling tasks con- 
sisting of several steps. This paper proposes a system with only ns+ 2 meta parameters, where ns 
is the number of sensor dimensions. All these parameters are related to the robotic platform and its 
application, but do not depend on the task. We believe that such a system, should it perform well 
enough, would be of tremendous help for developing and deploying robotic applications. Indeed, 
the current requirement of choosing by hand many parameters is a major obstacle to the deployment 
of programming by demonstration.",NIPS 2012,2012.0,10.3929/ETHZ-A-010007956,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a9e77ef57fe90ecd9d390711c4113f987bcd6ea4,https://www.semanticscholar.org/paper/a9e77ef57fe90ecd9d390711c4113f987bcd6ea4,Exploring Collective Behavior of Internet of Robotic Things for Indoor Plant Health Monitoring,"In this paper, we present an innovative way of sensing the health of indoor plants and performing actions using social media messaging service as a cost-saving exercise. For this, we explore the use of Internet of Robotic Things (IoRT), the incorporation of robotics aspect into wider IoT environment to transform the present landscape into the fully automated world. The dynamic nature of health of plants is inferred intelligently by the employed machine learning models which take the input readings of the sensors attached to the robotic arm of an autonomous mobile robot. Ericsson’s APPIoT platform is used for storing and analyzing the sensor data in a cloud background for setting newer thresholds. The actuation efforts are shared to a closed community through social media messaging platform like WhatsApp. The community members who received the intimation are expected to take necessary actions immediately. The system has been tested in an indoor environment of the Ericsson office in Chennai where the different types of plant pots are deployed on different floors. By exploring the collective behavior in our system proved to be a cost-efficient way of maintaining the health of the indoor plants.",2018 IEEE International Conference on Internet of Things and Intelligence System (IOTAIS),2018.0,10.1109/IOTAIS.2018.8600865,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2ee8fb97d178a2e3afca742d7c1355eeafd6a819,https://www.semanticscholar.org/paper/2ee8fb97d178a2e3afca742d7c1355eeafd6a819,The EDNA Public Safety Drone: Bullet-Stopping Lifesaving,"Urban gun violence in cities across the world is a serious issue for public safety agencies and disaster management organizations. This led us to the development of the EDNA drone, an aerial robotics solution designed to equip first responders in high-risk settings with lifesaving-edge tools for situational awareness and non-lethal conflict resolution. The EDNA is an unmanned aerial vehicle (UAV) that delivers the patent-pending “Predictive Probable Cause” technology. The EDNA drone is designed to provide automated real-time analysis to assist teams entering high-risk situations where gun violence may occur. By leveraging machine learning, biometric sensors, and advanced materials in the field and routing feedback to an intuitive augmented-reality interface, the EDNA will provide autonomous threat detection and bullet-stopping capabilities wherever those features are needed‐‐to groups such as Police and Sheriff's Departments, Fire Departments, and EMT and emergency rescue teams. Data from the EDNA drone's sensors is fed to machine learning algorithms running on the drone in real-time. Through a neural network trained on past data, the EDNA is able to detect the presence and location of firearms and explosives, even through walls or other obstacles. Through the use of advanced metal foams and composite materials, the armored drone can even stop bullets‐‐functionality which has obvious benefits for humanitarian deployment.",2018 IEEE Global Humanitarian Technology Conference (GHTC),2018.0,10.1109/GHTC.2018.8601597,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
00d32b57b089aa2a92001663061b7d2b9375f90c,https://www.semanticscholar.org/paper/00d32b57b089aa2a92001663061b7d2b9375f90c,Towards Designing a Socially Assistive Robot for Adaptive and Personalized Cognitive Training,"There is a growing body of multidisciplinary research on how robotic systems can be deployed in education and training by providing personalized tutoring session to the user. Socially Assistive Robotics (SAR) is an efficient tool for educational and health-care purposes. In this work, we present our SAR system for personalized and adaptive cognitive training. More specifically, we present the sequence learning task that provides measures for executive function assessment, which may indicate learning or even behavior disabilities in children. This work outlines the designing and evaluation process of such a system, including data collection and analysis. The long-term goal of this research is to develop interactive machine learning methods towards the design of an adaptive SAR system that provides a personalized training session by adjusting the session parameters and the robot’s behavior to maximize user engagement and performance.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
58db9bb9ce8e3ad3036942f1c7f6073e7ec4c5ad,https://www.semanticscholar.org/paper/58db9bb9ce8e3ad3036942f1c7f6073e7ec4c5ad,Multimodal Dataset of Human-Robot Hugging Interaction,"A hug is a tight embrace and an expression of warmth, sympathy and camaraderie. Despite the fact that a hug often only takes a few seconds, it is filled with details and nuances and is a highly complex process of coordination between two agents. For human-robot collaborative tasks, it is necessary for humans to develop trust and see the robot as a partner to perform a given task together. Datasets representing agent-agent interaction are scarce and, if available, of limited quality. To study the underlying phenomena and variations in a hug between a person and a robot, we deployed Baxter humanoid robot and wearable sensors on persons to record 353 episodes of hugging activity. 33 people were given minimal instructions to hug the humanoid robot for as natural hugging interaction as possible. In the paper, we present our methodology and analysis of the collected dataset. The use of this dataset is to implement machine learning methods for the humanoid robot to learn to anticipate and react to the movements of a person approaching for a hug. In this regard, we show the significance of the dataset by highlighting certain features in our dataset.",ArXiv,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
03ac8180391e6c8f064dad559e5e4c2350af827b,https://www.semanticscholar.org/paper/03ac8180391e6c8f064dad559e5e4c2350af827b,Staged deployment of interactive multi-application HPC workflows,"Running scientific workflows on a supercomputer can be a daunting task for a scientific domain specialist. Workflow management solutions (WMS) are a standard method for reducing the complexity of application deployment on high performance computing (HPC) infrastructure. We introduce the design for a middleware system that extends and combines the functionality from existing solutions in order to create a high-level, staged usercentric operation/deployment model. This design addresses the requirements of several use cases in the life sciences, with a focus on neuroscience. In this manuscript we focus on two use cases: 1) three coupled neuronal simulators (for three different space/time scales) with in-transit visualization and 2) a closed-loop workflow optimized by machine learning, coupling a robot with a neural network simulation. We provide a detailed overview of the application-integrated monitoring in relationship with the HPC job. We present here a novel usage model for large scale interactive multi-application workflows running on HPC systems which aims at reducing the complexity of deployment and execution, thus enabling new science.",2019 International Conference on High Performance Computing & Simulation (HPCS),2019.0,10.1109/HPCS48598.2019.9188104,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
005aced4eead356ed50a1b925d25508839db60fa,https://www.semanticscholar.org/paper/005aced4eead356ed50a1b925d25508839db60fa,Artificial Intelligence and Public Policy,"There is growing interest in the market potential of artificial intelligence (AI) technologies and applications as well as in the potential risks that these technologies might pose. As a result, questions are being raised about the legal and regulatory governance of AI, machine learning, “autonomous” systems, and related robotic and data technologies. Fearing concerns about labor market effects, social inequality, and even physical harm, some have called for precautionary regulations that could have the effect of limiting AI development and deployment. In this paper, we recommend a different policy framework for AI technologies. At this nascent stage of AI technology development, we think a better case can be made for prudence, patience, and a continuing embrace of “permissionless innovation” as it pertains to modern digital technologies. Unless a compelling case can be made that a new invention will bring serious harm to society, innovation should be allowed to continue unabated, and problems, if they develop at all, can be addressed later.",,2017.0,10.2139/ssrn.3021135,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
af8a8e7f9049537e86e161f7756a7cda646ba2ec,https://www.semanticscholar.org/paper/af8a8e7f9049537e86e161f7756a7cda646ba2ec,CCRP: a novel clone-based cloud robotic platform for multi-robots,"Recently, the cloud computing paradigm has evolved from various research fields. A new path of research, cloud robotics, has emerged which allows robots to inherit the enormous computing and storage capability of cloud. Advances in cloud computing technologies, networking, parallel computing and other evolving technologies, and the integration with multi-robot systems, make it possible to design systems with new capabilities. The main advantages of cloud robotics are in overcoming the limitations of on-board robot computing and storage capabilities and in improving energy efficiency. Nevertheless, there is a lack of cloud robotics frameworks that can provide a secured environment for multi-robot application. The implementation of a robust cloud robotic platform capable of handling multi-robot applications has been shown to be challenging. 
 
 
This research proposes a novel Clone-based Cloud Robotic Platform architecture (CCRP) which assigns a Virtual Machine (VM) clone of each individual robot's operating system in the cloud, enabling fast and efficient collaboration between them via the cloud's inner-network. The system utilises Robot Operating System (ROS) as a middleware and programmable environment for robot development. This model is using the OpenVPN as a communication protocol between the robot and the VM, which provides considerable enhancement for the security and additional network for the system to allow multi-master ROS deployment. The Quality of Service (QoS) for the system has been tested and evaluated in terms of performance, compatibility and scalability via comparison study, which examines the CCRP performance against a local system and a proxy-based cloud system. 
 
 
Two case studies have been deployed for different robot scenarios. Case study 1 was focused on a navigation task which includes the process of mapping and teleoperation implemented in Google public cloud. The real time response has been examined by using the CCRP to teleoperate the NAO and Turtlebot robots. A response time and video streaming delays were measured to assess the overall QoS performance. Case study 2 is composed of a face recognition task performed using the CCRP in a private cloud on an Openstack platform. The objective of this task was to evaluate the system ability of running the tasks in the cloud effectively and to assess the collaborative learning capability. During the CCRP development and deployment stages an optimization study was conducted to determine optimal parameters for data offloading to the cloud and energy efficiency of a low-cost robot. 
 
 
The result of the CCRP performance evaluation proved that it is capable of running on a public and private cloud platform for self-configuring and programmable robotic systems, as well as executing various applications on different robot types. The CCRP is facilitating the improvements to QoS performance, compatibility and scalability and is providing a secure cloud computing environment for on-board robots.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e29531b07b78a9dcb21a4801c4ef0cd12487a9b7,https://www.semanticscholar.org/paper/e29531b07b78a9dcb21a4801c4ef0cd12487a9b7,Cooperative Robots and Sensor Networks,,Cooperative Robots and Sensor Networks,2013.0,10.1007/978-3-642-39301-3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
22a6214d71c19301b8d579d6d144871ff8591464,https://www.semanticscholar.org/paper/22a6214d71c19301b8d579d6d144871ff8591464,Application of Computer Vision in the Construction Industry,"In recent years, there is a proliferation of computer vision applications in different industries. Computer vision enables machines to see visually and allows images or videos to be analysed with machine learning algorithms. An abundance of data can be generated daily from sensors, cameras, drones, autonomous robots, BIM (building information model) and other devices. With the help of AI, insights from big data can be drawn to improve construction design, workflow and schedule. In this paper, the focus is to look at computer vision applications that can be employed in the construction industry to resolve construction issues and improve productivity, security and safety of the worksites. 
 
Object detection using deep learning algorithm can be employed for inspection of construction material components. This may include contour detection, counting objects and determining size of objects. In terms of security on worksites, facial detection and recognition can be carried out at various points to ensure only qualified people are allowed to enter certain regions to carry out necessary work that include operating on a particular machine. Motion detection mechanism can be deployed in cameras to detect motion in certain regions for surveillance purposes. Real-time interactions of workers and machinery can be captured and workers’ emotions’ can be monitored to check on their wellbeing. Workers’ safety at worksite is always a top concern for companies. Drowsiness detection devices can be installed in cranes to monitor crane operators’ alertness at work. We will look at functionalities provided by OpenCV and discuss the setup, the use and boundary of applications such as object detection, motion detection, facial recognition and drowsiness detection that can be used in the construction industry.",SSRN Electronic Journal,2019.0,10.2139/ssrn.3487394,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d5af86aa820f994ccd5dce7fed8898a65ba87441,https://www.semanticscholar.org/paper/d5af86aa820f994ccd5dce7fed8898a65ba87441,Robot Navigation in Distorted Magnetic Fields,"This thesis investigates the utilization of magnetic field distortions for the localization and navigation of robotic systems. The work comprehensively illuminates the various aspects that are relevant in this context. Among other things, the characteristics of magnetic field environments are assessed and examined for their usability for robot navigation in various typical mobile robot deployment scenarios. A strong focus of this work lies on the self-induced static and dynamic magnetic field distortions of complex kinematic robots, which could hinder the use of magnetic fields because of their interference with the ambient magnetic field. In addition to the examination of typical distortions in robots of different classes, solutions for compensation and concrete tools are developed both in hardware (distributed magnetometer sensor systems) and in software. In this context, machine learning approaches for learning static and dynamic system distortions are explored and contrasted with classical methods for calibrating magnetic field sensors. In order to extend probabilistic state estimation methods towards the localization in magnetic fields, a measurement model based on Mises-Fisher distributions is developed in this thesis. Finally, the approaches of this work are evaluated in practice inside and outside the laboratory in different environments and domains (e.g. office, subsea, desert, etc.) with different types of robot systems. Zusammenfassung Diese Arbeit beschäftigt sich mit der Nutzbarmachung der Verzerrungen von Magnetfeldern für die Lokalisierung und Navigation von robotischen Systemen. Die Arbeit beleuchtet dabei umfassend die verschiedenen Aspekte, die hierbei relevant werden können. Unter anderem werden die Charakteristiken von Magnetfeldumgebungen in verschiedenen Szenarien untersucht, in denen Roboter typischerweise zum Einsatz kommen und auf ihre Nutzbarkeit für die Navigation hin untersucht. Einen großen Teil nimmt weiterhin die Untersuchung der selbstinduzierten statischen wie dynamischen Magnetfeldverzerrungen von komplexen kinematischen Robotern ein, die der Nutzung von Magnetfeldern entgegenstehen könnten. Hierzu werden im Rahmen der Arbeit neben der Ermittlung von typischen Verzerrungen Roboter verschiedener Klassen auch Lösungsansätze zur Kompensation und konkrete Werkzeuge sowohl in Hardware (verteilte MagnetometersensorikSysteme) als auch in Software entwickelt. Dabei werden unter anderem Ansätze des maschinellen Lernens zur Erfassung der statischen und dynamischen Verzerrungen verfolgt und klassischen Methoden zur Kalibrierung von Magnetfeldsensoren gegenübergestellt. Um die Zustandsschätzung mittels probabilistischer Methoden um die Möglichkeiten der Lokalisierung in Magnetfeldern zu erweitern, wird darüber hinaus in dieser Arbeit ein auf von Mises-Fisher-Verteilungen basierendes Messmodell entwickelt. Abschließend werden die Ansätze dieser Arbeit im konkreten Einsatz innerhalb und außerhalb des Labors in unterschiedlichen Umgebungen und Domänen (u.a. Unterwasser, Wüste) mit verschiedenen Arten von Robotersystemen evaluiert. Acknowledgements First and foremost, I would like to thank my colleagues who shared the very productive and stimulating PhD student retreats over the last years with me. The discussions and the suggestions given there were invaluable. In particular, I would like to thank my supervisor Prof. Dr. Frank Kirchner, who has always encouraged me to follow the not so well-trodden path of using distorted magnetic fields for localization purposes. I am very grateful for the opportunities that the German Research Center for Artificial Intelligence Robotics Innovation Center offered me with its unique collection of different types of robots and infrastructure, that I was fortunate to be able to use during the course of my work. Without this large variety of robots at the institute and the support of their system administrators, this thesis wouldn’t have been possible. Furthermore, I would like to thank my former team leaders Dr. Jan Albiez, Dr. Jakob Schwendner and Dr. Stefan Stiene, who not only backed me up in my daily work in order to pursue my goal, but also gave me valuable advice for my doctoral thesis due to their long experience in the field of robotics science. I would also like to thank Dr. Hendrik Müller from the Federal Institute for Geosciences and Natural Ressources (BGR), who, at the very beginning of my work, provided valuable insights into the world of magnetism and magnetometers from the perspective of a physicist. The extensive field trials campaign in the Mars-like environment of the desert of Utah (US) was a particularly one-of-a-kind experience that pushed me forward in my work. In this context I would like to say a special thank you to the whole field trials Utah team. I wish to thank Dr. Johannes Lemburg, Martin Fritsche and Christopher Gaudig, who always provided thematic as well as moral support when I felt a little bit lost in my endeavor. Special thanks go to Dr. Florian Cordes, who took it upon him to read through the entire thesis and mercilessly pointed out to me not only mistakes in sentence construction. Finally and above all, I would like to thank my family. This work is dedicated to them.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c8373eb721d292f33176c73e813c32eac97ecb5c,https://www.semanticscholar.org/paper/c8373eb721d292f33176c73e813c32eac97ecb5c,Conversational Artificial Intelligence Development Tools for K-12 Education,"Artificial intelligence (AI) education is becoming increasingly important as the adoption of AI technology, including conversational agents such as Amazon Alexa, Siri, and the Google Assistant, increases. Current educational and programming tools enable non-programmers to develop simple conversational agents, or advanced programmers to develop complex agents. However, to the author’s knowledge, there are no tools for nonor novice programmers to develop conversational agents for the purpose of learning AI and programming skills. This paper describes AI curriculum that includes content about conversational agents, machine learning (ML), and AI ethics, as well as a blocks-based conversational AI interface developed within MIT App Inventor. During a series of six workshops, students used this interface to develop conversational agents for social good, including a memory aide, math tutor, speech visualizer, and recycling assistant. In this paper, I present (1) the blocks-based interface, (2) the conversational AI curriculum, (3) how conversational AI directly relates to computational thinking skills, and (4) results from an initial small-scale study. The results show that through the curriculum and using the blocksbased conversational AI interface, students learned AI and ML concepts, programming skills, and to develop conversational agents for social good. Introduction and Related Work The importance of artificial intelligence (AI) education is becoming more evident with AI’s increasing ubiquity. For instance, in one study where children and adults observed a robotic toy navigating a maze, the majority of participants indicated they thought the toy was smarter than they (Druga et al. 2018). In addition to the relevance of teaching about AI technology, it is important to emphasize AI’s social implications. For instance, conversational agents may be used positively to help drivers navigate the bewildering streets of Boston, or negatively to phish for confidential information. The need for AI education is especially apparent considering AI democratization tools, like Google’s AIY, Anki’s programmable Cozmo robots, and Scratch’s Cognimates extensions (Touretzky et al. 2019), which enable people without Copyright c © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. extensive training in computer science to meaningfully participate in AI development. Other tools to democratize and teach AI include MIT App Inventor extensions, such as the image recognition, speech processing and text analysis extensions (Zhu 2019); PopBots, a platform for preschoolers to train and interact with AI (Williams, Park, and Breazeal 2019); and Machine Learning for Kids, a platform to generate machine learning (ML) models and develop web or mobile apps using Scratch or MIT App Inventor (Lane 2018). Each of these tools focus on teaching essential ML and AI concepts through empowering learners to develop AI-enabled projects. Nonetheless, there are relatively few AI democratization tools that address conversational AI (the ability of a machine to interact with humans using natural language), which is rapidly becoming ubiquitous with devices like the Amazon Echo (as well as becoming a hot area of research). Furthermore, the few tools addressing this area are typically created for one of two purposes: (1) to enable non-programmers to create standardized apps with little programming knowledge (e.g., fill-in-the-blank Alexa Skill Blueprints (Amazon 2019)) or (2) to enable skilled developers to create complex conversational agents (e.g., Google Actions Console (Google 2018)). These tools do not provide scaffolding for the beginneror non-programmer to learn AI development skills, or purposeful educational opportunities about the implications of conversational AI. To address this lack of educational, conversational AI development tools, I created a blocks-based programming interface in MIT App Inventor, as shown in Figure 1. This visual coding platform enables a range of development (from simple to highly complex apps), simplifies the programming process, and promotes computational thinking (CT) skills. More specifically, such blocks-based interfaces lower the barrier of entry to programming, enable quick prototyping, and encourage learning gains in computer science (Weintrop and Wilensky 2017). Furthermore, conversational AI programming involves complex technical terminology (e.g., intents, slots, long short-term memory (LSTM) networks, etc.); thus, to enable students to learn such complex concepts, it is helpful to ease learning in other areas (e.g., through a straightforward blocks-based development environment). Figure 1: User workflow to create a conversational AI agent. The user first implements the Voice User Interface (VUI) and endpoint function using a blocks-based interface. The blocks are converted to JSON and JavaScript, which define the agent’s functionality on Alexa devices. Additionally, the platform provides students with highagency, project-based learning opportunities. This is in contrast to tools such as Zhorai and PopBots, which are aimed at a younger audience, and are less focused on project development, but rather on exposure through conversational agent interaction (Van Brummelen, Lukin, and Lin 2019; Williams, Park, and Breazeal 2019). Research suggests that curricula in which students develop STEM projects increases students’ creative thinking skills, STEM skills efficacy, and STEM career aspirations (Lestari, Sarwi, and Sumarti 2018; Beier et al. 2019); thus, for the purpose of STEM and CT skill development, I created a project-based curriculum. This curriculum involves brainstorming realworld problem solutions, generating designs, and developing conversational agents, as described in the Curriculum section. I investigate the effectiveness of this curriculum and the conversational AI interface through a workshop-based, small-scale study, as outlined in the Results section. Motivational Scenario To provide a basis for understanding (1) the components of the conversational AI interface, (2) how someone would use the interface, and (3) the capabilities and limitations of the system, I present a scenario about how “Sheila” created a conversational AI app for her cousin “Jaidon”. Although the scenario is fictional, a version of Sheila’s Storybook App was implemented using the interface and presented to students in the workshops. Furthermore, actual applications students developed are discussed in the Results section. Sheila’s Storybook App Sheila, a seventh grade student, loves stories. When she was younger, she imagined jumping into the pages of her storybook and interacting with the characters. During a computer lesson, she heard about MIT App Inventor’s conversational AI interface and had a brilliant idea: to create a talking storybook. Sheila would create the storybook app using MIT App Inventor, run it on her tablet, and enable conversation using the Alexa app. The storybook would have the following main features: • You could swipe through “pages” of the storybook while reading and viewing illustrations on-screen • You could ask Alexa about the characters, setting, and narrative (e.g., Figure 2) • You could ask Alexa to read you the story, and as Alexa reads, the sentence on the app’s page would be highlighted • You could have “conversations” with the storybook characters, and when you ask a character a question, a response would be automatically generated Figure 2: Speaking with Alexa contextually with Sheila’s storybook. Modified from (Van Brummelen 2018). To implement the storybook app, Sheila first uploaded illustrations to MIT App Inventor and implemented pageflipping functionality by creating events in the blocks-based interface. When the “next” button was pressed, a counter would increase, and the next illustration would pop up on screen. The opposite event would occur when the “previous” button was pressed. After creating the mobile app (as shown in Figure 3), Sheila moved onto the conversational AI portion of the app. Figure 3: The storybook mobile app being developed on the MIT App Inventor website. Sheila added a new Alexa Skill to the project by clicking the Add Skill button. This brought her to a page where she could drag-and-drop and connect blocks together to create a conversational agent. First, she dragged out a define intent block so that the Voice User Interface (VUI) would recognize when someone said, “Tell me a story”. She wanted the agent to respond by telling a story about zorillas (a littleknown animal that Sheila absolutely loved!), so she also dragged out a when intent spoken block. She connected the when intent spoken block to a say block containing a text block with the first line of the story. Sheila also wanted people to be able to speak with the main character, Karabo the Zorilla. She didn’t want to write out all the possible answers to people’s questions though (that would take forever), so she decided to use a generate text block to generate Karabo’s answers instead. According to her seventh grade teacher, this block used machine learning to generate sentences sounding kind of like the stories in the block’s drop-down menu. Sheila imagined Karabo speaking kind of like Dr. Seuss, so she chose that one from the menu. After adding some additional functionality using blocks, as shown in Figure 4, Sheila sent the Alexa Skill and mobile app to her cousin Jaidon. Jaidon downloaded the app and started flipping through the pages, talking to the storybook as he went along. Jaidon was thrilled that he could listen to Alexa read him the story, especially since he didn’t know how to read himself. He also had a blast asking Karabo questions and hearing the infinite different ways Karabo would respond (despite the sentences not always being logical). He laughed when Karabo said, “Little cat in the hat, they can not eat them in your snow”. It sounded quite li",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e7ba7fbc0cd252a20cf771d6a39246a0a480ab66,https://www.semanticscholar.org/paper/e7ba7fbc0cd252a20cf771d6a39246a0a480ab66,Task-based behavior generalization via manifold clustering,"Machine learning algorithms can be expensive to deploy, in particular, those used in robotics applications that perform many variations of the same task. Solutions to one variation of a task may be found via Reinforcement Learning algorithms, and are typically modeled as a vector of N parameters encoding the robot's behavior policy. When N is large or executing robot trials is time-consuming, searching in the space of solutions becomes prohibitively expensive. In this paper, we introduce a method that allows robots to generalize behaviors by analyzing solutions to a small number of previously-trained related tasks. This allows for approximate policies for novel tasks to be rapidly estimated. We present a method that achieves this type of generalization by performing nonlinear regression directly on the policy manifold — i.e., the solution space spanned as we change the parameters describing tasks. Because tasks are typically described by few parameters, the corresponding policy manifold has few degrees of freedom, which leads to low-dimensional surfaces. We exploit this property to construct a function that maps task parameters to policy parameters (a parameterized skill). Our method uses manifold clustering techniques to deal with discontinuous manifolds, a challenging situation arising from physical obstacles or robot constraints. We evaluate our method on a set of robot manipulation tasks and show that it can efficiently estimate policies for novel tasks from a small number of training examples.",2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),2017.0,10.1109/IROS.2017.8206503,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b86c85870e71ca5b5aa8883e22b716dcb464819a,https://www.semanticscholar.org/paper/b86c85870e71ca5b5aa8883e22b716dcb464819a,Computational and Robotic Models of the Hierarchical Organization of Behavior,,Springer Berlin Heidelberg,2013.0,10.1007/978-3-642-39875-9,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5b0ffc40f91580ec9b5cff2b2e27b51b6f1a6c24,https://www.semanticscholar.org/paper/5b0ffc40f91580ec9b5cff2b2e27b51b6f1a6c24,Lawrence Berkeley National Laboratory Recent Work Title Accelerating the discovery of materials for clean energy in the era of smart automation Permalink,"| The discovery and development of novel materials in the field of energy are essential to accelerate the transition to a lowcarbon economy. Bringing recent technological innovations in automation, robotics and computer science together with current approaches in chemistry , materials synthesis and characterization will act as a catalyst for revolutionizing traditional research and development in both industry and academia. This Perspective provides a vision for an integrated artificial intelligence approach towards autonomous materials discovery , which, in our opinion, will emerge within the next 5 to 10 years. The approach we discuss requires the integration of the following tools, which have already seen substantial development to date: highthroughput virtual screening, automated synthesis planning, automated laboratories and machine learning algorithms. In addition to reducing the time to deployment of new materials by an order of magnitude, this integrated approach is expected to lower the cost associated with the initial discovery. Thus, the price of the final products (for example, solar panels, batteries and electric vehicles) will also decrease. This in turn will enable industries and governments to meet more ambitious targets in terms of reducing greenhouse gas emissions at a faster pace. volume 3 | mAY 2018 | 5 PERSPECTIVES © 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. NAture reviews | MAtEriAlS First, we briefly discuss advances in AI and then provide an overview of the main applications of materials for the clean energy sector. Next, we explore stateof-theart automated procedures for materials discovery, with a focus on machine learning. The field of organic materials is the farthest along in many of the areas required for an integrated platform, but along the way, we point out some of the notable advances in both inorganic materials and nanomaterials. Finally, we conclude and provide our vision for the next generation of integrated AI approaches towards autonomous materials discovery, which will emerge within the next 5–10 years. Advances in AI Scientific discoveries are usually associated with an insight: the act of intuitively seeing a phenomenon, which contrasts with systematic mechanistic learning. This is despite the fact that most of our discoveries are based on extensive preliminary studies. Insight is considered to be an exclusively human attribute, whereas systematic exploration is connected with automated platforms. However, this gap between a creative and intuitive targeted search and a systematic exploration continues to narrow as automated platforms become increasingly sophisticated and are able to process more complex information17,18. In addition to systematic screenings of large databases and building chemical structures according to a set of preprogramed rules, today's platforms can update the rules for analysing the available information and even search for more information that helps them to make specific decisions19. In this case, it is natural to expect that in the very near future such platforms will be able to not only predict the properties of materials but also test hypotheses by designing structures and characterizing them, becoming autonomous. It has already been demonstrated that combinatorial optimization procedures provide faster screening of the molecular space than traditional approaches based on intuition20. The pharmaceutical and chemical industries, as well as academic research environments, use these methods for the design of new molecules, reactions and materials21–26. However, combinatorial chemical synthesis makes an exhaustive search of the multidimensional molecular space out of reach26. As such, the community needs a more rational approach for exploration of this large space; this is where machine learning comes into play. The recent progress in machine learning and statistical inference methods can be viewed as a revolution in AI. Most machine learning methods, such as neural networks and Bayesian optimization, were developed decades ago but have not found widespread use until recently27,28. Basic research in AI continues to be backed by governments, industry, and public and private research institutes29. Today, machine learning methods are behind many commercial applications, such as Internet searches, natural language translation, and image and speech recognition. Recently, an upgraded version of AlphaGo, the Goplaying program from Google, which in part uses deep neural networks (DNNs) and reinforcement learning as key algorithms, beat the top human Go player30. Moreover, its playing style inspired other Go players. Since mid-2017, Cisco has used a multilayer supervised algorithm based on machine learning to analyse encrypted traffic (that is, HTTPS). This algorithm helps identify malware communication through passive monitoring and yields enhanced incident responses31. In economics, machine learning has also started to emerge, notably to predict economic growth32, to quantify predictive performance33 and to anticipate customer behaviour34. While recent progress is making its mark in nonscientific endeavours, the application of machine learning in science and medicine is also emerging: assisting physicians in interpreting computerbased medical images, processing biomedical signals and learning from patient data35–37. In late 2017, a DNN was successfully shown to enable the reconstruction of perceptual and subjective images from the activity of human brains38. Within the context of this Perspective, the application of several machine learning methods to computational chemistry has recently bourgeoned39–41. From the representation of aromaticity and conjugation in general42 to the prediction of protein–ligand affinities43, there is increasing interest in using DNNs and convolutional neural networks in a wide range of applications. Hybrid learning models, which combine different approaches to leverage their respective strengths, have shown great promise. Examples of hybrid learning models include Bayesian deep learning44, Bayesian conditional generative adversarial networks45 and deep Bayesian optimization46. The latter has been successfully applied to reverse engineer chemical reactions to quantitatively and qualitatively reproduce observed behaviours46. Machinelearning-based algorithms have also been intensively used to bypass expensive static47 and dynamic48–50 ab initio electronic structure calculations. Although exploration and discovery are more challenging than interpolation or optimization for AI, recent algorithmic developments show substantial promise for making advances in these areas. To overcome these challenges of inverse design in computational chemistry and to explore the openended chemical space, autoencoders and generative adversarial networks have emerged as powerful tools to generate novel molecular structures with desirable properties tailored to specific needs51–55. This progress is only the beginning of the integrated materials discovery revolution. The key component of an autonomous discovery approach lies in the synergy between machine learning and robotics. One might ask, “Why are robots better?” A cursory analysis of humans versus robots shows some clear advantages for the latter. First, robots can operate in more adverse conditions. This can be seen even outside chemistry; for example, robots were sent to Mars decades before humans. For chemistry, this ability can translate to procedures that are subject to high temperatures and/or pressures, toxic solvents and highly exothermic processes. Robots also excel at providing unbiased and reproducible routes towards materials discovery. For example, it was recently demonstrated that not only do machinelearning-based algorithms cover an application space of polyoxometalates approximately six times larger than a human approach but they also increase the accuracy of prediction by a relative value of 6.9%56. Recently, an autonomous infrastructure for the optimization of chemical reaction conditions through the Deep Reaction Optimizer (DRO), an algorithm based on deep reinforcement learning, was reported57. Furthermore, robots are better at recording reaction procedures independently of their outcome and reduce waste by rigorously following the stoichiometry of the experiments18. Robots also provide a natural platform for scaling chemical experiments, reducing the cost per experiment. In 2009, a hypotheticodeductive ‘robot scientist’, named Adam, was developed that could autonomously perform experiments, devise hypotheses and design experiments to validate the hypotheses in the area of functional genomics58. 6 | mAY 2018 | volume 3 © 2018 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. www.nature.com/natrevmats P e r s P e c t i v e s",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
505195a1db461b083ee30141923b1610ec2cb8ee,https://www.semanticscholar.org/paper/505195a1db461b083ee30141923b1610ec2cb8ee,Artificial intelligence and the dreaded 's.,"Numerous industries are being disrupted by growth in new technologies, especially information technologies, and healthcare is no exception. Advances in robotics, wireless sensor networks, 5D printing, and cloud technologies are reshaping countless industries. I am intrigued by the increasing importance of automation, machine learning, and artifi cial intelligence (AI) in healthcare. Let us explore three questions together: • Where are common applications of AI and automation in healthcare? • What implications for physician assistants (PAs) arise from increased automation and AI in caring for patients? • Did AI bring back the ’s that causes any self-respecting PA to cringe? I nearly panicked recently when I caught sight of the following two headlines from online articles about new healthcare technologies, which might lead a person to think the PAs of the future are not people at all. At the very least, I was ready to e-mail the AAPA communications team to combat those pesky apostrophes. The articles actually detailed advances in automation and AI within healthcare. Bright.MD raises another $8M for “virtual physician’s assistant” SmartExam (www.mobihealthnews.com/content/ brightmd-raises-another-8m-virtual-physicians-assistantsmartexam) Healthcare Chatbots: The Physician’s Assistant of the Future? (http://blog.kantarhealth.com/blog/brian-mondry/ 2016/11/28/healthcare-chatbots-the-physician’s-assistantof-the-future) Next, let us sort out AI and automation. According to Merriam Webster, artifi cial intelligence is the capability of a machine to imitate intelligent human behavior. Automation, on the other hand, is the automatically controlled operation of an apparatus, process, or system by mechanical or electronic devices that take the place of human labor. COMMON APPLICATIONS A widely adopted automation in healthcare is appointment reminder software that automatically reminds patients of their upcoming scheduled appointments, with options to customize the message and/or time it is delivered for patient preference. Similarly, missed appointment notifi cation systems can alert a PA to a potentially worrisome pattern of missed appointments for a patient identifi ed as high-risk. Robotics, commonly deployed in areas such as pharmacy and surgery, are automations proven to increase effi ciency and safety. According to CB Insights, about 86% of healthcare provider organizations, life science companies, and healthcare technology vendors are using AI technology. The most common applications seem to fall into one of ten categories: managing medical records and other data; doing repetitive jobs such as analyzing tests, interpreting radiologic studies, and data entry; helping design treatment plans; digital consultation (such as the Babylon app); virtual nurses (such as the Molly app), medication management (such as the AiCure app); drug development; precision medicine; health monitoring; and healthcare system analysis.1 Numerous tech giants are investing heavily in AI applications for healthcare as well, such as Microsoft’s Healthcare NExT initiative and Google’s Deepmind Health.",JAAPA : official journal of the American Academy of Physician Assistants,2018.0,10.1097/01.JAA.0000530302.23280.25,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
589c0d140fcd4e33b292920eba786a30c385f4db,https://www.semanticscholar.org/paper/589c0d140fcd4e33b292920eba786a30c385f4db,Interpolated Rigid Map Neural Networks for Anatomical Joint Constraint Modelling,"The demand for accurate individual and general kinematic joint models is increasing with growing applications in fields such as animation, biomechanics, motion capture, ergonomics and robot human interaction modelling. Many approaches have exploited unit quaternions to eliminate singularities when modelling orientations between limbs at a joint, leading to the development of a number of novel joint constraint validation and correction methods. A number of machine learning approaches have been applied to this modelling problem, as depending on training data either individual or general joint models can be created. Recent work has demonstrated the use of Rigid Maps to model regular conical constraints on the orientation of the limb. In this paper we extend this work deploying a modified Rigid Map Network with a continuous output.","International journal of simulation: systems, science & technology",2019.0,10.5013/ijssst.a.20.s1.13,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6a4f014ac3a5ac7607637574ae7f293ea9f7e786,https://www.semanticscholar.org/paper/6a4f014ac3a5ac7607637574ae7f293ea9f7e786,The Challenges for Interpretable AI for Well-being,"In this AAAI Spring symposium 2019, we discuss interpretable AI in the context of well-being AI. Interpretable AI is an artificial intelligence methods and systems, of which outputs can be easily understood by humans. Especially in the human health and wellness domains, making wrong predictions may lead to critical judgements in life or death situations. AI based systems must be well-understood. We define “well-being AI” as an AI research paradigm for promoting psychological well-being and maximizing human potential. Interpretable AI is important for well-being AI in senses that (1) to understand how our digital experience affects our health and our quality of life and (2) to design well-being systems that put humans at the center. One of the important keywords in understanding machine intelligence in human health and wellness is cognitive bias. Advances in big data and machine learning should not overlook some new threats to enlightened thought, such as the recent trend of social media platforms and commercial recommendation systems being used to manipulate people's inherent cognitive bias. The second important keyword is “social embeddedness”. Cognitive bias will be affected by how the AI is perceived particularly at the community or social level. Social embeddedness is the social science idea that actions of individuals are refracted by the social relations within their community. In our contexts, understanding relationships between AI and society is very important, which includes the issues on AI and future economics (such as basic income, impact of AI on GDP), or “well-being society (such as happiness of citizen life quality). This paper describes the detailed motivation, important keywords, the scope of interests and research questions in this symposium. Motivation for Interpretable AI for well-being Interpretable AI is an artificial intelligence methods and systems, of which outputs can be easily understood by humans. Recently, the European Union’s new General Data Protection Regulation (GDPR) has raised concerns about the emerging tools for automated individual decisionmaking. These tools use algorithms to make decisions based on user-level profiles, with the potential to significantly affect users. Recent AI technologies (e.g.: Deep Learning and other advanced machine learning methods) will change the world. However, excessive expectations for AI (e.g., the representation of general-purpose AI in science fiction) and threat theory (e.g. AI will lead to unemployment) distort the judgment of many people. Understanding both the potential and the limitations of the current AI technologies is therefore very important. Especially in the human health and wellness domains, interpretable AI remains a huge challenge. For example, “evidence-based medicine” requires us to show the current best evidence in making decisions about the care of patients. “Why did the system make this prediction?” will be a key question. Even if the system is not accurate, it must be explainable and predictable. Although statistical machine learning predicts the future based on past data, it is difficult to respond to a new event which has never seen in the past. Training data that has outliers or adversarially generated data may lead an AI-based system to make wrong predictions (sometimes with high confidence) in life or death situations in medical diagnoses. For AI to be safely deployed, these systems must be well-understood. One of the important goals in this year's symposium is to discuss the technical and philosophical challenges of interpretability for well-being AI. Understanding Cognitive Bias and Social Embeddedness AI also provides the new risk of amplifying our “cognitive bias” through machine learning, as we discussed in our previous AAAI18 Spring symposium on “beyond machine intelligence” (Kido and Takadama, 2018). In the recent trend of big data becoming personalized, corresponding AI technologies for manipulating one’s cognitive bias are starting to evolve; examples of this include social media platforms such as Twitter and Facebook, and commercial recommendation systems. According to the “Echo chamber effect,” people with the same opinion tend to form communities, which makes it felt that everyone else also shares the same opinion. Recently, there has also been a movement to use such cognitive bias in the political world. We welcome discussions on “cognitive bias” in human or personal robot communications. “Social embeddedness” of AI is also an important keyword in this symposium. We welcome diverse discussions on the relationships between AI and society. The topics on social embeddedness of AI may include such issues as “AI and future economics (such as basic income, impact of AI on GDP)” or “well-being society (such as happiness of citizen, life quality)”, etc. Cognitive Bias will be affected by how the AI is perceived particularly at the community (or societal) level. “Social embeddedness of AI” seems likely to become a significant area as AI continues to develop. Our Scope of Interests and Research Questions. We expect to discuss important interdisciplinary challenges for guiding future advances in well-being AI. We will have the following scope of interests in this symposium: (1) ""Excessive expectation for AI understanding possibilities and limitations of the current AI technologies"", (2) ""Technical and philosophical challenges on interpretability for well-being AI"" (3) ""Cognitive bias"" and ""social embeddedness of AI"" in human/robot communications, from the sociocultural/political aspects to the technical/practical, accuracy and efficiency issues in health, economics, and other fields. More technically, we have the following research questions in Interpretable AI for well-being. We need to deepen the understandings of the possibilities and limitations of the Machine Learning and other advanced analyses for Health & Wellness. 1 Interpretable AI/ML  How can we develop interpretable machine learning methods in well-being AI that provide ways to manage the complexity of a model and/or generate meaningful explanations?  How can we use the tools of causal inference to reason about fairness in well-being AI? Can causal inference lead to actionable recommendations and interventions? How can we design and evaluate the effect of interventions?  What are the societal implications of algorithmic exploration? How can we manage the cost that such exploration might pose to individuals? 2 Unintended consequence of algorithms in well-being AI  Can we use adversarial conditions to learn about the inner workings of algorithms?  Can we learn from the ways they fail on edge cases?  Can we achieve accountability in well-being AI?  How can we conduct reliable empirical black-box testing for ethically salient differential treatment?  How can we manage the risks that such unintended consequence might pose to users?",AAAI Spring Symposium: Interpretable AI for Well-being,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6b1103498a08298be4ea90afd800da6fcde74b53,https://www.semanticscholar.org/paper/6b1103498a08298be4ea90afd800da6fcde74b53,Upcoming oberseminars,"In case of intoxication, fast treatment means the difference between permanent organ damage and a harmless and short hospital stay. However, when the toxic agent is unknown to the clinician, a diagnosis must be reached based on the patient’s symptoms. This project aims to create a machine learning based decision support system for poison identification. The algorithm utilizes both established expert knowledge from literature, as well as observational clinical data provided by the Giftnotruf of the Klinikum rechts der Isar. We expect to achieve a superior classifier by using both data sources in comparison to algorithms, which operate only on literature data or patient data alone. We will compare different approaches of incorporating the expert knowledge into the learning process, including transfer learning and Physics-guided Neural Networks. Date & Time 10.Januar 2020 -11:00 Title Continual and incremental learning with less forgetting strategy Student Afshar Kakaei Type MA Kick-Off Supervisor Seong Tae Kim Director Prof. Nassir Navab Abstract Recently, deep learning has great success in various applications such as image recognition, object detection, and medical applications, etc. However, in the real world deployment, the number of training data (sometimes the number of tasks) continues to grow, or the data cannot be given at once. In other words, a model needs to be trained over time with the increase of the data collection in a hospital (or multiple hospitals). Medical experts could also define a new type of lesion. Then, the pre-trained network needs to be further trained to diagnose these new types of lesions with increased data. ‘Class-incremental learning’ is a research area that aims at training the learned model to add new tasks while retaining the knowledge acquired in the past tasks. It is challenging because DNNs are easy to forget previous tasks when learning new tasks (i.e., catastrophic forgetting). In real-world scenarios, it is difficult to store all training data, which is used when training DNN at the previous time due to the privacy issues of medical data. In this project, we will develop a solution to this problem in medical applications by investigating an effective and novel learning method. Date & Time 10.Januar 2020 -11:30 Title Bayesian Deep Learning for Medical Image Segmentation Student Jyotirmay Senapati Type MA Final Supervisor Sebastian Poelsterl Additional supervisors Abhijit Guha Roy Director Prof. Dr. Christian Wachinger Abstract Bayesian Deep Learning has gained a lot of popularity in the Deep Learning community due to its ability to generate well calibrated outputs by associating a confidence score along with the prediction. This has high implications in using deep learning for safety critical applications like Medical diagnosis, Autonomous driving etc. Recently, alot of work has explored and proposed different strategies for variational inference of Deep Networks to estimate the confidence. It is very difficult to say which is better for what application. Towards this end we explore these different strategies to identify their effectiveness targeting the application of segmentation Quality Control.Bayesian Deep Learning has gained a lot of popularity in the Deep Learning community due to its ability to generate well calibrated outputs by associating a confidence score along with the prediction. This has high implications in using deep learning for safety critical applications like Medical diagnosis, Autonomous driving etc. Recently, alot of work has explored and proposed different strategies for variational inference of Deep Networks to estimate the confidence. It is very difficult to say which is better for what application. Towards this end we explore these different strategies to identify their effectiveness targeting the application of segmentation Quality Control. Date & Time 10.Januar 2020 -12:00 Title Uncertainty-driven ultrasound pose estimation Student Navneet Madhu Kumar Type MA Kick-Off Supervisor Mohammad Farid Azampour Additional supervisors Raphael Prevost Director Prof. Dr. Nassir Navab Abstract Pose estimation from ultrasound is challenging due to the poor image quality, symmetric ambiguity in anatomical structures and large variations in the pose. Anatomical structures from different viewpoints will appear similar due to shape symmetries, occlusion, and repetitive textures. Thus, the pose estimation will have some degree of uncertainty. In this work, we propose to exploit this uncertainty to robustly estimate the pose for an ultrasound sweep. For each ultrasound image, the network predicts multiple poses. Uncertainty is measured using Monte Carlo dropout for each pose. We aggregate the results of the network for the unambiguous scans weighted by their uncertainty estimates in a voting framework to predict the pose. We show the benefits of our approach for initial pose prediction for a liver ultrasound sweep. Date & Time 10.Januar 2020 -12:30 Title Graph-based Point Cloud Registration for Tool Tracking Student Shervin Dehghani Type IDP Kick-Off Supervisor Mahdi Saleh Director Nassir Navab Abstract Registration of 3D meshes is a common problem useful in many domains such as medical image registration and tool tracking. In order to fit 3D models, we can use point clouds acquired from the surface. To process point clouds, which are unordered data by nature, we utilize a graph structure to represent patches. The graph is used to enfold information and is later used to localize salient keypoints. The pipeline is trained self-supervised and can capture shape and texture information encoded in a descriptor in order to register point clouds. This can be used for tool tracking and pose estimation. The learned graph network can describe and compress both rigid or deformable surfaces and finds keypoints from the point clouds for registration. Date & Time 10.Januar 2020 -13:00 Title 6D Pose Tracking as an Action Decision Process Student HyunJun Jung Type MA Final Supervisor Benjamin Busam Director Nassir Navab Abstract Object pose estimation is an integral part of robot vision, augmented reality and modern medical applications. Robust and accurate pose prediction of both object rotation and translation is a crucial element to enable precise and safe human-machine interactions and to allow visualization in mixed reality. Previous 6D pose estimation methods treat the problem either as a regression task or discretize the pose space to classify. We reformulate the problem as an action decision process where an initial pose is updated in incremental discrete steps that sequentially move a virtual 3D rendering towards the correct solution. While a neural network estimates likely moves from a single RGB image, it also takes the complementary decision of whether the pose is acceptable or the process needs further iteration. In comparison to previous approaches that learn an object-specific pose embedding, a decision process allows for a lightweight architecture while it naturally generalizes to unseen objects. Moreover, the coherent actions for pose update and process allow to dynamically reduce the computation cost if there are insignificant external changes in a video sequence. While previous methods have a static inference time, we can thereby automatically increase the runtime depending on the object motion. We evaluate the robustness and accuracy of our action decision network on video scenes with known and unknown objects and show how this can improve the state-of-the-art with object-specific models on YCB videos significantly. Date & Time 10.Januar 2020 -13:30 Title Histological image normalization using disentangled GAN Student Raghav Sharma Type MA Kick-Off Supervisor Tingying Peng Additional supervisors Carsten Marr Director Nassir Navab Abstract Staining and scanning of tissue samples for microscopic examination is fraught with undesirable colour variations arising from differences in raw materials and manufacturing techniques of stain vendors, staining protocols of labs, and colour responses of digital scanners. Colour inconsistency between training and testing image could affect the performance of deep learning based applications. As a result, colour normalisation techniques are proposed to normalise the colour of the target domain into the source domain. In this presentation, we show a novel histological colour transfer toolkit based on disentangled GAN to augment or to normalise images from different scanners. In particular, with such colour augmentation, we can guarantee our trained model is robust to colour variations. Date & Time 24.Januar 2020 -10:30 Title Tissue Property Estimation Using Medical Ultrasound Imaging Student Courtney Restivo Type MA Kick-Off Supervisor Walter Simson Additional supervisors Christoph Hennersperger Director Nassir Navab Abstract Characterizing tissue status is a critical step in determining patient outcome for many medical procedures, for example in the grading of liver disease or in the removal of tumors. Ultrasound imaging, due to providing real-time, inexpensive, non-ionizing, and feasible intraprocedural information, has become a common imaging modality for monitoring these procedures. However, clinical ultrasound systems do not incorporate sufficient characterization of tissue, therefore the doctors performing these procedures have no feedback on the extent of tissue changes. This thesis focuses on developing a method to characterize tissue change using ultrasound data. The aim is to correlate values of certain properties of the raw radiofrequency data after tissue change to tissue status. Both traditional classification methods, along with machine learning and deep learning based methods will be considered for this task. Date & Time 24.Januar 2020 -11:00 Title Digital Therapy System for Individual Rehabilitation Student Mohammad Bagheri & Negar Namdarian Type IDP Kick-Off Supervisor Felix Bork Director Prof. Nassir Navab",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5cb3baa1c13caf1d0a58da42639e4730bffd9579,https://www.semanticscholar.org/paper/5cb3baa1c13caf1d0a58da42639e4730bffd9579,Building a Sextbot : botrotica,"In this paper, I reflect on the the development and deployment botrotica—a situated, site-specific art installation and coding machine learning experiment for public interaction. The system, botrotica is a sextbot—a chat bot that is inspired by the explicit messages that we send to each other. This sextbot is an exploration in sexting developed for futuresmut, the 10th annual Smut Cabaret at Nextfest 2017, an emerging artist festival in Edmonton, Alberta, Canada. This project serves is case study on the incorporation of machine learning technology in interactive multimedia art installations. 1 How did we get here? The Internet enables us to explore our identities and develop ourselves through social communication. New technologies continue to change the way we interact with one another. How do we share ourselves publicly and privately? Who do we share ourselves with and what exactly are we sharing? We create ourselves by how we interact with each other, and as Marshall McLuhan would say, “the medium is the message [9]”. Modern artists often remind us of how these systems developed to connect us can drive us apart, transforming social media into antisocial forces [3]. Thoughts, ideas, pictures, sounds, and videos can be exchanged publicly and privately with passion partners. Sexting— the portmanteau of sex and texting added to the Merriam-Webster’s Collegiate Dictionary in August 2012—is the sending of sexually explicit messages or images by cell phone. The popularization of this technology has led to the emergence of a field of psycho-social research. Rather than increasing intimacy in these types of relationships, sexting may act as a buffer for physical intimacy and may be associated with risky behaviour in teens and young adults [1, 7]. Sexting has also been linked to sexual objectification [2]. This artistic installation uses botrotica to challenge questions of sexual communication, objectification, instigation, and artificial companionship. This work is inspired by the work of Sherry Turkle in “Alone Together” [10], and software: My Virtual Boyfriend [6] and SELF [5]. 2 What is botrotica? botrotica is a disembodied site-specific installation. It is situated abstractly behind a phone number. Willing and consensual participants only need access to a mobile phone for interaction. This interactive cyber-installation piece was turned-on for the duration of the futuresmut cabaret evening event. When participants texted the number they engaged the sextbot in a back-and-forth interaction which included rule-based preset dialog and short utterances generated from a deep neural network model trained on a large collection of online erotica. In addition to conversational dialogue, several action hooks into machine learning (ML) systems could be triggered to classify objects, faces, details, emotions, and not-safe-for-work (NSFW) predictions in images. This multimodal interaction was designed for ongoing engagement over the course of a cabaret evening. The system is designed to respond in a way that a late-night human sexter might. It has a slightly coy and dirty persona, asks for photos, and, if requested of, sends photos of web-scraped, human 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada. curated, sexy robots. If the user sent an image to the bot, then it would attempt to classify the objects in the image and send back the resulting predictions. This notion of ‘self-objectification’ has been re-interpreted, under the lens of post-feminist media culture, not as oppressive but as constituting a prime site of sexual liberation, value and pleasure [4]. botrotica would also classify potentially NSFW content with a likelihood probability. This probability was reinterpreted as a rating of the image, and images would be scored according to this rating. Images that scored higher would be then ranked on a leader board. The implied scoring metric was that the most NSFW image would be at the top of the leader board. For example, in response to an image (excluded), botrotica rather astutely responded: “73.42% explicit nudity. 37.27% graphic male nudity. I see a beautiful face. I am 70.0% sure. You look CONFUSED and a little bit CALM. I would guess you are somewhere between 35 and 52. Looks like a 2 dressed up as a 8. . . I see people, person, human, tattoo. . . ” During the evening, physical advertising was employed to attract users. Business cards scattered around the venue were embossed with the phone number and the words. . . helloworld–an homage to the traditional computer program used to introduce novices to a new language or toolset. A stack of cards sat on a lonely table, behind which sat a single flower and a TV/VCR combination playing looped 1980’s advertisements for adult phone lines with this phone number dubbed over top (See Fig. 1. There was a pre-recorded message for those who called the associated number which prompted the human to leave a voice message and then send an SMS for a faster response. botrotica is a micro-service backend and Twilio-backed voice and text interface frontend. Written in Python (Flask), then Dockerized, it runs on an Amazon p2.xlarge EC2 server and hooks via HTTP requests to Amazon ML Rekognition image analysis API. A trained Tensorflow character-level recurrent neural network (RNN) model ran on the server, performing live inference upon receiving HTTP requests. The model used a 3 layer, 512 unit long-short term memory (LSTM) architecture. The model is trained on a large collection of erotic fiction scraped from the internet. The system also included several curated response lists. A set of 200 of the sexiest sexts curated by a domain expert, and additional pre-defined lists included 10 requests each for ‘dreams’, ‘secrets’, ‘pictures’, as these are some of the most common themes in sexting. Lines from these sources were queried by an information retrieval model based on n-gram similarity. Due to the sensitive nature of the data, it was deleted immediately following deployment with no post-processing. The goal of the system was not systematic comparison of interactions engagement metrics, but rather it served as an art installation designed to challenge the users. The system had respectable popularity and interaction over the 5.5 hour event handling 847 messages (731 SMS + 136 MMS), most interactions were longer than 4 turns. There were 64 minutes of recorded voice messages from 144 voice calls to the number. Interactions varied from innocent banter to exchanging explicit images and dirty talk, between human and agent. 3 Eroticism in the age of AI botrotica serves as an interesting window into the world of sext messaging—only a small piece of possible erotic interaction with artificial intelligence. It illuminated how popular simple chat interactions can be if prepared, produced, developed, and deployed appropriately and in interesting ways. This project inspires many future research directions, including photo scavenger hunts, collaborative photo classification and interactive live-streaming. Future interactions might involve rooms full of interlocutors (some human, some not) in collective communication by embracing an online chatroom dynamic. What we share online changes as new technology is introduced. How and what we share on one social channel is different from how we share on another. We often build channels through which we communicate with scripted, humorous, pseudoembodiments in video games and online chatbots. Perhaps these ‘pretend’ channels are not ‘obscene’ or as chatbot pioneer Weizenbaum believed [11]. This installation addresses the obscenity through transparency, and by granting explicit agency the user, hence making it not obscene. By openly communicating about ourselves, we become the media and the message. The content and the way we share information differs if it is a human or machine listening; virtual humans immediately offer two appealing features: anonymity and rapport [8]. How we share our lusts, thoughts, fears, desires, emotions, judgments, interests, hopes, dreams and memories makes us who we are. Hopefully, art-project-cum-coding-experiments like this continue to challenge our own identities and how we present ourselves online.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aabd534d4acdae625d1e226e23a2011c24b48d44,https://www.semanticscholar.org/paper/aabd534d4acdae625d1e226e23a2011c24b48d44,Thesis Proposal: Automated Testing of Robotic and Cyberphysical Systems,"Robotics and cyberphysical systems are increasingly being deployed to settings where they are in frequent interaction with the public. Therefore, failures in these systems can be catastrophic by putting human lives in danger and causing extreme financial loss. Large-scale assessment of the quality of these systems before deployment can prevent these costly damages. Because of the complexity and other special features of these systems, testing, and more specifically automated testing, faces challenges. In this thesis proposal, I study the unique challenges of testing robotics and cyberphysical systems by conducting a number of qualitative, quantitative, and mixed method studies, and propose an end-to-end automated testing pipeline to provide tools and methods that can help roboticists in large-scale automated testing of their systems. My key insight is that we can use (low-fidelity) simulation to automatically test robotic and cyberphysical systems, and identify many potentially catastrophic failures in advance at low cost. My thesis statement is: Robotics and cyberphysical systems have unique features such as interacting with the physical world and integrating hardware and software components, which creates challenges for automated, large-scale testing approaches. Software-in-the-loop (lowfidelity) simulation can facilitate automated testing for these systems. Machine learning approaches (e.g., clustering) can be used to create an automated testing pipeline, which includes automated oracles and automated test input generation. To support this statement, I propose the following work. In the preliminary work, which is already completed, I conducted a qualitative study and interviewed robotics practitioners about their testing practices and challenges. I identified nine main challenges roboticists face while testing their systems. In a case study on ARDUPILOT autonomous vehicle software, I investigated the potential impact of using low-fidelity software-based simulation on exposing failures in robotics systems, and showed that low-fidelity simulation can be an effective approach in detecting bugs and errors with low cost in robotic systems. I propose to further study features in robotics simulators that are the most important for automated testing, and the challenges of using these simulators by conducting a large-scale survey with robotics practitioners. I propose an approach to automatically generate oracles for cyberphysical systems using clustering, which can observe and identify common patterns of system behavior. These patterns can be used to distinguish erroneous behavior of the system and act as an oracle for an automated testing pipeline. Finally, I propose to investigate automated test generation for these systems by first, identifying a suitable quality metric to evaluate the quality of test suites, and second, automatically generating test suites that target under-tested behaviors.",,2020.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f946093c3c28db88fb18cf60f24567b1e62ca714,https://www.semanticscholar.org/paper/f946093c3c28db88fb18cf60f24567b1e62ca714,Genetic Programming Based on Novelty Search,"Novelty Search (NS) is a unique approach towards search and optimization, 
where an explicit objective function is replaced by a measure 
of solution novelty. However, NS has been mostly used in evolutionary 
robotics, its usefulness in classic machine learning problems has been 
unexplored. This thesis presents a NS-based Genetic Programming 
(GP) algorithms for common machine learning problems, with the following 
contributions. It is shown that NS can solve real-world classification, 
clustering and symbolic regression tasks, validated on realworld 
benchmarks and synthetic problems. These results are made 
possible by using a domain-specific behavior descriptor, related to the 
concept of semantics in GP. Moreover, two new versions of the NS algorithm 
are proposed, Probabilistic NS (PNS) and a variant of Minimal 
Criteria NS (MCNS). The former models the behavior of each solution 
as a random vector and eliminates all the NS parameters while reducing 
the computational overhead of the NS algorithm; the latter uses a 
standard objective function to constrain and bias the search towards 
high performance solutions. The thesis also discusses the effects of NS 
on GP search dynamics and code growth. Results show that NS can be 
used as a realistic alternative for machine learning, and particularly for 
GP-based classification.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a07b2e2eda3034512a3a78c455c84f5d181a7a62,https://www.semanticscholar.org/paper/a07b2e2eda3034512a3a78c455c84f5d181a7a62,An Affective Computing Multilayer Cognitive Architecture for Evolutionary Cognitive Robotics,"Robots are entering our daily lives from self-driving cars to health-care robots. Historically, pre-programmed robots were vulnerable to changing conditions in daily life, primarily because of a lack of ability to generate novel, non-preset flexible solutions. Thus there is a need for robotics to incorporate adaptation, which is a trait of higher order natural species. This adaptation allows higher-order natural species to change their behaviours and internal mechanisms based on experience with often dynamic environment. The ability to adapt emerged through evolutionary processes. Evolutionary Robotics is an approach to create autonomous robots that are capable of automatically generating artificial behaviors and morphologies to achieve adaptation. Evolutionary robotics has the potential to automatically synthesize controllers for real autonomous robots and generate solutions to complete tasks in the uncertain real-world. Compared to the inflexibility of pre-programmed robots, evolutionary robots are able to learn flexible solutions to given tasks through evolutionary methods.  Cognitive robotics, a branch of artificial cognitive systems research, is such an attempt to create autonomous robots by applying bio-inspired methods. As the robot interacts with environment, an underlying cognitive system can learn its own solutions toward task completion. This learning-solution-from-interaction approach, also termed as a Reinforcement Learning (RL) approach, is widely applied in cognitive robotics to learn the solutions automatically. Ideally, the solutions can emerge in the cognitive system through the trial-and-error process of the RL approach without introducing human bias.  This thesis aims to develop an evolutionary cognitive architecture (system) for a robot that can learn adaptive solutions to complete tasks. Inspired by emotion theories, this work proposes Affective Computing Multilayer Cognitive Architecture (ACMCA), a universal cognitive architecture, which is able to learn diverse solutions. Extending from previous work, ACMCA has a five-layer structure, where each layer aims to achieve different components of the solutions. The position of this thesis is that introducing a novel emotion inspired multilayer architecture that produces task solutions through subsumption operations and underlying appropriate machine learning algorithms will allow a robot to complete admissible tasks.  ACMCA’s five layers are: primary reinforcer layer, secondary reinforcer layer, core affect state layer, strategy layer, and behaviour layer. This five-layer decomposition also meets the traditional decomposition of a mobile control system into functional modules (e.g. perception, modelling, planning, task execution, and motor control). Each layer contains computing nodes as functional modules that process various Stimuli, Actions, and their consequential Outcomes of the cognitive system. In this work, 17 computing nodes and their connections in ACMCA represent the solutions that a mobile robot has learned to complete navigation tasks in complex scenarios.  Inspired by the Constructive Theory 1 and the robotic subsumption system, this work proposes a contingency-based subsumption approach to construct ACMCA. This contingency is termed Stimuli-Action-Outcome Contingency (SAOC), which is extended from the Action-Outcome (AO) contingency of Construction Theory. SAOCs are represented by “if-then” rules, termed SAOC rules, which encapsulate Stimuli, Actions, and their consequential Outcomes, providing clear symbolic interpretations. That is, the symbolic meaning of a SAOC rule can be interpreted as: if the input stimulus is perceived, the output action will be advocated as a cognitive response, expecting the outcome of the action with an estimation of relevance. As low-level computing nodes encapsulate Stimulus, Actions, and Outcomes, high-level computing nodes can subsume these low-level ones through the form of SAOC rules. Therefore, the proposed ACMCA can be constructed by subsumption layers of Stimuli-Action-Outcome Contingency (SAOC) rules.  This work applies machine learning techniques to facilitate ACMCA’s real-world robotic implementation. This work selects Accuracy-based Learning Classifier Systems (XCS) algorithms as the underlying machine learning techniques that are deployed at computing nodes for the contingency-based subsumption operations. The mitosis approach of XCS and the XCS with a Combined Reward method (XCSCR) are two novel variants of XCS algorithm. They are proposed to amend two challenges that occur when the standard XCS approaches are applied for robotic applications. The mitosis approach introduces an accuracy pressure into the algorithm’s evolutionary process, improving the algorithms’ performance in robotic applications where noisy interferences exist. The XCSCR enables the policy to emerge earlier and more frequently than the existing benchmark approaches in multistep problems. Therefore, a robot with the XCSCR can handle a multistep scenario more effectively than those with the benchmarked algorithms.  This work conducts five experiments to test the capability of ACMCA and its underlying algorithms in learning solutions for robotic navigation tasks. The five experiments are conducted as follows: reflex-learning, IR-tuning, deliberation-establishing, emotion model, and combined reward assignment. As the results of the experiments, three different affective patterns have emerged in the first three experiments, an emotion model has emerged in the fourth experiments, and the fifth experiment explores ACMCA’s potential implementation in the life-long learning scenario.  These results demonstrate that ACMCA, a novel emotion inspired multilayer architecture, can produce task solutions through contingency-based subsumption operations and underlying appropriate machine learning algorithms, allowing a robot to complete admissible tasks through evolutionary processes. The contingency-based subsumption operations can establish three contingencies and one emotion model between the subsumed components by multiple RL agents which deploy the proposed mitosis approach of XCS algorithms. These three emotion patterns and emotion model can consistently improve the robot’s navigation performance with interpretable explanations. These two variants of XCS algorithms can amend shortfalls of the standard XCS approach in real-world robotic implementations. It has been demonstrated that the diverse solutions learned by ACMCA improve the navigation performance of the robot in terms of higher flexibility, reduction in continuous collisions and shorter navigation time consumption.",,,10.26686/wgtn.17148398.v1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4d9742c88c303b4437c930530bf89c47b2814368,https://www.semanticscholar.org/paper/4d9742c88c303b4437c930530bf89c47b2814368,Computational Limitations of Stochastic Universal Turing Machine,"The deployment of online algorithms is a natural problem. In this work, we argue the construction of reinforcement learning, which embodies the confusing principles of artificial intelligence. We verify that while robots can be made interposable, read-write, and omniscient , public-private key pairs can be made highly-available, symbiotic, and peer-to-peer.",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
47c29c305d0ec04a8c8222b09141244ad759cc49,https://www.semanticscholar.org/paper/47c29c305d0ec04a8c8222b09141244ad759cc49,Lifelong Learning : A Reinforcement Learning Approach,"s (9): Abstract 3: Efficient deep neural networks for perception in autonomous driving (Jose M. Alvarez, TRI) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:00 AM3: Efficient deep neural networks for perception in autonomous driving (Jose M. Alvarez, TRI) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:00 AM Abstract Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time.Convolutional neural networks have achieved impressive success in many tasks in computer vision such as image classification, object detection / recognition or semantic segmentation. While these networks have proven effective in all these applications, they come at a high memory and computational cost, thus not feasible for applications where power and computational resources are limited. In addition, the process to train the network reduces productivity as it not only requires large computer servers but also takes a significant amount of time (several weeks) with the additional cost of engineering the architecture. In this talk, I first introduce our efficient architecture based on filter-compositions and then, a novel approach to jointly learn the architecture and explicitly account for compression during the training process. Our results show that we can learn much more compact models and significantly reduce training and inference time. Bio: Dr. Jose Alvarez is a senior research scientist at Toyota Research Institute. His main research interests are in developing robust and efficient deep learning algorithms for perception with focus on autonomous vehicles. Previously, he was a researcher at Data61 / CSIRO (formerly NICTA), a Postdoctoral researcher at the Courant Institute of Mathematical Science, New York University, and visiting scholar at University of Amsterdam and Group Research Electronics at Volkswagen. Dr. Alvarez graduated in 2012 and he was awarded the best Ph.D. Thesis award. Dr. Alvarez serves as associate editor for IEEE Trans. on Intelligent Transportation Systems. Abstract 4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM4: Visual 3D Scene Understanding and Prediction for ADAS (Manmohan Chandraker, NEC Labs) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 09:30 AM Abstract: Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. Modern advanced driver assistance systems (ADAS) rely on a range of sensors including radar, ultrasound, LIDAR and cameras. Active sensors have found applications in detecting traffic participants (TPs) such as cars or pedestrians and scene elements (SEs) such as roads. However, camera-based systems have the potential to achieve or augment these capabilities at a much lower cost, while allowing new ones such as determination of TP and SE semantics as well as their interactions in complex traffic scenes. In this talk, we present several technical advances for vision-based ADAS. A common theme is to overcome the challenges posed by lack of large-scale annotations in deep learning frameworks. We introduce approaches to correspondence estimation that are trained on purely synthetic data but adapt well to real data at test-time. We introduce object detectors that are light enough for ADAS, trained with knowledge distillation to retain accuracies of deeper architectures. Our semantic segmentation methods are trained on weak supervision that requires only a tenth of conventional annotation time. We propose methods for 3D reconstruction that use deep supervision to recover fine TP part locations while relying on purely synthetic 3D CAD models. We develop deep learning frameworks for multi-target tracking, as well as occlusion-reasoning in TP localization and SE layout estimation. Finally, we present a framework for TP behavior prediction in complex traffic scenes that accounts for TP-TP and TP-SE interactions. Our approach allows prediction of diverse multimodal outcomes and aims to account for long-term strategic behaviors in complex scenes. ICML 2017 Workshop book Generated Mon Aug 13, 2018 Page 4 of 20 Bio: Manmohan Chandraker is an assistant professor at the CSE department of the University of California, San Diego and leads the computer vision research effort at NEC Labs America in Cupertino. He received a B.Tech. in Electrical Engineering at the Indian Institute of Technology, Bombay and a PhD in Computer Science at the University of California, San Diego. His personal research interests are 3D scene understanding and reconstruction, with applications to autonomous driving and human-computer interfaces. His works have received the Marr Prize Honorable Mention for Best Paper at ICCV 2007, the 2009 CSE Dissertation Award for Best Thesis at UCSD, a PAMI special issue on best papers of CVPR 2011 and the Best Paper Award at CVPR 2014. Abstract 6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM6: 2 x 15 Contributed Talks on Datasets and Occupancy Maps in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 10:30 AM Jonathan Binas, Daniel Neil, Shih-Chii Liu, Tobi Delbruck, DDD17: End-To-End DAVIS Driving Dataset Ransalu Senanayake and Fabio Ramos, Bayesian Hilbert Maps for Continuous Occupancy Mapping in Dynamic Environments Abstract 7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM7: Beyond Hand Labeling: Simulation and Self-Supervision for Self-Driving Cars (Matt Johnson, University of Michigan) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:00 AM Self-driving cars now deliver vast amounts of sensor data from large unstructured environments. In attempting to process and interpret this data there are many unique challenges in bridging the gap between prerecorded data sets and the field. This talk will present recent work addressing the application of deep learning techniques to robotic perception. We focus on solutions to several pervasive problems when attempting to deploy such techniques on fielded robotic systems. The themes of the talk revolve around alternatives to gathering and curating data sets for training. Are there ways of avoiding the labor-intensive human labeling required for supervised learning? These questions give rise to several lines of research based around self-supervision, adversarial learning, and simulation. We will show how these approaches applied to self-driving car problems have great potential to change the way we train, test, and validate machine learning-based systems. Bio: Matthew Johnson-Roberson is Assistant Professor of Engineering in the Department of Naval Architecture & Marine Engineering and the Department of Electrical Engineering and Computer Science at the University of Michigan. He received a PhD from the University of Sydney in 2010. He has held prior postdoctoral appointments with the Centre for Autonomous Systems CAS at KTH Royal Institute of Technology in Stockholm and the Australian Centre for Field Robotics at the University of Sydney. He is a recipient of the NSF CAREER award (2015). He has worked in robotic perception since the first DARPA grand challenge and his group focuses on enabling robots to better see and understand their environment. Abstract 8: Learning Affordance for Autonomous Driving (JianXiong Xiao, AutoX) in ICML Workshop on Machine Learning for Autonomous Vehicles 2017, 11:30 AM Today, there are two major paradigms for vision-based autonomous driving systems: mediated perception approaches that parse an entire scene to make a driving decision, and behavior reflex approaches that directly map an input image to a driving action by a regressor. In this paper, we propose a third paradigm: a direct perception based approach to estimate the affordance for driving. We propose to map an input image to a small number of key perception indicators that directly relate to the affordance of a road/traffic state for driving. Our representation provides a set of compact yet complete descriptions of the scene to enable a simple controller to drive autonomously. Falling in between the two extremes of mediated perception and behavior reflex, we argue that our direct perception representation provides the right level of abstraction. We evaluate our approach in a virtual racing game as well as real world driving and show that our model can work well to drive a car in a very diverse set of virtual and ",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
480a28e01cbb5526b4260ffddb1301ae4f4e8890,https://www.semanticscholar.org/paper/480a28e01cbb5526b4260ffddb1301ae4f4e8890,ZigBee based wireless indoor localization with sensor placement optimization towards practical home sensing*,"Indoor localization with wireless communication technologies is gaining popularity among the ubiquitous computing and intelligent robotics communities. Thanks to the rapid growth in statistical machine learning research, indoor localization methods are being actively pursued with accelerated localization accuracy. However, cost in terms of budget constraints with respect to the number of installed sensors should be considered towards practical home sensing, however few studies have addressed this issue. We propose an installation scenario for practical home sensing applications and a flexible procedure for it with optimal sensor selection algorithms. With this scenario, we assume that a large number of ZigBee devices are temporarily deployed to the home in the initial status, then redundant sensors are efficiently taken off via optimal sensor selection. Thanks to a wide variety of options to formulate sensor selection, we describe an empirical evaluation with various formulations. Specifically, we used prototype ZigBee-based wireless sensor systems in an actual residential environment spread over then evaluated the trade-offs between accuracy and the number of sensors. The empirical results suggest that the backward greedy sensor selection algorithm achieves the most stable performance and that a few selected sensors exhibited competitive performance compared to the initial large-number setting.",Adv. Robotics,2016.0,10.1080/01691864.2015.1132636,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
48932c71393b9a24df14f7cfaaca3743040d23c3,https://www.semanticscholar.org/paper/48932c71393b9a24df14f7cfaaca3743040d23c3,Designing and sharing activity recognition systems across platforms,"Features Concept Matching Self-Calibration, Self-Learning Figure 7. Representation of the level at which a common representation is assumed to share a recognition system between users (platforms) or domains. 90 • IEEE ROBOTICS & AUTOMATION MAGAZINE • JUNE 2011 A hybrid approach between sensor-level and feature-level sharing was further proposed by Kunze et al., who demonstrated that sensors can autonomously self-characterize their on-body placement [55] and orientation [56] using machine-learning techniques. They propose to use onbody sensor placement self-characterization as a way to select, among a number of preprogrammed ARCs, the one most suited for the detected sensor placement. Similarly in robotics, data from different sensors can be converted into identical abstract representations. For instance, 3-D point clouds can be measured by stereovision or a laser-range finder. Classifier-Level Sharing Transfer learning allows to translate a classification problem from one feature space to another [57] and was used to transfer perceptual categories across modalities in biological and artificial systems [58]. Conceptually, transfer learning may thus be used to translate the capability to recognize activities from one platform to another without enforcing a similar input space (i.e., sensors and features). Thus, the transfer does not affect higher-level reasoning. Practical principles allowing a system A to confer activity-recognition capabilities to another system B are outlined in [19]. Each system A and B is composed of a set of sensors SA, SB, ARCs ARCA, ARCB, and a unified communication protocol. The process of transfer learning works as follows (see Figure 8). l The user employs an activity-aware system A with ARCA and sensor set SA. For instance, a set of instrumented drawers is capable of reporting which one is being opened or closed in a storage-management scenario. l A new system is deployed in the user’s personal area network comprising a set of unknown new sensors SB (on body and/or in the user’s surroundings) and an untrained ARCB. For instance, the user wears a new sensorized wristband with an integrated acceleration sensor. l As the user performs activities, the ARCA recognizes them and broadcasts this information. l The new system B receives the class labels of the recognized activities. The ARCB incrementally learns the mapping between the signals of the sensor set SB and the activity classes. l Eventually, the systemA can be removed. The activity-recognition capability is now entirely provided by the system B. The underlying assumptions are the two systems that coexist for a longer time to operate transfer learning. In Figure 8, we show that, as the user interacts with a set of drawers, the body-worn system incrementally learns to recognize opening and closing gestures. In robotics, this sharing approach may be used to allow the robots with different sensory inputs to learn to recognize semantically identical activities or to learn how to use a new sensor when the robot parts are upgraded, thus easing programming. Symbolic-Level Sharing The reasoning program to infer higher level activities from spotted action primitives is shared between platforms. As the environment in which the two platforms operate may lead to the detection of semantically different action primitives, a direct transfer of the reasoning is not always possible. Carrying out a prior concept matching can address this. For instance, to reason about the activity of a user, one needs first to know in which room he is located. One environment may have a sensor allowing to detect the action primitive “room door activated.” Another environment may have a proximity infrared sensor allowing to detect “movement in the room.” The interpretation of the sensor data requires different features and classifiers in each case. However, although the classifiers deliver semantically different action primitives, Recognized Activities from Drawer Sensors Recognized Activities from On-Body Sensors",,2011.0,10.3929/ETHZ-A-010001702,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cbd1456f230de7a4e8e78690ea36ac4d6eeee688,https://www.semanticscholar.org/paper/cbd1456f230de7a4e8e78690ea36ac4d6eeee688,A Rapid Situational Awareness Development Framework for Heterogeneous Manned-Unmanned Teams,"This paper presents a robust framework for configuring and deploying a heterogeneous team of smart unmanned systems and human agents in dynamic and un-modeled environments to rapidly build mission critical situational awareness with selective details of potential areas of interest, especially focusing on minimized cognitive loading of the human agents. Five key components, namely control, communication, artificial intelligence (AI), platform, and visualization, merge seamlessly into a holistic framework to deliver this rapid situational awareness development capability to the heterogeneous manned unmanned team (MUM-T). In this framework, the overall control is seen as a combination of agent level control and mission level control. A common software, Robot Operating System (ROS), is used to establish communication, and consequently consensus, among the heterogeneous swarm of unmanned systems. These unmanned platforms are customized with co-processing hardware that can execute advanced artificial intelligence machine learning (AI/ML) modules to not only deliver stable and cooperative performance of these unmanned platforms in the swarm but also support human-centric human robot interaction (HRI). Finally, to reduce the cognitive burden on the human agents, a triaged visualization scheme, enabled through mixed reality (MR) technology, is implemented. This paper presents a preliminary proof of concept study for the presented hybrid map (i.e. 2D mapping with 3D detailing) construction framework, tested with a heterogeneous swarm of unmanned aerial vehicles (UAVs) of varying capabilities, teamed with a human operator.",NAECON 2018 - IEEE National Aerospace and Electronics Conference,2018.0,10.1109/NAECON.2018.8556769,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
10130e49a32568e667e9cc1afbfbfefde2d73c77,https://www.semanticscholar.org/paper/10130e49a32568e667e9cc1afbfbfefde2d73c77,Data Transfer via UAV Swarm Behaviours,"This paper presents an adaptive robotic swarm of Unmanned Aerial Vehicles (UAVs) enabling communications between separated non-swarm devices. The swarm nodes utilise machine learning and hyper-heuristic rule evolution to enable each swarm member to act appropriately for the given environment. The contribution of the machine learning is verified with an exploration of swarms with and without this module. The exploration finds that in challenging environments the learning greatly improves the swarm’s ability to complete the task. 
The swarm evolution process of this study is found to successfully create different data transfer methods depending on the separation of non-swarm devices and the communication range of the swarm members. This paper also explores the resilience of the swarm to agent loss, and the scalability of the swarm in a range of environment sizes. In regard to resilience, the swarm is capable of recovering from agent loss and is found to have improved evolution. In regard to scalability, the swarm is observed to have no upper limit to the number of agents deployed in an environment. However, the size of the environment is seen to be a limit for optimal swarm performance.",Journal of Telecommunications and the Digital Economy,2018.0,10.18080/AJTDE.V6N2.142,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8cf4d2367d41c8349eeee93c4aac46a10b43faae,https://www.semanticscholar.org/paper/8cf4d2367d41c8349eeee93c4aac46a10b43faae,Enabling intelligent battlefield healthcare through secure cyber medicine,"Providing advanced healthcare to the warfighters in the battlefield has proven challenging due to the difficult environmental conditions and geographical separation between the injured warrior and healthcare providers. The introduction of Cyber Medicine enables physicians and soldiers to utilize technologies including mobile apps, robotics, connectivity to wired/wireless networks, satellites, clouds, HPCs, and software to identify, assess and treat the injured. However, what happens when the adversary attacks the cyber domain in addition to the physical domain? The answer could be that all automated systems will become suspect whether they are embedded systems, information processing systems, diagnosis and triage systems, or remote surgical robotics. This analysis identifies areas where the intelligent Cyber-Medical System can provide better healthcare to the battlefield including services to the disadvantaged soldier at the edge. Architecting intelligent systems starts with learning the desired system operation, sifting through historical data and procedure outcomes, assessing vulnerabilities, then delivering systems that mimics and augment human performance to solve problems. The battlefield Cybermedicine has the challenge of increased cyber security risks due to the need to be deployed in hostile environment, and the challenge of dealing with injuries that are unique to the battlefield. Moreover, the viability of intelligent automation depends on reliable connectivity and availability of reliable data and infrastructure, while the battlefield lacks both those conditions. The goal of this study is to deliver medical services and alleviate the vulnerability impacts through more secure design, development, deployment, maintenance, and operations. Additionally, this paper introduces new cybermedicine concepts and architectures that benefit from the various types of Machine Learning and Artificial Intelligence to build Cyber-Medical System that can resist corruption from unauthenticated users/data, and from active malware and physical media attacks.",Defense + Commercial Sensing,2019.0,10.1117/12.2516021,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a511a986af07804261d3b35badf764c622e3f502,https://www.semanticscholar.org/paper/a511a986af07804261d3b35badf764c622e3f502,Artificial Intelligence: A European Perspective,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.",,2018.0,10.2760/11251,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e340e84f3337420dc1150739165c37e364ffe448,https://www.semanticscholar.org/paper/e340e84f3337420dc1150739165c37e364ffe448,Benchmarking Incremental Regressors in Traversal Cost Assessment,,ICANN,2019.0,10.1007/978-3-030-30487-4_52,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a812368fe1d4a186322bf72a6d07e1cf60067234,https://www.semanticscholar.org/paper/a812368fe1d4a186322bf72a6d07e1cf60067234,Gaussian processes for modeling of facial expressions,"Automated analysis of facial expressions has been gaining significant attention over the past years. This stems from the fact that it constitutes the primal step toward developing some of the next-generation computer technologies that can make an impact in many domains, ranging from medical imaging and health assessment to marketing and education. No matter the target application, the need to deploy systems under demanding, realworld conditions that can generalize well across the population is urgent. Hence, careful consideration of numerous factors has to be taken prior to designing such a system. The work presented in this thesis focuses on tackling two important problems in automated analysis of facial expressions: (i) view-invariant facial expression analysis; (ii) modeling of the structural patterns in the face, in terms of well coordinated facial muscle movements. Driven by the necessity for efficient and accurate inference mechanisms we explore machine learning techniques based on the probabilistic framework of Gaussian processes (GPs). Our ultimate goal is to design powerful models that can efficiently handle imagery with spontaneously displayed facial expressions, and explain in detail the complex configurations behind the human face in real-world situations. To effectively decouple the head pose and expression in the presence of large outof-plane head rotations we introduce a manifold learning approach based on multi-view learning strategies. Contrary to the majority of existing methods that typically treat the numerous poses as individual problems, in this model we first learn a discriminative manifold shared by multiple views of a facial expression. Subsequently, we perform facial expression classification in the expression manifold. Hence, the pose normalization problem is solved by aligning the facial expressions from different poses in a common latent space. We demonstrate that the recovered manifold can efficiently generalize to various poses and expressions even from a small amount of training data, while also being largely robust to corrupted image features due to illumination variations. State-of-the-art performance is achieved in the task of facial expression classification of basic emotions. The methods that we propose for learning the structure in the configuration of the muscle movements represent some of the first attempts in the field of analysis and intensity estimation of facial expressions. In these models, we extend our multi-view approach to exploit relationships not only in the input features but also in the multi-output labels. The structure of the outputs is imposed into the recovered manifold either from heuristically defined hard constraints, or in an auto-encoded manner, where the structure is learned automatically from the input data. The resulting models are proven to be robust to data with imbalanced expression categories, due to our proposed Bayesian learning of the target manifold. We also propose a novel regression approach based on product of GP experts where we take into account people’s individual expressiveness in order to adapt the learned models on each subject. We demonstrate the superior performance of our proposed models on the task of facial expression recognition and intensity estimation.",,2016.0,10.25560/44106,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ce73965da7b88aead853694dbabce85c2d04f463,https://www.semanticscholar.org/paper/ce73965da7b88aead853694dbabce85c2d04f463,Goal-Aware Team Affiliation in Collectives of Autonomous Robots,"Collaboration in teams is essential in robot collectives. In order to achieve goals, individual robots would otherwise not be able to accomplish. In a such a distributed and highly dynamic system, a global coordination might not be possible. In this paper, we analyse static team affiliations, defined at deployment time, and compare its efficiency against dynamic team affiliations generated during runtime using random selection. Since operators might not be able to determine all dynamic aspects of the given environment at the time of deployment, we further propose a novel, goal-aware approach to affiliate each robot with a team. This approach brings together insights from biology, sociology, and psychology. In this novel approach, robots only operate on aggregated information from the network which is potentially changing during runtime. Finally, we also introduce an approach to select a team affiliation during runtime using machine learning techniques. Using 60,000 randomised scenarios, we analyse the efficiency and further discuss the different benefits and drawbacks of the proposed approaches.",2018 IEEE 12th International Conference on Self-Adaptive and Self-Organizing Systems (SASO),2018.0,10.1109/SASO.2018.00020,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ab9f7751796988bd2de67e25de790d06f8671dcb,https://www.semanticscholar.org/paper/ab9f7751796988bd2de67e25de790d06f8671dcb,Learning Rough-Terrain Autonomous Navigation,"Autonomous navigation by a mobile robot through natural, unstructured terrain is one of the premier challenges in field robotics. The DARPA UPI program was tasked with advancing the state of the art in robust autonomous performance through challenging and widely varying environments. In order to accomplish this goal, machine learning techniques were heavily utilized to provide robust and adaptive performance, while simultaneously reducing the required development and deployment time. This paper describes the autonomous system, Crusher, developed for the UPI program, and the learning approaches that aided in its successful performance.",,2009.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bdd3cff96265ebd3786f963e51c1524bc95bfa1c,https://www.semanticscholar.org/paper/bdd3cff96265ebd3786f963e51c1524bc95bfa1c,Multimodal Information Fusion for Automatic Aesthetics Evaluation of Robotic Dance Poses,,Int. J. Soc. Robotics,2020.0,10.1007/S12369-019-00535-W,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cc68678c2ef7fab664e6a596533d1889a8eb2dfd,https://www.semanticscholar.org/paper/cc68678c2ef7fab664e6a596533d1889a8eb2dfd,Your System Gets Better Every Day You Use It: Towards Automated Continuous Experimentation,"Innovation and optimization in software systems can occur from pre-development to post-deployment stages. Companies are increasingly reporting the use of experiments with customers in their systems in the post-deployment stage. Experiments with customers and users are can lead to a significant learning and return-on-investment. Experiments are used for both validation of manual hypothesis testing and feature optimization, linked to business goals. Automated experimentation refers to having the system controlling and running the experiments, opposed to having the R&D organization in control. Currently, there are no systematic approaches that combine manual hypothesis validation and optimization in automated experiments. This paper presents concepts related to automated experimentation, as controlled experiments, machine learning and software architectures for adaptation. However, this paper focuses on how architectural aspects that can contribute to support automated experimentation. A case study using an autonomous system is used to demonstrate the developed initial architecture framework. The contributions of this paper are threefold. First, it identifies software architecture qualities to support automated experimentation. Second, it develops an initial architecture framework that supports automated experiments and validates the framework with an autonomous mobile robot. Third, it identifies key research challenges that need to be addressed to support further development of automated experimentation.",2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA),2017.0,10.1109/SEAA.2017.15,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6b77b4235bd455cf6df68cfa4fe4e2aa9401d06c,https://www.semanticscholar.org/paper/6b77b4235bd455cf6df68cfa4fe4e2aa9401d06c,Ethics in artificial intelligence: introduction to the special issue,,Ethics and Information Technology,2018.0,10.1007/s10676-018-9450-z,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3a14c8f03d794eb62153c69892e0de3a98b45c9c,https://www.semanticscholar.org/paper/3a14c8f03d794eb62153c69892e0de3a98b45c9c,"Clustering social cues to determine social signals: developing learning algorithms using the ""n-most likely states"" approach","Human-robot teaming largely relies on the ability of machines to respond and relate to human social signals. Prior work in Social Signal Processing has drawn a distinction between social cues (discrete, observable features) and social signals (underlying meaning). For machines to attribute meaning to behavior, they must first understand some probabilistic relationship between the cues presented and the signal conveyed. Using data derived from a study in which participants identified a set of salient social signals in a simulated scenario and indicated the cues related to the perceived signals, we detail a learning algorithm, which clusters social cue observations and defines an ""N-Most Likely States"" set for each cluster. Since multiple signals may be co-present in a given simulation and a set of social cues often maps to multiple social signals, the ""N-Most Likely States"" approach provides a dramatic improvement over typical linear classifiers. We find that the target social signal appears in a ""3 most-likely signals"" set with up to 85% probability. This results in increased speed and accuracy on large amounts of data, which is critical for modeling social cognition mechanisms in robots to facilitate more natural human-robot interaction. These results also demonstrate the utility of such an approach in deployed scenarios where robots need to communicate with human teammates quickly and efficiently. In this paper, we detail our algorithm, comparative results, and offer potential applications for robot social signal detection and machine-aided human social signal detection.",SPIE Defense + Security,2016.0,10.1117/12.2223900,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8379f1e6771a3ee61b5de8a4e2a7afbe3ad83a04,https://www.semanticscholar.org/paper/8379f1e6771a3ee61b5de8a4e2a7afbe3ad83a04,Samuel Barrett 1 / 4 RESEARCH STATEMENT,"My research focuses on investigating how robots and other agents should learn and cooperate in order to tackle realworld problems. Agents are entities that repeatedly interact with their environment in order to accomplish their goals. In order for robots and other agents to handle many real-world problems, they must be able to cooperate with other agents and humans. However, they may not always know these teammates ahead of time, so they will need to be able to learn and adapt to their teammates. For example, following a disaster, robots are brought in to perform search and rescue, and it is helpful for them to be able to cooperate with teammates from other companies and research laboratories as well as human responders. My research includes theoretical analysis of these problems, but an important component of my research is also to create behaviors for physical robots that exhibit these properties and test them in the real world. My areas of expertise are in machine learning, multi-agent systems, and robotics, especially focusing on reinforcement learning and transfer learning. Most existing approaches to multi-agent systems rely on all of the teammates sharing coordination and communication protocols, but these protocols are not always available. Therefore, my research focuses on how agents can learn to cooperate with a variety of teammates without pre-coordination, forming an ad hoc team . Ideally, agents that reason about ad hoc teamwork should be robust to both the teammates they may encounter as well as the tasks they must complete. My goal is to understand how agents can quickly adapt to a variety of teammates and real-world tasks. My thesis is the first to study how agents should behave on ad hoc teams. To tackle this problem, I created agents that learn models about how past teammates have acted and then use these models to cooperate with new teammates. This approach allows the agents to quickly adapt to their teammates over the course of a single episode. To improve on this approach, I additionally used transfer learning to create better models of the agent’s teammates after it observes its teammates for a short period of time. I plan on leveraging this research as well as my work on robot soccer to accomplish my long-term goal of creating robust agents that learn quickly.",,2013.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6cd905e2552e94a8994e52cda5dd1e45fd7f1a4c,https://www.semanticscholar.org/paper/6cd905e2552e94a8994e52cda5dd1e45fd7f1a4c,Let the machines out. towards hybrid social systems,"When Alan Turing proposed the imitation game as a method to investigate the question if machines can think, he described a social system. However, the various disciplines that have pursued this seminal enquiry rarely touch base with sociological concepts. Cybernetics developed into various interdisciplinary fields, yet it was mainly rooted in physiological models. In the meantime, the mainstream of AI focused on cognitive problem solving, predominately from a topdown approach. Traditional cognitive science rests on the concept of organisms as information processing systems so does Artificial Life, but from a biological simulation perspective. The recently revitalised branch of machine learning has been successful in deploying bottom-up models combined with large amounts of data. Large scale simulations of the brain are expected to deliver new knowledge about the human brain. ”Second-generation” cognitive science and developmental robotics are embodied and apply neural computation. One might be tempted to say that progress has been made on brains, bodies and on models of minds. I claim that there is something largely missing in this picture, which is the social aspect. There is Social AI, and it embraces a wide variety of topics and concerns from Stafford Beers cybernetic vision of society to simulations of interacting agents, complex systems theory, language, imitation and social learning, social network analysis and social bots, enactment, human-machine interaction, augmented and virtual environments, robot assisted therapy and behavioural game theory, to name a few. I also would like to include autonomous weapons, computer worms and viruses, in particular crypto-ransomware, into this context of social systems. From the other side, an interdisciplinary bridge is constructed under the label of digital sociology. The process of mutual approximation is accompanied by prolific discourses around machine ethics and emerging legal issues. A recently introduced topic of discussion is if robots should pay taxes. So AI observes sociology, and sociology observes AI yet they do not share a coherent theoretical program and fundamental ontological questions are still left to the philosophers. To propose an alternative route, I consider Niklas Luhmann’s theory of social systems as a suitable foundation for guiding the development of hybrid social systems. A hybrid social system is understood as a social assemblage in which minds and machines mingle: humans, machines, certain things, cyborgs. Some animals are welcome, too. To this end, I present a few selected features of Luhmann’s theory and briefly visit some of their theoretical foundations: distinctions, in particular the distinction between system and environment, autopoietic systems, radical constructivism, and (second order) cybernetics.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
73520f778e5d6033a05963a5309c248781d54a8f,https://www.semanticscholar.org/paper/73520f778e5d6033a05963a5309c248781d54a8f,Robots in Space: Sharing the Sidewalk with Autonomous Delivery Vehicles,"Industrial robots originated in mid-Twentieth Century factories where they increased the efficiency of manufacturing. Their implementation was an extension of earlier industrial automation such as the introduction of Henry Ford’s mechanized assembly line in 1913. In Ford’s assembly line, a rope-and-pully system advanced each vehicle from one worker to the next allowing each worker to remain stationary. Half a century later, in 1961, the first robotic arm, created by Unimate, was introduced to auto manufacturing, which further increased efficiency. More recently, following advancements in artificial intelligence and sensor technology, industrial robots have acquired greater autonomy and transformed the logistics and delivery industries. Like Ford’s assembly line, and Unimate’s robotic arm, Amazon’s fulfillment center robots, originally designed by Kiva Robotics, reduced the daily steps workers must take. Instead of walking through aisles to stock warehouse shelves or retrieve products for distribution, workers remain stationary, and the robots bring the products to them. Today, with even greater autonomy than their predecessors, robots are migrating out of factories, warehouses, and fulfillment centers and into neighborhood streets, sidewalks, and skies. The technological advancements that allowed robots to automate private industrial spaces, such as machine learning and sophisticated sensors, now enable autonomous delivery robots (ADVs) to travel independently in the outside world and deliver packages, meals, groceries, and other retail purchases to people’s homes.<br><br>This article focuses on the evolution of ADVs used for “last-mile delivery,” the final step of the delivery process that ends at the customer’s door. It breaks ADVs down into four different categories: unmanned aerial vehicles (UAVs or “drones”); self-driving cars; autonomous delivery pods; and sidewalk delivery robots, which are sometimes called personal delivery robots (PDRs). The article describes the risks and benefits of deploying ADVs for last-mile delivery and analyzes the laws and federal agencies that regulate them. Last mile delivery is generally thought to be “the most expensive and time-consuming part of the shipping process” because it is the most personalized and unpredictable. Industry estimates suggest that last-mile delivery can account for up to 53 percent of total shipping costs. ADV manufacturers claim they can reduce delivery time, increase efficiency, cut costs, improve the consumer experience, decrease traffic congestion, reduce carbon emissions, assist seniors and people with disabilities who may have decreased mobility, and democratize access to logistics and delivery resources for small businesses allowing them to compete with large corporations. Critics claim ADVs may negatively impact public health by encouraging inactivity, obstructing roads and sidewalks and impairing the mobility of seniors and people with disabilities, and endangering public safety due to their potential to collide with people who are not agile enough to get out of the way. ADVs may also reduce the need for human delivery workers, cause noise pollution, violate people’s privacy, and represent the increasing privatization of public spaces such as sidewalks. Though all ADVs will be discussed, my focus is primarily on sidewalk delivery robots because they are the newest and fastest growing segment of the ADV industry, and they face the fewest legal and regulatory hurdles. Particular attention will be paid to the differences between the laws that regulate sidewalk delivery robots and the laws that govern other types of ADVs. The article concludes by drawing lessons from the regulation of UAVs and self-driving cars to propose legislation to regulate sidewalk delivery robots that will increase their safety and utility while limiting the privatization of public spaces.",SSRN Electronic Journal,2019.0,10.2139/ssrn.3347466,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0c42c433e2b5b61028e9bb43c5e5f464feb3b140,https://www.semanticscholar.org/paper/0c42c433e2b5b61028e9bb43c5e5f464feb3b140,Intelligence Unleashed: An argument for AI in Education,"This paper on artificial intelligence in education (AIEd) has two aims. The first: to explain to a non-specialist, interested, reader what AIEd is: its goals, how it is built, and how it works. The second: to set out the argument for what AIEd can offer teaching and learning, both now and in the future, with an eye towards improving learning and life outcomes for all. Computer systems that are artificially intelligent interact with the world using capabilities (such as speech recognition) and intelligent behaviours (such as using available information to take the most sensible actions toward a stated goal) that we would think of as essentially human. At the heart of artificial intelligence in education is the scientific goal to make knowledge, which is often left implicit, computationally precise and explicit. In other words, in addition to being the engine behind much ‘smart’ ed tech, AIEd is also designed to be a powerful tool to open up what is sometimes called the ‘black box of learning,’ giving us more fine-grained understandings of how learning actually happens. Although some might find the concept of AIEd alienating, the algorithms and models that underpin ed tech powered by AIEd form the basis of an essentially human endeavor. Using AIEd, teachers will be able to offer learners educational experiences that are more personalised, flexible, inclusive and engaging. Crucially, we do not see a future in which AIEd replaces teachers. What we do see is a future in which the extraordinary expertise of teachers is better leveraged and augmented through the thoughtful deployment of well designed AIEd. We have available, right now, AIEd tools that could support student learning at a scale previously unimaginable by providing one-on-one tutoring to every student, in every subject. Existing technologies also have the capacity to provide intelligent support to learners working in a group, and to create authentic virtual learning environments where students have the right support, at the right time, to tackle real-life problems and puzzles. In the near future, we expect that teaching and learning will increasingly be supported by the thoughtful application of AIEd tools. For example, by lifelong learning companions powered by AI that can accompany and support individual learners throughout their studies - in and beyond school - and new forms of assessment that measure learning while it is taking place, shaping the learning experience in real time. If we are ultimately successful, we predict that AIEd will help us address some of the most intractable problems in education, including achievement gaps and teacher retention. AIEd will also help us respond to the most significant social challenge that AI has already brought - the steady replacement of jobs and occupations with clever algorithms and robots. It is our view that this provides a new innovation imperative in education, which can be expressed simply: as humans live and work alongside increasingly smart machines, our education systems will need to achieve at levels that none have managed to date. True progress will require the development of an AIEd infrastructure. This will not, however, be a single monolithic AIEd system. Instead, it will resemble the marketplace that has developed for smartphone apps: hundreds and then thousands of individual AIEd components, developed in collaboration with educators, conformed to uniform international data standards, and shared with researchers and developers worldwide. These standards will also enable system-level data collation and analysis that will help us to learn much more about learning itself – and how to improve it. Moving forward, we will need to pay close attention to three powerful forces as we map the future of artificial intelligence in education, namely pedagogy, technology, and system change. Paying attention to the pedagogy will mean that the design of new edtech should always start with what we know about learning. It also means that the system for funding this work must be simultaneously opened up and refocused, moving away from isolated pockets of R&D and toward collaborative enterprises that prioritise areas known to make a real difference to teaching and learning. Paying attention to the technology will mean creating smarter demand for commercial grade AIEd products that work. It also means the development of a robust, component-based AIEd infrastructure, similar to the smartphone app marketplace, where researchers and developers can access standardised components that have been developed in collaboration with educators. Paying attention to system change will mean involving teachers, students, and parents in co-designing new tools, so that AIEd will appropriately address the inherent “messiness” of real classroom, university, and workplace learning environments. It also means the development of data standards that promote the safe and ethical use of data. Said succinctly, we need intelligent technologies that embody what we know about great teaching and learning, embodied in enticing consumer grade products, which are then used effectively in real-life settings that combine the best of human and machine. We do not underestimate the new-thinking, inevitable wrong-turns, and effort required to realise these recommendations. However, if we are to properly unleash the intelligence of AIEd, we must do things differently - via new collaborations, sensible funding, and (always) a keen eye on the pedagogy. The potential prize is too great to act otherwise.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c53839618d04fc5526c7c137dd991e9b817b1bf8,https://www.semanticscholar.org/paper/c53839618d04fc5526c7c137dd991e9b817b1bf8,HAC-ER: A Disaster Response System based on Human-Agent Collectives,"Major natural or man-made disasters such as Hurricane Katrina or the 9/11 terror attacks pose significant challenges for emergency responders. First, they have to develop an understanding of the unfolding event either using their own resources or through third-parties such as the local population and agencies. Second, based on the information gathered, they need to deploy their teams in a flexible manner, ensuring that each team performs tasks in The most effective way. Third, given the dynamic nature of a disaster space, and the uncertainties involved in performing rescue missions, information about the disaster space and the actors within it needs to be managed to ensure that responders are always acting on up-to-date and trusted information. Against this background, this paper proposes a novel disaster response system called HAC-ER. Thus HAC-ER interweaves humans and agents, both robotic and software, in social relationships that augment their individual and collective capabilities. To design HAC-ER, we involved end-users including both experts and volunteers in a several participatory design workshops, lab studies, and field trials of increasingly advanced prototypes of individual components of HAC-ER as well as the overall system. This process generated a number of new quantitative and qualitative results but also raised a number of new research questions. HAC-ER thus demonstrates how such Human-Agent Collectives (HACs) can address key challenges in disaster response. Specifically, we show how HAC-ER utilises crowdsourcing combined with machine learning to obtain most important situational awareness from large streams of reports posted by members of the public and trusted organisations. We then show how this information can inform human-agent teams in coordinating multi-UAV deployments, as well as task planning for responders on the ground. Finally, HAC-ER incorporates an infrastructure and the associated intelligence for tracking and utilising the provenance of information shared across the entire system to ensure its accountability. We individually validate each of these elements of HAC-ER and show how they perform against standard (non-HAC) baselines and also elaborate on the evaluation of the overall system.
",AAMAS,2015.0,10.1613/JAIR.5098,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
59e10d1d4cd454635914cfd0ac5160a318fd0473,https://www.semanticscholar.org/paper/59e10d1d4cd454635914cfd0ac5160a318fd0473,UB09 Session 9,"In the domain of Wireless Sensor Networks (WSN), providing an effective security solution to protect the motes and their communications is challenging. Due to the hard constraints on performance, storage and energy consumption, normal network-security related techniques cannot be applied. Focusing on the ""Intrusion Detection"" problem, we propose a realworld application of our WSN Intrusion Detection System (WIDS). WIDS exploits the Weak Process Models to classify potential security issues in the WSN and to notify the operators when an attack tentative is detected. In this demonstration, we show how our IDS works, how it detects some basic attacks and how the IDS can evolve to fullfil the needs of secure WSN deployments. Download Paper (PDF) UB09.2 RESCUE: EDA TOOLSET FOR INTERDEPENDENT ASPECTS OF RELIABILITY, SECURITY AND QUALITY IN NANOELECTRONIC SYSTEMS DESIGN Authors: Cemil Cem Gürsoy1, Guilherme Cardoso Medeiros2, Junchao Chen3, Nevin George4, Josie Esteban Rodriguez Condia5, Thomas Lange6, Aleksa Damljanovic5, Raphael Segabinazzi Ferreira4, Aneesh Balakrishnan6, Xinhui Anna Lai1, Shayesteh Masoumian7, Dmytro Petryk3, Troya Cagil Koylu2, Felipe Augusto da Silva8, Ahmet Cagri Bagbaba8 and Maksim Jenihhin1 1Tallinn University of Technology, EE; 2Delft University of Technology, NL; 3IHP, DE; 4BTU Cottbus-Senftenberg, DE; 5Politecnico di Torino, IT; 6IROC Technologies, FR; 7Intrinsic ID B.V., NL; 8Cadence Design Systems GmbH, DE Abstract The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF)The demonstrator will introduce an EDA toolset developed by a team of PhD students in the H2020-MSCA-ITN RESCUE project. The recent trends for the computing systems include machine intelligence in the era of IoT, complex safety-critical applications, extreme miniaturization of technologies and intensive interaction with the physical world. These trends set tough requirements on mutually dependent extra-functional design aspects. RESCUE is focused on the key challenges for reliability (functional safety, ageing, soft errors), security (tamper-resistance, PUF technology, intelligent security) and quality (novel fault models, functional test, FMEA/FMECA, verification/debug) and related EDA methodologies. The objective of the interdisciplinary cross-sectoral team from Tallinn UT, TU Delft, BTU Cottbus, POLITO, IHP, IROC, Intrinsic-ID, Cadence and Bosch is to develop in collaboration a holistic EDA toolset for modelling, assessment and enhancement of these extra-functional design aspects. Download Paper (PDF) UB09.3 ASAM: AUTOMATIC SYNTHESIS OF ALGORITHMS ON MULTI CHIP/FPGA WITH COMMUNICATION CONSTRAINTS Authors: Amir Masoud Gharehbaghi, Tomohiro Maruoka, Yukio Miyasaka, Akihiro Goda, Amir Masoud Gharehbaghi and Masahiro Fujita, The University of Tokyo, JP Abstract Mapping of large systems/computations on multiple chips/multiple cores needs sophisticated compilation methods. In this demonstration, we present our compiler tools for multi-chip and multi-core systems that considers communication architecture and the related constraints for optimal mapping. Specifically, we demonstrate compilation methods for multi-chip connected with ring topology, and multi-core connected with mesh topology, assuming fine-grained reconfigurable cores, as well as generalization techniques for large problems size as convolutional neural networks. We will demonstrate our mappings methods starting from data-flow graphs (DFGs) and equations, specifically with applications to convolutional neural networks (CNNs) for convolution layers as well as fully connected layers. Download Paper (PDF) UB09.4 HEPSYCODE-MC: ELECTRONIC SYSTEM-LEVEL METHODOLOGY FOR HW/SW CO-DESIGN OF MIXED-CRITICALITY EMBEDDED SYSTEMS Authors: Luigi Pomante1, Vittoriano Muttillo1, Marco Santic1 and Emilio Incerto2 1Università degli Studi dell'Aquila DEWS, IT; 2IMT Lucca, IT Abstract Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF)Heterogeneous parallel architectures have been recently exploited for a wide range of embedded application domains. Embedded systems based on such kind of architectures can include different processor cores, memories, dedicated ICs and a set of connections among them. Moreover, especially in automotive and aerospace application domains, they are even more subjected to mixed-criticality constraints. So, this demo addresses the problem of the ESL HW/SW co-design of mixed-criticality embedded systems that exploit hypervisor (HPV) technologies. In particular, it shows an enhanced CSP/SystemC-based design space exploration step, in the context of an existing HW/SW co-design flow that, given the system specification is able to (semi)automatically propose to the designer: a custom heterogeneous parallel HPV-based architecture; an HW/SW partitioning of the application; a mapping of the partitioned entities onto the proposed architecture. Download Paper (PDF) UB09.5 CS: CRAZYSQUARE Authors: Federica Caruso1, Federica Caruso1, Tania Di Mascio1, Alessandro D'Errico1, Marco Pennese2, Luigi Pomante1, Claudia Rinaldi1 and Marco Santic1 1University of L'Aquila, IT; 2Ministry of Education, IT Abstract CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF)CrazySquare (CS) is an adaptive learning system, developed as a serious game for music education, specifically indicated for young teenager approaching music for the first time. CS is based on recent educative directions which consist of using a more direct approach to sound instead of the musical notation alone. It has been inspired by a paper-based procedure that is currently used in an Italian middle school. CS represents a support for such teachers who prefer involving their students in a playful dimension of learning rhythmic notation and pitch, and, at the same time, teaching playing a musical instrument. To reach such goals in a cost-effective way, CS fully exploits all the recent advances in the EDA domain. In fact, it is based on a framework composed of mobile applications that will be integrated with augmented reality HW/SW tools to provide virtual/augmented musical instruments. The proposed demo will show the main features of the current CS framework implementation. Download Paper (PDF) UB09.6 LABSMILING: A SAAS FRAMEWORK, COMPOSED OF A NUMBER OF REMOTELY ACCESSIBLE TESTBEDS AND RELATED SW TOOLS, FOR ANALYSIS, DESIGN AND MANAGEMENT OF LOW DATA-RATE WIRELESS PERSONAL AREA NETWORKS BASED ON IEEE 802.15.4 Authors: Carlo Centofanti, Luigi Pomante, Marco Santic and Walter Tiberti, University of L'Aquila, IT Abstract Low data-rate wireless personal area networks (LR-WPANs) are constantly increasing their presence in the fields of IoT, wearable, home automation, health monitoring. The development, deployment and testing of SW based on IEEE 802.15.4 standard (and derivations, e.g. 15.4e), require the exploitation of a testbed as the network grows in complexity and heterogeneity. This demo shows LabSmiling: a SaaS framework which connects testbeds deployed in a real-world-environment and the related SW tools that make available a meaningful (but still scalable) number of physical devices (sensor nodes) to developers. It provides a comforta",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
73bdfc10b2a200303d0ea36961cce1184d44dcf7,https://www.semanticscholar.org/paper/73bdfc10b2a200303d0ea36961cce1184d44dcf7,Artificial Intelligence and Labor: Media and Information Competencies Opportunities for Higher Education,,ECIL,2018.0,10.1007/978-3-030-13472-3_58,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c8cf3e051e46311f081785ba4941dc567e693e53,https://www.semanticscholar.org/paper/c8cf3e051e46311f081785ba4941dc567e693e53,Intelligent Music Interfaces,"Automatic music-understanding technologies (automatic analysis of music signals) make possible the creation of intelligent music interfaces that enrich music experiences and open up new ways of listening to music. In the past, it was common to listen to music in a somewhat passive manner; in the future, people will be able to enjoy music in a more active manner by using music technologies. Listening to music through active interactions is called active music listening. In this keynote speech I first introduce active music listening interfaces demonstrating how end users can benefit from music-understanding technologies based on signal processing and/or machine learning. By analyzing the music structure (chorus sections), for example, the SmartMusicKIOSK interface enables people to access their favorite part of a song directly (skipping other parts) while viewing a visual representation of the song's structure. I then introduce our recent challenge of deploying such research-level music interfaces as web services open to the public. Those services augment people's understanding of music, enable music-synchronized control of computer-graphics animation and robots, and provide various bird's-eye views on a large music collection. In the future, further advances in music-understanding technologies and music interfaces based on them will make interaction between people and music even more active and enriching.",IUI,2018.0,10.1145/3172944.3176184,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
41c339ea5b64dd923c064d63142e88501b260236,https://www.semanticscholar.org/paper/41c339ea5b64dd923c064d63142e88501b260236,Editorial: The Real Technology Revolution: Technology Justice,,Development,2019.0,10.1057/s41301-019-00230-3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
58de3fadcd94a72bbac26b2233a7a9eb5e5c2196,https://www.semanticscholar.org/paper/58de3fadcd94a72bbac26b2233a7a9eb5e5c2196,Adaptive data transfer methods via policy evolution for UAV swarms,"This paper presents an adaptive robotic swarm of Unmanned Aerial Vehicles (UAVs) enabling communications between separated non-swarm devices. The swarm nodes utilise machine learning and hyper-heuristic policy evolution to provide agility within the swarm, enabling each swarm member to select the most appropriate mobility policy for the environment given the swarm's abilities. The swarm evolution process of this study is found to successfully create different data transfer methods depending on the separation of non-swarm devices and the communication range of the swarm members. These methods are either human-designed, which the swarm adopts when most appropriate, or are novel hybridisations that the swarm creates for the problem. This paper also tests the swarm with individuals being removed during deployment. It is found that the swarm is immune to most alterations, though removal of specialised members of the heterogeneous swarm leads to temporary failure. The swarm evolution can then correct this failure by adjusting the swarm behaviour.",2017 27th International Telecommunication Networks and Applications Conference (ITNAC),2017.0,10.1109/ATNAC.2017.8215356,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,https://www.semanticscholar.org/paper/cdebf1de7b2ccb852b203708f9dc2e584a2abb0c,Comparing Human-Robot Proxemics between Virtual Reality and the Real World,"Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of Human-Robot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. Comparing Human-Robot Proxemics between Virtual Reality and the Real World Rui Li KTH Royal Institute of Technology Stockholm, Sweden Rui3@kth.se ABSTRACT Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other.Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI). To fully deploy the potential of VR and benefit HRI studies, we need to establish the basic understanding of the relationship between the physical, real-world interaction (Live) and VR. This study compared Live and VR HRI with a focus on proxemics, as proxemics preference can reflect comprehensive human intuition, making it suitable to be used to compare Live and VR. To evaluate the influence of different modalities in VR, virtual scenes with different visual familiarity and spatial sound were compared as well. Lab experiments were conducted with a physical Pepper robot and its virtual copy. In both Live and VR, proxemics preferences, the perception of the robot (competence and discomfort) and the feeling of presence were measured and compared. Results suggest that proxemic preferences do not remain consistent in Live and in VR, which could be influenced by the perception of the robot. Therefore, when conducting HRI experiments in VR, the perceptions of the robot need be compared before the experiments. Results also indicate freedom within VR HRI as different VR settings are consistent with each other. INTRODUCTION Virtual Reality (VR) is gaining more and more popularity as a research tool in the field of HumanRobot Interaction (HRI) [1][2][3][4]. VR has been used to test teleoperation and collect demonstration data to train machine learning algorithms, which showcased the effectiveness of learning visuomotor skills using data collected by consumer-grade devices [1]. VR teleoperation systems were proposed to crowdsource robotic demonstrations at scale [2]. A VR simulation framework was also proposed to replace the physical robot, as VR can enable high level abstraction in embodiment and multimodal interaction [3]. VR has also been used as a rapid prototyping tool to design in-vehicle interactions and interfaces for self-driving cars, which showed the evocation to genuine responses from test participants [4]. Compared to other HRI experiment methods, VR as an emerging interactive media provides unique advantages. VR HRI has the potential of having higher immersion and fidelity than picture based HRI, video-based HRI and simulated HRI. In situations where the perception of the robot is challenging, compared to on-screen viewing, VR display showed significant improvement on collaborative tasks [5]. When comparing VR HRI to the physical, realworld interaction (Live HRI), there is a trade-off between the two. VR experiences still cannot replace physical experiences due to system limitation, and limited interaction modalities etc. [6]. For example, system limitations such as limited field of view and low display resolution could reduce immersion and presence of the VR experience, resulting in different behaviors from Live experiments. Limited interaction modalities, such as the absence of touch, means that the participant could not feel the robot or even go through the robot, which could potentially break the entire interaction. Figure 1: Photograph of the Live experiment setting However, with the help of the distribution of consumer-grade VR devices and online crowdsourcing platforms, VR HRI has the potential to gain massive data for training robotic behavior and studying HRI related issues. Data collection through VR can also reduce noise and improve the data quality [1], which help to ease data processing and algorithm training. Furthermore, VR HRI experiments can test concepts and interactions without physical robots, making it more resource efficient and less expensive than Live HRI. Less hardware also means that the experiment will be less cumbersome to set up, easier to be reproduced and to ensure experiment quality. In this study, HRI Proxemics (the preferred personal space between a human and a robot) was compared to give a better justification and more basic understanding of the relationship between Live and VR. Proxemics preferences rely on lower level intuition [7], therefore, reflect the differences in the perceptions between Live and VR better. Compared to other HRI subject such as conversational (audio) or gaze behavior (visual), which are more modality dependent, proxemics can give a comprehensive understanding of the human responses. In addition, variations of modalities in VR can greatly influence human perception. For example, a higher visual familiarity of the physical environment in VR can decrease the effect of distance distortion [8]. Auditory inputs play another important role in VR, the addition of spatial sound can increase the sense of presence in VR and provide sound localization [9]. Thus, this work also compares VR settings with variance in modalities to evaluate the impacts of visual familiarity and spatial sound on VR HRI experiments. A 2 x 3 mixed design experiment was conducted to evaluate the differences between Live and VR HRI, as well as the influence of visual familiarity and spatial sound in VR. For the Live HRI, the pepper robot from Softbank Robotics was used (Figure 1). In the VR HRI, a 3D model of the same robot was used. To measure visual familiarity, the VR scene was created in Blender based on a 3D scan of the physical lab. The spatial sound was created by enabling the movement of the physical robot, due to the difficulties of engineering spatial sound. The interaction was implemented in Unity. As an objective measurement for proxemics preference, the minimum comfort distance (MCD) was measured. In addition, for the psychological perception of the experience, the feeling of presence was measured with the SUS questionnaire. For the perception of the robot, two relevant factors, competence and discomfort was measured with the ROSAS questionnaire.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d91b1b996678425418321d9de5b251778eb506d7,https://www.semanticscholar.org/paper/d91b1b996678425418321d9de5b251778eb506d7,Knowledge engineering with semantic web technologies for decision support systems based on psychological models of expertise,"Machines that provide decision support have traditionally used either a representation of human expertise or used mathematical algorithms. Each approach has its own limitations. This study helps to combine both types of decision support system for a single system. However, the focus is on how the machines can formalise and manipulate the human representation of expertise rather than on data processing or machine learning algorithms. It will be based on a system that represents human expertise in a psychological format. The particular decision support system for testing the approach is based on a psychological model of classification that is called the Galatean model of classification. The simple classification problems only require one XML structure to represent each class and the objects to be assigned to it. However, when the classification system is implemented as a decision support system within more complex realworld domains, there may be many variations of the class specification for different types of object to be assigned to the class in different circumstances and by different types of user making the classification decision. All these XML structures will be related to each other in formal ways, based on the original class specification, but managing their relationships and evolution becomes very difficult when the specifications for the XML variants are text-based documents. For dealing with these complexities a knowledge representation needs to be in a format that can be easily understood by human users as well as supporting ongoing knowledge engineering, including evolution and consistency of knowledge. The aim is to explore how semantic web technologies can be employed to help the knowledge engineering process for decision support systems based on human expertise, but deployed in complex domains with variable circumstances. The research evaluated OWL as a suitable vehicle for representing psychological expertise. The task was to see how well it can provide a machine formalism for the knowledge without losing its psychological validity or transparency: that is, the ability of end users to understand the knowledge representation intuitively despite its OWL format. The OWL Galatea model is designed in this study to help in automatic knowledge maintenance, reducing the replication of knowledge with variant uncertainties and support in knowledge engineering processes. The OWL-based approaches used in this model also aid in the adaptive knowledge management. An adaptive assessment questionnaire is an example of it, which is dynamically derived using the users age as the seed for creating the alternative questionnaires. The credibility of the OWL Galatea model is tested by applying it on two extremely different assessment domains (i.e. GRiST and ADVANCE). The conclusions are that OWLbased specifications provide the complementary structures for managing complex knowledge based on human expertise without impeding the end users’ understanding of the knowledgebase. The generic classification model is applicable to many domains and the accompanying OWL specification facilitates its implementations.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e3bc173e3e88af39b8c1eb91ba2a72199588131c,https://www.semanticscholar.org/paper/e3bc173e3e88af39b8c1eb91ba2a72199588131c,Recommendations for Future Development of Artificial Agents [Commentary],"A set of technologies, loosely referred to as “artificial agents,” is becoming more pervasive and more powerful in the current computing landscape. All artificial agents are built on a computational foundation. Some are purely computational, e.g., Internet bots, search engines, and others are physically embodied entities with computational decision-making components, e.g., robots, unmanned aerial vehicles (UAVs), and autonomous cars. The noteworthy feature of artificial agents - the feature that leads to the artificial agent label - is their capacity to operate autonomously. At some level or to some degree, artificial agents operate independently from the humans who design and deploy them. They are agents in the sense that we deploy them to perform tasks on our behalf and often these tasks involve learning and decision-making. Since humans previously performed many of these tasks, we mark the difference, that is, the machine performance of these tasks, by referring to them as “artificial"".",IEEE Technol. Soc. Mag.,2014.0,10.1109/MTS.2014.2363978,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a2b4475e2a22f60e7b4841cd433750f1e9f6ed03,https://www.semanticscholar.org/paper/a2b4475e2a22f60e7b4841cd433750f1e9f6ed03,IEEE Access Special Section Editorial: Artificial Intelligence and Cognitive Computing for Communication and Network,"With the rapid development of communication and network technologies, novel information services and applications are rapidly growing worldwide. Advanced communications and networks greatly enhance the user experience, and have amajor impact on all aspects of people’s lifestyles in terms of work, society, and the economy. Although advanced techniques have extensively improved users’ quality of experience (QoE), they are not adequate to meet the various requirements of seamless wide-area coverage, high-capacity hot-spots, low-power massive-connections, low-latency and high-reliability, and other scenarios. Therefore, it is a great challenge to develop smart communications and networks that support optimized management, dynamic configuration, and feasible services. Under the new service paradigm, artificial intelligence (AI) and cognitive computing are very promising approaches for dealing with dynamic and large-scale topology; thus, we should explore AI-based techniques, e.g., statistical learning, feedforward neural networks, deep recurrent neural networks, etc., for complicated decision making, network management, resource optimization, and in-depth knowledge discovery in complex environments. Furthermore, communications and network ecosystems must be upgraded with new capabilities such as machine learning, data analytics, and cognitive power for providing human intelligence. This Special Section aims at bringing together academic and industrial researchers to identify and discuss technical challenges and recent results related to smart communication and networks. To meet the extreme requirements of the user experience, energy efficiency, and performance in a complex network environment, novel design, configuration, and optimization of network communication are needed. This section discusses how to improve the QoS of communications and networks using AI and cognitive computing. The Call for Papers aroused great enthusiasm in the scientific community and received more than 100 submissions. Out of these, 47 articles were accepted for inclusion in the Special Section after a thorough revision process by at least two independent referees. First, the invited article, ‘‘Wearable affective robot,’’ by Chen et al., proposes a wearable affective robot equipped with cognitive computing. The authors introduce innovative research in the brain wearable field from the aspects of hardware design, EGG data acquisition, and analysis, user behavior perception, algorithm deployment, etc. Through the use of cognitive computing and continuous data acquisition in depth and breadth, the life modeling of a user is achieved, thus truly realizing a personalized intelligent robot. The article ‘‘The analysis of influencing factors of information dissemination on cascade size distribution in social networks,’’ by Dong et al., examines the influencing factors of information dissemination in the view of group cognitive computation, which quantitatively reveals the causality of cascade size power-law distribution from computational experiments, and clarifies the role of information attractiveness, influence, and information sources on the distribution of popularity in social networks. The article ‘‘A high quality task assignment mechanism in vehicle-based crowdsourcing using predictable mobility based on Markov,’’ by Jia et al., proposes a new method of task assignment for a vehicle-based crowdsourcing platform. It focuses on the mobility characteristics of the node, then builds a model based on the participant’s movement trajectory. It judges and matches the participants that are most suitable for the current task by inferring the participants’ position information in the next stage. In the article ‘‘An evolution model of group opinions based on social judgment theory,’’ by Lu et al., the authors explore how group opinions form and evolve in social situations and how personality impacts the evolution of opinions. Due to the limitations of the traditional Hegselmann–Krause (HK) model, the authors propose a novel agent-based model by combining social judgment theory and the bounded confidence principle, and give openness and vacillation to each agent. Finally, the effects of the two parameters on the model are compared and analyzed through system simulation. In the article, ‘‘DROM: Optimizing the routing in software-defined networks with deep reinforcement learning,’’ by Yu et al., the authors present a deep",IEEE Access,2020.0,10.1109/access.2020.3014475,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aa077b6c8d233b53dd307ec5382d8d57ae0b2a72,https://www.semanticscholar.org/paper/aa077b6c8d233b53dd307ec5382d8d57ae0b2a72,Leadership Challenges to the Deployment of Autonomous Weapons (AWS),"Autonomous Weapon Systems (AWS) are defined as robotic weapons that have the ability to sense and act unilaterally depending on how they are programmed. Such human-out-of-the-loop platforms will be capable of selecting targets and delivering lethality without any human interaction. This weapon technology may still be in its infancy, but both semi-autonomous and other pre-cursor systems are already in service. There are several drivers to a move from merely automatic weapons to fully autonomous weapons which are able to engage a target based solely upon algorithm-based decision-making. This requires material step-change in both hardware and software and, once deployed, posits a significant change in how humans wage war. But complex technical difficulties must first be overcome if this new independent and self-learning weapon category can legally be deployed on the battlefield. AWS also pose basic statutory, moral and ethical challenges. 
 
This paper identifies the manifest complexity involved in fielding a weapon that can operate without human oversight while still retaining value as a battlefield asset. The subject’s importance is that several well-tried concepts that have long comprised battlecraft may no longer be fit for purpose. While the recent development pace in these technologies may appear extraordinary, fundamental fault lines endure. The paper also notes the inter-dependent and highly coupled nature of the routines that are envisaged for AWS operation, in particular ramifications arising from its machine learning spine, in order to demonstrate how detrimental are these compromises to AWS deployment models. In highlighting AWS deployment challenges, the analysis draws on broad primary and secondary sources to conclude that Meaningful Human Control (MHC) should be a statutory requirement in all violent engagements.",,2020.0,10.2139/SSRN.3757523,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7ef90bd2df8c82f68d0bf5579dbb3a980e038250,https://www.semanticscholar.org/paper/7ef90bd2df8c82f68d0bf5579dbb3a980e038250,An architecture for robotic discovery and learning by experimentation,"Enabling machines to autonomously learn how to deal with novel situations, and to generalize the acquired knowledge in order to apply it to different situations, is a major challen g on the way towards building intelligent autonomous robots. In the past years, a number of approaches have attempted to lear n relational knowledge that is general enough to be applied to different domains and contexts [2, 11, 8]. What is however still missing is a general architecture in which autonomous robotic learning for usage in everyday environments can be structured well [6]. In pursuit of enabling truly autonomous robotic learning, the work in the XPERO project 1 set out to design computational methods and mechanisms which enable an embodied intelligent agent (robot) to learn naı̈ve physics concepts such as the geometry of motion or articulation only from realworld perceptions in a fully unsupervised way. By promoting the paradigm ofLearning by Experimentation , this approach significantly differs from other implementations of Roboti c Discovery and Experimentation such as [16, 1, 12]. Instead, Learning by Experimentationaims at enabling the agent to learn incrementally in an ”evolution of theories”. It shoul d develop new theories and gain new insights by systematicall y investigating unknown phenomena and discovering regulari ties in the observed data. Depending on the observed phenomenon the new theory might be a revision of an existing theory or be built upon existing theories. Learning by experimentation is a discovery process involving a sequence of steps, which finally terminate in the generation of a new or revised theory. The process is trigger ed by a failure to predict and explain a phenomenon that has been observed during the execution of everyday activities. In XPERO, inductive learning paradigms such as ILP (inductive logic programming) [7], qualitative model tree lea rning [15] or learning of qualitative differential equations or p artial derivatives [14, 13] were used to infer new concepts from real-world perceptions of naı̈ve physical phenomena. Indu ctive learning paradigms are not particularly new research fields",,2010.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3af91b1be29551c1cf19866f47e7035b7e139343,https://www.semanticscholar.org/paper/3af91b1be29551c1cf19866f47e7035b7e139343,Objective Image Quality Assessment: Facing The Real-World Challenges,"There has been a growing interest in recent years in the development of objective image quality assessment (IQA) models, whose roles are not only to monitor image quality degradations and benchmark image processing systems, but also to optimize various image and video processing algorithms and systems. While the past achievement is worth celebrating, a number of major challenges remain when we apply existing IQA models in realworld applications. These include obvious ones such as the challenges to largely reduce the complexity of existing IQA algorithms and to make them easy-to-use and easy-to-understand. There are also challenges regarding the applicability of existing IQA models in many real-world problems where image quality needs to be evaluated and compared across dimensionality, across viewing environment, and across the form of representations − specific examples include quality assessment for image resizing, color-togray image conversion, multi-exposure image fusion, image retargeting, and high dynamic range image tone mapping. Here we will first elaborate these challenges, and then concentrate on a specific one, namely the generalization challenge, which we believe is a more fundamental issue in the development, validation and application of IQA models. Specifically, the challenge is about the generalization capability of existing IQA models, which achieve superior quality prediction performance in lab testing environment using a limited number of subject-rated test images, but the performance may not extend to the real-world where we are working with images of a much greater diversity in terms of content and complexity. We will discuss some principle ideas and related work that might help us meet the challenges in the future. Introduction Over the past decades, a growing number of researchers and engineers in the image processing community have started to realize the importance of image/video quality assessment (IQA/VQA) [40, 29, 4]. This is not surprising because no matter what image/video processing problems we are working on, the same issues repeatedly come up − How should we evaluate the images generated from our algorithms/systems? How do we know our algorithm/system is creating an improvement between the input and output images, and by how much? How can we know one algorithm/system performs better than another, and by how much? What should be the quality criterion for which the design of our algorithms/systems should be optimized? Since the human eyes are the ultimate receivers in most image processing applications, human subjective visual testing would be a reliable solution. However, with the exponential increase of the volume of image/video data being generated daily, it becomes impossible to address these quality issues in a timely manner by subjective visual testing, which is slow, cumbersome and expensive. Instead, only trusted objective IQA models may potentially meet these needs. In academia, objective IQA has been a hot research topic, especially in the past 15 years [35, 4, 29]. First, the commonly used numerical disotrtion/quality measures in the past − the mean squared error (MSE) and the peak signal-to-noise ratio (PSNR) − have been shown to correlate poorly with perceived image quality [28, 30]. Second, a large number of perceptually more meaningful IQA models have been proposed, including full-reference (where a perfect quality reference image is available when evaluating a distorted image) [35, 4, 29], no-reference (where the reference image is not accessible) [34, 24, 31], and reduced-reference (where only partial information about the reference image is available) models [39, 36, 31, 29]. Third, several design principles have been discovered and repeatedly demonstrated to be useful in the design and improvement of IQA models. These include psychophysical and physiological visibility models [35, 4], the structural similarity (SSIM) approaches [28, 32, 33, 20, 49], the natural scene statistics (NSS) and information theoretic approaches [36, 39, 21, 31], the visual saliency based approaches [50], and the machine learning based approaches [6]. Fourth, a number of subject-rated image quality databases have been created and made publicly available [22, 7, 8, 17, 16, 47]. They provide a common benchmark platform for the evaluation and comparison of IQA models, among which several algorithms have achieved high correlations with the subjective mean opinion scores (MOSs) of the test images [23, 38, 33, 49]. In the video delivery industry, perceptual objective IQA methods such as the SSIM algorithm have been incorporated into many practical hardware and software systems to monitor image/video quality degradations and to test/compare image/video encoders and transcoders [27, 25, 26]. The wide use of SSIM has resulted in a Primetime Engineering Emmy Award given by the Academy of Television Arts and Sciences [1]. The remarkable development and successful deployment of modern IQA methods are definitely worth celebrating. Nevertheless, this does not necessarily mean that the existing IQA models have already met the real-world challenges. Otherwise, they should have made a much stronger impact and become a gamechanging factor in the industry. Using the video delivery industry as an example, even now most practitioners are still equating bitrate with quality in the practical design of video delivery architectures. However, using the same bitrate to encode different video content could result in dramatically different visual quality. Clearly, the perceptual quality of the video itself, which is presumably the ultimate evaluation criterion of the whole video delivery system, has not been placed at the driver’s seat. While it is understandable that quality degradation is inevitable at many stages in the video delivery chain due to practical constraints, the real concern here is that there is no existing protocol to monitor and control such quality degradation. As a result, various tricks have been used to manipulate the video content and network resources are allocated in suboptimal ways, leaving the creative intent of the content producers unprotected. While it is certain that the industry needs to be better informed about the great potentials of making the best use of IQA/VQA models, we believe that an equally important aspect that slows down the process is that the existing IQA/VQA models still do not meet many real-world challenges. In the following sections, we will elaborate some of these challenges and then focus on a specific one, namely the generalization challenge. We wish our discussions on some fundamental ideas could provide some useful insights for the future development of IQA models that may meet these real-world challenges. The Real-World Challenges Here we make a list of real-world challenges, many of which are described in more details through examples of practical scenarios. 1. It is highly desirable to reduce the complexity of the IQA/VQA algorithms so that they can be computed in realtime or in an even faster speed. This is especially useful in time-sensitive applications such as live broadcasting and videoconferencing. Many existing models are far from meeting this challenge. 2. It is essential to make the IQA/VQA scores easy-to-use and easy-to-understand. For example, the raw SSIM score does not have an explicit perceptual meaning, making it difficult to determine what level of SSIM index can warrant an excellent video quality and how much improvement in the SSIM index is sufficient to create visible quality improvement. Mapping the raw scores into a perceptually linear domain that is easily linked to human expressions about image quality is desirable. 3. The same video stream shown on different display devices could result in very different perceptual quality. For example, a strongly compressed video that exhibits very annoying artifacts on a large TV could appear to have fine quality when viewed on the screen of a smartphone. The quality may also change significantly when the video is watched on the same TV but at two different viewing distances, one at the default distance and the other at a very close distance. However, existing IQA/VQA models give the same score based on the video stream only, completely ignorant of the viewing device and viewing condition. 4. In a video-on-demand application, a high-quality highresolution (e.g., 4K) source video may be encoded into multiple video streams of different resolutions (e.g., 1080p, 720p, 360p, 240p, etc.) and different bit rates, aiming for satisfying a variety of user needs. In order to measure the quality of the encoded videos, most existing VQA models cannot be computed because the source (reference) and test videos have different spatial resolutions. 5. An image or video may need to be displayed on a screen that has a spatial resolution higher than that of the image resolution. As a result, spatial interpolation is performed. Again, most existing VQA models are not applicable because the reference and test images have different spatial resolutions. 6. An image or video of imperfect quality (e.g., being compressed at an earlier stage) is received and then transcoded to multiple images or videos with different bitrates and resolutions. Most existing IQA/VQA models are not applicable not only because they do not allow for cross-resolution quality assessment, but also because they assume the original reference image/video to have perfect quality, which is not the case here. How to carry out “degraded reference” IQA/VQA is a major challenge. 7. A high dynamic range (HDR) image (e.g., the pixels are in 10 or more bit depths) is tone mapped to a standard dynamic range (SDR) image (8 bits per pixel) in order to be visualized on an SDR display. There is certainly information loss that we would like to capture. However, most existing IQA models do not apply because they cannot compare images/videos with different dynamic ranges.",IQSP,2016.0,10.2352/ISSN.2470-1173.2016.13.IQSP-205,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b775bd1807572e45636e8508667edb5c9fd8cc72,https://www.semanticscholar.org/paper/b775bd1807572e45636e8508667edb5c9fd8cc72,The Risks of Artificial Intelligence to Security and the Future of Work,"Overview Future generations may look back at our time and identify it as one of intense change. In a few short decades, we have morphed from a machine-based society to an information-based society, and as this Information Age continues to mature, society has been forced to develop a new and intimate familiarity with data-driven and algorithmic systems. We use the term artificial agents to refer to devices and decisionmaking aids that rely on automated, datadriven, or algorithmic learning procedures (including artificial intelligence (AI) in its many manifestations).1 These include devices as banal as Roomba robots and online recommendation engines to more advanced cognitive systems like IBM’s Watson. Such agents are becoming an intrinsic part of our regular decisionmaking processes. Their emergence and adoption lead to a bevy of related policy questions. How do we reorient our thinking on relevant policy in this new regime? Where are our blind spots in this space? How do users, as well as affected populations, identify and remedy errors in logic or assumptions? What sectors are the ripest for disruption by artificial agents, and what approaches to regulation will be most effective? We wrote a previous report (Osoba and Welser, 2017) emphasizing the existence of blind spots and bias with respect to artificial agents in the criminal justice system, but other sectors will likely be impacted. This Perspective discusses the outcome of a structured exercise to understand what other areas might be affected by increasing deployment of artificial agents. We relied on a diverse group of experts to paint scenarios in which AI could have a significant impact. The Research Methodology section describes how we did this elicitation, fundamentally an exercise in forecasting. Thus, not The Risks of Artificial Intelligence to Security and the Future of Work",,2017.0,10.7249/PE237,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a4af51cc0422182e51f660056ade994339a2cd18,https://www.semanticscholar.org/paper/a4af51cc0422182e51f660056ade994339a2cd18,Distributed Online Evolution for Swarm Robotics,"Engineering multi-agent systems is difficult due to numerous constraints, such as noise, limited range of interaction with other agents, delayed feedback, and the distributed autonomy of the agents. One potential solution is to automate the design of multi-agent systems in simulation, using evolutionary algorithms (EAs) [1, 4]. In this paradigm, the EA evolves the behaviors of the agents (and their local interactions), such that the global task behavior emerges. A global observer monitors the collective, and provides a measure of performance to the individual agents. Agent behaviors that lead to desirable global behavior are hence rewarded, and the collective system is gradually evolved to provide optimal global performance [5]. There are several difficulties with this approach. First, a global observer may not exist. Second, some (but not all) agents may experience some form of reward for achieving task behavior, while others do not. Third, this reward may be delayed, or may be noisy. Fourth, the above paradigm works well in simulation (offline), but is not feasible for realworld online applications where unexpected events occur. Finally, the above paradigm may have difficulty evolving different individual behaviors for different agents (heterogeneity vs homogeneity). We propose a novel framework, called “Distributed Agent Evolution with Dynamic Adaptation to Local Unexpected Scenarios” (DAEDALUS), for engineering multi-agent systems that can be used either offline or online. We will explore how DAEDALUS can be used to achieve global aggregate behavior, by examining a case study, obstacle avoidance in robotic swarms. With the DAEDALUS paradigm, we assume that agents (whether software or hardware) move throughout some environment. As they move, they interact with other agents. These agents may be of the same species or of some other species [3]. Agents of different species have different roles in the environment. The goal is to evolve agent behaviors and interactions between agents, in a distributed fashion, Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 200X ACM X-XXXXX-XX-X/XX/XX ...$5.00. such that the desired global behavior occurs. Let us further assume that each agent has some procedure to control its own actions, in response to environmental conditions and interactions with other agents.The precise implementation of these procedures is not relevant, thus they may be programs, rule sets, finite state machines, real-valued vectors, force laws, or any other procedural representation. Agents have a sense of self-worth, or “fitness”. Each robot of the swarm is an individual in a population that interacts with its neighbors. Each robot contains a slightly mutated copy of the optimized control procedure found with offline learning with an offline EA. This ensures that our robots are not completely homogeneous. We allowed this slight heterogeneity because when the environment changes, some mutations perform better than others. The robots that perform well in the environment will have higher fitness than the robots that perform poorly. When low fitness robots encounter high fitness robots, the low fitness robots ask for the high fitness robot’s rules. Hence, better performing robots share their knowledge with their poorer performing neighbors. When we apply DAEDALUS to obstacle avoidance, we focus on two aspects of our swarm: reducing obstacle-robot collisions and maintaining the cohesion of the swarm. Robots are penalized if they collide with obstacles and/or if they leave their neighbors behind. The second scenario arises when the robots are left behind in cul-de-sacs. This causes the cohesion of the formation to be reduced. In our results, we demonstrate how robots overcome these scenarios using DAEDALUS. In our offline environment, 40 robots were trained with 90 randomly distributed smaller obstacles and one goal. The offline EA with a population size of 100 with 100 generations takes at least a week in real time to produce an optimal control procedure. To measure the performance of the DAEDALUS approach, an experiment was carried out with 60 robots, 5 goals in the long corridor, and 90 obstacles that are three times larger than the obstacles used in offline environment (i.e. higher obstacle density) randomly distributed between each goal. The experiment was averaged over 50 runs of different robot, goal, and obstacle placements. Each robot is given equal initial fitness and “seeded” with a mutated copy of the optimized control procedure learned in offline mode. If a robot collides with an obstacle, it’s fitness is reduced. Whenever a robot encounters another robot with higher fitness, it takes the relevant parameters pertaining to the obstaclerobot interaction of the better performing robot. Our online DAEDALUS approach was able to learn obstacle avoidance in less than 30 minutes in real time. This is an impressive improvement over offline learning. 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0 1 2 3 4 5 R ob ot s C ol lid ed /R ob ot s S ur vi ve d Goal Number Percentage of Robots that Collide with Obstacles Figure 1: The ratio of colliding robots versus the number of surviving robots, for 60 robots moving through 5 goals with 90 obstacles in between each goal. All results are averaged over 50 independent runs. Figure 1 shows the ratio of the number of robots that collided with obstacles versus the number of robots that survived to reach the goals. The graph indicates that after only 2 goals, the percentage of robots that collide with obstacles has dropped from about 36% to well under 5%. Inspection of the obstacle-robot control procedures indicates that the repulsive component increased through the online process of mutation and the copying of superior control procedures (this was confirmed via inspection of the mutated control procedures). This first experiment did not attempt to alleviate the situation where robots are left behind; in fact, only roughly 43% of the original 60 robots reach the final goal (see Figure 2, lower line). This is caused by the large number of cul-desacs produced by the large obstacle density. Our second experiment attempts to alleviate this problem by focusing on the robot-robot interactions. Our assumption was that the control procedure needs to provide stronger cohesion, so that robots aren’t left behind. If robots are stuck behind in cul-de-sacs (i.e. they make no progress towards the goal) and they sense neighbors, they slightly mutate the robot-robot interaction parameters of their control procedures. In a situation in which they do not sense the presence of neighbors and do not progress towards the goal, they rapidly mutate their robot-goal interaction causing a “panic behavior”. These relatively large perturbations of the control procedure allow the robots to escape their motionless state. Figure 2 shows the results of this second experiment. In comparison with the first experiment (with survival rates of 43%), the survival rates have increased to 68%. As a control experiment, we ran our offline EA on this more difficult task. After five goals, the survival rate is about 78%. As can be seen, the DAEDALUS approach provides results only somewhat inferior to the offline approach, in real time, while the robots are in the environment. Although not shown in the graph, it is important to point 0 10 20 30 40 50 60 0 1 2 3 4 5 R ob ot s S ur vi ve d Goal Number Number of Robots that Survived at Each Stage Survival is Important (online) Collisions are Important (online) Survival in offline Figure 2: A comparison of (a) the number of robots that survive when rules are learned using offline learning, (b) the number of robots that survive when using online learning (where the focus is on reducing collisions), and (c) the number of robots that survive when using online learning (and the focus in on survivability). out that the collision rates were not affected in the second experiment. Hence, we believe that it is quite feasible to combine both aspects in the future. Collision avoidance can be improved via mutation of the obstacle-robot interaction, while survival can be improved via mutation of the robotrobot interaction and robot-goal interaction. Future work of this research will focus more on the issue of credit assignment. Current work in classifier systems uses mechanisms such as “bucket-brigade” or “profit sharing” to allocate rewards to individual “agents” appropriately [2]. However, these techniques rely on global blackboards and assume that all agents can potentially act with all others, through a bidding process. We intend to modify these approaches so that they are fully distributed, and appropriate for online systems.",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,https://www.semanticscholar.org/paper/3f3f00fc84c5ea2f6fba2dea24d139dfcc2278bb,Visual Model Interpretation for Epidemiological Cohort Studies,"Epidemiological cohort studies investigate the cause and development of diseases in human populations. Conventional analyses are challenged by recently increasing study sizes, which is why the incorporation of machine learning gains popularity. State-of-the-art classifiers are however often hard to interpret – an important requirement in medical applications. This thesis addresses the gap between predictive power and interpretability in the context of cohort study analysis. Main contribution is the development of an interactive visual interface for the interpretation and comparison of probabilistic classifiers. It supports the analysis of important features at both global and individual level, computation of partial dependence, and iterative construction of meaningful feature groups. To analyse the longitudinal influence of features, the user can modify the feature set by removing a feature or replacing its value by a previous examination record. The developed visual interface is evaluated in two case studies in order to test its effectiveness for the generation and validation of research hypotheses. The case studies include a realworld epidemiological cohort study and synthetic data. The results indicate the interface’s usefulness for epidemiological research, but also reveal necessary further work for the deployment into a productive environment.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5db60e5fd5089dfc3df4c8f645a925ac20e290ab,https://www.semanticscholar.org/paper/5db60e5fd5089dfc3df4c8f645a925ac20e290ab,In Resonance with Technology's Cutting Edge [President's Message],"4 IEEE SyStEmS, man, & CybErnEtICS magazInE July 2018 T oday’s discourse among both technical professionals and lay technology enthusiasts is teeming with subject matter focused on innovations resulting from the research and practice of systems science and engineering, human– machine systems, and cybernetics. Whether it is complex systems en ­ abled by cybernetics, intelligence for robotic and vehicular autonomy, new capabilities enabled by advanc­ es in machine learning, augmented humans, human–machine fusion, or other forms of human–machine symbiosis, the dialog is vibrant in the technical and nontechnical sectors of society alike. My conversations with colleagues, laypersons, family, and friends consistently touch on various exciting aspects of science and tech­ nology, how quickly it is advancing, what it is enabling, where it is head­ ed, and its implications. Invariably, I find myself discussing ideas that all relate in some way to the fields of interest and focal activities of the IEEE Systems, Man, and Cybernetics Society (SMCS). Surely, this experi­ ence is also shared by the members of our Society, who must feel the same sense of powerful resonance that I do between the SMCS and the current frontiers of technology. Members of our Society do not share this sense of resonance in iso­ lation but may identify more strongly with it since we are directly engaged in our fields. The confluence of systems science and engineering, human–ma­ chine systems, and cybernetics is prev­ alent at the present­ day cutting edge of engineering. In fact, year after year, lead­ ing research organi­ zations that report on emerging and disrup­ tive technology trends consistently identify technologies that cor­ respond to areas of active research in the SMCS. Thus, this is an era of major impact for the Society, and I am excited to be sharing this time with our membership as we continue to advance technology for the benefit of humanity through our individual work and collaborations. As technologies and methodolo­ gies mature to meet new challenges, there is an increasing motivation to incorporate them into real­world sys­ tems. This is where our knowledge and tools for systems science and systems engineering are brought to bear. Additionally, at this time, there is a pronounced emphasis on human factors and the human relationships with the technologies that compose complex systems—to which we in the SMCS are well equipped to respond with appropriate human­centric solu­ tions. With cybernetics as a science of, and a transdisciplinary approach to, studying control and communica­ tions in machines and living things, we are well equipped to apply its ele­ ments to enable complex and increas­ ingly intelligent systems that interact with humans in a symbiotic or collab­ orative fashion. Interestingly, the hot topical areas referred to by the many specialized terms in SMCS member vernacular are often lumped by popular culture within the currently conflated term of artificial intelligence (AI) to the extent that any instance of soft­ ware performing intel­ ligent data pro cessing is even being referred to in pro noun form as an AI. The same can be said of terms of reference like cyberphysical sys tems. This situation presents an opportunity for our members to articulate our research in more specific terms, beyond our inner technical circles, asserting a claim on the inte­ gral ingredients of what are becoming known as AI, cyberphysical systems, and related capabilities. SMCS mem­ bers are engaged in the many facets of research that are collectively defining an exciting future and should be proud contributors to this, taking appropriate ownership and responsibility. Indeed, the Society is a nexus of technical talent, research activity, and advances for a large share of what is fueling today’s discourse. Students studying in our fields of interest (http://ieeesmc.org/about­smcs), their professors, and engaged professionals worldwide should see the SMCS as a home and community supporting and advancing their interests and career. In Resonance with Technology’s Cutting Edge","IEEE Systems, Man, and Cybernetics Magazine",2018.0,10.1109/msmc.2018.2833418,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e083ff4794bfe8a6b25e51264612bae062d7718a,https://www.semanticscholar.org/paper/e083ff4794bfe8a6b25e51264612bae062d7718a,Introduction to the Special Issue on Artificial Intelligence and Human-Robot Interaction,"Artificial Intelligence (AI) has had a transformational impact on Human-Robot Interaction (HRI) research over the past decade, enabling work in HRI to develop and investigate robots that can operate autonomously in far more challenging environments and far more complex scenarios than was possible ever before. Beyond laboratory studies, robots that explicitly interact with people as part of their functionality are increasingly being developed, productized, and deployed throughout the world, enabling ecologically valid ethnographic studies of interactions between humans and robots. These advances have been fueled by enabling technologies across many subfields of AI including machine learning, computer vision, task and motion planning, natural language understanding, and dialogue systems. It is not, however, the case that AI research produced polished, ready-off-the-shelf tools that researchers could pick up and effortlessly use to build their envisioned autonomous robot. Rather, the shift has been due to a new, hybrid approach to human-centered robotics research, facilitated by HRI researchers who acquired deep technical skill sets and an influx of AI researchers applying their expertise to HRI problems. More interdiscplinary research teams consisting of formerly AI and HRI researchers also formed, resulting in a vibrant sub-community at the intersection of AI and HRI who came together at the AAAI Fall Symposium on AI for Human-Robot Interaction for the last 4 years. This special issue was encouraged by the continued success and overwhelming popularity of this symposium. Our goal is to exemplify this community’s mature, high-quality, and original work, establishing T-HRI as a premier venue for work at the intersection of AI and HRI. Research at this intersection is particularly challenging due to the very need for interdiscplinary, multi-faceted skill sets. AI-HRI researchers need to both innovate in computational techniques and",ACM Transactions on Human-Robot Interaction,2018.0,10.1145/3279995,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0691c21a55a7a6361a058c904c7f1bff97023f58,https://www.semanticscholar.org/paper/0691c21a55a7a6361a058c904c7f1bff97023f58,A Methodology for the Refinement of Robots,"Biologists agree that flexible epistemologies are an interesting new topic in the field of operating systems, and security experts concur. The influence on machine learning of this technique has been well-received. On a similar note, for example, many methodologies harness the simulation of 4 bit architectures. The deployment of link-level acknowledgements would tremendously improve pasteurization.",,2018.0,10.19080/etoaj.2018.01.555556,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d4fb626ed46ce33e727ab402c7bff165700e9b4a,https://www.semanticscholar.org/paper/d4fb626ed46ce33e727ab402c7bff165700e9b4a,Visible light positioning for location-based services in Industry 4.0,"Industry 4.0 refers to the evolution in manufacturing from computerization to fully cyberphysical systems that exploit rich sensor data, adaptive real-time safety-critical control, and machine learning. An important aspect of this vision is the sensing and subsequent association of objects in the physical world with their cyber and virtual counterparts. In this paper we propose Visible Light Positioning (VLP) as an enabler for these Industry 4.0 applications. We also explore sensing techniques, including cameras (and depth sensors), and other light-based solutions for object positioning and detection along with their respective limitations. We then demonstrate an application of positioning for real time robot control in an interactive multiparty cyber-physical-virtual deployment. Lastly, based on our experience with this cyber-physical-virtual application, we propose Ray-Surface Positioning (RSP), a novel VLP technique, as a low cost positioning system for Industry 4.0.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5d634ee6d682805a5611edf79a45152e8228a300,https://www.semanticscholar.org/paper/5d634ee6d682805a5611edf79a45152e8228a300,A non-biological AI approach towards natural language understanding,"The problem being addressed in this paper is that using brute force in Natural Language Processing and Machine Learning combined with advanced statistics will only approximate meaning and thus will not deliver in terms of real text understanding. Counting words and tracking word order or parsing by syntax will also result in probability and guesswork at best. Their vendors struggle in delivering accurate quality and this results in ill-functioning applications. The newer generation methodologies like Deep Learning and Cognitive Computing are breaking barriers in the (Big Data) fields of Internet of Things, Robotics and Image/Video Recognition but cannot be successfully deployed for text without huge amounts of training and sample data. In the short term, we believe non-biological Artificial Intelligence will produce the best results for text understanding. Miia applied advanced Linguistic and Semantic Technologies combined with ConceptNet modeling and Machine Learning to successfully cater deep intelligent and cross-language quality to several industries.",2016 Future Technologies Conference (FTC),2016.0,10.1109/FTC.2016.7821768,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f3bc85689c7e9aacd15c173202711b7e505e6300,https://www.semanticscholar.org/paper/f3bc85689c7e9aacd15c173202711b7e505e6300,People Detection in 3d Point Clouds Using Local Surface Normals,,RoboCup,2012.0,10.1007/978-3-642-39250-4_15,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
688c72ca57e7bf544a9b8d594dba99e068e9a627,https://www.semanticscholar.org/paper/688c72ca57e7bf544a9b8d594dba99e068e9a627,Agent and Multi-Agent Systems. Technologies and Applications,,Lecture Notes in Computer Science,2012.0,10.1007/978-3-642-30947-2,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d9229b2720f480b8597826a82022c718cddfc9bb,https://www.semanticscholar.org/paper/d9229b2720f480b8597826a82022c718cddfc9bb,Robots that anticipate pain: Anticipating physical perturbations from visual cues through deep predictive models,"To ensure system integrity, robots need to proactively avoid any unwanted physical perturbation that may cause damage to the underlying hardware. In this paper, we investigate a machine learning approach that allows robots to anticipate impending physical perturbations from perceptual cues. In contrast to other approaches that require knowledge about sources of perturbation to be encoded before deployment, our method is based on experiential learning. Robots learn to associate visual cues with subsequent physical perturbations and contacts. In turn, these extracted visual cues are then used to predict potential future perturbations acting on the robot. To this end, we introduce a novel deep network architecture which combines multiple sub-networks for dealing with robot dynamics and perceptual input from the environment. We present a self-supervised approach for training the system that does not require any labeling of training data. Extensive experiments in a human-robot interaction task show that a robot can learn to predict physical contact by a human interaction partner without any prior information or labeling.",2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),2017.0,10.1109/IROS.2017.8206442,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3738f2a1b89de2e10312f756add3be44ffcd2b32,https://www.semanticscholar.org/paper/3738f2a1b89de2e10312f756add3be44ffcd2b32,Emotional feedback for service robots using hapto-acoustic interface,"A behavior-oriented human-machine communication system based on bio-inspired information processing is developed in this paper. A low cost acoustic interface using microphones is created as an emotional interaction channel for robotic assistants in domestic environments for the care of elderly people with mild cognitive impairment. Classification of communication signals such as striking, petting, speech and noise could be successfully realized using 3 condenser microphones and deployed on a robot prototype. The proposed framework comprised of an electronic microphone interface circuit, an ATmega128 microcontroller for processing of signals from the circuit, a Fast Fourier Transform (FFT) algorithm implementation on the microcontroller for Fourier analysis, LUFA library for USB data transfer from the microcontroller, a machine learning module based on Support Vector Machine (SVM) for signal classification and an amplifier based on an RFT transistor capsule for feedback. Results show that in all four cases, a successful differentiation with reliable and robust deviation between the signals can be made. The success rate with the SVM classifier shows with 87.5 % a highly promising classification result. It can be concluded that this novel approach is highly useful for non-verbal human-machine communication.",2015 Pattern Recognition Association of South Africa and Robotics and Mechatronics International Conference (PRASA-RobMech),2015.0,10.1109/ROBOMECH.2015.7359526,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
897fe76e777ee61edc4c1886a7cb37e0ca2471c7,https://www.semanticscholar.org/paper/897fe76e777ee61edc4c1886a7cb37e0ca2471c7,A Comparative Data Protection Analysis of Healthcare Robots (Part 1): Embodiment and Algorithms for HRI,"Population ageing is a critical social problem, which Europe and North America have been facing in recent years. Policymakers have introduced many options to solve this problem, including: creating additional positions for foreign caregivers to work, developing an elderly-friendly infrastructure in communities, or deploying robotics technology for supporting elderly people. Through the introduction of healthcare robots, they can support the elderly in walking, climbing, and getting out of bed independently, as well as non-physical support, such as daily communication to provide the elderly with necessary information or just to alleviate loneliness. However, by using robots, it will be easier to acquire large amounts of personal sensitive data and to provide healthcare information to patients repetitively. This may also cause new data protection issues such as (1) agent autonomy: How should we design a framework to avoid inappropriately stored data and be treated or transported by an intelligent agent? (2) Physical embodiment: We may see healthcare robots as the physical extension of an intelligent agent. Suppose there is something wrong about feeding data through the training process of machine learning, such as healthcare robots behaving abnormally, or in the worst case it might cause physical injury to patients. In other words, the consequences are safety critical. So far, no research has been done regarding the application of existing data protection laws to the emerging healthcare robotic technologies. In this chapter, we discuss the idea that the embodiment of an AI entity has particular significance for a law of algorithms because embodied forms of AI using algorithms may lead to the collection of personal information from users in social interactions. We note that “embodiment” as a feature of intelligent robots, has rarely been mentioned in the field of privacy and data protection law. Hence, in this chapter we investigate the relationship between embodied forms of systems that are controlled by algorithms and their effect on privacy and data protection law. As a technology which highlights the points we wish to make, we will focus on healthcare robots because they have the ability to engage humans in multiple ways using different types of social interactions. The motivation for the chapter is to determine how the use of an algorithmic driven system, such as a robot providing healthcare services and interacting socially with people, will influence privacy and data protection in humanrobot interactions.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7f21478983671eb3be62f15ab81760555b5bcdfd,https://www.semanticscholar.org/paper/7f21478983671eb3be62f15ab81760555b5bcdfd,A touchy subject : development and exploration of tactile sensing for perception and manipulation,"Demand for robotic solutions to problems in every area of society has been rapidly increasing. Some notable examples include providing independence and care to the elderly, managing degrading farm land, and reducing the risk to human life with agile robots for search and rescue. Many of these areas will require robots to either work alongside humans, or in environments that are unstructured. For these an artificial sense of touch will be crucial. Tactile sensing is a young field of research when compared to other sensing fields like computer vision. But just like computer vision, tactile sensing will open up important doors for robotics systems. Tactile provides an active sense for robots, allowing contact level perception on its influence on the world. It is this sensing and understanding of environmental influence that can be utilised for manipulation. This thesis presents novel developments in tactile sensing hardware, perception, and deployment. The work demonstrates the development of the TacTip tactile sensor to be better suited to rapid prototyping and complex morphologies. This is achieved by redeveloping the TacTip technology to be bot modular and 3D printable in a multi-material printer. Ultimately this allowed the for exploration the effects of biomimetic fingerprints on tactile perception of varies spatial scales. Demonstrating improvements in acuity of location perception with its inclusion. An investigation into using active perception algorithms for active manipulation is explored. Where the principle that existing algorithms that provide control for perception can be used such that perception for control is achieved. The work demonstrated the successful rolling of a cylinder on a table top using only tactile sensing, and highlights that the methods have a trade off between accuracy and reaction time. To improve the generality of the TacTip sensors tactile sensing, I present the development of a novel method for inferring a third dimension to the sensor data. This method deploys the mathematical principle of voronoi tessellation to the point data outputted from the sensor. This tessellation creates cells around each point, the areas of which can be interpolated to crate a 3D surface representation of the data. Ultimately, along with exploration of the raw point data, tactile features such as shear, pressure, and contact locations could be inferred with out the use of data intensive machine learning techniques. Lastly, this thesis present a fully tactile seven degree of freedom hand, fully equipped with the new TacTip developments and generalised feature inference. The hand was designed to be highly tactile, dexterous and relatively inexpensive tool. The hand is benchmarked on the YCB objectset with a closed loop adaptive grasp controller which demonstrates its viability for starting to explore tactile dexterous manipulation. Overall this thesis demonstrates developments in tactile sensing with accurate location perception, feature perception, simple manipulations, and grasp adaptation. All of these are components necessary for reaching the ultimate goal and bigger challenge of complex dexterous tactile manipulation.",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
11ec4a1719a330c3251bf56a43f14fb80a238bba,https://www.semanticscholar.org/paper/11ec4a1719a330c3251bf56a43f14fb80a238bba,Addressing Concerns about Responsibility,"Date of publication: 1 December 2014 set of technologies, loosely referred to as “artificial agents,” is becoming more pervasive and more powerful in the current computing landscape. All artificial agents are built on a computational foundation. Some are purely computational, e.g., Internet bots, search engines, and others are physically embodied entities with computational decision-making components, e.g., robots, unmanned aerial vehicles (UAVs), and autonomous cars. The noteworthy feature of artificial agents – the feature that leads to the artificial agent label – is their capacity to operate autonomously. At some level or to some degree, artificial agents operate independently from the humans who design and deploy them. They are agents in the sense that we deploy them to perform tasks on our behalf and often these tasks involve learning and decision-making. Since humans previously performed many of these tasks, we mark the difference, that is, the machine performance of these tasks, by referring to them as “artificial.” Responsibility issues are prominent in the discourse on artificial agents. Much attention has been given to the possibility that artificial agents might develop in ways that will make it impossible to hold humans responsible for their behavior. We believe that this concern misconstrues the situation and distracts attention from more important and more urgent issues. Rather than lamenting the possibility of artificial agents for which no one can be responsible, attention should be focused on how to develop artificial agents so as to ensure that humans can be responsible for their behavior. This involves attending to the optimal distribution of tasks among human and non-human (machine) components of artificial agent systems; appropriate designations of responsibilities to the humans operating in the system; and development and implementation of responsibility practices to support the assignments of tasks and responsibilities. As part of a National Science Foundation funded project (“Ethics for Developing Technologies: An Analysis of Artificial Agent Technology”), we developed a set of recommendations for the future development of artificial agents. An early version of the recommendations was presented to a small group of experts in the fields of robotics, computer science, philosophy, ethics, law, and policy. Based on the feedback received from these experts, the recommendations were revised. We present the final Recommendations for Future Development of Artificial Agents MEREL NOORMAN DEBORAH G. JOHNSON",,2014.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bbd04c1de5eac645ac70d04d0ea17055480c1fb3,https://www.semanticscholar.org/paper/bbd04c1de5eac645ac70d04d0ea17055480c1fb3,Robust servo-control for underwater robots using banks of visual filters,"We present an application of machine learning to the semi-automatic synthesis of robust servo-trackers for underwater robotics. In particular, we investigate an approach based on the use of Boosting for robust visual tracking of color objects in an underwater environment. To this end, we use AdaBoost, the most common variant of the Boosting algorithm, to select a number of low-complexity but moderately accurate color feature trackers and we combine their outputs. The novelty of our approach lies in the design of this family of weak trackers, which enhances a straightforward color segmentation tracker in multiple ways. From a large and diverse family of possible filters, we select a small subset that optimizes the performance of our trackers. The tracking process applies these trackers on the input video frames, and the final tracker output is chosen based on the weights of the final array of trackers. By using computationally inexpensive, but somewhat accurate trackers as members of the ensemble, the system is able to run at quasi real-time, and thus, is deployable on-board our underwater robot. We present quantitative cross-validation results of our spatio-chromatic visual tracker, and conclude by pointing out some difficulties faced and subsequent shortcomings in the experiments we performed, along with directions of future research in the area of ensemble tracking in real-time.",2009 IEEE International Conference on Robotics and Automation,2009.0,10.1109/ROBOT.2009.5152197,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ceefe076d1e96b065f60075d086d2c7bf99b8319,https://www.semanticscholar.org/paper/ceefe076d1e96b065f60075d086d2c7bf99b8319,"Taking a biologically inspired approach to the design of autonomous , adaptive machines","I l l u s t r a t I o n b y J u s t I n M e t z The auTomaTed design, construction, and deployment of autonomous and adaptive machines is an open problem. Industrial robots are an example of autonomous yet nonadaptive machines: they execute the same sequence of actions repeatedly. Conversely, unmanned drones are an example of adaptive yet non-autonomous machines: they exhibit the adaptive capabilities of their remote human operators. To date, the only force known to be capable of producing fully autonomous as well as adaptive machines is biological evolution. In the field of evolutionary robotics, one class of population-based metaheuristics—evolutionary algorithms—are used to optimize some or all aspects of an autonomous robot. The use of metaheuristics sets this subfield of robotics apart from the mainstream of robotics research, in which machine learning algorithms are used to optimize the control policy of a robot. As in other branches of computer science the use of a metaheuristic algorithm has a cost and a benefit. The cost is that it is not possible to guarantee if (or when) an optimal control policy will be found for a given robot. The benefit is few assumptions must be made",,2013.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8f2a47cd8a04aa7985d36c1106b01f35290be30f,https://www.semanticscholar.org/paper/8f2a47cd8a04aa7985d36c1106b01f35290be30f,Development of 80- and 100- Mile Work Day Cycles Representative of Commercial Pickup and Delivery Operation,"When developing and designing new technology for integrated vehicle systems deployment, standard cycles have long existed for chassis dynamometer testing and tuning of the powertrain. However, to this day with recent developments and advancements in plug-in hybrid and battery electric vehicle technology, no true “work day” cycles exist with which to tune and measure energy storage control and thermal management systems. To address these issues and in support of development of a range-extended pickup and delivery Class 6 commercial vehicle, researchers at the National Renewable Energy Laboratory in collaboration with Cummins analyzed 78,000 days of operational data captured from more than 260 vehicles operating across the United States to characterize the typical daily performance requirements associated with Class 6 commercial pickup and delivery operation. In total, over 2.5 million miles of realworld vehicle operation were condensed into a pair of duty cycles, an 80-mile cycle and a 100-mile cycle representative of the daily operation of U.S. class 3-6 commercial pickup and delivery trucks. Using novel machine learning clustering methods combined with mileage-based weighting, these composite representative cycles correspond to 90th and 95th percentiles for daily vehicle miles traveled by the vehicles observed. In addition to including vehicle speed vs time drive cycles, in an effort to better represent the environmental factors encountered by pickup and delivery vehicles operating across the United States, a nationally representative grade profile and key status information were also appended to the speed vs. time profiles to produce a “work day” cycle that captures the effects of vehicle dynamics, geography, and driver behavior which can be used for future design, development, and validation of technology. Introduction Under DOE-FOA-0001349 FY15 Award for Mediumand Heavy-Duty Vehicle Powertrain Electrification, Cummins and PACCAR jointly proposed the development of a range-extending plug-in hybrid electric Class 6 pickup and delivery truck. The goal of this project is to demonstrate an electrified vehicle that would deliver a minimum of 50% reduction in fuel consumption across a range of representative drive cycles. In addition to achieving the 50% fuel reduction target, the vehicle also needs to demonstrate as good or better drivability and performance while still meeting emissions requirements when compared to existing conventionally fueled baseline vehicles. Most existing duty cycles used to test conventional internal combustion powered vehicles are of a limited time duration. For example, the Hybrid Truck Utility Forum Class 6 Pickup and Delivery cycle is slightly more than one hour. When testing a system using only fuel as its energy source, this is acceptable; a onehour duty cycle can be used to represent the vehicle operation for the entire work day (e.g., fuel consumption in the middle of the day is very similar to fuel consumption at the end of the day). However, with plug-in electric vehicles, the system (battery characteristics and thermal management systems) may operate differently throughout the work day (especially near the end of the day). For example, the available battery energy may be completely spent prior to the completion of the route. A short duty cycle cannot simply be extrapolated. Evaluating the vehicle over the entire work day also provides the ability to interject appropriate stops that are typical of the Class 6-7 pickup and delivery application. These stops can range from several minutes to much longer and can have significant thermal effect on the vehicle and powertrain systems. These stops may also have a large impact on overall duty cycle mileage (and other duty cycle characteristics such as average speed) as the stops may account for roughly half of the work day. As part of the research and development team, the National Renewable Energy Laboratory (NREL) was been NREL/CP-5400-70943. Posted with permission. Presented at WCX 18: SAE World Congress Experience, 10-12 April 2018, Detroit, Michigan.",,2018.0,10.4271/2018-01-1192,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
31531d9fb3dff86961dbefdc8932fb4153ed293d,https://www.semanticscholar.org/paper/31531d9fb3dff86961dbefdc8932fb4153ed293d,Learning to Efficiently Pursue Communication Goals on the Web with the GOSMR Architecture,"
 
 We present GOSMR (""goal oriented scenario modeling robots""), a cognitive architecture designed to show coordinated, goal-directed behavior over the Internet, focusing on the web browser as a case study. The architecture combines a variety of artificial intelligence techniques, including planning, temporal difference learning, elementary reasoning over uncertainty, and natural language parsing, but is designed to be computationally lightweight. Its intended use is to be deployed on virtual machines in large-scale network experiments in which simulated users' adaptation in the face of resource denial should be intelligent but varied. The planning system performs temporal difference learning of action times, discounts goals according to hyperbolic discounting of time-to-completion and chance of success, takes into account the assertions of other agents, and separates abstract action from site-specific affordances. Our experiment, in which agents learn to prefer a social networking style site for sending and receiving messages, shows that utility-proportional goal selection is a reasonable alternative to Boltzmann goal selection for producing a rational mix of behavior.
 
",AAAI,2013.0,10.1609/aaai.v27i1.8469,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d71b10dd0ae6b5ed4668a91d67b98ea188fd6743,https://www.semanticscholar.org/paper/d71b10dd0ae6b5ed4668a91d67b98ea188fd6743,Competitive Benchmarking: Lessons Learned from the Trading Agent Competition,"Many important developments in artificial intelligence have been stimulated by organized competitions that tackle interesting, difficult challenge problems, such as chess, robot soccer, poker, robot navigation, stock trading, and others. Economics and artificial intelligence share a strong focus on rational behavior. Yet the real-time demands of many domains do not lend hemselves to traditional assumptions of rationality. This is the case in many trading environments, where self-interested entities need to operate subject to limited time and information. With the web mediating an ever broader range of transactions and opening the door for participants to concurrently trade across multiple markets, there is a growing need for technologies that empower participants to rapidly evaluate very large numbers of alternatives in the face of constantly changing market conditions. AI and machine-learning techniques, including neural networks and genetic algorithms, are already routinely used in support of automated trading scenarios. Yet, the deployment of these technologies remains limited, and their proprietary nature precludes the type of open benchmarking that is critical for further scientific progress.",AI Mag.,2012.0,10.1609/aimag.v33i2.2396,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6e0288b874320b1b6461016fde8b215c3ba46b90,https://www.semanticscholar.org/paper/6e0288b874320b1b6461016fde8b215c3ba46b90,Recognising activities by jointly modelling actions and their effects,"With the rapid increase in adoption of consumer technologies, including inexpensive but powerful hardware, robotics appears poised at the cusp of widespread deployment in human environments. A key barrier that still prevents this is the machine understanding and interpretation of human activity, through a perceptual medium such as computer vision, or RBG-D sensing such as with the Microsoft Kinect sensor. This thesis contributes novel video-based methods for activity recognition. Specifically, the focus is on activities that involve interactions between the human user and objects in the environment. Based on streams of poses and object tracking, machine learning models are provided to recognize various of these interactions. The thesis main contributions are (1) a new model for interactions that explicitly learns the human-object relationships through a latent distributed representation, (2) a practical framework for labeling chains of manipulation actions in temporally extended activities and (3) an unsupervised sequence segmentation technique that relies on slow feature analysis and spectral clustering. These techniques are validated by experiments with publicly available data sets, such as the Cornell CAD-120 activity corpus which is one of the most extensive publicly available such data sets that is also annotated with ground truth information. Our experiments demonstrate the advantages of the proposed methods, over and above state of the art alternatives from the recent literature on sequence classifiers.",,2015.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b0d52fc5c3c6797b77ba4c2347cf1f78a0d83ce4,https://www.semanticscholar.org/paper/b0d52fc5c3c6797b77ba4c2347cf1f78a0d83ce4,Cognitive Systems: Cognitive Systems Monographs,"Design of cognitive systems for assistance to people poses a major challenge to the fields of robotics and artificial intelligence. The Cognitive Systems for Cognitive Assistance (CoSy) project was organized to address the issues of i) theoretical progress on design of cognitive systems ii) methods for implementation of systems and iii) empirical studies to further understand the use and interaction with such systems. To study, design and deploy cognitive systems there is a need to considers aspects of systems design, embodiment, perception, planning and error recovery, spatial insertion, knowledge acquisition and machine learning, dialog design and human robot interaction and systems integration. The CoSy project addressed all of these aspects over a period of four years and across two different domains of application exploration of space and task / knowledge acquisition for manipulation. The present volume documents the results of the CoSy project. The CoSy project was funded by the European Commission as part of the Cognitive Systems Program within the 6th Framework Program.",,2010.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
defdb2b393c467cef465af4bf46f374650468050,https://www.semanticscholar.org/paper/defdb2b393c467cef465af4bf46f374650468050,Rapid Human-Computer Interactive Conceptual Design of Mobile and Manipulative Robot Systems,"Abstract : This project consists of the development of a software application for the user-guided design of a robotic system in conjunction with a computer-automated optimization system (Fig. 1). In Year 1 (Feb, 2011 - Feb 2012) we demonstrated an initial system for allowing a human user to configure a robot, its environment and to specify a task. In addition, we also demonstrated a paradigm for allowing a human-user to guide an automated optimization / machine learning system to collaborate in solving a problem. In Year 2 (Feb 2012 - Feb 2013) we integrated these two parts into a prototype desktop application and concluded the year with testing the effectiveness of this approach. In year 3 we extended our system so that it can support the crowd-sourcing of robotics by non-expert users: multiple users collectively influence an optimization method (Fig. 2). This final report is organized as follows. Section 2 is an overview of this project and summarizes its goals. Section 3 describes the successful crowdsourcing of robotics: we have shown that, when two non-experts interact with our robotics system, robot controllers are developed more rapidly compared to two users acting independently, or one user working alone. The main new deliverables are the submission of two manuscripts that documents the design, deployment and results from crowdsourcing robotics. Work described in the first manuscript was summarized in the previous QPR. Worked described in the second manuscript are summarized in this Report.",,2015.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
64b909f54c31d88e7ab3787d3aae976883cd14e2,https://www.semanticscholar.org/paper/64b909f54c31d88e7ab3787d3aae976883cd14e2,Final Report : Rapid Human-Computer Interactive Conceptual Design of Mobile and Manipulative Robot Systems Report Title,"Number of Papers published in peer-reviewed journals: Number of Papers published in non peer-reviewed journals: Final Report: Rapid Human-Computer Interactive Conceptual Design of Mobile and Manipulative Robot Systems Report Title This project consists of the development of a software application for the user-guided design of a robotic system in conjunction with a computer-automated optimization system (Fig. 1). In Year 1 (Feb, 2011 Feb 2012) we demonstrated an initial system for allowing a human user to configure a robot, its environment and to specify a task. In addition, we also demonstrated a paradigm for allowing a human-user to guide an automated optimization / machine learning system to collaborate in solving a problem. In Year 2 (Feb 2012 Feb 2013) we integrated these two parts into a prototype desktop application and concluded the year with testing the effectiveness of this approach. In year 3 we extended our system so that it can support the crowd-sourcing of robotics by non-expert users: multiple users collectively influence an optimization method (Fig. 2). This final report is organized as follows. Section 2 is an overview of this project and summarizes its goals. Section 3 describes the successful crowdsourcing of robotics: we have shown that, when two non-experts interact with our robotics system, robot controllers are developed more rapidly compared to two users acting independently, or one user working alone. The main new deliverables are the submission of two manuscripts that documents the design, deployment and results from crowdsourcing robotics. Work described in the first manuscript was summarized in the previous QPR. Worked described in the second manuscript are summarized in this Report. (a) Papers published in peer-reviewed journals (N/A for none) Enter List of papers submitted or published that acknowledge ARO support from the start of the project to the date of this printing. List the papers, including journal references, in the following categories: (b) Papers published in non-peer-reviewed journals (N/A for none) (c) Presentations Received Paper",,2015.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4fd164ceef598a7082b8e712d802f09c300df5a6,https://www.semanticscholar.org/paper/4fd164ceef598a7082b8e712d802f09c300df5a6,Segmental Deployment of Neural Network in Cloud Robotic System,"In this paper, we describe a new method for ep neural networks in the field of computer vision, which can effectively solve the difficulty of applying deep learning in the cloud robotic system. By segmenting the trained network, most of the computing tasks can be cut out and offloaded to the cloud. By effective feature extraction and compression methods, the computing power of robot and cloud can be integrated and coordinated. A method of selecting the split points of the network model and a method of data transmission and compression in the communication between robots and cloud after segmenting are given based on the characteristics of machine vision tasks, and the theoretical analysis is carried out. In the experiment, the effectiveness of all the above methods is verified by comparing the compression capability, response time and network performance of the actual network model. The experimental results show that with the use of segmental methods in cloud robotic system, the task of deep network is processed in real time, while the performance is almost guaranteed.",2018 IEEE 3rd International Conference on Cloud Computing and Internet of Things (CCIOT),2018.0,10.1109/CCIOT45285.2018.9032441,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9bfd438cf25a6c63f16e3bfa17cb5aed0de8d75b,https://www.semanticscholar.org/paper/9bfd438cf25a6c63f16e3bfa17cb5aed0de8d75b,Sensitivity Analysis For Building Evolving & Adaptive Robotic Software,"There has been a considerable growth in research and development of service robots in recent years. For deployment in diverse environment conditions for a wide range of service tasks, novel features and algorithms are developed and existing ones undergo change. However, developing and evolving the robot software requires making and revising many design decisions that can affect the quality of performance of the robots and that are non-trivial to reason about intuitively because of interactions among them. We propose to use sensitivity analysis to build models of the quality of performance to the different design decisions to ease design and evolution. Moreover, we envision these models to be used for run-time adaptation in response to changing goals or environment conditions. Constructing these models is challenging due to the exponential size of the decision space. We build on previous work on performance influence models of highly-configurable software systems using a machine-learning-based approach to construct influence models for robotic software.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b50d5ecc3e581cf6e0cfbc4c313e0cff78dd2a63,https://www.semanticscholar.org/paper/b50d5ecc3e581cf6e0cfbc4c313e0cff78dd2a63,Developing an Autonomous Swarm of Small Helicopters: Controlling Cooperative Team Behaviour for Search and Surveillance,"This work describes the components for controlling an autonomous swarm of small helicopters to execute search and surveillance tasks. The proposed system will be designed to support and be deployed by land forces. The aim is that the swarm can operate fully autonomously and use cooperative team strategies based on decentralized multiagent system control. The different components of the system are investigated by an interdisciplinary team in the Newcastle Robotics Lab comprising experts from computer engineering, computer science, computer vision, control, electrical engineering, machine learning, mechatronics, optimisation, wireless communications, signal processing and software engineering. The proposed system can help to reduce risk in land force missions that require search and exploration of dangerous or unknown environments.",,2013.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6cfc6c1694821d96728f70bdce39667b34623512,https://www.semanticscholar.org/paper/6cfc6c1694821d96728f70bdce39667b34623512,Incremental Adversarial Domain Adaptation,"Continuous appearance shifts such as changes in weather and lighting conditions can impact the performance of deployed machine learning models. Unsupervised domain adaptation aims to address this challenge, though current approaches do not utilise the continuity of the occurring shifts. Many robotic applications exhibit these conditions and thus facilitate the potential to incrementally adapt a learnt model over minor shifts which integrate to massive differences over time. Our work presents an adversarial approach for lifelong, incremental domain adaptation which benefits from unsupervised alignment to a series of sub-domains which successively diverge from the labelled source domain. We demonstrate on a drivable-path segmentation task that our incremental approach can better handle large appearance changes, e.g. day to night, compared with a prior single alignment step approach. Furthermore, by approximating the marginal feature distribution for the source domain with a generative adversarial network, the deployment module can be rendered fully independent of retaining potentially large amounts of the related source training data for only a minor reduction in performance.",ArXiv,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0cfcc755f0290ee1110f5eb1fb61f48104a85f0d,https://www.semanticscholar.org/paper/0cfcc755f0290ee1110f5eb1fb61f48104a85f0d,Perspectives on Ethics of AI : Computer Science ∗,"AI is a collection of computational methods for studying human knowledge, learning, and behavior, including by building agents able to know, learn, and behave. Ethics is a body of human knowledge, far from completely understood, that helps agents (humans today, but perhaps eventually robots and other AIs) decide how they and others should behave. The ethical issues raised by AI fall into two overlapping groups. First, potential deployments of AI raise ethical questions about the impacts they may have on human well-being, just like other powerful tools or technologies such as nuclear power or genetic engineering. Second, unlike other technologies, intelligent robots and other AIs have the potential to be considered as members of our society. Since they will make their own decisions about the actions they take, it is appropriate for humans to expect them to behave ethically. This requires AI research with the goal of understanding the structure, content, and purpose of ethical knowledge, well enough to implement ethics in artificial agents. This chapter describes a computational view of the function of ethics in human society, and discusses its application to three diverse examples. ∗Draft chapter for the Oxford Handbook of Ethics of AI, edited by Markus Dubber, Frank Pasquale, and Sunit Das, to appear, 2019. †kuipers@umich.edu. Computer Science & Engineering, University of Michigan, Ann Arbor, Michigan 48109 USA 1 Why Is the Ethics of AI Important? AI uses computational methods to study human knowledge, learning, and behavior, in part by building agents able to know, learn, and behave. Ethics is a body of human knowledge that helps agents (humans today, but perhaps eventually robots and other AIs) decide how they and others should behave. The ethical issues raised by AI fall into two overlapping groups. First, like other powerful tools or technologies (e.g., genetic engineering or nuclear power), potential deployments of AI raise ethical questions about their impact on human well-being. Second, unlike other technologies, intelligent robots (e.g., autonomous vehicles) and other AIs (e.g., high-speed trading systems) make their own decisions about the actions they take, and thus could be considered as members of our society. Humans should be able to expect them to behave ethically. This requires AI research with the goal of understanding the function, structure, and content of ethical knowledge well enough to implement ethics in artificial agents. As the deployment of AI, machine learning, and intelligent robotics becomes increasingly widespread, these problems become increasingly urgent. 2 What is the Function of Ethics? “At the heart of ethics are two questions: (1) What should I do?, and (2) What sort of person should I be?”1 Ethics consists of principles for deciding how to act in various circumstances, reflecting what is right or wrong (or good or bad) to do in that situation. It is clear that ethics (and hence what is considered right or wrong, or good or bad) changes significantly over historical time. Over similarly long historical time-scales, despite discouraging daily news reports, it appears that the societies of our world are becoming stronger, safer, healthier, wealthier, and more just and inclusive for their members.2 Two important sources of concepts help make sense of these changes. First, game theory contributes the abstraction of certain types of interactions among people as games3, and behavioral economics shows that these games not only have winners and losers, but the overall impact on the players collectively can be described as positive-sum, zero-sum, or negative-sum.4 Second, the theory of evolution, as applied to human and great ape cognition and sociality, shows how a way of life that depends on positive-sum cooperation among individuals is likely to 1 Russ Shafer-Landau, editor. Ethical Theory: An Anthology. Wiley-Blackwell, second edition, 2013. p. xi. 2 Robert Wright, Nonzero: The Logic of Human Destiny, Pantheon, 2000. Steven Pinker, The Better Angels of Our Nature: Why Violence Has Declined, Viking Adult, 2011. Steven Pinker, Enlightenment Now: The Case for Reason, Science, Humanism, and Progress, Viking, 2018. 3 John von Neumann and Oskar Morgenstern. Theory of Games and Economic Behavior. Princeton University Press, 1953. 4 Samuel Bowles, The Moral Economy: Why Good Incentives are No Substitute for Good Citizens. Yale University Press, 2016. provide for its society greater fitness than less cooperative ways of life.5 We can therefore think of the function of ethics as promoting the survival and thriving of the society by influencing the behavior of its individual members, summarized as: Ethics is a set of beliefs that a society conveys to its individual members, to encourage them to engage in positive-sum interactions and to avoid negative-sum interactions. As a society prospers, survives, and thrives, its individual members benefit as well, so ethical behavior is “non-obvious self-interest” for the individual. Philosophers would consider this to be a rule consequentialist position6, but one where the relevant consequences are the survival and thriving of society, not the pleasures and pains of its individual members. It is consequentialist because actions are not evaluated according to whether they are intrinsically right or wrong (by some criterion), but according to their long-term good or bad consequences for the survival and thriving of society. This position is rule consequentialism because the unit that is evaluated is not the individual action decision, but the set of ethical principles (often rules) adopted by society. Positive-sum and negative-sum interactions. Commerce and cooperation are paradigm positivesum interactions. When one person voluntarily trades or sells something to someone else, each party receives something that they value more highly than what they gave. When cooperating on a project, partners contribute toward a common goal, and reap a benefit greater than either could achieve alone. Theft and violence are examples of negative-sum interactions. The thief gains something from the theft, but the loss to the victim is typically greater than the gain to the thief. Violent conflict is the paradigm negative-sum interaction, since both parties may be worse off afterwards than before, possibly much worse off. (These are not cleanly separated cases. Violence in defense against external attack may be necessary to avoid a catastrophic outcome, and that defense itself is likely to be a cooperative project.) Cooperation, trust, and social norms. Cooperative projects among individuals are a major source of positive-sum outcomes. However, cooperation requires vulnerability, and trust that the vulnerability will not be exploited.7 Trust is a psychological state comprising the intention to accept vulnerability based on positive expectations of the intentions or behavior of another.8 5 Michael Tomasello. A Natural History of Human Morality. Harvard University Press, 2016. 6 Walter Sinnott-Armstrong. Consequentialism. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Winter 2015 edition, 2015. 7 Michael Tomasello. A Natural History of Human Morality. Harvard University Press, 2016. 8 D. M. Rousseau, S. B. Sitkin, R. S. Burt, and C. Camerer. Not so different after all: a cross-discipline view of trust. Academy of Management Review, 23(3):393–404, 1998. As intelligent robots or large corporations increasingly act as autonomous goal-seeking agents and therefore as members of our society, then they, too, need to be subject to the requirements of ethics, and need to demonstrate that they can trust and be trustworthy. Successful cooperation demonstrates the trustworthiness of the partners and produces more trust while exploitation reduces trust. By trusting each other enough to pool their resources and efforts, individuals working together can often achieve much more than the sum of their individual efforts working separately. Large cooperative projects, from raising a barn, to digging a canal, to creating an Interstate Highway System, produce large benefits for everyone. But if I spend a day helping raise your barn, I trust that in due time, you will spend a day helping to raise mine. And if taxes help pay for New York’s Erie Canal or the Pennsylvania Turnpike, I trust that, in due time, taxes will also pay for the Panama Canal linking the East and West Coasts, and the St. Lawrence Seaway providing access to the Great Lakes. Some of the states in the USA emphasize this with the name “Commonwealth”, meaning that shared resources provide shared prosperity. Social norms are behavioral regularities that we as individual members of society can generally count on when planning our activities. By trusting these (near) invariants, many aspects of our lives become simpler, more efficient, and less risky and uncertain. Maintaining a social norm is a kind of cooperative project without specified partners. I accept certain minor sacrifices in return for similar behaviors by (almost) everyone else, providing a (near) invariant that we all can rely on. For example, when having lunch at a cafe, condiments are freely available for my convenience, but I know not to pocket the extras, so they will continue to be available. Likewise, I trust that a simple painted stripe in the middle of a road I am driving on securely separates me from drivers going in the opposite direction, so I accept the minor sacrifice of not crossing that stripe even when my side is congested. Like explicit cooperative projects, social norms provide positive-sum results for society, saving resources that would otherwise go toward protection and recovery, making us individually and collectively better off. Each requires trust: acceptance of vulnerability to the other partners, along with confidence that few others will exploit that vulnerability, even for individual gain. I use the term “social norm” in",,2019.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
af01fce6e2424b02da5a78c88c09b3688846661d,https://www.semanticscholar.org/paper/af01fce6e2424b02da5a78c88c09b3688846661d,"Migrating robot control systems, towards the universality of robotic brains","Currently robot control systems are specifically designed, engineered and fine tuned for particular problems on particular robots. This leads to a significant waste of man-hours of engineer and Phd level work to implement and reimplement or adapt controllers for similar tasks on different robots resulting in an inefficient robotics industry as a whole. Thus the need to automate or at least semi-automate controller reusability arises. In this project we investigate the hurdles that need to be overcome in attaining controller universality and look into possible methods to bootstrap controllers to the different robot sensors and actuators. A case study was conducted on performing the migration of a controller from a wheeled robot with no mobile vision system (The Pioneer robot) to a legged robot with a mobile head mounted camera (An Aldebaran Nao). The two robots’ different modalities makes the task challenging. What does it mean for two different robots to perform the same task? Machine learning methods were deployed using artificial neural networks (ANN) to learn the entire sensor abstraction - decision system - robot motor API tree, leaving just sensor feature extraction and low level motor controls in the hands of engineers. The method works reasonably well, effectively linking a number of controllers designed for a Pioneer onto the Nao’s sensors and actuators. While preliminary, these methods provide insight into the future prospects of robots programming themselves and learning from each other with the help of humans.",,2012.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5a3d4cb13073bb89087b32e4bc4842c0c4443751,https://www.semanticscholar.org/paper/5a3d4cb13073bb89087b32e4bc4842c0c4443751,Technical Commentary,"Big data combined with machine learning has revolutionized fields such as computer vision, robotics, and natural language processing. In these fields, automated techniques that detect and exploit complex patterns hidden within large data sets have repeatedly outperformed techniques based on human insight and intuition. But despite the availability of enormous amounts of code (big code) that could, in theory, be leveraged to deliver similar advances for software, programming has proved to be remarkably resistant to this kind of automation. Much programming today consists of developers deploying keyword searches against online information aggregators such as Stack Overflow to find, then manually adapt, code sequences that implement desired behaviors. The current paper presents new techniques for leveraging big code to automate two programming activities: 1) selecting understandable names for JavaScript identifiers and 2) generating type annotations for JavaScript variables. The basic approach leverages large JavaScript code bases to build a probabilistic model that predicts names and type annotations given the surrounding context (which includes constants, JavaScript API calls, and variable uses in JavaScript expressions and statements). When run on programs with the original variable names obfuscated, the implemented system was able to recover the original variable names over 60% of the time. The results for type annotations are even more intriguing — the implemented system generates correct type annotations for over half of the benchmark programs. For comparison, the programmer-provided annotations are correct for only a bit over a quarter of these programs. The system is accessible via the Internet at jsnice.org with hundreds of thousands of users. These results demonstrate how this approach can help JavaScript programmers produce more easily readable and understandable programs. One potential longer range consequence could be the gradual emergence of a de facto standard for aspects of JavaScript programs such as variable names and the relationship between program structure and types. More broadly, the results also highlight the substantial redundancy present in JavaScript code worldwide and raise questions about just how much human effort is really required to produce this code. So why was this research so successful? First, the investigators chose a problem that was a good fit for machine learning over big code. Current machine learning techniques do not provide correct results; they instead only provide results that look like previous results in the training set. A variable name or type annotation predictor does not have to always be correct; it only needs to be correct enough of the time to be useful. And JavaScript programs share enough variable name and type annotation patterns to support a reasonably accurate model. A second reason is technical, specifically the development of a program representation that exposes relevant relationships between variables and the surrounding context, including how variables are used in JavaScript statements and expressions. Features exposed in this program representation enable the immediate application of conditional random fields, a standard technique in machine learning for structured prediction previously shown to be effective for solving problems in areas such as natural language processing and computer vision, to solve the learning and prediction problem. The development of a new approximate MAP inference algorithm for this domain enables the performance required for interactive use when working with thousands of labels per node (in contrast to many previous applications, which only work with tens of labels per node). So what can we expect to see in the future from this line of research? The most obvious next steps include a variety of automated programming assistants for tasks such as code search, code completion, and automatic patch generation. Here the assistant would interact with the programmer to guide the process of turning vague, uncertain, or underspecified goals into partially or fully realized code, with programmer supervision required to complete and/or ensure the correctness of the resulting code. It is less clear how to make progress on programming tasks with more demanding correctness, autonomy, or novelty requirements. One critical step may be finding productive ways to integrate probabilistic reasoning with more traditional logical reasoning as applied to computer programs. Future research, potentially inspired in part by the results presented in this paper, will determine the feasibility of this goal.",,2012.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b2236948408ebc6891331014e1be1b447a38794f,https://www.semanticscholar.org/paper/b2236948408ebc6891331014e1be1b447a38794f,Learning to Manipulate Objects with a Human Teacher,"Our research is motivated by the promise of robots that operate in human environments to assist people on a daily basis. It is not feasible to pre-program these robots will every skill needed, they will have to adapt and learn after they are deployed. In our view, these robots should be able to learn on their own–discovering new skills and mastering known skills through practice. Additionally, they must be able to learn from everyday people who are not familiar with robotic systems or Machine Learning algorithms. Importantly, these two “modes” of learning (social and non-social) are not mutually exclusive, but can each provide a different kind of learning opportunity for the robot. In this project we study the differences in social and non-social learning of object affordances. Does a human teacher make affordance learning faster or more efficient? What strategies to people employ to scaffold the robot’s learning process? There are several prior works aim to let a human teach skills to a robot by demonstration (Atkeson and Schaal 1997; Kuniyoshi, Inaba, and Inoue 1994; Nicolescu and Matarić 2003), or with high-level feedback to an exploratory learner (Kaplan et al. 2002; Saksida, Raymond, and Touretzky 1998). In our own prior work (Thomaz and Breazeal 2008), we began to explore the mutually beneficial relationship between self-exploration and guided learning. In the work presented here, we are working to give the teacher an even more interactive role in the teaching process, rather than simply providing verbal feedback and guidance.",,2008.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
72fdb40f49eb0a79ee9bd955369aa77deb21b63a,https://www.semanticscholar.org/paper/72fdb40f49eb0a79ee9bd955369aa77deb21b63a,Multilayer General Value Functions for Robotic Prediction and Control,"Predictions are a key component to intelligence and necessary for accurate motor control. In reinforcement learning, such predictions can be made through general value functions (GVFs). This paper introduces prosthetic arms as a domain for artificial intelligence and discusses the role that predictions play in prosthetic limb control. We explore the use of multilayer predictions, that is, predictions based on predictions, using robotic and simulation experiments. From these experiments two observations are made. The first is that compound predictions based on GVFs are viable in a robotic setting. The second, is that strong GVF predictors can be built from weaker ones with different input and target signals, similar to boosting. Finally, we theorize how such topologies might be used in transfer learning and in the simultaneous control of multiple actuators. Our approach to integrating machine intelligence with robotics has the potential to directly improve the real-world performance of bionic limbs. I. GENTLE INTEGRATION When combining machine intelligence systems with electromechanical devices such as mobile or mounted robots, it is natural to think of the machine intelligence as providing most or all of the key aspects of the robot’s control system. Integration of this kind is often challenging—it simultaneously addresses many important barriers faced by our computing technology—but is incredibly fruitful for both the fields of robotics and artificial intelligence. Another, complementary approach is the use of machine intelligence to supplement an existing control system or sensorimotor interface. Machine learning and artificial intelligence (AI) can augment the capacity of existing systems in small but important ways. While more modest in its aims, this kind of staged deployment is well suited to the refined study of individual machine learning methods as they impact real-world domains of use. It further provides a smooth pathway to machine intelligence seeing practical use within complete, deployed systems. In this paper we look specifically at the second, more gentle approach to integrating machine intelligence within a robotic device. In particular, we highlight one area where our group has made recent progress: improving robotic artificial limbs (Fig. 1) through real-time learning and utilization of temporally extended predictions. This setting lends itself well to translating algorithmic and conceptual advances into tangible benefit within a deployed environment; machine learning can improve the ability of people with amputations to control Fig. 1. Augmentative and restorative prosthetics are of specific interest for incrementally integrating AI into a robotic setting. Top: commercial limb system prescribed to an amputee for use during daily life. Bottom: research robot limb system with direct access to a rich sensorimotor stream [4]. their bionic limbs. Sharing the challenges and opportunities of prosthetics as a domain for AI Robotics is the first contribution of our paper. We present a brief overview of our machine learning work within the prosthetic domain, and follow on this overview with a concrete example on a simple robotic platform of how real-time predictions can be beneficially combined into a learning hierarchy. Lastly, we discuss how multilayer predictions can be integrated back into prosthetic control approaches to further extend their practical reach.",,2014.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a1ff48ac0f13e0a76c7c91fdeac7e89840364413,https://www.semanticscholar.org/paper/a1ff48ac0f13e0a76c7c91fdeac7e89840364413,Robotic Antarctic meteorite search: outcomes,"Automation of the search for and classification of Antarctic meteorites offers a unique case for early demonstration of robotics in a scenario analogous to geological exploratory missions to other planets and to the Earth's extremes. Moreover, the discovery of new meteorite samples is of great value because meteorites are the only significant source of extraterrestrial material available to scientists. In this paper we focus on the primary outcomes and technical lessons learned from the first field demonstration of autonomous search and in situ classification of Antarctic meteorites by a robot. Using a novel autonomous control architecture, specialized science sensing, combined manipulation and visual servoing, and Bayesian classification, the Nomad robot classified five indigenous meteorites during an expedition to the remote site of Elephant Moraine in January 2000. Nomad's expedition proved the rudiments of science autonomy and exemplified the merits of machine learning techniques for autonomous geological classification in real-world settings. On the other hand, the expedition showcased the difficulty in executing reliable robotic deployment of science sensors and a limited performance in the speed and coverage of autonomous search.",Proceedings 2001 ICRA. IEEE International Conference on Robotics and Automation (Cat. No.01CH37164),2001.0,10.1109/ROBOT.2001.933270,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cce600d54ef32b3f6312b9166da097e0f9eded8a,https://www.semanticscholar.org/paper/cce600d54ef32b3f6312b9166da097e0f9eded8a,Designing a Visual Analytics System for Industry-Scale Deep Neural Network Models,"The complexity of industry-scale deep learning models and datasets pose unique design, visualization, and system challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have designed, developed, and deployed ACTIVIS, a visual analytics system for interpreting industry-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both instanceand subset-level. ACTIVIS has been deployed on Facebook’s machine learning platform. This article is a summary for the VAST’17 paper (TVCG track) ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models [2]. 1 DESIGNING FOR INDUSTRY-SCALE MODELS Despite the increasing interest in developing visualization tools for deep learning interpretation [5–7], the complexity of large-scale models and datasets used in industry pose unique design challenges that are inadequately addressed by existing work. For example, while most existing visualization tools target image datasets, deep learning tasks in industry often involve different types of data, including text and numerical data. Furthermore, in designing tools for realworld deployment, it is a high priority that the tools be flexible and scalable, adapting to the wide variety of models and datasets used. These observations motivate us to design and develop ACTIVIS [2], a visual analytics system for deep neural network models, now deployed on Facebook’s machine learning platform. Since the ACTIVIS project started in April 2016, we have conducted participatory design sessions with over 15 Facebook engineers, researchers, and data scientists across multiple teams to learn about their visual analytics needs. We identified six key design challenges — for data, model, and analytics — that have not been adequately addressed by existing deep learning visualization tools. The challenges include the need to support: (1) diverse input data sources, (2) high data volume, (3) complex model architecture, (4) a great variety of models, (5) diverse subset definitions for analytics, and (6) both instanceand subset-level analyses. These challenges shape the main design goals of ACTIVIS. 2 ACTIVIS CONTRIBUTIONS ACTIVIS’s main contributions include: • A novel visual representation that unifies instanceand subsetlevel inspections of neuron activation, facilitating comparison of activation patterns for multiple instances. *e-mail: kahng@gatech.edu †e-mail: mortimer@fb.com ‡e-mail: adityakalro@fb.com §e-mail: polo@gatech.edu • An interface that tightly integrates an overview of graph-structured complex models and local inspection of neuron activations, allowing users to explore the model at different levels of abstraction. • A deployed system scaling to large datasets and models. • Case studies with Facebook engineers and data scientists that highlight how ACTIVIS helps them with their work. ACTIVIS’s multiple coordinated views help users get a high-level overview of the model from which the user can drill down to perform localized inspection of activations. ACTIVIS visualizes how neurons are activated by user-specified instances or instance subsets, to help users understand how a model derives its predictions. The subsets can be flexibly defined using data attributes, features, or output results, enabling model inspection from multiple angles. While many existing deep learning visualization tools support instancelevel exploration [6, 7], ACTIVIS is the first tool that simultaneously supports instanceand subset-level exploration. Both exploration strategies are common and effective, and they offer complementary analytics benefits. Instance-based analysis instructs how individual instances contribute to a model’s accuracy, but it is tedious to inspect many instances one by one. Subset-based analysis leverages input features or instance subsets to help reveal relationships between data attributes and machine learning algorithms’ outputs [3]. It is especially beneficial when dealing with huge datasets in industry, which may consist of millions or billions of data points. By exploring instance subsets and enabling their comparison with individual instances, users can learn how them models respond to many different slices of the data. We refer our readers to the longer version of our ACTIVIS [2] VAST’17 paper published in IEEE Transactions on Visualization and Computer Graphics.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
65bbc3f19e1582a79e56874b0b3b1b6901ec2c19,https://www.semanticscholar.org/paper/65bbc3f19e1582a79e56874b0b3b1b6901ec2c19,"HUMAN-ROBOT ENGAGEMENT in the Home , Workplace and Public Spaces ORGANIZING COMMITTEE","The use of semi-autonomous and autonomous robotic assistants to aid in care of the elderly is expected to ease the burden on human caretakers, with small-stage testing already occurring in a variety of countries. Yet, it is likely that these robots will need to request human assistance via teleoperation when domain expertise is needed for a specific task. As deployment of robotic assistants moves to scale, mapping these requests for human aid to the teleoperators themselves will be a difficult online optimization problem. In this paper, we design a system that allocates requests to a limited number of teleoperators, each with different specialities, in an online fashion. We generalize a recent model of online job scheduling with a worst-case competitive-ratio bound to our setting. Next, we design a scalable machine-learning-based teleoperator-aware task scheduling algorithm and show, experimentally, that it performs well when compared to an omniscient optimal scheduling algorithm.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
769346d6a5fac8e9ae4b27c48b7f0ba8504e348f,https://www.semanticscholar.org/paper/769346d6a5fac8e9ae4b27c48b7f0ba8504e348f,Anticipating Noxious States from Visual Cues using Deep Predictive Models,"To ensure system integrity, robots need to proactively avoid any unwanted physical perturbation that may cause damage to the underlying hardware. In this paper, we investigate a machine learning approach that allows robots to anticipate impending physical perturbations from perceptual cues. In contrast to other approaches that require knowledge about sources of perturbation to be encoded before deployment, our method is based on experiential learning. Robots learn to associate visual cues with subsequent physical perturbations and contacts. In turn, these extracted visual cues are then used to predict potential future perturbations acting on the robot. To this end, we introduce a novel deep network architecture which combines multiple sub-networks for dealing with robot dynamics and perceptual input from the environment. We present a self-supervised approach for training the system that does not require any labeling of training data. Inference is perfomed using approximate Bayesian inference and moment matching. Extensive experiments in a human-robot interaction task show that a robot can learn to predict physical contact by a human interaction partner without any prior information or labeling. Furthermore, the network is able to successfully predict physical contact from either depth stream input or traditional video input.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
eac4c54ac750ca242222afd2196ddb8be2341df6,https://www.semanticscholar.org/paper/eac4c54ac750ca242222afd2196ddb8be2341df6,Empirical Data on Attitudes Toward Autonomous Systems,"Combat automation, enabled by rapid technological advancements in artificial intelligence and machine learning, is a guiding principle in the conduct of war today. Yet, empirical data on the impact of algorithmic combat on military personnel remains limited. This chapter draws on data from a historically unprecedented survey of Australian Defence Force Academy cadets. Given that this generation of trainees will be the first to deploy autonomous systems (AS) in a systematic way, their views are especially important. This chapter focuses its analysis on five themes: the dynamics of human-machine teams; the perceived risks, benefits, and capabilities of AS; the changing nature of (and respect for) military labor and incentives; preferences to oversee a robot, versus carrying out a mission themselves; and the changing meaning of soldiering. We utilize the survey data to explore the interconnected consequences of neoliberal governing for cadets’ attitudes toward AS, and citizen-soldiering more broadly. Overall, this chapter argues that Australian cadets are open to working with and alongside AS, but under the right conditions. Armed forces, in an attempt to capitalize on these technologically savvy cadets, have shifted from institutional to occupational employers. However, in our concluding remarks, we caution against unchecked technological fetishism, highlighting the need to critically question the risks of AS on moral deskilling, and the application of market-based notions of freedom to the military domain.",Lethal Autonomous Weapons,,10.1093/oso/9780197546048.003.0010,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
82f2bc9ef1bd5f1192260d858caa36a4e8b6a50d,https://www.semanticscholar.org/paper/82f2bc9ef1bd5f1192260d858caa36a4e8b6a50d,"The Quest For ""Always-On"" Autonomous Mobile Robots","Building ``always-on'' robots to be deployed over extended periods of time in real human environments is challenging for several reasons. 
Some fundamental questions that arise in the process include: 
1) How can the robot reconcile unexpected differences between its observations and its outdated map of the world? 
2) How can we scalably test robots for long-term autonomy?
3) Can a robot learn to predict its own failures, and their corresponding causes?
4) When the robot fails and is unable to recover autonomously, can it utilize partially specified, approximate human corrections to overcome its failures?
We summarize our research towards addressing all of these questions. 
We present 1) Episodic non-Markov Localization to maintain the belief of the robot's location while explicitly reasoning about unmapped observations; 2) a 1,000km challenge to test for long-term autonomy; 3) feature-based and learning-based approaches to predicting failures; and 4) human-in-the-loop SLAM to overcome robot mapping errors, and SMT-based robot transition repair to overcome state machine failures.",IJCAI,2019.0,10.24963/ijcai.2019/893,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0a3602363412ecff456c32754469e23f66fd04de,https://www.semanticscholar.org/paper/0a3602363412ecff456c32754469e23f66fd04de,Decision-Making under Uncertainty for Social Robots,"In this thesis, we explore the problem of learning social behaviour for a semi-autonomous telepresence robot. We extract social cost functions from instantaneous and delayed feedback obtained from realworld experiments, so as to establish a performance metric for the behaviour of our autonomous agent in a social setting. Based on the identified social cost functions, we extract additional insight regarding the normative behaviour of the agent as a response to perceived social cues, and discuss the relevant state features to be used for autonomous control in the task of social positioning during conversation. We also propose a discretized proxemics-based state space representation for this task given the feedback collected during our experiments, and perform a statistical analysis on the relevance of the state features in a group conversation setting. In the second part of this work, we compare and discuss different approaches to the problem of obtaining a control policy for our task of positioning of the robot during social conversation. This is a challenging task due to the limited information available to the agent during execution. Specifically, we study the performance as well as the ease of implementation of Partially Observable Markov Decision Processes (POMDPs) against a Long Short Term Memory (LSTM) recurrent neural network. We show by validating both methods on real world experiments that the LSTM outperforms the POMDP in our task, and offer explanations for that result which can help guide the deployment of these frameworks for more general problems of decision-making under uncertainty in the field of Robotics.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e53489a1ebaaa26c037de35acd493f2850010665,https://www.semanticscholar.org/paper/e53489a1ebaaa26c037de35acd493f2850010665,Educational outdoor mobile robot for trash pickup,"Machines in general and robots in particular, appeal greatly to children and youth. With the widespread availability of low-cost open source hardware and free open source software, robotics has become central to the promotion of STEM education in schools, and active learning at college/university level. With robots, children in developed countries gain from technological immersion, or exposure to the latest technologies and gadgets. Yet, developing countries like India still lag in the use of robots at school and even college level In this paper, an innovative and low-cost educational outdoor mobile robot is developed for deployment by school children during volunteer trash pickup. The wheeled mobile robot is constructed with inexpensive commercial off-the-shelf components, including single board computer and miscellaneous sensors. It is remote controlled by children using smart phone app or video game controller. The robot is also equipped with air quality sensors to display air pollution levels to the children and public. The project can improve STEM education, environmental literacy and civic engagement among school children, while providing interdisciplinary service learning opportunities for college and university students. Results of a prototype mobile robot design, development, and deployment are presented to illustrate the effectiveness of the approach.",2016 IEEE Global Humanitarian Technology Conference (GHTC),2016.0,10.1109/GHTC.2016.7857304,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e964d7b551135137b1bebc624adf0235a17fad23,https://www.semanticscholar.org/paper/e964d7b551135137b1bebc624adf0235a17fad23,A model based design approach to system building using the e-Yantra educational robot,"The e-Yantra robot is the basis for a highly scalable embedded systems teaching program setting up 500 embedded systems labs in Indian engineering colleges. A key strategy to encourage rapid prototyping of applications has been to encourage reuse of code using a commodity robot with a standard API along with excellent documentation and training material. An important challenge has been to teach the reasoning process from a design through to an implementation deployed on an actual machine. Model based design is key to articulating such reasoning. A further challenge is to do this in an affordable manner where most available model- based IDEs are expensive proprietary systems using languages such as Esterel and SCADE. We illustrate with a ""Valet Parking"" application how our robotic eco-system facilitates the learning of important model-based design principles taking a high-level specification of a problem down to working code and even deriving test cases in the process. A novel feature of our approach is that we carry out design-time scheduling of various (concurrent) activities by analyzing dependencies between modules and obtain purely sequential C-code implemented on a microcontroller without the need for an RTOS. This case study is an exemplar of a model-based design approach for a large class of such robotic projects.",SIGBED,2014.0,10.1145/3036686.3036691,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6edc440b44f37133c44c8004ade1cdb425547aa7,https://www.semanticscholar.org/paper/6edc440b44f37133c44c8004ade1cdb425547aa7,REACT : Physical Interactions Models of Companion Robots Supervisors :,"Robotics used for assistance to frail people draws nowadays a considerable interest in the domain of e-health thanks to the extended range of applications that it offers, together with the scientific and technological challenges that it brings forth. While the first generation of robots was conceived for the execution of repetitive and specific industrial tasks, the new generation introduces robots as artificial companions with cognitive and interactive skills allowing them to operate in open worlds. In particular, assistive service robots aim at helping people with disabilities due to age or sickness, to improve their independence et well-being at the long-term, while continuing to live within their social circle, instead of retirement institutions that tend to become increasingly costly and less available. In order to integrate a robot within living spaces, it has to be able to physically interact with its environment. We distinguish the interactions allowing for a robot to operate in its environment, such as stairs traversal and delivery/retrieval of objects. Given the diversity in forms and the robot sensing limits, a probabilistic approach with respect to the behaviour of the robot is required in terms of its interaction. This in turn should be based on the definition of probabilistic models able to capture the variance as well as the novelty, allowing for a robot to generalise its actions, or otherwise, retract. In this context, we propose in this thesis to study the introduction of new personal assistance services, by the definition and deployment of probabilistic models of physical interaction, between the robot and its environment. These models will serve two objectives: (1) render the operation of the robot more reliable from the user perspective and (ii) homogenise a given service across heterogeneous robots. The research to pursue in this context amounts to associating the actions of the robot with its effects on the environment, where the least controllable effects of an interaction are treated stochastically. The identification of these effects should finally allow to delimit the steps composing a robot service and to fine-tune the most controllable via the application of machine learning.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2b0052242fdec5f5ca4a0d350bc1ad0cdfa04b99,https://www.semanticscholar.org/paper/2b0052242fdec5f5ca4a0d350bc1ad0cdfa04b99,On the Run,"Young animals gallop across fields, climb trees and immediately find their feet with enviable grace after they fall. And like our primate cousins, humans can deploy opposable thumbs and fine motor skills to complete tasks such as effortlessly peeling a clementine or feeling for the correct key in a dark hallway. Although walking and grasping are easy for many living things, robots have been notoriously poor at gaited locomotion and manual dexterity. Until now. Writing in Science Robotics, Hwangbo et al. report intriguing evidence that a data-driven approach to designing robotic software could overcome a long-standing challenge in robotics and artificial-intelligence research called the simulation–reality gap. For decades, roboticists have guided the limbs of robots using software that is built on a foundation of predictive, mathematical models, known as classical control theory. However, this method has proved in effective when applied to the seemingly simple problem of guiding robotic limbs through the tasks of walking, climbing and grasping objects of various shapes. A robot typically begins its life in simulation. When its guiding software performs well in the virtual world, that software is placed in a robotic body and then sent into the physical world. There, the robot will inevitably encounter limitless, and difficult to predict, irregularities in the environment. Examples of such issues include surface friction, structural flexibility, vibration, sensor delays and poorly timed actuators — devices that convert energy into movement. Unfortunately, these combined nuisances are impossible to describe fully, in advance, using mathematics. As a result, even a robot that performs beautifully in simulation will stumble and fall after a few encounters with seemingly minor physical obstacles. Hwangbo et al. have demonstrated a way of closing this performance gap by blending classical control theory with machine-learning techniques. The team began by designing a conventional mathematical model of a medium-dog-sized quadrupedal robot called ANYmal (Fig. 1). Next, they collected data from the actuators that guide the movements of the robot’s limbs. They fed this information into several machine-learning systems known as neural networks to build a second model — one that could automatically predict the idiosyncratic movements of the AMYmal robot’s limbs. Finally, the team inserted the trained neural networks into its first model and ran the hybrid model in simulation on a standard desktop computer. The hybrid simulator was faster and more accurate than a simulator that was based on analytical models. But more importantly, when a locomotion strategy was optimized in the hybrid simulator, and then transferred into the robot’s body and tested in the physical world, it was as successful as it was in simulation. This long-overdue breakthrough signals the demise of the seemingly insurmountable simulation–reality gap. The approach used by Hwangbo et al. hints at another major shift in the field of robotics. Hybrid models are the first step towards this change. The next step will be to retire analytical models altogether, in favour of machine-learning models that are trained using data collected from a robot’s real-world environment. Such data-pure approaches — referred to as endto-end training — are gaining momentum. Several innovative applications have already been reported, including articulated robotic arms, multi-fingered mechanical hands, drones and even self-driving cars. For now, roboticists are still learning to harness the power of faster computation, an abundance of sensor data and improvements in the quality of machine-learning algorithms. It is not yet clear whether it is time for universities to stop teaching classical control theory. However, I think that the writing is already on the wall: future roboticists will no longer tell robots how to walk. Instead, they will let robots learn on their own, using data that are collected from their own bodies. Many challenges remain, of course, and chief among them is the challenge of scalability. So far, end-to-end training has been applied to physical robots that have only a small number of actuators. The fewer the actuators, the fewer the parameters that are needed to describe the robot’s movements, and therefore the simpler the model is. The path to scalability will probably involve the use of more-hierarchical and modular machine-learning architectures. Further research needs to be done to learn whether end-to-end control can be scaled up to guide complex machines that have dozens of actuators, including humanoid robots, or large systems such as manufacturing plants or smart cities — urban areas that use digital technology to improve the lives of citizens. Another challenge is less technical and more personal. For some researchers, the transition from using relatively simple mathematical models to applying ‘black box’ machine-learning systems — in which the internal workings are unknown — signals the A R T I F I C I A L I N T E L L I G E N C E",,2015.0,10.1525/california/9780520282803.003.0003,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
08178c93f689086ccce348963cf808a47649a6e8,https://www.semanticscholar.org/paper/08178c93f689086ccce348963cf808a47649a6e8,Artificial Curiosity Emerging Human-Like Behavior: Toward Fully Autonomous Cognitive Robots,,,2016.0,10.1007/978-3-319-23392-5_28,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
05ff198da17b197959ac1735a9a9d6499ad2962b,https://www.semanticscholar.org/paper/05ff198da17b197959ac1735a9a9d6499ad2962b,Real-time Unsupervised Clustering,"In our research program, we are developing machine learning algorithms to enable a mobile robot to build a compact representation of its environment. This requires the processing of each new input to terminate in constant time. Existing machine learning algorithms are either incapable of meeting this constraint or deliver problematic results. In this paper, we describe a new algorithm for real-time unsupervised clustering, Bounded Self-Organizing Clustering. It executes in constant time for each input, and it produces clusterings that are significantly better than those created by the Self-Organizing Map, its closest competitor, on sensor data acquired from a physically embodied mobile robot. Clustering algorithms are unsupervised learning algorithms that employ a distance metric to organize their training inputs into clusters. The classification of a previously unseen input is determined by the cluster to which it is closest, again based on the distance metric. This enables a large input space to be compressed into a more compact representation. A useful application of this technique is to create models of mobile robot environments based on the robot’s sensor inputs. In both training and classification, the distances between the input and the representative example of each cluster must be calculated. By placing a fixed upper bound on the number of clusters, we limit the total number of calculations required by each training and classification operation. This, in turn, enables us to predict the cycle time of the algorithm, a necessity for the practical deployment of such an algorithm aboard a mobile robot. In this paper, we introduce a novel clustering algorithm, Bounded Self-Organizing Clusters (BSOC), that is designed to meet these constraints. In the robotics literature, the Kohonen Self-Organizing Map (SOM) has been used for this purpose as well. We argue that the design of the SOM leads to a relatively poor clustering, and that BSOC represents a significant improvement. We demonstrate this experimentally using both sonar and image data captured from a mobile robot. We begin with some background and describe related work. Next, we describe our physical system hardware, with an eye towards making clear the technical limitations to which any solution must conform. We then describe our learning algorithm, followed by our experiments and results. We then give our conclusions and speculations on future work.",MAICS,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
58e8ac8d773333fac03ee3e7ee34280de6fe60b4,https://www.semanticscholar.org/paper/58e8ac8d773333fac03ee3e7ee34280de6fe60b4,An Inherently Explainable Model for Video Activity Interpretation,"The ability of artificial intelligence systems to offer explanations for its decisions is central to building user confidence and structuring smart human-machine interactions. Understanding the rationale behind such a system’s output helps in making an informed action based on a model’s prediction. In this paper, we introduce a novel framework integrating Grenandar’s pattern theory structures to produce inherently explainable, symbolic representations for video activity interpretation. These representations provide semantically coherent, rich interpretations of video activity using connected structures of detected (grounded) concepts, such as objects and actions, that are bound by semantics through background concepts not directly observed, i.e. contextualization cues. We use contextualization cues to establish semantic relationships among entities directly hypothesized from video signal, such as possible object and actions labels, and infer a deeper interpretation of events than what can be directly sensed. We demonstrate the viability of this idea on video data primarily from the cooking domain by introducing a dialog model that uses these interpretations as the source of knowledge to generate explanations grounded in both video data as well as semantic connections between concepts. Introduction Intelligent agents have evolved tremendously and have achieved significant milestones such as approaching human capabilities in some domains ((Kheradpisheh et al. 2016)). However, despite these performance gains, the model’s ability to explain their decision appears to be constrained. Such ability to express the rationale behind its decision is vital when deploying models in an open, uncontrolled setting. For example, when taking vital decisions in highrisk areas like medical diagnosis (Caruana et al. 2015; Linder et al. 2014) and surveillance (Mahadevan et al. 2010; Junior, Musse, and Jung 2010) to name a few, the level of interaction between the human and a model is of high importance. It has also been established that a model with higher explainability is more likely to be trusted (Ribeiro, Singh, and Guestrin 2016) than a model with limited or no explainability. Explainable models have been explored to some extent in literature. Spanning a variety of application domains such as Copyright c © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. medical diagnosis (Shortliffe and Buchanan 1975), activity simulations such as those in the military (Core et al. 2006; Lane et al. 2005) and robotics (Lomas et al. 2012), these approaches have advocated models that are able to explain the approach undertaken to arrive at decisions but were not able to justify their decision to the user. There also have been model-agnostic approaches such as ((Baehrens et al. 2010; Ribeiro, Singh, and Guestrin 2016)) that attempt to explain the decision of machine learning models while treating them to be a black-box. However, some approaches, such as those advocated in (Biran and McKeown 2014; Hendricks et al. 2016), are able to support their decisions with explanations justifying them with evidence from visual and semantic cues. To extend the concept of explainability to video activity interpretation, we consider an explanation to be a description that explains and justifies the rationale of a model’s decision process. In addition to providing justification with respect to both feature-level evidence, we also focus on explaining how the semantic correlations are established among concepts that make up an activity (actions and objects). In open, uncontrolled environments, establishing justifiable semantic correlation is integral to a model’s success since the training data may not always be representative of all viable activities that one may encounter. It should be noted that we consider an explanation to be both introspective as well as retrospective. A model’s ability to provide sufficient justification for its decision requires in-depth knowledge about various concepts and the relationships that they share with other concepts. This use of prior knowledge can be considered to be analogous to how humans correlate the presence of certain concepts to aid in the current task. For example, in medical diagnosis (Ledley, Lusted, and Ledley 1959), it has been noted that the reasoning process used by doctors requires the establishment of correlation between symptoms (logical concepts) and probabilities to aid their diagnosis. Each symptom adds a certain value to the overall diagnosis and hence influences the direction of the reasoning process. This prior knowledge can be particularly helpful in identifying how two concepts can be related and why that relationship can contribute to the overall goal of the model. In this paper, we propose a novel framework that leverages Grenander’s Pattern Theory structures (Grenander The Workshops of the Thirty-Second AAAI Conference on Artificial Intelligence",AAAI Workshops,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
533b125d3b81bbaca567eae12b4d734877a66fc4,https://www.semanticscholar.org/paper/533b125d3b81bbaca567eae12b4d734877a66fc4,CPSGrader: Auto-Grading and Feedback Generation for Cyber-Physical Systems Education,"Abstract : Formal methods and machine learning together have the potential to enhance technologies for education. In this thesis, we consider the problem of designing CPSGrader, an automatic grader for laboratory-based courses in the area of cyber-physical systems. The work is motivated by a UC Berkeley course in which students program a robot for specified navigation tasks. Given a candidate student solution (control program for the robot), CPSGrader first checks whether the robot performs the task correctly under a representative set of environment conditions. If it does not, CPSGrader automatically generates feedback hinting at possible errors in the program. CPSGrader is based on a novel notion of constrained parameterized tests based on signal temporal logic (STL) that capture symptoms pointing to success or causes of failure in traces obtained from a realistic simulator. We define and solve the problem of synthesizing constraints on a parameterized test such that it is consistent with a set of reference solutions with and without the desired symptom. We also develop a clustering-based active learning technique that selects from a large database of unlabeled solutions, a small number of reference solutions for the expert to label. The goal is to achieve better accuracy of fault identification with fewer reference solutions as compared to random selection. We demonstrate the effectiveness of CPSGrader using two data sets: one obtained from an on-campus laboratory-based course at UC Berkeley, and the other from a massive open online course (MOOC) offering. In addition, CPSGrader was successfully deployed in the laboratory section of this MOOC on the edX platform.",,2014.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
df18ce8dee746d38a944dfc6a8fecb28c987a26f,https://www.semanticscholar.org/paper/df18ce8dee746d38a944dfc6a8fecb28c987a26f,Clustering Utilization for Coverage Improvement in Wireless Border Surveillance Networks,"In many border surveillance applications (such as military, homeland security, etc.), the wireless sensor networks cannot be deployed manually and the barrier coverage breaks can appear along a given surveillance line. This paper introduces a cluster-based algorithm and new metrics to determining the number and the positions of the additional nodes needed to be deployed by drones, robots, or moved in a network; in order to fill the gaps in a randomly deployed network. Simulation results show that the proposed algorithm optimizes the number of additional nodes and outperforms the alternative in 52,15% of cases while it performs similarly in the rest of the cases. The machine learning classification algorithms are used to show that the decision on choosing one or another algorithm is highly classifiable. Precisely, the proposed algorithm is shown to be the approach of choice in implementations where the sensing range is relatively small. Key-Words: algorithm, barrier coverage, clustering; machine learning, decision trees, neural networks, support vector machines, wireless sensor networks.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
98398e5639b537d0849ad372d3d9e8e2e81c8603,https://www.semanticscholar.org/paper/98398e5639b537d0849ad372d3d9e8e2e81c8603,Control of a robotic arm using a low-cost BCI,"Electromyography (EMG) measurements recently became accessible and inexpensive due to new hardware developed for general Brain Computer Interface (BCI) applications. Low-cost devices for EMG measurements, therefore, enable potential deployment of intelligent hand prosthetics for a wider range of people, for example, children in developing countries who cannot afford the required and constant replacement of actuated prosthetics. However, such inexpensive devices use passive surface electrodes and the limited processing power only allows for very basic signal processing. As a consequence, EMG signals are characterized by noise, drift, and low repeatability. This thesis investigates the use of machine learning methods, particularly classification methods, to validate the feasibility of a low-cost BCI device for controlling a robotic arm. Part of this study regards the experimental search for the location of the surface muscles of the forearm, that have to be found and measured for different movements. This thesis investigates preprocessing methods for the raw measurements with a bandpass filter and root mean square (RMS) over a time window. Finally, this thesis compares classification results with linear support vector machines, K nearest neighbors and a Gaussian classifier. In our findings, the support vector machine and the k nearest neighbor output the best results. Although low-cost electromyography devices measure less reliable signals, they are capable of classifying different hand movements. Zusammenfassung Das messen von Elektromyographie (EMG) wurde in letzter Zeit, aufgrund neuer Hardware für allgemeine Brain Computer Interface (BCI) Anwendungen, zugänglich und kostengünstig. Kostengünstige Geräte ermöglichen daher den potentiellen Einsatz von Hand Prothesen für ein breiteres Spektrum der Menschen. Zum beispiel für Kinder in Entwicklungsländern, welche sich keinen ständigen Austausch der Prothesen leisten können. Da kostengünstige Geräte passive Elektroden nutzen und eine begrenzte Rechenleistung haben, erlauben sie nur eine grundlegende Signalverarbeitung. Aus diesem Grund zeichnen sich die EMG signale durch Rauschen, Driften und einer niedrigen Wiederholbarkeit aus. Diese Thesis untersucht den Nutzen von Methoden des maschinellen Lernens. Insbesondere Methoden zum klassifizieren, um die Druchführbarkeit der Kontrolle eines Roboterarms mithilfe eines kostengünstigen BCI Gerätes zu überprüfen. Teil dieser Thesis ist die experimentelle Suche des genauen Orts von oberflächlichen Muskeln des Unterarms, welche für verschiedene Bewegungen gemessen werden müssen. Diese Arbeit untersucht einen Bandpassfilter und das Quadratische Mittel über ein Zeitfenster zum vorverarbeiten der Signale. Letzendlich werden die Ergebnisse der Klassifizierungen von dem K-Nächste-Nachbarn-Klassifizierer, der Support Vector Machine und einem Gausschen Klassifizierer miteinander verglichen. Die Ergebnisse zeigen, dass die Support Vector Machine und der K-Nächste-Nachbarn-Klassifizierer die besten Resultate liefern. Obwohl preiswerte Elektrographie Maschinen weniger zuverlässige Signale messen, sind sie fähig unterschiedliche Handbewegungen zu klassifizieren.",,2016.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7da6255c112cf75fada7684491d7856fd9d740af,https://www.semanticscholar.org/paper/7da6255c112cf75fada7684491d7856fd9d740af,Cognitive Robot Architectures,"We present the results of a survey of industrial developers to determine what they and their customers require from a cognitive robot. These are cast as a series of eleven functional abilities: 1) Safe, reliable, transparent operation. 2) High-level instruction and context-aware task execution. 3) Knowledge acquisition and generalization. 4) Adaptive planning. 5) Personalized interaction. 6) Self-assessment. 7) Learning from demonstration. 8) Evaluating the safety of actions. 9) Development and self-optimization. 10) Knowledge transfer. 11) Communicating intentions and collaborative action. I. INDUSTRIAL REQUIREMENTS While cognitive robotics is still an evolving discipline and much research remains to be done, we nevertheless need to have a clear idea of what cognitive robots will be able to do if they are to be useful to industrial developers and end users. The RockEU2 project canvassed the views of thirteen developers to find out what they and their customers want. The results of this survey follow, cast as a series of eleven functional abilities. A. Safe, reliable, transparent operation Cognitive robots will be able to operate reliably and safely around humans and they will be able to explain the decisions they make, the actions they have taken, and the actions they are about to take. A cognitive robot will help people and prioritize their safety. Only reliable behaviour will build trust. It will explain decisions, i.e. why it acted the way it did. This is essential if the human is to develop a sense of trust in the robot. A cognitive robot will have limited autonomy to set intermediate goals to when carrying out tasks set by users. However, in all cases it defers to the users preferences, apart from some exceptional circumstances, e.g. people with dementia can interact in unpredictable ways and the robot will be able to recognize these situations and adapt in some appropriate manner. The freedom to act autonomously will have formal boundaries and the rules of engagement will be set on the basis of †Much of the work described in this paper was conducted while the author was at the University of Skövde, Sweden. This research was funded by the European Commission under grant agreement No: 688441, RockEU2. three parameters: safety for people, safety for equipment, and safety of the robot system. The rules may change depending on the environment and a cognitive robot will not exceed the limits of safe operation. The limits may be application specific, e.g., the robot should not deviate further than a given specification/distance/etc. A cognitive robot will use this type of knowledge to act responsibly and will ask for assistance when necessary (e.g. before it encounters difficulties). In particular, in emergency situations, the robot will stop all tasks to follow some emergency procedure. Ideally, if the user is deliberately trying to misuse the robot, e.g. programming it to assist with some unethical task, a cognitive robot will cease operation. B. High-level instruction and context-aware task execution Cognitive robots will be given tasks using high-level instructions and they will factor in contextual constraints that are specific to the application scenario when carrying out these tasks, determining for themselves the priority of possible actions in case of competing or conflicting requirements. Goals and tasks will be expressed using high-level instructions that will exploit the robots contextual knowledge of the task. This will allow the robot to pre-select the information that is important to effectively carry out the task. The goals will reflect the users perspective. This means that all skills which implicitly define the goals are tightly linked to realworld needs and to the solution of specific problems, e.g., “get me a hammer”. The following guidelines will apply. • Instructions will use natural language and gestures to specify the goals. • Natural language will be relatively abstract but will be grounded in the codified organisational rules, regulations, and behavioural guidelines that apply to a given application environment. This grounding means that each abstract instruction is heavily loaded with constraints which should make it easier for the robot to understand and perform the task effectively. • The goals should be specified in a formalised and structured way, where the designer defines them well and can verify them. For example, teach the robot the environment it is working in, follow a described route to reach each of the target locations and reach these positions to carry out the task. These clearly-specified tasks are tightly coupled with risks and costs, e.g. of incorrect execution. Proceedings of EUCognition 2016 ""Cognitive Robot Architectures"" CEUR-WS Vol. 1855 6 • It should be possible for the robot to be given goals in non-specific terms (e.g. assist in alleviating the symptoms of dementia), guidelines on acceptable behaviour (or action policies), and relevant constraints, leaving it to the robot to identify the sub-goals that are needed to achieve these ultimate goals. • A cognitive robot will learn ways of measuring the success of outcomes for the objectives that have been set, e.g., creating a metric such as the owners satisfaction related not only to the directly specified objective but also the manner in which the job was done). It should be learn from these metrics. A cognitive robot will consider the contextual constraints that are specific to the application scenario. It will determine the priority of potential actions, e.g., in case of competing or conflicting needs. For example, the robot might know the procedure to be followed but the locations to be visited or the objects to be manipulated need to be specified (or vice versa). For example, when an automated harvester encounters a bale of straw, it can deal with it as an obstacle or something to be harvested, depending on the current task. For example, the robot might engage in spoken interaction with older adults until the goal is communicated unambiguously, using context to disambiguate the message and allow for the difficulties in dealing with different accents, imprecise speech, and poor articulation. A cognitive robot will know what is normal, i.e. expected, behaviour (possibly based on documented rules or practices) and it will be able to detect anomalous behaviour and then take appropriate action. The following guidelines will apply. • It will be possible to pre-load knowledge about the robots purpose and its operating environment, including any rules or constraints that apply to behaviour in that environment. • It will be possible to utilize domain-specific skill pools (e.g. from shared databases) so that the robot is preconfigured to accomplish basic tasks without having to resort to learning or development. • The robot will continually improve its skills (within limits of the goals and safety, see above) and share these with other robots. • The robot might assist the user by proposing goals from what it understood and the user makes the final selection. The level of detail in the description required by a cognitive robot will decrease over time as the robot gains experience, in the same way as someone new on the job is given very explicit instructions at first and less explicit instructions later on. One should need to demonstrate only the novel parts of the task, e.g., pouring liquid in a container, but not the entire process. It will be possible to instruct the robot off-line if there is no access to the physical site; e.g., using a simulation tool, with the robot then being deployed in the real scenario. C. Knowledge acquisition and generalization Cognitive robots will continuously acquire new knowledge and generalize that knowledge so that they can undertake new tasks by generating novel action policies based on their history of decisions. This will allow the rigor and level of detail with which a human expresses the task specification to be relaxed on future occasions. A cognitive robot will build and exploit experience so that its decisions incorporate current and long term data. For example, route planning in a factory, hospital, or hotel should take into account the history of rooms and previous paths taken, or it might take another look to overcome high uncertainty. In general, the robot will overcome uncertainty in a principled manner. A cognitive robot will generalize knowledge to new task by understanding the context of a novel task and extrapolating from previous experience. For example, a care-giving robot will reuse knowledge of a rehabilitation exercise, customizing it to another person. A welding robot will weld a new instance of a family of parts. In general, a cognitive robot will extract useful meaning from an interaction for a future and more general use, with the same or another user. This may extend to learn cultural preferences and social norms. For example, in a domestic environment, a cognitive robot will learn how to do simple household tasks, e.g. how to grasp different objects and them bring to a person that wants them. This will be continuously extended, allowing the robot to do more complex things, including cooking. D. Adaptive planning Cognitive robots will be able to anticipate events and prepare for them in advance. They will be able to cope with unforeseen situations, recognizing and handling errors, gracefully and effectively. This will also allow them to handle flexible objects or living creatures. A cognitive robot will be able to recognize that circumstances have changed to avoid situations where progress is impossible. It will also be able to recognize errors and recover. This may include retrying with a slightly different strategy. The learning process will be fast, ideally learning from each error. A cognitive robot will be able to learn how to handle errors, how to react to situations where, e.g., a human is doing something unexpected or parts are located in an unexpec",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cda7e3a94eda406b47ec0a4783587473bce1968e,https://www.semanticscholar.org/paper/cda7e3a94eda406b47ec0a4783587473bce1968e,Spatiotemporal coherent semantic maps with mobile robots,"Along the last decades persistent research endeavors in the areas of robotics andartificial intelligence revealed the great challenge of robot navigation, an areathat combines robot mobility with perception of the environment. Moreover,in modern human societies it is of great importance to build up machines thatcan be operated by non specialists or even by technologically illiterate people,such as youngsters or elderly. Therefore, the mobile robots to be released intothe market in the near future should possess, among others, the potential ofproducing meaningful internal perceptual representations of their own environment,capacitating them to cope a range of real-life situations and tasks. Tomake matters even more challenging, when it comes for mapping and navigation,robots should comprehend human concepts about places and objects,to skillfully deploy in human frequented environments. In response to thischallenge, intense research efforts, to build cognitive robots apt to competentlyperceive and understand their surroundings and to cooperate with humans,take place. With this goal in view, semantic mapping with mobile robots can constituteto a holistic solution in response to the aforementioned challenges. Thesemantic mapping is an augmented representation of the robot’s environmentthat -supplementary to the geometrical knowledge- encapsulates characteristicscompatible with human understanding. It provides several algorithmic opportunitiesfor innovative development of applications that will eventually lead tothe human robot interaction. The main objective of the PhD dissertation in handis the construction of accurate and consistent semantic maps facilitating amplerobot deployment in domestic environments.The motivation behind this PhD dissertation has been the observation thatthe plethora of the existing mapping and navigation algorithms are not ableto provide a sufficient representation of the environment in terms of humans’concepts. This is due to the fact that the mapping methods developed sofar focus on the construction of geometrical maps. Although some of thesesolutions proved to be capable of driving robots into specific target positions,they lack of high level cognition attributes, which would allow them to bring the human-robot interaction one step beyond. Aiming to remove this barrier, thisdissertation is oriented towards the direction of developing semantic mappingalgorithms for high level robot navigation. Therefore, this doctoral researchidentified the basic components of the semantic mapping and developed aninnovative solution for each one of them. Due to the fact that the semanticmapping requires an integrated system that comprises several subordinatemodules, a wide range of a algorithms that serve different tasks had to bedesigned and implemented. Within the context of this thesis several algorithmicmodules have been developed including a competent localization algorithm,novel simultaneously localization and mapping strategies, breakthrough place andobject recognition tactics, as well as the integration of all these methods undera time supervised framework able to produce consistent semantic maps. Dueto the fact that each subordinate module comprises an innovative solutionto the respective field, the resulting semantic mapping system constitutesa state of the art solution in the area of conceptual mapping with mobilerobots. The introduced algorithms exploit solely visual and depth sensors,while by combining basic tools from three different scientific areas such asrobotics, computer vision and machine learning the final objective is accomplished.Overall, the main contribution of this thesis to the advancement the stateof the art is the introduction of a stacked map hierarchy of four differenttype of maps, namely a metric, a topological, a labeled sparse topological andan augmented navigation one. Each of these representations accomplishes aunique purpose: (i) the metric is the physical (lower) layer; (ii) the topologicalone contains abstract geometrical information of the environment, i.e. pointclouds registered in a graph of nodes; (iii) the labeled sparse topological oneestablishes the spatiotemporal coherence by associating the respective nodes inthe topometric maps via place labels and geometrical transformations, enablingbidirectional exchange of information among the conceptual and metric mapsand, last, (iv) the augmented navigation map inheres the significance of thedetected places as well as their connectivity relationships, expressed in termsof their transition probability.The thesis in hand has been developed in a hierarchical fashion and can bedivided into four main chapters. The first one comprises a literature survey ofthe existing semantic mapping methods in which an explicit analysis of the sofar developed methods is sought. The insights of the semantic mapping arereviewed, the distinct components encompassing, to give a categorization of therelated literature, are studied the possible applications in mobile robotics areexamined and, lastly, the methods and databases available for benchmarkingare referred. Furthermore, a quality-based taxonomy of the existing semanticmapping methods highlights the dominant attributes such methods retain.More precisely, according to the scale, to which each method is expanded, the metric map could be either a single scene or a progressively created map.Another important attribute a typical semantic mapping method possessesis the existence of the respective topological map, that is an abstraction ofthe explored environment in terms of a graph. The nodes of such a graph areorganized in a geometrical manner, so as to simultaneously preserve conceptualknowledge about the explored places. Moreover, the modalities (single ormultiple visual cues) utilized to reason about the observed scene constitute anelement apt to distinguish the abundance of different methods. An additionalfeature in many recent semantic mapping techniques is the temporal coherencesuch a map reveals, which renders it useful for high-level activities, viz. taskplanning or human robot interaction.The second chapter refers to the description of the developed technologicalbackground required to build a consistent semantic map. Therefore, a majorcontribution of the first part is the development of an innovative visual odometryalgorithm able to operate in real time. This algorithm receives as input successivestereo pair of images from a stereoscopic camera mounted on a mobilerobot. It involves the detection of the salient landmarks between successiveimages. A depth estimation of these features is then obtained and a novelnon-iterative outlier detection and discarding methodology able to remove boththe mismatches between the features and the inserted errors due to the 3Dreconstruction procedure. A hierarchical motion estimation technique, whichproduces robust estimations for the movement of the robot is then adopted,thus providing refinements to the robot’s global position and orientation. Anadditional 3D reconstruction algorithm that operates on stereo images hasbeen developed providing accurate reconstruction of the area observed bythe robot. Moreover, the localization algorithm comprises the cornerstone forthe development of a simultaneously localization and mapping system suitablefor the 3D geometrical mapping of the explored environment. This 3D metricmapping system is based solely on an RGB-D sensor, where in course of robot’slocomotion 3D point clouds are merged with respect to the visual odometry.The resulted 3D map is refined by exploiting a random sample consensus planedetection algorithm accompanied by an iterative closest point registration step,among the dominant planes of the consecutive time instances, resulting thusin a very consistent geometrical 3D map. All the aforementioned developedalgorithms have been evaluated on a custom made robot platform bearing twostereoscopic cameras with different baselines and a RGB-D sensor.The third chapter encloses all the semantic mapping methods based onvisual cues that have been developed within this PhD dissertation. The firstone examines the overall traversability of the observed scene taking into considerationthe robot’s embodiment. This knowledge constitutes a cornerstone forthe autonomous robot navigation. The developed system utilizes an algorithm to retrieve specific characteristics of the environment using a stereo cameraand to produce a disparity map of the scene. Then, the v-disparity image iscalculated based on the disparity map. The v-disparity is then exploited by afeature extraction procedure to provide the system with respective conceptualvectors, which are used to train support vector machines in order to assess theoverall traversability of the scene. The traversable classified scenes are furtherprocessed and the likelihood distribution of the collision risk assessment, whenthe robot moves towards any direction, is calculated. The second part involvesthe description of a novel object recognition algorithm based on hierarchical temporalmemory networks. This constitutes a supervised learning method usedto recognize objects in different orientations. It introduces specific alternativerules for the design of each building block of a hierarchical temporal memorynetwork. These rules expand both the spatial and the temporal module ofthe network. Various type of input layers have been tested such as logppolarand saliency detection ones in order to find the solution that fits better in applicationsthat concern cluttered environments. The third part of this chaptercomprises the description of an innovative place classification algorithm. Withinthis work, in course of robot’s locomotion, salient visual features are detectedand they shape a bag-of-features problem, quantized by a neural gas to codethe spatial information for each scene. Each input image is transformed intoan appearance based histogram representation that abstracts the",,2014.0,10.12681/eadd/40861,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3bd5a7342f68937d0b58de1fe9f2bf71c3854125,https://www.semanticscholar.org/paper/3bd5a7342f68937d0b58de1fe9f2bf71c3854125,Interactive Image Segmentation for Model Adaption and Decision Support,"In many fields of Agricultural Management and Agricultural Engineering sophisticated algorithms based on complex environment models are used to generate decision-supporting information from various data sources. However, often these models highly depend on the proper adaption of their complex parameter sets to local ambient conditions and in many cases practitioners are not able to perform this adaption. Therefore a concept is shown here that allows the identification of objects in images and their linkage with meta-data in semi-automatic human-machine interaction. The approach combines the robustness of human experiences against spatially and temporarily local variations and the performance and reproducibility of statistical models. It can also be used as an easy way to adapt models to local ambient conditions, which allows recalibrating them more often, thereby increases stability against changes, iteratively improves them and opens the door for life-long machine learning. The software has been developed within the collaborative research project RemoteFarming.1 in which a remote farming robotic weed control system is being developed. The robotic weed control system will be used for in-row weed treatment in carrots at BBCH-scales 10 to 20 in organic farming. In this field weed control is currently conducted by hand. Within the project's first part RemoteFarming.1a an autonomous field robot – based on the platform BoniRob is being built. It is able to autonomously navigate on the field and has an actuator for mechanical treatment of weeds. Furthermore it uses synchronously triggered cameras and lighting units at different wavelengths which can capture high-contrast images of the plants in a shaded space underneath the robot. The detection/identification of weeds in RemoteFarming.1a is performed in a web-based approach by a remote worker, who marks the weeds in images captured by the robot on the field. Afterwards the mechanical actuator of the robot moves to those positions in the field which have been marked in the respective images and eliminates the weed plants. In the second part RemoteFarming.1b this system will be enriched with weed/crop classifiers and the detection/identification. The user will get a suggestion of possible weeds marked in his view and he can confirm or modify these suggestions before the weed will be treated. The software framework described here allows iteratively generating segmentations for images by human-machine interaction. After a first-shot segmentation the user can add marks in the image and after any added mark the segmentation gets improved. The segmentation is visualized by a semi-transparent ImageMap overlaying the original image. The algorithms that have been tested for performing the segmentation so far are Watershed and Graph-Cuts. During the process any arbitrary segment in the ImageMap – even unconnected regions can be assigned to an object. These objects then can be separated into groups and enriched with additional meta-data. Furthermore the ImageMaps can be grouped into Situations representing different field conditions. The framework's design is flexible with abstraction of front-end and back-end. On the back-end side a server version saves data in a relational database. Alternatively a stand-alone version provides the same functionality using XML to persist data. For the front-end a web-based version can be deployed on servers. Another front-end is implemented as App. This allows using the framework on mobile devices even without Internet connection, saving the gathered data temporarily in XML and persisting into DB once connected. The framework has been used within the collaborative research project RemoteFarming.1 for labeling of crop and weed plants. It allowed generating a sophisticated ground-truth for shape-matching algorithms and weed/crop classifiers. Regions of plants and even overlapping leafs have been marked, grouped to plants and assigned with labels (Species) and meta-data (BBCH-scale etc.). In the on-going project the system will be enriched with statistical models to provide the user improved first-shots for segmentation and plant classification. But geometric analyses of the labelled data collected at project beginning has already served as specific input for vague issues in requirement analysis for the remote farming robotic weed control system that will be developed.",,2013.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e5bd3b66665bd4496dbecbea0e90792df9ac818e,https://www.semanticscholar.org/paper/e5bd3b66665bd4496dbecbea0e90792df9ac818e,Editorial: Neural plasticity for rich and uncertain robotic information streams,"Models of adaptation and neural plasticity are often demonstrated in robotic scenarios with heavily pre-processed and regulated information streams to provide learning algorithms with appropriate, well timed, and meaningful data to match the assumptions of learning rules. On the contrary, natural scenarios are often rich of raw, asynchronous, overlapping and uncertain inputs and outputs whose relationships and meaning are progressively acquired, disambiguated, and used for further learning. Therefore, recent research efforts focus on neural embodied systems that rely less on well timed and pre-processed inputs, but rather extract autonomously relationships and features in time and space. The bio-inspired focus does not seek the most effective machine learning method to solve those problems, it rather points toward a better understanding of problem solving mechanisms in neural systems, which can in turn also provide viable solutions to difficult problems. 
 
Realistic models of plasticity must account for delayed rewards (Soltoggio et al., 2013a), noisy and ambiguous data (Soltoggio et al., 2013b), and emerging and novel input features during online and value learning (Krichmar and Rohrbein, 2013). Those factors have indeed been an emerging focus of search (e.g., Sporns and Alexander, 2003; Lungarella and Sporns, 2006; Martius et al., 2013), with a growing number of studies that cannot be reviewed in this short editorial. Such approaches model the progressive acquisition of knowledge by neural systems through experience in environments that may be affected by ambiguities, uncertain signals, delays, or novel features (Pugh et al., 2014; Soltoggio, 2015). This Research Topic in Frontiers in Neurorobotics explored fundamental properties and dynamics of neural learning systems that are naturally immersed in a rich information flow. We are pleased with the contributions collected in this Research Topic, each of which addresses key topics in this emerging and important field of research. 
 
One overarching problem in this field is that of making sense of large amounts of data from sensory systems in order to recognize particular situations and perform basic tasks. Parisi and colleagues took a self-organizing neural approach to action recognition using human pose-motion features. The Growing When Required (GWR) networks manifest a high-level structural plasticity that regulates network complexity in relation to the task (Parisi et al., 2015). Such a bio-inspired approach recorded state-of-the-art performance on a dataset of full-body actions captured with a depth sensor, with competitive results in a public benchmark of domestic daily actions. 
 
Another source of large, noisy and uncertain data is found in robotic tactile sensors. Chou et al. (2015) deployed a specific robot called CARL-SJR with a full-body tactile sensory area. CARL-SJR encourages people to communicate with it through gentle touch, and provides feedback to users by displaying bright colors on its surface. The time-delayed and uncertain nature of the interactions poses challenges to the formation of correct associations between stimuli, rewards and actions. The approach devised by Chou et al. (2015) experiments with a strongly bio-inspired architecture of spiking neurons with neuromodulated plasticity. The model abstracts brain areas such as the primary somatosensory cortex, prefrontal cortex, striatum, and the insular cortex to process noisy data generated directly from CARL-SJR's tactile sensory area. The result is a robust learning mechanism that reliably forms correct associations and preferences for directions without heavily pre-processed inputs. 
 
Uncertainty and large amount of data are also found in collaborative multi-robot scenarios in which multiple robots work alongside humans. Galbraith and colleagues propose a motor babbling approach to learn a complex set of relations and interactions with the 11-degrees-of-freedom RoPro Calliope mobile robot (Galbraith et al., 2015). Motor babbling of its wheels and arm enabled the Calliope to learn how to relate visual and proprioceptive information to achieve hand-eye-body coordination. 
 
Motor control is a problem in which neural plasticity results in high level of adaptation, adjusting neural systems to operate in combination with specific bio-mechanical structures and morphologies. (Burms et al., 2015) demonstrated the utility of modulated Hebbian plasticity in embodied computation for compliant robotics. In such scenarios, control policies are generally unknown due to the partial offload of control policies to morphological computation. Modulated Hebbian plasticity was shown to lead to hybrid controllers that naturally integrate the computations that are performed by the robot's body into a neural network architecture. Those results demonstrate the potential of universal applicability of plasticity rules to complex control problems. 
 
A similar problem was tackled in Dasgupta et al. (2015) in which they used distributed recurrent neural networks with synaptic adaptation to find a range of complex behaviors for walking robots. In particular, their approach demonstrated a remarkable flexibility in designing control systems that can work with multi-legged robots. A Central Pattern Generator is used to feed a self-adaptive reservoir network, which in turn provides motor control through a read-out integration unit. These results contribute to demonstrate the efficacy and continuous advancement of plastic neural models in complex input-output control scenarios. 
 
The overall vision provided by these research papers outlines an increasingly more effective deployment of plastic neural models to tackle complex perception and control problems in which noise, uncertainty and delays pose a challenge to many algorithms. This vision matches the intuition of bioinspired neurorobotics approaches that propose advanced, plastic neural systems as viable models when sensory-motor information flows approach the richness and complexity found in the behavior of biological systems. We foresee a continuous growing attention to this emerging research area, in particular related to the development of more effective, scalable and general neural learning algorithms to effectively tackle rich and uncertain robotic information streams.",Front. Neurorobot.,2015.0,10.3389/fnbot.2015.00012,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d337b81dafdab5587c1c1f1a2b3eb1c37d3c721f,https://www.semanticscholar.org/paper/d337b81dafdab5587c1c1f1a2b3eb1c37d3c721f,Towards Modeling Collaborative Task Oriented Multimodal Human-human Dialogues,"This research took place in the larger context of building effective multimodal
interfaces to help elderly people live independently. The final goal was to build a
dialogue manager which could be deployed on a robot. The robot would help elderly
people perform Activities of Daily Living (ADLs), such as cooking dinner, and
setting a table. In particular, I focused on building dialogue processing modules
to understand such multimodal dialogues. Specifically, I investigated the functions
of gestures (e.g. Pointing Gestures, and Haptic-Ostensive actions which involve
force exchange) in dialogues concerning collaborative tasks in ADLs.

This research employed an empirical approach. The machine learning based modules
were built using collected human experiment data. The ELDERLY-AT-HOME corpus was
built based on a data collection of human-human collaborative interactions in the
elderly care domain. Multiple categories of annotations were further conducted to
build the Find corpus, which only contained the experiment episodes where two
subjects were collaboratively searching for objects (e.g. a pot, a spoon, etc.),
which are essential tasks to perform ADLs.

This research developed three main modules: coreference resolution, Dialogue Act
classification, and task state inference. The coreference resolution experiments
showed that modalities other than language play an important role in bringing
antecedents into the dialogue context. The Dialogue Act classification experiments
showed that multimodal features including gestures, Haptic-Ostensive actions, and
subject location significantly improve accuracy. They also showed that dialogue
games help improve performance, even if the dialogue games were inferred
dynamically. A heuristic rule-based task state inference system using the results
of Dialogue Act classification and coreference resolution was designed and
evaluated; the experiments showed reasonably good results.

Compared to previous work, the contributions of this research are as follows:
1) Built a multimodal corpus focusing on human-human collaborative task-oriented
dialogues.
2) Investigated coreference resolution from language to objects in the real world.
3) Experimented with Dialogue Act classification using utterances, gestures and
Haptic-Ostensive actions.
4) Implemented and evaluated a task state inference system.",,2014.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9e5b88dc69fe2abbfd4c2e8380a33306fcea4e3e,https://www.semanticscholar.org/paper/9e5b88dc69fe2abbfd4c2e8380a33306fcea4e3e,Robotics research : the eighth international symposium,"1. Advanced Manipulation Session Summary.- Elastic Strips: Real-Time Path Modification for Mobile Manipulation.- Modeling and Control for Mobile Manipulation in Everyday Environments.- Scale-Dependent Grasps.- 2. Dynamics and Control Session Summary.- A General Formulation of Under-Actuated Manipulator Systems.- Towards Precision Robotic Maneuvering, Survey, and Manipulation in Unstructured Undersea Environments.- Where does the Task Frame Go?.- 3. Emergent Motions Session Summary.- Motion Synthesis, Learning and Abstraction through Parameterized Smooth Map from Sensors to Behaviors.- Safe Cooperative Robot Patterns via Dynamics on Graphs.- 4. Motion Planning Session Summary.- Motion Planning with Visibility Constraints: Building Autonomous Observers.- Motion Planning in Humans and Robots.- Local and Global Planning in Sensor Based Navigation of Mobile Robots.- Interleaving Motion Planning and Execution for Mobile Robots.- 5. Manufacturing Session Summary.- Opportunities for Increased Intelligence and Autonomy in Robotic Systems for Manufacturing.- Rapid Deployment Automation: Technical Challenges.- Stability of Assemblies as a Criterion for Cost Evaluation in Robot Assembly.- Towards a New Robot Generation.- 6. New Components Session Summary.- The Design of a Serial Communication Link for Built-in Servo Driver and Sensors in a Robot.- Omnidirectional Vision.- Small Vision Systems: Hardware and Implementation.- 7. Mobile Robots Session Summary.- Exploration of Unknown Environments with a Mobile Robot using Multisensorfusion.- Integration of Topological Map and Behaviors for Efficient Mobile Robot Navigation.- A Robotic Travel Aid for the Blind: Attention and Custom for Safe Behavior.- Automated Highways and the Free Agent Demonstration.- The Design of High Integrity Navigation Systems.- 8. Haptics Session Summary.- Tactile Displays for Increased Spatial and Temporal Bandwidth in Haptic Feedback.- Design of an Anthropomorphic Haptic Interface for the Human Arm.- Testing A Visual Phase Advance Hypothesis for Telerobots.- 9. Medical Session Summary.- Robot Assisted Surgery and Training for Future Minimally Invasive Therapy.- Surgery Simulation with Visual and Haptic Feedback.- Synergistic Mechanical Devices: A New Generation of Medical Robots.- 10. Learning from Human Session Summary.- Vision-based Behavior Learning and Development for Emergence of Robot Intelligence.- Using Human Development as a Model for Adaptive Robotics.- Developmental Processes in Remote-Brained Humanoids.- Animating Human Athletes.- 11. Future Robots Session Summary.- Mechanics and Control of Biomimetic Locomotion.- Robots: A Premature Solution for the Land Mine Problem.- Robots Integrated with Environments: A Perceptual Information Infrastructure for Robot Navigation.- Bio-robotic Systems Based on Insect Fixed Behavior by Artificial Stimulation.- 12. Projects in Japan Session Summary.- Physical Understanding of Manual Dexterity.- Tightly Coupled Sensor and Behavior for Real World Recognition.- Intelligence and Autonomy for Human-machine Cooperative System.- Biologically Inspired Approach to Autonomous Systems.- FNR: Toward a Platform Based Humanoid Project.- Current and Future Perspective of Honda Humanoid Robot.- List of Participants.",,1998.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ea89d5ab69627a9361a340d43866cc0950a50fa1,https://www.semanticscholar.org/paper/ea89d5ab69627a9361a340d43866cc0950a50fa1,Distributed Learning Algorithms for Sensor Networks,"Wireless sensor networks have received significant attention in the last decade owing to their widespread use not only in monitoring the physical world but also in surveillance. The energy and communication constraints of sensor nodes, coupled with distributed processing of sensed signals, lead to challenges in developing effective methods to perform desired inference tasks such as object detection or classification. Further, the lack of well-calibrated sensors is a major obstacle for the rapid deployment of sensor networks. This dissertation develops gossip-based learning algorithms for distributed signal processing in sensor networks. In gossip-based algorithms, sensor nodes share information with local neighbors to converge upon common knowledge about the sensed environment. Gossip-based methods allow for manageable communication among energy-constrained nodes and also accommodate changing network communication topologies. We consider three related problems and develop gossip-based processing solutions. We first consider the problem of joint signature estimation and node calibration using distributed measurements over a large-scale sensor network. We develop a new Distributed Signature Learning and Node Calibration algorithm, called D-SLANC, which estimates the signature of a commonly-sensed source signal and simultaneously estimates calibration parameters local to each sensor node. The approach we take is ii to model the sensor network as a connected graph and make use of the gossip-based distributed consensus to update the estimates at each iteration of the algorithm. We prove convergence of the algorithm to the centralized data pooling solution. We also compare its performance with the Cramér-Rao bound (CRB), and study the scaling performance of both the CRB and the D-SLANC algorithm. Secondly, we develop a gossip-based algorithm for distributed `1-optimization in a large-scale sensor network setting. Specifically, we consider sensor nodes which can measure only a part of the entire measurement vector. We formulate the `1optimization problem as quadratic optimization and develop a distributed, gossipbased algorithm using the projected-gradient approach. We analyze the performance of the proposed algorithm using synthetic data and compare it with a standard `1 solver. Third, we consider the problem of distributed classifier learning in a large-scale sensor network setting. We adopt a machine learning approach to the problem and develop a distributed, gossip-based algorithm that learns the optimal (large-margin) hyperplane separating the two classes, using the projected-gradient approach. We illustrate the performance of the proposed algorithm using both synthetic and realworld datasets.",,2010.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
dd471e1dbd95193a35c58356f591098618812655,https://www.semanticscholar.org/paper/dd471e1dbd95193a35c58356f591098618812655,Artificial Curiosity Emerging Human-like Behaviour - A Fundation for Fully Autonomous Cognitive Machines,"This paper is devoted to autonomous cognitive machines by mean of the design of an artificial curiosity based cognitive system for autonomous high-level knowledge acquisition from visual information. Playing a chief role as well in visual attention as in interactive high-level knowledge construction, the artificial curiosity (e.g. perceptual and epistemic curiosities) is realized through combining visual saliency detection and Machine-Learning based approaches. Experimental results validating the deployment of the investigated system have been obtained using a humanoid robot acquiring visually knowledge about its surrounding environment interacting with a human tutor. As show the reported results and experiments, the proposed cognitive system allows the machine to discover autonomously the surrounding world in which it may evolve, to learn new knowledge about it and to describe it using human-like natural utterances.",IJCCI,2013.0,10.5220/0004628604070419,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
510818b2ee749013ba7ace574c19d1352eaf5698,https://www.semanticscholar.org/paper/510818b2ee749013ba7ace574c19d1352eaf5698,MURFI 2016-FROM CARS TO MARS : APPLYING AUTONOMOUS VEHICLE NAVIGATION METHODS TO A SPACE ROVER MISSION,"The Mars Utah Rover Field Investigation (MURFI) 2016 mission was a Mars Rover field analogue mission run by the UK Space Agency (UKSA) in collaboration with the Canadian Space Agency (CSA). MURFI consisted of a “field team” at the Mars analogue site near Hanksville (Utah, USA), and an “Operations Team” based in the Mission Operations Centre (MOC) at UKSA’s Harwell Campus (Oxfordshire, UK). The Rover platform for the mission comprised the Oxford Robotics Institute’s (ORI) ARC Q14 Space Rover (Q14), and the mission provided a unique opportunity for ORI to test the performance of several of their advanced navigation and autonomous driving algorithms in realistic planetary exploration conditions over a period of several weeks (Figure 1). MURFI’s core objectives included a realistic imitation of the first 10 sols of a possible future Mars Rover mission. Following completion of the core mission, the ORI Rover team engaged in a series of ambitious trials and data gathering scenarios based on ORI’s broad suite of navigation and autonomy algorithms which have been developed primarily for on-road terrestrial applications. The objectives were to (i) assess the performance of these systems in this radically different environment, (ii) determine what systems modifications (if any) might be needed to operate effectively in a typical planetary surface scenario, and (iii) to identify which new capabilities these techniques might bring to the field of planetary surface navigation and exploration. The results of these trials were very encouraging, showing good baseline performance of the techniques deployed even though these had not been modified in any way to reflect the change of environment, and indicating several avenues for further exploration and development of ORI’s techniques which could generate substantial benefits for the Space community. During the core mission, the Rover drives for each sol were conducted autonomously using a single mastmounted stereo camera sensor. Waypoint files for the planned drives were designed by the MOC, transmitted to the field site, and uploaded to Q14 for autonomous execution using ORI’s OxfordVO visual odometry (VO) application the same application which forms the kernel of the VO software for the forthcoming ExoMars mission. The accuracy in following the planned drives during this phase was highly satisfactory, with the Rover typically finishing the drive within a few centimetres of the planned location. During the post-MURFI trials programme, the team implemented ORI’s “Dub4” suite to provide “teach-andrepeat” (T&R) functionality. Dub4 uses a single stereo camera to create vision-based maps in highly unstructured environments which are then used to localise and navigate autonomously. This facilitates safe, rapid retracing of a previously driven path, enabling interesting science sites subsequently identified through data analysis to be rapidly revisited by the Rover. Accuracy in following the previously driven path was good despite large areas of the driven environment being relatively featureless. The use of an affordable monocular camera as an effective localisation sensor using the feature-rich desert floor was investigated, with encouraging results. Data was collected so that dense reconstructions of the terrain around the rover can be generated in a future phase of our work. This reconstruction capability has the potential to create an extremely powerful visualisation tool for generating rich 3-D mesh representations to be utilised by mission scientists to more effectively focus the science effort. Future work will also concentrate on performance enhancements by adapting existing ORI techniques to the specifics of the planetary surface environment, and developing enhanced machine learning autonomy approaches along the path towards the implementation of a true “robot geologist”.",,2017.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e724ebb4d4cf746e283cad96a2514fca9940be53,https://www.semanticscholar.org/paper/e724ebb4d4cf746e283cad96a2514fca9940be53,Assistive Robots as Future Caregivers: The RAPP Approach,,"Progress in Automation, Robotics and Measuring Techniques",2015.0,10.1007/978-3-319-15847-1_17,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9355afcc904c7e0185dca0bdf81097885a1d45a1,https://www.semanticscholar.org/paper/9355afcc904c7e0185dca0bdf81097885a1d45a1,Competitive Simulations: Lessons Learned from the Trading Agent Competition,,KES-AMSTA,2012.0,10.1007/978-3-642-30947-2_1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9b1ae183ee0f9a83a6c2b632184c6ecda2616b85,https://www.semanticscholar.org/paper/9b1ae183ee0f9a83a6c2b632184c6ecda2616b85,Editorial [intro. to Guest Editorial by Prof. Raff D'Andrea],"Summary form only given, as follows. The Editor-in-Chief presents a Guest Editorial by Prof. Raff D'Andrea of ETH Zurich. Dr. D'Andrea played a key role in designing and developing the innovative technology behind Kiva Systems, which was acquired by Amazon.com in May 2012 for US$775M. Kiva Systems invented a radical new approach to warehouse automation and logistics based on large numbers of small mobile platforms that move between stations for order fulfillment, where human workers perform recognition, grasping, loading, and unloading of items from the platforms. The systems required many advances in design and engineering to achieve the necessary Quality (with a capital Q: reliability, efficiency, precision) for deployment in major warehouses (if you haven't seen them, there are cool videos showing scores of bright orange platforms in action on the Internet). In the Guest Editorial, Raff summarizes key insights, such as using feedback from low-cost sensing and computation, combined with data analytics and machine learning, to achieve high accuracy and precision from inexpensive mechanical components. Raff also proposes several exciting challenges for future research, such as new algorithms for high-level planning and scheduling that can optimize the number of mobile platforms, and a call for a reliable indoor equivalent to GPS. He argues for new system design tools that apply innovations in formal verification and model uncertainty and information flow. Raff closes with an exciting and characteristically bold Grand Challenge for Robotics and Automation. Kiva Systems is an inspiring success story in our field and Raff is the ideal person to describe it.",IEEE Trans Autom. Sci. Eng.,2012.0,10.1109/TASE.2012.2214675,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9d47d3b59e1881e0321a5828dc3e19a528587e0e,https://www.semanticscholar.org/paper/9d47d3b59e1881e0321a5828dc3e19a528587e0e,Open architecture for robot controllers,"The paper discusses a generic, open architecture for industrial or non-industrial robot controllers allowing system designers and robot manufacturers to develop rapid deployment automation solutions for particular mechanics of robot manipulators. The paper describes a multiple-axis open controller design for a mobile AGV platform carrying a robotic arm, with inclination control to provide horizontal alignment in any terrain configuration. The navigation and locating of the mobile robot platform, the motion control of the robotic arm, as well as monitoring, learning, program editing, debugging and execution are embedded in a multiprocessor system developed around a Motion Control solution for which a structured programming language was developed. A strongly coupled multi-processor architecture embedding model learning, control and man-machine GUI functionalities is described both as hardware implementing and basic software system design (RTOS, multitasking and operating modes). Experimental results are reported for the motion control of the 5-d.o.f. robot arm.",19th International Workshop on Robotics in Alpe-Adria-Danube Region (RAAD 2010),2010.0,10.1109/RAAD.2010.5524587,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
6f070c05dbd342d3906ee2fbdd2c3529ed558851,https://www.semanticscholar.org/paper/6f070c05dbd342d3906ee2fbdd2c3529ed558851,Designing and evaluating techniques to mitigate misinformation spread on microblogging web services,"Online social media is a powerful platform for dissemination of information during important realworld events. Beyond the challenges of volume, variety and velocity of content generated on online social media, veracity poses a much greater challenge for effective utilization of this content by citizens, organizations, and authorities. Veracity of information refers to the trustworthiness / credibility / accuracy / completeness of the content. Over last few years social media has also been used to disseminate misinformation in the form of rumors, hoaxes, fake images, and videos. We aim to address this challenge of veracity or trustworthiness of content posted on social media. The spread of such untrustworthy content online has caused the loss of money, infrastructure and threat to human lives in the offline world. We focus our work on Twitter, which is one of the most popular microblogging web service today. We provide an in-depth analysis of misinformation spread on Twitter during real-world events. We propose and evaluate automated techniques to mitigate misinformation spread in real-time. The main contributions of this work are: (i) we analyzed how true versus false content is propagated through the Twitter network, with the purpose of assessing the reliability of Twitter as an information source during real-world events; (ii) we showed the effectiveness of automated techniques to detect misinformation on Twitter using a combination of content, meta-data, network, user profile and temporal features; (iii) we developed and deployed a novel framework for providing indication of trustworthiness / credibility of tweets posted during events. We evaluated the effectiveness of this real-time system with a live deployment used by real Twitter users. First, we analyzed Twitter data for 25+ global events from 2011-2014 for the spread of fake images, rumors, and untrustworthy content. Some of the prominent events analyzed by us are: Mumbai blasts (2011), England Riots (2011), Hurricane Sandy (2012), Boston Marathon Blasts (2013), Polar Vortex (2014). We identified tens of thousands of tweets containing fake images, rumors, fake websites, and by malicious user profiles for these events. We performed an in-depth characterization study of how this false versus the true data is introduced and disseminated in the Twitter network. Second, we showed how features of meta-data, network, event and temporat from user-generated content can be used effectively to detect misinformation and predict its propagation during realworld events. Third, we proposed and evaluated an automated methodology for assessing credibility of information in tweets using supervised machine learning and relevance feedback approach. We developed and deployed a real-time version in TweetCred, a system that assigns a credibility score to tweets. TweetCred, available as a browser plug-in, has been installed and used by 1,808 real Twitter users. During ten months of its deployment, the credibility score for about 12 million tweets was computed, allowing us to evaluate TweetCred in terms of accuracy, performance, effectiveness and usability. The system TweetCred built as part of this thesis work is used effectively by emergency responders, firefighters, journalists and general users to obtain credible content from Twitter. This thesis work has shown that measuring credibility of the Twitter content is possible using semi-automated techniques, and the results can be valuable to the real-world users. The insights obtained from this research and deployment provide a basis for building more sophisticated technology to tackle similar problems on different social media.",,2015.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4e2177a18dc326364d029107ba3435cfa4fa01cd,https://www.semanticscholar.org/paper/4e2177a18dc326364d029107ba3435cfa4fa01cd,Knowledge acquisition planning for inference from large databases,"It is shown that by combining techniques from machine learning and robot planning, knowledge acquisition (KA) planning provides a framework for automatically applying diverse and complex analytical tools to extremely large collections of data. KA planning is the process of automatically combining inferential tools such as induction, search, database lookup, and statistical analysis into methods for addressing complex query statements. This process depends on having both a domain model that supports subgoal decomposition and a library of KA actions annotated with the type and form of required input data, expected outcomes of the action, estimates of computational resources needed, and so on. When analytical tools are annotated in this way, it is possible to automatically select and deploy them and combine their results, even if they run on incompatible platforms. KA planning was implemented in a program that learned about diagnosing lung tumors. A larger project is underway in the domain of molecular biology.<<ETX>>",Twenty-Third Annual Hawaii International Conference on System Sciences,1990.0,10.1109/HICSS.1990.205173,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
892b0a92fd927cb0c6ac425285467c810328e2e4,https://www.semanticscholar.org/paper/892b0a92fd927cb0c6ac425285467c810328e2e4,FPGA-Based Real-Time Color Tracking for Robotic Formation Control,"In construction automation, the tracking of worker location or a moving object is important for labor monitoring, resource management, and machine coordination. For object tracking, a camera is often utilized to obtain information of construction vehicle motion which can then be employed in coordination controls. Real-time tracking of autonomous vehicles, particularly for the control of multiple targets in formations, still suffers from constraints imposed in computation resources. Here the field programmable gate array (FPGA) technology is applied in a prototypical tracking system for vehicles by using a CMOS camera to detect their color-tags. The raw image from the Bayer color pattern is used to indicate the 2-dimensional position of vehicles and encrypted infrared commands are issued to deploy them in a leader-follower formation. It is shown that the novel system-on-programmable-chip with parallel control cores design can efficiently handle color recognition and multi-vehicle control while significantly reduces memory requirement and computation time. Introduction The digital camera has become a popular sensor for monitoring and surveillance systems. By improvement of the semiconductor technology, modern digital cameras with high pixel number can provide greater image detail in various applications. A digital image sensor usually utilizes an array consists of the charged-couple device (CCD) or complementary metal oxide semiconductor (CMOS). By passing light through the lens and color filter array (CFA), the real image is transformed into the mosaic-like RGB image projected on the digital image sensor array (Lukac and Plataniotis, 2007). The raw RGB image then is reconstructed as a meaningful image for human perception by de-mosaicking (Lee, 2005) and color correction (Gonzalez and Woods, 2002). For sensing applications, the additional image processing to extract object’s color strength or contour (Nixon and Aguado, 2008) is required for every image pixel. Moreover, it should be noted that imaging processing consumes a lot of memory space and most of computation resources in a computer system. The digital camera is also a popular sensor in a multi-robot system (Fierro et al., 2002) for obstacle detection and environment learning. Such a system is a very valuable tool in the development of an automated coordination system for construction vehicles. In a real-time object tracking and detection system, particularly for moving targets, the picture acquisition and image processing speed critically determines the system performance. Low computation speed will cause the loss of precise driving control for the multirobot system and results in collisions. When the robot system needs a high resolution camera to improve imaging details of the object, the coherent time delay with massive computation for image processing will occur. On the other hand, even the powerful microprocessor can mitigate time delay, the higher power consumption is also unsuitable for a mini robot system with battery powered operation (Braunl, 2006). Thus a compromise scheme for the multi-vehicle system in image processing issues is sharing the image processing and strategy making tasks in an external server (Fierro et al., 2002). Therefore, the vehicle carries only a simple embedded system which captures and sends image data to the server by wireless in order to get a balance between power consumption and execution speed. Based on the same idea, we propose a new colour-based tracking system for vehicle coordination using color-tag recognition for identification with an external server. This scheme will achieve higher performance of power management and real-time control speed. In our external se when runn propose u designed b of low pow designed e digital cam et al., 2000 burden in leader-foll The pa prediction resource u Some disc Processi Image in comput better cho prediction Bayer Patt The m The co the circled need to re define the G ⇔ B ⇔ where Color Pred In real reflection 26th Internatio design, each v rver by an en ing the whol se the Field P y using an FP er, low cost, xternal server era equipped ), and the dyn the FPGA ch owing strateg per is organiz , and noise fil sage, and the ussion of the ng of Raw I processing wi ation speed. T ice for lower and noise filt ern osaic-like RG",,2009.0,10.22260/ISARC2009/0042,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
08edca06f440cf8ec8078a1fad12e467b041dac4,https://www.semanticscholar.org/paper/08edca06f440cf8ec8078a1fad12e467b041dac4,Work in Progress: Development and Implementation of a Self-guided Arduino Module in an Introductory Engineering Design Course,"This Work in Progress paper discusses the implementation of an online module designed to teach basic Arduino programming skills to students enrolled in a first-year engineering design course. The learning objectives for students were (1) to learn the basics of Arduino programming through hands-on activities, (2) to connect with the numerous online resources available for creating their own projects for personal or class purposes, and (3) to gain a sense of curiosity about what types of challenges and problems they may be able to solve with their newfound skills. This module was deployed in the introductory design course beginning Fall semester 2017, and feedback from the initial module participants has been largely positive. Preliminary results suggest that module participants were able to expand upon the basic skills taught during the module to utilize the technology in their semester design projects effectively. In addition, participants expressed curiosity about ways they could use this technology in the future. Introduction This Work in Progress paper describes the development and implementation of a self-guided online Arduino module in a required, multidisciplinary design course for first-year engineering students. This introductory engineering design course is a project-based course required for all engineering majors on campus as part of the freshman engineering program. The course, which begins with a design challenge description and culminates with students demonstrating a working physical prototype by the end of the semester, serves as a first exposure to the engineering design cycle for many students. This project-based approach is used by numerous other first year engineering programs and is certainly not unique to this particular course, but it comes with both advantages and disadvantages [1], [2], [3], [4], [5]. On the positive side, students typically enjoy the hands on aspect of the course and are able to experience firsthand what it takes to design and build a product. However, most first year students have minimal prior experience with the tools required for design and fabrication, and this inexperience can severely restrict the types of design challenges that can be assigned. Development of meaningful design projects that are still approachable for a first year student can be difficult, and this challenge can be compounded by the limited instructional time and large student-to-instructor ratio. In the case of this particular course, the assigned design projects have always had the option of a purely mechanical solution, as students were not required to have any prior knowledge of electronics, and teaching techniques such as basic circuitry and microcontroller programming was not the focus of the course. Most students ended up building purely mechanical devices using the woodworking shop available to course participants. There are always a handful of students each semester who utilize skills from past experiences such as high school robotics clubs or other programming activities to devise a solution that employs more than simple mechanics. These students’ solutions are often more creative and require more critical thinking and troubleshooting. End of the semester feedback showed that students enjoyed the project overall, but felt that their lack of prior programming and electronics knowledge severely limited their projects, especially compared to other students in the course who had prior experience in these areas. The development of the Arduino module was motivated by four semesters worth of similar feedback from course participants indicating that basic microcontroller programming knowledge would have been valuable not only for completing the projects in the introductory design course, but also for use in upper level engineering courses and projects of personal interest. Not only were students eager to learn microcontroller basics, instructors also recognized that by introducing students to these skills, the resulting projects may be higher quality and the range of design challenges that can be assigned may be broadened. Use of microcontroller technology in freshman level courses is not new, and these concepts have been taught through in-person laboratory instruction with positive results at numerous universities [2], [3], [6]. Additionally, multiple universities have employed a “flipped classroom” approach to teaching microcontroller basics with online instruction prior to in-class lab activities [1], [7], [8]. This work aims to build on these successful examples by developing and implementing a fully online and self-guided microcontroller module in the introduction to design curriculum. By designing the module for online instruction and support, the instructors were able to accommodate the large number of students, overcome lab space limitations, and minimize the required in-class time for the lessons. This online module was developed and deployed in the course beginning Fall semester 2017. While a similar module could be developed for any microcontroller, instructors chose to focus on the Arduino UNO for this beginner level module, as it is cost-effective (approximately $25 per board), widely used, and open source, with an active community of educators and hobbyists sharing content online [9]. The fully online Arduino module has the benefit of having minimal impact on the overall course format, and it allows students to work at their own pace through the lessons, as there is likely a wide range of incoming knowledge for students in this first year course. As there have been urgent calls for post-secondary institutions to produce engineers who not only have a sound technical background but are also entrepreneurially minded and critical thinkers [10], [11], this module was designed not only to provide students with technical skills, but also to inspire curiosity about other ways this technology could be used and encourage students to engage with the online community and resources related to this technology. While the Arduino module is intended to equip students with skills that can be used in the classroom setting, it is also intended to give students a glimpse into the growing Maker Movement both on campus and worldwide [12]. With this in mind instructors designed the module to meet the following three learning objectives: Learning Objective #1: Students will learn the basics of programming an Arduino Uno microcontroller board through hands-on activities. Learning Objective #2: Students will connect with the numerous online resources available for creating their own projects for class or personal purposes. Learning Objective #3: Students will gain a sense of curiosity about what types of challenges they may be able to solve using their new skills and connections to the online community. This paper will discuss the development of this module, the preliminary results from the initial module deployment in Fall semester 2017 including student response and incorporation of microcontroller technology into the final projects, and the future work to be done to fully evaluate the effectiveness of the module at meeting its three learning objectives. Course Overview and Arduino Module Implementation Course Overview The introductory engineering design course has an enrollment of nearly 600 students per semester and consists of one 50 minute lecture and one 50 minute lab session per week as well as an online software component administered through the Canvas learning management system. The lecture and laboratory content center around a semester-long team design and fabrication project. The design challenge is different each semester, with past projects including marble launchers, arcade game design, and Rube Goldberg machines. The first implementation of the Arduino module was in Fall 2017, and in this semester the design challenge required students to design a Rube Goldberg machine that could unlock a door in precisely 15 seconds using as many steps as possible. Teams of four students spent the entire semester learning the engineering design process through this hands-on project. To complement the skills students learn in lecture and lab, the online software component of the course consists of five-week long self-guided modules that provide students with basic instruction in common engineering software programs that may be useful for their design projects and in their future engineering coursework. Prior to Fall 2017, students chose two of four options (MATLAB, AutoCAD, Microsoft Excel, or Siemens NX) to learn over the course of the semester. Because this software framework was already in place, the initial Arduino module was developed to be offered as a fifth option for students to choose from. This implementation gave a large number of students access to the module content with minimal changes to the overall course structure. Arduino Module Development Before the online module was developed, the concept was tested in a live presentation format during two “Pop-Up Courses” offered during the spring of 2017. 14 students participated in these non-credit workshops, and they provided valuable insight about which skills the students were most interested in learning and the appropriate level of difficulty and depth for the course material. With this initial feedback, a five-week module was developed as outlined in Table 1. Table 1: 5-week Arduino Module Outline Week Instructional Content Assessments 1 Video 1: Arduino Hardware and Basic Electronics (11 minutes) Video 2: Arduino Software (5 minutes) Assignment, online quiz 2 Video 1: Writing Digital Signals (8 minutes) Video 2: Reading Digital Signals (8 minutes) Assignment, online quiz 3 Video 1: Serial Communications (5 minutes) Video 2: Analog Signals (13 minutes) Assignment, online quiz 4 Video 1: Libraries and Servos (10 minutes) Video 2: Review and Useful Resources (6 minutes) Assignment, online survey 5 None Comprehensive online quiz While a wide va",,2018.0,10.18260/1-2--31282,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e2a688d932133af07ff9075437866e990eb3cd9e,https://www.semanticscholar.org/paper/e2a688d932133af07ff9075437866e990eb3cd9e,Teaching robots social autonomy from in situ human supervision,"TEACHING ROBOTS SOCIAL AUTONOMY FROM IN SITU HUMAN SUPERVISION Emmanuel Senft Traditionally the behaviour of social robots has been programmed. However, increasingly there has been a focus on letting robots learn their behaviour to some extent from example or through trial and error. This on the one hand excludes the need for programming, but also allows the robot to adapt to circumstances not foreseen at the time of programming. One such occasion is when the user wants to tailor or fully specify the robot’s behaviour. The engineer often has limited knowledge of what the user wants or what the deployment circumstances specifically require. Instead, the user does know what is expected from the robot and consequently, the social robot should be equipped with a mechanism to learn from its user. This work explores how a social robot can learn to interact meaningfully with people in an efficient and safe way by learning from supervision by a human teacher in control of the robot’s behaviour. To this end we propose a new machine learning framework called Supervised Progressively Autonomous Robot Competencies (SPARC). SPARC enables non-technical users to control and teach a robot, and we evaluate its effectiveness in Human-Robot Interaction (HRI). The core idea is that the user initially remotely operates the robot, while an algorithm associates actions to states and gradually learns. Over time, the robot takes over the control from the user while still giving the user oversight of the robot’s behaviour by ensuring that every action executed by the robot has been actively or passively approved by the user. This is particularly important in HRI, as interacting with people, and especially vulnerable users, is a complex and multidimensional problem, and any errors by the robot may have negative consequences for the people involved in the interaction. Through the development and evaluation of SPARC, this work contributes to both HRI and Interactive Machine Learning, especially on how autonomous agents, such as social robots, can learn from people and how this specific teacher-robot interaction impacts the learning process. We showed that a supervised robot learning from their user can reduce the workload of this person, and that providing the user with the opportunity to control the robot’s behaviour substantially improves the teaching process. Finally, this work also demonstrated that a robot supervised by a user could learn rich social behaviours in the real world, in a large multidimensional and multimodal sensitive environment, as a robot learned quickly (25 interactions of 4 sessions during in average 1.9 minutes) to tutor children in an educational game, achieving similar behaviours and educational outcomes compared to a robot fully controlled by the user, both providing 10 to 30% improvement in game metrics compared to a passive robot.",,2018.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4867a63e5edd7ee10f28067d6dd5b07d69321de7,https://www.semanticscholar.org/paper/4867a63e5edd7ee10f28067d6dd5b07d69321de7,Thesis Title: A Framework for Improving the Performance of Signature-based Network Intrusion Detection Systems,"Network Intrusion detection systems (NIDSs) have been widely deployed in different network environments (e.g., banks, schools) to defend against a variety of network attacks (e.g., Trojans, worms). Generally, a network intrusion detection system can be classified into two categories: signature-based NIDS and anomaly-based NIDS. In realworld applications, the signature-based NIDS is more prevalent than the anomaly-based detection as the false alarm rate of the former is much lower than the latter. However, we identify three major issues that can greatly affect the performance of a signature-based NIDS. Expensive signature matching. The traditional signature matching in a signature-based NIDS is too expensive that the computing burden is at least linear to the size of an incoming string. Therefore, the operational burden of a signature-based NIDS could be significantly increased in a large-scale network environment. Overhead network packets. In a large-scale network environment, a signature-based NIDS usually has to drop lots of network packets since the number of incoming packets exceeds its maximum processing capability. Massive false alarms. Although the false alarm rate of a signature-based NIDS is much smaller than that of an anomaly-based NIDS. The number of false alarms generated by a signature-based NIDS can still increase the difficulty in analyzing true alarms and adversely affect the analysis results. To mitigate the above issues, in this thesis, we propose several approaches in improving the performance of a signature-based NIDS such as Snort in the following three aspects: Signature matching improvement.We design an exclusive signature matching scheme to help perform a more efficient signature matching with the purpose of enhancing the performance of signature matching in a heavy traffic environment. Network packet filtration and reduction. To mitigate this issue, we advocate the method of constructing a packet filter such as blacklist-based packet filter, list-based packet filter and trust-based packet filter to help filter out target network packets for a signature-based NIDS such as Snort in terms of IP reputation. This packet filter can be deployed in front of a signature-based NIDS and reduce its workload in an intensive traffic network. False alarm reduction. To resolve this issue, we design several false alarm filters such as machine-learning based false alarm filters, alarm filters using knowledge-based alert verification and context-based alarm filters to help reduce false alarms (or non-critical alarms) that are generated by a signature-based NIDS. A Framework. In addition, we further propose a framework by combining the above work to overall improve the performance of a signature-based NIDS such as Snort. As a case study of the framework, we implement an enhanced filter mechanism (shortly EFM) that consists of three major components: a context-aware blacklist-based packet filter, an exclusive signature matching component and a KNN-based false alarm filter. In particular, the component of context-aware blacklist-based packet filter is responsible for filtering out network packets in terms of IP reputation. The exclusive signature matching component is implemented in the context-aware blacklist-based packet filter and aims to speed up the signature matching. At last, the component of KNN-based false alarm filter is responsible for filtering out false alarms which are produced by the context-aware blacklist-based packet filter and the NIDS. In the evaluation, the experimental results demonstrate that our framework is promising and by deploying with the EFM, the performance of a signature-based NIDS such as Snort can be improved in the aspects of network packet filtration, signature matching improvement and false alarm reduction.",,2013.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
349e889830c94a87ae601fdbc849ee72053d4ef0,https://www.semanticscholar.org/paper/349e889830c94a87ae601fdbc849ee72053d4ef0,Editorial: Introduction to the Special Issue with Papers from the Fifth International Conference on Development and Learning (ICDL),"In June 2006, 168 researchers from 11 countries converged on the campus of Indiana University Bloomington to participate in the Fifth International Conference on Development and Learning (ICDL 05). Ever since its inaugural meeting at Michigan State University in 2000, ICDL has had a unique thematic focus. The conference brings together empirical scientists working in a broad range of natural and social sciences, such as developmental psychologists and neuroscientists, with researchers who are more focused on theoretical models and engineering, such as neural network or robotic systems. The goal is to identify principles of learning and development that are held in common between natural and artificial systems (Weng et al., 2001). The payoff is twofold: capturing these principles will pave the way to a better understanding of how development unfolds in animals and humans, and it will also lead to new kinds of technological systems that are capable of autonomous development. Learning and development are traditional core topics for the journal Adaptive Behavior. This special issue presents a cross-section of cutting edge topics and approaches exemplified by five papers presented at ICDL 05. Eighteen out of a total of 93 papers that were presented at ICDL 05 received nominations by reviewers and members of the program committee to be included in this special issue. The five papers presented here were among those that received the highest overall rankings. After the conference, incorporating discussion and feedback, these papers were extensively revised and expanded, and they underwent a second round of peer review. The papers cover a range of topics, including reinforcement learning, the development of visual attention, modeling gaze following, the construction of interaction histories for autonomous agents, and the concept of distributed intelligence. Together they provide a snapshot of the breadth and sophistication of this emerging interdisciplinary effort into fundamental principles of autonomous development. Reinforcement learning, and more specifically temporal difference (TD) learning, has been a strong focus in machine learning as well as in human and animal experimentation. While it has been quite successful as a framework for specific forms of biological learning, relatively little work has been done to investigate its interactions with other components of a cognitive architecture, or in the context of autonomous behavior. William Alexander’s article attempts to integrate TD learning with models of attentional shifting in an autonomous agent. The proposed model incorporates effects of learning on the attentional representation of stimuli, and it does so in real time. The results point to a clear performance gain and better match to empirical data when attentional effects are combined with TD learning. This illustrates the importance of building models that integrate different functionalities and operate within a common cognitive architecture. Matthew Schlesinger, Dima Amso, and Scott Johnson have studied the role of visual attention in infant development and argued that active deployment of attentional resources is crucial for the development of a visual object perception, specifically visual completion. Building on this empirical work, the current article presents a detailed model of the developing visual system in an attempt to use the model to gain insight into",Adapt. Behav.,2007.0,10.1177/1059712307078641,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4e40f03c01ddc62673bb419910f28f906cac9d6b,https://www.semanticscholar.org/paper/4e40f03c01ddc62673bb419910f28f906cac9d6b,Modeling of Magnetic Fields and Extended Objects for Localization Applications,"The level of automation in our society is ever increasing. Technologies like selfdriving cars, virtual reality, and fully autonomous robots, which all were unimaginable a few decades ago, are realizable today, and will become standard consumer products in the future. These technologies depend upon autonomous localization and situation awareness where careful processing of sensory data is required. To increase efficiency, robustness and reliability, appropriate models for these data are needed. In this thesis, such models are analyzed within three different application areas, namely (1) magnetic localization, (2) extended target tracking, and (3) autonomous learning from raw pixel information. Magnetic localization is based on one or more magnetometers measuring the induced magnetic field from magnetic objects. In this thesis we present a model for determining the position and the orientation of small magnets with an accuracy of a few millimeters. This enables three-dimensional interaction with computer programs that cannot be handled with other localization techniques. Further, an additional model is proposed for detecting wrong-way drivers on highways based on sensor data from magnetometers deployed in the vicinity of traffic lanes. Models for mapping complex magnetic environments are also analyzed. Such magnetic maps can be used for indoor localization where other systems, such as gps, do not work. In the second application area, models for tracking objects from laser range sensor data are analyzed. The target shape is modeled with a Gaussian process and is estimated jointly with target position and orientation. The resulting algorithm is capable of tracking various objects with different shapes within the same surveillance region. In the third application area, autonomous learning based on high-dimensional sensor data is considered. In this thesis, we consider one instance of this challenge, the so-called pixels to torques problem, where an agent must learn a closedloop control policy from pixel information only. To solve this problem, highdimensional time series are described using a low-dimensional dynamical model. Techniques from machine learning together with standard tools from control theory are used to autonomously design a controller for the system without any prior knowledge. System models used in the applications above are often provided in continuous time. However, a major part of the applied theory is developed for discretetime systems. Discretization of continuous-time models is hence fundamental. Therefore, this thesis ends with a method for performing such discretization using Lyapunov equations together with analytical solutions, enabling efficient implementation in software.",,2015.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7991b7b0317c0d29bc209f18acc18b5933ac71be,https://www.semanticscholar.org/paper/7991b7b0317c0d29bc209f18acc18b5933ac71be,"Part B 1 ] 1 ( to be evaluated in Step 1 ) Ubiquitous , spatiotemporal , multimodal action recognition Act Now","Action and activity recognition lie at the core of a panoply of scenarios in human machine interaction, ranging from gaming, mobile computing and video retrieval to health monitoring, surveillance, robotics and biometrics. The problem, however, is made really challenging by the inherent variability of motions carrying the same meaning, the unavoidable over-fitting due to limited training sets, and the presence of numerous nuisance factors such as locality, viewpoint, illumination, and occlusions that make real-world deployment extremely difficult. The most successful recent approaches, which mainly classify bags of local features, have reached their limits: only understanding the spatial and temporal structure of human activities can help us to successfully locate and recognise them in a robust and reliable way. We propose here to develop novel frameworks for the integration of spatiotemporal action structure in both generative and discriminative models, pushing for a breakthrough ripe with enormous exploitation potential. A new class of hierarchical part-based discriminative models originally developed for object recognition are reinvented for action localisation and recognition, as a fundamental way of coping with complex activities formed by series of simple actions and addressing the issues with locality and multiple actors. New manifold learning techniques for generative graphical models are developed to tackle the presence of nuisance factors and improve their generalisation power. Finally, novel classes of graphical models able to handle whole convex sets of probabilities are formulated in order to address the issue of overfitting due to the limited size of the training sets. As companies are heavily investing on virtual mice, smart TVs, phones and cars, and range sensors are changing clinical practice and the entertainment industry, the timeliness and potential impact of this project could not be understated.",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0e6393c4a76e8d802e347fbaec7c31d8a469bcc9,https://www.semanticscholar.org/paper/0e6393c4a76e8d802e347fbaec7c31d8a469bcc9,A Model Based Design Approach To System Building Using The e-Yantra Educational Robot,"The e-Yantra robot is the basis for a highly scalable embedded systems teaching program setting up 500 embedded systems labs in Indian engineering colleges. A key strategy to encourage rapid prototyping of applications has been to encourage reuse of code using a commodity robot with a standard API along with excellent documentation and training material. An important challenge has been to teach the reasoning process from a design through to an implementation deployed on an actual machine. Model based design is key to articulating such reasoning. A further challenge is to do this in an affordable manner where most available model-based IDEs are expensive proprietary systems using languages such as Esterel and SCADE. We illustrate with a ""Valet Parking"" application how our robotic eco-system facilitates the learning of important model-based design principles taking a high-level specification of a problem down to working code and even deriving test cases in the process. A novel feature of our approach is that we carry out design-time scheduling of various (concurrent) activities by analyzing dependencies between modules and obtain purely sequential C-code implemented on a microcontroller without the need for an RTOS. This case study is an exemplar of a model-based design approach for a large class of such robotic projects.",,2014.0,10.1145/2829957.2829963,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f2888e4f1458e6ccdfd7c6943c7ca390cbfa2819,https://www.semanticscholar.org/paper/f2888e4f1458e6ccdfd7c6943c7ca390cbfa2819,An Approach to a Complete Recognition Module for Legged Robots in the RoboCup Domain,"A need for autonomous electro-mechanical machines performing tasks that can save human efforts and even be deployed in hazardous environments along with the research in the field of human mind and body has led to new fields often called AI Robotics. To provide a mean for stimulating students and researchers around the world for promotion of this new field, an annual world championship, Robocup, has been organized with start in 1997. During these competitions, the state-of-the-art technical achievements within Sensor Systems, Mechanics, Artificial Intelligence, Machine Learning and many other related fields are presented by the participants. The final goal is to create autonomous humanoids that can win a soccer match against humans by 2050. 
 
Computer Vision and Robotics are tightly bound together as vision is the most important part of the sensorimotor system enabling a robot to act in a dynamic environment. This includes recognizing landmarks for localization, obstacles for collision avoidance, and objects for manipulation. To go one step further in this direction, the challenge within Sony AIBO quadruped ERS-7 class for year 2004 was to recognize objects in different lighting conditions and to recognize and remember the position of newly detected landmarks. For this class of robots, the playground was a green carpet, surrounded by white walls. Two nets and four beacons in four different two-color combinations were the only permanent landmarks. 
 
The object recognition module was decided only to be designed for rendering the task of recognition of all predetermined as well as unknown landmarks on and around the field given a set of image frames containing shapes presented as color blobs. Most importantly, the module should be designed in a way that it could recognize the regular objects independent of the lighting conditions and calculate the angle and distance to each one as well as the confidence for the calculated results. The outcome was thereafter to be saved in a common space for the other modules to use and act upon. 
 
As the most challenging object to recognize and estimate the distance and view angle to is the ball, the step by step approache to a successful solution is presented in the report. The next subject mostly discussed is how to recognize the nets, where several factors affect the precision and confidence. Other subjects as filtering the information, how to consider the pose of the head, and how to use a voting system for raising the confidence are discussed thoroughly. Also some pros and cons of each method are looked at. For the reasons discussed throughout the text, the recognition was primarily based on color verification and not the shape of the blobs. The module was implemented in Matlab for unit test and to verify the theories and later on implemented, assembled, and compiled in C++ for the final run. 
 
Although individual tests in the lab environment showed on a successful module in recognizing objects on and around the field, lack of a consistent color segmentation table resulted in a poor quality of recognition in variable lighting conditions during the competitions. Nevertheless, the module is reusable as the methods are as general as possible to fit different platforms with diverse frame rates and dimensions. 
 
Regardless of the outcome, the thesis work presented in this report lays a ground for a vision module that can perform the task of recognition of known and unknown objects. In particular, it reports on the methodology used for recognition of objects with estimation of their distances and angles to the AIBO, for which a software vision module is implemented. The contribution of this thesis is the methodology for detection of the peripheral of the ball blob, using a voting system, an attempt to merge blobs, specially for finding the rectangular shapes and correct estimation of their sizes, and an investigation about using rule based verification of the objects, methods that had not been used before by any teams in previous years.",,2010.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,https://www.semanticscholar.org/paper/88ea92d8b618525ff2a7815c08f0fc2c68d8daa2,Human Activity Inference via physical sensing in support of Industrial Equipment Maintenance,"The paper describes an active research project at Intel’s High Volume Manufacturing (HVM) facility located at Leixlip, Co. Kildare, Ireland. The project explores the practical aspects of deploying RFID transponders, subtle sensing platforms and machine learning based inferencing in a harsh, realworld environment. The key features of the sensing platform, the data collection process and the translation of data into information using visualization and inferencing techniques are described.",,2006.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3516e17605a0e18918f898a2283c4e94a70d4e72,https://www.semanticscholar.org/paper/3516e17605a0e18918f898a2283c4e94a70d4e72,Opportunities and Challenges in Association and Episode Discovery from Electronic Health Records,"83 guidance; machine understanding of human behavior, emotional, and physiological states; and the need to respond appropriately to unintended stimuli. Numerous machine perception , cognition, and communication challenges also remain such as image-guided intervention, speech and language understanding, two-hand-like manipulative dexterity, and learning systems that adapt to an individual's long-term change of state. Solutions likely lie at the intersection of new and ongoing research in computer science, materials, psychology, and neuroscience. The societal pressure to mitigate the healthcare crisis presents an unprecedented opportunity for computing , information science, and engineering. Whereas the pursuit of understanding the pathogenesis of disease will be accelerated with new algorithms and increasingly powerful computation and data architec-tures, we look to other computation-enabled means to provide additional avenues to the pursuit of quality of life. Multidisciplinary approaches are required to engineer a privacy-maintaining information infrastructure with secure, real-time access to unprecedented amounts of heterogeneous health, medical, and treatment data. New generations of algorithms must be developed to utilize the resulting global resource of population-based evidence for assisted discovery, knowledge creation, and even individual point-of-care decisions. Ana-lytics based on modeling phenomena ranging from the physiology of humans to their social interactions are required to optimize therapies ranging from molecular medicine to be-havioral interventions. Such advances in human-centered computing in combination with standardization and commercialization of unobtrusive sensing and robotics will trigger a disruptive change in health-care and wellbeing by empowering individuals to more directly participate. Finally, partnerships among academic , industrial, and governmental bodies are required to enable these computer science innovations and realize their deployment in order to help transform healthcare. Will Barkis is a AAAS science and technology policy fellow at the American Association for the Advancement of Science. Contact him at wbarkis@gmail.com. As healthcare practices, both small and large, move from traditional paper-based patient charts to electronic health records (EHRs), new opportunities are emerging for secondary uses of data captured as part of routine care. Such opportunities include not only traditional research meth-odologies involving relatively small cohorts of selected patients, but also large-scale data mining analyses encompassing hundreds of thousands or even millions of patients at once. Performing these nontraditional analyses has required novel computational approaches, sometimes borrowing from techniques originally developed in other elds such as genomics and network theory. Additionally, to interpret such large volumes of data in a meaningful",,,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0e4a015b0cb2863e7d010f22d443cee43c1c7957,https://www.semanticscholar.org/paper/0e4a015b0cb2863e7d010f22d443cee43c1c7957,Improving the adaptability of multi-mode systems via program steering,"A multi-mode software system contains several distinct modes of operation and a controller for deciding when to switch between modes. Even when developers rigorously test a multi-mode system before deployment, they cannot foresee and test for every possible usage scenario. As a result, unexpected situations in which the program fails or underperforms (for example, by choosing a non-optimal mode) may arise. This research aims to mitigate such problems by creating a new mode selector that examines the current situation, then chooses a mode that has been successful in the past, in situations like the current one. The technique, called program steering, creates a new mode selector via machine learning from good behavior in testing or in successful operation. Such a strategy, which generalizes the knowledge that a programmer has built into the system, may select an appropriate mode even when the original controller cannot. We have performed experiments on robot control programs written in a month-long programming competition. Augmenting these programs via our program steering technique had a substantial positive effect on their performance in new environments.",ISSTA '04,2004.0,10.1145/1007512.1007540,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d77278dbeb60609f780aff5b8b0672bfe19d3e6c,https://www.semanticscholar.org/paper/d77278dbeb60609f780aff5b8b0672bfe19d3e6c,"Computer Aided Systems Theory - EUROCAST 2005, 10th International Conference on Computer Aided Systems Theory, Las Palmas de Gran Canaria, Spain, February 7-11, 2005, Revised Selected Papers",,EUROCAST,2005.0,10.1007/11556985,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1386d1f29cd34dcf426da6dd4badc484e9c3a0e9,https://www.semanticscholar.org/paper/1386d1f29cd34dcf426da6dd4badc484e9c3a0e9,A novel robotic sound localization and separation using non-causal filtering and Bayesian fusion,"Binaural localization and separation of two concurrent sound sources using a humanoid head is a challenging task, especially if the localization is done in the three dimensions using only the two ears of the humanoid. From a biological perspective, localization of sound sources, and the perceptual separation of sound mixtures by the human hearing organ are two closely interrelated processes. From a signal processing perspective, both localization and separation may be treated as two separate problems. This paper presents a novel method to localize and separate the sound sources, which are situated in high reverberation areas through the adaptation of a Multiple-Input-Multiple-Output system (MIMO) with a Bayesian network. It is shown that, taking account of connections between blind system identification and blind source separation, the sound sources are not only efficiently separated, they can also be accurately localized in the 3D space surrounding the humanoid head. This artificial head is equipped with two inner microphones and two outer microphones. The presented algorithm deploys a non-causal filter system using the known neural networks based principle of information propagation in combination with the machine learning based technique of Bayesian networks. Simulation results validated with experimental measurements showed that this method is outperforming in terms of localization accuracy and in solving the well-known front-back confusion problem.",2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP),2016.0,10.1109/MLSP.2016.7738857,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
32dc930d36bc786be018f0ae3790b05ee3b45774,https://www.semanticscholar.org/paper/32dc930d36bc786be018f0ae3790b05ee3b45774,"Artificial Intelligence: Methodology, Systems, and Applications, 14th International Conference, AIMSA 2010, Varna, Bulgaria, September 8-10. 2010. Proceedings",,AIMSA,2010.0,10.1007/978-3-642-15431-7,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7a314e3ad8bf9edafe4782cac865d4a07a10b58b,https://www.semanticscholar.org/paper/7a314e3ad8bf9edafe4782cac865d4a07a10b58b,Programming Robots using High-Level Task Descriptions,"In order to be truly robust, deployed robot systems must be capable of adaptation. No matter how much we know about a particular task or environment, some amount of on-site tweaking is inevitable if we want the system to perform as well as it can. This tweaking causes the behavior of the robot to change, often by only a small amount, to better fit its environment. Machine learning can be used to “program” mobile robots, given some initial knowledge. We present a method of gathering this knowledge without the need for technical expertise in robotics or programming. The method relies on semanitic information from restricted natural language descriptions of the task to be performed. The behavior can then be modified interactively without knowledge of the underlying control mechanisms. This method allows us to tap into the experience of local experts who have done the task before, without having to train them first in computers or robotics.",,2004.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
482b87b9f4833bb87def32fa940606e4ae63ae83,https://www.semanticscholar.org/paper/482b87b9f4833bb87def32fa940606e4ae63ae83,Preface: Toward a Growing Computational Replica of the Human Mind,"The fields of artificial intelligence, machine learning, machine vision, and robotic control have all produced successes that demonstrate superiority of machines over the human for narrow problems, and have even sometimes performed tasks that are impossible for humans. Despite these successes, a true human-level machine intelligence remains a far-off dream. Nevertheless, people of the 21st century are looking forward to new intelligent artifacts: partners rather than tools, machines that may become useful members of the human society or replace humans at the frontiers. Are we ready to take this challenge seriously? A contraargument goes like this. Producing a replica of a Picasso painting does not make the painter a Picasso. Similarly, a capable but brittle machine can be engineered to take on some cognitive tasks of a human, but it may fail to work in an unexpected domain. Yet the current approach to engineering new technology is focused on incremental expansion and integration of machine replicas of human intelligent capabilities — e.g., replacing traffic cops and housemaids with intelligent traffic lights and Roombas. Extrapolating on this trajectory, it seems that future artifacts are bound to continue automating more and more cognitive tasks that were previously considered as exclusively human expertise. This process results in high-cost state-of-the-art solutions for narrow problems, while even the most intelligent of these systems will never be on an equal scale with humans. This approach is unlikely to produce a chain-reaction factor that could change our civilization in several years. Yet, there is an alternative to the approach of highly-engineered incremental development and integration. A way to understand the flavor of this alternative is by looking at what we may expect from it, and how it is different from the above. Our main expectation for the new technology is its low cost: it should make most human-level cognitive abilities available at a much lower price than they are available today (in their natural or machine replica form). How? By designing the systems so that they can learn on their own, set their own learning goals and develop their own skills in new and unanticipated domains. As a result, artifacts will be able to grow up, taking care of themselves in their own design, education, development and deployment, with minimal and gradually vanishing over time cost of required human interventions during this process! In other words, artifacts will be able to decide what abilities to develop and …",AAAI Fall Symposium: Biologically Inspired Cognitive Architectures,2008.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5514b5016d57049b3064a11873822e16de5c151a,https://www.semanticscholar.org/paper/5514b5016d57049b3064a11873822e16de5c151a,Multi-modal features for real-time detection of human-robot interaction categories,"Social interactions unfold over time, at multiple time scales, and can be observed through multiple sensory modalities. In this paper, we propose a machine learning framework for selecting and combining low-level sensory features from different modalities to produce high-level characterizations of human-robot social interactions in real-time.
 We introduce a novel set of fast, multi-modal, spatio-temporal features for audio sensors, touch sensors, floor sensors, laser range sensors, and the time-series history of the robot's own behaviors. A subset of these features are automatically selected and combined using GentleBoost, an ensemble machine learning technique, allowing the robot to make an estimate of the current interaction category every 100 milliseconds. This information can then be used either by the robot to make decisions autonomously, or by a remote human-operator who can modify the robot's behavior manually (i.e., semi-autonomous operation).
 We demonstrate the technique on an information-kiosk robot deployed in a busy train station, focusing on the problem of detecting interaction breakdowns (i.e., failure of the robot to engage in a good interaction). We show that despite the varied and unscripted nature of human-robot interactions in the real-world train-station setting, the robot can achieve highly accurate predictions of interaction breakdowns at the same instant human observers become aware of them.",ICMI-MLMI '09,2009.0,10.1145/1647314.1647337,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
797bf13ab7b10aca7b1a51dd05d2b9037cbc8144,https://www.semanticscholar.org/paper/797bf13ab7b10aca7b1a51dd05d2b9037cbc8144,Robotic Platform: A XML-Based Extensible Robot Intelligence Architecture (XRIA),,ICSR,2010.0,10.1007/978-3-642-17248-9_5,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ce8633de279a4937820c0e103ab706982a0125bd,https://www.semanticscholar.org/paper/ce8633de279a4937820c0e103ab706982a0125bd,Work in progress - an integrated programming environment suitable for distance learning,"Autonomous robots are used as a vehicle to learn programming in C and Java, to design neural network controllers, to experiment with finite state machines and to explore operating system concepts. Our environment is deployed as (i) physical robots, (ii) multi-robot simulation written as a Java application, (iii) distributed multi-robot simulation using client-server architecture running on the Web. This paper details the design rationale, implementation, deployment and reports evaluation of the work to date. The software is freely available from the author at c.price@worc.ac.uk.","34th Annual Frontiers in Education, 2004. FIE 2004.",2004.0,10.1109/FIE.2004.1408454,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ad5d928e3b8da09a8433539f4cdf2fb8c3d202b3,https://www.semanticscholar.org/paper/ad5d928e3b8da09a8433539f4cdf2fb8c3d202b3,Rolling locomotion of a deformable soft robot with built-in power source,"In this paper we present a neural network architecture for learning of robotic grasping tasks. The neural network model allows acquisition of different neural representations of the grasping task through a successive learning over two stages in a strategy that uses already learned representations for the acquisition of the subsequent knowledge. Systematic computer simulations have been carried out in order to test learning and generalization capabilities of the system. The proposed model can be used as a high level controller for a robotic dexterous hand during learning and execution of grasping tasks. Adaptive stair-climbing behaviour with a hybrid legged-wheeled robot M. Eich, F. Grimminger and F. Kirchner Robotics Group German Research Center for Artificial Intelligence Bremen, Germany markus.eich@dfki.de Abstract Inspired by quadruped animals we developed the hybrid legged-wheeled robot ASGUARD. We showed already that this robot is able to cope with a variety of stairs, very rough terrain, and is able to move very fast on fat ground. We will describe a versatile adaptive control approach for such a system which is based only on proprioceptive data. An additional inclination and roll feedback is used to make the same controller more robust in terms of stair climbing capabilities. At the same time, high velocities can be reached on fat ground without changing the configuration of the system. By using twenty compliant legs, which are mounted around four individually rotating hip-shafts, we abstract from the biological system. For the locomotion control we use an abstract model of bio-inspired Central Pattern Generators (CPG), which can be found in biological systems from humans to insects. In contrast to existing work, ASGUARD uses the sensed feedback of the environment to adapt the walking pattern in real time.Inspired by quadruped animals we developed the hybrid legged-wheeled robot ASGUARD. We showed already that this robot is able to cope with a variety of stairs, very rough terrain, and is able to move very fast on fat ground. We will describe a versatile adaptive control approach for such a system which is based only on proprioceptive data. An additional inclination and roll feedback is used to make the same controller more robust in terms of stair climbing capabilities. At the same time, high velocities can be reached on fat ground without changing the configuration of the system. By using twenty compliant legs, which are mounted around four individually rotating hip-shafts, we abstract from the biological system. For the locomotion control we use an abstract model of bio-inspired Central Pattern Generators (CPG), which can be found in biological systems from humans to insects. In contrast to existing work, ASGUARD uses the sensed feedback of the environment to adapt the walking pattern in real time. On the mechanized inspection of glass fiber plastic pipes and pipe joints P. Chatzakos, A. Lagonikas, D. Psarros, V. Spais, K. Hrissagis and A. Khalid Zenon SA, 5 Kanari str., Glyka Nera, Athens, Greece. TWI Ltd, Granta Park, Great Abington, Cambridge, United Kingdom. pchatzak@zenon.gr Abstract In this paper the development of a robotic multi-axis crawler to accurately deploy inspection equipment on glass fiber plastic pipe and pipe joints is presented. The developed crawler is able to carry a range of sensors to automatically inspect complex geometry components, making traditional scanners redundant. First results show that the dexterity of the mechanized system is such that component coverage and positional accuracy are maximized using only a minimum number of degrees of freedom.In this paper the development of a robotic multi-axis crawler to accurately deploy inspection equipment on glass fiber plastic pipe and pipe joints is presented. The developed crawler is able to carry a range of sensors to automatically inspect complex geometry components, making traditional scanners redundant. First results show that the dexterity of the mechanized system is such that component coverage and positional accuracy are maximized using only a minimum number of degrees of freedom. Development and gait generation of the biped robot stepper-senior Y. Liu, M. Zhao, J. Zhang, L. Li, X. Su and H. Dong Department of Automation Tsinghua University Beijing, China liuyu@mails.tsinghua.edu.cn Abstract This paper presents the development and gait generation of biped robot Stepper-Senior. Parallel double crank mechanism and elastic materials are introduced in the 10 DOF lower limbs to mechanically restrict the sole to contact flat with the ground for stable fast walking. Virtual Slope Walking is used for biped gait generation, which is a simple method with strongly intuitive parameters for real-time utilization. In walking experiment, Stepper-Senior reaches the speed of 0.65 m/s and accomplishes omnidirectional walking.This paper presents the development and gait generation of biped robot Stepper-Senior. Parallel double crank mechanism and elastic materials are introduced in the 10 DOF lower limbs to mechanically restrict the sole to contact flat with the ground for stable fast walking. Virtual Slope Walking is used for biped gait generation, which is a simple method with strongly intuitive parameters for real-time utilization. In walking experiment, Stepper-Senior reaches the speed of 0.65 m/s and accomplishes omnidirectional walking. Development of an underground explorer robot based on an earthworm’s peristaltic crawling H. Omori, T. Nakamura and T. Yada Department of Precision Mechanics, Faculty of Science and Engineering Chuo University Tokyo, Japan h_omori@bio.mech.chuo-u.ac.jp Abstract Although shield tunnel construction and tunnel boring machines have developed a great deal, the machines are still large in size and consume large amounts of energy. A robot small enough to investigate under the ground by itself would extend the range of underground investigation both under the Earth‟s surface and, in the future, on the moon. This study focuses on the peristaltic crawling of an earthworm as a locomotion mechanism for an underground explorer robot. In peristaltic crawling, extension and contraction waves are propagated in the anteroposterior direction by varying the thickness and length ofAlthough shield tunnel construction and tunnel boring machines have developed a great deal, the machines are still large in size and consume large amounts of energy. A robot small enough to investigate under the ground by itself would extend the range of underground investigation both under the Earth‟s surface and, in the future, on the moon. This study focuses on the peristaltic crawling of an earthworm as a locomotion mechanism for an underground explorer robot. In peristaltic crawling, extension and contraction waves are propagated in the anteroposterior direction by varying the thickness and length of the earthworm‟s segments, and a large surface area is brought into contact during motion. Furthermore, it requires no more space than that of an excavation part on the anterior of the robot. The proposed robot consists of several parallel link mechanisms. We confirmed that the robot could move on a plane surface, also climb a tube, and in perforated dirt. Good performance was observed in the experiments. Rolling locomotion of a deformable soft robot with built-in power source Y. Matsumoto, H. Nakanishi and S. Hirai Department of Robotics Ritsumeikan University Shiga, Japan rr008033@se.ritsumei.ac.jp Abstract Locomotion over rough terrain has been achieved mainly by rigid body systems, including crawlers and leg mechanisms. We have proposed an alternative method, which uses deformation of a robot body, and developed a prototype of this deformable robot. But, electric power was externally supplied, such that power supply cables hindered the locomotion of the robot. We describe here the rolling locomotion of a deformable soft robot with an internally supplied power source. We applied dynamic simulation with particle-based modelling to analyze the rolling motion of this robot and found that increased weight had little influence on the kinematics performance of this robot on flat surfaces. Increased weight, however, was effective in providing greater stability on slopes.Locomotion over rough terrain has been achieved mainly by rigid body systems, including crawlers and leg mechanisms. We have proposed an alternative method, which uses deformation of a robot body, and developed a prototype of this deformable robot. But, electric power was externally supplied, such that power supply cables hindered the locomotion of the robot. We describe here the rolling locomotion of a deformable soft robot with an internally supplied power source. We applied dynamic simulation with particle-based modelling to analyze the rolling motion of this robot and found that increased weight had little influence on the kinematics performance of this robot on flat surfaces. Increased weight, however, was effective in providing greater stability on slopes. GRACE – Generic robotic architecture to create emotions T. H. H. Dang, S. L. Zarshenas and D. Duhaut IFI Francophone Institute for Informatics South Brittany University Vannes, France dthha@ifi.edu.vn Abstract In this paper, we present a model of emotions that we proposed in EmotiRob project. We make a comparison of recent models of emotions and show that our model is generic in basing on the theories of emotions of Ortony et al., Lazarus, Scherer and then the personality theory of Meyers-Brigg and Meyers.In this paper, we present a model of emotions that we proposed in EmotiRob project. We make a comparison of recent models of emotions and show that our model is generic in basing on the theories of emotions of Ortony et al., Lazarus, Scherer and then the personality theory of Meyers-Brigg and Meyers. Motion design for an insectomorphic robot on unstable obstacles Y. F. Golubev and V. V. Korianov Mech.-Math. dept., Lomono",,2008.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
896466136b49f02799dc19bda44f872f43383c4a,https://www.semanticscholar.org/paper/896466136b49f02799dc19bda44f872f43383c4a,Multi-cue Perception for Robotic Object Manipulation : How Spatio-temporal Integration of Multi-modal Information Aids Task Execution,"The thesis presents a robotic perception system for manipulation tasks in human living environments, that eases the deployment of autonomous service robots in new environments by enabling it to autonomously learn, model and localize the objects it perceives through the integration of geometric categorization and instance recognition. To do so, we designed, tested and analyzed novel methods for: (i) learning general object-related principles from the Internet based on their part composition, (ii) computing descriptors that facilitate generalization between instances of the same category, (iii) classification that is robust to large variations between training and testing data, (iv) and surface reconstruction capable of dealing with partial information and cluttered scenes, while respecting physical constraints. These enable robots to take advantage of their capability to explore their surroundings and that they are equipped with sensors with different advantages, by the integration of prior, spatio-temporal and multi-cue information. The developed system and the underlying modules and principles were shown to outperform alternative solutions in extensive experiments, and were applied in realworld robotic demonstration scenarios, proving the advantage of an embodied, taskadapting approach towards perception.",,2014.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
79954a9cff07652b73e99064c87c0eeb0c95b640,https://www.semanticscholar.org/paper/79954a9cff07652b73e99064c87c0eeb0c95b640,A framework for an artificial immunity and speech based navigation for mobile robots,In recent years speech recognition technology and immunity based algorithms have made an impact in various areas and are deployed for a wide range of applications. This paper describes a learning process of a mobile robot which takes speech input as commands and performs some navigation task through a distinct man-machine interaction with the application of the learning based on the Artificial Immune System. For this purpose a 4-channel radio controlled Wheelbot and Microsofpsilas Speech SDK for speech recognition is employed. The speech recognition system is trained to recognize defined commands and the robot has been designed to navigate based on the instruction through the Speech Commands. The position of the obstacles are learnt and avoided by the help of immune algorithm.,2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence),2008.0,10.1109/CEC.2008.4630956,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a1304687f648ccf5abe7cd6e74eed1c4acaa852c,https://www.semanticscholar.org/paper/a1304687f648ccf5abe7cd6e74eed1c4acaa852c,"Artificial Intelligence and Computational Intelligence, International Conference, AICI 2009, Shanghai, China, November 7-8, 2009. Proceedings",,AICI,2009.0,10.1007/978-3-642-05253-8,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
41bb9b5e794e7528c0691283a2e27a7b57bde711,https://www.semanticscholar.org/paper/41bb9b5e794e7528c0691283a2e27a7b57bde711,ROILA : RObot Interaction LAnguage,"The number of robots in our society is increasing rapidly. The number of service robots that interact with everyday people already outnumbers industrial robots. The easiest way to communicate with these service robots, such as Roomba or Nao, would be natural speech. However, the limitations prevailing in current speech recognition technology for natural language is a major obstacle behind the unanimous acceptance of speech interaction for robots. Current speech recognition technology is at times not good enough for it to be deployed in natural environments, where the ambience influences its performance. Moreover, state-of-art automatic speech recognition has not advanced far enough for most applications, partly due to the inherent properties of natural languages that make them difficult for a machine to recognize. Examples are ambiguity in context and homophones (words that sound the same but have different meanings). As a consequence of the prior discussed problems at times miscommunication occurs between the user and robot. The mismatch between humans’ expectations and the abilities of interactive robots often results in frustration for the user. Palm Inc. faced a similar problem with hand writing recognition for their handheld computers. They invented Graffiti, an artificial alphabet, that was easy to learn and easy for the computer to recognize. Our Robot Interaction Language (ROILA) takes a similar approach by offering a speech recognition friendly artificial language that is easy to learn for humans and easy to understand for robots with an ultimate goal of outperforming natural language in terms of speech recognition accuracy. There exist numerous artificial languages, Esperanto for example; but to the best of our knowledge these artificial languages were not designed to optimize human machine/robot interaction but rather to improve human-human communication. The design of ROILA was an iterative process having iterations within each step. It started off with a linguistic overview of a pre-selection of existing artificial languages across the dimensions of morphology (grammar) and phonology (the sounds of the language). The artificial languages were also analyzed in comparison to natural languages. The overview resulted in a number of linguistic trends that we would carefully incorporate in the design of ROILA with the claim that whatever linguistic features are common amongst these exist- ing languages would be easier to learn if they are made part of ROILA. The actual construction of the ROILA language began with the composition of its vocabulary. A genetic algorithm was implemented which generated the best fit vocabulary. In principle, the words of this vocabulary would have the least likelihood of being confused with each other and therefore be easy to recognize for the speech recognizer. Experimental evaluations were conducted on the vocabulary to determine its recognition accuracy. The results of these experiments were used to refine the vocabulary. The third phase of the design was the design of the grammar. Using the questions, options, and criteria (QOC) technique, rational decisions were made regarding the selection of grammatical markings. Recognition accuracy and ease of human learnability were two important criteria. In the end we drafted a simple grammar that did not have irregularities or exceptions in its rules and markings were represented by adding isolated words rather than inflecting existing words of a sentence. As a conclusion to the design phase and also as a proof of concept we designed an initial prototype of ROILA by using the LEGO Mindstorms NXT platform. ROILA was demonstrated in use to instruct a LEGO robot to navigate in its environment, analogous to the principles of the turtle robot. As a final evaluation of ROILA we conducted a large scale experiment of the language. ROILA was exposed to Dutch high school students who spent three weeks learning and practicing the language. A ROILA curriculum was carefully designed for the students to aid them in their learning both in school and at home. In-school learning was more interactive and hands on as the students tested their ROILA skills by speaking to and playing with LEGO robots. At the end of the curriculum the students attempted a ROILA proficiency test and if successful they were invited to play a complete game with a LEGO robot. Throughout the whole learning process, subjective and objective experiences of the students was measured to determine if indeed ROILA was easy to learn for the students and easy to recognize for the machine. Our results indicate that ROILA was deemed to have a better recognition accuracy than English and that it was preferred more by the students in comparison to English as their language of choice while interacting with LEGO Mindstorms robots.",,2011.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1751a9e03580d90e7fe2cf6b0cbf70f1200ee95f,https://www.semanticscholar.org/paper/1751a9e03580d90e7fe2cf6b0cbf70f1200ee95f,AUTOMATIC OPTIMIZATION USING INTEGRATED PERFORMANCE PRIMITIVES How to Make Use of IPP,"The rapid expansion of computer processing power combined with the rapid development of digital camera capability has resulted in equally rapid advances in computer vision capability and use. Intel has long been at the forefront of enabling this advance on the computer hardware and software side. Computer vision software is supported by the free Open Source Computer Vision Library (OpenCV) that optionally may be highly optimized by loading the commercial Intel Integrated Performance Primitives (IPP). IPP now automatically supports OpenCV with no need to change or even recompile the user’s source code. This functionality enables development groups to deploy vision and provides basic infrastructure to experts in vision. OpenCV has long supported “geometric vision” from camera calibration, motion tracking in 2D, finding the camera location given a known 3D object, on up to producing depth maps from stereo vision. This paper describes using OpenCV for “learning-based vision,” where objects such as faces, or patterns such as roads, are learned for segmentation and recognition. INTRODUCTION The Open Source Computer Vision Library (OpenCV) [1] is a free, open source collection of computer vision routines geared mainly towards human-computer interaction, robotics, security, and other vision applications where the lighting and context of use cannot be controlled. OpenCV is not geared towards factory machine vision where the environmental conditions can be controlled and one generally knows what one is looking for, although there is a large overlap. OpenCV was designed for enablement and infrastructure. Many groups who could make use of vision were prevented from doing so due to lack of expertise; OpenCV enables these types of groups to add functionality such as face finding and tracking in a few lines of code. Other groups have vision expertise but were uselessly recreating vision algorithms that were already standard; OpenCV provides experts with a solid vision infrastructure and thereby allows experts to work at a higher level rather than have to worry about the basics. Because of the above, OpenCV’s BSD type license is designed to promote free commercial and research use. Optionally, users may install the IPP libraries and benefit from highly optimized code without needing to recompile via the use of automatically selected optimized dynamic linked libraries. OpenCV support for vision is extensive. It supports routines for input, display, and storage of movies and single images. Image-processing debug is supported by drawing and text display functions. Image processing itself is handled through convolution, thresholding, morphological operations, floodfills, histogramming, smoothing, pyramidal sub-sampling and a full suite of image algebra and arithmetic. Geometry is supported by Delaney triangulation, calibration, fundamental and essential matrices computation, image alignment, and stereo depth calculation. A full range of feature detection algorithms exists from corner detectors, Canny edge operators, blob finders, scale invariant features, and so on. Shape descriptors such as Hu moments, contour processing, Fourier descriptors, convex hulls, and connected components exist. Motion is covered via several types of optical flow, background learning and differencing, motion templates, and motion gradients. Learning-based vision is supported through feature histogram comparison, image statistics, template-based correlation, decision trees, and statistical boosting on up to convolutional neural networks. OpenCV was released in Alpha in 2000, Beta in 2003, and will be released in official version 1.0 in Q4 2005. If the Intel Integrated Performance Primitives (IPP) library [2] is Intel Technology Journal, Volume 9, Issue 2, 2005 Learning-Based Computer Vision with Intel’s Open Source Computer Vision Library 120 optionally installed, OpenCV will automatically take advantage of and swap in the hand optimized routines in IPP providing a substantial speed-up to many vision routines. In this paper, we describe computer vision routines based on learning. Learning-based vision has applications to image-based Web mining, image retrieval, video indexing, security, etc. OpenCV has strong and growing support for learning-based vision. We start out, however, by first discussing a recent change to OpenCV, full IPP support, and then move on to discuss two learning applications. We begin by describing automatic optimization of OpenCV using IPP and then we discuss using OpenCV for learned object finding and tracking (face), and end with abstract pattern segmentation (road finding). AUTOMATIC OPTIMIZATION USING INTEGRATED PERFORMANCE PRIMITIVES How to Make Use of IPP Intel Integrated Performance Primitives (IPP) library is a large collection of low-level computational kernels highly optimized for Intel architectures, including the latest Pentium, Itanium, and XScale processors. It consists of multiple domains that reside in separate dynamic libraries: signal and image processing, matrix processing, audio and video codecs, computer vision, speech recognition, cryptography, data compression, text processing, etc. It can be retrieved from http://www.intel.com/software/products/ipp [2]; full evaluation versions for Windows and Linux , and a free non-commercial version for Linux are available. OpenCV is able to automatically detect and use the IPP library once the latter is installed; there is no need to recompile it. On Windows, make sure that the bin subdirectory of IPP is in the system path, for example, if IPP is installed to “C:\Program Files\Intel\IPP”, add “C:\Program Files\Intel\IPP\bin” to the path. On Linux the IPP dynamic libraries should be already in one of the standard paths after installation. To check whether OpenCV has found IPP or not, the user application may call the cvGetModuleInfo() function: ® Pentium, Itanium, and XScale are all registered trademarks of Intel Corporation or its subsidiaries in the United States and other countries. ∗ All other brands and names are the property of their respective owners. const char* opencv_libraries = 0; const char* addon_modules = 0; cvGetModuleInfo( 0, &opencv_libraries,",,2005.0,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3eb124a43e3802348d7c716e3a963c6d3a959ed8,https://www.semanticscholar.org/paper/3eb124a43e3802348d7c716e3a963c6d3a959ed8,Wendell Wallach and Colin Allen: moral machines: teaching robots right from wrong,,Ethics and Information Technology,2010.0,10.1007/s10676-010-9239-1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b216d74b2f54ebbc675060b80cbe3b0c8c7c0fb3,https://www.semanticscholar.org/paper/b216d74b2f54ebbc675060b80cbe3b0c8c7c0fb3,"FUZZY LOGIC AND PROBABILITY APPLICATIONS: Bridging the Gap, edited by Timothy J. Ross, Jane M. Booker and W. Jerry Parkinson. Society for Industrial and Applied Mathematics, Philadelphia, and American Statistical Association, Alexandria, Virginia, 2002, xxiii + 409 pages, ISBN 0-89871-525-3.","LEARNING BAYESIAN NETWORKS, by Richard E. Neapolitan. Prentice Hall, Upper Saddle River, NJ, 2004, xv þ 674 pages, ISBN 0-13-012534-2. This book is the first unified text as well as a useful reference on the theory and applications of Bayesian networks. It deals with probabilistic modeling, especially in the areas of computer science, computer engineering, and electrical engineering. It is also a valuable resource for courses on expert systems, machine learning, and artificial intelligence. Appropriate for classroom teaching or self-instruction, the text is organized to provide fundamental concepts in an accessible, practical format. Beginning with a basic theoretical introduction, the author then provides a comprehensive discussion of inference, methods of learning, and applications based on Bayesian networks. The book contains hundreds of examples and problems which make learning of rather complex concepts easy. ENTROPY MEASURES, MAXIMUM ENTROPY PRINCIPLE AND EMERGING APPLICATIONS, edited by Karmeshu. Springer-Verlag, Heidelberg and New York, 2002, x þ 297 pages, ISBN 3-540-00242-1. This book is dedicated to the memory of Professor J. N. Kapur for his significant contributions to the field of entropy measures and maximum entropy applications. BOOK REVIEWS AND ABSTRACTS 638 D ow nl oa de d by [ Q ue en sl an d U ni ve rs ity o f T ec hn ol og y] a t 1 3: 57 3 1 O ct ob er 2 01 4 Eminent scholars from various fields of applied information theory have contributed to this festschrift volume which was planned on the occasion of his 75th birthday. The articles cover topics in the areas of physical, biological, engineering and social sciences such as information technology, soft computing, nonlinear systems and molecular biology with a thematic coherence. The volume will be useful to researchers working in these fields enabling them to see the underlying unity and power of entropy optimization frameworks. MULTI-ROBOT SYSTEMS: From Swarms to Intelligent Automata, Volume III, edited by Alan C. Schultz, Lynne E. Parker, and Frank E. Schneider. Kluwer Academic Publishers, Dordrecht, Boston and London, 2003, ix þ 307 pages, ISBN 1-4020-1185-7. This proceedings volume documents recent cutting-edge developments in multi-robot systems research and is the result of the Second International Workshop on Multi-Robot Systems that was held in March 2003 at the Naval Research Laboratory in Washington, D.C. This workshop brought together top researchers working in areas relevant to designing teams of autonomous vehicles, including robots and unmanned ground, air, surface, and undersea vehicles. The workshop focused on the challenging issues of team architectures, vehicles learning adaptation, heterogeneous group control and cooperation, task selection, dynamic autonomy, mixed initiative, and human and robot team interaction. A broad range of applications of this technology are presented in this volume including UCAVs (Unmanned Combat Air Vehicles), planetary exploration, assembly in space, cleanup, and urban search and rescue. This proceedings volume represents the contributions of the top researchers in this field and serves as a valuable tool for professionals in this interdisciplinary field. NUMERICAL AND ANALYTICAL METHODS FOR SCIENTISTS AND ENGINEERS USING MATHEMATICA, by Daniel Dubin. Wiley-Interscience, Hoboken, NJ, 2003, xvi þ 633 pages, ISBN 0-471-26610-8. Written from the perspective of a physicist rather than a mathematician, this book focuses on modern practical applications in the physical and engineering sciences, attacking these problems with a range of numerical and analytical methods, both elementary and advanced. Incorporating the widely used and highly praised Mathematicaw software package, the author offers solution techniques for the partial differential equations of mathematical physics such as Poisson’s equations, the wave equation, and Schrödinger’s equation, including Fourier series and transforms, Green’s functions, the method of characteristics, grids, Galerkin and simulation methods, elementary probability theory, and statistical methods. The book is designed for both advanced undergraduate and graduate students in the physical and engineering sciences, as well as professionals who want to learn these methods. The full text of the book is also provided electronically on an accompanying CD, along with animotions, user-modifiable source code, and links to related Web material. BOOK REVIEWS AND ABSTRACTS 639 D ow nl oa de d by [ Q ue en sl an d U ni ve rs ity o f T ec hn ol og y] a t 1 3: 57 3 1 O ct ob er 2 01 4 COMPUTATIONAL INTELLIGENT SYSTEMS FOR APPLIED RESEARCH: Proceedings of the Fifth International FLINS Conference, edited by Da Ruan, Pierre D’hondt, and Etienne E. Kerre. World Scientific, Singapore, 2002, xii þ 591 pages, ISBN 981-238-066-3. FLINS is an acronym for Fuzzy Logic and Intelligent Technologies in Nuclear Science. FLINS 2002 is the fifth in a series of FLINS conferences and covers state-of-the-art research and development in computational intelligence for applied research in general and for unclear science and engineering in particular. This book outlines the trends in computational intelligence in control, decision-making, and nuclear engineering, and presents the latest developments of computational intelligent systems in applied research and nuclear applications. ENGINEERING AND SCIENTIFIC COMPUTATIONS USING MATLAB, by Sergey E. Lyshevski. Wiley-Interscience, Hoboken, NJ., 2003, x þ 227 pages, ISBN 0-471-46200-4. Going beyond the traditional MATLAB user manuals and college texts, this book guides you through the most important aspects and basics of MATLAB programming and problem solving from fundamentals to practice. Augmenting its discussion with a wealth of practical worked-out examples and qualitative illustrations, the book demonstrates MATLAB’s capabilities and offers step-by-step instructions on how to apply the theory to a practical realworld problem. The book is user-friendly and comprehensive in scope. NON-LOCALITY AND MODALITY, edited by Tomasz Placek and Jeremy Butterfield. Kluwer Academic Publishers, Dordrecht, Boston and London, 2002, x þ 352 pages, ISBN 1-4020-0661-6. This book collects together the invited contributions to the NATOAdvanced Research Workshop on Modality, Probability, and Bell’s Theorems, which was held in the Kolegium Polonijne in Cracow, Poland, on August 19–23, 2001. It contains 20 contributions that are divided into several groups. The first group consists of three papers that give a philosophical re-examination of the views of Bohr and von Neumann on quantum theory, especially concerning non-locality. It is followed by a group of papers regarding the various interpretations of quantum theory and a group focusing on entanglement and non-locality. The next group of papers applies unsharp quantum theory to the Bell and Kochen-Specker theorems. Finally, there are two groups of papers about more philosophical aspects of quantum theory, one focusing on philosophical doctrines about causation and one on doctrines about modality. THE NATURE OF TIME: Geometry, Physics, and Perception, edited by Rosolino Buccheri, Metod Saniga, and William Mark Stuckey. Kluwer Academic Publishers, Dordrecht, Boston and London, 2003, xvii þ 446 pages, ISBN 1-4020-1200-4. The book is a product of the NATO Advanced Research Workshop on the Nature of Time that was held in Tatranská Lomnica, Slovakia, on May 21-24, 2002. It consists of BOOK REVIEWS AND ABSTRACTS 640 D ow nl oa de d by [ Q ue en sl an d U ni ve rs ity o f T ec hn ol og y] a t 1 3: 57 3 1 O ct ob er 2 01 4 40 contributions that are grouped into four chapters: 1. Internal Times and Consciousness; 2. Mathematical Approaches to the Concept of Time; 3. The Physicist’s View of Time; and 4. Integrative Science’s View of Time. Each chapter also contains an overview of issues covered in that chapter. As these chapter titles suggest, the concept of time is examined from several distinct perspectives. The contributions taken as a whole represent the most up-todate scholarly research on the enigmatic nature of time. The book should be of interest to readers of this journal since it contains a lot of material pertaining to systems of various kinds. SYSTEMATIC ORGANISATION OF INFORMATION IN FUZZY SYSTEMS, edited by Pedro Melo-Pinto, Horia-Nicolai Teodorescu, and Toshio Fukuda. IOS Press, Amsterdam, 2003, viii þ 399 pages, ISBN 1-58603-295-X. This volume is based on contributions presented at the NATO Advanced Workshop (devoted to the subject area described by the title of the volume), which took place in Vila Real, Portugal, on October 24–26, 2001. It consists of 24 contributions that discuss recent developments in the area of fuzzy systems, especially those pertaining to their role in information processing and knowledge discovery. The book is an important resource for researchers and graduate students in computer, information and systems sciences. NONLINEAR DIFFERENCE EQUATIONS: Theory with Applications to Social Science Models, by Hassan Sedaghat. Kluwer Academic Publishers, Dordrecht, Boston, and London, xv þ 388 pages, ISBN 1-4020-1116-4. This book presents a rare mix of the latest mathematical theory and procedures in the area of nonlinear difference equations and discrete dynamical systems, together with applications of this theory to models in economics and other social sciences. The theoretical results include not only familiar topics on chaos, bifurcation, stability and instability of cycles and equilibria, but also some recently published and some as yet unpublished results on these and related topics (e.g. the theory of semiconjugates). In addition to rigorous mathematical analysis, the book discusses several social science models and analyzes some of them in substantial detail. This book is of potential interest to professionals and graduate students in mathematics and applied mathematics, as well as researchers in",Int. J. Gen. Syst.,2003.0,10.1080/03081070310001618272,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
17e42ec161af3e4281649649f3562d5f45cb1e4b,https://www.semanticscholar.org/paper/17e42ec161af3e4281649649f3562d5f45cb1e4b,Theoretical advances and applications of intelligent paradigms,"What is an “intelligent paradigm”? While it is difficult to provide an exact definition for this term, the interests in intelligent paradigms research generally focus on designing and developing of computerized machines or systems that exhibit the capabilities of learning from experience, adapting to the surrounding environment, as well as understanding and controlling ones thinking or reasoning process. These are some crucial characteristics of intelligent paradigms so that they can be deployed as usable and useful tools to assist humans in daily activities. Indeed, recent research and development in intelligent paradigms has opened up the way for a number of theoretical advances and successful applications of intelligent techniques and approaches in various domains. Diverse intelligent paradigms are available in the literature, encompassing, to name a few, artificial neural networks, evolutionary algorithms, multiagent systems, artificial immune systems, swarm intelligence, knowledge-based systems, case-based reasoning, as well as hybrid intelligent systems in which these paradigms are contained. Applications of intelligent paradigms also span across various fields, covering, to name a few, information processing, decision making, control and robotics, industrial and medical diagnosis, data mining, e-learning and e-commerce, knowledge management, as well as virtual reality and multimedia. In this special issue, a total of eight articles are collected to showcase a small fraction of some recent advances in theory and application of intelligent paradigms. These articles present research into various intelligent paradigms, with their effectiveness in tackling different real-world problems demonstrated and discussed. A brief outline of each article is as follows. A powered wheelchair is an effective vehicle to help elderly or handicapped people move around the ordinary areas. Song et al. design of an electromyogram (EMG) pattern classifier that is robust against muscular fatigue effects for powered wheelchair control. It is discovered that variations of feature values owing to the effect of muscular fatigue are consistent for sustained duration. This finding leads to a new fatigue compensation method, and the fuzzy Min-Max neural network is employed as a robust EMG-based pattern classifier through the adaptation process of its hyperboxes. The proposed approach demonstrates improved performance for continuous control of powered wheelchair. Path planning is a fundamental problem in mobile robotics. The work by Chakraborty et al. addresses the issue of multi-robot path planning by using parallel differential evolution algorithms. Both centralized and distributed realizations for multi-robot path planning are studied, and the performances of the methods are compared with respect to a few pre-defined yardsticks. Most industrial plants are complex, nonlinear, time varying with time delay, and difficult to control. A fuzzy logic-based system is suitable for their control as it combines measurements, experts’ knowledge and op-",J. Intell. Fuzzy Syst.,2009.0,10.3233/IFS-2009-0410,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
acac5733e9c12a0655006905aae814b85402408b,https://www.semanticscholar.org/paper/acac5733e9c12a0655006905aae814b85402408b,The impact of robots on socio-environmental fields,"Robots – machines with artificial intelligence, physical movement, and environmental awareness – are increasing in sophistication and areas of use. Robots entered society in substantial numbers only 40 years ago as factory labour. They now occupy homes, hospitals, farms, and battlefields as an unpaid caste in a split-labour market. Their capacity for autonomous behaviour and experiential learning permits them to team with humans in some tasks while replacing them in others. Task sharing has shaped robot evolution toward an android model. As they displace humans, robots will thin out the social capital upon which human solidarity depends. While not a ‘species’ threat, their use as industrial and military tools concentrates wealth and power, sharpens class conflict, and increases international inequality. Because they lack the biological dependence that is the structural basis for environmental care, their deployment tends to erode environmental sustainability as well as reciprocity-based human social solid...",,2011.0,10.1504/IER.2011.038880,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8ec56c3b91e96f59a91293e7b6d1522d347fad18,https://www.semanticscholar.org/paper/8ec56c3b91e96f59a91293e7b6d1522d347fad18,Cross-Language Interfacing and Gesture Detection with Microsoft Kinect,"Stage provides an immersive visualization environment for mission operations, in which we integrate Google Streetview and the Kinect Natural User Interface (NUI). The Kinect is controlled through a C++ dynamic link library, where it is interfaced to other environments: Java, Unity, and Teamcenter Visualization. Through the Kinect API, we retrieve the positions of various skeleton points, and use them to perform gesture processing and identification. The goal of the project is expand the uses of Stage to more than mission operations, and to explore the degree of seamlessness that we can achieve by using the Kinect NUI. This will highly depend on the types of gestures that are used to perform commands as well as the accuracy of the detection algorithms. We found that the learning curve of our system to be low since the number and complexity of the commands are low. However, other factors such as fatigue limit the feasibility of the NUI as a primary input method. 1. Background Stage is an experimental visualization environment for mission operations. For it to serve this purpose, it should handle three functions: input, display, and control. We define the following terms as follows: Input—Collect and process data from device (robot, spacecraft, etc.) Display—Display data from device to user in a usable format Control—Process user’s input and converts to commands recognized by device As of now, the display function is fairly complete, and we have fabricated input by using Mars’ ground data and Google Streetview. Control functions are purely client-side for the time being. The distinguishing factor of Stage is that the display is 12 ft. high, 270 degree cylindrical shaped. Figure 1 illustrates the physical configuration of Stage. Three projectors are required to illuminate the whole screen. The advantage this set-up has compared to traditional display methods is that it maintains the relative position of objects. The code-base of Stage is written in Java. The 3D environment is rendered and managed by the Ardor3D library. In 3D rendering, the maximum view angle a camera is allowed is about 179. A piecewise rendering trick is used to achieve the full 270 degree view angle. Figure 1. Stage Configuration 2. Google Streetview Undocumented API We have found that the raw Google Streetview image data is not available through their public API. Therefore we access them through an undocumented API described by Jamie Thompson. It describes the sequence and methods on retrieving the raw image data of a given latitude and longitude pair. Note: There is a problem regarding the assumption of image dimensions (refer to Appendix A) in Thompson’s article. The result is a spherical projected image of the location desired. This spherical image can be displayed correctly by creating a sphere and mapping the Streetview image to the sphere as a texture within the 3D scene. 3. Google Streetview Integration We have added support for Google Streetview in Stage. It takes a pair of latitude and longitude coordinates, and retrieves the image data from Google’s servers. The image is processed and rendered with the 270 degree view geometry. Basically, the behavior of the program is similar to Google Earth, but with a wider view angle. This includes the ability to walk around various locations on earth, and the program will appropriately load and transition between images. 4. Cross-Language Kinect Interface Stage supports keyboard, mouse, and Nintendo Wii Remote inputs. Now we would like to include Microsoft Kinect support. The challenge associated with this is that Stage is written in Java, and the Kinect API supports C/C++ and C#. We solved this problem by writing interface code on each end: Java and C++. Our C++ interface is a dynamic link library (DLL) which uses the Kinect API and exports the Kinect functions. The memory mapping of the data structures is known, so any program that can call functions from this DLL should be able to retrieve data coming from the Kinect. We call this the wrapper DLL. The Java side of the interface accesses the wrapper DLL through the Java Native Access (JNA) library. The JNA library allows us to load the DLL and call functions through the DLL and store the output to a memory block. The raw data is remapped into a proper Java-native data structure that we can use to perform gesture processing. We use a similar architecture set-up to interface the Kinect to Unity, a 3D game development tool, and Teamcenter Visualization, a CAD design tool. We will focus more on the implementation details used for Stage, however. Refer to the figures found in Appendix B. 5. Gesture Detection The method used to detect gestures in Stage is similar to the operation of a deterministic finite automaton (DFA). It is a clearly defined DFA in which at each state the system evaluates a condition, which can return either -1, 0, or 1. A value of -1 signals the system to return to the beginning state, 0 will result in no state change, and 1 will cause the system to advance a state. All states are ordered and there is no skipping of states or backtracking except for returning to the beginning state. A gesture is detected when the final state is reached. Figure 2 is a DFA of a four state gesture. We generalize that a gesture with more states are more complex. For example, our implementation of a wave gesture consists of four states. The gestures used to control Stage are probably the simplest type of gestures possible, which are composed of two states. Figure 2. DFA of Four State Gesture 6. Challenges Ardor3D was quite difficult to work with. The documentation was difficult to find that we thought it did not exist. It required inspection of the source code to understand the behavior when we needn’t involve ourselves with the implementation details. The gestures we used in Stage are simple. If we wish to implement more complex gestures, the deconstruction of the gesture will be difficult. The wave gesture we have implemented as a test did not have a good detection rate. It resulted in primarily false negatives. There is also the possibility of multiple gestures interfering with each other. 7. Discussion In developing and testing the gestures, we have found that they can allow a user to finely control the camera and perform movements. However, we have had issues with unintended control, which is inherent with this type of input. It requires the user to focus only on controlling Stage. We have also found that the behavior of the Kinect is not consistent across different machines. Another issue with using the Kinect is user fatigue. The average user will become tired after about one hour of use, whereas traditional input methods can be used for several hours before fatigue sets in. While this disqualifies the Kinect as a primary input method, it may be a valuable tool as a secondary input device. 8. Future Work Stage is currently in a dismantled state. For future work, we would like to deploy the software and use it on the actual Stage platform instead of performing tests on standard display devices. This would allow us to test factors such as whether one Kinect sensor is sufficient for the whole environment. In terms of using Stage for a mission operation, there is lots of work needed to be done. To begin, a stitching algorithm for constructing either spherical or cylindrical projected images from realtime camera data can be written. Another important function for Stage to have is the ability to issue RAPID commands, which is a communications protocol for controlling robots. Future performance requirements will necessitate that Stage be ported over to a faster programming language, such as C/C++ or C#. We are currently having performance difficulties with processing and rendering Streetview images with the Java environment. It seems unlikely that Stage will be able to process and 30-60 frames per second of panoramic images without significant performance enhancements. Acknowledgments The work described in this paper was supported by the National Science Foundation under grant number 0852088 to California State University, Los Angeles. It was carried out at the Jet Propulsion Laboratory, California Institute of Technology, under a contract with the National Aeronautics and Space Administration. We would also like to acknowledge David Torosyan for collaborating with the work done on Stage, the CURE Program, and Section 317F. References Ardor3D. Retrieved 15 Aug 2011. . Thompson, Jamie. Google Streetview Static API. 15 May 2010. . UNITY: Game Development Tool. Retrieved 15 Aug 2011. . Vrsinsky, Jan. 360Cities – Panoramic Photography Blog. 10 June 2010. . Appendix A. Google Streetview Undocumented API Given the latitude/longitude or panoId of a location, retrieve the metadata. The panoId is a unique identifier string for a location, which is used internally by Google. The metadata can be retrieved at the following URLs: http://cbk0.google.com/cbk?output=json&ll=[latitude],[longitude] http://cbk0.google.com/cbk?output=json&panoid=[panoId] The output type can be changed to ‘xml’ if xml output is desired. The metadata is used to retrieve the panoId of the location. With the panoId, it is possible to retrieve the image tiles and construct the spherical projected image. Several tiles put together form the entire image. The dimension of each tile is 512x512 pixels. Tiles are accessed at the following URL: http://cbk0.google.com/cbk?output=tile&panoid=[panoId]&zoom=[zoo m]&x=[x]&y=[y] Possible values for vary from 1-5, but values of 4 and 5 may not be available at all areas. The and parameters are used to indicate the coordinates of the tile piece; these values are always positive and start at 0,0 . The maximum values of these coordinates vary, even for identical zoom levels. In the case of an invalid , , or parameter, it will return a single channel image which will be all black. This makes the detection of out of bound values simple. Retrieve the tiles by ",,2011,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a995b6410bda8827ace150ea94302be49a5fb2c4,https://www.semanticscholar.org/paper/a995b6410bda8827ace150ea94302be49a5fb2c4,Climbing and Walking Robots: Proceedings of the 8th International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines (CLAWAR 2005),"Plenary Papers.- Common Situation Awareness as Basis for Human-Robot Interface.- Gait Restoration by Functional Electrical Stimulation.- Space Robotics.- Bio-Engineering and Biological Inspired Systems.- ASYSTENT - Control System of a Manipulator for Keyhole Surgery.- A Biologically Inspired Model For Quadruped Locomotion.- Fuzzy Logic Control Strategy for Functional Electrical Stimulation in Bipedal Cycling.- Insect-inspired, Actively Compliant Hexapod Capable of Object Manipulation.- Modeling and Simulation of Humanoid Stair Climbing.- Design Issues of Spring Brake Orthosis: Evolutionary Algorithm Approach.- Recent Developments in Implantable and Surface Based Dropped Foot Functional Electrical Stimulators.- Fluidically Driven Robots with Biologically Inspired Actuators.- Climbing, Navigation and Path Planning.- Concept for Energy-autarkic, Autonomous Climbing Robots.- Navigation of Walking Robots: Path Planning.- Study on Mobility of Connected Crawler Robot by Using GA.- A Robot that Climbs Walls using Micro-structured Polymer Feet.- Novel Solutions to Design Problems of Industrial Climbing Robots.- Fast Pointing and Tracking System for Mobile Robot Short Range Control via Free Space Optical Laser Line of Sight Communication Link.- Control of CLAWAR.- 4 Legs Positioning Control of the Quadruped Robot by LVS using Stereo Ominidirectional Camera.- Control of a 3D Quadruped Trot.- Research on Obstacle-navigation Control of a Mobile Robot for Inspection of the Power Transmission Lines Based on Expert System.- Measure of the Propulsion Dynamic Capability of a Walking System.- Experimental Walking Results of LUCY, a Biped with Pneumatic Artificial Muscles.- Hip Joint Control of a Legged Robot for Walking Uniformly and the Self-lock Mechanism for Compensating Torque Caused by Weight.- Controlling Dynamic Stability and Active Compliance to Improve Quadrupedal Walking.- Complete Stability Analysis of a Control Law for Walking Robots with Non-permanent Contacts.- Vision-based Stabilization of the IDP Flat Output.- Bus Communication in Robot System Control.- A Hybrid Locomotion Control Approach.- Model and Control of Joints Driven by Fluidic Muscles with the Help of Advanced Automatic Algorithm Generation Software.- Modelling and Control of a X4 Bidirectional Rotors.- Stability Measure Comparison for the Design of a Dynamic Running Robot.- Control Architecture and Walking Strategy for a Pneumatic Biped Robot.- Time-scaling Control of a Compass Type Biped Robot.- Design Methodology and Gait Generation.- Mechanical Design of Step-Climbing Vehicle with Passive Linkages.- Integrated Structure-control Design of Dynamically Walking Robots.- Intuitive Design and Gait Analysis for a Closed Loop Leg Mechanism of a Quadruped with Single Actuator.- Design of a Cockroach-like Running Robot for the 2004 SAE Walking Machine Challenge.- Finding Adequate Optimization Criteria to Solve Inverse Kinematics of Redundant Bird Leg Mechanism.- Integrated System of Assisted Mechatronic Design for Oriented Computer to Automatic Optimising of Structure of Service Robots (SIDEMAR).- The Construction of the Four Legged Prototype Robot ARAMIES.- Application of Waves Displacement Algorithms for the Generation of Gaits in an All Terrain Hexapod.- Extensive Modeling of a 3 DOF Passive Dynamic Walker.- Development of Biped Robots at the National University of Colombia.- Design of a Low Cost Force and Power Sensing Platform for Unmanned Aerial Vehicles.- Hopping and Legged Robots.- Observer Backstepping for Height Control of a Resonance Hopping Robot.- Standing up with Motor Primitives.- Multiple Terrain Adaptation Approach Using Ultrasonic Sensors for Legged Robots.- Sliding Mode Observer with No Orientation Measurement for a Walking Biped.- Humanoid Robots.- Detection and Classification of Posture Instabilities of Bipedal Robots.- Development of a Low-Cost Humanoid Robot: Components and Technological Solutions.- Analysis of Humanoid Robot Lower Extremities Force Distribution in Standing Position.- ZMP Human Measure System.- Mechanical Design and Dynamic Analysis of the Humanoid Robot RH-0.- Advanced Motion Control System for the Humanoid Robot Rh-0.- Humanoid Vertical Jump with Compliant Contact.- Locomotion.- A 3D Galloping Quadruped Robot.- Kineto-static Analysis of an Articulated Six-wheel Rover.- Momentum Compensation for the Dynamic Walk of Humanoids Based on the Optimal Pelvic Rotation.- Walk Calibration in a Four-legged Robot.- Peristaltic Locomotion: Application to a Worm-like Robot.- Impact Shaping for Double Support Walk: From the Rocking Block to the Biped Robot.- Proposal of 4-leg Locomotion by Phase Change.- Introducing the Hex-a-ball, a Hybrid Locomotion Terrain Adaptive Walking and Rolling Robot.- Stability Control of an Hybrid Wheel-Legged Robot.- Manipulation and Flexible Manipulators.- Hybrid Control Scheme for Tracking Performance of a Flexible system.- Predesign of an Anthropomorphic Lightweight Manipulator.- Design of a ""Soft"" 2-DOF Planar Pneumatic Manipulator.- Simulation and Experimental Studies of Hybrid Learning Control with Acceleration Feedback for Flexible Manipulators.- BNN-based Fuzzy Logic Controller for Flexible-link Manipulator.- Design Constraints in Implementing Real-time Algorithms for a Flexible Manipulator System.- Pay-Load Estimation of a 2 DOF Flexible Link Robot.- Design of Hybrid Learning Control for Flexible Manipulators: a Multi-objective Optimisation Approach.- Intelligent Modelling of Flexible Manipulator Systems.- Wafer Handling Demo by SERPC.- Vision Control for an Artificial Hand.- Robotic Finger that Imitates the Human Index Finger in the Number and Distribution of its Tendons.- Modular, Reconfigurable Robots.- Methods for Collective Displacement of Modular Self-reconfigurable Robots.- Suboptimal System Recovery from Communication Loss in a Multi-robot Localization Scenario using EKF Algorithms.- ORTHO-BOT: A Modular Reconfigurable Space Robot Concept.- Motion of Minimal Configurations of a Modular Robot: Sinusoidal, Lateral Rolling and Lateral Shift.- Modularity and System Architecture.- The Modular Walking Machine, Platform for Technological Equipments.- YaMoR and Bluemove - An Autonomous Modular Robot with Bluetooth Interface for Exploring Adaptive Locomotion.- On the Development of a Modular External-pipe Crawling Omni-directional Mobile Robot.- Modularity and Component Reuse at the Shadow Robot Company.- CLAWAR Design Tools to Support Modular Robot Design.- Powering, Actuation, Efficiency.- Pneumatic Actuators for Serpentine Robot.- Nontraditional Drives for Walking Robots.- Energy Efficiency of Quadruped Gaits.- Bellows Driven, Muscle Steered Caterpillar Robot.- On the Application of Impedance Control to a Non-linear Actuator.- MACCEPA: the Actuator with Adaptable Compliance for Dynamic Walking Bipeds.- A Design of a Walking Robot with Hybrid Actuation System.- Manipulators Driven by Pneumatic Muscles.- Sensing and Sensor Fusion.- New Advances on Speckle-velocimeter for Robotized Vehicles.- Information Processing in Reactive Navigation and Fault Detection of Walking Robot.- Intelligent Sensor System and Flexible Gripper for Security Robots.- Search Performance of a Multi-robot Team in Odour Source Localisation.- A ""T-shirt Based"" Image Recognition System.- Object Shape Characterisation using a Haptic Probe.- Detection of Landmines Using Nuclear Quadrupole Resonance (NQR): Signal Processing to Aid Classification.- Software and Computer-aided Environments.- Simulator for Locomotion Control of the Alicia3 Climbing Robot.- A General Platform for Robot Navigation in Natural Environments.- Simulations of the Dynamic Behavior of a Bipedal Robot with Torso Subjected to External Eisturbances.- System Analysis, Modelling and Simulation.- Analysis of the Direct and Inverse Kinematics of ROMA II Robot.- Simulation of a Novel Snake-Like Robot.- An Actuated Horizontal Plane Model for Insect Locomotion.- Industrial Applications.- Machine Vision Guidance System for a Modular Climbing Robot used in Shipbuilding.- A Locomotion Robot for Heavy Load Transportation.- Using Signs for Configuring Work Tasks of Service Robots.- System for Monitoring and Controlling a Climbing and Walking Robot for Landslide Consolidation.- Non-destructive Testing Applications.- Small Inspection Vehicles for Non-Destructive Testing Applications.- Automated NDT of Floating Production Storage Oil Tanks with a Swimming and Climbing Robot.- 7-axis Arm for NDT of Surfaces with Complex & Unknown Geometry.- Personal Assistance Applications.- Elderly People Sit to Stand Transfer Experimental Analysis.- A Portable Light-weight Climbing Robot for Personal Assistance Applications.- Modeling and Control of Upright Lifting Wheelchair.- A Humanoid Head for Assistance Robots.- An Application of the AIGM Algorithm to Hand-Posture Recognition in Manipulation.- Security and Surveillance Applications.- AirEOD: a Robot for On-board Airplanes Security.- AIMEE: A Four-Legged Robot for RoboCup Rescue.- Modular Situational Awareness for CLAWAR Robots.- Space Applications.- Design Drivers for Robotics Systems in Space.- A Robotics Task Scheduler - TAPAS.- Mobile Mini-Robots For Space Application.- Teleagents for Exploration and Exploitation in Future Human Planetary Missions.- An Expandable Mechanism for Deployment and Contact Surface Adaptation of Rover Wheels.- A New Traction Control Architecture for Planetary Exploration Robots.- The Lemur II-Class Robots for Inspection and Maintenance of Orbital Structures: A System Description.- Lemur IIb: a Robotic System for Steep Terrain Access.- Tele-operation, Social and Economic Aspects.- Virtual Immersion for Tele-Controlling a Hexapod Robot.- Economic Prospects for Mobile Robotic Systems, New Modular Components.",,2006,10.1108/IR.2006.33.4.321.1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
887dae86a52e4b7b05c96ff343e191266cbb6e15,https://www.semanticscholar.org/paper/887dae86a52e4b7b05c96ff343e191266cbb6e15,Invited Talks and Panels AAAI-04 Keynote Address : Intelligent Systems and the Nation ’ s Vision for Space Exploration,"Over the past few months, NASA has been in the news numerous times both for the exciting discoveries of the pair of Mars Rovers as well as the country’s “Vision for Space Exploration.” This vision focuses on the joint human and robotic exploration of the solar system starting with a return to the moon and then the human exploration of Mars. In addition, the vision includes a continued array of exciting robotic missions to Mars as well as other destinations. There are numerous challenges that must be addressed to accomplish this vision. One critical area of technology is the broad area of information technology with a particular emphasis on intelligent systems. Future human exploration will require a new level of technology enabling NASA to deploy smarter, more adaptive systems. These systems must be designed to complement their human partners. In this talk, Daniel Clancy will present a brief overview of the new vision and then will talk in depth about a range of technologies that have been developed and demonstrated by NASA over the past few years. In particular, he will focus on how advanced artificial intelligence technologies have been developed and demonstrated within a robust environment and then in some cases deployed on NASA missions. Technologies that will be discussed include: automated planning and scheduling, humancentered computing, remote collaboration tools, machine learning and multi-model interaction using EEG and EMG signals.",,2004,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f16236e892422b014961fb7d1aeede041fe07965,https://www.semanticscholar.org/paper/f16236e892422b014961fb7d1aeede041fe07965,IMPROVING PERFORMANCE OF A JELLY-TRACKING UNDERWATER VEHICLE USING RECOGNITION OF ANIMAL MOTION MODES,"A vision-based automatic tracking system for gelatinous animals has been developed and demonstrated under a program of joint research between the Stanford University Aerospace Robotics Lab and the Monterey Bay Aquarium Research Institute (MBARI). In field tests using MBARI's ROV Ventana in the Monterey Bay, this system has demonstrated fully autonomous closed-loop control of Ventana to track a jellyfish for periods up to 1.5 hours. In these tests, conventional PID and Sliding Mode Control laws have both been used that rely primarily on the measurement of relative position errors derived from the vision-based system. This tracking system has been designed for both ROV and AUV deployments. One difference between the logic embedded in this system and the way human pilots operate is that human pilots typically exploit their a priori knowledge of how a jellyfish moves in formulating their control commands. That is, they do not rely solely on lead information determined through differentiation. Presented here is a first step for incorporating this additional knowledge-based lead information into the automatic control system. The ultimate goal is determine if this can be used to improve the overall performance and robustness of the tracking task. A key step towards quantification of motion behavior of gelatinous animals is a reliable capability to detect motion mode changes. This paper focuses on recognition of mode changes by applying techniques in real-time computer vision and supervised machine learning in the form of a support vector machine (SVM). Methods are presented to distinguish between active and resting modes, and to detect and measure rhythmic patterns in the body motions of these animals.",,2003,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
57899fc54678abaf19a690515dcc2a63867d9810,https://www.semanticscholar.org/paper/57899fc54678abaf19a690515dcc2a63867d9810,IJATS 1 ( 2 ),"Guest editoral Preface i Recent years have seen growing research interests in multi-agent systems. More and more contemporary technological challenges require distributed dynamical systems as a solution paradigm. Multi-agent systems are considered to be an excellent solution for these challenges of today's society, including electronic markets, robots, sensor systems, etc. As agent-based systems get larger and more complex, there is a compelling need for agents to learn and adapt to their dynamic environments. Indeed, how to control, coordinate and optimize adaptive multi-agent systems is one of the emerging multidisciplinary research areas today. Such systems are often deployed in real-world situations with stochastic environments where agents have limited perception and communication capabilities. Furthermore, in a number of distributed domains without centralized control, different agents will have different behaviours, capabilities, learning strategies, etc. There is a pressing need, then, to both, study and develop the convergence of multiple learners using the same learning scheme as well as understand the emergent dynamics of multiple learners with varying learning schemes. Whereas research in machine learning involving single agents is as old as the field of computational intelligence itself, interest in studying the techniques for and dynamics of multiple concurrent learners began around mid 1990's. To encourage discussion and research on these issues, a workshop on "" Adaptation and Learning in Multi-agent Systems "" was organized in association with IJCAI-95 in Montreal, Canada. Since then a number of workshops and symposia have been held on the topic and several journal special issues have also been published. It is particularly heartening for us to note that research papers on agent and multi-agent learning are a regular feature in most major AI and machine learning conferences including AAAI, IJCAI, ICML, AAMAS, NIPS, etc. Several satellite workshops , focusing on this topic, have also been organized in conjunction with these premier international conferences, with ALAg 2007 as a most recent highlight at an AAMAS conference. ALAg 2007 has been organized with the idea of bringing these workshops into some explicitly organized form. Concurrently, the Adaptive Learning Agents and Multi-agent Systems (ALAMAS) workshop has established as a series in Europe with yearly editions starting from 2001. The focus here was also on different facets of learning and adaptation in the multi-agent world. In 2008 the two workshop series joint into one workshop "" ALAg&ALAMAS "" held at the AAMAS in May 2008 in Estoril (Portugal). This workshop was very …",,2008,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
be43803704936febcf16f70cfa177223cb94ebc6,https://www.semanticscholar.org/paper/be43803704936febcf16f70cfa177223cb94ebc6,"New Trends in Applied Artificial Intelligence, 20th International Conference on Industrial, Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2007, Kyoto, Japan, June 26-29, 2007, Proceedings",,IEA/AIE,2007,10.1007/978-3-540-73325-6,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aa5ee51f7477c1464f2b23548bda37e7cad63aa9,https://www.semanticscholar.org/paper/aa5ee51f7477c1464f2b23548bda37e7cad63aa9,Teaching artificial intelligence as the year 2000 approaches,"In the August 1992, Communications of the ACM, Maurice Wilkes discusses how researchers in artificial intelligence have not as yet been able to write programs enabling a machine to pass Turing’s test. While not wishing to give the impression that Turing’s dream could in fact ever come true or that AI can progress much beyond expert system design, Wilkes suggests that researchers would have a better chance of reaching Turing’s goal with analogue rather than digital machines. But are digital machines really to blame for the lack of progress in machine intelligence us dreamed of by Turing? A question for educators in particular is: Why, at this point in time, in the face of so little progress and so much pessimism, should a course whose goal is seeking artificial intelligence in Turing’s sense be taught? If it should be taught, how does one teach it? In this paper I offer answers to these questions with the aim in mind of showing the importance of AI to a computer science curriculum. L SHOULD ARTIFICIAL INTELLIGENCE BE TAUGHT? In his article “Artificial Intelligence as the Year 2000 Approaches” published in the August, 1992, issue of Communications of the ACM, Maurice Wilkes reasons “it is difficult to escape the conclusion that, in the 40 years that have elapsed since 1950, no tangible progress has been made towards realizing machine intelligence in the sense that Turing had envisaged. Perhaps, the time has come to face the possibility that it never will be realized with a digital computer.” [10] Wilkes points out that digital computers work within a purely logical system. Within such a system, however, it is well known that there are things that simply cannot be done. He mentions Godel’s proof and Desergues’s theorem as examples of the limitations such systems have been shown to face, as well as the inability of modern digital computers to solve differential equations directly. Furthermore, thinking and intelligence seem to be intimately connected with learning, but unresrricfed learning programs that would make machines intelligent in Turing’s sense don’ t exist. The reason they don’ t exist is not because the AI community hasn’t tried hard enough. Indeed, Wilkes cites some admirable Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. ACM-24thCSE-2/93 -lN, USA 01993 ACM 0-89791 -566-6193 /000210038 . ..$1 .50 attempts such aa Samuel’s checker player, which still falls short of the mark of being an unrestricted learning program in the sense that Samuel “found that he could devise ways in which the program could be automatically adjusted to optimize performance, but no way in which the program could invent an entirely new measure and add it to the set it already possessed. In other words, he could write a program to deal with mathematical forms, but not with intellectual concepts.” If the learning progmms produced so far always (as Wilkes claims they have) turn out to be programs that optimize their performance by modifying their internal state, either by adjusting parameters or by updating data structures, then we really don’t have anything as yet coming close to mirroring human intelligence. Wilkes concludes by suggesting that “we take as a working hypothesis that intelligent behavior in Turing’s sense is outside the range of the digital computer.” Such a negative hypothesis, according to WWces, still has merit for guiding research in artificial intelligence in that “a recognition that Turing’s dream is not going to be realized with a digital computer would perhaps help students avoid unpromising lines of research.” Many agree with Wilkes’ view that traditional AI with its emphasis on symbol manipulation and fiist-order logic has not lived up to its expectations. For instance, AI has been widely criticized as not contributing significantly to the solution of realworld problems in machine vision and robotics, or language understanding and translation. As a result, Al has not been seen by many aa P means by which we may gain a better understanding of human cognition, thought processes, and concept formation. Is the digital computer to blame for this lack of progress? As Wilkes points out, the digital computer is an abstraction+ne which a human designer finds useful as a way of organizing his thoughts-that perforce works within a logical system. Is an analogue computer then the answer? Do analogue devices process information in a way that comes closer to the way in which human brains function to process information? Are they more “naturally” suited to achieve Turing’s dream of machine intelligence? WNces refrains from making any predictions concerning analogue computers and the future of AI cautioning us that the analogue devices we are able to construct may themselves be subject to limitations which may or may not parallel those of digital machines. As yet, anyway, no one knows what the future prospects hold for analogue devices turning Turing’s dream into a reality. What are the prospects for neural networlm, fuzzy logic systems, or object-oriented prograrrum “rig? Again there is much PMsimism as to the success such alternatives will provide for constructing machine intelligence as long as we are still primarily working with digital computers, simulations on digital computers,",SIGCSE '93,1993,10.1145/169070.169100,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c67c89b6c7ea70effbd5f2ebe9ef2f5c7aa927d2,https://www.semanticscholar.org/paper/c67c89b6c7ea70effbd5f2ebe9ef2f5c7aa927d2,"Advances in Information Systems: Second International Conference, ADVIS 2002, Izmir, Turkey, October 23-25, 2002. Proceedings","Databases and Data Warehouses.- Preserving Aggregation in an Object-Relational DBMS.- Database Compression Using an Offline Dictionary Method.- Representation of Temporal Unawareness.- Scalable and Dynamic Grouping of Continual Queries.- Uncertainty in Spatiotemporal Databases.- Integrity Constraint Enforcement by Means of Trigger Templates.- Current, Legacy, and Invalid Tuples in Conditionally Evolving Databases.- Magic Sets Method with Fuzzy Logic.- Information Retrieval.- Information Retrieval Effectiveness of Turkish Search Engines.- Comparing Linear Discriminant Analysis and Support Vector Machines.- Cross-Language Information Retrieval Using Multiple Resources and Combinations for Query Expansion.- Extracting Shape Features in JPEG-2000 Compressed Images.- Comparison of Normalization Techniques for Metasearch.- On the Cryptographic Patterns and Frequencies in Turkish Language.- Automatic Stemming for Indexing of an Agglutinative Language.- Pattern Acquisition for Chinese Named Entity Recognition: A Supervised Learning Approach.- Information Systems.- The Information System for Creating and Maintaining the Electronic Archive of Documents.- KiMPA: A Kinematics-Based Method for Polygon Approximation.- The Implementation of a Robotic Replanning Framework.- A 300 MB Turkish Corpus and Word Analysis.- Adaptation of a Neighbor Selection Markov Chain for Prefetching Tiled Web GIS Data.- Web Based Automation Software for Calculating Production Costs in Apparel Industry.- Multi-agent Technologies and Systems.- Knowledge Representation in the Agent-Based Travel Support System.- Intelligent Agents in Virtual Worlds.- Characterizing Web Service Substitutivity with Combined Deductive and Inductive Engines.- Modular-Fuzzy Cooperation Algorithm for Multi-agent Systems.- Minimax Fuzzy Q-Learning in Cooperative Multi-agent Systems.- A Component-Based, Reconfigurable Mobile Agent System for Context-Aware Computing.- A FIPA-Compliant Agent Framework with an Extra Layer for Ontology Dependent Reusable Behaviour.- Evolutionary Algorithms.- Vibrational Genetic Algorithm (Vga) for Solving Continuous Covering Location Problems.- Minimal Addition-Subtraction Chains Using Genetic Algorithms.- Preserving Diversity through Diploidy and Meiosis for Improved Genetic Algorithm Performance in Dynamic Environments.- Ant Systems: Another Alternative for Optimization Problems?.- System Programming.- Augmenting Object Persistency Paradigm for Faster Server Development.- Power Conscious Disk Scheduling for Multimedia Data Retrieval.- Task Scheduling with Conflicting Objectives.- A Fast Access Scheme to Meet Delay Requirement for Wireless Access Network.- New Information Technologies in Education.- Dokuz Eylul University-Distance Education Utilities Model.- Problem-Based Learning as an Example of Active Learning and Student Engagement.- Use of PBL Method in Teaching IT to Students from a Faculty of Education: A Case Study.- Interval Matrix Vector Calculator-The iMVC 1.0.- Distributed and Parallel Data Processing.- Efficient Code Deployment for Heterogeneous Distributed Data Sources.- Efficient Parallel Modular Exponentiation Algorithm.- Interprocedural Transformations for Extracting Maximum Parallelism.- On Methods' Materialization in Object-Relational Data Warehouse.",,2002,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aabb74bf88f742f0f0cb80a887ff3e1dc1b6b87a,https://www.semanticscholar.org/paper/aabb74bf88f742f0f0cb80a887ff3e1dc1b6b87a,An Overview of Human Activity Recognition Using Wearable Sensors: Healthcare and Artificial Intelligence,,ICIOT,2021,10.1007/978-3-030-96068-1_1,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ca0b256240a84fe42e7c0246b4a2c09798e49917,https://www.semanticscholar.org/paper/ca0b256240a84fe42e7c0246b4a2c09798e49917,"Guest editorials: P2P computing for 5G, beyond 5G (B5G) networks and internet-of-everything (IoE)",,Peer-to-Peer Netw. Appl.,2020,10.1007/S12083-020-01001-5,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ec0b23cb39b041a249c0e9fdd09cf69503de7fb4,https://www.semanticscholar.org/paper/ec0b23cb39b041a249c0e9fdd09cf69503de7fb4,Tactile based Intelligence Touch Technology in IoT configured WCN in B5G/6G-A Survey,"Touch enabled sensation and actuation is expected to be one of the most promising, straightforward and important uses of the next generation communication networks. In light of the next generation (B5G/6G) system’s need for low latency, the infrastructure should be reconfigurable and intelligent in order to be able to work in real time and interoperable with the existing wireless network. It has a drastic impact on the society due to its high precision, accuracy, reliability and efficiency as well as the ability to connect a user from far away or remote areas. Such a touch-enabled interaction is primarily concerned with the real time transmission of the tactile based haptic information over the internet, in addition to the usual audio, visual and data traffic, thus enabling a paradigm shift towards establishing a real time control and steering communication system. Due to the existing system’s latency and overhead, it creates delays and limits the usability of the future applications. In light of the aforementioned concerns, this study proposes an intelligent touch-enabled system for B5G/6G and IoT based wireless communication network that incorporates the AR/VR technologies. The tactile internet and network slicing serve as the backbone of the touch technology which incorporates intelligence from techniques such as artificial intelligence and machine/deep learning. The survey also introduces a layered and interfacing architecture complete with its E2E solution for the intelligent touch based wireless communication system. It is anticipated for the next generation system to provide numerous opportunities for various sectors utilizing AR/VR technology in robotics and healthcare facilities, all with the intension of helping in addressing severe problems faced by the society. Conclusively the article presents a few use cases concerning the deployment of touch infrastructure in automation and robotics as well as in intelligent healthcare systems, assisting in the diagnosis and treatment of the prevailing covid-19 cases. The paper concludes with some considerable future research aspects of the proposed system with few of the ongoing projects pertaining to the development in the incorporation of the next generation (6G) system. Author",IEEE Access,2022,10.1109/access.2022.3148473,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
395b153b5f3d4445e4e2920f5cb7e6531a970a62,https://www.semanticscholar.org/paper/395b153b5f3d4445e4e2920f5cb7e6531a970a62,Read PDF Home Automation Using Internet Of Things modernh.com,"hat bereits begonnen. Ihr Merkmal ist die ungeheuer schnelle und systematische Verschmelzung von Technologien, die die Grenzen zwischen der physischen, der digitalen und der biologischen Welt immer stärker durchbrechen. Wie kein anderer ist Klaus Schwab, der Vorsitzende des Weltwirtschaftsforums, in der Lage aufzuzeigen, welche politischen, wirtschaftlichen, sozialen und kulturellen Herausforderungen diese Revolution für uns alle mit sich bringt.This work presents the design and model implementation of a novel home automation system applying the Internet of Things (IoT) technology. It seeks simplified design protocols for developing a robust home automation system to deal with the problems of complexity, multiple incompatible standards and the resulting expenses in the existing systems. The embedded system featuresthe ubiquitous low-cost 32-bit ESP8266 System-on-chip (SoC) module interfaced to some sensors and actuators for interaction inthe home. Flexibility in the remote access, operation and management is achieved through HTML5 based intuitive mobile and web GUI applications. Web Application Messaging Protocol (WAMP) is deployed to ensure that individual applications and systems seamlessly communicate with a relatively high level of security using robust web service security protocol. This system offers a cost-effective and efficient solution, excluded, which are present mostly in other solutions, because the costs of a dedicated public IP address and a high-end computer are excluded, which are present mostly in other solutions.This book focuses on the emerging advances in distributed communication systems, big data, intelligent computing and Internet of Things, presenting state-of-the-art research in frameworks, algorithms, methodologies, techniques and applications associated with data engineering and wireless distributed communication technologies. In addition, it discusses potential topics like performance analysis, wireless communication networks, data security and privacy, human computer interaction, 5G Networks, and smart automated systems, which will provide insights for the evolving data communication technologies. In a nutshell, this proceedings book compiles novel and high-quality research that offers innovative solutions for communications in IoT networks.This edited book presents point of view and the work being undertaken by active researchers in the domain of IOT and its applications with societal impact. The book is useful to other researchers for the understanding of the research domain and different points of views expressed by the experts in their contributed chapters. The contributions are from both industry and academia; hence, it provides a rich source of both theoretical and practical work going on in the research domain of IOT.Advances in Computing, Communication, Automation and Biomedical Technology aims to bring together leading academic, scientists, researchers, industry representatives, postdoctoral fellows and research scholars around the world to share their knowledge and research expertise, to advances in the areas of Computing, Communication, Electrical, Civil, Mechanical and Biomedical Systems as well as to create a prospective collaboration and networking on various areas. It also provides a premier interdisciplinary platform for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, and concerns as well as practical challenges encountered, and solutions adopted in the fields of innovation.This books objective is to explore the concepts and applications related to Internet of Things with the vision to identify and address existing challenges. Additionally, the book provides future research directions in this domain, and explores the different applications of IoT and its associated technologies. Studies investigate applications for crowd sensing and sourcing, as well as smart applications to healthcare solutions, agriculture and intelligent disaster management. This book will appeal to students, practitioners, industry professionals and researchers working in the field of IoT and its integration with other technologies to develop comprehensive solutions to real-life problemsThis book presents selected research papers on current developments in the fields of soft computing and signal processing from the Third International Conference on Soft Computing and Signal Processing (ICSCSP 2020). The book covers topics such as soft sets, rough sets, fuzzy logic, neural networks, genetic algorithms and machine learning and discusses various aspects of these topics, e.g., technological considerations, product implementation and application issues.Over 60 recipes will help you build smart IoT solutions and surprise yourself with captivating IoT projects you thought only existed in Bond moviesAbout This Book- This book offers key solutions and advice to address the hiccups faced when working on Arduino- based IoT projects in the real world- Take your existing skills and capabilities to the next level by building challenging IoT applications with ease.- Be the tech disruptor you always wanted to be with key recipes that help you solve Arduino IoT related problems smarter and faster.- Put IoT to work through recipes on building Arduino-based devices that take control of your home, health, and life!Who computational techniques with traditional computing methods has inspired researchers and academics alike to focus on developing innovative computational techniques. In the near future, computational techniques may provide vital solutions by effectively using evolving technologies such as computer vision, natural language processing, deep learning, machine learning, scientific computing, and computational vision. A vast number of intelligent computational algorithms are emerging, along with increasing computational power, which has significantly expanded the potential for developing intelligent applications. These proceedings of the International Conference on Inventive Computation Technologies [ICICT 2019] cover innovative computing applications in the areas of data mining, big data processing, information management, and security.There is no doubt that there has been much excitement regarding the pioneering contributions of artificial intelligence (AI), the internet of things (IoT), and blockchain technologies and tools in visualizing and realizing smarter as well as sophisticated systems and services. However, researchers are being bombarded with various machine and deep learning algorithms, which are categorized as a part and parcel of the enigmatic AI discipline. The knowledge discovered gets disseminated to actuators and other concerned systems in order to empower them to intelligently plan and insightfully execute appropriate tasks with clarity and confidence. The IoT processes in conjunction with the AI algorithms and blockchain technology are bound to lay out a stimulating foundation for producing and sustaining smarter systems for society. Advancing Smarter and More Secure Industrial Applications Using AI, IoT, and Blockchain Technology articulates and accentuates various AI algorithms, fresh innovations in the IoT, and blockchain spaces. The domain of transforming raw data to information and to relevant knowledge is gaining prominence with the availability of data ingestion, processing, mining, analytics algorithms, platforms, frameworks, and other accelerators. Covering topics such as blockchain applications, Industry 4.0, and cryptography, this book serves as a comprehensive guide for AI researchers, faculty members, IT professionals, academicians, students, researchers, and industry professionals.“With futuristic homes on the rise, learn to control and automate the living space with intriguing IoT projects.” About This Book Build exciting (six) end-to-end home automation projects with Raspberry Pi 3, Seamlessly communicate and control your existing devices and build your own home automation system, Automate tasks in your home through projects that are reliable and fun Who This Book Is For This book is for all those who a new interconnectivity your world. Style and approach End to end home automation projects with 3.Throughout human history, technological advancements human our advancements, been discover over large this human intervention. These advancements may become essential may book The book includes high-quality research work by academicians and industrial field computing and communication, full-length papers, research-in-progress papers and case studies related to all the areas of data mining, machine learning, Internet of things (IoT) and information security.This book presents chapters from diverse range of authors on different aspects of how Blockchain and IoT are converging and the impacts of these developments. The book provides an extensive cross-sectional and multi-disciplinary look into this trend and how it affects artificial intelligence, cyber-physical systems, and robotics with a look at applications in aerospace, agriculture, automotive, critical infrastructures, healthcare, manufacturing, retail, smart transport systems, smart cities, and smart healthcare. Cases include the impact of Blockchain for IoT Security; decentralized access control systems in IoT; Blockchain architecture for scalable access management in IoT; smart and sustainable IoT applications incorporating Blockchain, and more. The book from and practitioners from diverse perspectives. how Blockchain and IoT are converging and the impacts of these developments on technology and its application; Discusses IoT and Blockchain from cross-sectional and multi-disciplinary perspectives; Includes contributions from researchers, academics, and professionals from around the world.With the recent growth of big data and the internet of things (IoT), individuals can now upload, retrieve, store, and collect massive amounts of information to help drive decisions a",,2022,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0ca2e97e9b09f0c5d723f8b9844eadb4a036af14,https://www.semanticscholar.org/paper/0ca2e97e9b09f0c5d723f8b9844eadb4a036af14,Life Cycle Assessment (LCA) of biogas and biomethan used in building energy systems,"the of a is disturbed. is by heavy valuable information that can be of assistance in reducing catastrophic losses. The objective of this study is to assess the vulnerability of different types of building structures to landslides. To accomplish the aim, it is first necessary to identify the multiple factors related to landslides, to evaluate the related contribution of the factors causing slope failures, to establish a relationship between the factors and the landslides. Secondly, based on the previously defined parameters and existing methodologies it is required to develop a general methodology for assessing the vulnerability of predefined types of building structures. and is clarify the the procedures and the model that will be used to design shot -earth reinforced structures. A second aim of this project is to control if and how the water uptake might influene the strength of the material. The project includes laboratory test on cement pastes and mortars with techniques such as isothermal calorimetry and rheolometer. This internship will be conducted in the framework of “HISTO-RENO” a European research project funded by the Interreg France-Switzerland fund. This project aims to develop a web-based simplified energy pre-audit tool to guide the building owner in their energy-related renovation. The goal of this internship is to derive statistical distributions on the input parameters (U-values, surface area of the main elements: external walls, roof, slab, windows, etc.) of the building heating demand of the current Swiss building stock with the help of the information of individual buildings in the Building energy certificate database (EPC), called “CECB” in Switzerland. The methodology of the analysis will be based on the previous work from Streicher et al 2018. The statistical analysis on the heating demand parameters will then serve to define probability density function (PDF) as input for a stochastic simplified energy audit tool used in a preliminary step of a renovation process indicating the estimates in kWh, carbon footprint and costs of a set of renovation measures. The Laboratory of Solar thermal Energy and Building Physics (LESBAT) from HEIG-VD has been funded by the Interreg France-Keywords: this topic, the minimum duration of this internship is 4 months (preferably 6 months). Proven knowledge in numerical and scientific programming tools like Python or R is required in order to successfully achieve the goals of this internship. analyse, 2) in the literature review of existing LCA of biogas and biomethan in order to get a range of carbon footprint for a unit of biogas/biomethan, 3) in the LCA of the heat and/or electricity produced with the technologies selected and 4) in the comparative LCA of heat production with other technologies (e.g. electric-based air-to-water heat pump) or electricity production in micro-CHP comparing biogas/biomethan solutions with electricity ones (using electricity from the grid). The data used for the carbon footprint of the Swiss electricity mix will be based on the ones available at HEIG-VD. in numerical and programming tools like duration 4 months, preferentially 6 months. Environmental alterations trigger changes in the underlying plant physiological processes portrayed by distinct variations of the electrical potential. Advanced signal processing and data analysis techniques enabled an automatic recognition of patterns in the electrical response of plants growing under typical production conditions allowing the identification of a plant's health status with high accuracy. However, current developments are based on classical machine learning algorithms requiring the extraction of features from the signal. The proposed project aims to extend the existing modeling approach by developing a classification framework that will extract features in an automated manner, such as applying deep-learning-based algorithms. system based on machine Commercial orchards are increasingly dependent on proper irrigation to ensure the highest yields and optimize production quality. Still, current monitoring tools need greater accuracy that could be achieved by incorporating indicators based directly on the plants. Moreover, tomatoes show difficulty adapting to the water and nutritional contributions provided by automatic systems in the greenhouses, resulting in physiological damage of the fruit, such as skin “cracking” leading to important yield losses. The main objective of the project is to model the growth of the fruits by using intelligent data analysis techniques on data from fruit dendrometer and micro-climate measurements in combination with the expertise of agronomists, to provide a tool for fruit growers that would help them predict physiological damage of the fruits and improve the quality of the crops, while optimizing harvest timing and reducing water usage. is spermatozoa morphology is one of the fundamental parameters for evaluating sperm quality. Evaluation of the morphology from microscopic sperm images could help reduce the required time and the observer-based variability of the manual analysis currently used as a clinical gold standard. Moreover, morphological abnormalities represent various forms and shapes on different cell parts, making classification a challenging task. This project aims to use image processing and machine learning algorithms on spermatozoa images to automatically distinguish abnormal from normal cells and classify different abnormal sperm morphology. to use advanced signal processing and machine learning algorithms on ECG signals to model patterns that identify and automatically discriminate different heart anomalies represented by the ECG curves. the of a kid's survival. to study, design, implement and evaluate a secure geolocation solution for mobiles. Today, geolocation on smartphones is mainly achieved through GNSS, Wi-Fi positioning, or BLE beacons. Still, none offer a strong guarantee as they can be unavailable or spoofable. After completing a state-of-the-art of existing technologies and their availability on mobile platforms, the trainee will propose a solution and realize a PoC. Several approaches are possible to realize this project; one possibility is the design of a BLE beacon integrating cryptographic features. Deep neural networks have shown to be very good at image classification and object recognition tasks. The objective of this project is to train a custom system to process and analyze satellite images (both from day and night). To achieve this, we will take advantage of pretrained models provided by the major actors in the domain and proceed to fine-tune them with our own data. Potential applications include, forest monitoring, population growth analyses, socio-economic issues, etc. For more information: http://iict-space.heig-vd.ch/ape The increasing availability of wearable sensors embedded in smartphones, watches and physical activity trackers has open the door to original applications, mainly in health and wellness improvement. One typically collects data by means of sensors like GPS, accelerometers, gyroscopes, barometers, microphones, cameras, depth sensors, etc. To make sense of these data, Machine learning algorithms can be used to establish correlations among the variables under investigation, and as in every attempt to understand high-dimensional data, visualization and dimensionality reduction techniques can suggest new knowledge about the aspects of the person's life being monitored. The objective of this project is to deal with diverse application domains including self-tracking of physical activity, self-tracking and characterization of style and performance in sport (e.g., racket sports, running), daily-life logging , or 24/7 self-monitoring as a means to enhace our wellbeing. more information: http://iict-space.heig-vd.ch/ape such robots more human-like with the aim of increasing our trust in them. For more For the conception phase, you’ll have to create wireframe of user interface. MEI can help with this phase. The development expected is a proof of concept for the fullstack. of law imposes new specialized hardware solutions to serve the increasing computing demand. System composed by several domain specific accelerators are available, but from the system integration and programming point of view they rely on custom solutions. The idea of this project is exploring the state of the art in compiler infrastructure for heterogeneous hardware and implement a prototype to measure real benefit and compromises of these solutions. Several research project have already been carried out in our laboratory on this subject. in we the and this will on extending its functionalities and benchmark it extensively. Data centres demand more and more computation efficiency. Standard CPU are unable to cope with the demand and GPU can only serve specific computation patterns. FPGAs are an attractive technology in this field, but its integration in the data centre infrastructure is not trivial. Smart Network interface (NICs) solutions are attractive for offloading many filtering and computation directly at the network attachment point relieving the CPU of many tasks. This project will explore the state of the art in the domain with the aim at developing a prototype capable to off-load tasks to an FPGA. management between edge and cloud. To maximize performance and minimize the energy consumption of both edge devices and cloud platforms, there is a need to develop efficient resource management techniques able to take workload allocation decisions, on when and where to execute the workload, in the edge to cloud continuum in an elastic way. To exploit elasticity, these techniques need to be aware of the underlying hardware and software stack, which often consist on a lightweight virtualization (like containers) deployed on ARM or RISC-V based edge devices. This project proposes the design of heuristic and meta-heuristic based workload m",,2022,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
241be5393dba38bbc634d9147618fe11c161eb09,https://www.semanticscholar.org/paper/241be5393dba38bbc634d9147618fe11c161eb09,Rapidly-exploring Random Forest: Adaptively Exploits Local Structure with Generalised Multi-Trees Motion Planning,"Sampling-based motion planners perform exceptionally well in robotic applications that operate in highdimensional space. However, most works often constrain the planning workspace rooted at some fixed locations, do not adaptively reason on strategy in narrow passages, and ignore valuable local structure information. In this paper, we propose Rapidly-exploring Random Forest (R R F*)—a generalised multitrees motion planner that combines the rapid exploring property of tree-based methods and adaptively learns to deploys a Bayesian local sampling strategy in regions that are deemed to be bottlenecks. Local sampling exploits the local-connectivity of spaces via Markov Chain random sampling, which is updated sequentially with a Bayesian proposal distribution to learns the local structure from past observations. The trees selection problem is formulated as a multi-armed bandit problem, which efficiently allocates resources on the most promising tree to accelerate planning runtime. R R F* learns the region that is difficult to perform tree extensions and adaptively deploys local sampling in those regions to maximise the benefit of exploiting local structure. We provide rigorous proofs of completeness and optimal convergence guarantees, and we experimentally demonstrate that the effectiveness of R R F*’s adaptive multi-trees approach allows it to performs well in a wide range of problems. I . I N T R O D U C T I O N Motion planning is one of the fundamental methods for robots to navigate and integrate with the real-world. Obstacles and physical constraints are ubiquitous regardless of the type of robotic applications, and we wish to safely and efficiently navigate the robot from some initial state to a target state. Sampling-based motion planners (SBPs) are of a class of robust methods to perform motion planning. SBPs do no need to explicitly construct the often intractable high-dimensional Configuration Space (C-space). SBP samples C-space randomly for valid connections and iteratively builds a roadmap of connectivity. SBPs are guaranteed to find a solution if one exists [1]. Further developments on SBPs had granted asymptotic optimality [2]—a guarantee that the solution will converge, in the limit, to the optimal solution. One of the fundamental issues with SBPs lies in SBP’s approach—the exceedingly low likelihood of sampling within narrow passages. Intuitively, regions with low visibility will have a low probability to be randomly sampled. Therefore, when SBPs perform roadmap-based planning by random sampling and creating connections in C-space, highly constrained regions become problematic as they limit the connectivity of free space [3], [4]. Narrow passages severely restrict the performance of SBPs because SBPs throw away the unlikely samples that do fall within narrow passages if Correspondence to tin.lai@sydney.edu.au †School of Computer Science, The University of Sydney, Australia. Fig. 1. Overview illustration of R R F* using multiple trees to adaptively plans on the current most promising tree, which rapidly explores visible space and exploits local structures with Bayesian local sampling. Green and red nodes refers to the initial qinit and target qtarget configuration; teal and purple trees are the rooted Tinit, Ttarget and local trees respectively. Orange hatched circles are the current states of local samplers sampling for their respective local trees. R R F* learns to creates new local trees in highly constrained regions that are hard to extend from root trees, of which will be benefited from proposing informed samples through Bayesian local sampling. the tree failed to expand. Consequently, narrow passages will bottleneck the tree’s growth until a series of tree expansions had successfully created connections within the restricting narrow passages. Our contribution is an incremental multi-trees SBP— Rapidly-exploring Random Forest (R R F*)—that learns from sampling information and adjust planning strategy accordingly. We formulate R R F* to learns regions that are likely to be bottlenecked and adaptively deploys Bayesian local sampling to exploits C-space’s local structure. Unlike previous local SBPs that deploy local trees everywhere regardless of the nearby region’s complexity, R R F* utilises the rapid growth of the rooted trees approach for open spaces and adaptively uses local trees within bottlenecks. Bayesian local sampling tackles the narrow passage problem by performing sequential Markov chain Monte Carlo (MCMC) random walks within the passage, and at the same time, updates its proposal distribution from sampled outcomes. In additions, R R F* plans with multiple trees and allocates planning resources on the most promising tree by the reward signal from our multi-armed bandit formulation. We provide rigorous proofs on completeness and optimal convergence guarantees. ar X iv :2 10 3. 04 48 7v 1 [ cs .R O ] 7 M ar 2 02 1 Experimentally, we show that R R F* achieves superior results in a wide range of scenarios, especially, R R F* yields high sample efficiency in highly constrained C-space. I I . R E L AT E D W O R K One of the most influential works on SBPs is Probabilistic Roadmap (P R M), which creates a random roadmap of connectivity that can be reused [5]. Rapidly-exploring Random Tree (R R T) [6] follows a similar idea but instead uses a tree structure to obtain a more rapid single-query solution. Most SBPs minimises some cost, e.g., distance metric or control cost. Therefore, P R M∗ [7] and R R T* [8] are introduced that exhibit asymptotic optimal guarantee. The runtime performance of SBPs had been one of the main focus in existing works. For example, to address the narrow passage problem, some planners focus the random sampling to specific regions in C-space [9]. Such an approach often requires some technique to discover narrow passages, e.g., using bridge test [10], space projection [11], heuristic measures of obstacles boundary [12], [13], and densely samples at discovered narrow regions [14]–[16]. There are also sampling techniques that improve sampling efficiency by using a restricted or learned sampling distribution [17]–[20], which either formulate some regions for generating samples or deploy a machine learning approach to learn from experience. However, while those approaches improved sampling efficiency, they do not directly address the limited visibility issue within narrow passages. Some planners had employed a multi-tree approach to explore C-space more efficiently. For example, growing bidirectional trees can speed up the exploration process because tree extensions at different origin are subject to a different degree of difficulty [21]. Potential tree locations can be searched with the bridge test, followed by a learning technique to model the probability of tree selection [22]. Local trees had been employed for growing multiple trees in parallel [4], [23]. However, current approaches utilise local trees regardless of the complexity of the nearby regions. While those approaches had improved sampling efficiency, it would be more beneficial to deploy local planners dependent on the C-space complexity adaptively. Several SBPs had utilised Markov Chain Monte Carlo (MCMC) for local planning, as it allows utilisation of information observed from previous samples [24]. Monte Carlo random walk planner searches free space by constructing a Markov Chain to propose spaces with high contributions [25]. The roadmap of a PRM can be formulated as the result of simultaneously running a set of MCMC explorations [26]. Therefore, the connectives between feasible states can be modelled as a chain of samples walking within the free space. Our proposing R R F* exploits this property by utilising a Bayesian approach in proposing chained samples by sequentially updating our belief on the space that are deemed to be bottlenecks. I I I . R A P I D LYE X P L O R I N G R A N D O M F O R E S T Motion planning’s objective is to construct a feasible trajectory from an initial configuration qinit to a target configuration qtarget, where q ∈ C ⊆ R denotes a state the C-space and d ≥ 2 is the dimensionality. The obstacle spaces Cobs denotes the set of invalid states, and the set of free space Cfree is defined as the closure set cl(C \ Cobs). In motion planning, there is often some cost function that the planner wants to optimise. Problem 1 (Asymptotic optimal planning): Given C,Cobs, a pair of initial qinit and target qtarget configurations, a cost function Lc : σ → [0,∞), and let Γ(Cfree) denotes the set of all possible trajectories in cl(Cfree). Find a solution trajectory σ∗ that exhibits the minimal cost. That is, find σ∗ such that σ∗(0) = qinit, σ∗(1) = qtarget, and Lc(σ∗) = minσ∈Γ(Cfree) Lc(σ). A. High-level description Fig. 2. Illustration of R R F* performing Bayesian local sampling for a local tree. The spherical proposal distribution is overlaid on the local sampler as a blue curve, which is sequentially learned after 10 previous failed samples (purple arrows). The transparent grey arrows illustrates the likelihood of sampling 20 times from the probability distribution shown. R R F* uses multiple trees to adaptively explores and exploits different regions of C-space. Traditional approaches use only a single tree rooted at qinitial to construct a roadmap of connected configurations. The tree grows outwards by sampling random configurations that create a new connection to the closest node. Since the tree expansion is limited to a local scope bounded by the neighbourhood visibility of the frontier tree nodes, such an approach will often reject updates from valid samples when there exists no free route from the closest existing node towards the sampled configuration. R R F* overcomes this limitation by addressing the sampling-based motion planning problem with a divide-and-conquer approach. R R F* follows the approach in [27] which uses local trees for Bayesian local sampling. However, instead of purely",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1cc51626fd59ff28e24a9984fa7ab1f0a9353d84,https://www.semanticscholar.org/paper/1cc51626fd59ff28e24a9984fa7ab1f0a9353d84,The mismeasure of the human: Big data and the ‘AI turn’ in global governance,"ANTHROPOLOGY TODAY VOL 37 NO 1, FEBRUARY 2021 MARIA SAPIGNOLI Maria Sapignoli is heading the Max Planck independent research group, AIming toward the future: Policing, governance and artificial intelligence at the Max Planck Institute for Social Anthropology. Her email is sapignoli@eth.mpg.de. ‘I am here to help humanity create the future.’ Symbolic and performative are the words we might choose to describe the conversation between Sophia, the life-sized ‘social robot’, and the United Nations Deputy SecretaryGeneral, Amina J. Mohammed, on 11 October 2017. That day, Sophia entered the UN Headquarters in New York to participate in the event The future of everything – sustainable development in the age of rapid technological change, and to explain how artificial intelligence (AI) can and will help to create a better future for humanity (Fig. 1).1 While the more vaunted claims seem far away, digital technologies and AI systems are already transforming our world socially, economically and politically and affect people’s everyday lives, most of the time in invisible ways.2 The ‘artificial intelligence turn’, in which data processing and automatic reasoning have become central in governance and decision making (Gurumurthy & Bharthur 2018), is having immediate consequences for a wide range of issues. These include (to mention a few): who will be targeted as a criminal; which families obtain resources or who is investigated for fraud; which justice campaigns will succeed in being heard (or not); where a mass grave containing evidence of a war crime is likely to be found; what aid is available and where it will be directed; and how human rights monitoring and intervention occur. Such developments in technoscientific knowledge produce and emerge out of an intellectual and cultural movement that Upendra Baxi (2007: 214) refers to as ‘the benign posthuman’, which affirms the possibility of improving the human condition through applied reason. Virginia Eubanks (2018) analyses the effects that the digitalization of governance practices have on the vulnerable and poor in the USA, showing that certain individuals and groups become even more visible to states by way of over-policing and data collection, while others among the marginalized produce results that are ‘unreadable to’, or ‘errors of’, AI systems, thereby becoming victims of neglect in the system of ‘good governance’. These systems undermine basic rights while making it increasingly difficult to legally challenge adverse decisions because of their invisibility, opacity, and often the absence of legal regulatory frameworks.3 Along similar lines to Eubanks, Philip Alston, outgoing UN Special Rapporteur on extreme poverty and human rights, presented a report to the UN General Assembly on what he calls ‘the rise of the digital welfare state’.4 He considers how semi-automated and automated decisionmaking technologies, often presented as ‘simple administrative technical innovations’, are revolutionizing the interactions between governments and the most vulnerable in society; warning that nations around the world are ‘stumbling zombie-like into a digital welfare dystopia’ in which AI and other technologies are used to target, surveil, punish and further marginalize the poorest people. These contributions reveal that decision making and sociality already take place through digital infrastructures, machine learning, algorithmic assemblages and quasiautomatized predictions, blurring the boundary between human and not-human agency, and creating new human realities at the same time. This development raises challenges and concerns, and signals the need for legal transformations, not just in the policy areas where they are used, but also for society as a whole. What happens when it is not just states that are adopting AI in governance practices, but also international institutions, such as the United Nations? What happens when these systems are used to manage conflict situations, humanitarian intervention, to administer logistics as well as develop law and policies? What is the place of these technologies in deploying or expressing relationships of power? How do they contribute to the creation of the reality of the intervention, through deciding what does or does not matter? How, in other words, does the ‘AI turn’ of governance contribute to the ways human belonging is conceptualized and acted upon? How is collective life imagined and how can anthropologists approach it? In this paper, I offer some reflections on the possible effects that the AI turn of global bureaucracies has for human rights practice, particularly in the case of the United Nations (Fig. 2 & 4). This turn begins with the UN’s intervention in major crises, and continues with the creation of more mundane policies and law. I also want to show that, beyond the policy and crisis-intervention orientations of AI, we are witnessing the creation of new foundations for human belonging and being. All this is taking place through the automatization of decision making in the context of the increased interdependence between private and public sectors.",Anthropology Today,2021,10.1111/1467-8322.12627,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5a607eeabc7fefcbe330ee9c0d896396885c2da4,https://www.semanticscholar.org/paper/5a607eeabc7fefcbe330ee9c0d896396885c2da4,A Virtual Mechanism Approach for Exploiting Functional Redundancy in Finishing Operations,"We propose a new approach to programming by the demonstration of finishing operations. Such operations can be carried out by industrial robots in multiple ways because an industrial robot is typically functionally redundant with respect to a finishing task. In the proposed system, a human expert demonstrates a finishing operation, and the demonstrated motion is recorded in the Cartesian space. The robot’s kinematic model is augmented with a virtual mechanism, which is defined according to the applied finishing tool. This way, the kinematic model is expanded with additional degrees of freedom that can be exploited to compute the optimal joint space motion of the robot without altering the essential aspects of the Cartesian space task execution as demonstrated by the human expert. Finishing operations, such as polishing and grinding, occur in contact with the treated workpiece. Since information about the contact point position is needed to control the robot during the operation, we have developed a novel approach for accurate estimation of contact points using the measured forces and torques. Finally, we applied iterative learning control to refine the demonstrated operations and compensate for inaccurate calibration and different dynamics of the robot and human demonstrator. The proposed method was verified on real robots and real polishing and grinding tasks. Note to Practitioners—This work was motivated by the need for automation of finishing operations, such as polishing and grinding, on contemporary industrial robots. Existing approaches are both too complex and too time-consuming to be applied in flexible and small-scale production, which often requires the frequent deployment of new applications. Our approach is based on programming by demonstration and enables the programming of finishing operations also for users who are not specialists in robot programming. Programming by demonstration is especially useful for teaching finishing operations because it enables the transfer of expert knowledge about finishing skills to robots without providing lengthy task descriptions or manual coding. Besides the human demonstration of the desired operation, the proposed approach also requires the availability of the kinematic model for the machine tool applied to carry out the finishing operation. We provide several practical examples of grinding and polishing tools and how to integrate them into our approach. Another feature of the proposed system is that user demonstrations of finishing operations can be transferred between different combinations of robots and machine tools.",IEEE Transactions on Automation Science and Engineering,2021,10.1109/tase.2020.3032075,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
68aff2731ad4ee1e83309d3b2f7ff240d7987208,https://www.semanticscholar.org/paper/68aff2731ad4ee1e83309d3b2f7ff240d7987208,Design Development And Fabrication Of Sugarcane Bud,"Design, Development and Fabrication of a Precision Autocollimating Solar Sensor /PASS/DESIGN, DEVELOPMENT, AND FABRICATION OF PROTOTYPE LOW-ENRICHED SUPERHEATER FUEL ELEMENTS (LESH).Design, Development and Fabrication of a Deployable/retractable Truss Beam Model for Large Space Structures ApplicationDesign, Development and Testing of Calspan/Chrysler Research Safety Vehicle Phase II. Volume I. Final Technical ReportDesign, Development, and Fabrication of a Sealed, Brushless Dc Motor Final ReportCompendium of Industrial Research, Design, and Development Facilities Available in Uttar PradeshDesign, Development and Fabrication of an Advanced High-Precision Robotic System for MicrosurgeryDesign, development, fabrication and testing of an operational prototype surge protectiveDesign, Development, and Fabrication of the FMU-26/B and FMU-26A/B Bomb FuzesDesign, Test, and Microfabrication of MEMS and MOEMSDesign and Development of RFID and RFID-Enabled Sensors on Flexible Low Cost SubstratesDesign for ManufacturingDesign and Fabrication of an Internally Insulated Filament Wound Liquid Hydrogen Propellant TankCalifornia. Court of Appeal (2nd Appellate District). Records and BriefsScientific and Technical Aerospace ReportsFurniture DesignDesign, Development and Fabrication of a Solar Experiment Alignment Sensor (SEAS)Axiomatic Design and Fabrication of Composite StructuresAdvanced Technology for Design and Fabrication of Composite Materials and StructuresRobotic Fabrication in Architecture, Art and Design 2014Proceedings of the U.S./U.S.S.R. Seminar on Problems of Design, Development, Fabrication and Test of Breeder Reactor ComponentsNuclear Science AbstractsRobotic Fabrication in Architecture, Art and Design 2018Research, Development, and Mechanization in the United States Post Office DepartmentDistributed Intelligence In DesignAeronautical Engineering ReviewDesign Methodologies for Space Transportation SystemsDesign, Development, and Fabrication of a Sealed, Brushless DC MotorDesign, Fabrication, Properties and Applications of Smart and Advanced MaterialsU.S. Government Research ReportsProcesses and Design for ManufacturingAEC Authorizing Legislation, Fiscal Year 1969Smart Material Systems and MEMSMaterials, Design and Manufacturing for Lightweight VehiclesDesign, Development and Fabrication of a New Generation Semiconductor X-ray DetectorNuclear Regulatory Legislation, 109th Congress, 2nd SessionDigital Design and FabricationDesign, Development, and Fabrication of a Electronic Analog Microminiaturized Electronic Analog Signal to Discrete Time Interval ConverterDesign, Development, Fabrication, and Testing of a Synchronous Condenser for a High-power Three-phase Traction DriveProduct Design for Manufacture and Assembly In response to tremendous growth and new technologies in the semiconductor industry, this volume is organized into five, information-rich sections. Digital Design and Fabrication surveys the latest advances in computer architecture and design as well as the technologies used to manufacture and test them. Featuring contributions from leading experts, the book also includes a new section on memory and storage in addition to a new chapter on nonvolatile memory technologies. Developing advanced concepts, this sharply focused book— Describes new technologies that have become driving factors for the electronic industry Includes new information on semiconductor memory circuits, whose development best illustrates the phenomenal progress encountered by the fabrication and technology sector Contains a section dedicated to issues related to system power consumption Describes reliability and testability of computer systems Pinpoints trends and state-of-theart advances in fabrication and CMOS technologies Describes performance evaluation measures, which are the bottom line from the user’s point of view Discusses design techniques used to create modern computer systems, including high-speed computer arithmetic and high-frequency design, timing and clocking, and PLL and DLL designDesign for Manufacturing assists anyone not familiar with various manufacturing processes in better visualizing and understanding the relationship between part design and the ease or difficulty of producing the part. Decisions made during the early conceptual stages of design have a great effect on subsequent stages. In fact, quite often more than 70% of the manufacturing cost of a product is determined at this conceptual stage, yet manufacturing is not involved. Through this book, designers will gain insight that will allow them to assess the impact of their proposed design on manufacturing difficulty. The vast majority of components found in commercial batch-manufactured products, such as appliances, computers and office automation equipment are either injection molded, stamped, die cast, or (occasionally) forged. This book emphasizes these particular, most commonly implemented processes. In addition to chapters on these processes, the book touches upon material process selection, general guidelines for determining whether several components should be combined into a single component or not, communications, the physical and mechanical properties of materials, tolerances, and inspection and quality control. In developing the DFM methods presented in this book, he has worked with over 30 firms specializing in injection molding, die-casting, forging and stamping. Implements a philosophy which allows for easier and more economic production of designs Educates designers about manufacturing Emphasizes the four major manufacturing processesPresenting unified coverage of the design and modeling of smart microand macrosystems, this book addresses fabrication issues and outlines the challenges faced by engineers working with smart sensors in a variety of applications. Part I deals with the fundamental concepts of a typical smart system and its constituent components. Preliminary fabrication and characterization concepts are introduced before design principles are discussed in detail. Part III presents a comprehensive account of the modeling of smart systems, smart sensors and actuators. Part IV builds upon the fundamental concepts to analyze fabrication techniques for silicon-based MEMS in more detail. Practicing engineers will benefit from the detailed assessment of applications in communications technology, aerospace, biomedical and mechanical engineering. The book provides an essential reference or textbook for graduates following a course in smart sensors, actuators and systems.This book introduces various advanced, smart materials and the strategies for the design and preparation for novel uses from macro to micro or from biological, inorganic, organic to composite materials. Selecting the best material is a challenging task, requiring tradeoffs between material properties and designing functional smart materials. The development of smart, advanced materials and their potential applications is a burgeoning area of research. Exciting breakthroughs are anticipated in the future from the concepts and results reported in this book.The work described in this document was performed in compliance with the scope of work as specified in Contract AF 08(635)-2850 tendered Honeywell Ordnance Division on 13 June 1962. All phases of a complete development program were carried out in order to achieve the goal of developing a safe, highly reliable fuze compatible with available subsonic and supersonic delivery systems. The final result of this development program was a multi-purpose fuze operable in three different modes: impact short-delay, impact medium-delay, and airburst. Fuzes were subjected to every environmental, functional, and safety test for development of fuzes required by the Air Force and by the contract. A program for the development of fuzes incorporating a retard-mode capability into the fuze was conducted, but the mode could not be included without extensive fuze redesign. Several recommendations were made by the contractor to expend additional efforts under the production program to effect the following: loading simplification, battery firing device simplification or integration, safing and arming mechanism simplification, and general safety improvements. (Author).The book presents research from Rob|Arch 2018, the fourth international conference on robotic fabrication in architecture, art, and design. In capturing the myriad of scientific advances in robotics fabrication that are currently underway – such as collaborative design tools, computerised materials, adaptive sensing and actuation, advanced construction, on-site and cooperative robotics, machine-learning, human-machine interaction, large-scale fabrication and networked workflows, to name but a few – this compendium reveals how robotic fabrication is becoming a driver of scientific innovation, cross-disciplinary fertilization and creative capacity of an unprecedented kind.pt.1: Considers S. 2880 and companion H.R. 14905, to authorize appropriations for AEC. Focuses on general budget and reactor development program; pt.2: Continuation of hearings on AEC FY69 authorization. Appendix includes reports. a. ""National Accelerator Laboratory, Design Report 1968, Universities Research Associates, "" prepared by AEC 1968 (p. 1223-1456). b. ""Report of Ad Hoc Panel on Low-Beta Toroidal Plasma Research, "" Sept. 1967 (p. 1459-1583). c. ""Bronco Oil Shale Study, "" prepared by AEC, Interior Dept, CER Geonuclear Corp., and Lawrence Radiation Laboratory, Oct. 13, 1967 (p. 1743-1813).Compiles statutues and materials relating to nuclear regulatory legislation through the 109th Congress, 1st Session.Hailed as a groundbreaking and important textbook upon its initial publication, the latest iteration of Product Design for Manufacture and Assembly does not rest on those laurels. In addition to the expected updating of data in all chapters, this third edition has been revised to provide a top-notch textbook for ",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,https://www.semanticscholar.org/paper/76e78bc520bdc550e762a5d82bd6aca84bd8c5bb,Data Driven Methods For Fault Detection And Diagnosis In Chemical Processes Advances In Industrial Control Epdf File,"Guaranteeing a high system performance over a wide operating range is an important issue surrounding the design of automatic control systems with successively increasing complexity. As a key technology in the search for a solution, advanced fault detection and identification (FDI) is receiving considerable attention. This book introduces basic model-based FDI schemes, advanced analysis and design algorithms, and mathematical and control-theoretic tools. This second edition of Model-Based Fault Diagnosis Techniques contains: • new material on fault isolation and identification and alarm management; • extended and revised treatment of systematic threshold determination for systems with both deterministic unknown inputs and stochastic noises; • addition of the continuously-stirred tank heater as a representative process-industrial benchmark; and • enhanced discussion of residual evaluation which now deals with stochastic processes. Model-based Fault Diagnosis Techniques will interest academic researchers working in fault identification and diagnosis and as a text it is suitable for graduate students in a formal university-based course or as a self-study aid for practising engineers working with automatic control or mechatronic systems from backgrounds as diverse as chemical process and power engineering. With pressure increasing to utilise wastes and residues effectively and sustainably, the production of biogas represents one of the most important routes towards reaching national and international renewable energy targets. The biogas handbook: Science, production and applications provides a comprehensive and systematic guide to the development and deployment of biogas supply chains and technology. Following a concise overview of biogas as an energy option, part one explores biomass resources and fundamental science and engineering of biogas production, including feedstock characterisation, storage and pre-treatment, and yield optimisation. Plant design, engineering, process optimisation and digestate utilisation are the focus of part two. Topics considered include the engineering and process control of biogas plants, methane emissions in biogas production, and biogas digestate quality, utilisation and land application. Finally, part three discusses international experience and best practice in biogas utilisation. Biogas cleaning and upgrading to biomethane, biomethane use as transport fuel and the generation of heat and power from biogas for stationery applications are all discussed. The book concludes with a review of market development and biomethane certification schemes. With its distinguished editors and international team of expert contributors, The biogas handbook: Science, production and applications is a practical reference to biogas technology for process engineers, manufacturers, industrial chemists and biochemists, scientists, researchers and academics working in this field. Provides a concise overview of biogas as an energy option Explores biomass resources for production Examines plant design and engineering and process optimisation This book provides a complete picture of several decision support tools for predictive maintenance. These include embedding early anomaly/fault detection, diagnosis and reasoning, remaining useful life prediction (fault prognostics), quality prediction and self-reaction, as well as optimization, control and self-healing techniques. It shows recent applications of these techniques within various types of industrial (production/utilities/equipment/plants/smart devices, etc.) systems addressing several challenges in Industry 4.0 and different tasks dealing with Big Data Streams, Internet of Things, specific infrastructures and tools, high system dynamics and non-stationary environments . Applications discussed include production and manufacturing systems, renewable energy production and management, maritime systems, power plants and turbines, conditioning systems, compressor valves, induction motors, flight simulators, railway infrastructures, mobile robots, cyber security and Internet of Things. The contributors go beyond state of the art by placing a specific focus on dynamic systems, where it is of utmost importance to update system and maintenance models on the fly to maintain their predictive power. In many industrial applications early detection and diagnosis of abnormal behavior of the plant is of great importance. During the last decades, the complexity of process plants has been drastically increased, which imposes great challenges in development of model-based monitoring approaches and it sometimes becomes unrealistic for modern largescale processes. The main objective of Adel Haghani Abandan Sari is to study efficient fault diagnosis techniques for complex industrial systems using process historical data and considering the nonlinear behavior of the process. To this end, different methods are presented to solve the fault diagnosis problem based on the overall behavior of the process and its dynamics. Moreover, a novel technique is proposed for fault isolation and determination of the root-cause of the faults in the system, based on the fault impacts on the process measurements. Reliability Analysis and Asset Management of Engineering Systems explains methods that can be used to evaluate reliability and availability of complex systems, including simulation-based methods. The increasing digitization of mechanical processes driven by Industry 4.0 increases the interaction between machines and monitoring and control systems, leading to increases in system complexity. For those systems the reliability and availability analyses are increasingly challenging, as the interaction between machines has become more complex, and the analysis of the flexibility of the production systems to respond to machinery failure may require advanced simulation techniques. This book fills a gap on how to deal with such complex systems by linking the concepts of systems reliability and asset management, and then making these solutions more accessible to industry by explaining the availability analysis of complex systems based on simulation methods that emphasise Petri nets. Explains how to use a monitoring database to perform important tasks including an update of complex systems reliability Shows how to diagnose probable machinery-based causes of system performance degradation by using a monitoring database and reliability estimates in an integrated way Describes practical techniques for the application of AI and machine learning methods to fault detection and diagnosis problems",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e9db360184bd05e81e22e4863c552047ffca4754,https://www.semanticscholar.org/paper/e9db360184bd05e81e22e4863c552047ffca4754,Multi Uav Cooperative Surveillance With Spatio Temporal Ebooks File,"Covering the design, development, operation and mission profiles of unmanned aircraft systems, this single, comprehensive volume forms a complete, stand-alone reference on the topic. The volume integrates with the online Wiley Encyclopedia of Aerospace Engineering, providing many new and updated articles for existing subscribers to that work. Time-Critical Cooperative Control of Autonomous Air Vehicles presents, in an easy-to-read style, the latest research conducted in the industry, while also introducing a set of novel ideas that illuminate a new approach to problem-solving. The book is virtually self-contained, giving the reader a complete, integrated presentation of the different concepts, mathematical tools, and control solutions needed to tackle and solve a number of problems concerning time-critical cooperative control of UAVs. By including case studies of fixed-wing and multirotor UAVs, the book effectively broadens the scope of application of the methodologies developed. This theoretical presentation is complemented with the results of flight tests with real UAVs, and is an ideal reference for researchers and practitioners from academia, research labs, commercial companies, government workers, and those in the international aerospace industry. Addresses important topics related to time-critical cooperative control of UAVs Describes solutions to the problems rooted in solid dynamical systems theory Applies the solutions developed to fixed-wing and multirotor UAVs Includes the results of field tests with both classes of UAVs Across the globe, the past several years have seen a tremendous increase in the role of cooperative autonomous systems. The field of cooperative control and optimization has established itself as a part of many different scientific disciplines. The contents of this hugely important volume, which adds much to the debate on the subject, are culled from papers presented at the Seventh Annual International Conference on Cooperative Control and Optimization, held in Gainesville, Florida, in January 2007. First used in military applications, unmanned aerial vehicles are becoming an integral aspect of modern society and are expanding into the commercial, scientific, recreational, agricultural, and surveillance sectors. With the increasing use of these drones by government officials, business professionals, and civilians, more research is needed to understand their complexity both in design and function. Unmanned Aerial Vehicles: Breakthroughs in Research and Practice is a critical source of academic knowledge on the design, construction, and maintenance of drones, as well as their applications across all aspects of society. Highlighting a range of pertinent topics such as intelligent systems, artificial intelligence, and situation awareness, this publication is an ideal reference source for military consultants, military personnel, business professionals, operation managers, surveillance companies, agriculturalists, policymakers, government officials, law enforcement, IT professionals, academicians, researchers, and graduate-level students. Discover what lies beyond the bleeding-edge of autonomous airborne networks with this authoritative new resource Autonomous Airborne Wireless Networks delivers an insightful exploration of recent advances in the theory and practice of using airborne wireless networks to provide emergency communications, coverage and capacity expansion, information dissemination, and more. The distinguished engineers and editors have selected resources that cover the fundamentals of airborne networks, including channel models, recent regulation developments, self-organized networking, AI-enabled flying networks, and notable applications in a variety of industries. The book evaluates advances in the cutting-edge of unmanned aerial vehicle wireless network technology while offering readers new ideas on how airborne wireless networks can support various applications expected of future networks. The rapidly developing field is examined from a fresh perspective, one not just concerned with ideas of control, trajectory optimization, and navigation. Autonomous Airborne Wireless Networks considers several potential use cases for the technology and demonstrates how it can be integrated with concepts from selforganized network technology and artificial intelligence to deliver results in those cases. Readers will also enjoy: A thorough discussion of distributed drone base station positioning for emergency cellular networks using reinforcement learning (AI-enabled trajectory optimization) An exploration of unmanned aerial vehicle-to-wearables (UAV2W) indoor radio propagation channel measurements and modelling An up-to-date treatment of energy minimization in UAV trajectory design for delay tolerant emergency communication Examinations of cache-enabled UAVs, 3D MIMO for airborne networks, and airborne networks for Internet of Things communications Perfect for telecom engineers and industry professionals working on identifying practical and efficient concepts tailored to overcome challenges facing unmanned aerial vehicles providing wireless communications, Autonomous Airborne Wireless Networks also has a place on the bookshelves of stakeholders, regulators, and research agencies working on the latest developments in UAV communications. ""The ability to fly multiple unmanned aerial vehicles (UAVs) in collaboration has the potential to expand the scope of feasible UAV missions and could become the backbone of future UAV missions. However, despite having garnered significant research interest, there is no indication that systems supporting collaborative operation of multiple UAVs are close to achieving field deployment. The challenge of successfully deploying a quality system is inherently complex, and systems engineering offers an approach to handle the complexities. Effective application of systems engineering requires both knowledge breadth and depth. This thesis presents the results of a consolidation of information intended to support the conduct of systems engineering activities; and describes an experiment to ascertain the sensitivities of some key operational parameters, e.g., acquisition, pointing, and tracking. The experiment was conducted using Automatic Dependent Surveillance Broadcast (ADS-B) and visual tracking equipment employing state-of-the-art technology to understand the operating challenges and requirements of using this equipment to provide situational awareness for a UAV pilot""--Abstract. This book compiles some of the latest research in cooperation between robots and sensor networks. Structured in twelve chapters, this book addresses fundamental, theoretical, implementation and experimentation issues. The chapters are organized into four parts namely multi-robots systems, data fusion and localization, security and dependability, and mobility. This two volume set constitutes the refereed post-conference proceedings of the Second International Conference on Machine Learning and Intelligent Communications, MLICOM 2017, held in Weihai, China, in August 2017. The 143 revised full papers were carefully selected from 225 submissions. The papers are organized thematically in machine learning, intelligent positioning and navigation, intelligent multimedia processing and security, intelligent wireless mobile network and security, cognitive radio and intelligent networking, intelligent internet of things, intelligent satellite communications and networking, intelligent remote sensing, visual computing and three-dimensional modeling, green communication and intelligent networking, intelligent ad-hoc and sensor networks, intelligent resource allocation in wireless and cloud networks, intelligent signal processing in wireless and optical communications, intelligent radar signal processing, intelligent cooperative communications and networking. A comprehensive review of the state of the art in the control of multi-agent systems theory and applications The superiority of multi-agent systems over single agents for the control of unmanned air, water and ground vehicles has been clearly demonstrated in a wide range of application areas. Their large-scale spatial distribution, robustness, high scalability and low cost enable multi-agent systems to achieve tasks that could not successfully be performed by even the most sophisticated single agent systems. Cooperative Control of Multi-Agent Systems: Theory and Applications provides a wide-ranging review of the latest developments in the cooperative control of multi-agent systems theory and applications. The applications described are mainly in the areas of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Throughout, the authors link basic",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
75994ebb52094581dcb7d145795f6bafe6e276bb,https://www.semanticscholar.org/paper/75994ebb52094581dcb7d145795f6bafe6e276bb,Influence of artificial intelligence (AI) on firm performance: the business value of AI-based transformation projects,"The main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations.,This study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations’ AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase.,AI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features/technologies to reconfigure their processes.,AI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations’ performance, the profitability of their investments in AI transformation projects, and some competitive advantage. This study also allows managers to consider AI not as a single technology but as a set/combination of several different configurations of IT in the various company’s business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure.,This article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set/combination of the several different configurations of IT in various industries.",Bus. Process. Manag. J.,2020,10.1108/bpmj-10-2019-0411,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5986fe7a07cfeece0f8596e306abdeecc0465f82,https://www.semanticscholar.org/paper/5986fe7a07cfeece0f8596e306abdeecc0465f82,Artificial intelligence for defense applications,"Artificial intelligence (AI) is a set of algorithmic techniques, tools, and technologies that provide machines with the ability to perform tasks that normally require human intelligence – to perceive the world, learn from experience, reason about information, represent knowledge, act, and adapt. Given the multitude of rapid technological advancements in AI, the defense community has emphasized the importance of leveraging these very technologies to be prepared to fight and win the wars of the future. As one of the ways to modernize key capabilities, the defense community has specifically mentioned the need to invest broadly in the military application of AI, including rapid application of commercial breakthroughs, to gain competitive military advantages. To solve some of the most critical problems facing the defense community, the future force requires the ability to converge capabilities from across multiple domains at speeds and scales beyond human cognitive abilities. This special issue is composed of six papers that promote an understanding of AI for defense applications, as well as providing awareness into some of the state-of-theart research and development activities in AI that are applicable to defense applications spanning fraud detection for national security, computer vision for satellite imagery analysis, hidden Markov modeling for the maritime domain, deep learning for radio frequency systems, representation learning for militarily relevant graphs, and robot swarms for military reconnaissance and surveillance. First, the paper by Kerwin and Bastian investigates the national security challenge of predicting fraud, as criminals continually exploit the electronic financial system to defraud consumers and businesses by finding weaknesses in the system, including in audit controls. Their work uses stacked generalizations via meta-learning combined with a resampling methodology particularly useful for the imbalanced fraud data structure to improve fraud detection for national security. Second, the paper by Humphries, Parker, Jonas, Adams, and Clark investigates the problem of quickly and accurately identifying building and road infrastructure via satellite imagery for the execution of tactical military operations in an urban environment. Their work uses an object detection algorithm powered by convolutional neural networks to predict both buildings and road intersections present in an image, as well as use of a contourfinding algorithm for data labeling. Third, the paper by Caelli, Mukerjee, McCabe, and Kirszenblat tackles the problem of integrated sensor and tactical information fusion from a number of sources to enable rapid decision throughput based upon situation awareness for maritime surveillance missions. Their work develops a method using a hidden Markov model to objectively encode, summarize, and analyze airborne maritime surveillance crew activities to gain insights into probabilistic relationships between the attention switching across sensor types and surveyed objects over the entire mission. Fourth, the paper by Clark, Hauser, Headley, and Michaels investigates the radio frequency system problem of automatic modulation classification for situational awareness. Their work examines how useful a synthetically trained system is expected to be when deployed without considering the environment within the synthesis, how training data augmentation can be leveraged for deep learning in the radio frequency domain, and what impact knowledge of degradations to the signal caused by the transmission channel contributes to radio frequency system performance. Fifth, the paper by Lawley, Frey, Mullen and WissnerGross explores the sparse graph representation learning problem for network link prediction and node classification tasks and whole-network reconstruction applicable to militarily relevant graphs such as social and sensor",,2021,10.1177/15485129211009072,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4a1b1e54455ce6339f1ad5cf8b0ea2abed919cc2,https://www.semanticscholar.org/paper/4a1b1e54455ce6339f1ad5cf8b0ea2abed919cc2,Rotten-Fruit-Sorting Robotic Arm: (Design of Low Complexity CNN for Embedded System),": Industrial Automation has revolutionized the processing industry due to its high accuracy, the time it saves, and its ability to work without tiring. Being the most fundamental part of automation machines, robotic arms are being used as a fundamental component in many types of domestic as well as commercial automation units. In this paper, we proposed a low-complexity convolutional neural network (CNN) model and successfully deployed it on a locally generated robotic arm with the help of a Raspberry Pi 4 module. The designed robotic arm can detect, locate, and classify (based on fresh or rotten) between three species of Mangos (Ataulfo, Alphonso, and Keitt), on a conveyor belt. We generated a dataset of about 6000 images and trained a three-convolutional-layer-based CNN. Training and testing of the network were carried out with MatLab, and the weighted network was deployed to an embedded environment (Raspberry Pi 4 module) for real-time classiﬁcation. We reported a classiﬁcation accuracy of 98.08% in the detection of fresh mangos and 95.75% in the detection of rotten mangos. For the designed robotic art, the achieved angle accuracy was 93.94% with a minor error of only 2 ◦ . The proposed model can be deployed in many food- or object-sorting industries as an edge computing application of deep learning.","The 1st International Conference on Energy, Power and Environment",2022,10.3390/engproc2021012109,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
32480856d3c70b3a5d737352376e553517d8a58b,https://www.semanticscholar.org/paper/32480856d3c70b3a5d737352376e553517d8a58b,List of TA topics: EE1 (Communications and Signal Processing),"Description: The specific goals of the PhD projects are to numerically simulate and demonstrate different optical fiber designs, components, and devices suitable for mid-infrared technology. Student will be thoroughly trained in simulating optical fiber technology using COMSOL Multiphysics and hands-on experience in handling current state-of-the-art Opto-electronics instruments such as CCD sensors, oscilloscope, spectrum analyzer etc. Interested students are encouraged to contact the PI for further discussions. Description: Erbium-doped optical amplifiers are the backbone of optical communication, used both for transmission and detection at the receiving end. Most of the photonic detectors in the transmission window have a typical sensitivity of the -24 dBm. A very low noise signal amplifier is needed which can detect the weak signal and increase the sensitivity of the system to -60/65 dBm or even less. The project will exploit optical parametric amplification, a type of nonlinear optical effect, which allows for low-noise optical amplification with very low degradation of the signal-to-noise ratio. Different gain mediums will be explored. There will be a strong emphasis on compact and robust amplifier systems. The student will have the opportunity to learn and model nonlinearity in different materials, amplifier physics, signal processing, and hands-on experience with current state-of-the-art characterization pieces of equipment such as spectrum analyzers, oscilloscopes, CCD sensors, etc. The project is highly relevant for students looking for the opportunity both in academia and industry. Interested students are encouraged to contact the PI for further discussions. Description: Fiber optics based on long thin strands of glass and electronics based on planar chips have become the backbone of the electronics and telecommunication industry. There have been attempts to fabricate multi-material fibers for exploiting the exotic properties of both semiconductors and glasses. Recent fabrication progress in multi-material fabrication technology has raised interest in this technology. This project will explore and demonstrate next-generation novel semiconductor fibers to exploit the complementary properties of both semiconductors and glasses. We aim to find out suitable compositions of materials that are feasible to fabricate using current state-of-the-art fabrication technology leading to all-fiber optoelectronics systems. The student will have the opportunity to learn to model semiconductor physics and optical fibers using semiconductor physics and RF modules of COMSOL Multiphysics, hands-on experience with current-state-of-the-art cleanroom fabrication facilities, and characterization techniques. Interested students are encouraged to contact the PI for further discussions. Description: The objective of this project is to develop an on-board charger (OBC) system for four-wheeler electric vehicles (EVs), compatible with both 3-phase, level-3 (22 kW) charging and 1-phase, level-2 (7 kW) charging. Such a converter would represent an ""universal"" charging solution allowing both fast, public-space charging from 3-phase EVSEs as well as slow, overnight charging from 1-phase residential power outlets. The overarching design goal will be to develop a very high power-density converter prototype, which can offer significant size and weight benefits to the EV system, compared to conventional approaches. This will be achieved primarily through innovations in converter topology by exploring low device-count, single-stage circuits, which allow interfacing with 3-phase as well as 1-phase ac and can operate with small-sized energy buffer capacitors. Additionally, advances in soft-switching methods (enabling switching frequencies of several 100s of kHz) , modulation, control and high-frequency magnetics design will also be pursued. assay low-cost front-end electronics embedded systems integrated biosensors applications, nucleic in adulterants and biomarkers to techniques to understand fundamental aspects of interaction of biological analytes with such systems as well as realizing commercializable applications. Our works Bioengineering (WRCB) and can forward to a and Description: The goal of this project is to develop a reconfigurable CMOS Receiver System-on-chip (SOC) (RF + Digital) for NavIC and GPS in 28-nm CMOS technology. The receiver SOC will extract the Position, Velocity, and Time (PVT) information of the user/receiver from the satellite signals of NavIC and GPS for Standard Positioning Services (SPS). PhD students will get trained in the area of RF/Analog/Mixed-signal IC design. They will do circuit design and layout of novel High performance circuits such as LNA, Mixed, PLL, VCO, Filters, Analog to Digital Converters (ADC). Students will also be involved in Chip measurements in the lab. Description: The goal of this project is to develop a reconfigurable CMOS Receiver System-on-chip (SOC) (RF + Digital) for NavIC and GPS in 28-nm CMOS technology. The receiver SOC will extract the Position, Velocity, and Time (PVT) information of the user/receiver from the satellite signals of NavIC and GPS for Standard Positioning Services (SPS). PhD students will get trained in the area of digital IC design flow. They will design digital baseband starting from algorithms --> Matlab --> HDL --> FPGA verification --> ASIC implementation --> Timing closure/Place and Rout activities. Students will also be involved in Chip measurements in the lab. The PhD aspect comes in the design of innovative implementation of complex algorithms required for decoding NAVIC and GPS signals (Correlator design). Description: Artificial smart skin is a growing area finding its applications in robotics and medical applications. This technology also brings new aspects of energy provision for the artificial skin. In this project we will work on the flexible layers of generators with a CMOS power management ASIC which will be designed, fabricated and integrated on the artificial skin. Description: Various emerging sensing applications need energy efficient and noise-tolerant analog signal conditioning for the high performance, followed by novel hybrid low-energy high-resolution architectures for the analog to digital conversion (ADC). The purpose of this project is to extend such functionalities with high performance to the various domains of information for better tolerance to the internal and external noise as well as calibration for the selective analog to digital conversion. electric fields, temperature, strain fields, and pressure. NVCs can detect magnetic fields with a high spatial resolution down to the nanoscale and on samples in environmental conditions not suitable for alternative solid-state magnetometers. Similarly, thin flakes of hBN are upcoming platform in the domain of spin-based sensing. This project will be to develop precision quantum sensors. Further, we will investigate a variety of applications of these magnetometers, in particular – investigating bio-magnetic materials and 2D materials with spatially dependent local magnetic field. Description: The project involves design, fabrication and characterization of optical silicon chip based micro-ring resonators for the generation of frequency combs. These study include efficient light confinement within waveguides and resonators and manipulation of nano-scale objects using guided waves. The frequency combs will be used to calibrate optical clocks used for time standards and for spectroscopy. Development of machine learning techniques for ""Beyond Moore"" device simulation platforms. Description: Developing a composite device to circuit modelling platform for spintronic neuromorphic architecture. This will typically feature basic device modelling with the integration into h-spice based circuit models for a given target functionality. Description: Topological insulators are known to host ""dissipation-free"" electrical conduction. The project will aim to evaluate and design electrical interconnects that can exploit this dissipation free channel. The project will involve developing circuit models starting from the physics of topological insulators. Description: Heavy-metal-and-ferromagnet-heterostructure-based spintronic devices exhibit various interesting properties which open up possibilities for using them for neuromorphic computing — a brain-inspired computing paradigm which often uses in-memory-computing architectures, spike-based information flow, and oscillatory behaviour for artificial-intelligence (AI) applications where low energy consumption is a necessary requirement (edge AI/ tiny ML, robotics, etc). This project focuses on carrying out device experiments and device-circuit-system co-simulations to investigate three major aspects of such spintronics-based neuromorphic computing: (a) Use of domain-wall motion for non-volatile analog weight storage, to be used as synapses in crossbar arrays for implementing non-spiking fully connected and convolutional neural networks (b) Use of domain-wall motion as an integrator for mimicking spiking neurons in spiking neural networks (c) Use of the synchronizaion dynamics of spin Hall nano oscillators for oscillator-based pattern recognition.. Description: Large band gap and ease of growth of gallium oxide has led to a new wave in wide bandgap power devices and electronics. At the same time, many fundamental challenges in gallium oxide processing and device fabrication offer an opportunity to explore new device physics and engineer high impact solutions. This project will explore the fabrication of normally off high power gallium oxide transistors for power switching applications. It will involve experimental device fabrication, characterisation and TCAD simulations. Description: India has announced plans to set up up to 300,000,000,000 W of Solar PV by 2030 to combat climate change. This significant amount of PV deployment is going to happen with new techno",,2022,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4191f403cb8a7c6f43a7b69e619b06b179ab322c,https://www.semanticscholar.org/paper/4191f403cb8a7c6f43a7b69e619b06b179ab322c,Application of Tubular Reactor Technologies for the Acceleration of Biodiesel Production,"The need to arrest the continued environmental contamination and degradation associated with the consumption of fossil-based fuels has continued to serve as an impetus for the increased utilization of renewable fuels. The demand for biodiesel has continued to escalate in the past few decades due to urbanization, industrialization, and stringent government policies in favor of renewable fuels for diverse applications. One of the strategies for ensuring the intensification, commercialization, and increased utilization of biodiesel is the adaptation of reactor technologies, especially tubular reactors. The current study reviewed the deployment of different types and configurations of tubular reactors for the acceleration of biodiesel production. The feedstocks, catalysts, conversion techniques, and modes of biodiesel conversion by reactor technologies are highlighted. The peculiarities, applications, merits, drawbacks, and instances of biodiesel synthesis through a packed bed, fluidized bed, trickle bed, oscillatory flow, and micro-channel tubular reactor technologies are discussed to facilitate a better comprehension of the mechanisms behind the technology. Indeed, the deployment of the transesterification technique in tubular reactor technologies will ensure the ecofriendly, low-cost, and large-scale production of biodiesel, a high product yield, and will generate high-quality biodiesel. The outcome of this study will enrich scholarship and stimulate a renewed interest in the application of tubular reactors for large-scale biodiesel production among biodiesel refiners and other stakeholders. Going forward, the use of innovative technologies such as robotics, machine learning, smart metering, artificial intelligent, and other modeling tools should be deployed to monitor reactor technologies for biodiesel production.",Bioengineering,2022,10.3390/bioengineering9080347,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
14c7796270b8e86a96d6f54951cd4c34ee4b88c0,https://www.semanticscholar.org/paper/14c7796270b8e86a96d6f54951cd4c34ee4b88c0,Leveraging Tactile Sensors for Low Latency Embedded Smart Hands for Prosthetic and Robotic Applications,"Tactile sensing is a crucial perception mode for robots and human amputees in need of controlling a prosthetic device. Today, robotic and prosthetic systems are still missing the important feature of accurate tactile sensing. This lack is mainly due to the fact that the existing tactile technologies have limited spatial and temporal resolution and are either expensive or not scalable. In this article, we present the design and implementation of a hardware–software embedded system called SmartHand. It is specifically designed to enable the acquisition and real-time processing of high-resolution tactile information from a hand-shaped multisensor array for prosthetic and robotic applications. During data collection, our system can deliver a high throughput of 100 frames per second, which is $13.7\times $ higher than previous related work. This has allowed the collection of a new tactile dataset consisting of 340 000 frames while interacting with 16 objects from everyday life during five different sessions. Together with the empty hand, the dataset presents a total of 17 classes. We propose a compact yet accurate convolutional neural network that requires one order of magnitude less memory and $15.6\times $ fewer computations compared with related work without degrading classification accuracy. The top-1 and top-3 cross-validation accuracies on the collected dataset are, respectively, 98.86% and 99.83%. We further analyze the intersession variability and obtain the best top-3 leave-one-out-validation accuracy of 77.84%. We deploy the trained model on a high-performance ARM Cortex-M7 microcontroller achieving an inference time of only 100 ms minimizing the response latency. The overall measured power consumption is 505 mW. Finally, we fabricate a new control sensor and perform additional experiments to provide analyses on sensor degradation and slip detection. This work is a step forward in giving robotic and prosthetic devices a sense of touch by demonstrating the practicality of a smart embedded system that uses a scalable tactile sensor with embedded tiny machine learning.",IEEE Transactions on Instrumentation and Measurement,2022,10.48550/arXiv.2203.15069,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
3ac4c4bda5791b076eca9a633f7226dab1cbf9b6,https://www.semanticscholar.org/paper/3ac4c4bda5791b076eca9a633f7226dab1cbf9b6,Editorial for the special issue on Machining Science,"Machining has been at the centre of manufacturing technologies since the start of the industrial revolution, and so it is fitting that this topic receives special attention in a journal that can trace its origins back to 1847. The first publication of the Proceedings of the Institution of Mechanical Engineers describes a healthy debate concerning the machining of gear teeth, alongside an obituary to George Stephenson himself – the ‘father of railways’ and the founding president of the Institution of Mechanical Engineers. The next 60 years saw a steady growth in scientific and technical dissemination of knowledge concerning machining, which is epitomised by Taylor’s famous monograph of 1907. Over 100 years later, we have of course seen huge leaps in our understanding of all manufacturing processes. But the strive for increased productivity and quality is now also matched by a need for resource efficiency in light of societal challenges such as climate change and pollution. The contributions included in this special issue seek to demonstrate how machining science research is playing a role in addressing this challenge: a rethinking of manufacturing is underway as a consequence of machining learning, ubiquitous data, and networked computing and machining science is a key part of this shifting manufacturing landscape. The topics that are covered are intentionally diverse: they illustrate a vibrant and creative scientific approach across the spectrum of material removal processes, and show emergent approaches that can harness flexible manufacturing processes, as well as data-driven and intelligent automation. The special issue begins with a focus on novel techniques for monitoring the performance of machine tools and their cutting operations. Here, there have been great developments in machine learning techniques that can be brought to bear on production processes. To pick just two examples from the manuscripts: McLeay et al. develop fault detection techniques based upon unsupervised learning methods, and Moore et al. also demonstrate how machine learning concepts can be applied to machine health monitoring. The deployment of these novel monitoring techniques necessitates effective measurement capabilities, and on novel manufacturing problems this can itself be a challenge. Alhadeff et al. explore wear measurements in micro milling, whilst Duboust et al. characterise surface roughness in machining of composites. The machining of new materials, and workpieces produced using novel additive manufacturing techniques, also presents challenges. Several manuscripts within this special issue address these problems, focussing for example on Inconel (Curtis et al.), Titanium (Khan et al.) and metal-matrix composites (Saberi et al.). Finally, the development of state-of-the art modelling techniques can help to improve the performance of machining processes, focussing for example on dynamic effects (Urena et al.) and Robotics (Rooker et al.). At The University of Sheffield, we have been fortunate to be able to explore these avenues of research within the remit of an EPSRC Centre for Doctoral Training in Machining Science (Grant Reference EP/ L016257/1). This special issue was borne from discussions with the journal’s editorial board, in particular Professors Maropoulos and Long, as a consequence of the doctoral training centre. Consequently, much of the work included in this special issue has been inspired by the work within the doctoral training centre. The guest editors, who are co-directors of the centre, are grateful for the support of the journal’s editorial office who have ensured an independent peer review process for these manuscripts. We also express our thanks to Dr Francesca Breeden for her assistance in coordinating the special issue.","Proceedings of the Institution of Mechanical Engineers, Part B: Journal of Engineering Manufacture",2021,10.1177/09544054211021034,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e11ebf43c3cde2aaf44f7450be225279070a0ccd,https://www.semanticscholar.org/paper/e11ebf43c3cde2aaf44f7450be225279070a0ccd,Enhancing Humans Trust in Robots through Explanations,"Robots have moved away from manufacturing environments and are now deployed as social robots in human environments such as in hotels, shops, hospitals and as office coworkers. These robots complement human capabilities and skills with their own robotic skills.
With the advancement in the technological capabilities of robots, the roles of such sophisticated robots are evolving from obedient deterministic machines to companions or teammates. Meanwhile, the role of humans is also changing from operators to that of team members. Therefore, robots are expected to collaborate and contribute productively with humans as teammates. We expect robots to develop social intelligence to behave smartly, and also assist us to perform complex tasks. Still, robots lack the features that would permit them to be considered full-fledge teammates by their human counterparts. Inadequacy of humans trust has been identified as a pre-eminent factor behind the unacceptability of robots as trustworthy teammates.
Trust is an essential factor for accomplishing the full potential of human-robot teamwork. Trust directly affects a human's willingness to receive robot-produced information and suggestions, and hence, the future use of robots also depends on trust. If humans do not trust robots, they may not utilize their robotic features to their full potential. Research is on-going to address the establishment and endorsement of efficient and successful approaches for an extensive spectrum of Human-Robot Interaction issues. Pragmatic evaluations and investigations in the field of Human-Computer Interaction have already examined humans' trust in technical systems mostly on issues such as reliability and accuracy of performance.
We hypothesize that to integrate robots into human-environment successfully, robots must make their decision-making transparent to the humans in the mixed human-robot team. We argue that the trust humans place in their robotic companions is influenced by the humans' achieving some understanding of the robot's decision-making process. We propose to achieve higher levels of trust in robots, by making the robots produce explanations in human understandable terms. Our thesis is that the explanations from robots shall express how a decision is made and why the decision-made is selected as best among all other decisions.
By augmenting robots with explanation capabilities, we facilitate humans to comprehend the behaviour of robots and help in establishing successful and trustworthy human-robot interaction. Artificial intelligence researchers, within the area of expert systems, have also provided sufficient motivation to consider the contribution of explanations to building humans trust and to the acceptability of these systems. Also, systems that provide explanations after some failure received more tolerant behaviour from humans. Providing explanations for decisions is believed to be one of the most important capabilities of robots. However, to the best of our knowledge, there is still a gap in the current human-robot interaction literature. We notice there is very little experimental verification that could show that explanations facilitate and certainly affect humans trust and acceptance of robots. Previous research [1] used a different method to increase transparency by having a simulated robot to provide explanations of its actions. Explanations did not improve the team's performance and trust was identified as an influential factor only under the conditions of high-reliability.
To better comprehend the emerging topic of trust, we adopted the human-in-the-loop approach, by providing clear explanations, with emphasis on the transparency and justification of the robot decisions. We report on two user studies investigating the effect of a robot's explanations with different modalities (text and audio) on the humans' level of trust during human-robot physical interactions.
For user study 1, our setting consists of an interactive game-playing environment (the partial information game Domino), in which the robot partners with a human to form a team. Since in the game there are two adversarial teams, the robot plays two roles: the already mentioned partner with a human in a team, but also as an adversary facing the second team of two humans. Explanations from the partner robot not only provide insight into the robot's decision-making process, but also help in improving humans' learning of the task. We evaluated the human participants' implicit trust in the robot by performing multi-modal scrutiny i.e., recording human participants' facial expressions, and affective states during the game-play sessions. We also used questionnaires to measure participants' explicit trust and perception of the robot attributes. Our results show that the human participants considered the robot with explanations' ability as a trustworthy team-mate.
For user study 2, human participants performed a decision-making task in collaboration with a real robot. For the proposed method, we set the focus of our inquiry through humans' conformation and acceptance of the robot's answers, as a new objective measure of the human-robot trust relationship. We found that human participants trusted and conformed more with the robot's decisions (communicated with explanations), as compared to their own decisions. Meanwhile, subjective measures using questionnaires also reported an increase in trust of human participants towards the robot. Through our experimental investigations, we conclude that explanations can be generally used as an effective communication modality for robots to earn human trust in social environments.",,2021,10.25904/1912/4071,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a5ca4aec409637c8c80544d17a2ab90f6bebb18f,https://www.semanticscholar.org/paper/a5ca4aec409637c8c80544d17a2ab90f6bebb18f,Digital Transformation Journey of Field Operations at Abu Dhabi Offshore Field in UAE,"
 Field operations generate large volumes of data from various equipment and associated Meta data such as inspection due dates, maintenance schedule, people on board, etc. The data is often stored in silos with a data guardian for each entity. The objective of this project was to volarize the data by developing engineered KPI's to drive decision making and make data accessible for everyone in the organization to foster cross collaboration.
 Data analytics and visualization solutions were developed to automate low value-added tasks either using robotic process automation scripts or business intelligence reporting tool. Data was residing either in spreadsheet or native applications. With support of IT, centralized database was established. Scrum agile project management techniques were used to develop digital solutions. A high-level digital road map was created consulting all teams including stake holders. Use cases were identified and captured in lean A3 problem solving format. Each use case clearly identified the benefits to organization, and this was used to prioritize the use cases. A sprint was set-up with agile team and products were developed as per end user's expectation. The constant feedback loop via daily stand-up meetings helped the team deliver value added products.
 Digital solutions were developed to automate low value-added tasks so employees can focus on improving systems instead of producing reports. By developing engineering KPI's and predictive analytics, technical authority could shift from reactive maintenance to pro-active maintenance. Using linear regression machine learning, early warning digital solution was developed to monitor and notify technical authority to clean strainers. The production team achieved 0.75 full time equivalent (FTE) in time savings by automating reports. By visualizing operations data such as flaring, production profiles; the team minimized flaring leading to 1% OPEX cost saving. Around 10% of chemical budget was saved by monitoring chemical injections at all platforms. Similar cost savings were achieved by visualizing data for other disciplines such as maintenance and HSE teams. By being better informed about wells annuli pressure build-up via email notifications, wells integrity team reduced the associated risk. By forming a multi-disciplinary agile team with business and delivery team, digital team deployed 20+ digital products over a short time frame of 2 years.","Day 1 Mon, November 15, 2021",2021,10.2118/207386-ms,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f739a81418019b7557762215e48d2a8ed5a2d289,https://www.semanticscholar.org/paper/f739a81418019b7557762215e48d2a8ed5a2d289,Computer Vision based Polyethylene Terephthalate (PET) Sorting for Waste Recycling,"Recycling plays a vital role in saving the planet for future generations as it allows keeping a clean environment, reducing energy consumption, and saving materials. Of special interest is the plastic material which may take centuries to decompose. In particular, the Polyethylene Terephthalate (PET) is a widely used plastic for packaging various products that can be recycled. Sorting PET can be performed, either manually or automatically, at recycling facilities where the post-consumed objects are moving on the conveyor belt. In particular, automated sorting can process a large amount of PET objects without human intervention. In this paper, we propose a computer vision system for recognizing PET objects placed on a conveyor belt. Specifically, DeepLabv3+ is deployed to segment PET objects semantically. Such system can be exploited using an autonomous robot to compensate for human intervention and supervision. The conducted experiments showed that the proposed system outperforms the state of the art semantic segmentation approaches with weighted IoU equals to 97% and Mean BFscore equals to 89%. Keywords—PET; recycling; computer vision; machine learning",International Journal of Advanced Computer Science and Applications,2021,10.14569/ijacsa.2021.0121069,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4edb34030de1a3134190db2d181c2deba3324241,https://www.semanticscholar.org/paper/4edb34030de1a3134190db2d181c2deba3324241,PIRMedic: physics-driven fault diagnosis for PIR sensors,"Passive Infra-Red (PIR) sensors are an integral part of modern living. They have diverse applications ranging from automatic lighting and heating control in smart buildings, towel dispensers in washrooms, security alarms (for intrusion detection) to even human detection robots (for search and rescue). Unfortunately, PIR sensors are prone to failures during deployment due to reasons such as environmental damage, incorrect installation and component degradation among others that can lead to incorrect or faulty data. Currently, such failures are typically detected using either : (a) heavily engineered data-driven, statistical approaches that can have high false positive rates due to unseen data patterns or (b) expensive methods that use additional hardware such as video cameras or a golden reference sensor. The second approach inhibits scalability. In this work, we first create a taxonomy for the most common PIR sensor failures. We then present PIRMedic---a physics-driven approach, implemented at the edge, to detect the various classes of failures. We show that we can both detect and diagnose the failures in a PIR sensor using an intrinsic hardware signal viz., the analog output from the pyroelectric element in the sensor. Using this hardware signal in conjunction with frequency analysis and supervised machine learning methods, we obtain a high accuracy of 98 -- 99% in failure detection and diagnosis. We evaluate our methods using real-world deployments in three distinct locations, in different environment and usage conditions.",BuildSys@SenSys,2021,10.1145/3486611.3486658,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
05956c3c3b62c07e6726f7ef8a4b340ddcbe3f97,https://www.semanticscholar.org/paper/05956c3c3b62c07e6726f7ef8a4b340ddcbe3f97,Senior Design Instructors for 2021-2022 Contact Information,"In this project, we are creating a health equity dashboard for Connecticut. Our goal is the bring different data sources together and display to the users to show what types of programs or services should be provided in the different places in Connecticut. In order to accomplish this, we will be using Connecticut’s Census data along with datasets from Centers for Medicare & Medicaid Services. We hope our webpage will be informative of the services we should provide in the various towns and counties of Connecticut. The CT National Guard Cyber Range 2021 Project focuses on working with the ongoing project that was developed last year. This year, the goal was to move the cyber range to the cloud for remote access. Currently, the cyber range simulates two blue team scenarios: a ransomware attack and a vulnerable Active Directory access page. As cyber attacks–more specifically ransomware attacks–are rising in use, this cyber security project holds real world, practical applications. With this year’s team, each scenario is now remotely deployable, allowing CTNG trainees to practice National Guard battle plans from wherever. The project uses Amazon Web Services for cloud compute virtual machines, with Terraform for automating virtual machine deployment. Furthermore, practice scenarios are now easily addable due to the abstraction that was made possible through virtualization. The current scenarios are built with a variety of technologies, from Powershell scripts and Active Directory on Windows to Apache on Linux, but a wider variety is possible with ability to build and add any scenarios desired. Making the ongoing project more easily modifiable and adaptable allows the CTNG and any further teams to build quicker. This is essential as cyber security is now more important than ever, and any solutions in the field must be able to evolve quickly. and deprecated. Additional steps towards were taken by updating the protocol that certain tasks utilize from communicating with a SOAP to utilizing REST commands. by other the so-called Condition-Based Maintenance (CBM) where the maintenance is only conducted when it is really needed other than scheduled so that the maintenance cost is reduced and engine time-on-wing is extended; Doing the maintenance at the right place allows the needed facility and part inventory are in place to support the maintenance (for commercial airlines, this is usually the hubs). As an engine OEM, Pratt & Whitney is committed to provide dependable jet engine health management. The key to support doing the right maintenance at the right time and the right place is to identify a failure accurately and reliably at an early stage so that there will be ample time for the aircraft to get to the right place for maintenance. For most engine components, a failure is a state of the component that a fault is progressed to. By definition, a fault is a physical imperfection or impairment that is responsible for failure while a failure is a state of inability to perform a normal function. As shown in the P-F curve in Figure 1, the earlier a fault is detected, the less cost to repair would incur. It is well-known that the earlier a fault is detected, the more likely the detection is a false alarm. Excessive false alarms would defeat the propose of doing the right maintenance. Also, some of the fault may lead to safety critical failures, in which case, the recall rate of the detection has to be 100%. Overall, the goal for this project is to implement a machine learning algorithm which can accurately identify and diagnose engine faults. Battery powered IoT devices are a common tool in today’s jobsite. However, the downtime required to charge or replace dead batteries is costly at a large scale, so it is in a company’s best interest to optimize the battery life of their IoT devices. It is not uncommon to produce a noticeable decrease in the battery life of a device after even the slightest of updates to its firmware. Therefore, it is valuable to have the ability to run constant checks to ensure a device’s expected battery life stays within a tight range. Our project will provide companies and technology developers an easy means of testing firmware changes before finalizing device updates in order to catch these power-draining bugs. We have partnered with Triax Technologies to develop an open-source continuous integration (CI) testing platform for analyzing the change in power consumption of a device following any update. Our goal is to simplify the user experience by creating a modular and highly-configurable framework that allows developers to use the CI platform and power profiler of their choosing and have complete control of all aspects of the testing. The output to the user includes graphical interpretations of the analysis and readable results that can be sent via email or added as a comment on a pull request. The project is focused on creating and implementing two features on The Whelen Engineering Company’s web-based application: the Whelen Cloud Platform (WCP). WCP provides a real-time vehicle tracking service to emergency services with an interactive map that displays the locations and status of all vehicles. Our job was to implement two features for the the Live Map designed to be useful for coordinating fire fighting services during an emergency. While these features were originally specific to fire trucks, we designed them in such a way that they could be useful for any organization that Whelen provides services for, whether it be law enforcement, emergency medical services, or even the Department of Transportation. that A cyber range is a virtual environment in which users can simulate cyber attacks, test cybersecurity technology, host competitions, etc. In order to address the current limitations of cyber ranges, our team proposes that a domain specific language (DSL) should be used for cyber range development. A DSL within a cyber range would allow for customization of cyber range scenarios and standardization of scenario development. Our team will create a prototype of a DSL that could be implemented within a cyber range using Xtext, and write an NSF proposal supporting the use of DSLs in a cyber range. Our project also addresses the lack of accessibility with current cyber ranges. We suggest a hybrid licensing model, as well as more cost effective platforms to run our cyber range on (Raspberry Pi, existing cyber ranges, etc.). The team has created a prototype Google Chrome extension to adapt website content to be more accessible. The tool adjusts color schemes, font size, font family, and line and letter spacing to benefit users diagnosed with dyslexia and minor visual impairments. The resulting software will allow users to have equal opportunity to access and digest information online. The goal of our project is primarily about implementing three post-quantum signature verification algorithms to be run on the Xilinx ZCU102 development board and running performance tests on said algorithms. We seek to use the performance metrics in order to determine which algorithm has the best trade off between speed and security, especially in the context where the ARM processor must be reset and the boot time minimized. Our project develops software for a specific vibro-tactile biofeedback system. This system is designed to help firefighters navigate smoke filled buildings. Our vibro-tactile biofeedback system uses patterns of vibration felt on the body to provide new ways of perceiving direction. UWB radio based positioning system is used to determine the ranging values and location of a mobile unit. These systems are integrated together to create a prototype device that can be used to blind-navigate around an indoor environment. For this project, Sonalysts would like to have the students design and develop an AR application that interacts with static exhibits. Our project is an augmented reality mobile application that is able to render animated 3D holograms in real-time and geospatially anchor them on physical objects and/or tags in the real world. It is capable of image recognition, object detection, geolocation mapping, marker recognition, and it can support an augmented reality view that contains spatial awareness and AR sessions. In addition to this, our application is able to render animated 3D object models, retrieve and fetch them from a cloud database in real-time, contain a secure user authentication and authorization service, and should have a friendly and interactive user interface. When climate disasters occur and damage properties, insurance companies are expected to pay for losses covered by a client’s policy. The issue, however, is that many of these policies are determined by historical data. This historical data determines how much an insurance company is expected to pay if a claim is made and under what circumstances a claim is valid. This past data is rapidly becoming more and more dissimilar to what is happening today due to changes in the frequency and severity of climate disasters. It is vital that better solutions are developed in order to reflect the current circumstances. It is for these reasons that the project Climate Risk Strategy Analyzer (CRISTAL) was created. The purpose of CRISTAL is to act as a tool for broker agencies to evaluate their clients current insurance policies and determine what further coverage may be needed when the current climate situation is taken into consideration. By combining a climate risk analysis and a coverage gap analysis, CRISTAL will provide users with predictive and prescriptive analyses. Such analyses will include possible climate events that can affect a client’s business, automated recommendations related to their policies, and advisories about potential financial changes. In order to achieve this goal, our aim Students will provide a virtual reality experience where a remote user in a VR headset can peripherals can control a telepresence robot to",,2021,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8d39ec491fb0f9bbc23de9659d75cb95c5bc475b,https://www.semanticscholar.org/paper/8d39ec491fb0f9bbc23de9659d75cb95c5bc475b,Foreword,"In preparation for writing this foreword, I looked through old emails (really old emails) dating back to early 1998, when we were planning the “First Workshop on Embodied Conversational Characters.” In and amongst detailed menu planning for the workshop (I haven’t changed a bit since those days) are emails floating the idea of publishing a book with the best papers from the workshop. We were already starting to see a shift in the literature, away from “lifelike computer characters” (Microsoft’s Clippy was presented at a workshop with that name) and “believable computer characters” (characters whose behavior was believable, but that did not do anything for people), and we wanted the book to reflect that shift. We particularly wanted to highlight the fact that embodied conversational characters did not only talk but also listened. They were capable of understanding as well as generating language and non-verbal behavior, and they did so in the service of humans—they were agents, like travel agents or real estate agents. To that end, I sent an email to the chapter authors with the following tidbits. I wrote: Next, a note about terminology. After long debate, we’ve decided to call the book Embodied Conversational *Agents*, and not *Characters* (for marketing reasons, in part) so you might want to follow this terminology in your chapter. Finally, do make sure to focus on the *communicative* abilities of your systems, since this is what distinguishes this work—and this book—from previous volumes on believable characters, software agents and so forth. It’s amusing to read this today when we take for granted the agentive nature of our conversational systems. At this point, we assume that embodied conversation agents (ECAs) are designed primarily to accomplish work for people. We also take for granted that ECAs must both understand and talk. However, when the Embodied Conversational Agents book was conceived, both of those features were only newly possible. In turn, the title of the current volume highlights the most recent technological innovation, which is the ability of the systems not just to do work for humans but to interact socially with them in the process, in many cases using social interaction as a way to bootstrap task performance. It’s illuminating to look at two other debates that took place during this same period. The first concerns what kinds of data are used to create the most natural behaviors for an ECA. The second concerns whether it is ethical to build natural-acting ECAs. While there was beginning to be consensus in the late 1990s on the idea that conversational characters could do more than just look pretty, there were three schools of thought about the proper inspiration for the conversational behaviors of ECAs (as they were called). Some of the authors in the original volume worked with actors to understand what kinds of language and non-verbal behaviors were most evocative of normal human conversation. These researchers hewed to the belief that ECAs should behave in a somewhat exaggerated fashion, like actors on a stage, in order to seem natural to their human interlocutors. Other authors believed that, being native speakers of their own language, and acculturated to the customs of their own society, the simple intuitions of the researcher were sufficient to design human-like conversational behaviors. A third group believed that psychological and linguistic studies of human conversation were the only proper inspiration for the behaviors of ECAs. Today, while a few researchers still work with actors or rely on their own intuitions, the community of researchers in ECAs (and in today’s socially interactive agents) mostly rely on empirical psychological and linguistic studies of human behavior as their inspiration. Some of these researchers carry out their own studies, and some rely on extant literature, but in both cases they rely on normal everyday humans for inspiration rather than actors or computer scientists. The debate is interesting in the face of today’s focus on big data. In fact, the increasing reliance in the field of artificial intelligence (AI) on machine learning techniques to analyze human behavior has led to a parallel increase in ECA systems that rely on deep learning techniques applied to large corpora, often of naturally produced human conversational behavior, to generate appropriate verbal and non-verbal conversational behavior. In other words, AI has brought us closer to the human-inspired ECAs of the past by bringing a focus on corpora of natural behaviors. At the same time, however, it has taken us further away from those human-inspired ECAs of the past because the corpora are too large to be examined by the human eye. Another debate that evoked heated interchanges in the late 1990s, and that is useful to contemplate today, was whether we should even contemplate deploying ECAs as interfaces to computational systems in the first place. Many if not most of the authors in the 1998 volume believed that ECAs represented a more natural way of interacting with computational systems than a keyboard and a mouse. Their work was predicated on the assumption that interacting with a human-like agent was a more intuitive manner of accessing technical systems. To other computer scientists of the era, however, ECAs were downright evil. Perhaps most famously, human–computer interaction researcher Ben Schneiderman saved his strongest invectives for human-like agents and their designers. In 1995, he wrote Anthropomorphic terms and concepts have continually been rejected by consumers, yet some designers fail to learn the lesson. Talking cash registers and cars, SmartPhone, SmartHome, Postal Buddy, Intelligent Dishwasher, and variations have all come and gone. Even the recent variation of Personal Digital Assistant had to give way to the more service oriented name now used in the Apple Newton ads: MessagePad. We’ll leave it to the psychoanalysts to fathom why some designers persist in applying human attributes to their creations … But, possibly, just possibly, all this heated debate is excessive and agents will merely become the Pet Rock of the 1990s—everyone knows they’re just for fun (Ben Shneiderman 1995. ACM Interactions. 2, 1, 13–15). Today, fears about whether robots will steal jobs, and whether machine learning will make it hard to tell who is human and who is an AI, have once again launched debates on whether human-like agents are a good or bad influence on society. These debates have led to a stronger focus on transparency in AI, a concern with bias in data, and a much-needed conversation on the ethics of where ECAs should and should not be used. These contemporary debates, however, and contra Shneiderman’s predictions, show that anthropomorphic agents have stood the test of time. The topic has inspired passion and dedication in a whole new generation of researchers. To that end, here 20 years later is a two-volume follow-up from our 1998 Embodied Conversational Agents book, with more than 25 chapters, showing the depth, breadth, innovation, creativity, and, yes, effectiveness, of human-inspired agents. Justine Cassell",The Handbook on Socially Interactive Agents,2021,10.1145/3477322.3477323,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c78ff7ca730c85269065dd16ed6e8aea31e1634d,https://www.semanticscholar.org/paper/c78ff7ca730c85269065dd16ed6e8aea31e1634d,Agentless Insurance Model Based on Modern Artificial Intelligence,"Since past couple of years, Agents have been a crucial part of the financial sector, primarily focusing on the Auto Insurance sector, whose key responsibilities are centered around finding new prospective customers and maintaining a relationship with existing customers. But with every other company streamlining their business processes with the latest Technology, Insurance Industry is not too far behind. Currently, Insurance Industry has dived and started exploring the online space. Prospective customers can now get online insurance quotes, chat with an online robot and even purchase an Insurance policy online. Digitalization, Automation, and Streamlining are key buzzwords in every type of business sector. Given the above trends, Insurance Agents seem to be an unnecessary expense. In this paper, we propose an Artificial-Intelligence driven approach that eliminates the need for a human Insurance Agent that will ultimately reduce the overall cost for the end customer. As part of our contribution to the above problem statement, we have proposed a Software Application where four Statistical Models are deployed. These Models are tasked with determining prospective customers who will likely buy an Insurance Policy, identifying customers who are likely to cancel a policy so that we can provide them with something better, identifying customers submitting fraudulent insurance claims and finally a Recommendation System Model to recommend updates to current policy to existing policy of Customers. In our Experimentation Results, we identified a cluster of customers who were most likely to buy a product using an Unsupervised Statistical Machine Learning model.",2021 IEEE 22nd International Conference on Information Reuse and Integration for Data Science (IRI),2021,10.1109/IRI51335.2021.00013,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8c12838c4659a9190ae7b1021515216399be1ef2,https://www.semanticscholar.org/paper/8c12838c4659a9190ae7b1021515216399be1ef2,Scaling Intelligence,"T A D 5 : 2 Intelligence without action is inert. As the solicited contributions to this volume demonstrate, actionable intelligence relies on a common ground from which architecture and allied disciplines can leverage depths and breadths of knowledge to mobilize new technologies. The Op/Positions essays examine preexisting local knowledge in historical places, enhance discovery through systems-based workflows, and foster the transformational shift from invisible smartness to holistic, design trade-offs that produce more humane and cooperative cities. As Jyoti Hosagrahar notes, place-intelligence provides current generations with a scalable and reflective framework that values the past, promotes deeper foundations, and connects resilient community design and well-being to informed decision-making. Similarly, Azam Khan posits a systems-based approach for leveraging existing knowledge to solve increasingly complex problems holistically. The emergent metaheuristic tools expand architectural design ability, enhance discovery, and yield more energy-efficient and less wasteful buildings. Norbert Streitz advocates for resetting priorities at an urban scale and generating principles that simultaneously privilege the individual and the collective. The resulting types of affordances and ethical alignments could balance data harvesting with people’s need for interactive, communicative, and cooperative spaces and places. The Research Methodology contributions critically examine a site’s latent potential and propose challenging new ways for testing and improving the lived condition at all scales. Whether at the intimate scale of one human-robot interaction or applied to industry-level protocols or full-scale testing scenarios, real-world applied research design necessitates collecting and analyzing large data sets. Jim Tørresen examines predictive intelligent system design, comprising ethical sensor data collection, robot interaction, and human-centric artificial intelligence to anticipate and respond to elderly care needs. Integrating artificial intelligence and problem-solving best practices can interactively adapt to a user’s needs and draw upon years of industry-based construction knowledge. Lukas Kirner, Elisa Lublasser, and Sigrid Brell-Cokcan developed enhanced methods for elevating existing construction industry processes through interdisciplinary collaboration, robot-assisted interaction, laboratory experimentation, factoryto-field investigation, and full-scale testing. The jump from laboratory experiments to full-scale prototyping requires the refinement of previous data exchanges and information flows to produce generalizable results. Maintaining quantitative and qualitative data research design, controlled trials, and procedural rigor requires close monitoring and comparison of real-time data collections and digital simulations. In their Details+ contribution, Jonathan Heppner and Thomas Robinson deployed intelligent testing on an innovative post-tensioned, gravity-resistant, and lateral force-resistant rocking wall system at full scale and detail level. The lab-tested results generated valuable insights into damage-resistant construction methods, informed broader building practices, and demonstrated that their previously unproven assembly could prevent massive failure and save lives. Increasing the use of enhanced digital/computational methods brings renewed attention to gaining greater control over the software and tools used to generate and validate design decisions at all scales. Re/Views addresses these issues, examines interoperable software platforms, compares gaming engines, integrates sensors, and surveys current, emerging, and projected use of autonomous robots across the AEC industry. Karen Kensek presents strategies for improving workflows and overcoming software limitations through customizable add-in solutions for existing Building Information Modeling processes. Enhanced parametric interoperability and data functionality, streamlined procedures, and verified code-compliance bolster deliberative intelligence. Christopher Morse compares game engines that combine visualization, communication, and design in robust, adaptable, flexible, real-time, and interactive environments. Immersive, customizable, connective, and cloud-based integration inform architectural research and professional practice. Peter Kerr investigates scalable interactions with technology, examining affordances and benefits of sensor nodes connected via intelligent Building Management Systems (iBMS). Alvise Simondetti, Nicholas Bachand, Aifric Delahunty, James Griffith, and Julius Sustarevas examine the unfolding paradigm shift toward autonomous robotics, artificial intelligence, and machine learning as architecture moves beyond task-specific operations to inform scalable and sustainable design that augment and complement human capabilities. As shown by these authors, in its performative function, and when viewed through the prismatic lenses of technology, architecture, and design, scaling intelligence successfully narrows the gap between empirical observation, applied research, and professional practice. Scaling Intelligence",Technology|Architecture + Design,2021,10.1080/24751448.2021.1967048,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7e85a1462d85730543ef206a923fae64d9236b4f,https://www.semanticscholar.org/paper/7e85a1462d85730543ef206a923fae64d9236b4f,Digital Twin: Universal User Interface for Online Management of the Manufacturing System,"
 Industry 4.0 concept enables connecting a multitude of equipment to computer simulations through IoT and virtual commissioning, but using conventional interfaces for each separate piece of equipment for control and maintenance of Digital Twins is not always an optimal solution. Industrial Digital Twins software toolkits usually consist of simulation or offline programming tools. It can even connect real machines and controllers and sensors to feed a simulation with actual production data and later analyze it. Moreover, Virtual Reality (VR) and Augmented Reality (AR) are used in different ways for monitoring and design purposes. However, there are many software tools for the simulation and re-programming of robots on the market already, but those are a limited number of software that combine all these features, and all of those send data only in one way, not allowing to re-program machines from the simulations. The related research aims to build a modular framework for designing and deploying Digital Twins of industrial equipment (i.e., robots, manufacturing lines), focusing on online connectivity for monitoring and control. A developed use-case solution enables one to operate the equipment in VR/AR/Personal Computer (PC) and mobile interfaces from any point globally while receiving real-time feedback and state information of the machinery equipment. Gamified multi-platform interfaces allow for more intuitive interactions with Digital Twins, providing a real-scale model of the real device, augmented by spatial UIs, actuated physical elements, and gesture tracking.
 The introduced solution can control and simulate any aspect of the production line without limitation of brand or type of the machine and being managed and self-learning independently by exploiting Machine Learning algorithms. Moreover, various interfaces such as PC, mobile, VR, and AR give an unlimited number of options for interactions with your manufacturing shop floor both offline and online. Furthermore, when it comes to manufacturing floor data monitoring, all gathered data is being used for statistical analysis, and in a later phase, predictive maintenance functions are enabled based on it.
 However, the research scope is broader; this particular research paper introduces a use-case interface on a mobile platform, monitoring and controlling the production unit of three various industrial- and three various mobile robots, partially supported by data monitoring sensors. The solution is developed using the game engine Unity3D, Robot Operation System (ROS), and MQTT for connectivity. Thus, developed is a universal modular Digital Twin all-in-one software platform for users and operators, enabling full control over the manufacturing system unit.",Volume 2B: Advanced Manufacturing,2021,10.1115/imece2021-69092,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a64ec7722ce666491134c843968f0d89d7b713f9,https://www.semanticscholar.org/paper/a64ec7722ce666491134c843968f0d89d7b713f9,A Systematic Study of Wearable Multi-Modal Capacitive Textile Patches,"We present a study on wearable capacitive patches with engineered dielectrics to be used as a strain sensor and a piezoelectric sensor with multi-modal sensing capabilities. The patches consist of a parallel plate capacitive structure with highly (100%) stretchable textile electrodes and silicone or PDMS elastomers. The gauge factor of the capacitive strain sensor is enhanced two-fold with the inclusion of high- ${k}$ Barium Titanate (BTO) nanoparticles dispersed in the silicone dielectric layer without sacrificing their excellent linearity or superb durability easily exceeding 2000 cycles. In addition to enhanced dielectric response, inclusion of BTO as well as other additives, namely PZT and PVDF-TrFE, in PDMS also induces piezoelectricity in the sensors at different weight ratios systematically studied by bending and vibrational tests. Corona poling of the piezo dielectrics improves their voltage output by at least two folds. Besides the piezoelectricity, tests on unpoled samples also uncovered evidence for triboelectricity in the nanocomposites. The proposed textile patches can simultaneously detect multiple stimuli such as strain, pressure, temperature, bending, vibration, and acoustic feedback. Deployed in appropriate locations and geometries, the patches can capture critical information on the type, strength, and duration between episodic movements and environmental factors, as demonstrated via a shoe-insert, elbow tracker, respiratory and heart-rate monitors. Accordingly, capacitive textile patches offer unique capabilities for multi-modal sensing in biomedical and robotic applications as well as overall system integration that are advantageous for sensor fusion and machine learning.",IEEE Sensors Journal,2021,10.1109/JSEN.2021.3059224,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
779f047d6340bec1048d8f0679bc5543ce148ad8,https://www.semanticscholar.org/paper/779f047d6340bec1048d8f0679bc5543ce148ad8,Autonomics: In search of a foundation for next-generation autonomous systems,"Significance Autonomous systems are replacing humans in a variety of tasks, and in the years to come, such systems will become central and crucial to human life. They will include vehicles of all kinds, medical and industrial robots, agricultural and manufacturing facilities, traffic management systems, and much more. While many organizations strive to develop the next generation of trustworthy, cost-effective autonomous systems, a major gap exists between the challenges in developing these and the state of the art. There is a crucial need for a common scientific and engineering foundation for developing these systems, which we term “autonomics.” We believe that such a foundation will dramatically accelerate the deployment and acceptance of high-quality autonomous systems, for the benefit of human society. The potential benefits of autonomous systems are obvious. However, there are still major issues to be dealt with before developing such systems becomes a commonplace engineering practice, with accepted and trustworthy deliverables. We argue that a solid, evolving, publicly available, community-controlled foundation for developing next-generation autonomous systems is a must, and term the desired foundation “autonomics.” We focus on three main challenges: 1) how to specify autonomous system behavior in the face of unpredictability; 2) how to carry out faithful analysis of system behavior with respect to rich environments that include humans, physical artifacts, and other systems; and 3) how to build such systems by combining executable modeling techniques from software engineering with artificial intelligence and machine learning.",Proceedings of the National Academy of Sciences,2019,10.1073/pnas.2003162117,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2b001d6e62fbf659cb0bfc743cfcd73d29918fd9,https://www.semanticscholar.org/paper/2b001d6e62fbf659cb0bfc743cfcd73d29918fd9,Geo-Spatiotemporal Intelligence for Smart Agricultural and Environmental Eco-Cyber-Physical Systems,,,2020,10.1007/978-3-030-52067-0_21,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
49ad4917e194642a76b507776940bf8091f2b70d,https://www.semanticscholar.org/paper/49ad4917e194642a76b507776940bf8091f2b70d,How to translate artificial intelligence? Myths and justifications in public discourse,"Automated technologies populating today’s online world rely on social expectations about how “smart” they appear to be. Algorithmic processing, as well as bias and missteps in the course of their development, all come to shape a cultural realm that in turn determines what they come to be about. It is our contention that a robust analytical frame could be derived from culturally driven Science and Technology Studies while focusing on Callon’s concept of translation. Excitement and apprehensions must find a specific language to move past a state of latency. Translations are thus contextual and highly performative, transforming justifications into legitimate claims, translators into discursive entrepreneurs, and power relations into new forms of governance and governmentality. In this piece, we discuss three cases in which artificial intelligence was deciphered to the public: (i) the Montreal Declaration for a Responsible Development of Artificial Intelligence, held as a prime example of how stakeholders manage to establish the terms of the debate on ethical artificial intelligence while avoiding substantive commitment; (ii) Mark Zuckerberg’s 2018 congressional hearing, where he construed machine learning as the solution to the many problems the platform might encounter; and (iii) the normative renegotiations surrounding the gradual introduction of “killer robots” in military engagements. Of interest are not only the rational arguments put forward, but also the rhetorical maneuvers deployed. Through the examination of the ramifications of these translations, we intend to show how they are constructed in face of and in relation to forms of criticisms, thus revealing the highly cybernetic deployment of artificial intelligence technologies.",,2020,10.1177/2053951720919968,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
0f301e0b04f87733895e54ea0e5552b36e7c9b44,https://www.semanticscholar.org/paper/0f301e0b04f87733895e54ea0e5552b36e7c9b44,Multiplanar 3D fluoroscopy redefines tool–lesion relationship during robotic‐assisted bronchoscopy,"Guided bronchoscopy platforms have rapidly evolved over the past two decades to allow high-yield tissue acquisition of suspicious peripheral pulmonary nodules. Robotic-assisted bronchoscopy platforms aim to overcome limitations of conventional guided bronchoscopy by allowing for precision access to smaller and more peripheral lesions. The Ion Robotic-Assisted Endoluminal Platform (Intuitive Surgical, Sunnyvale, CA, USA) features a console-controlled catheter with an outer diameter of 3.5 mm and a 2.0-mm working channel. Localization within the airway tree is based on a shape-sensing fibre embedded along the catheter’s length and navigation is based on a computed tomography (CT)-generated virtual road map. Following navigation, adjunctive imaging techniques are used to confirm approximation to the lesion. Conventional modalities include radial probe endobronchial ultrasound (RP-EBUS) and 2D fluoroscopy. Cios-Spin (Siemens Healthineers, Forchheim, Germany) is a mobile 2D–3D C-arm that produces intraoperative 1952 × 1952 pixel images over an area of 295 × 295 mm. Multiplanar 3D imaging is obtained by performing an orbital scan or a spin. Image reconstruction in the axial, coronal and sagittal projections allows intraoperative determination of tool–lesion relationship within lung tissue. Similar to multiplanar fluoroscopy, cone-beam CT (CBCT) provides intraoperative tool–lesion relationship; however, unlike multiplanar fluoroscopy, CBCT platforms are bulky and stationary, require a dedicated operational space and allow less flexibility in utilization across different locations and medical specialties. Moreover, compared with multiplanar fluoroscopy, CBCT entails higher capital costs. Augmented fluoroscopy is achieved by applying machine-learning algorithms supplemented by preoperative CT to 2D fluoroscopy as a navigation tool. Augmented fluoroscopy, however, does not provide detailed CT-like images. Early studies of robotic-assisted bronchoscopy report navigation feasibility of 88–96% and diagnostic yield of 69–79%. A case series of 3D fluoroscopy combined with electromagnetic navigation bronchoscopy indicated excellent intraoperative visualization of tool–lesion relationship. The combination of robotic-assisted bronchoscopy with intraoperative confirmation of tool–lesion relationship by multiplanar 3D fluoroscopy for the sampling of peripheral lung lesions has yet to be reported in humans. This study’s objective was to evaluate the feasibility of a novel combination of robotic-assisted bronchoscopy guided by intraoperative, multiplanar 3D fluoroscopy in the sampling of peripheral pulmonary lesions. We performed a retrospective, single-centre review of all patients who underwent 3D fluoroscopy-guided robotic-assisted bronchoscopy for the sampling of peripheral lung lesions in our institution between 1 March and 30 March 2020. The study protocol was approved by the Memorial Sloan Kettering institutional review board (#20-102). An informed consent waiver was provided on the basis of a retrospective patient record review. All study participants’ privacy was protected, and data confidentiality was maintained. Robotic-assisted bronchoscopy was performed with the Ion platform under general anaesthesia. Following navigation to the target, RP-EBUS (UM-S20-17S or UMS20-20R-3; Olympus Corp, Tokyo, Japan) was advanced and sonographic target visualization was classified as concentric, eccentric or no view. Once position was confirmed, a 19-, 21or 23-G Flexision cytology needle (Intuitive Surgical) was deployed. Needle size was determined by the operator at the time of the procedure. A breath-hold was then initiated, and intraoperative 3D fluoroscopy spin was performed with the Cios-Spin mobile 3D C-arm, utilizing the standard imaging protocol and the soft tissue algorithm. If deemed necessary, the robotic catheter was redirected to optimize tool– lesion relationship and repeat RP-EBUS and/or 3D fluoroscopy were employed for further confirmation. A maximum of three 3D fluoroscopy spins were allowed per patient to limit radiation exposure. Once a satisfactory position was achieved, the lesion was sampled by needle. Forceps biopsies were obtained at the discretion of the operator. All 3D fluoroscopy images were retrospectively reviewed, and needle–lesion relationship was determined by a consensus between four expert interventional pulmonologists. Tool–lesion relationship was classified as centre hit (tool in the centre of the lesion), off-centre hit (tool in the periphery of the lesion or traversing it with the tip distal to the lesion) or miss (tool outside the lesion). Study data were managed using Research Electronic Data Capture (REDCap) tools. Over the study period, 10 lesions were sampled in five female and three male patients. The median age was 64 years (range: 36–83). Data related to lesion, procedure and sampling results are summarized in Table 1. The median lesion size was 1.45 cm (range: 0.73–7.74) and 9 of 10 lesions were radiographically solid. Navigation to target was successful in all cases. The RP-EBUS view was concentric in six and eccentric in four lesions. A median of 1.5 3D fluoroscopy spins (range: 1–3) were performed per patient. As illustrated Received 16 August 2020; invited to revise 20 September 2020; revised 22 September 2020; accepted 12 October 2020 Associate Editor: Pyng Lee; Senior Editor: Phan Nguyen",Respirology,2020,10.1111/resp.13966,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
068060ed6023fd7dd226c3c7c97a5f59d1885399,https://www.semanticscholar.org/paper/068060ed6023fd7dd226c3c7c97a5f59d1885399,Robust Vision-Based Pose Correction for a Robotic Manipulator Using Active Markers,,ISER,2020,10.1007/978-3-030-71151-1_47,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ea14cbd5dd4e72ee13d429c1f80ac76f232c363b,https://www.semanticscholar.org/paper/ea14cbd5dd4e72ee13d429c1f80ac76f232c363b,BIOMIMETIC CONTROL OF SPACEBORNE MANIPULATOR FOR DEBRIS REMOVAL AND ON-ORBIT SERVICING,"A traditional approach to space debris mitigation is the removal and de-orbiting of debris. We propose a novel approach which involves in-situ resource utilization of salvaged debris for re-use as a development of on-orbit services. This could be regarded as a more sustainable approach to space debris control. Only robotic manipulation is flexible enough to deal with both large and small debris unlike harpoons and nets which generate complex uncontrollable dynamic interactions between the robotic free-flyer, the target and the flexible umbilical connecting the two. This favors space debris mitigation through the deployment of free-flyer spacecraft mounted with dexterous manipulators which provide controlled interaction with the target [1]. Robot manipulators have been the workhorse of industrial applications for a range of tasks where precision positioning is required including machining, welding, sanding, spraying and assembly. Debris mitigation will require robust robotic grasping. Currently, most approaches to tactility in robotic grasping rely on either significant processing resources or soft robotics. There is little doubt that robust and adaptable robotic grasping would be a boon in both orbital debris removal and on-orbit servicing [2]. We propose here a novel bio-inspired error-learning approach to manipulator control that specifically addresses the requirement for adaptability and compliance to a range of dynamic tasks under dynamic environmental conditions. We propose to solve this problem by accommodating a different payload inputs and a force control model in the error learning algorithm. To avoid unreliable predictions for robustness, the error learning models of the robotic manipulator were trained on data accounting for possible perturbations and different payloads. We have introduced a task specific model that is able to learn from its errors (make error predictions) under different payloads and varying environmental dynamics, to accommodate space debris removal of varying sizes. This is entirely different to model predictive control. For now, we present here, a twolayer approach to grasping: (i) hybrid force/impedance control through feedback, which is the traditional approach but delays in the feedback cycle can generate instabilities; (ii) the addition of a feedforward predictive capability to partially circumvents this problem by emulating the function of the human cerebellum (effectively, a Kalman filter-based neural network). Essentially, we have developed appropriate neural net models for varying sizes of payloads and dynamics – using models of the 7 degrees-of-freedom Barrett Arm.",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e5f7951406318a3bbb76f2ae5358469e49d3a6f7,https://www.semanticscholar.org/paper/e5f7951406318a3bbb76f2ae5358469e49d3a6f7,"The Industry 4.0 Talent Pipeline: A Generational Overview of the Professional Competencies, Motivational Factors, and Behavioral Styles of the Workforce","This research seeks to identify emerging trends, pinpoint challenges and gain data-driven insights into the forces shaping the technical talent pipeline of Industry 4.0 in the United States, specifically Southeast Michigan which has one of the largest concentrations of engineers and technicians in the country. The rapid advance of digital technology has revolutionized engineering and industry. It is dramatically shaping the technical talent landscape. Simultaneously, major cultural changes are being forced by generational transition and leadership succession. To prosper in the Industry 4.0 ecosystem, individuals and organizations will be required to develop 21st century skill sets. The talent pipeline is failing to provide sufficient quantities of workers and calls for stepping up Industry 4.0 reskilling have become ever more urgent. Five themes have emerged: 1. The Workforce Must Embrace Frequent and Constant Change. 2. Teams Must be Flexible, Adaptive and Collaborative. 3. Companies Must Create Cultures of Inclusion and Transparency 4. Workers Must Become Life-long Learners & Dynamic Thinkers 5. The Need for Accelerated Education Workforce Development Reform An empirical investigation, focused on Southeastern Michigan was conducted with the support of local industry, educational institutions and government agencies. Three key segments of upcoming generations, currently advancing in the talent pipeline, were investigated: 1) future technical leaders, 2) future engineers and 3) future tradespeople. Based on responses to a series of questions using the TTI TriMetrix® DNA assessment suite a data-driven, validated assessment instrument, this research presents an overview of the development of 25 professional competencies that contribute to superior performance. Individual motives and behavioral styles are also explored. These findings provide some valuable insights and direction into what educators, industry and policy makers should address to upgrade the technical talent pipeline in the age of Industry 4.0 in order to protect and ensure the Unites States global leadership position. Overview of the Industry 4.0 Talent Landscape The rapid advance of digital technology is revolutionizing engineering and industry. The term “Industry 4.0” is now commonly used in reference to this revolution. Industry 4.0 is a result of the convergence of digital, biological and physical technologies. Industry 4.0 dynamics are dramatically shaping the technical talent landscape. Simultaneously, cultural changes are being driven by generational transition. As the Baby Boomer generation (1946-64) exits the workforce, the Millennial generation (1981-96) and the Generation Z’ers (1997-2012) rise up as the majority of the technical workforce. The combination of technological and cultural change has made the transformation to Industry 4.0 difficult to manage, especially for legacy companies and small and medium sized enterprises. Socio-cultural transformation has become a challenge confronting industry. Knowledge and experience are exiting the workforce in mass through the retirement of the Baby Boomer generation. A shortage of qualified talent has emerged, in part due to the smaller numbers of Generation X’ers (1965-80), who now make up 51% of the management and leadership positions. With an average of 20 years of workplace experience, Generation X’ers are assuming the top executive roles [1]. Socio-cultural transformation is complicated due to each generation’s radically different views on work-life balance. Millennials make up the largest segment of the working population. Not far behind, the Gen Zers are now entering the workforce in entry level positions. Both generations have their own unique attributes. For example, a common attribute of Gen Z is that they tend to be very inclusive in nature and willing to rally around causes. The technologies of Industry 4.0 coupled with generational transition are driving the need to step up workforce development efforts. Industry 4.0 skills have become critical components of labor markets [2]. Developing and retaining an Industry 4.0-ready workforce demands that industry and educators go beyond traditional reskilling and upskilling initiatives. Organizations must focus on career strategies, talent mobility and re-engineering ecosystems to drive organizational reinvention and new business models. Objectives of this Research This research seeks to build on the work of Pistrui and Kleinke et al. [3] [4] and Petrick, and McCreary [5] to identify emerging trends, pinpoint challenges and gain data-driven insights into the forces shaping the technical talent pipeline and Industry 4.0 in the United States, and in particular Southeast Michigan. Research objectives include: 1 Building on previous and ongoing research findings to provide a deeper and more comprehensive understanding of the talent pipeline in United States. 2 Evaluating three key segments of the talent pipeline: 1) next-generation leaders, 2) undergraduate engineering students and 3) skilled trade apprentices. 3 Defining the fundamental DNA of the talent pipeline in terms of the professional competencies, motivational factors and behavioral styles of the three groups. 4 Producing data-driven insights that industry and education collaborators can use to modify and strengthen the talent pipeline in United States. 5 Developing new knowledge and strategies to attract, develop and retain top Industry 4.0 talent in the United States. Emerging Workforce Themes and the Talent Horizon To prosper in the Industry 4.0 ecosystem, individuals and organizations will be required to develop 21st century skill sets. The talent pipeline is failing to provide sufficient quantities of workers and calls for stepping up Industry 4.0 reskilling have become ever more urgent [2]. The factories of the Industry 4.0 digital age are very different from the legacy operations of the automation-age of Industry 3.0. In 2015 alone, nearly 100,000 robots were deployed in automotive factories and a further 65,000 were installed in electronics factories as automation continues to reshape the size of the labor pool [6]. With the advent of Industry 4.0, each robot installation is also a digital installation as the robots are equipped with technologies such as machine learning, cloud computing and big data. The work of the labor pool must evolve. A recent study conducted by the Ralph C. Wilson, Jr. Foundation reports that in Southeast Michigan 30% of the middle skilled labor pool will be displaced by automation by 2030 [7]. There will also continue to be a shortage of workers with the skills industry is seeking. Many people will need to transition from “traditional careers” where they have trained to do specific tasks, (i.e. mechanical engineer) to “multitrack careers” where they will have multiple simultaneous jobs such as engineer, data analyst, and network administrator [4] [8]. To successfully navigate the Industry 4.0 environment (and beyond), organizations will need to integrate four (and soon 5) different generations into their workforce. This will be no easy task given the generational differences coupled with the shortage of qualified talent. Table 1 presents an overview of the emerging themes shaping the workforce environment. Four central themes are having dramatic impact on the Industry 4.0 work environment: Table 1 Emerging Workforce Themes 1. The Workforce Must Embrace Frequent and Constant Change Industry 4.0 is expected to significantly increase the pace of change. Companies need to be aware of the implications of disruption to their workforce [2] [3] [8]. 2. Teams Must be Flexible, Adaptive and Collaborative Team agility, an entrepreneurial mindset and the ability to persist through failure are fundamental to creating and sustaining networks of interrelated teams [4] [5] [23]. 3. Companies Must Create Cultures of Inclusion and Transparency This requires breaking down traditional hierarchies, implementing agile methodologies and embracing the changing nature of work tasks [11] [8] [9]. 4. Workers Must Become Life-long Learners & Dynamic Thinkers Dynamic thinking requires empathy, collaboration, experimentalism and a focus on solving problems and creating value for other humans [2] [3] [8]. Sources: Schwab, 2016; Pistrui and Kleinke, 2019; Arena, 2018; Pistrui and Kleinke, 2018; Brachman, 2018. Insights into 21st Century Industry 4.0 Skills, Mindsets and Cultures of Performance Research conducted by Pistrui et al. (2018; 2019) provides insights into three categories and specific types of skills that industry and educators view as critical for the 21st Century [3] [4]. Their research identified three categories of skills that are important to develop and employ in an Industry 4.0 environment including discerning skills, people skills and purposeful skills (see Table 2). These findings parallel the work of Petrick and McCreary (2019), Bawany (2019), Schwab (2019), and Arena (2018) who all identified similar skill sets, common trends and empirical findings [5] [9] [10] [11]. Table 2 21st Century Industry 4.0 Categories and Skill Types [3] [4] [5] [9] [10] [11] Sources: Pistrui and Kleinke, 2018, 2019 Petrick and McCreary (2019), Bawany (2019), Schwab (2019), Arena (2018). The need to develop discerning skills is a result of the disruption and uncertainties associated with Industry 4.0. This applies to both companies and educational institutions. People need to be able to identify patterns and make new connections in ways never imagined before. Moving forward, they must envision and create new products, efficient services and better user experiences. The research of Petrick and McCreary (2019) included 404 manufacturing companies directly involved in creating and implementing smart technologies. The findings were similar to the work of Pistrui et al., reporting that creativity and innovation, the ability to be forward looking, and having an improvement mindset are all re",,2020,10.18260/1-2--35341,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
981d1f317673c1ae3c0942e73207c85909e6490c,https://www.semanticscholar.org/paper/981d1f317673c1ae3c0942e73207c85909e6490c,Bringing RPA to the next level using AI,"Introduction: Artificial Perspicacity (AI) is transmuting the digital landscape in every field it reaches. The Robotic Process Automation (RPA) revolution goes hand in hand with the advances that Artificial Astuteness is making to transform industries ecumenical. Ultimately, Artificial Perspicacity augments RPA and its implements to surpass prospects. With this already established, it’s valid to verbally express that the world is gearing up for the robotic revolution. Albeit we are already experiencing many of its applications, there’s still plenty of potentials to tap into. What is RPA precisely? RPA or Robotic Process Automation is software technology, as dictated by business logic and structured inputs, aimed to program applications or robots to perform rudimental tasks, just like humans would, in an automated setting. RPA bots can mimic virtually any human action, emulating and integrating actions with digital systems to execute a business process. Robotic Process Automation (RPA) enables organizations to engender virtual workforces that increase process efficiency, truncate errors, and cut operational costs. However, even when enterprises implement RPA, many are challenged with scaling it across the organization and identifying all the potential business processes that can and should be automated for maximum ROI. This presentation will discuss how companies overcome these challenges utilizing AI powered process revelation. Automation has been transmuting the nature of work for over a decennium. It has superseded labour intensive tasks, undertaken perpetual ones, escalated the haste of engenderment and engendered incipient streams of work. Most organizations are already on RPA journey, which has resulted in productivity amendment, cost savings, process time amelioration with perpetual and rule-predicated processes. The commencement of the peregrination involves POCs, pilots, and initial automations. The next frontier is about scaling the  
deployment and adoption of cognitive technologies such as AI, analytics, machine learning that emulate human comportment. The transition involves transformation from running RPA on a few processes to scaling up RPA across the enterprise. This session would deal with strategies & challenges in enterprise wide adoption of Automation - crossing over from RPA to RPA+Cognitive , identifying the right operating model & establishing governance, Leadership & aptitude development , orchestrating stakeholders and board level endorsement. According to Investopedia, Robotic Process Automation is the “software that can be facilely programmed to do rudimentary tasks across applications just as human workers do.” Consequentiality of Robotic Process Automation According to insights developed by McKinsey&Company, RPA offers the potential ROI of 30-200% in the first year of avail alone. This staggering figure is met with the verbal expression made by Leslie Willcocks that “RPA takes the robot out of the human.” Companies and employees are taking notice, which is why everyone is so agog to invest in robotics and its emerging technologies. In essence, RPA is consequential because it is transforming the way businesses operate by availing automate perpetual tasks that are a component of a quotidian routine with a higher degree of efficiency than if performed by a human. Akin to cognitive automation, chatbots, and artificial perspicacity, RPA performs significantly more expeditious and more cost-efficaciously than human resources. Many fear that RPA implements and technologies can be perilous as they take jobs out of human hands. But you shouldn’t authentically worry as there is more to gain than lose when it comes to RPA. To put it into further context, implementing Robotic Process Automation into your workplace can avail with tasks such as monitoring customer activity to discover opportunities to upsell, monitoring client comportment to identify areas of opportunity, truncating cycle times in perpetual tasks to gain competitive advantage, capturing and analyzing bulks of information to provide more expeditious replication times, and more. In a wide range of industries that span healthcare, indemnification, finance, and more, there are many cumbersomely hefty-hitters that have already adopted RPA implements into their processes, including Wal-Mart, Ernst & Puerile, Walgreens, American Express, and more. These early-adopters have benefitted from minimizing staffing costs and human errors with the implementation of RPA technology.",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
24f263d5b6b689bf45d89a677d2b842831680360,https://www.semanticscholar.org/paper/24f263d5b6b689bf45d89a677d2b842831680360,Designing and Building a 1st stage dataset for embodied music-making (musicking).,"The aim of this research project was to design and develop a dataset that captured embodied music-making (hereafter musicking (Small 1989)) for use in human-computer interaction between AI and human musicians. The proposed solution to this challenge was to capture embodied musicking through data harvesting of physical and sonic elements from a musician embodied in the flow of musicking. This report describes the small-scale, first stage proof-ofconcept design, development and deployment of such a system. The hard question here was to develop a dataset that could inform perception of music-AI so that it could co-create within the realtime flow of musicking with other machines and/or human musicians. The results highlight the importance that any modes of capture other than real-world musicking, would be a setup for failure. Problem definition To date my experiments with Ai and musician interaction have dealt with symbolic representations of musicking. These were working successfully with positive responses from the human musicians regarding the sense of co-operation between them and the AI inside the flow of musicking. The next stage was to develop a compositional system that incorporated traditional models of machine learning i.e. statistical predication and correlations between multi-dimensional features. In this case this would be to try to identify, build and develop a trained ML AI from features of musicking inside the embodied flow of musicking. Embodied musicking – a definition The working definition of embodied musicking adopts and extends the following definitions. 1. Embodied interaction (Leman & Nils et al) Nils et al (2009) define the embodied interaction in music performance as: ‘In music performance the embodied interaction with the music implies the corporeal attunement of the musician to the sonic event that results from the performance. The embodied experience of participating in the musical environment in a direct and engaged way is based on the direct perception of the musical environment and on a skill-based coping with the challenges (affordances and constraints) that arise from the complex interaction within this musical environment. It becomes an optimal embodied experience (flow) when the musician is completely immersed in the created musical reality (presence) and enjoys himself through the playfulness of the performance. Therefore, direct perception of the musical environment, skill-based playing and flow experience can be conceived of as the basic components of embodied interaction and communication pattern.’ 2. Musicking (Small) The composer and music theorist Christopher Small describes the embodiment of music as musicking. He defines it as: ‘to music is to take part’, which can happen ‘in any capacity, in a musical performance, whether by performing, by listening, by rehearsing or practicing, by providing material for performance (what we call composing)’ (Small 1998:9). 3. Flow theory (Csíkszentmihályi) Csíkszentmihályi’s Flow Theory (1975) supports Small’s argument that the acts of doing in music are to be considered an immersive and embodied experience. Csíkszentmihályi defined flow as ‘the state in which people are so involved in an activity that nothing else seems to matter’ (Csíkszentmihályi 1975: 4). Csíkszentmihályi also discusses the shared nature of flow when groups become absorbed in an activity such as music performance. He points out that ‘they shift into a common mode of experience [...] This mode is characterized by a narrowing of the focus of awareness, so that irrelevant perceptions and thoughts are filtered out, by loss of self-consciousness, by a responsiveness to clear goals and unambiguous feedback, and by a sense of control over the environment.’ 4. The poetics of embodied musicking In short, through the act of musicking, musicians become embodied in the music through a sense of incorporation within their environment (the soundworld), shared effort, and a loss of awareness of their day-to-day wakefulness and corporeal self-consciousness. This was a driving philosophy through the design, development and deployment of the embodied musicking dataset. Furthermore, when musicians make music it is not a process of outputting sound into the world, but an embodied experience of becoming the sound they create in the flow of musicmaking. We are probably familiar with the notion of musicians “speaking through their instrument” i.e. they embody their musical instrument to a point where there is no felt separation between their understanding of them-selves and the extension of their self through this instrument. But, there are further embodiment processes that happen in the flow. Firstly, they reach out and bring into their bodily sense of experience the music that is produced. And secondly, when they make music with other musicians, they are equally reaching out to feel the presence of these others as sound, and to bring that back into their bodily experience. This embodiment process is a dance of sorts: to touch, to feel, to sense, to work with, to play with, to hide and seek and flirt and subvert, with others through the flow. So my research question is rather fundamental ... What does artificial intelligence need to do in order to stimulate this dance in the flow?' Initial design The initial design concentrated on 2 aspects of embodied musicking. Firstly, that physical gesture and sound actuation are linked not by merely physical movement instantiating air vibration through an instrument, but the reverse: that the being-as-sound (the musician) is in fact instigating movement from the body like a corporeal puppet, in order for it to obey and make the sound that is called for. The dataset’s initial design needed to capture the essence of the physical gesture (and thereby the instantiated movement for sound production) and the properties of that sound. To this end the design was Sonic gesture = complex physical gesture ----------------------Time The primary features of the dataset would capture the complex of bodily movement of the musician with the physical properties of the sound. Defining the complex of bodily movements was an important aspect to this design, as I understood through personal experience and in conversation with other musicians, that, say, when an arm moves a cello bow, it is the whole body that is involved in the embodied music gesture, not just the isolated limb, or joint. Therefore, the inter-relationship between the whole complex of bodily movements would indicate embodied musicking. The inference I made through this process was that one isolated feature of this dataset might be imbued with a sense of embodied musicking, and this alone might be able to synthesise the perception of embodied musicking for the AI. By extension and hypothetical proposition, that the essence of this dataset – imbued with creative and embodied musicking could be used in a variety of creative applications not just music. The design for this first stage dataset was: Sequential ID; Body Part (Head, Body, Left hand, right hand); Audio Analysis (FFT fundamental, amplitude) Figure 1. Research environment featuring human musician (author) on table-top guitar, with robot moving on the floor. In the foreground the Kinect image of the tracked musician; background shows the telemetry coming from the robot AI. Capture mechanism To capture the primary features, I designed a simple hardware system using a Kinect and a microphone. A Max patch was created to capture the audio stream from the microphone and immediately split into fundamental frequency and amplitude using the fzero~ object. The Kinect was streaming selected data of Head, Body, Left hand, right hand from NIMate 2, through OSCulator as a filter, into a Max MSP patch which was collating all the data streams into a single list using the coll object. The baud/ collection rate was not set to a constant instead this was triggered by physical movement. I am unsure whether this was the correct decision, but at the time I wanted to prioritise instantiated sequences of embodied musicking events, and not accurate feature extraction through time. In future work, alternatives need to be explored. I was to be the initial – and only – participant for this first stage as I could control (unfortunately bias) the interaction with the dataset harvesting and experiential training (see below). I designed a percussion-based improvisation environment using table-top electric guitar and ancillary percussion instruments with a complex system of small microphones (see figure 1), as this would offer me a rich sonic playground to explore musically, and limited my physical gestures to the areas around the table-top guitar for easy capture by the Kinect. Initial capture – seed data An initial dataset was captured from a short solo improvisation and was to be used as the seed data for the follow-on experiential process. There were several problems in the capture of this data, primarily that the Kinect and the MacBook regularly lost contact and much of the signal was dropped. This meant deleting large portions of the dataset, but enough was salvaged to operate as a seed. In total 17935 lines of data were captured, figure 2 shows 2",,2020,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
b5584fa1e1d02b42c91a2c20adc6b7bd19957019,https://www.semanticscholar.org/paper/b5584fa1e1d02b42c91a2c20adc6b7bd19957019,Toward Socially Aware Robot Navigation in Dynamic and Crowded Environments: A Proactive Social Motion Model,"Safe and social navigation is the key to deploying a mobile service robot in a human-centered environment. Widespread acceptability of mobile service robots in daily life is hindered by robot’s inability to navigate in crowded and dynamic human environments in a socially acceptable way that would guarantee human safety and comfort. In this paper, we propose an effective proactive social motion model (PSMM) that enables a mobile service robot to navigate safely and socially in crowded and dynamic environments. The proposed method considers not only human states (position, orientation, motion, field of view, and hand poses) relative to the robot but also social interactive information about human–object and human group interactions. This allows development of the PSMM that consists of elements of an extended social force model and a hybrid reciprocal velocity obstacle technique. The PSMM is then combined with a path planning technique to generate a motion planning system that drives a mobile robot in a socially acceptable manner and produces respectful and polite behaviors akin to human movements. Note to Practitioners—In this paper, we validated the effectiveness and feasibility of the proposed proactive social motion model (PSMM) through both simulation and real-world experiments under the newly proposed human comfortable safety indices. To do that, we first implemented the entire navigation system using the open-source robot operating system. We then installed it in a simulated robot model and conducted experiments in a simulated shopping mall-like environment to verify its effectiveness. We also installed the proposed algorithm on our mobile robot platform and conducted experiments in our office-like laboratory environment. Our results show that the developed socially aware navigation framework allows a mobile robot to navigate safely, socially, and proactively while guaranteeing human safety and comfort in crowded and dynamic environments. In this paper, we examined the proposed PSMM with a set of predefined parameters selected based on our empirical experiences about the robot mechanism and selected social environment. However, in fact a mobile robot might need to adapt to various contextual and cultural situations in different social environments. Thus, it should be equipped with an online adaptive interactive learning mechanism allowing the robot to learn to auto-adjust their parameters according to such embedded environments. Using machine learning techniques, e.g., inverse reinforcement learning [1] to optimize the parameter set for the PSMM could be a promising research direction to improve adaptability of mobile service robots in different social environments. In the future, we will evaluate the proposed framework based on a wider variety of scenarios, particularly those with different social interaction situations and dynamic environments. Furthermore, various kinds of social cues and signals introduced in [2] and [3] will be applied to extend the proposed framework in more complicated social situations and contexts. Last but not least, we will investigate different machine learning techniques and incorporate them in the PSMM in order to allow the robot to automatically adapt to diverse social environments.",IEEE Transactions on Automation Science and Engineering,2017,10.1109/TASE.2017.2731371,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d5ab35e5090aa23093b717841742da9cf44861e6,https://www.semanticscholar.org/paper/d5ab35e5090aa23093b717841742da9cf44861e6,Game-Theoretic Safety Assurance for Human-Centered Robotic Systems,"In order for autonomous systems like robots, drones, and self-driving cars to be reliably introduced into our society, they must have the ability to actively account for safety during their operation. While safety analysis has traditionally been conducted offline for controlled environments like cages on factory floors, the much higher complexity of open, human-populated spaces like our homes, cities, and roads makes it unviable to rely on common design-time assumptions, since these may be violated once the system is deployed. Instead, the next generation of robotic technologies will need to reason about safety online, constructing high-confidence assurances informed by ongoing observations of the environment and other agents, in spite of models of them being necessarily fallible.This dissertation aims to lay down the necessary foundations to enable autonomous systems to ensure their own safety in complex, changing, and uncertain environments, by explicitly reasoning about the gap between their models and the real world. It first introduces a suite of novel robust optimal control formulations and algorithmic tools that permit tractable safety analysis in time-varying, multi-agent systems, as well as safe real-time robotic navigation in partially unknown environments; these approaches are demonstrated on large-scale unmanned air traffic simulation and physical quadrotor platforms. After this, it draws on Bayesian machine learning methods to translate model-based guarantees into high-confidence assurances, monitoring the reliability of predictive models in light of changing evidence about the physical system and surrounding agents. This principle is first applied to a general safety framework allowing the use of learning-based control (e.g. reinforcement learning) for safety-critical robotic systems such as drones, and then combined with insights from cognitive science and dynamic game theory to enable safe human-centered navigation and interaction; these techniques are showcased on physical quadrotors—flying in unmodeled wind and among human pedestrians—and simulated highway driving. The dissertation ends with a discussion of challenges and opportunities ahead, including the bridging of safety analysis and reinforcement learning and the need to ``close the loop'' around learning and adaptation in order to deploy increasingly advanced autonomous systems with confidence.",,2019,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a5d85748055ff954e38c4c1f24138d614d4323fa,https://www.semanticscholar.org/paper/a5d85748055ff954e38c4c1f24138d614d4323fa,SmartDashCam: Automatic Live Calibration for DashCams,"Dashboard camera installations are becoming increasingly common due to various Advanced Driver Assistance Systems (ADAS) based services provided by them. Though deployed primarily for crash recordings, calibrating these cameras can allow them to measure real-world distances, which can enable a broad spectrum of ADAS applications such as lane-detection, safe driving distance estimation, collision prediction, and collision prevention Today, dashboard camera calibration is a tedious manual process that requires a trained professional who needs to use a known pattern (e.g., chessboard-like) at a calibrated distance. In this paper, we propose SmartDash-Cam, a system for automatic and live calibration of dashboard cameras which always ensures highly accurate calibration values. Smart-DashCam leverages collecting images of a large number of vehicles appearing in front of the camera and using their coarse geometric shapes to derive the calibration parameters. In sharp contrast to the manual process we are proposing the use of a large amount of data and machine learning techniques to arrive at calibration accuracies that are comparable to the manual process. SmartDashCam implemented using commodity dashboard cameras estimates realworld distances with mean errors of 5.7 % which closely rivals the 4.1% mean error obtained from traditional manual calibration using known patterns.",2019 18th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN),2019,10.1145/3302506.3310397,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
09ea8130811e9eb8ff15776ef4cf03d48a0af2bb,https://www.semanticscholar.org/paper/09ea8130811e9eb8ff15776ef4cf03d48a0af2bb,"Parallel Problem Solving from Nature - PPSN X, 10th International Conference Dortmund, Germany, September 13-17, 2008, Proceedings",,PPSN,2008,10.1007/978-3-540-87700-4,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
72d57ad1f5bf0c035c75739b308117c9d6ba05bd,https://www.semanticscholar.org/paper/72d57ad1f5bf0c035c75739b308117c9d6ba05bd,Dealing realistically with the artificial intelligence revolution,"The world is engaged in a competition for leadership in the array of technologies and applications often grouped under the umbrella of artificial intelligence. This competition has often been compared to the nuclear arms race of the Cold War, but the comparison is misleading on many fronts. Artificial intelligence is not one technology but many, and those technologies may be used in a wide variety of classifying, optimizing, and predictive applications, many and probably most not military in nature. Unlike the US Manhattan Project and the Soviet Union’s equally secretive early nuclear program, AI research is done in both the private and public sectors; information about private sector research is regularly shared among participants in the field and, therefore, among countries. Progress is rapid, and knowledge about that progress is seldom contained to one country alone. But the dissimilarity between today’s AI research programs and the highly secretive, government-controlled efforts that led to 60,000 nuclear weapons in the 1980s does not mean warnings about an AI arms race now are rare. In 2017, for instance, Russian President Vladimir Putin described the AI race in martial terms, as if he were Sauron, the dark lord of Middle-earth, seeking The One Ring to Rule them All: “Whoever becomes the leader in this sphere will become the ruler of the world.” The term “AI arms race” also seems to have a natural attraction for news headline writers around the world (even when the articles under those headlines explain why the competition in AI technologies really is not an arms race). But if it does not constitute an arms race per se, the worldwide competition in AI research and development does present the world with tricky ethical choices, particularly in the military arena – and a huge management problem. Clearly, some combinations of AI algorithms, robotics, and other emergent technologies – killer robots that choose their targets independent of human control, for example – may pose extraordinary dangers to humanity. Other applications involving one of more of the technologies in the AI basket, however, could and likely will provide the world with almost magical benefits, detecting, for example, diseases that doctors would miss, thereby saving countless lives, and guiding driverless cars that kill many thousands of fewer people than human-steered vehicles. Directing fast-moving and decentralized AI research in positive directions, while minimizing its potentially enormous negative potentials, is almost certainly the most difficult “dual-use” technology challenge the world has faced to date. For this issue, we asked four highly regarded experts for their views on the global AI competition. In “The frame problem: The AI ‘arms race’ isn’t one,” University of Cambridge researcher Heather M. Roff explains why using an arms race orientation when discussing the global competition in the AI field is not only fundamentally inaccurate, but potentially dangerous. “[T]alking about technological competition – in research, adoption, and deployment – in all sectors of multiple economies and in warfare is not really an arms race. Indeed, to frame this competition in military terms risks the adoption of policies or regulations that could escalate rivalry between states and increase the likelihood of actual conflict,” Roff writes. Cyberspace scholar Chris C. Demchak agrees that AI competition does not qualify, in itself, as an arms race. But in its rivalry with the United States, a rising, authoritarian China focused on technological supremacy may threaten freedom of expression, human rights, and democratic self-government around the world, Demchak suggests in her article, “China: determined to dominate cyberspace and AI.” “The rise of AI, a subset of cyber – as well as machine learning, quantum computing, and other new technologies – does not herald a new arms race so much as an enhanced, possibly exponentially accelerated, underlying competition between rising China and the United States,” Demchak notes. “Given the dual nature of most cyber tools, those worried about an AI arms race should rather be more concerned about the profound disruption to the existing global order that China’s rise to the top of the cyber pack could pose.” Brenda Leong, senior counsel and director of strategy at the Washington, DC-based think tank Future of Privacy Forum, sees a more generalized threat to",Bulletin of the Atomic Scientists,2019,10.1080/00963402.2019.1604812,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
c0e9e26ff57d1b805c43e76668f6543c70bea8b5,https://www.semanticscholar.org/paper/c0e9e26ff57d1b805c43e76668f6543c70bea8b5,Enactivism and Robotic Language Acquisition: A Report from the Frontier,"In this article, I assess an existing language acquisition architecture, which was deployed in linguistically unconstrained human–robot interaction, together with experimental design decisions with regard to their enactivist credentials. Despite initial scepticism with respect to enactivism’s applicability to the social domain, the introduction of the notion of participatory sense-making in the more recent enactive literature extends the framework’s reach to encompass this domain. With some exceptions, both our architecture and form of experimentation appear to be largely compatible with enactivist tenets. I analyse the architecture and design decisions along the five enactivist core themes of autonomy, embodiment, emergence, sense-making, and experience, and discuss the role of affect due to its central role within our acquisition experiments. In conclusion, I join some enactivists in demanding that interaction is taken seriously as an irreducible and independent subject of scientific investigation, and go further by hypothesising its potential value to machine learning.",Philosophies,2019,10.3390/PHILOSOPHIES4010011,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
de54d22466f6078ea5295692c8cd3c686a316874,https://www.semanticscholar.org/paper/de54d22466f6078ea5295692c8cd3c686a316874,Introducing an All-mechanized Surgical Assistant for Use in Reconstructive Surgeries,"1 Copyright © 2019 The Authors. Published by Wolters Kluwer Health, Inc. on behalf of The American Society of Plastic Surgeons. This is an open access article distributed under the Creative Commons Attribution License 4.0 (CCBY), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Sir, T is currently a critical lack of surgeons in many advanced countries, including Japan. A geographic shortage of plastic surgeons is also sometimes considered to be a social problem.1,2 The key to solving this issue depends on proper and thoughtful deployment of surgical resources and on how smoothly the medical education system can provide residents with appropriate educational opportunities. During surgeries, surgical assistants often do nothing but simple tasks such as keeping the surgical field clear and simply follow top-down orders throughout the operation. Performance of these activities is a waste of talent and time for an assistant who has similar or higher skills than the senior surgeon. Alternatively, if the assistant is at the training level, he or she rarely gets effective learning opportunities due to standing in a position from which it is not possible to see the surgical field clearly. This situation provides young surgeons with nothing but a very poor view and position from which to have new surgical ideas. A totally automated operation could be one solution to these problems. Automated, unmanned hotels and retail services are becoming common. Machines and robots have already replaced people for helpful and essential assistance in daily life, and utilization of similar systems in the role of a surgical assistant is plausible. A good example is provided by the Octopus (MEDNOSBRO GmbH, Rudolfstetten, Switzerland), a device that serves as a versatile retractor system, and has 3 joints similar to those in a human upper limb, which allows precise all-direction maneuvers. In addition to this function, the system has flexible settings that allow a surgeon to place the machine in ideal positions to have a perfect view only by tightening or loosening a special screw attached to the device. A single surgeon can then complete an entire surgery using various tip parts of the retractor. Just like an assistant, the system plays a major role in the surgery, including facilitating a variety of flap elevations. Our surgical team has introduced the Octopus device into many kinds of reconstructive surgery, although mainly breast reconstruction, and we have shown that many plastic and reconstructive surgeries can be performed without an assistant. The device enables a single surgeon to complete the surgery alone, even in emergency situations, and never disturbs the surgeon’s work, loses focus, or becomes tired. The device makes it possible to reduce labor costs and to place sufficient talent in suitable positions, which releases assistants from unfruitful positions and consequently allows them to contribute much more in another field. If assistants are inexperienced residents, they can freely observe a surgery with a clear view that may not differ from the surgeon’s point of view, which allows acquisition of an understanding of a detailed procedure in an efficient way. There have been many beneficial results of device-assisted operations in various surgical fields,3–5 but relatively few in plastic and reconstructive surgery. However, this approach has great potential in this field, and we believe that this new surgical concept will spread quickly in the future.",Plastic and reconstructive surgery. Global open,2019,10.1097/GOX.0000000000002403,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
28361f2a56e0b5afd1189245587358598a5af7f5,https://www.semanticscholar.org/paper/28361f2a56e0b5afd1189245587358598a5af7f5,A Quantitative Analysis of Dressing Dynamics for Robotic Dressing Assistance,"Assistive robots have a great potential to address issues related to an ageing population and an increased demand for caregiving. Successful deployment of robots working in close proximity with people requires consideration of both safety and human-robot interaction. One of the established activities of daily living where robots could play an assistive role is dressing. Using the correct force profile for robot control will be essential in this application of human-robot interaction requiring careful exploration of factors related to the user’s pose and the type of garments involved. In this paper a Baxter robot was used to dress a jacket onto a mannequin and human participants considering several combinations of user pose and clothing type (base layers), whilst recording dynamic data from the robot, a load cell and an IMU. We also report on suitability of these sensors for identifying dressing errors, e.g. fabric snagging. Data was analyzed by comparing the overlap of confidence intervals to determine sensitivity to dressing. We expand the analysis to include classification techniques such as decision tree and support vector machines using k-fold cross-validation. The 6-axis load cell successfully discriminated between clothing types with predictive model accuracies between 72-97%. Used independently, the IMU and Baxter sensors were insufficient to discriminate garment types with the IMU showing 40-72% accuracy, but when used in combination this pair of sensors achieved an accuracy similar to the more expensive load cell (98%). When observing dressing errors (snagging) Baxter’s sensors and the IMU data demonstrated poor sensitivity but applying machine learning methods resulted in model with high predicative accuracy and low false negative rates (≤5%). The results show that the load cell could be used independently for this application with good accuracy but a combination of the lower cost sensors could also be used without a significant loss in precision which will be a key element in the robot control architecture for safe human-robot interaction.",Front. Robot. AI,2017,10.3389/frobt.2017.00013,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
56c8cefc1d6bb70456050d13e194132b570b9bc5,https://www.semanticscholar.org/paper/56c8cefc1d6bb70456050d13e194132b570b9bc5,The moving frame method in dynamics: Reforming a curriculum and assessment,"Rigid body dynamics, a gateway course to the mechanical engineering major (and related majors), focuses on a view of motion that is not commensurate with the contemporary age in which mobile devices have on-board inertial firmware. The traditional approach to this topic deploys a mathematical notation, and associated algebra, that inordinately privileges the inertial frames and 2D motion. This limits the study of machines to two-dimensional problems, lends an appearance of whimsy to solutions that obfuscates the theory of motion. We propose a new mathematical approach to dynamics to reinvigorate the discipline and motivate students. The new approach uses modern mathematical tools which have been distilled to tractability: Lie Group Theory, Cartan’s Moving Frames and a new compact notation from Geometrical Physics. The reconstructed course abandons the cross product—a toxic algebraic operation due to its failure to adhere to associativity. We minimize the use of vectors and replace them with rotation matrices. Sophomores learn to solve 3D Dynamics problems with as much ease as solving 2D problems. Typical problems include the precession of tops, gyroscopes, inertial devices to prevent ship roll at sea, and 3D robot and crane kinetics. A critical aspect of this new method is the consistency: the notation is the same for 3D and 2D problems, from advanced robotics to introductory dynamics, students learn the name notational method. The first objective of this paper presents the new mathematical approach to rigid body dynamics—it amounts to an introductory, yet simplified, lecture on a new method. The second objective presents assessment over a three-year period. In the first year, we taught using the old 2D vector-based approach. In the second year, we transitioned to the new method and compared student perceptions in the first two years. In the third year, the course was refined. The goal of this effort is to retain students in mechanical engineering by offering them a new view of the discipline, rather than simple pedagogical course interventions such as e-learning or flipped classrooms. The course content is delivered using the emerging visualization technology: WebGL. WebGL represents the future of the 3D web. It requires no downloads and no plugins. Students are directed to a web site where all images for the lectures are 3D and interactive. The animations run on cell phones, laptops and other mobile devices. It is the contention of this paper that modernizing the math will do more to reduce attrition than learning interventions. This new approach reduces conceptual difficulties that accompany 2D restrictions. It opens many questions on how students perceive 3D space and invites research into how exploiting more modern mathematical math may improve learning.",,2018,10.1177/0306419017730633,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
1a6e847a75917d9f6c3d3c3d3ff6bc8505f50b86,https://www.semanticscholar.org/paper/1a6e847a75917d9f6c3d3c3d3ff6bc8505f50b86,Vehicular Communications And Networks Architectures Protocols Operation And Deployment Woodhead Publishing Series In Electronic And Optical Materials,"A Trust-driven Privacy Architecture for Vehicular Ad-hoc NetworksIntelligent Vehicular Networks and CommunicationsEmerging Wireless Communication and Network TechnologiesVehicular NetworksBio-inspired Routing Protocols for Vehicular Ad-Hoc NetworksInformation Security of Intelligent Vehicles CommunicationVehicular Cloud Computing for Traffic Management and SystemsVehicular ad hoc NetworksMedical Internet of ThingsNetworking and Telecommunications: Concepts, Methodologies, Tools, and ApplicationsInterand Intra-Vehicle CommunicationsIT Convergence and Security 2017Optimisation Des Communications V2V Et V2I Dans Un Réseau de Véhicules OpéréRoadside Networks for Vehicular CommunicationsCloud and IoT-Based Vehicular Ad Hoc NetworksVehicle-To-Vehicle and Vehicle-To-Infrastructure CommunicationsBlockchainenabled Fog and Edge Computing: Concepts, Architectures and ApplicationsWireless Device-to-Device Communications and NetworksInterference in Vehicle-to-vehicle Communication NetworksWireless Networks and Industrial IoTGreen Computing in Network SecurityVehicular Communications for Smart Cars5G-Enabled Vehicular Communications and NetworkingAdvances in Delay-Tolerant Networks (DTNs)Rail Vehicle MechatronicsQuality of Service Architectures for Wireless Networks: Performance Metrics and ManagementCommunication Technologies for VehiclesVehicular NetworkingVehicular-2-X Communication断言Telematics Communication Technologies and Vehicular Networks: Wireless Architectures and ApplicationsVehicular TechnologiesCharacterization, Avoidance and Repair of Packet Collisions in Inter-Vehicle Communication NetworksIntelligent Transportation SystemsResearch Anthology on Architectures, Frameworks, and Integration Strategies for Distributed and Cloud ComputingRoadside Networks for Vehicular Communications: Architectures, Applications, and Test FieldsVehicular-2-X CommunicationEurope and MENA Cooperation Advances in Information and Communication TechnologiesVehicular Communications and NetworksCapacity Analysis of Vehicular Communication Networks During the last 15 years, the interest in vehicular communication has grown, especially in the automotive industry. Due to the envisioned mass market, projects focusing on Car-to-X communication experience high public visibility. This book presents vehicular communication in a broader perspective that includes more than just its application to the automotive industry. It provides, researchers, engineers, decision makers and graduate students in wireless communications with an introduction to vehicular communication focussing on car-to-x and train-based systems. Emphasizes important perspectives of vehicular communication including market area, application areas, and standardization issues as well as selected topics featuring aspects of developing, prototyping, and testing vehicular communication systems. Supports the reader in understanding common characteristics and differences between the various application areas of vehicular communication. Offers both an overview of the application area and an in-depth discussion of key technologies in these areas. Written by a wide range of experts in the field.Universal vehicular communication promises many improvements in terms of acdent avoidance and mitigation, better utilization of roads and resources such as time and fuel, and new opportunities for infotainment applications. However, before widespread acceptance, vehicular communication must meet challenges comparable to the trouble and disbelief that accompanied the introduction of traf c lights back then. The rst traf c light was installed in 1868 in London to signal railway, but only later, in 1912, was invented the rst red-green electric traf c light. And roughly 50 years after the rst traf c light, in 1920, the rst four-way traf c signal comparable to our today’s traf c lights was introduced. The introduction of traf c signals was necessary after automobiles soon became prevalent once the rst car in history, actually a wooden motorcycle, was constructed in 1885. Soon, the scene became complicated, requiring the introduction of the “right-of-way” philosophy and later on the very rst traf c light. In the same way the traf c light was a necessary mean to regulate the beginning of the automotive life and to protect drivers, passengers, as well as pedestrians and other inhabitants of the road infrastructure, vehicular communication is necessary to accommodate the further growth of traf c volume and to signi cantly reduce the number of accidents.This is the second volume of proceedings including selected papers from the International Conference on IT Convergence and Security (ICITCS) 2017, presenting a snapshot of the latest issues encountered in the field. It explores how IT convergence and security issues are core to most current research, industrial and commercial activities and consists of contributions covering topics including machine learning & deep learning, communication and signal processing, computer vision and applications, future network technology, artificial intelligence and robotics. ICITCS 2017 is the latest in a series of highly successful",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
8f1644359a2679dc961392b28e3fae6df2ad48cf,https://www.semanticscholar.org/paper/8f1644359a2679dc961392b28e3fae6df2ad48cf,The Subsea Pipeline Inspection Paradigm - What Next ?,"
 Shell in the UK has a vast network of more than 200 pipelines & umbilicals covering some 3000+ kilometres. Historically, Shell has executed Side Scan Sonar Surveys along these pipelines using a Remotely Operated Towed Vehicle and subsequently followed up with ROV based surveys & inspections. However, in 2018, the respective Geomatics & Subsea Maintenance / Pipelines Departments decided to take advantage of new & emerging innovative technologies and compiled a minimal technical scope & tender document to tap into the latest that the market could offer. Consequently, Shell UK awarded DeepOcean (Norway) with a contract for their ""Fast Digital Imaging Service"" and embarked on a 45 day survey campaign. In 2019, the same subsea inspection project will be executed once again and the lessons learned ought to inspire and excite many different disciplines and communities, both internally within Shell and externally e.g OGA - Oil & Gas Authority & other valued stakeholders.
 The paper highlights the key technologies that were deployed and how the new deliverables & business insights take us down the road to Digitalisation including scope for future Machine Learning & Automation processes. Challenges arising from the acquisition and managing the associated data sets shall also be discussed. The speaker will spark dialogue at the end by asking the respective communities how robotics and artificial intelligence will change the industry landscape ?","Day 1 Tue, September 03, 2019",2019,10.2118/195755-MS,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
cc79c0fa6e39ea58910fe54bb9b966d333d76cb3,https://www.semanticscholar.org/paper/cc79c0fa6e39ea58910fe54bb9b966d333d76cb3,Data Engineering Project (Educating for the Future PhUSE Working Group),"With an expected 100% increase, over the next 3 years, of data from non-EDC sources (such as smartphones, wearables and custom apps) the traditional methods of managing data for clinical trials presents executives with a resourcing headache. As such, many companies are looking for lower cost strategies to mitigate this resourcing challenge. However, citing case studies from other industries, there are new methodologies/technologies in data engineering which could enable automation of much of the “heavy-lifting” currently practiced in clinical data management and statistical programming. This paper discusses the Data Engineering Project within the PhUSE Computational Science (CS) Working Group, Educating for the Future, with a view to educate clinical data managers in data engineering principles so that they can be prepared, equipped and effective in dealing with the coming “data tsunami” heading to the shores of clinical research. INTRODUCTION Did you realise we are living in the age of the Fourth Industrial Revolution? Perhaps you have been busy downloading a myriad of “apps” designed to make your life easier or connecting on social media, uncovering relationships and associations you didn’t even know you had. Perhaps you have been shopping a global marketplace, comparing prices, quality and availability, all at your fingertips and in a minutes’ time. While this has been happening, the Fourth Industry Revolution has been evolving at exponential proportions . Just ask Siri! The term “Industrie 4.0”, was originated in Germany, as a government-led initiative, to transform manufacturing through advanced digital capability. Thus creating the concept of a “smart factory”, based on four key design principles : 1. Interconnection of machines, devices, sensors and people 2. Vast amounts of useful information (data) to drive decision making 3. Technical assistance to aid humans, for example to visualise data or to perform tasks that may be a safety risk for a human. 4. The use of cyber-physical systems to make decisions on their own and to perform tasks as autonomously as possible. Emerging from the premise of “Industrie 4.0” is the advent of the term “The Fourth Industrial Revolution” (also referred to as “4IR” or “I4.0”). This term originated in 2016 when described by Klaus Schwab (Founder and Executive Chairman of the World Economic Forum), as a “technological revolution that will fundamentally alter the way we live, work, and relate to one another”. Klaus goes on to describe it as a digital revolution with innovative uses of a combination of technologies that build upon the premise of the third revolution (i.e. electronics and information technology to automate production). As a result, emerging technologies have brought forth advancements in fields such as artificial intelligence, robotics, the Internet of Things, autonomous vehicles, 3D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing. This rapid evolution will undoubtedly affect industries world-wide, already disrupting many industries, such as travel agencies, video rentals and bookstores . The pharmaceutical industry is also experiencing the impacts of I4.0. Digital and mobile technologies have brought on significant advancements in data acquisition and accessibility as it relates to health care and patient data. A recent study, conducted in 2017, by Tufts University in collaboration with Veeva Systems, shows that close to 100% 2 of companies surveyed, estimate utilization of various electronic data sources in clinical studies. As shown in Figure 1. below, companies utilizing e-sources such as, smartphones, custom applications, and mobile health, will more than double in the next 3 years . Therefore, requiring greater capabilities in handling large volumes of data, data from multiple sources and data of varying formats. As with other industries, data will become a critical asset and the effective utilisation of this data can play a key role in driving growth in the business while bringing novel therapies to the patients who need them. source: Tufts – Veeva 2017 EClinical Landscape Study. Tufts University, 2018, pp. 11–13, Tufts – Veeva 2017 EClinical Landscape Study. In this paper, we will focus on the works of the Data Engineering Project within the Educating for the Future Working Group. With the formation of the Working Group in early 2018, the team had taken on the mission to explore how data engineering techniques, successfully deployed in other industries, could be utilised in the pharmaceutical industry, with a goal to facilitate the education of the pharmaceutical industry on these techniques. We will share with you some introductory information about Data Engineering and Data Science and explore how embracing new data engineering techniques may affect the industry culture. You will learn about use cases of Data Engineering in other industries and how advances in digital capability have affected their business model. We will also share some of the many software packages and tools available to enable automation, commonly used in Data Engineering and Data Science. Finally we will reflect on the benefits that data standardisation has brought to the pharmaceutical industry and share our vision for disseminating information to facilitate your learning going forward. DATA ENGINEERING To start this learning journey, exploring the term “Data Engineering” opens the door to the vast opportunities and roles available today with the overarching goal to optimise the use of data in day to day business operations. In doing a simple search on the internet, “what is data engineering?”, one will find many posts expressing their understanding of Data Engineering, with some variation. However, what is clear is that Data Engineering encompasses the many considerations that need to be taken into account to optimally curate, transform, secure and disseminate data suitable for analysis. As technology and tools have become more advanced, building such a platform and infrastructure requires engineers and architects of both general and specific expertise. The Data Engineer combines knowledge in areas such as software development, infrastructure, data architecture, data warehousing, cloud technology and data cleaning in order to design, build and 3 test solutions that define the pipelines of data throughout the enterprise, making the data accessible to the organisation. [5] [27] [31] Optimised Data Engineering appropriately balances the efficiency of an automated process against the cost of development and maintenance of that process, ensuring repetitive processes that require humans to write code, press keys, cut-and-paste and update documents are minimised or eliminated. DATA SCIENCE Often paired with the term “Data Engineering” is also the term “Data Science”. According to Kelle O’Neal and Charles Roe: “Data Science allows enterprises the ability to turn their data assets into a narrative. Data Science allows that narrative to be expanded across timelines, in different data spaces that trace from the past into the future, with much more involved questions and answers about an enterprise, different potential outcomes, and repercussions based on recommendations. Data Science employs a range of mathematical, business, and scientific techniques to solve complex problems about an organisation’s data assets.” [26] In contrast, the focus of the Data Engineer is on the process from data curation to dissemination and the focus of the Data Scientist is on the analytics of the data, thus extracting knowledge from the data. To achieve quality data capture, near-real-time accessibility and meaningful analytics, one cannot function without the other, and effective teamwork optimises the value of each role. As such, an analytics team would be composed of distinct roles/capabilities : ● Data Engineers (in areas such as database architecture, database development, machine learning architecture, ETL scripting, etc.) ● Data Scientists ● Business Analysts Data Engineering brings together the broad expertise, of these roles, to ensure the data are curated and accessible to the Data Scientist, and in our environment today, this process is becoming more and more complex. Therefore, expertise in curating big-data and data of varying formats (structured and unstructured) is a critical core competency to optimise the potential impact of these digital assets (i.e. the data). The Data Scientist works deep in the data, utilizing various tools and techniques to discover patterns in the data that may drive decision making for the business. Optimising utilisation of the data to enable accurate conclusions can bear greater value to the organisation. As an example, per Tom Eunice’s post, “a fraud-detection algorithm may be very accurate when based on many months of historical data. However, months of historical data may not always be available. Designing a fraud-detection model that is still accurate using historical data from only a few days would be of more use and more practical to implement.” [17] The Business Analyst helps the Data Scientist understand the meaning of the data and the relevance of any discovered relationships. Initially, uncovering relationships in the data and upon further investigation, identifies meaningful patterns that may reveal information that otherwise may not have been known. [17] As you will see in the sections to follow, the full complement of the roles in an analytics team is what drives the business value. One discipline without the other (e.g. data engineering without data science) will result in missed opportunities. In the sections to follow, we often refer to Data Engineering, however, due to the close ties to Data Science, some examples elude to both Data Engineering and Data Science. USE CASES FROM OTHER INDUSTRIES In this section, we present three use cases from the transportation, retail, and agricultural industries. T",,2019,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
76747636a3682ea8ae657a236901f2f12f71c7f5,https://www.semanticscholar.org/paper/76747636a3682ea8ae657a236901f2f12f71c7f5,"devoted to Concept Theory , Classification , Indexing and Knowledge Representation Contents","Knowledge organization (KO) is considered a distinctive disciplinary focus of information science, with strong connections to other intellectual domains such as philosophy, computer science, psychology, sociology, and more. Given its inherent interdisciplinarity, we ask what might a map of the physical, cultural, and intellectual geography of the KO community look like? Who is participating in this discipline’s scholarly discussion, and from what locations, both geographically and intellectually? Using the unit of authorship in the journal Knowledge Organization, where is the nexus of KO activity and what patterns of authorship can be identified? Cultural characteristics were applied as a lens to explore who is and is not participating in the international conversation about KO. World Bank GNI per capita estimates were used to compare relative wealth of countries and Hofstede’s Individualism dimension was identified as a way of understanding attributes of countries whose scholars are participating in this dialog. Descriptive statistics were generated through Excel, and data visualizations were rendered through Tableau Public and TagCrowd. The current project offers one method for examining an international and interdisciplinary field of study but also suggests potential for analyzing other interdisciplinary areas within the larger discipline of information science. Hauser, Elliott and Joseph T. Tennis. 2019. “Episemantics: Aboutness as Aroundness.” Knowledge Organization 46 (8): 590595. 16 references. DOI:10.5771/0943-7444-2019-8-590. Abstract: Aboutness ranks amongst our field’s greatest bugbears. What is a work about? How can this be known? This mirrors debates within the philosophy of language, where the concept of representation has similarly evaded satisfactory definition. This paper proposes that we abandon the strong sense of the word aboutness, which seems to promise some inherent relationship between work and subject, or, in philosophical terms, between word and world. Instead, we seek an etymological reset to the older sense of aboutness as “in the vicinity, nearby; in some place or various places nearby; all over a surface.” To distinguish this sense in the context of information studies, we introduce the term episemantics. The authors have each independently applied this term in slightly different contexts and scales (Hauser 2018a; Tennis 2016), and this article presents a unified definition of the term and guidelines for applying it at the scale of both words and works. The resulting weak concept of aboutness is pragmatic, in Star’s sense of a focus on consequences over antecedents, while reserving space for the critique and improvement of aboutness determinations within various contexts and research programs. The paper finishes with a discussion of the implication of the concept of episemantics and methodological possibilities it offers for knowledge organization research and practice. We draw inspiration from Melvil Dewey’s use of physical aroundness in his first classification system and ask how aroundness might be more effectively operationalized in digital environments. Aboutness ranks amongst our field’s greatest bugbears. What is a work about? How can this be known? This mirrors debates within the philosophy of language, where the concept of representation has similarly evaded satisfactory definition. This paper proposes that we abandon the strong sense of the word aboutness, which seems to promise some inherent relationship between work and subject, or, in philosophical terms, between word and world. Instead, we seek an etymological reset to the older sense of aboutness as “in the vicinity, nearby; in some place or various places nearby; all over a surface.” To distinguish this sense in the context of information studies, we introduce the term episemantics. The authors have each independently applied this term in slightly different contexts and scales (Hauser 2018a; Tennis 2016), and this article presents a unified definition of the term and guidelines for applying it at the scale of both words and works. The resulting weak concept of aboutness is pragmatic, in Star’s sense of a focus on consequences over antecedents, while reserving space for the critique and improvement of aboutness determinations within various contexts and research programs. The paper finishes with a discussion of the implication of the concept of episemantics and methodological possibilities it offers for knowledge organization research and practice. We draw inspiration from Melvil Dewey’s use of physical aroundness in his first classification system and ask how aroundness might be more effectively operationalized in digital environments. Broughton, Vanda. 2019. “The Respective Roles of Intellectual Creativity and Automation in Representing Diversity: Human and Machine Generated Bias.” Knowledge Organization 46(8): 596606. 82 references. DOI:10.5771/0943-7444-2019-8-596. Abstract: The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. The paper traces the development of the discussion around ethical issues in artificial intelligence, and considers the way in which humans have affected the knowledge bases used in machine learning. The phenomenon of bias or discrimination in machine ethics is seen as inherited from humans, either through the use of biased data or through the semantics inherent in intellectually-built tools sourced by intelligent agents. The kind of biases observed in AI are compared with those identified in the field of knowledge organization, using religious adherents as an example of a community potentially marginalized by bias. A practical demonstration is given of apparent religious prejudice inherited from source material in a large database deployed widely in computational linguistics and automatic indexing. Methods to address the problem of bias are discussed, including the modelling of the moral process on neuroscientific understanding of brain function. The question is posed whether it is possible to model religious belief in a similar way, so that robots of the future may have both an ethical and a religious sense and themselves address the problem of prejudice. Chen, Shu-Jiun. 2019. “Semantic Enrichment of Linked Personal Authority Data: A Case Study of Elites in Late Imperial China.” Knowledge Organization 46(8): 607-614. 13 references. DOI:10. 5771/0943-7444-2019-8-607. Abstract: The study uses the Database of Names and Biographies (DNB) as an example to explore how in the transformation of original data into linked data, semantic enrichment can enhance engagement in digital humanities. In the preliminary results, we have defined instance-based and schema-based categories of semantic enrichment. In the instance-based category, in which enrichment occurs by enhancing the content of entities, we further determined three types, including: 1) enriching the entities by linking to diverse external resources in order to provide additional data of multiple perspectives; 2) enriching the entities with missing data, which is needed to satisfy the semantic queries; and, 3) providing the entities with access to an extended knowledge base. In the schema-based categories that enrichment occurs by enhancing the relations between the properties, we The study uses the Database of Names and Biographies (DNB) as an example to explore how in the transformation of original data into linked data, semantic enrichment can enhance engagement in digital humanities. In the preliminary results, we have defined instance-based and schema-based categories of semantic enrichment. In the instance-based category, in which enrichment occurs by enhancing the content of entities, we further determined three types, including: 1) enriching the entities by linking to diverse external resources in order to provide additional data of multiple perspectives; 2) enriching the entities with missing data, which is needed to satisfy the semantic queries; and, 3) providing the entities with access to an extended knowledge base. In the schema-based categories that enrichment occurs by enhancing the relations between the properties, we Knowl. Org. 46(2019)No.8 KO KNOWLEDGE ORGANIZATION Official Journal of the International Society for Knowledge Organization ISSN 0943 – 7444 International Journal devoted to Concept Theory, Classification, Indexing and Knowledge Representation have identified two types, including: 1) enriching the properties by defining the hierarchical relations between properties; and, 2) specifying properties’ domain and range for data reasoning. In addition, the study implements the LOD dataset in a digital humanities platform to demonstrate how instances and entities can be applied in the full texts where the relationship between entities are highlighted in order to bring scholars more semantic details of the texts. Clavi",,2019,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
527750817353f9663343b70013f8e2cd3dc6e460,https://www.semanticscholar.org/paper/527750817353f9663343b70013f8e2cd3dc6e460,"Special issue on smart interactions in cyber‐physical systems: Humans, agents, robots, machines, and sensors","In recent years, there has been increasing interaction between humans and non‐human systems as we move further beyond the industrial age, the information age, and as we move into the fourth‐generation society. The ability to distinguish between human and non‐human capabilities has become more difficult to discern. Given this, it is common that cyber‐physical systems (CPSs) are rapidly integrated with human functionality, and humans have become increasingly dependent on CPSs to perform their daily routines. The constant indicators of a future where human and non‐human CPSs relationships consistently interact and where they allow each other to navigate through a set of non‐trivial goals is an interesting and rich area of research, discovery, and practical work area. The evidence of convergence has rapidly gained clarity, demonstrating that we can use complex combinations of sensors, artificial intelligence, and data to augment human life and knowledge. To expand the knowledge in this area, we should explain how to model, design, validate, implement, and experiment with these complex systems of interaction, communication, and networking, which will be developed and explored in this special issue. This special issue will include ideas of the future that are relevant for understanding, discerning, and developing the relationship between humans and non‐ human CPSs as well as the practical nature of systems that facilitate the integration between humans, agents, robots, machines, and sensors (HARMS). Contributions that demonstrate the integration of HARMS using practical experimental results were invited for this issue. Papers that show the design, models, or techniques of advancement were selected in the wider context of large, complex systems, including those involving multiple, heterogeneous actors. The first paper “Deep Compression of Convolutional Neural Networks with Low‐Rank Approximation,” by Marcella Astrid and Seung‐Ik Lee, captures the application of deep neural networks (DNNs) to connect the world with cyber‐physical systems (CPSs), which have attracted much attention. However, DNNs require a large amount of memory and computational cost, which hinders their use in the relatively low‐end smart devices that are widely used in CPSs. In this paper, the authors aim to determine whether DNNs can be efficiently deployed and operated in low‐end smart devices. To do this, they develop a method to reduce the memory requirement of DNNs and to increase the inference speed, while maintaining the performance (for example, accuracy) close to the original level. The parameters of DNNs are decomposed using a hybrid of canonical polyadic–singular value decomposition, and are approximated using a tensor power method, and they are fine‐ tuned by performing iterative one‐shot hybrid fine‐tuning to recover from the decreased accuracy. In this study, they evaluate their method on frequently used networks. The authors also present results from extensive experiments on the effects of several fine‐tuning methods, as well as the importance of iterative fine‐tuning and decomposition techniques. The authors demonstrate the effectiveness of the proposed method by deploying compressed networks in smartphones. The second paper, “Human‐like Sign Language Learning Method with Deep Learning,” by Ki-Baek Lee and others proposes a human‐like sign language learning method with a deep learning technique. Inspired by the fact that humans can learn sign language from just a set of pictures in a book, in the proposed method, the input data are pre‐processed into an image. In addition, the network is partially pre‐ trained to imitate the preliminarily obtained knowledge of humans. The learning process is implemented with a well‐ known network, that is, a convolutional neural network. Twelve sign actions are learned in ten scenarios and can be recognized with an accuracy of 99% in an environment involving low‐cost equipment and limited data. The results demonstrate that the system is highly practical as well as accurate and robust. The third paper, “Optimization‐based Humanoid Robot Navigation Using Monocular Camera in Indoor Environment,” shows how robot navigation gives robots mobility. For this reason, mobility is one of the robot fields that have been extensively studied since robots have been developed. In recent years, there has been increased interest in personal service robots for homes and public facilities. As a result, the navigation of the robot in the home environment,",ETRI Journal,2018,10.4218/etrij.18.3018.0000,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
972b20f1a231cb991f43b5a8dc99b8892c6a3db9,https://www.semanticscholar.org/paper/972b20f1a231cb991f43b5a8dc99b8892c6a3db9,Stress Detection for Keystroke Dynamics,"Background. Stress can profoundly affect human behavior. Critical-infrastructure operators (e.g., at nuclear power plants) may make more errors when overstressed; malicious insiders may experience stress while engaging in rogue behavior; and chronic stress has deleterious effects on mental and physical health. If stress could be detected unobtrusively, without requiring special equipment, remedies to these situations could be undertaken. In this study a common computer keyboard and everyday typing are the primary instruments for detecting stress. Aim. The goal of this dissertation is to detect stress via keystroke dynamics – the analysis of a user’s typing rhythms – and to detect the changes to those rhythms concomitant with stress. Additionally, we pinpoint markers for stress (e.g., a 10% increase in typing speed), analogous to the antigens used as markers for blood type. We seek markers that are universal across all typists, as well as markers that apply only to groups or clusters of typists, or even only to individual typists. Data. Five types of data were collected from 116 subjects: (1) demographic data, which can reveal factors (e.g., gender) that influence subjects’ reactions to stress; (2) psychological data, which capture a subject’s general susceptibility to stress and anxiety, as well as his/her current stress state; (3) physiological data (e.g., heart-rate variability and blood pressure) that permit an objective and independent assessment of a subject’s stress level; (4) self-report data, consisting of subjective self-reports regarding the subject’s stress, anxiety, and workload levels; and (5) typing data from subjects, in both neutral and stressed states, measured in terms of keystroke timings – hold and latency times – and typographical errors. Differences in typing rhythms between neutral and stressed states were examined to seek specific markers for stress. Method. An ABA, single-subject design was used, in which subjects act as their own controls. Each subject provided 80 typing samples in each of three conditions: (A) baseline/neutral, (B) induced stress, and (A) post-stress return/recovery-to-baseline. Physiological measures were analyzed to ascertain the subject’s stress level when providing each sample. Typing data were analyzed, using a variety of statistical and machine learning techniques, to elucidate markers of stress. Clustering techniques (e.g., K-means) were also employed to detect groups of users whose responses to stress are similar. Results. Our stressor paradigm was effective for all 116 subjects, as confirmed through analysis of physiological and self-report data. We were able to identify markers for stress within each subject; i.e., we can discriminate between neutral and stressed typing when examining any subject individually. However, despite our best attempts, and the use of state-of-the-art machine learning techniques, we were not able to identify universal markers for stress, across subjects, nor were we able to identify clusters of subjects whose stress responses were similar. Subjects’ stress responses, in typing data, appear to be highly individualized. Consequently, effective deployment in a realworld environment may require an approach similar to that taken in personalized medicine.",,2018,10.1184/R1/6723227.V3,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
232d11a7a3181f071c4d82a3578930eff9af8d93,https://www.semanticscholar.org/paper/232d11a7a3181f071c4d82a3578930eff9af8d93,"Advances in Grid Computing - EGC 2005, European Grid Conference, Amsterdam, The Netherlands, February 14-16, 2005, Revised Selected Papers",,EGC,2005,10.1007/b137919,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
510a5a1154f25500ce32abc6ed72b65a72077daf,https://www.semanticscholar.org/paper/510a5a1154f25500ce32abc6ed72b65a72077daf,A thermal emotion classifier for improved human-robot interaction,"In their expanding role as tutors, home and healthcare assistants, robots must effectively interact with individuals of varying ability and temperament. Indeed, deploying robots in long-term social engagements will almost certainly require robots to reliably detect and adapt to changes in the demeanor of social partners to promote trust and more productive collaboration. However, the recognition of emotional state typically relies on the interpretation of very subtle cues, often varying from one person to the next. In addition, while facial expressions, body posture and features of speech have been used to detect affective changes, the robustness of these measures is often hindered by cultural and age differences. Recently, infrared thermography has shown promise in detecting guilt, fear and stress, indicating that it may be a viable sensing modality for improved human-robot interaction. In this study, we evaluated the efficacy of using a far infrared (FIR) camera for detecting robot-elicited affective response compared to video-elicited affective response by tracking thermal changes in five areas of the face. Further, we analyzed localized changes in the face to assess whether thermal and electrodermal responses to emotions elicited by traditional video techniques and by robots are similar. Finally, we performed principal component analysis to reduce the dimensionality of data and evaluated the performance using machine learning techniques for classifying thermal data by emotion state, resulting in a thermal classifier with a performance accuracy of 77.5%.",2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),2016,10.1109/ROMAN.2016.7745198,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d85e0d876fa2a02241b665087ccd7084f3843cf1,https://www.semanticscholar.org/paper/d85e0d876fa2a02241b665087ccd7084f3843cf1,Evaluation of the extent of integration of humanoids in tackling social-economic challenges confronting the Africa continent,"The sustenance and growth of any nation, depends critically and essentially on her level of development. A country seen as developed when is able to provide qualitative life for her citizenry.1 The African continent amidst her huge human, material and natural resources has been battling with the problems of development even after many of the countries in the region has gained her independence. Economic Researchers have overtime attributed this challenge to factors like: Inadequate executive capacity responsible for the formulation and implementation of developmental policies, Lack of good governance, High level of corruption and indiscipline and mono-economic base of most countries in the region. However, European and Asian countries experience enviable growth and development patterns attributable to : the development of agricultural sector, improved mass education of her citizenry, development of indigenous industries, export oriented strategy, disciplined leadership style, existence of efficient bureaucracy human resources development, encouragement of a dynamic private sector working in cooperation with the government towards a society-wide vision of development, institutional capacity building and attention to the problems of governance, consistency and policy stability.2–4 It is said that human and technology go hand in hand. There have been rapid advances in technology in most western world and this has led to a surge of both government and public interest in automation and robotics.5–7 Robotics, which is an out spring from Mechatronics, has overtime evolved due to advances in Control Technologies and Computer Science. The rapid progress in innovative Sensors and Actuators technologies and breakthroughs in Data Science specifically in the area of Artificial Intelligence (AI) and Machine Learning (ML) have resulted in the development of Intelligent Robots.8 It is believed that the future would be a place where man and robots cohabit for the realization of man’s all round needs. In recent times, various kinds of practical assistive robots have been developed to suits mans need in different fields. For instance in the medical and welfare fields, exoskeleton power assist robots, assistive manipulators, robotic beds, and intelligent wheelchairs have been developed to cater for critical challenges in these areas by providing direct and physical assist either disabled people or their helpers.9 Several robots have been developed for the Industries, Educational sector, Aviation and Transport sectors, Commercial sectors etc. The quest to build a human like robot led to the development of Humanoids. This kind of robots which is seen as the most advanced creature of Man is envisioned to be the most ideal assistants and companion of human beings. This set of robots can interact with man and carry out functions independently. Humanoids are believed to be more efficient and effective than man in some areas as they only a stable source of power to function. As such they can be deployed to undertaking physically and mentally challenging routines.10 Humans in different situations need companion at different point in time. Cases abound where man is not able to get companion or counsel from other humans immediately when sort. This could be due to the fact that they are far away off/out of communication or they are no more. Thus, humanoids could fit in to meet this need as they communicate in a socially intelligent manner, recognizes and can learn human behaviors and psychology.11–13 Many western countries have embraces the use of Automation and Robotics in ameliorating her development challenges as: Robots have increased their productivity and competitiveness, increased productivity has lead to increased demand thus creating new job opportunities. Automation also has led overall to an increase in labor demand and positive impact on wages, and Robots complement and augment labor. Currently, the level of integration of these intelligent robots in both domestic and public environments of most third world countries like Africa has been very insignificant. This to an extent could be attributed to the Government polices; Religious and Cultural believe of the people. Most of the robots in the regions are mostly industrial .Very few available robots in the region are humanoids. Many have not been really integrated into public place and private homes. This is due to the fact that majority are of them are not built to adapt properly with the culture, religion and psychology of the people in the region. As such people find it difficult to accept these robots as companions and helpers in either homes, schools, hospitals, shopping malls, airports and other public places. If Africa is to fully embrace humanoids, there is an urgent need to build humanoids that are adaptive to the cultural, religious and psychological inclination of people in this region. To be able realize this; there is great need for more advanced research cognitive science/reasoning, adaptive learning and human",ICRA 2018,2018,10.15406/IRATJ.2018.04.00103,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
ded67de0581d3c8b410473e123a9ba0edd7237a7,https://www.semanticscholar.org/paper/ded67de0581d3c8b410473e123a9ba0edd7237a7,"Dry Rivers, Scary Strangers: Are Financial And Cyber Crises Alike?","The internet and the financial system show crucial affinities: both are tightly interconnected global networks whose orderly functioning is a prerequisite for economic prosperity. In financial and cyber crises alike, vulnerabilities are a consequence of distorted economic incentives, contagion is fast, and the most serious risk is loss of trust. Lessons learned from financial meltdowns translate to the cybersecurity world: stability cannot be achieved until policies are in place to address all of these issues. Steps have been taken to rectify incentives, as exemplified by recent European Union legislation. Data that helps identify weak nodes are still scarce, notwithstanding recent efforts. The preservation of trust is the hardest challenge: in the financial system, a global governance framework was put in place to help maintain and rebuild confidence at critical junctions, but conflicting national interests make it difficult to establish a cyber equivalent. 1 Please cite as: Biancotti, Claudia and Paolo Ciocca, “Dry Rivers, Scary Strangers: Are Financial and Cyber Crises Alike?,” in Demchak, Chris C. and Benjamin Schechter, eds. Military Cyber Affairs: Systemic Cyber Defense 3, no. 2 (2018). 2 Senior Economist at the Bank of Italy, the Italian central bank. The views here expressed are those of the authors and should not be attributed to the Bank of Italy. 3 Commissioner at Consob, the Italian Securities and Exchange Commission. The views here expressed are those of the authors and should not be attributed to Consob. 1 Biancotti and Ciocca: Are Financial And Cyber Crises Alike? Published by Scholar Commons, 2018 2 Advanced economies are immersed in cyberspace. In 2016, 95 percent of businesses in OECD countries had a broadband internet connection; 77 percent had a web presence. More than half of the adult population had purchased a product or service online, compared to 36 per cent in 2010. As digitalization progresses, a growing share of production and consumption activities depend on connectivity. From an economist’s point of view, there are evident analogies between the internet and the global financial system: both are tightly interconnected networks that provide lifeblood to the real economy, via transfers of information and funding, respectively. Indeed, a financial crisis and a cyber crisis look alike in three key dimensions: (i) vulnerabilities accumulate because of excessive risk-taking on the part of some agents, which eventually translates to systemic risk on account of interdependencies; (ii) disruption can start at a single weak point and spread to the whole system in a matter of days or even hours; (iii) the ultimate casualty is trust: once it is lost, transactions – and the whole economy – can grind to a halt as counterparties disconnect from each other. Policy responses can be deployed to address these problems so that crises can be prevented or at least managed effectively. In the financial system, safeguards have been established over time: examples are strict capital requirements for lenders, orderly resolution procedures for failing institutions, and collection of micro-level data aimed at identification of individual weak nodes. They are not all-encompassing, but they do help reduce the risk. Where cyberspace is concerned, this process is still in its infancy. Some results have been achieved with respect to (i) above. A small but insightful literature on the economics of cybersecurity points out that distorted economic incentives, rather than technically sophisticated attacks, are at the heart of the problem. Software is born vulnerable because of network externalities. For products such as operating systems and messaging platforms, the value increases with the size of the installed base; developers 4 OECD, OECD Digital Economy Outlook 2017, (Paris: OECD Publishing, 2017), 161-171. 5 European Central Bank, “The Eurosystem Household Finance and Consumption Survey: Results from the First Wave” , Statistics Papers Series no. 2 (April 2013): 7-8. 6 See among others: Ross Anderson, “Why Internet Security Is Hard – An Economic Perspective”, Proceedings of the 17 Annual Computer Security Applications Conference (December 2001). Hal Varian, “Managing Online Security Risks”, The New York Times, June 1, 2000. Tyler Moore and Ross Anderson, “Internet Security”, in The Oxford Handbook of the Internet Economy, ed. Martin Peitz and Joel Waldfogel (Oxford: Oxford University Press, 2011). 2 Military Cyber Affairs, Vol. 3 [2018], Iss. 2, Art. 7 https://scholarcommons.usf.edu/mca/vol3/iss2/7 DOI: https://doi.org/10.5038/2378-0789.3.2.1061 3 forego security as they scramble to get to the market first, attract a critical mass of users, and shut competition off. The absence of developer liability for buggy software does not help. The market for cyber defense is plagued by information asymmetries. Vendors know more than their customers: they may have an opportunity to push whatever solution maximizes their own profit, independent of how effective it is. Finally, the cost of cyber attacks in many cases is not fully internalized by the immediate victims: for example, the owner of a vulnerable IoT device that gets recruited into a botnet typically has no statutory liability (yet) for damage caused by the botnet. Some corrective measures have already been introduced, while others are being drafted. In the European Union, the General Data Protection Regulation (GDPR) – coming into force in May 2018 – imposes steep fines to businesses that put the confidentiality of personal data at risk and mandates disclosure of breaches to both authorities and data subjects. The Directive on Security of Networks and Information Systems (NIS), also coming into force this year, introduces cyber protection requirements and incident disclosure obligations for key players in sectors such as energy, finance, and healthcare. A regulation proposal put forth by the European Commission envisages an EU-wide framework for security certification of hardware and software, fashioned after the notoriously strict CE scheme for safety, health and environmental protection. These are necessary steps, but they are not enough; the effects of GDPR and NIS are confined to certain cases or sectors. More theoretical work is needed to define broader principles: generalized liability for damage caused to third parties may be a good idea, yet an ordinary citizen whose email account gets spoofed by phishers should probably not be forced to compensate victims. With respect to (ii), there is still a significant knowledge gap about the location and interconnections of weak nodes. As pointed out by the G7 Finance Ministers and Central Bank Governors in 2017, “reliable, impartial, comprehensive and widely accessible” data on the frequency and economic impact of cyber attacks are still rare. The same goes for information on network and economic connections, e.g. through digital and physical supply chains. Evidence from the Bank of Italy’s business surveys suggests that cyber risk may be concentrated among high-tech, non-ICT businesses, which are more exposed and interesting to attackers compared to low-tech ones, and less proficient at defense than the ICT sector. The data also suggest that mass adoption of relatively simple internet-based solutions, such as e-commerce 7 G7 Finance Ministers and Central Bank Governors, Bari Communiqué, May 12-13, 2017. 8 Claudia Biancotti, “The Price of Cyber (In)security: Evidence from the Italian Private Sector”, Bank of Italy Occasional Papers no. 407 (November 2017). 3 Biancotti and Ciocca: Are Financial And Cyber Crises Alike? Published by Scholar Commons, 2018 4 platforms, cloud computing services or IoT devices is a stronger risk factor than the selective adoption of advanced technologies, like machine learning or industrial robotics. These indications are vital in understanding which sectors of the economy need urgent intervention, be it in terms of awareness campaigns, dedicated incentives, or regulation. The problem sub (iii) is the hardest to solve. After the 2009 financial meltdown, trust was only restored after a series of large-scale, highly controversial injections of public money in the banking system, and substantial reinforcement of a global governance framework which encompasses broad-based organizations such as the Financial Stability Board, the Bank for International Settlements and the International Monetary Fund, regional institutions such as the European System of Financial Supervision, and national authorities. In a cyber crisis, there is no immediately evident equivalent of a public-sector backstop. Perhaps more importantly, a global governance framework is very hard to build because in cyberspace a crisis is generally triggered by an intentional act of aggression; an adversary is in the picture and may serve, directly or indirectly, the interests of a nation-state. In this sense, the right comparison might be with currency or trade wars; the problem is mostly one of political, diplomatic and military relations, especially in a world where the weight of authoritarian governments increases. This is where input from economics is not sufficient and must be complemented by scholarship in the various facets of international relations. 4 Military Cyber Affairs, Vol. 3 [2018], Iss. 2, Art. 7 https://scholarcommons.usf.edu/mca/vol3/iss2/7 DOI: https://doi.org/10.5038/2378-0789.3.2.1061",Military Cyber Affairs,2018,10.5038/2378-0789.3.2.1061,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
d45b79eafa5d9de90ae2705bf861776739c0f58f,https://www.semanticscholar.org/paper/d45b79eafa5d9de90ae2705bf861776739c0f58f,TuLUMIS-a tunable LED-based underwater multispectral imaging system,"Multispectral imaging (MSI) is widely used in terrestrial applications to help increase the discriminability between objects of interest. While MSI has shown potential for underwater geological and biological surveys, it is thus far rarely applied underwater. This is primarily due to the that fact light propagation in water is subject to wavelength dependent attenuation and tough working conditions in the deep ocean. In this paper, a novel underwater MSI system based on a tunable light source is presented which employs a monochrome still image camera with flashing, pressure neutral color LEDs. Laboratory experiments and field tests were performed. Results from the lab experiments show an improvement of 76.66% on discriminating colors on a checkerboard by using the proposed imaging system over the use of an RGB camera. The field tests provided in situ MSI observations of pelagic fauna, and showed the first evidence that the system is capable of acquiring useful imagery under real marine conditions. © 2018 Optical Society of America under the terms of the OSA Open Access Publishing Agreement OCIS codes: (010.4450) Oceanic optics; (110.4234) Multispectral and hyperspectral imaging; (330.6180) Spectral discrimination; (120.4820) Optical systems. References and links 1. T. A. Carrino, A. P. Crósta, C. L. B. Toledo, and A. M. Silva, “Hyperspectral remote sensing applied to mineral exploration in Southern Peru: A multiple data integration approach in the chapi chiara gold prospect,” Int. J. Appl. Earth Obs. 64, 287–300 (2018). 2. H. Pu, D. Liu, J.-H. Qu, and D.-W. Sun, “Applications of imaging spectrometry in inland water quality monitoring-a review of recent developments,” Water, Air, & Soil Pollution 228, 131 (2017). 3. V. Leemans, G. Marlier, M.-F. Destain, B. Dumont, and B. Mercatoris, “Estimation of leaf nitrogen concentration on winter wheat by multispectral imaging,” Proc. SPIE 10213, 102130I (2017). 4. A. I. Ropodi, E. Z. Panagou, and G.-J. E. Nychas, “Multispectral imaging (MSI): A promising method for the detection of minced beef adulteration with horsemeat,” Food Control 73, 57–63 (2017). 5. G. Johnsen, Z. Volent, E. Sakshaug, F. Sigernes, and L. H. Pettersson, Remote sensing in the Barents Sea (Tapir Academic, 2009), Chap. 6. 6. G. Johnsen, Z. Volent, H. Dierssen, R. Pettersen, M. Van Ardelan, F. Søreide, P. Fearns, M. Ludvigsen, and M. Moline, “Underwater hyperspectral imagery to create biogeochemical maps of seafloor properties,” in Subsea Optics and Imaging, J. Watson, O. Zielinski, eds. (Woodhead, 2013). 7. J. Tegdan, S. Ekehaug, I. M. Hansen, L. M. S. Aas, K. J. Steen, R. Pettersen, F. Beuchel, and L. Camus, “Underwater hyperspectral imaging for environmental mapping and monitoring of seabed habitats,” in Proceedings of IEEE/MTS OCEANS’15 (IEEE, 2015), pp. 1–6. 8. G. Johnsen, M. Ludvigsen, A. Sørensen, and L. M. S. Aas, “The use of underwater hyperspectral imaging deployed on remotely operated vehicles methods and applications,” IFAC-PapersOnLine 49, 476–481 (2016). 9. A. A. Mogstad and G. Johnsen, “Spectral characteristics of coralline algae: a multi-instrumental approach, with emphasis on underwater hyperspectral imaging,” Appl. Opt. 56, 9957–9975 (2017). 10. Ø. Sture, M. Ludvigsen, and L. M. S. Aas, “Autonomous underwater vehicles as a platform for underwater hyperspectral imaging,” in Proceedings of IEEE/MTS OCEANS’17 (IEEE, 2017), pp. 1–8. 11. D. L. Bongiorno, M. Bryson, T. C. Bridge, D. G. Dansereau, and S. B. Williams, “Coregistered hyperspectral and stereo image seafloor mapping from an autonomous underwater vehicle,” J. Field Robot. (2017). 12. L. Bian, J. Suo, G. Situ, Z. Li, J. Fan, F. Chen, and Q. Dai, “Multispectral imaging using a single bucket detector,” Sci. Rep. -UK 6, 24752 (2016). Vol. 26, No. 6 | 19 Mar 2018 | OPTICS EXPRESS 7811 #320160 https://doi.org/10.1364/OE.26.007811 Journal © 2018 Received 19 Jan 2018; revised 12 Feb 2018; accepted 12 Feb 2018; published 16 Mar 2018 13. S. Jin, W. Hui, Y. Wang, K. Huang, Q. Shi, C. Ying, D. Liu, Q. Ye, W. Zhou, and J. Tian, “Hyperspectral imaging using the single-pixel fourier transform technique,” Sci. Rep. -UK 7, 45209 (2017). 14. X. Cao, T. Yue, X. Lin, S. Lin, X. Yuan, Q. Dai, L. Carin, and D. J. Brady, “Computational snapshot multispectral cameras: Toward dynamic capture of the spectral world,” IEEE Signal Proc. Mag. 33, 95–108 (2016). 15. M. K. Griffin and H.-h. K. Burke, “Compensation of hyperspectral data for atmospheric effects,” Lincoln Laboratory Journal 14, 29–54 (2003). 16. C. Mobley, E. Boss, and C. Roesler, Ocean Optics Web Book http://www.oceanopticsbook.info/. 17. I. Vasilescu, C. Detweiler, and D. Rus, “Color-accurate underwater imaging using perceptual adaptive illumination,” Auton. Robot. 31, 285 (2011). 18. I. Leiper, S. Phinn, and A. G. Dekker, “Spectral reflectance of coral reef Benthos and substrate assemblages on Heron Reef, Australia,” Int. J. Remote Sens. 33, 3946–3965 (2012). 19. T. Treibitz, B. P. Neal, D. I. Kline, O. Beijbom, P. L. Roberts, B. G. Mitchell, and D. Kriegman, “Wide field-of-view fluorescence imaging of coral reefs,” Sci. Rep. -UK 5, 7694 (2015). 20. D. G. Zawada and C. H. Mazel, “Fluorescence-based classification of caribbean coral reef organisms and substrates,” PloS one 9, e84570 (2014). 21. H. Holden and E. LeDrew, “Hyperspectral discrimination of healthy versus stressed corals using in situ reflectance,” J. Coastal Res. 850–858 (2001). 22. A. Chennu, P. Färber, G. De’ath, D. de Beer, and K. E. Fabricius, “A diver-operated hyperspectral imaging and topographic surveying system for automated mapping of benthic habitats,” Sci. Rep. -UK 7, 7122 (2017). 23. R. Pettersen, G. Johnsen, P. Bruheim, and T. Andreassen, “Development of hyperspectral imaging as a bio-optical taxonomic tool for pigmented marine organisms,” Org. Divers. Evol. 14, 237–246 (2014). 24. P. A. Letnes, I. M. Hansen, L. M. Aas, I. Eide, R. Pettersen, L. Tassara, J. Receveur, S. le Floch, J. Guyomarch, L. Camus, and J. Bytingsvik, “Underwater hyperspectral classification of deep sea corals exposed to a toxic compound,” bioRxiv (2017). 25. Y. Guo, H. Song, H. Liu, H. Wei, P. Yang, S. Zhan, H. Wang, H. Huang, N. Liao, Q. Mu, J. Leng, and W. Yang, “Model-based restoration of underwater spectral images captured with narrowband filters,” Opt. Express 24, 13101–13120 (2016). 26. H. R. Morris, C. C. Hoyt, and P. J. Treado, “Imaging spectrometers for fluorescence and raman microscopy: acousto-optic and liquid crystal tunable filters,” Appl. Spectrosc. 48, 857–866 (1994). 27. A. Gleason, R. Reid, and K. Voss, “Automated classification of underwater multispectral imagery for coral reef monitoring,” in Proceedings of IEEE/MTS OCEANS’07 (IEEE, 2007), pp. 1–8. 28. J.-I. Park, M.-H. Lee, M. D. Grossberg, and S. K. Nayar, “Multispectral imaging using multiplexed illumination,” in Proceedings of IEEE Conference on Computer Vision (IEEE, 2007), pp. 1–8. 29. H. Blasinski and J. Farrell, “Computational multispectral flash,” in Proceedings of IEEE Conference on Computational Photography (IEEE, 2017), pp. 1–10. 30. M. B. Bouchard, B. R. Chen, S. A. Burgess, and E. M. Hillman, “Ultra-fast multispectral optical imaging of cortical oxygenation, blood flow, and intracellular calcium dynamics,” Opt. Express 17, 15670–15678 (2009). 31. X. Delpueyo, M. Vilaseca, S. Royo, M. Ares, L. Rey-Barroso, F. Sanabria, S. Puig, J. Malvehy, G. Pellacani, F. Noguero, G. Solomita, and T. Bosch, “Multispectral imaging system based on light-emitting diodes for the detection of melanomas and basal cell carcinomas: a pilot study,” J. Biomed. Opt. 22, 065006 (2017). 32. D. Swinehart, “The beer-lambert law,” J. Chem. Educ 39, 333 (1962). 33. Y. Du, C.-I. Chang, H. Ren, C.-C. Chang, J. O. Jensen, and F. M. D’Amico, “New hyperspectral discrimination measure for spectral characterization,” Opt. Eng. 43, 1777–1786 (2004). 34. C. M. Bishop, Pattern Recognition and Machine Learning (Springer, 2006). 35. D. Manolakis, D. Marden, and G. A. Shaw, “Hyperspectral image processing for automatic target detection applications,” Lincoln Laboratory Journal 14, 79–116 (2003). 36. Itseez, “Open source computer vision library,” https://github.com/opencv/opencv (2017). 37. J. Sticklus and T. Kwasnitschka, “Verfahren und vorrichtung zur herstellung von in vergussmasse vergossenen leuchten,” (2015). DE Patent 102,014,118,672. 38. Lumileds Holding B.V., “DS105 LUXEON Z color line product datasheet,” https://www.lumileds.com/ uploads/415/DS105-pdf (2017). 39. D. Akkaynak, E. Chan, J. J. Allen, and R. T. Hanlon, “Using spectrometry and photography to study color underwater,” in Proceedings of IEEE/MTS OCEANS’11 (IEEE, 2011), pp. 1–8. 40. D. Coffin, “DCRaw Version 9.27,” https://www.cybercom.net/~dcoffin/dcraw/ (2016). 41. B. Fiedler, “Short cruise report RV Maria S. Merian MSM61,” https://www.ldf.uni-hamburg.de/ merian/wochenberichte/wochenberichte-merian/msm58-2-msm61/msm61-scr.pdf (2017).",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bb6fb2823d82c7d41a9c66d8897727cb5445277c,https://www.semanticscholar.org/paper/bb6fb2823d82c7d41a9c66d8897727cb5445277c,Cognitive Acoustic Analytics Service for Internet of Things,"The rapid development of the Internet of Things (IoT) has brought great changes for non-contact and non-destructive sensing and diagnosis. For every inanimate object can tell us something by the sound it makes, acoustic sensor demonstrates great advantages comparing to conventional electronic and mechanic sensors in such cases: overcoming environmental obstacles, mapping to existing use cases of detecting problems with human ears, low cost for deployment, etc. It could be widely applied to various domains, such as predictive maintenance of machinery, robot sensory, elderly and baby care in smart home, etc. Whether we can use the acoustic sensor data to understand what is happening and to predict what will happen relies heavily on the analytics capabilities we apply to the acoustic data, which has to overcome the obstacles of noise, disturbance and errors, and has to meet the requirement of real-time processing of high volume signals with large number of sensors. In this paper, we propose a scalable cognitive acoustics analytics service for IoT that provides the user an incremental learning approach to evolve their analytics capability on non-intuitive and unstructured acoustic data through the combination of acoustic signal processing and machine learning technology. It first performs acoustic signal processing and denoising, enables acoustic signal based abnormal detection based on sound intensity, spectral centroid, etc. Then based on the accumulated abnormal data, a supervised learning method is performed as baseline and a neural network based classifier is used to recognize acoustic events in different scenarios with various volume of sample data and requirement of accuracy. In addition, acoustic sensor arrays processing is supported for localization of moving acoustic source in more complex scenario. In this paper, we designed a hybrid computing structure. Finally, we conduct experiments on acoustic event recognition for machinery diagnosis, and show that the proposed system can achieve high accuracy.",2017 IEEE International Conference on Cognitive Computing (ICCC),2017,10.1109/IEEE.ICCC.2017.20,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
bd1a8094a14f68528d8910b7455f630e6d4cd36c,https://www.semanticscholar.org/paper/bd1a8094a14f68528d8910b7455f630e6d4cd36c,SIGLOG monthly 198,"Deadline: January 15th, 2018 Submission Deadline: January 22nd, 2018 Rebuttal: March 22 25th, 2018 Notification: April 2nd, 2018 Camera-Ready: May 2nd, 2018 FSCD Conference: July 9 12th, 2018 FLoC Conference: July 6 19th, 2018 * PROGRAM COMMITTEE CHAIR Helene Kirchner, Inria * CONFERENCE & WORKSHOP CHAIR: Paula Severi, Leicester U. 13TH WORKSHOP ON GAMES FOR LOGIC AND PROGRAMMING LANGUAGES (GaLoP 2018) Call for Abstracts Thessaloniki, Greece, 14-15 April, 2018 http://www.gamesemantics.org * GaLoP is an annual international workshop on game-semantic models for logics and programming languages and their applications. This is an informal workshop that welcomes work in progress, overviews of more extensive work, programmatic or position papers and tutorials. * GaLoP XII will be held in Thessaloniki, Greece, on 14-15 April 2018 as a satellite workshop of ETAPS (http://www.etaps.org/). * IMPORTANT DATES Submission: 22 January 2018 Notification: 12 February 2018 Workshop: 14-15 April 2018 * Invited talks Guy McCusker (Bath) Matteo Mio (Lyon) Ulrich Schopp (Munchen) * Programme Chairs Ugo Dal Lago (Bologna, co-chair) Gabriel Sandu (Helsinki, co-chair) 4th ACM CYBER-PHYSICAL SYSTEM SECURITY WORKSHOP (ACM CPSS’18) Incheon, Korea, June 4, 2018 (in conjunction with ACM AsiaCCS’18) http://jianying.5gbfree.com/cpss/CPSS2018/ Extended submission deadline: Jan 27, 2018 (23:59 GMT) * CONFERENCE OUTLINE Cyber-Physical Systems (CPS) consist of large-scale interconnected systems of heterogeneous components interacting with their physical environments. There are a multitude of CPS devices and applications being deployed to serve critical functions in our lives. This workshop will provide a platform for professionals from academia, government, and industry to discuss how to address the increasing security challenges facing CPS. Besides invited talks, we also seek novel submissions describing theoretical and practical security solutions to CPS. Papers that are pertinent to the security of ACM SIGLOG News 37 January 2018, Vol. 5, No. 1 embedded systems, IoT, SCADA, smart grid, and critical infrastructure networks are all welcome. * IMPORTANT DATES Extended submission deadline: Jan 27, 2018 (23:59 GMT) Notification: Mar 10, 2018 Camera-ready due: Mar 31, 2018 * Program Chairs: Dieter Gollmann (Hamburg University of Technology, Germany & NTU, Singapore) Jianying Zhou (SUTD, Singapore) * Further information CPSS Home: http://jianying.5gbfree.com/cpss/ Email: cpss2018@easychair.org CONTINUITY, COMPUTABILITY, CONSTRUCTIVITY: FROM LOGIC TO ALGORITHMS 2017 Second Call for Submissions (Postproceedings) Deadline for submission: 1 February 2018 * After the successful start of the new EU-MSCA-RISE project ""Computing with Infinite Data"" (CID) and the excellent Workshop CCC 2017 in Nancy (France) in June this year, we are planning to publish a collection of papers dedicated to the meeting and to the project as a Special Issue in the open-access journal LOGICAL METHODS IN COMPUTER SCIENCE. * The issue should reflect progress made in Computable Analysis and related areas, and is not restricted to work in the CID project or presented at the Workshop. * Submissions are welcome from all scientists on topics in the entire spectrum from logic to algorithms including, but not limited to: Exact real number computation, Correctness of algorithms on infinite data, Computable analysis, Complexity of real numbers, real-valued functions, etc. Effective descriptive set theory, Constructive topological foundations, Scott’s domain theory, Constructive analysis, Category-theoretic approaches to computation on infinite data, Weihrauch degrees, Randomness and computable measure theory, Other related areas. * EDITORS: Ulrich Berger (Swansea, UK) Pieter Collins (Maastricht, NL) Mathieu Hoyrup (Nancy, FR) Victor Selivanov (Novosibirsk, RUS) Dieter Spreen (Siegen, DE) Martin Ziegler (KAIST, KR) * DEADLINE FOR SUBMISSION: 1 February 2018 * If you intend to submit a paper for the special issue, please inform us by sending email to: spreen@math.uni-siegen.de by 1 January 2018 COMPUTABILITY IN EUROPE: SAILING ROUTES IN THE WORLD OF COMPUTATION (CiE 2018) Final Call for Papers ACM SIGLOG News 38 January 2018, Vol. 5, No. 1 Kiel, Germany July 30 August 3, 2018 http://cie2018.uni-kiel.de * CiE 2018 is the fourteenth conference organized by CiE (Computability in Europe), a European association of mathematicians, logicians, computer scientists, philosophers, physicists and others interested in new developments in computability and their underlying significance for the real world. * THE PROGRAMME COMMITTEE cordially invites all researchers (European and non-European) to submit their papers in all areas related to computability for presentation at the conference and inclusion in the proceedings at https://easychair.org/conferences/?conf=cie2018 Submission guidelines are available on the conference web-site. * The CONFERENCE PROCEEDINGS will be published by LNCS, Springer Verlag. * IMPORTANT DATES: Deadline for abstract submission: January 17, 2018 Deadline for article submission: February 1, 2018 Notification of acceptance: April 6, 2018 Early registration before: May 30, 2018 * TUTORIAL SPEAKERS: Pinar Heggernes (Bergen, Norway), Bakhadyr Khoussainov (Auckland, NZ) * INVITED SPEAKERS: Kousha Etessami (Edinburgh, UK) Johanna Franklin (Hempstead, US) Mai Gehrke (Nice, France) Alberto Marcone (Udine, Italy) Alexandra Silva (London, UK) Jeffrey O. Shallit (Waterloo, Canada) * SPECIAL SESSIONS: Approximation and Optimisation, Bioinformatics and Bio-inspired Computing, Computing with Imperfect Information, Continuous Computation, History and Philosophy of Computing, SAT-Solving. * WORKSHOP: Women in Computability. * Check the web-site for further details on the conference, such as organisation and grants. SPECIAL ISSUE OF IJAR ON ""DEFEASIBLE AND AMPLIATIVE REASONING"" Call for Papers * Classical reasoning is not flexible enough when directly applied to the formalization of certain nuances of decision making as done by humans. These involve different kinds of reasoning such as reasoning with uncertainty, exceptions, similarity, vagueness, incomplete or contradictory information and many others. * Everyday reasoning usually shows the two salient intertwined aspects below: Ampliative aspect: augmenting the underlying reasoning by allowing more conclusions. Defeasible aspect: curtailing the underlying reasoning by either disregarding or disallowing some conclusions that somehow ought not ACM SIGLOG News 39 January 2018, Vol. 5, No. 1 to be sanctioned. * This special issue aims at bringing together work on defeasible and ampliative reasoning from the perspective of artificial intelligence, cognitive sciences, philosophy and related disciplines in a multi-disciplinary way, thereby consolidating the mission of the DARe workshop series. * The submission url is: http://www.evise.com/evise/jrnl/IJA * When submitting your manuscript, please select ‘‘VSI:DARe special issue’’ as the article type. * If you have any enquiries, please feel free to contact us at dare.to.contact.us@gmail.com * IMPORTANT DATES Submission deadline: 15 February 2018 Notification: 1 November 2018 Publication date: 1 January 2019 * Guest editors Richard Booth, Cardiff University, UK Giovanni Casini, University of Luxembourg Szymon Klarman, Semantic Integration Ltd., UK Gilles Richard, Universite Paul Sabatier, France Ivan Varzinczak, CRIL, Univ. Artois & CNRS, France THE 17TH IEEE INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS AND COGNITIVE COMPUTING (ICCI*CC’18) UC Berkeley, CA, USA July 15-18, 2018 http://www.ucalgary.ca/icci_cc/iccicc-18 * The IEEE ICCI*CC series is a flagship conference of its field sponsored by IEEE Computer, Computational Intelligence and SMC Societies. The theme of ICCI*CC’18 is on Cognitive Machine Learning, Brain-Inspired Systems and Cognitive Robotics. * You are welcome to submit a paper to IEEE ICCI*CC’18 or to organize a special session related to the theme of the conference. The Proceedings of ICCI*CC’18 will be published by IEEE CS Press (EI Indexed). A good rate of selected papers from the proceedings will be recommended to leading international journals and/or IEEE transactions with ISI/EI indexes. * IMPORTANT DATES Submission deadline: February 16, 2018. 25th WORKSHOP ON LOGIC, LANGUAGE, INFORMATION AND COMPUTATION (WoLLIC 2018) Call for Papers July 24th-27th, 2018, Bogota, Colombia http://wollic.org/wollic2018/ * WoLLIC is an annual international forum on inter-disciplinary research involving formal logic, computing and programming theory, and natural language and reasoning. Each meeting includes invited talks and tutorials as well as contributed papers. * Contributions are invited on all pertinent subjects, with particular ACM SIGLOG News 40 January 2018, Vol. 5, No. 1 interest in cross-disciplinary topics. Typical but not exclusive areas of interest are: foundations of computing and programming; novel computation models and paradigms; broad notions of proof and belief; proof mining, type theory, effective learnability; formal methods in software and hardware development; logical approach to natural language and reasoning; logics of programs, actions and resources; foundational aspects of information organization, search, flow, sharing, and protection; foundations of mathematics; philosophy of mathematics; philosophy of language; philosophical logic. * IMPORTANT DATES: Mar 11, 2018: Paper title and abstract deadline Mar 18, 2018: Full paper deadline Apr 15, 2018: Author notification Apr 22, 2018: Final version deadline (firm). * Further details: http://wollic.org/wollic2018/ WOMEN IN LOGIC WORKSHOP (WiL 2018) Call for Papers July 8, 2018, Oxford UK https://sites.google.com/site/womeninlogic2018/welcome/ * Affiliated with LICS (http://lics.siglog.org/lics18/) Held as part of FLoC (http://www.floc2018.org/) * We are holding the 2nd Women in Logic (WiL) workshop as a LICS 2018 associated w",SIGL,2018,10.1145/3183645.3183651,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
2fd91d32c82d43297d41f4cd514e8beef117f7cd,https://www.semanticscholar.org/paper/2fd91d32c82d43297d41f4cd514e8beef117f7cd,Implementation of Modern Technology in Agriculture,"This Agribot is a robot designed for agricultural purposes. As one of the trends of development on automation and intelligence of agricultural machinery in the 21st century, all kinds of agricultural robots have been researched and developed to implement a number of agricultural productions in many countries. This Bot can perform basic elementary functions like picking, harvesting, weeding, pruning, planting, grafting. Agricultural robots or Agribot is a robot deployed for agricultural purposes. The main area of application of robots in agriculture today is at the harvesting stage. Emerging applications of robots or drones in agriculture include weed control, cloud seeding, planting seeds, harvesting, environmental monitoring and soil analysis. Some Examples of Computer Vision Guided Farm Robots Include: Drones which can be fitted with the latest multi spectral, network-connected sensors in order to image crops or the environment, survey the landscape, and even analyze the fertility of certain areas of soil then send this data across the network to be analyzed and processed. Harvesting Robots which are not only able to pick the fruit but using their computer vision programs, can sort the produce based on size and ripeness. Weeding and Spraying Robots with computer based vision systems are being deployed on tractors to automate spraying for weed control. With the use of artificial intelligence and machine learning techniques to enhance the precision of processes such as weeding. The motivation for this project came from the countries where economy is based on agriculture. Our idea presents a system with high speed of operation for an advanced agriculture process which includes grass cutter, Seed sowing, Sprinkler, Ploughing and solar panel for battery charging. The robotic system is an electromechanical (conveys a sense that it has agency of its own) and artificial agent which is steered by DC motor which has four wheels. Generally in farm lots of time consumes for grass cutting, seed sowing and spraying work, here is an approach to reduce farmer time for cultivation and increase farm efficiency by using multipurpose agricultural robot. The key objectives of this research work are to design and evaluate a novel and effectual Agribot for the agriculture and cultivation applications to assist the farmers. The work is based on the evaluation of projected design on assorted parameters to analyze the overall integrity. Keywords— Agribot, Grass cutter, Seed sowing, Sprinkler, Ploughing and solar panel.",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
57b672bbd41fa4fae7b4307191978599513107f4,https://www.semanticscholar.org/paper/57b672bbd41fa4fae7b4307191978599513107f4,Data Engineering Project (Educating For The Future PhUSE WG),"With an expected 100% increase, over the next 3 years, of data from non-EDC sources (such as smartphones, wearables and custom apps) the traditional methods of managing data for clinical trials presents executives with a resourcing headache. As such, many companies are looking for lower cost strategies to sure up this shortfall in resourcing. However, citing case studies from other industries, there are new methodologies/technologies in data engineering which could enable automation of much of the “heavy-lifting” currently practiced in clinical data management and statistical programming. This paper discusses the Data Engineering Project within the PhUSE Computational Science (CS) Working Group, Educating For The Future, with a view to educate clinical data managers in data engineering principles so that they can be prepared, equipped and effective in dealing with the coming “data tsunami” heading to the shores of clinical research. INTRODUCTION Did you realise we are living in the age of the Fourth Industrial Revolution? Perhaps you have been busy downloading a myriad of “apps” designed to make your life easier or connecting on social media, uncovering relationships and associations you didn’t even know you had. Perhaps you have been shopping a global marketplace, comparing prices, quality and availability, all at your fingertips and in a minutes’ time. While this has been happening, the Fourth Industry Revolution has been evolving at exponential proportions ​. Just ask Siri! The term “Industrie 4.0”, was originated in Germany, as a government-led initiative, to transform manufacturing through advanced digital capability. Thus creating the concept of a “smart factory”, based on four key design principles ​: 1. Interconnection of machines, devices, sensor and people 2. Vast amounts of useful information (data) to drive decision making 3. Technical assistance to aid humans, for example to visualise data or to perform tasks that may be of safety concern for a human. 4. The use of cyber-physical systems to make decisions on their own and to perform tasks as autonomously as possible. Emerging from the premise of “Industrie 4.0” is the advent of the term “The Fourth Industrial Revolution” (also referred to as “4IR” or “I4.0”). This term originated in 2016 when described by Klaus Schwab (Founder and Executive Chairman of the World Economic Forum), as a “technological revolution that will fundamentally alter the way we live, work, and relate to one another”. Klaus goes on to describe it as a digital revolution with innovative uses of a combination of technologies that build upon the premise of the third revolution (i.e. electronics and information technology to automate production). As a result, emerging technologies have brought forth advancements in fields such as ​artificial intelligence, robotics, the Internet of Things, autonomous vehicles, 3D printing, nanotechnology, biotechnology, materials science, energy storage, and quantum computing. This rapid evolution will undoubtedly affect industries world-wide, already disrupting many industries, such as travel agencies, video rentals and bookstores​ . The pharmaceutical industry is also experiencing the impacts of I4.0. Digital and mobile technologies has brought on significant advancements in data acquisition and accessibility as it relates to health care and patient data. As reported in the Tufts-Veeva eClinical Landscape study in 2017, data coming from sources such as, smartphones, custom applications, and mobile health are expected to double in the next 3 years ​. Therefore requiring greater capabilities in handling large volumes of data, as well as data from coming in through various data streams and 1 PhUSE EU Connect 2018 formatting. As with other industries, data will become a critical asset to their business and the effective utilisation of this data can play a critical role in driving growth in the business and bringing novel therapies to the patients who need them. In this paper, we will focus on the works of the Data Engineering Project within the Educating For The Future Working Group. With the formation of the Working Group in early 2018, the team had taken on the mission to explore how data engineering techniques, successfully deployed in other industries, could be utilised in the pharmaceutical industry, with a goal to ​facilitate the education ​of the pharmaceutical industry on these techniques. We will share with you some introductory information about Data Engineering and Data Science and explore how embracing new data engineering techniques may affect the industry culture. You will learn about use cases of Data Engineering in other industries and how advances in digital capability have affected their business model. We will also share some of the many software packages and tools available to enable automation, commonly used in Data Engineering and Data Science. Finally we will reflect on the benefits that data standardisation has brought to the pharmaceutical industry and share our vision for disseminating information to facilitate your learning going forward. DATA ENGINEERING To start this learning journey, exploring the term “Data Engineering” opens the door to the vast opportunities and roles available today centered around data. In doing a simple search on the internet, ​“what is data engineering?​”, one will find many posts expressing their understanding of Data Engineering with some variation but also some similarity. However, what is clear is that Data Engineering encompasses the many considerations that need to be taken into account to optimally curate, transform, secure and disseminate data suitable for analysis. As technology and tools have become more advanced, building such a platform and infrastructure requires engineers and architects of both general and specific expertise. The Data Engineer combines knowledge in areas such as software development, infrastructure, data architecture, data warehousing, cloud technology and data cleaning in order to design, build and test solutions that define the pipelines of data throughout the enterprise, making the data accessible to the organisation.​ [5] [27] [31] Optimised Data Engineering appropriately balances the efficiency of an automated process against the cost of development and maintenance of that process, ensuring repetitive processes that require humans to write code, press keys, cut-and-paste and update documents are minimised or eliminated. DATA SCIENCE Often paired with the term “Data Engineering” is also the term “Data Science”. According to Kelle O’Neal and Charles Roe: “Data Science allows enterprises the ability to turn their data assets into a narrative. Data Science allows that narrative to be expanded across timelines, in different data spaces that trace from the past into the future, with much more involved questions and answers about an enterprise, different potential outcomes, and repercussions based on recommendations. Data Science employs a range of mathematical, business, and scientific techniques to solve complex problems about an organisation’s data assets.” ​ In contrast, the focus of the Data Engineer is on the process from data curation to dissemination and the focus of the Data Scientist is on the analytics of the data, thus extracting knowledge from the data. To achieve quality data capture, near-real-time accessibility and meaningful analytics, one cannot function without the other, and effective teamwork optimises the value of each role. As such, an analytics team would be composed of distinct roles/capabilities​ : ● Data Engineers (in areas such as database architecture, database development, machine learning architecture, ETL scripting , etc.) ● Data Scientists ● Business Analysts Data Engineering brings together the broad expertise, of these roles, to ensure the data are curated and accessible to the Data Scientist, and in our environment today, this process is becoming more and more complex. Therefore, 2 PhUSE EU Connect 2018 expertise in curating big-data and data of varying formats (structured and unstructured) is a critical core competency to optimise the potential impact of these digital assets (i.e. the data). The Data Scientist works deep in the data, utilizing various tools and techniques to discover patterns in the data that may drive decision making for the business. Optimising utilisation of the data to enable accurate conclusions can bear greater value to the organisation. As an example, per Tom Eunice’s post, “a fraud-detection algorithm may be very accurate when based on many months of historical data. However, months of historical data may not always be available. Designing a fraud-detection model that is still accurate using historical data from only a few days would be of more use and more practical to implement.” ​ The Business Analyst helps the Data Scientist understand the meaning of the data and the relevance of any discovered relationships. Initially, uncovering relationships in the data and upon further investigation, identifies meaningful patterns that may reveal information that otherwise may not have been known. ​ As you will see in the sections to follow, the full complement of the roles in an analytics team is what drives the business value. One discipline without the other (e.g. data engineering without data science) will result in missed opportunities. In the sections to follow, we often refer to Data Engineering, however, due to the close ties to Data Science, some examples elude to both Data Engineering and Data Science. USE CASES FROM OTHER INDUSTRIES In this section, we present three use cases from the transportation, retail, and agricultural industry. The use cases illustrate the importance and usage of Data Engineering. In each example the data collected, the consumer of the data, and the value of the organisation is reviewed. Similarities and potential applications to the pharmaceutical industry are discussed. UBER When",,2018,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9738fddf9ab49ca9459f2d93182a83ca01e5b7a8,https://www.semanticscholar.org/paper/9738fddf9ab49ca9459f2d93182a83ca01e5b7a8,Prediction of Component Failures of Telepresence Robot with Temporal Data,"With recent advances in computer and sensor technologies in the last few decades, the use of robots for various applications has increased enormously. The reliability of robots depends on the minimization of component failures and downtime. To improve the reliability, periodic monitoring of components and their behavior are essential to inference component fatigue and potential breakdowns. Since fully autonomous robots are very expensive, telepresence robots are affordable for mass scale deployment and can be controlled by a trained human operator like avatars. To increase the efficiency and to reduce the downtime of telepresence robot service, it is essential to observe the various commands performed on the robot and to analyze the samples of component status over a long period. We propose an efficient data driven model with a collection of frequent time-stamped data from various components of a telepresence robot and predict potential failure warnings. The collected historical datasets are analyzed to determine an accurate machine learning model for increased failure prediction of components. Analysis of this large collection of data will be performed on a cloud computing platform to alleviate the computational load on telepresence robots. With the incoming temporal data, this machine learning model predicts the component status and probability of failure in real-time. Potential Applications of the proposed approach also includes detection of component malfunction, estimating the degree of movement of various components for satisfactory level of performance, and migration of workload among multiple telepresence robots in a team work environment.",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
80040e993990f21db4741381c02165c3db06ace0,https://www.semanticscholar.org/paper/80040e993990f21db4741381c02165c3db06ace0,Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain,"One of the elements missing from virtual environments in the emerging cyber domain is an element of active opposition. For example, in a training simulation the instructor assigns the student a task or objective, and the student then practices within the environment (the “cyber range”) until they feel comfortable with the task or are able to demonstrate the requisite level of mastery. The environment may have static defenses, such as access control or firewalls, or a fixed set of intrusion methods to defend against, but it typically lacks any active opposition that might adapt defensive or offensive actions (e.g., monitor logs, blocked connections, exploit switching or information gathering). This is akin to training fighter pilots against adversaries who know how to use their weapons, but do not have any tactical or strategic goals beyond that. This is unfortunate for two reasons: 1) it trains cyber operators to behave as though opponents do not have a tangible existence or do not have higher-level goals, and 2) it ignores an opportunity to tailor the student’s learning experience through adjustable adversary behavior. Cognitive agents have the potential to transform the cyber operations training experience. The application of cognitive agents to the roles of cyber offense and defense would provide a more complete cyber ecology for training purposes and thus a more realistic training experience for the student. There are two key challenges to creating such cyber agents: 1) modeling the complex, and continually evolving, processes of cyber operations within a cognitive architecture, and 2) defining the tools and data standards to enable cognitive agents to interoperate with networks in a portable way. This paper discusses novel models of cyber offensive and defensive behavior based on observation and elaboration of human expertise, as well as an approach to the creation of software adapters that translate from task-level actions to network-level events to support agent-network interoperability. ABOUT THE AUTHORS Randolph M. Jones, PhD, is a senior artificial intelligence engineer at Soar Technology, and co-founded Soar Technology in 1998. Dr. Jones received his BS in mathematics and computer science from UCLA, and he received his M.S. and Ph.D. in information and computer science from the University of California, Irvine. Ryan O’Grady is the technical lead for Soar Technology’s emerging business area in cyberspace training and visualization, and a senior software engineer in the Intelligent Training business area. Mr. O’Grady received a BSE in Computer Science Engineering from the University of Michigan in 2004. Certifications: Security+, CPTE, OSCP Denise Nicholson, PhD, CMSP, is the Director of Soar Technology’s new Technology Area ""X"" leading an effort to explore, identify and pursue innovative applications of intelligent systems for critical and challenging problems, such as Cyber Security. Dr. Nicholson has a Ph.D. and M.S. in Optical Sciences from the University of Arizona, and a B.S. in Electrical Computer Engineering from Clarkson University. Robert Hoffman, PhD, is a Senior Research Scientist at the Florida Institute for Human and Machine Cognition (IHMC). He is senior editor of the Department on Human-Centered Computing of IEEE: Intelligent Systems. His latest book is Accelerated Expertise: Training to High Proficiency in A Complex World (2014, Taylor & Francis). Larry Bunch is a Senior Research Associate at IHMC. He received his BS in computer science from the University of West Florida and has published extensively concerning software agents, semantic policies and reasoning, and large-scale event visualizations. Jeffrey M. Bradshaw, PhD, is a Senior Research Scientist at IHMC. He co-edits the HCC Department of IEEE Intelligent Systems and has published widely in software agents, semantic technologies, digital policy management, and human-agent-robot teamwork (HART). Ami Bolton, PhD, is a Program Officer at the Office of Naval Research (ONR). Her programs focus on enhancing individual and team decision-making and combat effectiveness through advances that improve perception, cognition, and team coordination. Dr. Bolton received a M.S. in Human Factors from the Florida Institute of Technology, and Ph.D. in Applied Experimental & Human Factors Psychology from University of Central Florida. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 2 of 10 Modeling and Integrating Cognitive Agents Within the Emerging Cyber Domain Randolph M. Jones, Ryan O'Grady, Denise Nicholson, Robert Hoffman, Larry Bunch, Jeffrey Bradshaw, and Ami Bolton Soar Technology IHMC Office of Naval Research Ann Arbor, MI Pensacola, FL Arlington, VA rjones@soartech.com, ryan.ogrady@soartech.com, denise.nicholson@soartech.com, rhoffman@ihmc.us, lbunch@ihmc.us, jbradshaw@ihmc.us, amy.bolton@navy.mil Cyber warfare presents a persistent and evolving threat to military and civilian information systems. Both DoD (Parrish, 2013) and ODNI (Pellerin, 2013) rank cyber warfare as our top national security concern. In addition to threats to our defensive forces, cyber attacks pose an economic threat on the order of one trillion dollars (Ponemon, 2013). Although individual cyber-warfare tools operate at extremely fast speeds, aggressors increasingly pursue a “cyber kill-chain” (Hutchins et al., 2010) over days, weeks, or months. Would-be cyber aggressors are constantly changing their attack vectors to take advantage of security lapses by human resources and the latest vulnerabilities in information technology. These human-speed activities are guided by cognitive behavior that includes a variety of types of goals and expertise: script-kiddies, ideological activists, investigators, financial criminals, intelligence agents, or cyber warfighters (Lathrop et al., 2010). At the human, cognitive level, offense depends on and reacts to responses of defenders (Pfleeger & Caputo, 2012) and users (Bowen et al., 2012) that are also cognitively driven. Current cyber-warfare tools comprise suites of technical mechanisms that respond to the tools that aggressors and defenders use, but not to the individuals themselves. Human tactics are currently addressed through human-staffed wargames at cyber ranges (Merit, 2013; Pridmore, 2012). Human role-players are expensive, not repeatable, and not deployable as an automated system. There is an emerging need for cognitive-level synthetic cyber offense and defense, to ensure realistic cyber simulation and training. Building effective training systems for cyber warfare presents a suite of unique problems: Offensive and defensive activity is highly dynamic. The characteristics of target network environments are driven by the users of the system and their current activities, which are highly variable and unpredictable. User behavior often creates vulnerabilities that can be exploited. Cyber warfighters themselves are extremely adaptive and creative. In order to meet their objectives they will change tactics or tools based on opportunities detected in a computer network or responses initiated by adversaries or users. Current training environments do not adequately capture the dynamic and cognitive-level characteristics of cyber warfare. They are unable to capture the purposefulness, creativity, and adaptability of actual cyber warfighters. Studying previous offensive and defensive scenarios in a classroom environment is an effective means of understanding the building blocks of cyber warfare, but falls short of creating the skills needed to deal with a creative and time-sensitive event or a sophisticated but dynamic plan. Computerized unit tests can build fundamentals for dealing with individual components cyber warfare, but they do not help the trainee learn to recognize and make sense of the larger picture, nor do they capture the dynamic nature of networks and users. If cyber warfighters are to learn to respond to a cunning and adaptive opponent, they need to train against cunning and adaptive opponents. An effective cyber-warfare training system must be adaptable and deal with the changing nature of a networked environment. It must be able to model the dynamic nature of cyber aggressors, users, and defenders. It must create a virtual environment that replicates the environment that the trainee will ultimately operate in. An appropriate virtual environment also creates the opportunity for accurate post-event forensic analysis by providing access to databases, configurations, and system logs. This paper presents our efforts to address these issues through the development of cognitive agents for cyber offense and defense. The Soar cognitive architecture described in this paper is not to be confused with Soar Technology, the affiliation of some of the authors. Soar is not a commercial product, but is available under a General Public License from http://soar.eecs.umich.edu/ maintained by the University of Michigan. The Soar architecture provides the technological foundation for the cognitive agents described here. Interservice/Industry Training, Simulation, and Education Conference (I/ITSEC) 2015 2015 Paper No. #15232 Page 3 of 10 MODELING AND INTEGRATION CHALLENGES In order to build realistic cognitive agents, the agents must encode appropriate domain expertise, and they must interact with a realistic cyber environment (Jones & Laird, 1997). In addition to realism, cost effective cognitive agents also need to address these related issues: Reduce cost of realistic role playing in cyber-warfare simulation, system engineering, and analysis of cyber operations. Enable end-user updating of agent knowledge with minimal support from software engineers, both through coaching by instructor Subject-Matter Experts (SMEs) and through explicit addition of new knowledge about cyber tactics. Be readily adaptable to a wide range of network structures, devices, and protocols. In o",,2015,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
71af6878cf6528e2df24a856809106059484e259,https://www.semanticscholar.org/paper/71af6878cf6528e2df24a856809106059484e259,Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation,"One of the principal themes the NATO Science and Technology Organization (STO) is fostering in 2017 is ""Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"". Simulation might play a significant role to play in these developments as it can act as a testbed for such concepts and support the military decision makers in future operations that are enhanced by AI. Simulation is already making a significant impact in the development of AI outside of the defence sector. Companies such as DeepMind and Nvidia are using computer games and simulations to “train” AI and autonomous systems, analogous to humans training in simulations. The rate of progress is high, driven by increases in computing power, availability of data and improved algorithms. AI can now “beat” humans at many computer and board games and is moving towards tackling more strategic games that have parallels with military C2. If such developments translate into the defence sphere then we could foresee humans and autonomous systems training in the same simulation systems, both separately and together, and the AI in the autonomous system being the same as that in the simulation. As autonomous systems proliferate across the nations, M&S technology and techniques might be used to improve the interoperability of autonomous systems. To maximise such synergies, it will be essential that NATO embraces all communities that have an interest in AI. Assessing the risks of potential adversary’s use of AI and commercial autonomous systems is also necessary. Despite recent advances, AI development still faces significant technological and ethical challenges and these must be monitored and addressed as necessary. 1.0 CONTEXT Artificial Intelligence or AI is a technology or concept that has developed over many decades and periodically becomes mainstream news. In the latter half of the 2010s this is still very much the case with regular forecasts of its impact on society, jobs and the world economy. Some of these predictions appear to be nearing reality and AI devices are even entering the home. AI also has the potential to influence and sometimes disrupt the ways that companies and organisations operate and AI-based technology revolutions are anticipated. AI also has enduring impact on media and culture and like much technology it can be considered to have beneficial and harmful uses and its impact has and will have political and ethical implications. Over the decades many AI predictions have come true to some degree but in some cases not at all or only partially. Good examples of this are to be found in transport where passenger aircraft have considerable levels of automation but society remains some way off from accepting pilotless passenger aircraft. For STO-MP-MSG-149 11 1 Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation railways, some are now fully automated whilst others continue to put high reliance on the human. Cars can now park themselves and have high levels of automation but are yet to be fully autonomous in all environments and applications. There is no doubt however, that there is a trend towards greater use of autonomous systems and AI. This is being driven by ever greater processing power together with the ability for very large data sets (“big data”) to be captured and used to help build more capable AI. Such resources can also be accessed online in the cloud, driving down the cost of developing and distributing AI programs. The military have developed and deployed autonomous systems for a very long time, for example in the use of land and sea mines. In the 20th century proximity fuzes came into service that were semi-intelligent, sensing and exploding at the most appropriate time for the target. Analogue computers also assisted operators as part of fire control calculations and missiles and rockets in World War 2 become remotely piloted or fully autonomous. With the advent of digital computing, autonomy in military systems is commonplace, reducing the manpower requirement or in assisting the human, but there remains a significant ethical dimension in the use of fully autonomous systems. Developments in AI and autonomous systems outside of defence are of significant interest as they may provide answers how to better manage and interpret data within military command and control systems but also because they may enhance potential adversary’s capabilities. This was recognised in 2017 by the NATO Science and Technology Organization’s (STO) which made ""Military Decision Making using the tools of Big Data and Artificial Intelligence (AI)"" one of its principal themes. The modelling and simulation (M&S) community has strived itself to develop AI, reducing or eliminating the need for human input. Sometimes termed Semi-Automated Forces (SAF) or Computer-Generated Forces (CGF) this has benefits in analysis and training, reducing the number of role players and improving consistency. Simulation itself is now being used to “train” AI/autonomous systems, as such environments are repeatable and controllable and can generate highly tailorable data output. However, reproducing credible and realistic behaviours in simulation remains a significant challenge and the M&S community continues to strive to enhance its AI. The computer games industry also sees AI as a challenge as games can easily lose their entertainment value if their AI is poorly implemented. 2.0 WHAT IS ARTIFICIAL INTELLIGENCE? Artificial intelligence (AI) is a broad topic area as it depends on what nature of human intelligence are being replicated and that AI technology can take many different forms. The Oxford Dictionary definition is “The theory and development of computer systems able to perform tasks normally requiring human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages.” The nature of the intelligence can range from “narrow” intelligence which is highly tailored or specialised through to artificial “general” intelligence which is flexible, adaptive and inventive, much like the human brain. There are many approaches to AI for example, decision tress, fuzzy logic and neural nets with some approaches becoming synonymous with AI. For example, machine learning is an approach that gives ""computers the ability to learn without being explicitly programmed” by learning from and making predictions from data. In broad terms AI is the ‘what’, machine learning is an approach to the ‘how’, and self-driving cars might be the ‘why’. Machine learning methods are based on learning data representations, as opposed to task-specific algorithms. Learning can be supervised, partially supervised or unsupervised. Neural nets or networks are computer systems modelled on the human brain and nervous system with an interconnected group of nodes, akin to the vast network of neurons in a brain. Deep or reinforcement learning, which is inspired by the way animals seem to learn, has taken the neural nets approach and added layers of nodes taking advantage of current day higher processing power and making significant advances in image recognition for example and is generally seen as at the current forefront of AI technology. Developments in Artificial Intelligence – Opportunities and Challenges for Military Modeling and Simulation 11 2 STO-MP-MSG-149 An autonomous system builds on the use of AI and extends it into the physical world, in for example a robot or vehicle, requiring an awareness of the world through sensors, a task(s) and minimal human intervention. Some argue that some AI algorithms are not really showing intelligence but are a predetermined and limited set of responses to a predetermined and limited set of inputs. Professor Isbell of Georgia Tech suggests that systems should have two features before they can be considered AI. Firstly, they must learn over time as their environment changes. Secondly, their challenge must be demanding too for humans to learn, so a machine programmed to automate repetitive work would not be considered an AI system. Another example would be the AI in many computer video games; it may appear to represent human behaviour but this is preprogrammed and there is little or no learning over time. 3.0 HISTORICAL CONTEXT An overview of the general history of AI including work on AI by the M&S community will be provided to give context to the progress currently being made and the possible trajectory of the field into the future.",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f7486af64ff61068bc37088db3d87fdd7509eefe,https://www.semanticscholar.org/paper/f7486af64ff61068bc37088db3d87fdd7509eefe,The Application Study On The Customer Service Chatting Robot Based On Bot Framework,"with the rapid development of speech recognition technology, the speech chatting robot has become as the breakthrough of the artificial intelligence. The voice chatting robot shall be such a typical application field in customer service, it can provide the customer with efficient and convenient service for 24 hours. In this study it develops an intelligent speech chatting robot for customer service by using the BOT FRAMEWORK released by Microsoft, the usage of such robot has greatly improved the the efficiency and satisfaction degree of customer service. 1 CHATTING ROBOT REVIEW The chatting hereby mentioned refers to voice chatting. In comparison with text input, the voice chatting has so many advantages such as quick speed for inputting, convenient and easy to use, without spatial limit etc. Due to the lower recognition degree of computer on the voice previously, it may be quite hard for computer to understand the meaning of the natural language, it also obstructed the application of voice interaction. Along with the deepened development of machine learning technique, the computer’s ability to process and recognize voice has been improved greatly, it’s already possible for computer to correctly and precisely understand the voice instruction. In the artificial intelligent times, the voice intelligence has been become as the new breakthrough and also the tremendous development has also been gained in such field. Currently the intelligent voice chatting robot has been widely used in the fields of personal life assistant and commercial service such as knowledge Q & A, weather inquiry, agenda reminding, traffic navigation, self-assistant catering and ticketing etc. The intelligent voice robot has attracted the attention of foreign internet giants highly and they’ve also released their own relative products accordingly, such as Apple’s Siri, Google’s Google Now and Cortana, small ice released by Microsoft, Amazon’s Echo intelligent speaker, Facebook’s Messenger Platform, Tencent’s QQ robot, Baidu’s Xiao Du, Taobao’s online customer service etc. The cognitive revolution raking the voice and language as the entry should must be the necessary path for the artificial intelligence. The application of intelligent voice chatting robot shall be more widely with brighter and brighter development prospect. One of the typical application field shall be the customer service. Because manual customer service can hardly realize the service within 24 hours at any time and anywhere; the online message also cannot response the customer with satisfied service due to its slow inputting speed and slow customer service response and processing. But the voice chatting robot for customer service could realize the service with high efficiency and convenience in 7 days for a week and 24 hours in a day, it should be very suitable for customer service field. 2 BOT FRAMEWORK REVIEW BOT FRAMEWORK is such a framework released by Microsoft for constructing the intelligent chatting robot. It’s a comprehensive product, to provide the simple and easy-to-use platform for developing the personalized intelligent chatting robot. The developer could use such framework to structure and deploy the robot with high quality, it enables the user having the voice chatting in high quality at any time and anywhere. Such framework includes Bot Builder SDK,Bot Connector,Developer Portal and Bot Directory. In addition, it also provides a simulator to test the developed voice chatting robot. BOT FRAMEWORK can understand the natural language and analyze the image etc. Initially it released 22 API that can be integrated in the application. The user can use these APIs to develop the personalized intelligent chatting robot.. It’s worth noting that Microsoft’s BOT was not such a robot based on hardware with the traditional meaning, but such a service framework similar to the online customer service. By using such framework it can realize the development in one time and the deployment of application in multiple places and times. This shall attribute to the convenience brought by Framework Channel. The BOT can be configured to following channels, see attached figure 1. Fig. 1. Channels Channels ----------------------Email customer service Short message service",,2017,10.2991/JIMEC-17.2017.24,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5caa4d385dde4382a5f7b335f00374eea5e01410,https://www.semanticscholar.org/paper/5caa4d385dde4382a5f7b335f00374eea5e01410,Controlling Tensegrity Robots through Evolution using Friction based Actuation,"Traditional robotic structures have limitations in planetary exploration as their rigid structural joints are prone to damage in new and rough terrains. In contrast, robots based on tensegrity structures, composed of rods and tensile cables, offer a highly robust, lightweight, and energy efficient solution over traditional robots. In addition, tensegrity robots can be highly configurable by rearranging their topology of rods, cables, and motors. However, these highly configurable tensegrity robots pose a significant challenge for locomotion due to their complexity. This study investigates a control pattern for successful locomotion in tensegrity robots through an evolutionary algorithm. A twelve-rod hardware model is rapidly prototyped to utilize a new actuation method based on friction. A web-based physics simulation is created to model the twelve-rod tensegrity ball structure. Squarewaves are used as control policies for the actuators of the tensegrity structure. Monte Carlo trials are run to find the most successful number of amplitudes for the square-wave control policy. From the trials’ results, an evolutionary algorithm is implemented to find the most optimized solution for locomotion of the twelve-rod tensegrity structure. The software pattern coupled with the new friction based actuation method can serve as the basis for highly efficient tensegrity robots in space exploration. Keywords— Robotics, Tensegrity, Evolutionary Algorithm, Machine Learning, NASA Introduction Tensegrity robots are a new field of robotics that diverge from the traditional sense of robotics. Traditional robotics relies on robots composed of rigid joints. Tensegrity structures, on the other hand, are composed of pure tension and compression elements as shown in Fig. 1 [12]. The lack of lever arms makes tensegrity structures resistant to force magnification in joints or other points of failure. The goal of these structures is to take part in low-cost planetary exploration [1]. In this task, tensegrities have numerous benefits to offer over traditional robots: • Highly Reconfigurable: Topology of rods, cables, and motors can be changed to add new functionality. • Light-weight: The structures are made of tubes/rods and cables/elastic lines. • Energy efficient: Dynamic movement of the tensegrity structure results in efficient locomotion. • Scalability: Composed of rods, cables, and actuators, the tensegrity structure can be scaled up in size without a significant cost difference. • Ease of Deployment: The shock absorbent structure allows for a smoother landing on planetary missions. • Robust to failure: The inherent tension network distributes harmful external forces to reduce the structural damage.",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
5c8b739d4066cf343a5851a20a816f506ea000be,https://www.semanticscholar.org/paper/5c8b739d4066cf343a5851a20a816f506ea000be,Vision-Based Autonomous Navigation of Multirotor Micro Aerial Vehicles,"espanolResumen: El objetivo de esta tesis es conseguir una navegacion basada en vision fiable y explorar el potencial de los algoritmos del estado del arte moderno en vision por computador para la robotica aerea, y especialmente aplicados a drones de tipo multirotor. Dado que las camaras digitales pueden proveer potencialmente de una cantidad enorme de informacion al robot, que son menos costosas que otras alternativas de sensado y que son muy ligeras; los metodos de estimacion basados en vision son muy prometedores para el desarrollo de aplicaciones civiles con drones. La parte principal de esta tesis consiste en el diseno, la implementacion y la evaluacion de tres modulos noveles de proposito general para multirotores que utilizan metodos basados en vision para tomar decisiones de navegacion. El primer modulo propuesto es un controlador de navegacion para multirotores que fue disenado para conseguir un vuelo fiable en entornos sin disponibilidad de senal GPS por medio del uso de la vision por computador. Con este objetivo en mente, la estimacion de velocidad basada en el flujo optico del suelo fue seleccionada para explorar las implicaciones de la utilizacion de metodos basados en la vision como principal sistema de medida para la retroalimentacion del controlador. Fijar y respetar una limitacion en la velocidad de navegacion fue identificada como una manera fiable de asegurar que la estimacion de velocidad basada en flujo optico no falle. El controlador propuesto tiene en cuenta las limitaciones de velocidad establecidas por el uso de un metodo de estimacion de la posicion basado en odometria visual. Esta capacidad del controlador de respetar una velocidad maxima preconfigurada ha sido utilizada con exito para realizar investigacion en la navegacion descentralizada simultanea de multiples drones. Una capacidad comunmente buscada en drones, es la realizacion de un seguimiento visual y la estabilizacion del vuelo mientras se adquieren imagenes de un objeto de interes. El segundo modulo propuesto en esta tesis es una arquitectura para el seguimiento en vuelo de objetos. Esta arquitectura fue desarrollada con el proposito de explorar el potencial de los metodos modernos de seguimiento visual basados en aprendizaje de maquina, o “machine learning” en ingles. Este tipo de algoritmos tienen como objeto determinar la posicion de un objeto en el flujo de imagenes. Los metodos de seguimiento visual tradicionales funcionan correctamente en las condiciones para las cuales fueron originalmente disenados, pero a menudo fallan en situaciones reales. La solucion propuesta ha demostrado que los metodos actuales de seguimiento visual basados en aprendizaje de maquina permiten conseguir un seguimiento en vuelo fiable de una gran variedad de objetos. El despliegue rapido de drones autonomos en entornos desconocidos es todavia un problema actual en investigacion. Uno de los principales desafios es el calculo de trayectorias que permitan una navegacion rapida en dichos entornos. Con este problema en mente, el tercer modulo propuesto permite la generacion automatica de trayectorias seguras en entornos abarrotados de obstaculos. Este modulo ha sido probado utilizando mapas adquiridos en tiempo real a bordo del dron mediante el uso de algoritmos de fotogrametria basados en la vision por computador. Aun usando navegacion basada en GPS, mediante la realizacion de experimentos se ha mostrado que el despliegue rapido de drones es factible incluso utilizando solo el ordenador y los sensores a bordo del dron. Parte de los modulos desarrollados han sido liberados en codigo abierto, contribuyendo al entorno de desarrollo “Aerostack” de codigo abierto del CVAR, previamente CVG, (UPM). Los modulos propuestos en esta tesis han sido repetidamente probados con exito en experimentos fuera del laboratorio y en eventos publicos, demostrando su fiabilidad y potencial, y promoviendo su evaluacion en diferentes escenarios. EnglishSummary / Abstract: The aim of this thesis is to attain reliable vision-based navigation and explore the potential of state of the art Computer Vision algorithms in Aerial Robotics, and specifically for multirotor drones. Since digital cameras can potentially provide a vast amount of information to the robot, are less expensive than their alternatives and are very lightweight, vision-based estimation methods are very promising for the development of civilian applications for drones. The main part of this thesis consists on the design, implementation and evaluation of three novel general purpose modules for multirotors that utilize vision-based methods to make navigation decisions. The first proposed module is a novel multirotor navigation controller that was designed to achieve reliable flight in GPS-denied environments by using Computer Vision. With this objective in mind, ground optical flow speed estimation was selected to explore the implications of using vision-based methods as the main measurement feedback for the controller. Setting and enforcing a speed limitation during navigation was identified as a reliable way to ensure that the optical flow speed estimation method does not malfunction. The proposed controller takes into account the speed limitations established by the vision-based odometry estimation method. In addition, by leveraging this capability of the controller to ensure navigation respecting the preconfigured maximum speed, it has also been successfully utilized to research on decentralized multi-drone navigation. A common objective for drones is to be able to fly and stabilize looking at and close-by to an object of interest. The second module proposed in this thesis is a novel vision-based object tracking architecture for multirotor drones, which was designed to explore the reliability of machine learning based visual trackers to provide feedback to an object following controller. Visual trackers have the purpose to determine the position of an object in an image stream and are, therefore, good candidates to achieve this objective. However traditional visual tracking methods, though very effective in the specific conditions for which they were designed, have the drawback of not generalizing well to the myriad of different visual appearances that real objects exhibit. The proposed architecture has demonstrated that current machine learning based visual tracking algorithms can reliably provide feedback to a visual servoing controller. The fast deployment of autonomous drones in unknown environments is still an ongoing research problem. One of the main challenges to achieve it is the calculation of trajectories that allow fast navigation in such areas. With this problem in mind, the third proposed module is a trajectory planner that delivers smooth safe trajectories in relatively cluttered environments. The algorithm has been tested experimentally on maps obtained using an on-board real-time capable Computer Vision mapping method. Although other modules would be needed to decide the exact navigation objectives of the drone, using this planner we have shown that it is feasible to deploy drones in unknown outdoors environments, by leveraging the good qualities of maps obtained using state of the art photogrammetry methods. Part of these modules have been released as open-source, contributing to the CVAR's, previously CVG, (UPM) Open-Source Aerial Robotics Framework “Aerostack”, which main purpose is enabling drone civilian applications. The proposed modules have been repeatedly evaluated with successful results in several out-of-the-lab demonstrations, therefore showing their potential and good qualities. The utilization of the proposed modules in public events has promoted their further testing in different settings and applications, e. g. indoors and outdoors flight, narrow corridors, passage through windows and landing in different conditions.",,2017,10.20868/upm.thesis.47726,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
9e8402112d6ad0484920d238dfe0ec87b49a3c17,https://www.semanticscholar.org/paper/9e8402112d6ad0484920d238dfe0ec87b49a3c17,"Design and development of methodologies, technologies, and tools to support people with disabilities","Assistive Technologies (ATs) is an umbrella term that includes, from the one hand, assistive, adaptive, and rehabilitative devices for people with disabilities and, from the other hand, the process needed to select, locate, and use them. ATs promote greater independence by enabling people to perform tasks that they were formerly unable to accomplish (or had great difficulty accomplishing) by providing enhancements to, or changing methods of interacting with, the technology needed to accomplish such tasks. Researching on ATs means to focus both on the individuals, the users, the design, and the consecutive development of any kind of technology that could ease, or even improve, everyday life of disabled, elderly people, and people who are following rehabilitative programs. This dissertation spans on ATs that, starting from a common root and deriving from the realm of Information Technology, have been applied and deployed to several groups of individuals with disabilities. Starting from the issue of detecting hand poses, gestures, and signs for enabling novel paradigms for human-machine interaction, three approaches for hand tracking and gesture recognition from single markerless observation have been developed. The first approach comprises machine learning techniques and optimized features to boost performances. The second one comprises a 3D model of a human hand and optimization techniques. The third approach applies machine learning and statistical techniques on top of technology specifically designed for tracking human hands. Starting from these results, hand gesture recognition has then been proposed to enable new interaction paradigms, suitable for individuals with disabilities, in the eld of Human-Robot collaboration. A reliable real time protocol to remotely control anthropomorphic robotic actuators has been implemented. This protocol allows the user to send commands to one (or many) robotic actuator by simply moving his/her hand; it has been designed, modeled, and formally validated resorting to a knowledge-driven agile approach. This dissertation proposes two use cases enabled from the outcomes of the research activities. The former one is a remote communication system for deafblind individuals based on Sign Languages (SLs) with tactile feedback. With the support of SL experts, I have identified a list of fundamental hand movements and gestures to be recognized accurately. The developed algorithms were successfully tested involving 80+ volunteers (both proficient and not in SLs). This communication system is ready to be used concurrently by many people, allowing 1-to-many communication. In addition, it supports different input (cameras and sensors for non-invasive markerless hand tracking) and output (upper-limb anthropomorphic robotic interfaces) systems. The latter one is a telerehabilitation setup for upper-limb post-stroke rehabilitation, comprising vision-based input and a hand exoskeleton. Knowledge derived from the research activities has been applied to two projects, whom outcomes are discussed in this dissertation, as well. The former one lies in the realm of character recognition and aims at improving accessibility of mathematical and scientific documents for blind and deafblind individuals. The latter one aims at developing inclusive interfaces to a web platform under development for preserving and disseminating the cultural heritage of deaf and deafblind communities. All the research activities presented in this dissertation have involved a strict and direct contact with end-user associations and persons who benefit from the results of the research itself, and have been widely discussed and tested with them.",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
919d6334d721970a2b2b9428e16013b1e9677715,https://www.semanticscholar.org/paper/919d6334d721970a2b2b9428e16013b1e9677715,Swarms in the Third Offset,"Advances in swarm technology is part of the Department of Defense’s Third Offset Strategy which is a plan for overcoming reduced military force structure and declining technological superiority against potential U.S. adversaries. The components of the Third Offset represent the enabling capabilities of swarm behavior which could be adopted in the future force. Therefore, this paper investigates whether the U.S. military should focus greater research and development efforts on swarm-capable systems that are low-cost, numerous, unmanned, and fast. The first area of discussion includes swarm initiatives that could allow the military to transition away from expensive and heavy weapons platforms. Second, self-driving vehicles, automated logistics, and aerial drones in industry could translate to autonomous supply trains, reduced soldier error, and targeting missions in the military. Third, adversaries are pursuing swarm capabilities. While swarms show great promise, there are some major legal and ethical obstacles to swarm-capable systems. Lastly, recommendations are offered as a way ahead for swarm initiatives. Swarms in the Third Offset The fiercest serpent may be overcome by a swarm of ants. —Admiral Isoroku Yamamoto During the 2017 Super Bowl 51 half-time performance, Intel demonstrated the control of 300 drones and broke the world record a few months earlier with 500 drones controlled by a single operator. In October of 2016, the U.S. Military conducted the largest deployment ever of micro-swarms. Dubbed the Perdix micro drone, these small, inexpensive, battery-powered, propeller-driven air vehicles were launched by three F/A18 Super Hornets. Just over a year prior in 2015, the Advanced Robotic Systems Engineering Laboratory at the Naval Postgraduate School in Monterey, California, held the record with 50 simultaneous airborne unmanned aerial vehicles controlled by a single operator. This author predicts that 500 drones will quickly increase to 1,000 and 10,000 agents in just a few a year’s time while being scalable, adaptable, distributed, and collective. 5 Advances in swarm technology is part of the Department of Defense’s (DoD) Third Offset Strategy which is a plan for overcoming (“offsetting”) reduced military force structure and declining technological superiority against potential U.S. adversaries. 6 Key components of this strategy include dominance in artificial intelligence, machine learning, robotics, unmanned systems, and increased autonomy. Collectively, the components of the Third Offset represent the enabling capabilities of swarm behavior which could be wholly adopted in the future force. Therefore, should the DoD focus research and development efforts on swarm-capable ISR (intelligence, surveillance, reconnaissance) and weapons systems that are lowcost, numerous, unmanned, and fast? This paper will provide an overview of swarms and explore three major areas to address this primary question. While the promise of 2 swarm behavior appears great, there are some major obstacles to swarm-capable systems which will be presented. Lastly, recommendations will be offered as a way ahead for DoD swarm initiatives. Swarms can offer multiple operational advantages in terms of speed, intelligence gathering, coordinated effects (kinetic and non-kinetic), and efforts. Humans typically cannot control more than four to five discrete elements at a time. Swarm-based systems allow for collective and distributed control of hundreds and thousands of agents where the operator is executing mission objectives and not focused on individual agent control. Humans will maintain overall operational control but low-level decisions from individual agents within the swarm could be made autonomously to fulfill mission objectives within pre-defined rules of engagement. The first area of discussion includes swarm initiatives that could allow the DoD to transition away from expensive, heavy, and human-centric weapons platforms such as legacy tanks, manned fighters, and submarines. These systems are difficult to deploy, require large amounts of maintenance and fuel, and may be out-matched in a many operator to one vehicle paradigm. Second, the advent of self-driving vehicles, automated logistics, and aerial drones in the commercial sector could translate to autonomous supply trains, reduced soldier fatigue and error, and targeting missions in the military. Academia partnered with private industry could propel these innovations since the technology has dual-use capabilities in transportation, search and rescue, agricultural monitoring, and homeland security. The open source community has enabled these advances but has also allowed for their proliferation and use among enemy forces and terrorist organizations.",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
441ddc05abb939e25b21065414d2fe9660d53635,https://www.semanticscholar.org/paper/441ddc05abb939e25b21065414d2fe9660d53635,RPA for Telcos: The Next Wave of BPM Evolution,"As various disruptive digital technologies continue to upend the traditional business models and trigger unprecedented changes in customer behavior, the telecommunication industry must fundamentally reimagine its operations. A swift modernization of existing business processes and cost-prohibitive systems is vital for wireless carriers to capitalize on the emerging opportunities to grow revenues in the digital era.<br><br>Over the years, several inefficiencies have crept into processes across the sector. For instance, the typical operations landscape of many communications services providers (CSPs) continues to be weighed down by disparate systems, siloed processes, and highly labor-intensive activities that hinder business agility. This, in turn, adversely impacts telcos’ ability to effectively respond to the rapidly evolving customer demands, and diminishes their ability to compete effectively with new-age cross-industry players. Unless the industry streamlines its operations first, any economies of scale and transformation benefits that may result from digitization initiatives will at best be suboptimal. Specifically, telcos need to evaluate their existing IT infrastructure and application systems in view of the dynamic business priorities. Advanced technologies such as automation, cognitive computing, and artificial intelligence (AI) will help wireless carriers effectively transform their core operations, and realize business outcomes faster for higher return on investment (ROI). From an operational standpoint, this would translate into telcos’ manual, time-consuming, and expensive processes being replaced with automated software solutions that are more reliable and agile. To be fair to CSPs, they embraced process automation more than a decade ago. Many telcos have since launched several business process management (BPM) projects to eliminate manual intervention in different functions spanning their operations support systems (OSS) and business support systems (BSS). Be it for optimizing the workflows underpinning network inventory and configuration and service provisioning, or automating processes concerning revenue and order management, operators have aggressively adopted BPM. <br><br>Where does then the new wave of process automation, enabled by the likes of AI and machine learning, fit in? Does this technology stack–known as robotic process automation (RPA)–require a standalone deployment, or can it be integrated with the BPM implementation in a non-intrusive way? And, what tangible business benefits can telcos accrue from these investments?",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
38a550167f423fe4c8cf988778fb7576be9c94e4,https://www.semanticscholar.org/paper/38a550167f423fe4c8cf988778fb7576be9c94e4,Adaptive Sampling For Efficient Online Modelling,"Troy Wilson, BComm (Hons 1), MQF Doctor of Philosophy The University of Sydney June 2017 Adaptive Sampling for Efficient Online Modelling This thesis examines methods enabling autonomous systems to make active sampling and planning decisions in real time. Gaussian Process (GP) regression is chosen as a framework for its non-parametric approach allowing flexibility in unknown environments. The first part of the thesis focuses on depth constrained full coverage bathymetric surveys in unknown environments. Algorithms are developed to find and follow a depth contour, modelled with a GP, and produce a depth constrained boundary. An extension to the Boustrophedon Cellular Decomposition, Discrete Monotone Polygonal Partitioning is developed allowing efficient planning for coverage within this boundary. Efficient computational methods such as incremental Cholesky updates are implemented to allow online Hyper Parameter optimisation and fitting of the GP’s. This is demonstrated in simulation and the field on a platform built for the purpose. The second part of this thesis focuses on modelling the surface salinity profiles of estuarine tidal fronts. The standard GP model assumes evenly distributed noise, which does not always hold. This can be handled with Heteroscedastic noise. An efficient new method, Parametric Heteroscedastic Gaussian Process regression, is proposed. This is applied to active sample selection on stationary fronts and adaptive planning on moving fronts where a number of information theoretic methods are compared. The use of a mean function is shown to increase the accuracy of predictions whilst reducing optimisation time. These algorithms are validated in simulation. Algorithmic development is focused on efficient methods allowing deployment on platforms with constrained computational resources. Whilst the application of this thesis is Autonomous Surface Vessels, it is hoped the issues discussed and solutions provided have relevance to other applications in robotics and wider fields such as spatial statistics and machine learning in general.",,2017,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,https://www.semanticscholar.org/paper/4f8c4025b2ce0c0eb837b7389bba31f6f1cae7c7,Implementing Operational Analytics using Big Data Technologies to Detect and Predict Sensor Anomalies,"Operational analytics when combined with Big Data technologies and predictive techniques have been shown to be valuable in detecting mission critical sensor anomalies that might be missed by conventional analytical techniques. Our approach helps analysts and leaders make informed and rapid decisions by analyzing large volumes of complex data in near real-time and presenting it in a manner that facilitates decision making. It provides cost savings by being able to alert and predict when sensor degradations pass a critical threshold and impact mission operations. Operational analytics, which uses Big Data tools and technologies, can process very large data sets containing a variety of data types to uncover hidden patterns, unknown correlations, and other relevant information. When combined with predictive techniques, it provides a mechanism to monitor and visualize these data sets and provide insight into degradations encountered in large sensor systems such as the space surveillance network. In this study, data from a notional sensor is simulated and we use big data technologies, predictive algorithms and operational analytics to process the data and predict sensor degradations. This study uses data products that would commonly be analyzed at a site. This study builds on a big data architecture that has previously been proven valuable in detecting anomalies. This paper outlines our methodology of implementing an operational analytic solution through data discovery, learning and training of data modeling and predictive techniques, and deployment. Through this methodology, we implement a functional architecture focused on exploring available big data sets and determine practical analytic, visualization, and predictive technologies. APPROACH This study developed an operational analytics implementation that uses Big Data technologies and machine learning algorithms to determine and predict sensor anomalies. A previous study [1] showed that Big Data Analytics can uncover anomalies that may be missed through conventional analyses. This study enhances that effort and shows a methodology to implement operational analytics that can be applied toward common solutions for data analysis. Our operational analytics implementation relies on continuous learning from historical data to analyze data in the stream of real-time operations. In the previous study, where data was identified that can be used to uncover anomalies, this implementation extends that approach and now identifies trends and correlations that reveal anomalies that can be missed by traditional analytic techniques with limited datasets. This study adopted a three-step methodology to implementing operational analytics – Discovery, Modeling and Operations as shown in Fig. 1. Copyright © 2016 Advanced Maui Optical and Space Surveillance Technologies Conference (AMOS) – www.amostech.com Fig. 1. Operational Implementation Approach Fig. 1 shows the three steps to implement operational analytics and the continuous feedback between learning and operational deployment. The following sections will elaborate on the methodology employed as applied to a realworld problem of analyzing large datasets such as would be encountered at an operational site.",,2016,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
34da0580c4f9f564efd93e5dce1100f30cd03915,https://www.semanticscholar.org/paper/34da0580c4f9f564efd93e5dce1100f30cd03915,Many human activities are a bottleneck in progress,"SCIENCE sciencemag.org T echnological innovations are penetrating all areas of science, making predominantly human activities a principal bottleneck in scientific progress while also making scientific advancement more subject to error and harder to reproduce. This is an area where a new generation of artificial intelligence (AI) systems can radically transform the practice of scientific discovery. Such systems are showing an increasing ability to automate scientific data analysis and discovery processes, can search systematically and correctly through hypothesis spaces to ensure best results, can autonomously discover complex patterns in data, and can reliably apply small-scale scientific processes consistently and transparently so that they can be easily reproduced. We discuss these advances and the steps that could help promote their development and deployment. Applying AI to the practice of science is not new. AI pioneer and Nobel laureate Herbert Simon hypothesized that cognitive mechanisms involved in scientific discovery are a special case of general human capabilities for problem-solving and, with colleagues, developed systems in the 1970s and 1980s that demonstrated reasoning capabilities for analyzing scientific data ( 1). Also in the 1970s, Joshua Lederberg (another Nobel winner) and colleagues developed the DENDRAL system for analyzing mass spectrometry data in order to hypothesize molecular structures ( 2). More recent breakthroughs, such as robot scientists and software that formulates laws for complex dynamical systems, demonstrate broader applicability of AI techniques for scientific discovery ( 3). Over the past two decades, AI has seen accelerating scientific advances and concomitant commercial-sector successes because of advances on three fronts: steady scholarly advances, especially as success has increased the numbers of interested participants; Moore’s law and steady exponential increases in computing power; and exponential increases in, and broad availability of, relevant data in volumes never previously seen. Those scientific efforts that have leveraged AI advances have largely harnessed sophisticated machine-learning techniques to create correlative predictions from large sets of “big data.” Such work aligns well with the current needs of petaand exascale science. However, AI has far broader capacity to ac-",,2014,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
148b7d09bc9513a8b269b393e15f95c878f4c112,https://www.semanticscholar.org/paper/148b7d09bc9513a8b269b393e15f95c878f4c112,Movement-Related Desynchronization in EEG-based Brain-Computer Interface applications for stroke motor rehabilitation,"Neurological degenerative diseases like stroke, Alzheimer, Amyothrophic Lateral Sclerosis (ALS), Parkinson and many others are constantly increasing their incidence in the world 
health statistics as far as the mean age of the global population is getting higher and higher. 
This leads to a general need for effective, at-home and low-cost rehabilitative and health-daily-care tools. The latter should consist either of technological devices implemented for operating in a remote way, i.e. tele-medicine is quickly spreading around the world, or very-advanced computer-based and robotic systems to realize intense and repetitive trainings. This is the challenge in which Information and Communications Technology (ICT) is asked to play a major role in order to bring medicine to reach further advancements. 
Indeed, no way to cope with these issues is possible outside a strong and vivid cooperation among multi-disciplinary teams of clinicians, physicians, biologists, neuro-psychologists and engineers and without a resolute pushing towards a widespread inter-operability between Institutes, Hospitals and Universities all over the world, as recently highlighted during the main International conferences on ICT in healthcare. The establishment of well-defined standards for gathering and sharing data will then represent a key element to enhance the efficacy of the aforementioned collaborations. 
Among the others, stroke is one of the most common neurological pathologies being the second or third cause of mortality in the world; moreover, it causes more than sixty 
percent survivors remain with severe cognitive and motor impairments that impede them in living normal lives and require a twenty-four-hours daily care. As a consequence, on one side stroke survivors experience a frustrating condition of being completely dependent on other people even to perform simple daily actions like reach and grasp an object, hold a glass of water to drink it and so on. States, by their side, have to take into account additional costs to provide stroke patients and their families with appropriate cares and supports to cope with their needs. For this reason, more and more fundings 
are recently made available by means of grants, European and International projects, programs to exchange different expertise among various countries with the aim to study 
how to accelerate and make more effective the recovery process of chronic stroke patients. 
The global research about this topic is conducted on several parallel aspects: as regard as the basic knowledge of brain processes, neurophysiologists, biologists and engineers are 
particularly interested in an in-depth understanding of the so-called neuroplastic changes that brain daily operates in order to adapt individuals to life changes, experiences and to realize more extensively their own potentialities. 
Neuroplasticity is indeed the corner stone for most of the trainings nowadays adopted by the standard as well as the more innovative methods in the rehabilitative programs for post-stroke recovery. Specifically speaking, motor rehabilitation usually includes long term, repetitive and intense goal-directed exercises that promote neuroplastic mechanisms such as neural sprouting, synapto-genesis and dendritic branching. These processes are strictly related with motor improvements and their study could - one day - serve as prognostic measures of the recovery. 
Another aspect of this eld of neuroscience research is the number of applications that it makes feasible. One of the most exciting is to connect an injured brain to a computer or a robotic device in a Brain-Computer or Brain-Machine Interface (BCI or BMI) scheme aiming at bypassing the impairments of the patient and make him/her autonomously move again or train his/her motor abilities in a more effective way. This kind of research can already count an amount of literature that provides several proofs of concept that these heterogeneous systems constituted by humans and robots can work at the purpose. 
A particular application of BCI for restoring or enhancing, at least, the reaching abilities of chronic stroke survivors was implemented and is still currently being improved at I.R.C.C.S. San Camillo Hospital Foundation, an Institute for the rehabilitation from neurological diseases located in Lido of Venice and partially technically supported by the Department of Information Engineering of Padua in range of an agreement signed in 2009. 
This specific BCI platform allows patients to train and improve their reaching movements by means of a robotic arm that provides a force that helps patients in completing the training exercise, i.e. to hit a predetermined target. This force feedback is however subject to a strict condition: during the movement, the person has to produce the expected pattern of cerebral activity. Whenever this is accomplished, a force is delivered proportionally to the entity of the latter activity, otherwise the patient is obliged to operate without any help. In this way, this platform implements the so-called operant-learning, that is one of the most effective conditioning techniques to make a subject learn or re-learn a task. If, on one hand, the primary and explicit task is to improve a movement, on the other side the secondary but most important task is to deploy the perilesional part of the brain - still healthy - in becoming responsible for the control of the movement. It is a popular and widely-accepted opinion within the neuroscience community, indeed, that a healthy region of the sensorimotor area nearby the damaged one - which was previously in charge of performing the (reaching) movement - can optimally accomplish the impaired motor function substituting the original control area. 
Technically speaking, the main crucial feature that can ensure the effectiveness of the whole system is the precise and in real-time identification and quantification of the cerebral pattern associated with the movement, the worldwide named movement-related desynchronization (MRD). Starting from its original definition, passing through the most used techniques for its recognition, the thesis work presents a series of criticisms of the current signal processing method to detect the MRD and a complete analysis of the possible features that can better represent the movement condition and that can be more easily extracted during the on-line operations. 
Brain - it is well-known - learns by trials and errors and it needs a slightly-delayed (in the range of fraction of seconds) feedback of its performance to learn a task in the best way. This BCI application was born with the purpose to provide the above-mentioned feedback: however, this is only feasible if a computationally easy and contingent signal processing technique is available. This thesis work would like to cope with the lack of a well-planned real-time signal analysis in the current experimental protocol.",,2014,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
24555885019b49fbbf031f71257c51db739a059c,https://www.semanticscholar.org/paper/24555885019b49fbbf031f71257c51db739a059c,NRAO/Socorro Colloq. Abstract,"Across the electromagnetic spectrum from radio frequencies to very-high energy gamma-ray energies, and even in new “multi-messenger” channels like neutrinos and gravitational waves, comprehensive, time-domain, sky surveys are now being developed and deployed. These surveys will be powerful engines for identifying transient behavior and they will drive an explosive growth the number of opportunities to explore time domain phenomena in astronomical sources. I argue that optimization of the scientific return in this opportunity-rich environment will require a qualitative change in current observational approach applied to the study of transients. The key assumptions that drive of my conclusion are: (1) That multi-wavelength observational follow-up will be essential for unraveling the physics of the transients; (2) A significant fraction of time-domain transients of interest cannot have follow-up observations scheduled ahead-of-time (we don’t where and when they will happen); and (3) The new surveys will generate so many events that follow-up systems will be overwhelmed—making triage essential. I present the case that the optimization solution must employ autonomous robotic telescopes, globally distributed telescope networks, and artificial intelligence/machine learning. The “Thinking” Telescopes Project at Los Alamos National Laboratory has applied these new technologies to construct a network of RAPTOR (RAPid Telescopes for Optical Response) robotic telescopes that autonomously collect realtime optical observations of Gamma-Ray Bursts (GRBs). I will discuss why this ecosystem of robotic telescopes was built, how we built it, and how it is changing our understanding of GRBs—nature’s largest explosions since the Big Bang. And, I will discuss an approach to real-time transient follow-up called a Dynamic Coalition Architecture that has great promise for the optimization of the scientific return in the coming opportunity-rich environment. November 13, 2015 11:00 am Array Operations Center Auditorium All NRAO employees are invited to attend via video, available in Charlottesville Auditorium, Green Bank Auditorium, and VLA Video Conference Room.",,2015,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
06fafe3ce4b1ac6e6a0fbf718e5acf8fc785e3f4,https://www.semanticscholar.org/paper/06fafe3ce4b1ac6e6a0fbf718e5acf8fc785e3f4,"Governing the Ungovernable: Algorithms, Bots, and Threats to Our Information Comfort-Zones","Laws, norms, policies, and institutions have failed to keep up with advances in artificial intelligence. Popularly, we still think of governance of these systems using quotes and metaphors from science fiction authors. The public awareness of the sophistication and capabilities of current systems are also skewed, often in extremes: predicting robot warfare and mind control or suffering complete naivete. Furthermore, the public often equates artificial intelligence with physical animatronics or other macro-embodiments like self-driving cars. 
 
The reality is one in which intelligent systems are embedded in more and more everyday products and services. The so-called “internet of things” represents a kind of ubiquitous computing that anticipates our needs and provides us information like time on daily commute without asking, in the case of Google Now, or adjusts the room temperature based on usage patterns, in the case of the Nest. More basically though, we see smarter algorithms powering seemingly neutral services like Google’s search engine or Facebook’s news feed. 
 
We are entering an era of personalization and technological service by means of machine learning. The concomitant effects of widespread intelligent systems are still unknown for the long-term, especially given that laws, norms, policies, and institutions will eventually develop. However in the near-term, we are already seeing the positive outcomes of convenience and superior service delivery overshadowed by problematic outcomes like censorship and privacy invasion. And beyond the consumer application of this technology, there exist weaponized versions in use by activists such as those under the banner of Anonymous, and states or terrorists pursuing toolkits for cyberwarfare. 
 
This panel will discuss the current capabilities and emerging trends in the deployment and use of intelligent systems, as well as the unique ethical, legal, and political challenges posed by them. We will explore the gaps that already exist in policies around issues like privacy and cyberwarfare, and how they are exacerbated by new intelligent systems and degrees of automation. Panelists will offer suggestions for policymakers, technology creators, and average users, as well as future research needs to advance the cause of effective governance around intelligent systems. 
 
 
SPEAKERS 
Erhardt Graeff (moderator) is a graduate student at the MIT Media Lab and MIT Center for Civic Media. He has studied and spoken about the civic potential of bots, as well as their privacy issues surrounding their commercial use. Erhardt holds an MPhil in Modern Society and Global Transformations from the University of Cambridge. 
 
Tim Hwang is a researcher of web communities, intelligent systems, and the economics of the Internet. He currently is principal investigator of the Social Architecture of Intelligent Systems project at the Data & Society Research Institute in New York. He has worked with the Mozilla Foundation, the Berkman Center for Internet and Society, the Electronic Frontier Foundation, Creative Commons, Google, Tumblr, and Imgur. He is also the co-founder of ROFLCon, a series of conferences on memes and internet culture. 
 
Kate Darling is a Research Specialist at the MIT Media Lab, and a Fellow at the Harvard Berkman Center for Internet & Society and the Yale Information Society Project. Her passion for technology and robots has led her to interdisciplinary fields: After co-teaching a robot ethics course at Harvard Law School, she now increasingly writes and lectures at the intersection of law and robotics, with focus on the legal impact of social issues. Kate holds a PhD in Intellectual Property at the ETH Zurich. 
 
Paulo Shakarian is a Major in the U.S. Army and an Assistant Professor in the Department of Electrical Engineering and Computer Science, U.S. Military Academy, West Point. Additionally, he is a Research Fellow with the West Point Network Science Center, as well as an Affiliate Scholar with the West Point Cyber Research Center. Paulo is also the primary investigator for the Algorithmic Network Science Group. He is the lead author of Introduction to Cyber-Warfare: A Multidisciplinary Approach. Paulo holds a Ph.D. in Computer Science from the University of Maryland, College Park. 
 
Kashmir Hill is a senior online editor at Forbes writing about privacy, technology and the law at The Not-So Private Parts. She has previously worked for the International Herald Tribune in Hong Kong; The Washington Examiner; the National Press Foundation; and Covington & Burling in Washington, D.C.",,2014,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
a82d5ed26e3245dcdd47309d6b4dd996a1b4e325,https://www.semanticscholar.org/paper/a82d5ed26e3245dcdd47309d6b4dd996a1b4e325,Announcing a Name Change and New Scope,"our transaction���s name has changed - from IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS PART B: CYBERNETICS to a more concise Transactions on Cybernetics (TCyb). For the past many months, the publications of the IEEE Systems, Man, and Cybernetics Society (SMCS) have been undergoing a major reorganization. The reorganization was based on numerous discussions with the SMCS membership and positive feedback that were received and on recommendations from the 2006 IEEE Periodicals Review. In November 2011, the IEEE Technical Activities Board approved the name and scope changes of all three transactions: 1) IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS PART A: SYSTEMS AND HUMANS to IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS; 2) IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS PART C: APPLICATIONS AND REVIEWS to IEEE TRANSACTIONS ON HUMAN���MACHINE SYSTEMS; and 3) this transaction. The realignment of the three transactions is meant to help better situate research topics, areas, and articles for the society through its publications. Here are the associated scopes for the three transactions. IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS - The scope of the IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS: SYSTEMS includes the fields of systems engineering. It includes issue formulation, analysis and modeling, decision making, and issue interpretation for any of the systems engineering life cycle phases associated with the definition, development, and deployment of large systems. In addition, it includes systems management, systems engineering processes, and a variety of systems engineering methods such as optimization, modeling, and simulation. IEEE TRANSACTIONS ON CYBERNETICS - The scope of IEEE TCyb includes computational approaches to the field of cybernetics. Specifically, the transactions welcomes papers on communication and control across machines or between machine, human, and organizations. The scope of TCyb includes areas such as computational intelligence, computer vision, neural networks, genetic algorithms, machine learning, fuzzy systems, cognitive systems, decision making, and robotics, to the extent that they contribute to the theme of cybernetics or demonstrate an application of cybernetics principles. IEEE TRANSACTIONS ON HUMAN���MACHINE SYSTEMS - The scope of the IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS includes the fields of human machine systems. It covers human systems and human organizational interactions, including cognitive ergonomics, system test and evaluation, and human information processing concerns in systems and organizations.",IEEE Trans. Cybern.,2013,10.1109/TCYB.2012.2232971,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7d4fb2af1232d49f60ff194d2da46846cf729323,https://www.semanticscholar.org/paper/7d4fb2af1232d49f60ff194d2da46846cf729323,International symposium on distributed computing and artificial intelligence 2008 (DCAI 2008),,,2008,10.1007/978-3-540-85863-8,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
577363b3f2e593b349f409a8c6996b8359dfcf3c,https://www.semanticscholar.org/paper/577363b3f2e593b349f409a8c6996b8359dfcf3c,Robots in the home,"Robots are coming, but what does this mean for ordinary folks? First of all, don’t believe all the hype. Lots of hobbyists and small ventures would have you believe that robots are already here, capable of a wide variety of interactions, including healthcare and monitoring medication compliance, security monitoring, education, errands, and entertainment. Robots are, of course, used in manufacturing, in search-and-rescue missions, and in the military. But when we get away from industry and the military and discuss machines that are reasonably priced, most of these so-called applications are more imagination than reality, with unreliable mechanisms barely able to get through demonstrations. For everyday home applications, the use of robots is restricted to entertainment, vacuum cleaners, and lawn mowers. Note, however, that the definition of “robot” varies widely, often being used for anything mobile, even though controlled by a human. Personally, I would classify intelligent home appliances as robots: my coffee maker, microwave oven, dishwasher, and clothes washer and dryer have more intelligence and actuators than robot vacuum cleaners—and they are also a lot more expensive. But they don’t move around the room, which for many people disqualifies them from the label of “robot.” Given that any successful product for the home must be affordable, reliable, safe, and usable by everyday people, what might a home robot do? And what would it look like? In the home, form probably will follow function. A kitchen robot might be built into the counter space, with dishwasher, pantry, coffee maker, and cooking units all arranged so that they can communicate with one another and pass items readily back and forth. An entertainment robot might take on a humanoid appearance (as in Wow Wee’s Robosapien), or an animal-like one (as in Sony’s Aibo). And robots that vacuum or mow lawns will look like, well, vacuum cleaners and lawn mowers. Making robots work well is incredibly difficult. Their sensory apparatus is limited because sensors are expensive and interpretation (especially commonsense knowledge) is still more suited for research than deployment. Robotic arms are expensive to build and not very reliable. This limits the range of possibilities. Mowing and vacuuming? Sure. Sorting laundry? Hard, but do-able. Picking up dirty items around the home? Doubtful. How about assistants for the elderly or those who need medical supervision? This is a booming area of exploration, but I am skeptical. Today’s devices are not reliable, versatile, or intelligent enough-not yet, anyway. Moreover, the social aspects of the interaction are far more complex than the technical ones, something the technology-driven enthusiasts typically fail to recognize. Three likely directions for the future are entertainment, home appliances, and education. We can start with today’s existing devices and slowly add on intelligence, manipulative ability, and function. Start small and build. The market for robots that entertain by being cute and cuddly is already well established. The second generation of vacuum cleaners is smarter than the first. Sony’s dog gets smarter and less expensive with each new version. We don’t yet think of washing machines, microwave ovens, and coffee makers as robots, but why not? They don’t move around the house, but they are getting better and smarter every year. And when the coffee maker is connected to the pantry and dishwasher, that will be a home robot worthy of the name—same for the coupling of sorting, washing, drying, and storing clothes. Education is a powerful possibility. There is already a solid basis of educational devices that aid learning. Today’s robots can read aloud in engaging voices. They can be cute and lovable-witness the responses to the multiple quasi-intelligent animals on the toy market. A robot could very well interact with a child, offering educational benefits as well. Why not have the robot help the child learn the alphabet, teach reading, vocabulary, pronunciation, basic arithmetic, maybe basic reasoning? Why not music and art, geography and history? And why restrict it to children? Adults can be willing and active learners. Now this is a direction worthy of exploration: robot as teacher. Not to replace school, not to replace human contact and interaction, but to supplement them. The beauty here is that these tasks are well within the abilities of today’s devices. They don’t require much mobility or sophisticated manipulators. Many technologists dream of implementing Neil Stephenson’s children’s tutor in his novel The Diamond Age: Or, A Young Lady’s Illustrated Primer. Why not? Here is a worthy challenge. T H E W A Y I S E E IT",Interactions,2005,10.1145/1052438.1052473,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
528f541adca5fe926af12dcacf4ddf8767b26c58,https://www.semanticscholar.org/paper/528f541adca5fe926af12dcacf4ddf8767b26c58,Smart-homes: A New Challenge for Artificial Intelligence,"Recent advances in information and communication technology opened the perspective of embedding “intelligence” in a number of components of our home, transforming it into a kind of multi-functional robot, usually referred to as “smart-home”. Leaving apart an excess of technology that probably we don’t want in our everyday life, a number of potential facilities exist that could effectively increase the quality of life by solving important social problems, such as security, elderly people leaving alone, child care, handicapped support. Here, we will investigate how AI techniques can be effectively integrated into smart-home technology solving important problems, which are beyond the reach of non-AI techniques. The first application area is “security and protection” intended either as “protection” against intruders or as protection/rescue of a person with respect to possible accidents in the domestic environment. A number of techniques have been proposed to detect risk conditions or to detect the occurrence of an accident. Some of them are based on image analysis and others make use of specific sensors that the person to be protected is required to wear. However, no single technique is fully reliable for deploying critical applications. We will discuss how AI methods are essential in order to trespass these limitations. The second application area is user interface. Even though simple traditional interfaces, such as switches, buttons, or infrared remote-controls, are the ones most users still prefer, there are a number of cases where advanced interfaces such as vocal or gestural are required. This is the case for handicapped people with severe mobility reduction due to aging or disease. This is a challenging area where machine learning and adaptive algorithms can be exploited to quickly develop specific solutions tailored to the user physiological conditions. The third application area is energy management, with the goal of reducing both the cost for the end user and the consumption of natural resources. Addressing this problem requires addressing two separate tasks. The first one consists of developing the monitoring systems that are able to identify and explain energy wastes. The second one deals with the planning of energy usage. In fact, available energy may have different costs during the day depending on the sources (public network, solar panels, wind mills), which in turn may depend on the weather conditions and on the geological location. Both tasks are classical AI topics, involving fault diagnosis, planning and machine learning. We will discuss the peculiar aspects, which are more challenging with this respect. 5th Indian International Conference on Artificial Intelligence (IICAI-11)",IICAI,2011,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
680fc7e5572e64be83d10a299b828e99a612ad5b,https://www.semanticscholar.org/paper/680fc7e5572e64be83d10a299b828e99a612ad5b,Science Magazine,"SCIENCE sciencemag.org T echnological innovations are penetrating all areas of science, making predominantly human activities a principal bottleneck in scientific progress while also making scientific advancement more subject to error and harder to reproduce. This is an area where a new generation of artificial intelligence (AI) systems can radically transform the practice of scientific discovery. Such systems are showing an increasing ability to automate scientific data analysis and discovery processes, can search systematically and correctly through hypothesis spaces to ensure best results, can autonomously discover complex patterns in data, and can reliably apply small-scale scientific processes consistently and transparently so that they can be easily reproduced. We discuss these advances and the steps that could help promote their development and deployment. Applying AI to the practice of science is not new. AI pioneer and Nobel laureate Herbert Simon hypothesized that cognitive mechanisms involved in scientific discovery are a special case of general human capabilities for problem-solving and, with colleagues, developed systems in the 1970s and 1980s that demonstrated reasoning capabilities for analyzing scientific data ( 1). Also in the 1970s, Joshua Lederberg (another Nobel winner) and colleagues developed the DENDRAL system for analyzing mass spectrometry data in order to hypothesize molecular structures ( 2). More recent breakthroughs, such as robot scientists and software that formulates laws for complex dynamical systems, demonstrate broader applicability of AI techniques for scientific discovery ( 3). Over the past two decades, AI has seen accelerating scientific advances and concomitant commercial-sector successes because of advances on three fronts: steady scholarly advances, especially as success has increased the numbers of interested participants; Moore’s law and steady exponential increases in computing power; and exponential increases in, and broad availability of, relevant data in volumes never previously seen. Those scientific efforts that have leveraged AI advances have largely harnessed sophisticated machine-learning techniques to create correlative predictions from large sets of “big data.” Such work aligns well with the current needs of petaand exascale science. However, AI has far broader capacity to ac-",,2009,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
aeb54c3ac5b8cf2ca4bcdea391811f8f596568df,https://www.semanticscholar.org/paper/aeb54c3ac5b8cf2ca4bcdea391811f8f596568df,Scalable Wall-Socket Multimedia Grid Computing,"Multimedia data is rapidly gaining importance along with recent deployment of publicly accessible digital tele vision archives, and surveillance cameras in public locations. In a few years, analyzing the content of multimedia data will be a problem of phenomenal proportions, as digital video may produce high data rates, and multimedia archives steadily run into Petabytes of storage space. Consequently, for urgent problems in multimedia content analy sis, Grid computing is rapidly becoming indispensable. For the last decade, theasyand efficient use of distributed resources (ultimately at a world-wide scale) has been the foremost visionary aim (or ’promise’) of the field of Grid computing. Today, with the advent of easy-to-use programming models and run-time systems, as developed by this proposal’s team members, Grids indeed are maturing towards a viable commodity for non-experts in highperformance computing. Moreover, our current results indicate that, particularly with the increasing deployment o f high-speed optical networks, efficient world-wide execution of massively communicating distributed applications is achievable. In other words, realization of the ’promise of the Grid’ is now finally within reach. This proposal aims to realize easy, efficient, and scalable distributed supercomputing for the multimedia domain. It does so by integrating several solutions that are designed t o remove the intrinsic difficulties of large-scale employmen t of Grids. Specifically, we aim to develop an application in which a digital camera (potentially as part of a robot system) is capable of real-time ’recognition’ of objects from a set of learned objects, while being connected to a largescale Grid system comprising of cluster computers located in Europe, and potentially even world-wide. Apart from the appealing nature of a demonstration including visual infor mation (i.e., the camera data) and speech (i.e., the real-ti me reaction of the system), we aim to show true Wall-Socket Grid Computing— meaning that the application is being compiled and started on a local desktop machine, with Grid resources being employed entirely transparently. 1 Application domain and technologies This is a time of transition in which information is more and more composed of multimedia items , i.e. a combination of pictorial, textual, and auditory data. The computer ized access to the content of such information is generally recognized as a tremendous challenge [6]. This is because the automatic deduction of semantics from multimedia data requires sophisticated techniques for data structuring, t ransformation, analysis, classification, and learning. In part icular, due to the nature of the data, the challenge is to discove r and interpret tiny fractions of useful information in a whir lwind of meaningless noise. Given the increasing storage and connectivity of multimedia data, automatic multimedia content analysis (MMCA) is becoming an ever more important research area. Multimedia content analysis considers all aspects of the automated extraction of new knowledge from multimedia data streams and archives. This proposal concentrates on image and video, as these contain the bulk of all data. Research in this area has made a giant leap forward with the introduction of machine learned multimedia analysis, yiel ding automatic categorization by visual object types, such a s human faces, cars, interviews, etcetera [7]. Fundamental MMCA research questions include: • Can we automatically find genres in images from a statistical evaluation of large image sets? • Can we learn to find objects in images and video streams from partially annotated image sets? Scientifically, these questions deal with the philosophica l foundations of cognition and the naming of things. Practically, solutions are urgently needed given the increasin g volume of multimedia data. 1.1 Color-based object recognition The path to finding computerized MMCA solutions that can compete with the visual capabilities of the human brain is littered with many fundamental problems. One such problem — which is the focus of this proposal — is that of",,2008,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
f9bcf83011066d01ad166bb3d54b4d7b8a3e947c,https://www.semanticscholar.org/paper/f9bcf83011066d01ad166bb3d54b4d7b8a3e947c,Commentary on : Monitoring and Diagnosis On-Board Software Module for Mars Driller ( MODI ),"This paper describes the MODI project, an ongoing effort to demonstrate the viability of an on-board software module for Monitoring and Diagnosis of remote systems, in particular those planned for deployment within the context of Martian exploration. The specific remote system of interest in the current prototype development effort is a drilling and sampling system that is expected to be onboard an early rover mission of the European Space Agency’s (ESA’s) Mars Exploration program. As pointed out in the paper, the ability to autonomously monitor and diagnose various mechanical systems (together with the companion ability to replan and reschedule activities in response to detected problems) is truly an enabler of robotic planetary exploration. Given both the substantial delay in communications with earth-based ground stations and the inherent uncertainty of remote rover activities, the prospect of ground-based mission control (via telepresence) is problematic at best, it will result in an extremely inefficient execution process, with corresponding degradation in scientific return. Efficient mission operations in this setting instead requires a division of responsibility, where groundbased decision-making takes a more strategic focus, and the mechanics of foreseeing task-level execution problems and managing task execution are handled locally (via onboard software systems). In light of this requirement, the project described in this paper is well motivated and important. The paper takes what it calls a fuzzy, knowledge-based approach to developing a monitoring and diagnosis module for the Martian drill application. Under this approach, variables that describe various sensory inputs about aspects of the drill are modeled as fuzzy sets, and the important relationships between these variables/sets are expressed as rules. The paper summarizes the current prototype model, consisting of 3 “set points” (variables, such as the drill’s RPM speed, whose values are imposed by humans and assumed to be non-varying), 7 drill-related parameters (variables for which nominal values and ranges are derived from data and monitored for alerting conditions), and 1 environment parameter characterizing terrain (to be incorporated in a future version of the prototype). A set of rules defined over these set points and parameters characterize the set of conditions under which an alert should be issued, with the fuzzy set membership functions associated with the variables in the rule determining the severity of the alarm. The domains of various drill parameters, the set of 36 distinct drillingscenarios for which monitoring rules were specified, and the set of monitoring rules were all developed through joint analysis between the authors and the company responsible for manufacturing the drill. The fuzzy set membership function of each drill variable for each scenario was derived from mean and standard deviation data using a machine learning process. Initial experiments designed to confirm the correctness of the inference process are reported and show the system’s ability to appropriately detect alarm conditions of various severity. Overall, the use of fuzzy logic would seem a natural approach to specifying a monitoring and diagnosis module for the drilling application, given the predisposition for natural variation (uncertainty) in sensor readings. The model developed in the paper seems to provide a promising initial solution, and no doubt this model will be enriched and refined as the project progresses. The projected capability to detect terrain type, for example, should greatly increase the robustness of the current prototype. To its credit from the perspective of producing a demonstrable prototype, the paper takes a rather straightforward approach to model specification i.e., identify all possible scenarios, construct specific membership functions for each variable for each scenario, and develop one or more rules for each scenario to delineate alarm conditions. However, this leads me to ask two broad questions.",,2006,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
7430a71e90737e2701e5f919862eb595a7def5da,https://www.semanticscholar.org/paper/7430a71e90737e2701e5f919862eb595a7def5da,Agents in the Enterprise,"The discovery of useful applications of agent technology is currently generating considerable interest in both research and business domains. The term ‘agent technology’ refers to a bag of software technologies and methods that realise systems comprised of multiple autonomous, goal-seeking agents that employ cooperation and collaboration to automate or systematise some problem space that existing technologies cannot. This paper cites analysis on the adoption of agent technologies and surveys the characteristics of today’s enterprise computing environment to build a case for the use of agent technology. Relevant characteristics of the technology that appear useful in the enterprise context include anthropomorphism (as a basis for decomposing complex, loosely coupled business systems), adaptability and the ability to manage change, and the concept of deploying multi-agent-based solution architectures one agent at a time, such that each individual agent enacts both a valuable point of automation in its own right and a node in a multi-agent solution. We report on Agentis’ experience in pursuing this approach by using three indicative agent-based solutions. Introduction new generation of software tools and techniques—collectively known as ‘agent technology’— mises to manage, in real time, the complexities of today’s enterprise. Although there have been able successes of enterprise-scale agent-based solutions to date, most have been in narrow industrial ains (such as robotics and flexible manufacturing) or applications involving simulation within ally constrained business domains. These are important classes of applications but they do not ak from the technology’s roots in research, nor do they significantly advance the more widespread ption of agent technology. e are interested in the benefits that the technology can bring to enterprise computing rather than in technology per se. This paper surveys the characteristics of today’s enterprise (and its computing ironment) and builds a case for the application of agent technology. Agent technology gives the tion architect a set of natural structures for designing enterprise solutions and applications. We eve that anthropomorphism is a useful basis for decomposing the next generation of complex, sely coupled business systems. An anthropomorphic approach provides useful mappings between ple (who perform situated acts in an essentially social context) and agents (which enact roles, pursue ls, have perception and beliefs, and follow plans toward intentions) that perform collaborative work ehalf of people. A g e n t i s I n t e r n a t i o n a l 2 0 0 5 T a y l o r , E v a n s G r e e n w o o d , O d e l l 1 A G E N T S I N T H E E N T E R P R I S E A S W E C 2 0 0 5 But structure is only one dimension of a successful enterprise solution. The enterprise imposes strict constraints on its systems, and some of these contradict the assumptions underlying established agentoriented approaches and agent runtime platforms. Previous attempts at deploying agents in the enterprise have failed partially because they have not adequately considered the essential characteristics of enterprise systems, such as performance, scalability, reliability, business transparency, and adaptability. We offer some pragmatic solutions to each of these problems—an approach based on bringing agents to the enterprise, rather than the enterprise to agent technology. 1.1 The Promise of Agent Technology in the Enterprise Software agents are autonomous problem-solving computational entities that can demonstrate more effective operation in dynamic and open environments than can traditional technologies. In the enterprise, individual agents can automate complex control and monitoring behaviours not possible with current technologies, or they can operate in collaboration with other agents (including both people and software) to achieve automation in highly dynamic and complex environments. Agents go beyond objects (for example, Java objects) in that they implement autonomy by exercising choice over their internal actions and external interactions. An agent may exercise autonomy by selecting what processing to perform (at runtime, based on prevailing conditions) to achieve a particular goal. Agents in a multi-agent system may collaborate to achieve individual or shared objectives. Unlike objects, agents may be assigned one or more goals or tasks by their owners and then set running to achieve those goals. Agent-based systems are implemented using a wide range of current technologies, including object-oriented languages and distributed object services, Web Services, directories, and many others. The development of agent technologies is surfacing a range of specific techniques and algorithms for dealing with interactions in dynamic, open environments. These include solutions, patterns, protocols and standards for balancing reaction and deliberation in individual agents, learning from and about other agents in the environment, eliciting and acting upon user preferences, finding ways to negotiate and cooperate with other agents, and developing means of forming and managing coalitions. Multiagent solutions map naturally to problem spaces typified by complexity, loose coupling, dynamism and collaboration, where exiting agents (normally people) employ a high degree of autonomy, specialisation, interpretation and skill to plan, resolve deals, allocate resources or negotiate paths. For example, multi-agent systems are providing faster and more effective methods of resource allocation in complex environments, such as the management of utility networks, than human-centred approaches. In businesses where timely provisioning or customer servicing is important (such as telecommunications), agents are monitoring and managing provisioning processes and the overarching service level agreements. In healthcare, agents are automating patient monitoring and providing intelligent integration of monitoring devices. Other applications can be found anywhere where markets, economies or societies, or biological environments need to be monitored or managed (Agentlink III, 2004). 1.2 The Current State of Agent Technology Adoption Agent technology’s broad and encompassing base makes assessments and predictions on its adoption in organisations difficult. Gartner’s July 2004 analysis places various agent technologies, application domains and drivers at different points in the adoption cycle (Gartner 2004a-f). In the infrastructure domain, business process execution languages (BPEL) are on the rise with between 1% and 5% market penetration. Basic Web Services for service definition and application integration (using SOAP and WSDL) are achieving mainstream adoption as evidenced by implementation by major software vendors, with 20% to 50% market penetration. Advanced Web Services offering higher quality of service (to enable advanced business-critical functions over standards-based networks using SOAP, WSDL, UDDI, WS-Security and WS-R) depend on the availability and maturity of standards, and implementations are not yet fully available from vendors. Gartner observes that narrow business domains for the technology have not yet emerged, but that this demand is currently expressed in the building momentum of the Semantic Web. The Semantic Web— the adoption of metadata and protocols so that machines can automatically process, integrate and reason © A g e n t i s I n t e r n a t i o n a l 2 0 0 5 T a y l o r , E v a n s G r e e n w o o d , O d e l l 2 A G E N T S I N T H E E N T E R P R I S E A S W E C 2 0 0 5 about data across different applications—has less than 1% market penetration. Another current transformational concept, the ‘Trading Grid’ (an interconnection of networks and marketplaces to support virtual organisations) is also at the very start of the adoption cycle. Electronic markets (eMarketplaces) now have up to 5% market penetration. Each of these domains and technology families are predicted by Gartner to take up to 10 years to plateau. Intelligent agents as a whole are perceived as having been ‘over-hyped’ to date. However, selfservice agents (a specific type of intelligent agent that acts on a customers’ or business’s behalf to automate transactions) are catching on and have reached up to 5% penetration. In all these cases, however, these are lightweight agents and applications of agents, with the mainstream of agent technologies still to engage the marketplace. 2 Agent-based Systems in the Enterprise Today’s enterprise computing environment is demanding of applications in terms of performance and scalability, adaptability, and verifiability. Because of this, many successful agent prototypes fail when moved into production environments. Enterprise-class workloads, for example, are typified by high volumes of relatively simple transactions, and relatively fixed interaction patterns. By contrast, most multi-agent runtime platforms support low volumes but complex interactions with each other and with their environment. The task of mapping agent concepts and the necessary runtime support services onto enterprise transaction platforms (such as J2EE) is not a trivial process. Important issues include dealing with threads, knowing when and how to use stateless and stateful objects in an agent’s implementation, agent identity, object and database locking, support for the EJB container’s interface, efficient agent belief or data persistence, and support for loosely-coupled inter-agent communication. Other important characteristics of the enterprise that an agent-based system must address include reliability, extensibility, maintainability, and the need to coexist in today’s highly secured and intensively managed operational environments. Each of these issues requires the application of a range of techniques and approaches to reach a workable solution. Agent platforms (and agent product vendors who claim enterprise-ready runtime and operational ",,2005,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
39af7ac01ec91f07bf18aba53cf6d43ab34a1792,https://www.semanticscholar.org/paper/39af7ac01ec91f07bf18aba53cf6d43ab34a1792,From the guest editor,,,2009,10.1007/BF00987293,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
91903c0cae572554ff6828366d1461b083c582de,https://www.semanticscholar.org/paper/91903c0cae572554ff6828366d1461b083c582de,"Intelligent problem solving : methodologies and approaches : 13th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems, IEA/AIE 2000, New Orleans, Louisiana, USA, June 19-22, 2000 : proceedings","Keynote Presentation.- Multisensor Data Fusion.- Intelligent Agents I.- Implementing Multi-party Agent Conversations.- Agreement and Coalition Formation in Multiagent-Based Virtual Marketplaces.- A Framework for the Development of Cooperative Configuration Agents.- Java-Based Distributed Intelligent Agent Architecture for Building Safety-Critical Tele-Inspection Systems on the Internet.- Artificial Neural Network I.- The Use of AI Methods for Evaluating Condition Dependent Dynamic Models of Vehicle Brake Squeal.- Towards an Estimation Aid for Nuclear Power Plant Refuelling Operations.- Drilling Performance Prediction Using General Regression Neural Networks.- Identifying Significant Parameters for Hall-Heroult Process Using General Regression Neural Networks.- Data Mining I.- Mapping Object-Oriented Systems to Distributed Systems Using Data Mining Techniques.- Scaling the Data Mining Step in Knowledge Discovery Using Oceanographic Data.- Information Management and Process Improvement Using Data Mining Techniques.- Combinatorial Optimization.- Combinatorial Optimization A Comparative Analysis of Search Methods as Applied to Shearographic Fringe Modelling.- Vision Guided Bin Picking and Mounting in a Flexible Assembly Cell.- A Brokering Algorithm for Cost & QoS-Based Winner Determination in Combinatorial Auctions.- An Overview of a Synergetic Combination of Local Search with Evolutionary Learning to Solve Optimization Problems.- Expert Systems I.- Maintenance of KBS's by Domain Experts.- A Simulation-Based Procedure for Expert System Evaluation.- Gas Circulator Design Advisory System: A Web Based Decision Support System for the Nuclear Industry.- Expert Systems and Mathematical Optimization Approaches on Physical Layout Optimization Problems.- Diagnosis I.- Locating Bugs in Java Programs - First Results of the Java Diagnosis Experiments Project.- Application of a Real-Time Expert System for Fault Diagnosis.- Operative Diagnosis Algorithms for Single-Fault in Graph-Based Systems.- On a Model-Based Diagnosis for Synchronous Boolean Network.- DermatExpert: Dermatological Diagnosis through the Internet.- Best Papers.- Aerial Spray Deposition Management Using the Genetic Algorithm.- Dynamic Data Mining.- Information Systems I.- Knowledge-Intensive Gathering and Integration of Statistical Information on European Fisheries.- Using a Semantic Model and XML for Document Annotation.- Understanding Support of Group in Web Collaborative Learning, Based on Divergence among Different Answering Processes.- Fuzzy Logic and Its Applications.- Fuzzy Modeling Approach for Integrated Assessments Using Cultural Theory.- Fuzzy Knowledge-Based System for Performing Conflation in Geographical Information Systems.- Modeling of, and Reasoning with Recurrent Events with Imprecise Durations.- Linguistic Approximation and Semantic Adjustment in the Modeling Process.- A Fuzzy Inference Algorithm for Lithology Analysis in Formation Evaluation.- Intelligent Agents II.- Approximating the 0-1 Multiple Knapsack Problem with Agent Decomposition and Market Negotiation.- Design and Development of Autonomous Intelligence Smart Sensors.- ADDGEO: An Intelligent Agent to Assist Geologist Finding Petroleum in Offshore Lands.- SOMulANT: Organizing Information Using Multiple Agents.- Design.- Inventiveness as Belief Revision and a Heuristic Rule of Inventive Design.- A Decision Support Tool for the Conceptual Design of De-oiling Systems.- ProCon: Decision Support for Resource Management in a Global Production Network.- Intelligent Infrastructure That Support System's Changes.- Diagnosis II.- Using Description Logics for Case-Based Reasoning in Hybrid Diagnosis.- Printer Troubleshooting Using Bayesian Networks.- Using XML and Other Techniques to Enhance Supportability of Diagnostic Expert Systems.- Learning and Diagnosis in Manufacturing Processes through an Executable Bayesian Network.- Expert Systems II.- Solving Large Configuration Problems Efficiently by Clustering the ConBaCon Model.- XProM: A Collaborative Knowledge-Based Project Management Tool.- Building Logistics Networks Using Model-Based Reasoning Techniques.- A Supporting System for Colored Knitting Design.- Machine Learning and Its Applications.- Learning Middle-Game Patterns in Chess: A Case Study.- Meta-classifiers and Selective Superiority.- Logic and Its Applications.- The Formal Specification and Implementation of a Modest First Order Temporal Logic.- Determining Effective Military Decisive Points through Knowledge-Rich Case-Based Reasoning.- A Constraint-Based Approach to Simulate Faults in Telecommunication Networks.- A Least Common Subsumer Operation for an Expressive Description Logic.- Pattern Recognition.- Blob Analysis Using Watershed Transformation.- A Novel Fusion of Holistic and Analytical Paradigms for the Recognition of Handwritten Address Fields.- Pawian - A Parallel Image Recognition System.- An Automatic Configuration System for Handwriting Recognition Problems.- Detection of Circular Object with a High Speed Algorithm.- Artificial Neural Networks II.- Neural Network Based Compensation of Micromachined Accelerometers for Static and Low Frequency Applications.- Improving Peanut Maturity Prediction Using a Hybrid Artificial Neural Network and Fuzzy Inference System.- CIM - The Hybrid Symbolic/Connectionist Rule-Based Inference System.- A Neural Network Document Classifier with Linguistic Feature Selection.- Color Pattern Recognition on the Random Neural Network Model.- Integrating Neural Network and Symbolic Inference for Predictions in Food Extrusion Process.- Natural Language Processing.- Automatic Priority Assignment to E-mail Messages Based on Information Extraction and User's Action History.- Information Extraction for Validation of Software Documentation.- Object Orientation in Natural Language Processing.- Genetic Algorithm.- A Study of Order Based Genetic and Evolutionary Algorithms in Combinatorial Optimization Problems.- Nuclear Power Plant Preventive Maintenance Planning Using Genetic Algorithms.- Progress Report: Improving the Stock Price Forecasting Performance of the Bull Flag Heuristic with Genetic Algorithms and Neural Networks.- Advanced Reservoir Simulation Using Soft Computing.- Information Systems II.- Forest Ecosystem Management via the NED Intelligent Information System.- Friendly Information Retrieval through Adaptive Restructuring of Information Space.- A Smart Pointer Technique for Distributed Spatial Databases.- Distributed Problem Solving.- Deploying the Mobile-Agent Technology in Warehouse Management.- A Lightweight Capability Communication Mechanism.- Model-Based Control for Industrial Processes Using a Virtual Laboratory.- Autonomous Agents for Distributed Problem Solving in Condition Monitoring.- Modeling Issues for Rubber-Sheeting Process in an Object Oriented, Distributed and Parallel Environment.- Intelligent Agents III.- Reasoning and Belief Revision in an Agent for Emergent Process Management.- System Design and Control Framework for an Autonomous Mobile Robot Application on Predefined Ferromagnetic Surfaces.- Intelligent and Self-Adaptive Interface.- Agent Architecture: Using Java Exceptions in a Nonstandard Way and an Object Oriented Approach to Evolution of Intelligence.- Artificial Neural Networks III.- Neural Network Based Machinability Evaluation.- Performance of MGMDH Network on Structural Piecewise System Identification.- Black-Box Identification of the Electromagnetic Torque of Induction Motors: Polynomial and Neural Models.",,2000,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
e0ff1e95e940e29e7cdc99a84492a37e3a051786,https://www.semanticscholar.org/paper/e0ff1e95e940e29e7cdc99a84492a37e3a051786,RIACS FY2001 Annual Report,"Recently, there has been shift from consideration of optimal decisions in games to a consideration of optimal decision-making programs for dynamic, inaccessible, complex environments such as the real world. Perfect rationality is impossible in these environments, because of prohibiting deliberation complexity. Anytime algorithms attempt to trade off result quality for the time or memory needed to generate results. Bounded rational agents are ones that always take the actions that are expected to optimize their performance measure, given the percept sequence they have seen so far and limited resources they have. Process algebras, with basic programming operators, has been used to study the behaviors of interactive multi-agent systems and leading to more expressive models than Turing Machines, e.g., Interaction Machines. By extending process algebra operators with von Neumann/Morgenstern’s costs/utilities, anytime algorithms can be viewed as a basis for a general theory of computation. As the result we shift a computational paradigm from the design of agents achieving one-time goals, to the agents who persistently attempt to optimize their happiness. We call this approach $-calculus (pronounced “cost-calculus”), which is a higher-order polyadic process algebra with a utility (cost) allowing to capture bounded optimization and metareasoning in distributed interactive AI systems. $-calculus extends performance measures beyond time to include answer quality and uncertainty, using k Omega-optimization to deal with spatial and temporal constraints in a flexible way. This is a very general model, just as neural networks or genetic algorithms, leading to a new programming paradigm (cost languages) and a new class of computer architectures (cost-driven computers). The NSERC supported project on $-calculus aims at investigation, design and implementation of a wide class of adaptive real-time distributed complex systems exhibiting meta-computation and optimization. It has also been applied to the Office of Naval Research SAMON robotics testbed to derive GBML (Generic Behavior Message-passing Language) for behavior planning, control and communication of heterogeneous Autonomous Underwater Vehicles (AUVs). Some preliminary ideas have also been utilized in the 5th Generation ESPRIT SPAN project on integration of objectoriented, logic, procedural and functional styles of programming in parallel architectures. It appears that $-calculus can be useful for the NASA Information Power Grid (IPG) Project. The IPG testbed provides access to a widely distributed network of high performance computers. $calculus resource-bounded optimization allows for flexible allocation of resources and scalability needed to tackle hard computation problems, thus $-calculus could provide a unifying metasystem framework for the Information Power Grid. Biosketch: Dr. Eberbach is a Professor at School of Computer Science, Acadia University and an Adjunct Professor at Faculty of Graduate Studies, Dalhousie University, Canada. Previously he was Senior Scientist at Applied Research Lab, The Pennsylvania State University, Visiting Professor at The University of Memphis, USA, Research Scientist at University College London, U.K., Assistant Professor in Poland, and he also has industrial experience. Professor Eberbach’s current work is in the areas of process algebras, resource bounded optimization, autonomous agents and mobile RIACS FY2001 Annual Report October 2000 through September 2001 -135robotics. General topics of interest are new computing paradigms, languages and architectures, distributed computing, concurrency and interaction, evolutionary computing and neural nets. More information about projects, publications, courses taught can be found at http://cs.acadiau.ca/~eberbach October 27, 2000: Feng Zhao, Ph.D.,Principal Scientist, Xerox PARC “Smart Sensors, Collaborative Sensemaking” Imagine a world in which we live where smart roads would be able to tell us when they need repair and which is the best direction to get to the Giants game, smart factories would stock up just enough inventory, ... The rapid advances in micro-electro-mechanical systems (MEMS) and lower-power wireless networking have enabled a new generation of tiny, cheap, networked sensors that can be “sprayed” on roads, across machines, and on walls. However, these massively distributed sensor networks must overcome a set of technological hurdles before they become widely deployable. Keeping up with the constant onslaught of sensory data from say 100,000 sensors is akin to drinking from a fire hose. The Xerox PARC Smart Matter Diagnostics and Collaborative Sensing Project studies the fundamental problems of distilling high-level, humaninterpretable knowledge from distributed heterogeneous sensor signals in a rapid and scalable manner. We are developing powerful algorithms and software systems to enable a wide range of applications, from sensor-rich health monitoring of electro-mechanical equipment to human-aware environments that leverage sensors to support synergistic interactions with the physical world. Biosketch: Feng Zhao is a Principal Scientist in the Systems and Practices Laboratory at Xerox PARC. Dr. Zhao leads the Smart Matter Diagnostics Project that investigates how sensors and networking technology can change the way we build and interact with physical devices and environments. His research interest includes distributed sensor data analysis, diagnostics, qualitative reasoning, and control of dynamical systems. Dr. Zhao received his PhD in Electrical Engineering and Computer Science from MIT in 1992, where he developed one of the first algorithms for fast N-body computation and phase-space nonlinear control synthesis. From 1992 to 1999, he was Assistant and Associate Professor of Computer and Information Science at Ohio State University. His INSIGHT Group developed the SAL software tool for rapid prototyping of spatio-temporal data analysis applications; the tool is currently used by a number of other research groups. Currently, he is also Consulting Associate Professor of Computer Science at Stanford. Dr. Zhao was National Science Foundation and Office of Naval Research Young Investigator, and an Alfred P. Sloan Research Fellow in Computer Science. He has authored or co-authored about 50 peer-reviewed technical papers in the areas of smart matter, artificial intelligence, nonlinear control, and programming tools. October 12, 2000: Irem Tumer, Intelligent Health and Safety Group NASA/Ames “Influence of Variations on Systems’ Performance And Safety” High-risk aerospace components have to meet very stringent quality, performance, and safety requirements. Any source of variation is of concern, as it may result in scrap or rework (translating into production delays), poor performance (translating into customer dissatisfaction), and potentially unsafe flying conditions (translating into catastrophic failures). As part of the Intelligent RIACS FY2001 Annual Report October 2000 through September 2001 -136Health and Safety group, we have been designing controlled experiments to understand various sources of variations in helicopter transmissions, collecting vibration data, and analyzing the data for indicators of the variations. We are looking for normal and abnormal sources of variation that affect performance and indicators of these variations to provide warning about potential failures during flight. The experiments include: • Flight tests using an AH-1 and an OH-58 helicopter, to determine the variations introduced due to regular maneuvering and the covariance with environmental conditions, engine torque, etc.; • OH-58 transmission test-rig tests to determine the effect of variations due to different levels of torque, mast bending, and mast lifting forces, as well as pinion reinstallation effects; • Machinery Fault Simulator tests to test the effect of prefabricated defects and inherent design and manufacturing variations on gears, bearings, etc. In this talk, I will present an overview of our group’s research goals, discuss the experiments and go over some of the results from the data analyses conducted so far. I will then discuss the current work and future directions in developing formalized methods for design and manufacturing engineers, using the variation information from empirical and analytical studies. RIACS FY2001 Annual Report October 2000 through September 2001 -137III.B RIACS-Supported Workshops As part of its mission of fostering ties with the academic community in IT, RIACS provides financial, administrative, and technical support for selected workshops involving RIACS scientists. The following workshops were supported during this reporting year: Workshop on Verification and Validation of Software The RIACS Workshop on the Verification and Validation of Autonomous and Adaptive Systems took place at Asilomar Conference Center, Pacific Grove, CA, 5-7 Dec 2000. Discussions included: V&V of Intelligent Systems: How to verify and validate systems featuring some form of AI-based technique, such as model-based, rule-based or knowledge-based systems. V&V of Adaptive Systems: How to verify and validate systems featuring adaptive behavior, either in the form of parametric adaptation (e.g. neural nets, reinforcement learning) or control adaptation (e.g. genetic programming). V&V of Complex Systems: How to verify and validate systems with different interacting parts, either within a given location (e.g. layered control architectures) and among several locations (homogenous or heterogeneous multi-agent systems). Workshop on Model-based Validation of Intelligence Lina Khatib (Kestrel) and Charles Pecheur co-organized a symposium on “Model-based Validation of Intelligence” as part of the AAAI Spring Symposium Series in March 2001. We provided the technical content (announcement, reviews and selection of articles, final program) while AAAI provided the logistics (rooms, registra",,2001,,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy')
