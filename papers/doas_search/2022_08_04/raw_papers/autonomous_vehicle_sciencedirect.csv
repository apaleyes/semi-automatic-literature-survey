id,type,publication,publisher,publication_date,database,title,url,abstract,query_name,query_value
10.1016/j.compeleceng.2022.108163,Journal,Computers and Electrical Engineering,scopus,2022-09-01,sciencedirect,Intelligent energy and ecosystem for real-time monitoring of glaciers,https://api.elsevier.com/content/abstract/scopus_id/85133281340,"The United Nations is deeply concerned about global warming and its impacts on natural resources. Simultaneously, it has been recommended that cutting-edge technologies be employed to predict the impact of climate change on natural reservoirs such as glaciers. With the motivation of the above facts, this study investigates the impact and significance of emerging and cutting-edge technologies like Remote Sensing, the Internet of Things (IoT), Artificial Intelligence (AI), Unmanned Aerial Vehicles (UAVs), and robots implementation for the digitalization in glaciers. The study identified that convergence of AI, Machine Learning (ML), and Deep Learning approaches with Spatio-temporal data empowers to detect non-linear characteristics, especially in high mountainous regions due to their diversity and unpredictable nature. The article suggested valuable recommendations such as establishing an intelligent eco-system in glaciers, low-cost intelligent IoT devices with intelligent energy algorithms, ML empowered edge devices, glacier-resistant rescue robots, and Wearable IoT-based safety guide devices.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rsase.2022.100787,Journal,Remote Sensing Applications: Society and Environment,scopus,2022-08-01,sciencedirect,A novel drone-based system for accurate human temperature measurement and disease symptoms detection using thermography and AI,https://api.elsevier.com/content/abstract/scopus_id/85132238263,"The world continues to witness several waves of COVID-19 spread due to the emergence of new variants of the SARS-CoV-2 virus. Stopping the spread requires synergistic efforts that include the use of technologies such as unmanned aerial vehicles and machine learning. This paper presents a novel system for detecting disease symptoms from a distance using unmanned aerial vehicles equipped with thermal and visual image sensors. A hardware/software system that uses thermography to accurately calculate the skin temperature of targeted individuals using thermal cameras is developed. In addition, machine vision algorithms are developed to recognize human actions such as coughing and sneezing, which are paramount symptoms of respiratory infections. The proposed system is implemented and tested in outdoor environments. The results of experiments showed that the system can determine the skin temperature of multiple targeted individuals simultaneously with an error of less than 1 °C. The field experiments showed that the developed system is capable of simultaneously measuring the temperature of more than 10 individuals in less than 5 seconds. Just to give a perspective, it takes at least 3 seconds to measure one individual's temperature if this was done using traditional methods. Furthermore, the results showed that the system has accurately detected actions such as coughing and sneezing with almost 96% accuracy at a real-time performance of 28 frames/second.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neunet.2022.04.008,Journal,Neural Networks,scopus,2022-08-01,sciencedirect,Risk-based implementation of COLREGs for autonomous surface vehicles using deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85129960792,"Autonomous systems are becoming ubiquitous and gaining momentum within the marine sector. Since the electrification of transport is happening simultaneously, autonomous marine vessels can reduce environmental impact, lower costs, and increase efficiency. Although close monitoring is still required to ensure safety, the ultimate goal is full autonomy. One major milestone is to develop a control system that is versatile enough to handle any weather and encounter that is also robust and reliable. Additionally, the control system must adhere to the International Regulations for Preventing Collisions at Sea (COLREGs) for successful interaction with human sailors. Since the COLREGs were written for the human mind to interpret, they are written in ambiguous prose and therefore not machine-readable or verifiable. Due to these challenges and the wide variety of situations to be tackled, classical model-based approaches prove complicated to implement and computationally heavy. Within machine learning (ML), deep reinforcement learning (DRL) has shown great potential for a wide range of applications. The model-free and self-learning properties of DRL make it a promising candidate for autonomous vessels. In this work, a subset of the COLREGs is incorporated into a DRL-based path following and obstacle avoidance system using collision risk theory. The resulting autonomous agent dynamically interpolates between path following and COLREG-compliant collision avoidance in the training scenario, isolated encounter situations, and AIS-based simulations of real-world scenarios.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.patcog.2022.108614,Journal,Pattern Recognition,scopus,2022-07-01,sciencedirect,Learning residue-aware correlation filters and refining scale for real-time UAV tracking,https://api.elsevier.com/content/abstract/scopus_id/85125856281,"Unmanned aerial vehicle (UAV)-based tracking finds its applications in agriculture, aviation, navigation, transportation and public security, etc and develops rapidly recently. However, due to limitations of computing resources, battery capacity, requirement of low power and maximum load of UAV, the deployment of deep learning-based tracking algorithms in UAV is currently not feasible and therefore discriminative correlation filters (DCF)-based trackers have stood out in UAV tracking community for their high efficiency and appealing robustness on a single CPU. But confronted with difficult challenges the efficiency and accuracy of existing DCF-based approaches is still not satisfying. Inspired by the good optimization properties associated with residue representation, in this paper we exploit the residue nature inherent to videos and propose residue-aware correlation filters which demonstrate better convergence properties in filter learning. In addition, we propose a scale refinement strategy to improve the wildly adopted discriminative scale estimation in DCF-based trackers, which, in fact, greatly impacts the precision and accuracy of the trackers since accumulated scale error degrades the appearance model as online updating goes on. Extensive experiments are conducted on four UAV benchmarks, namely, UAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The results show that our method achieves state-of-the-art performance in UAV tracking.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2022.03.026,Journal,Neurocomputing,scopus,2022-06-14,sciencedirect,A review on varying-parameter convergence differential neural network,https://api.elsevier.com/content/abstract/scopus_id/85126975231,"Inspired by the nature of actual dynamics systems with time-varying parameters, varying-parameter convergence differential neural network (termed as VP-CDNN) has been put forward and played a crucial role in obtaining the real-time solution of algebraic equations and optimization problems. Plenty of fruitful literatures report that such a neural network breaks the bottlenecks of the conventional algorithms and presents superior convergence performance and strong anti-noise capability in the time-varying problem solving. This paper presents an overall review about VP-CDNN in different mathematical problems solving such as time-varying quadratic-programming equation, time-varying Sylvester equation, nonlinear and nonconvex equation and so on. Besides its extension forms such as anti-noise VP-CDNN, finite-time VP-CDNN, fuzzy VP-CDNN and discrete-time VP-CDNN are briefly introduced in mathematical problems solving. Additionally, the applications of VP-CDNN in robot motion planning, unmanned aerial vehicles, venture investment and other applications are illustrated for practical implementation. The conclusion summarizes the superiority of VP-CDNN and indicates several future research direction.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isprsjprs.2022.04.002,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2022-06-01,sciencedirect,Vision based crown loss estimation for individual trees with remote aerial robots,https://api.elsevier.com/content/abstract/scopus_id/85128243118,"With the capability of capturing high-resolution imagery data and the ease of accessing remote areas, aerial robots are becoming increasingly popular for forest health monitoring applications. For example, forestry tasks such as field surveys and foliar sampling which are generally manual and labour intensive can be automated with remotely controlled aerial robots. In this study, we propose two new online frameworks to quantify and rank the severity of individual tree crown loss. The real-time crown loss estimation (RTCLE) model localises and classifies individual trees into their respective crown loss percentage bins. Experiments are conducted to investigate if synthetically generated tree images can be used to train the RTCLE model as real images with diverse viewpoints are generally expensive to collect. Results have shown that synthetic data training helps to achieve a satisfactory baseline mean average precision (mAP) which can be further improved with just some additional real imagery data. We showed that the mAP can be increased approximately from 60% to 78% by mixing the real dataset with the generated synthetic data. For individual tree crown loss ranking, a two-step crown loss ranking (TSCLR) framework is developed to handle the inconsistently labelled crown loss data. The TSCLR framework detects individual trees before ranking them based on some relative crown loss severity measures. The tree detection model is trained with the combined dataset used in the RTCLE model training where we achieved an mAP of approximately 95% suggesting that the model generalises well to unseen datasets. The relative crown loss severity of each tree is estimated, with deep representation learning, by a probabilistic encoder from a fully trained variational autoencoder (VAE) model. The VAE is trained end-to-end to reconstruct tree images in a background agnostic way. Based on a conservative evaluation, the estimated crown loss severity from the probabilistic encoder generally showed moderate agreement with the expert’s estimation across all species of trees present in the dataset. All the software pipelines, the dataset, and the synthetic dataset generation can be found in the GitHub link.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2022.104854,Journal,Engineering Applications of Artificial Intelligence,scopus,2022-06-01,sciencedirect,Heuristic-driven strategy for boosting aerial photography with multi-UAV-aided Internet-of-Things platforms,https://api.elsevier.com/content/abstract/scopus_id/85128181273,"Unmanned Aerial Vehicles (UAVs) are gaining much attractiveness due to the emerging 5G, Internet of things applications and the advances in artificial intelligence. Their application fields encompass both civil and military domains. UAVs can ubiquitously supply many Internet-of-things-driven services; as such, they can be configured into airborne networks to provide flexible aerial views, which is essential for photography and videography-based applications like 3D mapping and real-time monitoring. However, many research challenges need to be tackled to facilitate the deployment of such promising applications. This paper addresses a non-convex NP-hard problem of deploying a fleet of UAVs equipped with rotating gimbal-mounted cameras over large-scaled terrains. The problem is heuristically solved in two phases. Firstly, we introduce a fast parallel multi-verse swarm optimization algorithm. A hybrid multi-objective heuristic coalescing two relevant concepts of stochastic optimization: (1) Improved Multi-Objective Particle Swarm Optimization; (2) Improved Multi-Objective Multi-verse Optimization. This heuristic holds several algorithmic tweaks, such as Pareto-based population splitting, Taguchi-based parameters tuning, adaptive mutation, and is used to derive the most near-optimal hovering coordinates of UAVs under two customized objective functions. Secondly, we adopt an efficient gimbal-based rotation and synchronization strategy allowing the UAVs to rotate their cameras resourcefully in four cardinal directions to boost the aerial photographing coverage with few maneuvers. The claimed performance is rigorously endorsed with intensive simulations and comparative analyses (e.g., analysis of variances, Wilcoxon test, performance index) against twenty multi-objective bio-inspired heuristics. The results show that our approach has the upper hand in terms of efficiency and accuracy.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jnca.2022.103341,Journal,Journal of Network and Computer Applications,scopus,2022-05-01,sciencedirect,Survey on computation offloading in UAV-Enabled mobile edge computing,https://api.elsevier.com/content/abstract/scopus_id/85123949491,"With the increasing growth of internet-of-things (IoT) devices, effective computation performance has become a critical issue. Many services provided by IoT devices (e.g., augmented reality, location-tracking, traffic systems, and autonomous driving) require intensive real-time data processing, which demands powerful computational resources. Mobile edge computing (MEC) has been introduced to effectively handle this problem reliably over the internet. The inclusion of a MEC server allows computationally intensive tasks to be offloaded from IoT devices. However, communication overhead and delays are major drawbacks. With the advantages of high mobility and low cost, unmanned aerial vehicles (UAVs) can mitigate this issue by acting as MEC servers. The offloading decisions for such scenarios involve service latency, energy/power consumption, and execution delays. For this reason, this study reviews UAV-enabled MEC solutions in which offloading was the focus of research. We compare the algorithms qualitatively to assess features and performance. Finally, we discuss open issues and research challenges in terms of design and implementation.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2022.104717,Journal,Engineering Applications of Artificial Intelligence,scopus,2022-04-01,sciencedirect,Robust EMRAN-aided coupled controller for autonomous vehicles,https://api.elsevier.com/content/abstract/scopus_id/85124465739,"This paper presents a coupled, neural network-aided longitudinal cruise and lateral path-tracking controller for an autonomous vehicle with model uncertainties and experiencing unknown external disturbances. Using a feedback error learning mechanism, an inverse vehicle dynamics learning scheme utilizing an adaptive Radial Basis Function (RBF) neural network, referred to as the Extended Minimal Resource Allocating Network (EMRAN) is employed. EMRAN uses an extended Kalman filter for online learning and weight updates, and also incorporates a growing/pruning strategy for maintaining a compact network for easier real-time implementation. The online learning algorithm handles the parametric uncertainties and eliminates the effect of unknown disturbances on the road. Combined with a self-regulating learning scheme for improving generalization performance, the proposed EMRAN-aided control architecture aids a basic PID cruise and Stanley path-tracking controllers in a coupled form. Its performance and robustness to various disturbances and uncertainties are compared with the conventional PID and Stanley controllers, along with a comparison with a fuzzy-based PID controller and an active disturbance rejection control (ADRC) scheme. Simulation results are presented for both slow and high speed scenarios. The root mean square (RMS) and maximum tracking errors clearly indicate the effectiveness of the proposed control scheme in achieving better tracking performance in autonomous vehicles under unknown environments.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compeleceng.2022.107700,Journal,Computers and Electrical Engineering,scopus,2022-03-01,sciencedirect,Modeling multiple vehicle interaction constraints for behavior prediction of vehicles on highways,https://api.elsevier.com/content/abstract/scopus_id/85123282977,"In the context of autonomous driving and road situation awareness, this manuscript introduces a Bayesian network that enables the prediction of participant vehicles (PVs) circulating on a highway. The network architecture combines Long-Short-Term-Memory recurrent Deep networks and Support Vector Machines as computational nodes in a graph. The network inputs are real data acquired by radar and synthetic data generated mimicking the real ones. The interaction among multiple vehicles is handled explicitly by introducing the Allowance Factors that model the constraints of the possible interactions of the PVs and the ego car in the trajectory forecast estimation. Results obtained on the conducted tests show the ability of the system to predict the motion of the vehicles up to 6 s in advance with a 92% mean average accuracy. Additionally, if implemented into automatic trajectory planners, the introduced modeling approach enables the prediction and avoidance of possible vehicles’ collisions.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.oceaneng.2021.110477,Journal,Ocean Engineering,scopus,2022-02-15,sciencedirect,Safe deep reinforcement learning-based adaptive control for USV interception mission,https://api.elsevier.com/content/abstract/scopus_id/85122992343,"This paper aims to develop a safe learning scheme of the USV interception mission. A safe Lyapunov boundary deep deterministic policy gradient (SLDDPG) algorithm is presented for the USV interception mission. The uniformly ultimate bounded (UUB) stability of control systems is analyzed under finite safety constraints. A single neuron proportional adaptive control (SNPAC) is applied to pre-train the deep policy network for speeding up the training process. The proposed method is evaluated by a series of simulations of the USVs interception and tracking mission. Compared with the existing results, our method can fast converge to the feasible solution subject to safety constraints and demonstrate a high performance in stability and safety by virtual-reality experiments.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jag.2021.102652,Journal,International Journal of Applied Earth Observation and Geoinformation,scopus,2022-02-01,sciencedirect,Developing a deep learning-based layer-3 solution for thermal infrared large-scale photovoltaic module inspection from orthorectified big UAV imagery data,https://api.elsevier.com/content/abstract/scopus_id/85122505895,"The increasing adoption of photovoltaic(PV) technology highlights the need for efficient and large-scale deployment-ready inspection solutions. In the thermal infrared imagery-based inspection framework, we develop a robust and versatile deep learning model for the classification of defect-related patterns on PV modules. The model is developed from big UAV imagery data, and designed as a layer-3 building block that can be implemented on top of any two-stage PV inspection workflow comprising: (1)An aerial Structure from Motion– MultiView Stereo (SfM-MVS) photogrammetric acquisition/processing stage, at which a georeferenced thermal orthomosaic of an inspected PV site is generated, and which enables to locate precisely defective modules on field; then (2)an instance segmentation stage that extracts the images of modules. Orthomosaics from 28 different PV sites were produced, comprising 93220 modules with various types, layouts and thermal patterns. Modules were extracted through a developed semi-automatic workflow, then labeled into six classes. Data augmentation and balancing techniques were used to prepare a highly representative and balanced deep learning-ready dataset. The dataset was used to train, cross-validate and test the developed classifier, as well as benchmarking with the VGG16 architecture. The developed model achieves the state-of-art performance and versatility on the addressed classification problem, with a mean F1-score of94.52%. The proposed three-layer solution resolves the issues of conventional imagery-based workflows. It ensures highly accurate and versatile defect detection, and can be efficiently deployed to real-world large-scale applications.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2021.106574,Journal,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,Perennial ryegrass biomass retrieval through multispectral UAV data,https://api.elsevier.com/content/abstract/scopus_id/85122407603,"Frequent biomass measurement is a key activity for optimal perennial ryegrass (Lolium perenne) management in intensive forage-based dairy operations. Due to the necessary high-frequency (i.e., weekly or monthly) pasture monitoring and continuous trend of larger dairy farms, such activity is perceived as an operational bottleneck. Consequently, substantial effort is directed to the development of accurate and automated technological solutions for biomass assessment. The popularization of unmanned aerial vehicles (UAVs) combined with multispectral cameras should allow for an optimal observational system able to deploy machine learning algorithms for near real-time biomass dry-matter (DM) mapping. For successful operation, these systems should deliver radiometrically accurate orthomosaics and robust models able to generalize across different periods. Nevertheless, the accuracy of radiometric calibration and generalization ability of these models is seldom evaluated. Also, such pipelines should require minimum processing power and allow for fast deployment. This study has established a two-year experiment comparing reflectance measurements between a handheld spectrometer and a commercial multispectral UAV camera. Different algorithms based on regression-tree architecture were contrasted regarding accuracy, speed, and model size. Model performances were validated, providing error-metrics for baseline accuracy and temporal validation. The results have shown that the standard procedure for multispectral imagery radiometric calibration is sub-optimal, requiring further post-processing and presenting low correlation with handheld measurements across spectral bands and dates. Nevertheless, after post-calibration, the use of spectral imagery has presented better baseline error than the point-based sensors, respectively displaying an average of 397.3 and 464.2 kg DM/ha when employed alongside the best performing algorithm (i.e., Cubist). When trained and validated across different years, model performance was largely reduced and deemed unfit for operational purposes. The Cubist/M5 family of algorithms have exhibited advantageous characteristics such as compact model structure, allowing for a higher level of model interpretability, while displaying a smaller size and faster deployment than the Random Forest, Boosted, and Bagged Regression Trees algorithms.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aap.2021.106473,Journal,Accident Analysis and Prevention,scopus,2022-02-01,sciencedirect,Mining patterns of autonomous vehicle crashes involving vulnerable road users to understand the associated factors,https://api.elsevier.com/content/abstract/scopus_id/85118989110,"Autonomous or automated vehicles (AVs) have the potential to improve traffic safety by eliminating majority of human errors. As the interest in AV deployment increases, there is an increasing need to assess and understand the expected implications of AVs on traffic safety. Until recently, most of the literature has been based on either survey questionnaires, simulation analysis, virtual reality, or simulation to assess the safety benefits of AVs. Although few studies have used AV crash data, vulnerable road users (VRUs) have not been a topic of interest. Therefore, this study uses crash narratives from four-year (2017–2020) of AV crash data collected from California to explore the direct and indirect involvement of VRUs. The study applied text network and compared the text classification performance of four classifiers - Support Vector Machine (SVM), Naïve Bayes (NB), Random Forest (RF), and Neural Network (NN) and associated performance metrics to attain the objective. It was found that out of 252 crashes, VRUs were, directly and indirectly, involved in 23 and 12 crashes, respectively. Among VRUs, bicyclists and scooterists are more likely to be involved in the AV crashes directly, and bicyclists are likely to be at fault, while pedestrians appear more in the indirectly involvements. Further, crashes that involve VRUs indirectly are likely to occur when the AVs are in autonomous mode and are slightly involved minor damages on the rear bumper than the ones that directly involve VRUs. Additionally, feature importance from the best performing classifiers (RF and NN) revealed that crosswalks, intersections, traffic signals, movements of AVs (turning, slowing down, stopping) are the key predictors of the VRUs-AV related crashes. These findings can be helpful to AV operators and city planners.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.inffus.2021.09.004,Journal,Information Fusion,scopus,2022-02-01,sciencedirect,Multimodal Earth observation data fusion: Graph-based approach in shared latent space,https://api.elsevier.com/content/abstract/scopus_id/85115401406,"Multiple and heterogenous Earth observation (EO) platforms are broadly used for a wide array of applications, and the integration of these diverse modalities facilitates better extraction of information than using them individually. The detection capability of the multispectral unmanned aerial vehicle (UAV) and satellite imagery can be significantly improved by fusing with ground hyperspectral data. However, variability in spatial and spectral resolution can affect the efficiency of such dataset's fusion. In this study, to address the modality bias, the input data was projected to a shared latent space using cross-modal generative approaches or guided unsupervised transformation. The proposed adversarial networks and variational encoder-based strategies used bi-directional transformations to model the cross-domain correlation without using cross-domain correspondence. It may be noted that an interpolation-based convolution was adopted instead of the normal convolution for learning the features of the point spectral data (ground spectra). The proposed generative adversarial network-based approach employed dynamic time wrapping based layers along with a cyclic consistency constraint to use the minimal number of unlabeled samples, having cross-domain correlation, to compute a cross-modal generative latent space. The proposed variational encoder-based transformation also addressed the cross-modal resolution differences and limited availability of cross-domain samples by using a mixture of expert-based strategy, cross-domain constraints, and adversarial learning. In addition, the latent space was modelled to be composed of modality independent and modality dependent spaces, thereby further reducing the requirement of training samples and addressing the cross-modality biases. An unsupervised covariance guided transformation was also proposed to transform the labelled samples without using cross-domain correlation prior. The proposed latent space transformation approaches resolved the requirement of cross-domain samples which has been a critical issue with the fusion of multi-modal Earth observation data. This study also proposed a latent graph generation and graph convolutional approach to predict the labels resolving the domain discrepancy and cross-modality biases. Based on the experiments over different standard benchmark airborne datasets and real-world UAV datasets, the developed approaches outperformed the prominent hyperspectral panchromatic sharpening, image fusion, and domain adaptation approaches. By using specific constraints and regularizations, the network developed was less sensitive to network parameters, unlike in similar implementations. The proposed approach illustrated improved generalizability in comparison with the prominent existing approaches. In addition to the fusion-based classification of the multispectral and hyperspectral datasets, the proposed approach was extended to the classification of hyperspectral airborne datasets where the latent graph generation and convolution were employed to resolve the domain bias with a small number of training samples. Overall, the developed transformations and architectures will be useful for the semantic interpretation and analysis of multimodal data and are applicable to signal processing, manifold learning, video analysis, data mining, and time series analysis, to name a few.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2022.03.068,Conference Proceeding,Procedia Computer Science,scopus,2022-01-01,sciencedirect,The Security Concerns on Cyber-Physical Systems and Potential Risks Analysis Using Machine Learning,https://api.elsevier.com/content/abstract/scopus_id/85132207250,"The use of engineering to drive down costs and improve productivity has been an ongoing business exercise since the first Industrial Revolution. The term Cyber-Physical System is a wide range of different computing technologies embedded with the next-generation engineered systems into the physical world. Connected Cyber-Physical Systems (CPS) improve the lives of people and increase industry and manufacturing efficiency. It is affecting many branches of life such as transportation, healthcare and medicine, the environment, and energy. Industry 4.0 integrates humans, machines, and data to provide a holistic and interlinked approach to manufacturing, hence, increasing privacy concerns. For example, Autonomous Vehicles (AV) can be driven without a pilot and those systems can be hacked if there is a breach in the system. Nowadays, most of the systems are interconnected to the internet and nothing can be considered fully safe. Therefore, with this increase of security threats and privacy concerns, there is a need to assess and evaluate the trade-off between enhancements and improvements in manufacturing and the possible threats and security risks in the context of Cyber-Physical Systems. We need to bridge the gaps and overcome some of these limitations. In this work, we studied the security concerns emerging from interconnected Cyber-Physical systems, devices, and services in Industry 4.0. To identify security vulnerabilities, we have chosen the energy dataset because energy is the key point of every Cyber-Physical system so aimed to show the importance of energy, and the K-Means algorithm implemented which is an advanced Machine Learning and potential risks detected.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.trpro.2022.02.037,Conference Proceeding,Transportation Research Procedia,scopus,2022-01-01,sciencedirect,A Novel Image and Audio-based Artificial Intelligence Service for Security Applications in Autonomous Vehicles,https://api.elsevier.com/content/abstract/scopus_id/85127522953,"Autonomous Vehicles (AVs) can potentially reduce the accident risk while a human is driving. They can also improve the public transportation by connecting city centers with main mass transit systems. Creating a system that can provide a sense of security to the passenger, when the driver is missing, remains a challenging task. In this work, an image and audio-based approach, supported by novel Artificial Intelligence (AI) algorithms, is proposed as a service to increase the security and trust inside an autonomous shuttle. The two modalities, running in real-time, can detect petty crimes scenarios such as screaming, bag snatching, people fighting and vandalism and enable notifications to authorized personnel for proper actions. The proposed solution is deployed on a Jetson AGX Xavier to favor power efficiency and seamless integration and achieves up to 96% accuracy. Thus, the envisioned system exhibits high potential for transforming security and safety in emerging autonomous public transportation infrastructure.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2022.01.014,Journal,ISA Transactions,scopus,2022-01-01,sciencedirect,"Intelligent framework for automated failure prediction, detection, and classification of mission critical autonomous flights",https://api.elsevier.com/content/abstract/scopus_id/85123893673,"Autonomous flights are the major industry contributors towards next-generation developments in pervasive and ubiquitous computing. Modern aerial vehicles are designed to receive actuator commands from the primary autopilot software as input to regulate their servos for adjusting control surfaces. Due to real-time interaction with the actual physical environment, there exists a high risk of control surface failures for engine, rudder, elevators, and ailerons etc. If not anticipated and then timely controlled, failures occurring during the flight can have severe and cataclysmic consequences, which may result in mid-air collision or ultimate crash. Humongous amount of sensory data being generated throughout mission-critical flights, makes it an ideal candidate for applying advanced data-driven machine learning techniques to identify intelligent insights related to failures for instant recovery from emergencies. In this paper, we present a novel framework based on machine learning techniques for failure prediction, detection, and classification for autonomous aerial vehicles. The proposed framework utilizes long short-term memory recurrent neural network architecture to analyze time series data and has been applied at the AirLab Failure and Anomaly flight dataset, which is a comprehensive publicly available dataset of various fault types in fixed-wing autonomous aerial vehicles’ control surfaces. The proposed framework is able to predict failure with an average accuracy of 93% and the average time-to-predict a failure is 19 s before the actual occurrence of the failure, which is 10 s better than current state-of-the-art. Failure detection accuracy is 100% and average detection time is 0.74 s after happening of failure, which is 1.28 s better than current state-of-the-art. Failure classification accuracy of proposed framework is 100%. The performance analysis shows the strength of the proposed methodology to be used as a real-time failure prediction and a pseudo-real-time failure detection along with a failure classification framework for eventual deployment with actual mission-critical autonomous flights.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.trc.2021.103499,Journal,Transportation Research Part C: Emerging Technologies,scopus,2022-01-01,sciencedirect,Do autonomous vehicles drive like humans? A Turing approach and an application to SAE automation Level 2 cars,https://api.elsevier.com/content/abstract/scopus_id/85120490088,"Fully automated vehicles (AVs) are set to become a reality in future decades and changes are to be expected in user perceptions and behavior. While AV acceptability has been widely studied, changes in human drivers’ behavior and in passengers’ reactions have received less attention. It is not yet possible to ascertain the risk of driver behavioral changes such as overreaction, and the corresponding safety problems, in mixed traffic with partially AVs. Nor has there been proper investigation of the potential unease of car occupants trained for human control, when exposed to automatic maneuvers. The conjecture proposed in this paper is that automation Level 2 vehicles do not induce potentially adverse effects in traditional vehicle drivers’ behavior or in occupants’ reactions, provided that they are indistinguishable from human-driven vehicles. To this end, the paper proposes a Turing approach to test the “humanity” of automation Level 2 vehicles. The proposed test was applied to the results of an experimental campaign carried out in Italy: 546 car passengers were interviewed on board Level 2 cars in which they could not see the driver. They were asked whether a specific driving action (braking, accelerating, lane keeping) had been performed by the human driver or by the automatic on-board software under different traffic conditions (congestion and speed). Estimation results show that in most cases the interviewees were unable to distinguish the Artificial Intelligence (AI) from the human driver by observing random responses with a 95% significance level (proportion of success statistically equal to 50%). However, in the case of moderate braking and lane keeping at >100 km/h and in high traffic congestion, respondents recognized AI control from the human driver above pure chance, with 62–69% correct response rates. These findings, if confirmed in other case studies, could significantly impact on AVs acceptability, also contributing to their design as well as to long-debated ethical questions. AI driving software could be designed and tested for “humanity”, as long as safety is guaranteed, and autonomous cars could be allowed to circulate as long as they cannot be distinguished from human-driven vehicles in recurrent driving conditions.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.dsp.2021.103290,Journal,Digital Signal Processing: A Review Journal,scopus,2022-01-01,sciencedirect,Deep residual learning-based cognitive model for detection and classification of transmitted signal patterns in 5G smart city networks,https://api.elsevier.com/content/abstract/scopus_id/85118634214,"Primary user (PU) signal detection or classification is a critical component of cognitive radio (CR) related wireless communication applications. In CR, the PU detection methods are mostly based on statistical models, and their detection performance heavily relies on the accuracy of assumed models. In this paper, we design a novel detector, dubbed as PU-Net, that dynamically learns the PU activity patterns in a cognitive 5G smart city, where a network of unmanned aerial vehicles (UAVs) is deployed as flying base stations to serve the Internet-of-Things (IoT) users. Unlike the traditional schemes, the PU-Net is free from signal-noise model assumptions and is leveraged through deep residual learning integrated with atrous spatial pyramid pooling (ASPP) to sense the PU's transmitted signal patterns in the network. The PU-Net detects and classifies the active and idle PU states by exploiting the multilevel spatial-temporal features in the signal and noise frames. The proposed model is trained using locally synthesized Rayleigh channel-impaired data with large variability of modulated signals and different noise floor regimes. Additionally, the PU-Net model is blind-tested and evaluated on real-world over-the-air signals and with variable-length frames and varying channel effects at secondary users (SUs). With extensive experiments, it is shown that PU-Net outperforms other benchmark detectors, obtaining an accuracy of 0.9974, with 0.9978 recall and 0.9970 precision in detecting and classifying the PU transmitted signal patterns. Correspondingly, the proposed PU-Net can be adopted for IoT/UAV-assisted communication systems in optimizing spectrum efficiency and resolving the coexistence issues in 5G and beyond networks.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.inffus.2021.07.004,Journal,Information Fusion,scopus,2022-01-01,sciencedirect,SaccadeFork: A lightweight multi-sensor fusion-based target detector,https://api.elsevier.com/content/abstract/scopus_id/85112374720,"Commercialization of self-driving applications requires precision and reliability of the perception system due to the highly dynamic and complex road environment. Early perception systems either rely on the camera or on LiDAR for moving obstacle detection. With the development of vehicular sensors and deep learning technologies, the multi-view and sensor fusion based convolutional neural network (CNN) model for detection tasks has become a popular research area. In this paper, we present a novel multi-sensor fusion-based CNN model–SaccadeFork–that integrates the image and upsampled LiDAR point clouds as the input. SaccadeFork includes two modules: (1) a lightweight backbone that consists of hourglass convolution feature extraction module and a parallel dilation convolution module for adaptation of the system to different target sizes; (2) an anchor-based detection head. The model also considers deployment of resource-limited edge devices in the vehicle. Two refinement strategies, i.e., Mixup and Swish activation function are also adopted to improve the model. Comparison with a series of latest models on public dataset of KITTI shows that SaccadeFork can achieve the optimal detection accuracy on vehicles and pedestrians under different scenarios. The final model is also deployed and tested on a local dataset collected based on edge devices and low-cost sensor solutions, and the results show that the model can achieve real-time efficiency and high detection accuracy.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ejrs.2021.08.007,Journal,Egyptian Journal of Remote Sensing and Space Science,scopus,2021-12-01,sciencedirect,Smart farming for improving agricultural management,https://api.elsevier.com/content/abstract/scopus_id/85114414365,"The food shortage and the population growth are the most challenges facing sustainable development worldwide. Advanced technologies such as artificial intelligence (AI), the Internet of Things (IoT), and the mobile internet can provide realistic solutions to the challenges that are facing the world. Therefore, this work focuses on the new approaches regarding smart farming (SF) from 2019 to 2021, where the work illustrates the data gathering, transmission, storage, analysis, and also, suitable solutions. IoT is one of the essential pillars in smart systems, as it connects sensor devices to perform various basic tasks. The smart irrigation system included those sensors for monitoring water level, irrigation efficiency, climate, etc. Smart irrigation is based on smart controllers and sensors as well as some mathematical relations. In addition, this work illustrated the application of unmanned aerial vehicles (UAV) and robots, where they can be achieved several functions such as harvesting, seedling, weed detection, irrigation, spraying of agricultural pests, livestock applications, etc. real-time using IoT, artificial intelligence (AI), deep learning (DL), machine learning (ML) and wireless communications. Moreover, this work demonstrates the importance of using a 5G mobile network in developing smart systems, as it leads to high-speed data transfer, up to 20 Gbps, and can link a large number of devices per square kilometer. Although the applications of smart farming in developing countries are facing several challenges, this work highlighted some approaches the smart farming. In addition, the implementation of Smart Decision Support Systems (SDSS) in developing countries supports the real-time analysis, mapping of soil characteristics and also helps to make proper decision management. Finally, smart agriculture in developing countries needs more support from governments at the small farms and the private sector.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.115380,Journal,Expert Systems with Applications,scopus,2021-11-30,sciencedirect,Intelligent control of an UAV with a cable-suspended load using a neural network estimator,https://api.elsevier.com/content/abstract/scopus_id/85111010813,"Unmanned aerial vehicles (UAVs) have been proved very useful in civil and military sectors: defense, security, shipping, construction, agriculture, entertainment, etc. Some of these applications, especially those related to transport and logistic operations, require the use of suspended loads that may make the vehicle unstable. In order to deal with this non-linear complex system with a changing mass, further research on modelling and control must be developed. In this work, a new intelligent control strategy is proposed and applied to a quadrotor with a cable-suspended load. The UAV carrying a suspended load has two different dynamic behaviors, depending on the state of the cable. Thus, we proposed to model the complete system using the hybrid automata formalism. Using this novel UAV model approach, a hybrid control is designed based on feedback linearization controllers combined with an artificial neural network, which acts as an online estimator of the unknown mass. The suspended load is dealt with as an external disturbance. Simulation results show how the on-line learning control scheme increases the robustness of the control and it is able to stabilize the quadrotor without any information about neither the position of the load nor the tension of the cable. Additionally, the computational complexity of the proposal is studied to show the feasibility of the implementation of this intelligent control strategy on real hardware.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.115343,Journal,Expert Systems with Applications,scopus,2021-11-30,sciencedirect,A framework for 3D tracking of frontal dynamic objects in autonomous cars,https://api.elsevier.com/content/abstract/scopus_id/85108361303,"Both recognition and 3D tracking of frontal dynamic objects are crucial problems in an autonomous vehicle, while depth estimation as an essential issue becomes a challenging problem using a monocular camera. Since both camera and objects are moving, the issue can be formed as a structure from motion (SFM) problem. In this paper, to elicit features from an image, the YOLOv3 approach is utilized beside an OpenCV tracker. Subsequently, to obtain the lateral and longitudinal distances, a nonlinear SFM model is considered alongside a state-dependent Riccati equation (SDRE) filter and a newly developed observation model. Additionally, a switching method in the form of switching estimation error covariance is proposed to enhance the robust performance of the SDRE filter. The stability analysis of the presented filter is conducted on a class of discrete nonlinear systems. Furthermore, the ultimate bound of estimation error caused by model uncertainties is analytically obtained to investigate the switching significance. Simulations are reported to validate the performance of the switched SDRE filter. Finally, real-time experiments are performed through a multi-thread framework implemented on a Jetson TX2 board, while radar data is used for the evaluation.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2021.09.008,Journal,Neurocomputing,scopus,2021-11-20,sciencedirect,Fast intent prediction of multi-cyclists in 3D point cloud data using deep neural networks,https://api.elsevier.com/content/abstract/scopus_id/85115024624,"Inferring the intended actions of road-sharing users with autonomous ground vehicles in particularly vulnerable ones like cyclists is considered one of the tough tasks facing the wide-spread deployment of autonomous ground vehicles. One of the main reasons for that is the scarcity of the available datasets for that task due to the difficulty in obtaining those datasets in real environments. In this work, we first propose a pipeline that can synthetically produce 3D LiDAR data of cyclists hand-signalling a set of intended actions that are commonly done in real environments. Given the synthetically-produced labelled 3D LiDAR data sequences, we trained a framework that can simultaneously detect, track and give predictions about the intended actions of multi-cyclists in the scene on time. The proposed framework was evaluated using both synthetic and real data from a physical 3D LiDAR sensor. Our proposed framework has scored competitive and robust results in both synthetic and real environments with 88% in 
                        
                           
                              
                                 F
                              
                              
                                 1
                              
                           
                        
                      measure with higher frame per second rate (12.9 FPS) than the 3D LiDAR sensor frame rate (10 Hz).",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.sciaf.2021.e00979,Journal,Scientific African,scopus,2021-11-01,sciencedirect,Using the Mavic 2 Pro drone for basic water quality assessment,https://api.elsevier.com/content/abstract/scopus_id/85122683792,"This paper assessed the capability of the Dà-Jiāng Innovations (DJI) Mavic 2 Pro Drone (unmanned aerial vehicle – UAV) for the collection and delivery of river water samples for basic water quality assessments. The primary objective of this paper was to evaluate how this UAV model could help in generating large water quality data sets in the developing world to assist in the design and implementation of water quality monitoring and assessment programs, which are often a challenge due to data paucity and resources. We hypothesized that the traditional approach (portable hand meters) to measuring in-situ water parameters, including pH, dissolved oxygen, electrical conductivity, and turbidity could not yield significant water quality data variations from those collected by the Mavic 2 Pro. The UAV was equipped with a plastic bottle attached to a three-meter rigid thin line for sample collection. Samples were collected at stations 50 m apart over a 300 m river length. The drone captured samples in wind conditions of about 10.1 km/h with ease. About 350 mL of samples were collected per mission. A paired t-test was performed to determine the parameter differences between the two approaches. We conclude that, given similar environmental, physical conditions and pilot experience, Mavic 2 Pro can generate large and much more reliable datasets at faster rates than the traditional approach. The drone also avoided obstacles with ease, a perfect technology for use in rural rivers. However, pilot efficiency and precision, including agitation during flight require further investigations considering their potential parameter influences. Similar future tests should investigate the performance of this drone model and data reliability over a long river course to ascertain its capability and suitability in various conditions in ecological applications.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.oceaneng.2021.109531,Journal,Ocean Engineering,scopus,2021-09-15,sciencedirect,A remote anomaly detection system for Slocum underwater gliders,https://api.elsevier.com/content/abstract/scopus_id/85109609494,"Marine Autonomous Systems (MAS) operating at sea beyond visual line of sight need to be self-reliant, as any malfunction could lead to loss or pose a risk to other sea users. In the absence of fully automated on-board control and fault detection tools, MAS are piloted and monitored by experts, resulting in high operational costs and limiting the scale of observational fleets that can be deployed simultaneously. Hence, an effective anomaly detection system is fundamental to increase fleet capacity and reliability. In this study, an on-line, remote fault detection system is developed for underwater gliders. Two alternative methods are analysed using time series data: feedforward deep neural networks estimating the glider’s vertical velocity and an autoencoder. The systems are trained using field data from four baseline deployments of Slocum gliders and tested on six deployments of vehicles suffering from adverse behaviour. The methods are able to successfully detect a range of anomalies in the near real time data streams, whilst being able to generalise to different glider configurations. The autoencoder’s error in reconstructing the original signals is the clearest indicator of anomalies. Thus, the autoencoder is a prime candidate to be included into an all-encompassing condition monitoring system for MAS.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.114937,Journal,Expert Systems with Applications,scopus,2021-09-15,sciencedirect,Search and rescue operation using UAVs: A case study,https://api.elsevier.com/content/abstract/scopus_id/85104654242,"Many people go missing in the wild every year. In this paper, the Search and Rescue (SAR) mission is conducted using a novel system comprising an Unmanned Aerial Vehicle (UAV) coupled with real-time machine-learning-based object detection system embedded on a smartphone. Human detection from UAV in the wilderness is a challenging task, because of many constraints involved such as lack of computing and communication infrastructures. We proposed a novel combination of a robust architecture deployed on a smartphone and a novel Convolutional Neural Network (CNN) model to fulfil the goals of the project. Our approach achieved 94.73% of accuracy and 6.8 FPS on a smartphone. Our approach is highly portable, cost-effective, fast with high accuracy. This novel system is expected to contribute significantly to maximise chances of saving lives in the wild. This developed system has been recently launched by Police Scotland to facilitate the SAR teams to locate missing persons in Scotland wilderness.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.trc.2021.103272,Journal,Transportation Research Part C: Emerging Technologies,scopus,2021-09-01,sciencedirect,Spatial-temporal pricing for ride-sourcing platform with reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85108979514,"Ever since the emergence of ride-sourcing services, the spatial–temporal pricing problem has been a hot research topic in both the transportation and management fields. The difficulty lies in simultaneously obtaining the optimal multivariable solution for spatial pricing and sequential solution for dynamic pricing, considering the heterogeneity, dynamics, and imbalance of on-demand ride supply/demand. Due to this problem's complexity, most studies have simplified the modeling setting and omitted the complicated matching and waiting process between drivers and passengers. To go beyond the existing models, this paper proposes a reinforcement learning enhanced agent-based modeling and simulation (RL-ABMS) system to reveal the complex mechanism in the ride-sourcing system and tackle the problem of spatial–temporal pricing for a ride-sourcing platform. The reinforcement learning approach proximal policy optimization (PPO) is implemented in the RL-ABMS system, where two feed-forward neural networks are built as critic and actor. The critic judges the goodness of the current state, and the actor generates the optimal pricing strategy.Compared with the fixed pricing strategy, the experimental results on a real-world urban network show that dynamic pricing raises the platform's profit to 1.25 times, and spatial–temporal pricing even raises it to 1.85 times. Besides, the number of idle drivers/vehicles has significantly dropped under the spatial–temporal pricing strategy, which indicates that our proposed strategy has a remarkable effect on coordinating supply and demand in the ride-sourcing market.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.measurement.2021.109541,Journal,Measurement: Journal of the International Measurement Confederation,scopus,2021-08-01,sciencedirect,Data-driven vehicle modeling of longitudinal dynamics based on a multibody model and deep neural networks,https://api.elsevier.com/content/abstract/scopus_id/85105601897,"The vehicle dynamics simulation and preview control require a dedicated vehicle model, such as multibody dynamics model. However, the multibody model has higher computational complexity which affects the response time of the vehicle controller. This issue can be alleviated by using a data-driven vehicle dynamics model due to its effective generalization and computational speed. In this work, we propose a data-driven modeling approach based on deep neural networks (DNNs) for computing and predicting the vehicle characteristics. The high-fidelity simulations of a validated vehicle multibody model are performed for data acquisition. This data is then used for training and testing the proposed model. The DNN inputs comprise the initial speed of the vehicle and the torque applied on front wheels to imitate vehicle acceleration and deceleration. The DNN outputs comprise the driving distance and the longitudinal velocity of the vehicle. The dynamics characteristics resulting from both the data-driven model and the multibody model are investigated and compared. Furthermore, the accuracy of the data-driven model is analyzed in terms of various error functions. The data-driven model is verified by using the results obtained from a commercial software package. The simulation results show that the data-driven vehicle model predicts the accurate velocity and driving distance in real-time. The data-driven model can be used for real-time simulation and preview control in autonomous vehicles.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2021.114820,Journal,Expert Systems with Applications,scopus,2021-08-01,sciencedirect,Machine Learning for industrial applications: A comprehensive literature review,https://api.elsevier.com/content/abstract/scopus_id/85102967505,"Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demonstrated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are categorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are production planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.bdr.2021.100241,Journal,Big Data Research,scopus,2021-07-15,sciencedirect,“Brains” for Robots: Application of the Mivar Expert Systems for Implementation of Autonomous Intelligent Robots,https://api.elsevier.com/content/abstract/scopus_id/85107957013,"Recently the contemporary robotic systems can manipulate different objects and make decisions in a range of situations due to significant advances in innovation technologies and artificial intelligence. The new expert technologies can handle millions of instructions on computers and smartphones, which allow them to be used as a tool to create “decision-making systems” for autonomous robots. The goal of this paper was to create a dynamic algorithm of robot actions that can be used in the decision module has been considered. It is proposed to use Mivar expert systems of a new generation for high-level control. The experiment results showed that Mivar decision-making systems can control groups of small robots and even an unmanned autonomous car in real time. The algorithms created in the Mivar environment can be very flexible, and their build-up depends only on engineering approaches. In addition to traditional low-level robot control systems, a Mivar decision-making system has been implemented, which can be considered as universal “Brains” for autonomous intelligent robots and now knowledge bases can be created and various robots can be trained for practical tasks.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rse.2021.112434,Journal,Remote Sensing of Environment,scopus,2021-07-01,sciencedirect,Estimation of root zone soil moisture from ground and remotely sensed soil information with multisensor data fusion and automated machine learning,https://api.elsevier.com/content/abstract/scopus_id/85104083550,"Root zone soil moisture (RZSM) estimation and monitoring based on high spatial resolution remote sensing information such as obtained with an Unmanned Aerial System (UAS) is of significant interest for field-scale precision irrigation management, particularly in water-limited regions of the world. To date, there is no accurate and widely accepted model that relies on UAS optical surface reflectance observations for RZSM estimation at high spatial resolution. This study is aimed at the development of a new approach for RZSM estimation based on the fusion of high spatial resolution optical reflectance UAS observations with physical and hydraulic soil information integrated into Automated Machine Learning (AutoML). The H2O AutoML platform includes a number of advanced machine learning algorithms that efficiently perform feature selection and automatically identify complex relationships between inputs and outputs. Twelve models combining UAS optical observations with various soil properties were developed in a hierarchical manner and fed into AutoML to estimate surface, near-surface, and root zone soil moisture. The addition of independently measured surface and near-surface soil moisture information to the hierarchical models to improve RZSM estimation was investigated. The accuracy of soil moisture estimates was evaluated based on a comparison with Time Domain Reflectometry (TDR) sensors that were deployed to monitor surface, near-surface and root zone soil moisture dynamics. The obtained results indicate that the consideration of physical and hydraulic soil properties together with UAS optical observations improves soil moisture estimation, especially for the root zone with a RMSE of about 0.04 cm3 cm−3. Accurate RZSM estimates were obtained when measured surface and near-surface soil moisture data was added to the hierarchical models, yielding RMSE values below 0.02 cm3 cm−3 and R and NSE values above 0.90. The generated high spatial resolution RZSM maps clearly capture the spatial variability of soil moisture at the field scale. The presented framework can aid farm scale precision irrigation management via improving the crop water use efficiency and reducing the risk of groundwater contamination.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comnet.2021.108057,Journal,Computer Networks,scopus,2021-06-19,sciencedirect,A deep reinforcement learning-based multi-optimality routing scheme for dynamic IoT networks,https://api.elsevier.com/content/abstract/scopus_id/85104075448,"With the development of Internet of Things (IoT) and 5G technologies, more and more applications, such as autonomous vehicles and tele-medicine, become more sensitive to network latency and accuracy, which require routing schemes to be more flexible and efficient. In order to meet such urgent need, learning-based routing strategies are emerging as strong candidate solutions, with the advantages of high flexibility and accuracy. These strategies can be divided into two categories, centralized and distributed, enjoying the advantages of high precision and high efficiency, respectively. However, routing becomes more complex in dynamic IoT network, where the link connections and access states are time-varying, hence these learning-based routing mechanisms are required to have the capability to adapt to network changes in real time. In this paper, we designed and implemented both centralized and distributed Reinforcement Learning-based Routing schemes combined with Multi-optimality routing criteria (RLR-M). By conducting a series of experiments, we performed a comprehensive analysis of the results and arrived at the conclusion that the centralized is better suited to cope with dynamic networks due to its faster reconvergence (2.2 
                        ×
                      over distributed), while the distributed is better positioned to handle with large-scale networks through its high scalability (1.6 
                        ×
                      over centralized). Moreover, the multi-optimality routing scheme is implemented through model fusion, which is more flexible than traditional strategies and as such is better placed to meet the needs of IoT.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.treng.2021.100068,Journal,Transportation Engineering,scopus,2021-06-01,sciencedirect,Real-time traffic quantization using a mini edge artificial intelligence platform,https://api.elsevier.com/content/abstract/scopus_id/85111397458,"Traffic analysis is dependent on reliable and accurate datasets that quantify the vehicle composition, speed and traffic density over a long period of time. The utilisation of big data is required if equitable and efficient transportation networks are to be realised for smart, interconnected cities of the future. The rapid and widespread adoption of digital twins, IoT (Internet of Things), artificial intelligence and mini edge computing technologies serve as the catalyst to rapidly develop and deploy smart systems for real-time data acquisition of traffic in and around urban and metropolitan areas. This paper presents a proof of concept of a mini edge computing platform for real-time edge processing, which serves as a digital twin of a multi-lane freeway located in Pretoria, South Africa. Video data acquired from an Unmanned Aerial Vehicle (UAV) is processed using a neural network architecture designed for real-time object detection tracking of vehicles. The implementation successfully counted vehicles (cars and trucks) together with an estimation of the speed of each detected vehicle. These results compare favourably to the ground truth data with vehicle counting accuracies of 5% realised. Detection of sparse motorcycles and pedestrians were less than optimal. This proof of concept can be easily scaled and deployed over a wide geographic area. Integration of these cyber-physical assets can be incorporated into existing video monitoring systems or fused with optical sensors as a single data acquisition system.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.bios.2021.113088,Journal,Biosensors and Bioelectronics,scopus,2021-05-15,sciencedirect,Machine learning-based cytokine microarray digital immunoassay analysis,https://api.elsevier.com/content/abstract/scopus_id/85101500120,"Serial measurement of a large panel of protein biomarkers near the bedside could provide a promising pathway to transform the critical care of acutely ill patients. However, attaining the combination of high sensitivity and multiplexity with a short assay turnaround poses a formidable technological challenge. Here, the authors develop a rapid, accurate, and highly multiplexed microfluidic digital immunoassay by incorporating machine learning-based autonomous image analysis. The assay has achieved 12-plexed biomarker detection in sample volume <15 μL at concentrations < 5 pg/mL while only requiring a 5-min assay incubation, allowing for all processes from sampling to result to be completed within 40 min. The assay procedure applies both a spatial-spectral microfluidic encoding scheme and an image data analysis algorithm based on machine learning with a convolutional neural network (CNN) for pre-equilibrated single-molecule protein digital counting. This unique approach remarkably reduces errors facing the high-capacity multiplexing of digital immunoassay at low protein concentrations. Longitudinal data obtained for a panel of 12 serum cytokines in human patients receiving chimeric antigen receptor-T (CAR-T) cell therapy reveals the powerful biomarker profiling capability. The assay could also be deployed for near-real-time immune status monitoring of critically ill COVID-19 patients developing cytokine storm syndrome.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compag.2021.106091,Journal,Computers and Electronics in Agriculture,scopus,2021-05-01,sciencedirect,DeepWay: A Deep Learning waypoint estimator for global path generation,https://api.elsevier.com/content/abstract/scopus_id/85103275872,"Agriculture 3.0 and 4.0 have gradually introduced service robotics and automation into several agricultural processes, mostly improving crops quality and seasonal yield. Row-based crops are the perfect settings to test and deploy smart machines capable of monitoring and manage the harvest. In this context, global path generation is essential either for ground or aerial vehicles, and it is the starting point for every type of mission plan. Nevertheless, little attention has been currently given to this problem by the research community and global path generation automation is still far to be solved. In order to generate a viable path for an autonomous machine, the presented research proposes a feature learning fully convolutional model capable of estimating waypoints given an occupancy grid map. In particular, we apply the proposed data-driven methodology to the specific case of row-based crops with the general objective to generate a global path able to cover the extension of the crop completely. Extensive experimentation with a custom made synthetic dataset and real satellite-derived images of different scenarios have proved the effectiveness of our methodology and demonstrated the feasibility of an end-to-end and completely autonomous global path planner.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2020.103701,Journal,Robotics and Autonomous Systems,scopus,2021-02-01,sciencedirect,On deep learning techniques to boost monocular depth estimation for autonomous navigation,https://api.elsevier.com/content/abstract/scopus_id/85098871371,"Inferring the depth of images is a fundamental inverse problem within the field of Computer Vision since depth information is obtained through 2D images, which can be generated from infinite possibilities of observed real scenes. Benefiting from the progress of Convolutional Neural Networks (CNNs) to explore structural features and spatial image information, Single Image Depth Estimation (SIDE) is often highlighted in scopes of scientific and technological innovation, as this concept provides advantages related to its low implementation cost and robustness to environmental conditions. In the context of autonomous vehicles, state-of-the-art CNNs optimize the SIDE task by producing high-quality depth maps, which are essential during the autonomous navigation process in different locations. However, such networks are usually supervised by sparse and noisy depth data, from Light Detection and Ranging (LiDAR) laser scans, and are carried out at high computational cost, requiring high-performance Graphic Processing Units (GPUs). Therefore, we propose a new lightweight and fast supervised CNN architecture combined with novel feature extraction models which are designed for real-world autonomous navigation. We also introduce an efficient surface normals module, jointly with a simple geometric 2.5D loss function, to solve SIDE problems. We also innovate by incorporating multiple Deep Learning techniques, such as the use of densification algorithms and additional semantic, surface normals and depth information to train our framework. The method introduced in this work focuses on robotic applications in indoor and outdoor environments and its results are evaluated on the competitive and publicly available NYU Depth V2 and KITTI Depth datasets.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ocecoaman.2020.105478,Journal,Ocean and Coastal Management,scopus,2021-02-01,sciencedirect,Autonomous litter surveying and human activity monitoring for governance intelligence in coastal eco-cyber-physical systems,https://api.elsevier.com/content/abstract/scopus_id/85097580928,"The human impact on the coastal ecosystems is a global environmental concern. Due to the growing urbanization, industrialization, and transportation, this impact on the living and non-living components of the coastal area is expected to further increase in the coming years. Artificial intelligence based automation of the coastal monitoring, including data collection, analysis and decision making, provides real-time insights and opportunities for large-scale coastal management and governance. In this paper, a framework for autonomous litter surveying and human activity monitoring for governance intelligence in coastal eco-cyber-physical systems (ecoCystem) is presented. A large dataset of more than 20,000 images focused on smart coastal management is collected to model the real world scenarios. A combination of various artificial intelligence based methods are used for automatic detection and classification of various litter in the coastal environment. Furthermore, the proposed framework is capable of autonomous monitoring of humans activities and detection of illegal entry of vehicles and boats to the beach area. The accuracy of the proposed autonomous system is 87% for correct classification of fully visible litter and 95% for fully visible vehicles. The experimental results show that the application of computer vision and machine learning for autonomous litter classification shows promising results for increasing the speed and scale of litter surveying in the coastal area. Further training of the artificial intelligence models is necessary for increasing the accuracy of the proposed framework and real-world deployment in the coastal environment. The proposed human activity monitoring system can be used for autonomous coastal law enforcement and real-time and active protection of the coastal zones.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2020.09.001,Journal,Future Generation Computer Systems,scopus,2021-02-01,sciencedirect,Formal approach to thwart against drone discovery attacks: A taxonomy of novel 3D obfuscation mechanisms,https://api.elsevier.com/content/abstract/scopus_id/85091758369,"The pervasive capabilities and myriad of mission performance abilities of Unmanned (Combat) Aerial Vehicles (UAVs/UCAVs) have exponentially grown their deployment possibilities in the recent past. Advancements in artificial intelligence, sensing technologies and autonomous guidance, navigation and control capabilities have further fueled wide-scale deployments of UAVs, both for military and commercial applications, ranging from autonomous air taxis and cargo deliveries to intelligence surveillance, reconnaissance, and combat missions. Most of these applications consume Global Navigation Satellite System (GNSS) based location information for their services, which is also shared in real-time with ground control stations and centralized service operators, often using insecure communication channels. This limitation has significantly raised the location privacy concerns of aerial vehicles, deployed to conduct user-centric, safety-critical and localization-sensitive operations. A compromise of location privacy of a UAV can pose serious threats, including stalking, theft or damage of UAV/payload or even use of GNSS-guided munitions. These emerging threats call for robust and trust-worthy solutions for preserving the location privacy of aerial vehicles.
                  This paper proposes a novel obfuscation-based mechanism to safeguard location information against privacy attacks. Our proposed solution conceals actual information by transmitting modified location parameters, either after diluting their accuracy or by fabricating deceptive trajectories for a known eavesdropper. Based on these two broad categories, defined as Attenuation and Deception-based obfuscation techniques respectively, we also present a novel taxonomy of 3D obfuscation mechanisms, supported by formal descriptions of underlying operators. The operators can be used independently or in conjunction to satisfy diverse mission-specific obfuscation profiles. The proposed operators have been practically implemented and evaluated using a customizable obfuscator deployed over a Global Positioning System (GPS) guided UAV. The field experiments validate the efficacy, security and deployability of the proposed solution against location-privacy threats.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.future.2020.08.046,Journal,Future Generation Computer Systems,scopus,2021-02-01,sciencedirect,A drone-based networked system and methods for combating coronavirus disease (COVID-19) pandemic,https://api.elsevier.com/content/abstract/scopus_id/85090189689,"Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus. It is similar to influenza viruses and raises concerns through alarming levels of spread and severity resulting in an ongoing pandemic worldwide. Within eight months (by August 2020), it infected 24.0 million persons worldwide and over 824 thousand have died. Drones or Unmanned Aerial Vehicles (UAVs) are very helpful in handling the COVID-19 pandemic. This work investigates the drone-based systems, COVID-19 pandemic situations, and proposes an architecture for handling pandemic situations in different scenarios using real-time and simulation-based scenarios. The proposed architecture uses wearable sensors to record the observations in Body Area Networks (BANs) in a push–pull data fetching mechanism. The proposed architecture is found to be useful in remote and highly congested pandemic areas where either the wireless or Internet connectivity is a major issue or chances of COVID-19 spreading are high. It collects and stores the substantial amount of data in a stipulated period and helps to take appropriate action as and when required. In real-time drone-based healthcare system implementation for COVID-19 operations, it is observed that a large area can be covered for sanitization, thermal image collection, and patient identification within a short period (2 KMs within 10 min approx.) through aerial route. In the simulation, the same statistics are observed with an addition of collision-resistant strategies working successfully for indoor and outdoor healthcare operations. Further, open challenges are identified and promising research directions are highlighted.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-820276-0.00010-8,Book,Unmanned Aerial Systems: Theoretical Foundation and Applications: A Volume in Advances in Nonlinear Dynamics and Chaos (ANDC),scopus,2021-01-01,sciencedirect,"Unmanned aerial systems: Autonomy, cognition, and control",https://api.elsevier.com/content/abstract/scopus_id/85126812295,"The increasing trend towards a higher level of autonomy in unmanned aerial systems (UASs) had led to lower requirements for control by the human operator and to an increasing capability to perform complex tasks by reacting to the environmental influences. Nevertheless, current UASs are designed to function in static and predictable environments. Therefore, it is envisaged that the existing uncertainties and dynamic changes, caused when an unmanned aerial vehicle (UAV) is operating in an unknown environment, would reduce its performance significantly. The uncertainties can be also incurred through interaction with other complex and intelligent systems, such as humans. We present a compact literature survey of UAS control and navigation as a basic knowledge to develop UASs from the perspective of the control engineer. Besides, we present several control strategies to maintain a UAS, as well as multi-UASs, under a network setting under various scenarios. Several simulations are given to illustrate the performance of the controllers in MATLAB®. Advances in computing power and algorithms currently enable the development of systems with a high degree of autonomy. Nonetheless, there is a large gap between practical operation in the real-world and laboratory implementation, as safe deployment of UASs requires validation of their behavior under almost all envisaged scenarios. A reliable and autonomous operation of such a system requires design and development of a cognitive control system that acquires knowledge and understanding of the surrounding environment via perception, reasoning, and learning. Cognitive control systems in UASs will enhance their safety and performance. Cognitive control can also be used in cooperative execution of complex tasks where multiple agents such as humans, machines, or both interact. Such UASs will have a great potential to be used in extreme environments such as search and rescue in the case of disaster, nuclear decommissioning operations, deep-sea exploration, mining, etc.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2021.10.131,Conference Proceeding,IFAC-PapersOnLine,scopus,2021-01-01,sciencedirect,Side-scan sonar imaging: Real-time acoustic streaming,https://api.elsevier.com/content/abstract/scopus_id/85120861854,"Most data collected with autonomous underwater vehicles (AUVs) can only be reviewed and analyzed post-mission. This limitation is due to the low-bandwidth acoustic links used for communication between AUV and surface, which are typically devoted to monitoring a few mission-critical parameters. In this work, we propose a plug-and-play solution that would enable live streaming of side-scan sonar data on such low-bandwidth modems with no additional hardware requirements. Using principles from image super resolution, we first downsample and compress sonar images on the sensor(AUV)-side and then estimate the full high-quality sonar image on the receiver(surface)-side. The bandwidth requirements for this livestreaming are below 1.87 kbit/s while still providing reconstructions difficult to distinguish from the originals. Our complete end-to-end system has been implemented using EvoLogics underwater modems and successfully tested in our laboratory and harbor basin.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.dcan.2021.09.006,Journal,Digital Communications and Networks,scopus,2021-01-01,sciencedirect,DeepPOSE: Detecting GPS spoofing attack via deep recurrent neural network,https://api.elsevier.com/content/abstract/scopus_id/85119525374,"The Global Positioning System (GPS) has become a foundation for most location-based services and navigation systems, such as autonomous vehicles, drones, ships, and wearable devices. However, it is a challenge to verify if the reported geographic locations are valid due to various GPS spoofing tools. Pervasive tools, such as Fake GPS, Lockito, and software-defined radio, enable ordinary users to hijack and report fake GPS coordinates and cheat the monitoring server without being detected. Furthermore, it is also a challenge to get accurate sensor readings on mobile devices because of the high noise level introduced by commercial motion sensors. To this end, we propose DeepPOSE, a deep learning model, to address the noise introduced in sensor readings and detect GPS spoofing attacks on mobile platforms. Our design uses a convolutional and recurrent neural network to reduce the noise, to recover a vehicle's real-time trajectory from multiple sensor inputs. We further propose a novel scheme to map the constructed trajectory from sensor readings onto the Google map, to smartly eliminate the accumulation of errors on the trajectory estimation. The reconstructed trajectory from sensors is then used to detect the GPS spoofing attack. Compared with the existing method, the proposed approach demonstrates a significantly higher degree of accuracy for detecting GPS spoofing attacks.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2021.02.012,Conference Proceeding,Procedia Computer Science,scopus,2021-01-01,sciencedirect,The impact of the soft errors in convolutional neural network on GPUS: Alexnet as case study,https://api.elsevier.com/content/abstract/scopus_id/85105461752,"Convolutional Neural Networks (CNNs) have been increasingly deployed in many applications, including safety critical system such as healthcare and autonomous vehicles. Meanwhile, the vulnerability of CNN model to soft errors (e.g., caused by radiation induced) rapidly increases, thus reliability is crucial especially in real-time system. There are many traditional techniques for improve the reliability of the system, e.g., Triple Modular Redundancy, but these techniques incur high overheads, which makes them hard to deploy. In this paper, we experimentally evaluate the vulnerable parts of Alexnet mode (e.g., fault injector). Results show that FADD and LD are the top vulnerable instructions against soft errors for Alexnet model, both instructions generate at least 84% of injected faults as SDC errors. Thus, these the only parts of the Alexnet model that need to be hardened instead of using fully duplication solutions.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ndteint.2020.102341,Journal,NDT and E International,scopus,2020-12-01,sciencedirect,Automatic delamination segmentation for bridge deck based on encoder-decoder deep learning through UAV-based thermography,https://api.elsevier.com/content/abstract/scopus_id/85089808135,"Concrete deck delamination often demonstrates strong variations in size, shape, and temperature distribution under the influences of outdoor weather conditions. The strong variations create challenges for pure analytical solutions in infrared image segmentation of delaminated areas. The recently developed supervised deep learning approach demonstrated the potentials in achieving automatic segmentation of RGB images. However, its effectiveness in segmenting thermal images remains under-explored. The main challenge lies in the development of specific models and the generation of a large range of labeled infrared images for training. To address this challenge, a customized deep learning model based on encoder-decoder architecture is proposed to segment the delaminated areas in thermal images at the pixel level. Data augmentation strategies were implemented in creating the training data set to improve the performance of the proposed model. The deep learning generated model was deployed in a real-world project to further evaluate the model's applicability and robustness. The results of these experimental studies supported the effectiveness of the deep learning model in segmenting concrete delamination areas from infrared images. It also suggested that data augmentation is a helpful technique to address the small size issue of training samples. The field test with validation further demonstrated the generalizability of the proposed framework. Limitations of the proposed approach were also briefed at the end of the paper.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.conengprac.2020.104630,Journal,Control Engineering Practice,scopus,2020-11-01,sciencedirect,Vision-based robust control framework based on deep reinforcement learning applied to autonomous ground vehicles,https://api.elsevier.com/content/abstract/scopus_id/85090751679,"Given the recent advances in computer vision, image processing and control systems, self-driving vehicles has been one of the most promising and challenging research topics nowadays. The design of vision-based robust controllers to keep an autonomous car in the center of the lane, despite uncertainties and disturbances, is still an ongoing challenge. This paper presents a hybrid control architecture that combines Deep Reinforcement Learning (DRL) and Robust Linear Quadratic Regulator (RLQR) for vision-based lateral control of an autonomous vehicle. Evolutionary estimation is used to model the vehicle uncertainties. For performance comparison, a DRL method and three other hybrid controllers are also evaluated. The inputs for each controller are real-time semantically segmented RGB camera images which serve as the basis to calculate continuous steering actions to keep the vehicle on the center of the lane with a constant velocity. Simulation results show that the proposed hybrid RLQR with evolutionary estimation of uncertainties architecture outperforms the other algorithms implemented. It presents lower tracking errors, smoother steering inputs, total collision avoidance and better generalization in new urban environments. Furthermore, it significantly decreases the required training time.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2020.103624,Journal,Robotics and Autonomous Systems,scopus,2020-11-01,sciencedirect,Associated Reality: A cognitive Human–Machine Layer for autonomous driving,https://api.elsevier.com/content/abstract/scopus_id/85089815046,"Advanced Driver Assistance Systems (ADAS) and Automated and Autonomous Vehicles (AV) are cooperative systems and processes that use: artificial intelligence, cognitive methods, cloud technologies, cooperative vehicle-to-everything-communications (V2X), software–hardwareplatforms, sensor platforms and incipient intelligent transport infrastructures, to get self-driving systems and smart connected mobility services. This paper, to support automated driving systems (assisted, semi-autonomous and fully autonomous vehicles), introduces a cognitive layer called Associated Reality to enhance the involved information, knowledge and communication processes. The architecture defined includes an augmented Local Dynamic Map, with complementary layers, and an augmented Graph Database, with complementary semantic–cognitive relations, for the considered purpose, in cooperative human–machine and machine–machine systems. Virtual augmented landmarks are defined to improve the connectivity and intelligence of the involved spatial-information systems. Different structure landmarks and sequence landmarks (which includes regular, repetitive and periodic landmarks) are defined, categorized and used in diverse visual localization and mapping scenarios, for autonomous driving. In this paper, it is also shown, as a proof-of-concept for vehicle localization and mapping in road tunnels, the visual detection of different sequences of periodic luminaires, using YOLO v3 for the corresponding LED lights detection, or a specific alternative procedure developed with very low computational cost.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ssci.2020.104919,Journal,Safety Science,scopus,2020-11-01,sciencedirect,A systems-based application for autonomous vessels safety: Hazard identification as a function of increasing autonomy levels,https://api.elsevier.com/content/abstract/scopus_id/85088891787,"During the last decade, a series of autonomous vessels projects have been conducted strengthening the vision of the positive impacts on human safety, cost savings and environmental protection using advanced Artificial Intelligence (AI) based on data fused algorithms from various sensors. Although these projects have demonstrated the technological feasibility of this idea, the hypothesis that autonomous ships will be safer needs to be tested and safety constraints must be established. The main objective of this paper is to determine the hazards for the system as a function of the vessel’s autonomy level. This is accomplished by implementing the System Theoretic Process Analysis (STPA) on specific levels of increasing autonomy, as described by the International Maritime Organization (IMO), where the human element is limited to remote monitoring and the on-board control system makes most of the decisions and takes most of the actions. STPA assumes that accidents are caused by unsafe interactions among system components, none of which may have failed. STPA presents an advantage over other hazard analysis tools, by being more flexible in terms of required information and by having a top-down functional model approach. The STPA’s results are used to determine how hazard occurrence changes, considering the interactions among system components and hard to identify failure points. These results will be used to evaluate the hypothesis that autonomous shipping will be safer and potentially contribute to the ongoing discussion for the regulatory framework that must be implemented for autonomous shipping to become a large-scale reality.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.petrol.2020.107509,Journal,Journal of Petroleum Science and Engineering,scopus,2020-11-01,sciencedirect,"Design and construction of the knowledge base system for geological outfield cavities classifications: An example of the fracture-cavity reservoir outfield in Tarim basin, NW China",https://api.elsevier.com/content/abstract/scopus_id/85087076723,"Tahe oilfield, located in NW Tarim Basin, is one of the largest and most difficult fracture cavity reservoirs in the world. Different fracture cavities, different generated mechanisms, and different oil production capacities. In order to study the significant parameters that can characterize the categories of facture-cavity. This research adopted outfield manual measurement, 3D digital modeling technique to obtain characterization parameters. According to experienced geological survey, typical outcrops were selected, then scanned by UAV (Unmanned aerial vehicle). Consequently, 3D digital models, including real coordinates and parameter information, were established by Agisoft Photoscan. Through geological testing results, various combination characteristic patterns of relative categories were analyzed. By using digital measure tool, combined with manually measured data, the parameters were extracted from the 3D digital model (DM). Then an initial geological database was established. For furtherly analyzing the database, the mathematic statistics methods of multiple linear regression (MLR), neural network technique (NNT) and discriminative classification technique (DCT) were applied. Using software of SPSS statistics 17.0, more than 200 groups of geological data (various categories of fracture-cavity) were optimally processed. Consequently, the significant characteristic parameters were interpreted to determine diverse categories. The results showed that: (1) cavity width, height, fracture length and cavity aspect ratio were significant parameters to classify runoff cavity categories. (2) Fault-controlled cavities could be accurately classified by fracture length and fracture density. (3) The main cavity categories could be distinguished by cavity width, cavity height and fracture density. Performances of the approach have been examined with 10 percentages of the samples, and a good agreement performed in the simulated results, and anastomosis rate was more than 80%. The researched results have critical guiding significance to evaluate types of fracture-cavity, develop and explore of fracture-cavity reservoirs. The construction technique of knowledge base can be applied for diverse fracture-cavity reservoirs in the various formations in different areas in the world.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.image.2020.115969,Journal,Signal Processing: Image Communication,scopus,2020-10-01,sciencedirect,Re-identification framework for long term visual object tracking based on object detection and classification,https://api.elsevier.com/content/abstract/scopus_id/85089267326,"In this paper, we address the problem of long-term visual object tracking and we present an efficient real-time single object tracking system suitable for integration in autonomous platforms that need to encompass intelligent capabilities. We propose a novel long-term tracking framework for classification based re-detection and tracking, that incorporates state estimation, object re-identification and automated management of tracking and detection results. Our method integrates a novel object re-identification technique which efficiently filters a number of detection candidates and systematically corrects the tracking results. Through extensive experimental validation on the UAV123, UAV20L and TLP datasets, we demonstrate the effectiveness of the proposed system and its advantage over several state-of-the art trackers. The results furthermore highlight the proposed tracker’s ability to handle challenges arising from real-world and long-term scenarios, such as variations in pose, scale, occlusions and out-of-view situations. Furthermore, we propose a variant that is suitable for deployment on autonomous robots, such as Unmanned Aerial Vehicles.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.sysarc.2020.101835,Journal,Journal of Systems Architecture,scopus,2020-10-01,sciencedirect,MBBNet: An edge IoT computing-based traffic light detection solution for autonomous bus,https://api.elsevier.com/content/abstract/scopus_id/85087859478,"Traffic light detection is a key module in the autonomous driving system to enhance the interactions between drivers and unmanned vehicles. In recent studies, deep neural networks are widely used for traffic light detection and resource/power consumption is a major concern for model deployment in vehicular edge devices. This paper proposes a novel light-weight deep CNN model that integrates the multi-backbone of state-of-the-art architectures for the self-driving traffic light detection. The MBBNet (Multi-BackBone Network) consists of three common convolutional backbones, i.e., the normal, residual and highway (DenseNet) convolutional modules. Simple ensemble of those backbones may incur high computational load. Therefore, channel compression is adopted to control the model parameters, while guaranteeing the accuracy for mobile and embedded hardware. Evaluation of a dataset collected from real road conditions demonstrate the robustness of our detection system, and it achieves higher accuracy (accuracy > 0.94 and 
                        
                           A
                           v
                           e
                           r
                           a
                           g
                           e
                           _
                           I
                           O
                           U
                           >
                           74.05
                           %
                        
                     ) for self-driving buses. In terms of resource consumption, the trained model size is 1.35 MB, and can process high-resolution images (1280 × 960) at 14 FPS (frames per second) on low-power edge devices.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ast.2020.105965,Journal,Aerospace Science and Technology,scopus,2020-10-01,sciencedirect,Towards a PDE-based large-scale decentralized solution for path planning of UAVs in shared airspace,https://api.elsevier.com/content/abstract/scopus_id/85086828428,"Recently, there has been a tremendous increase of interest in utilizing Unmanned Aerial Vehicles (UAVs) for a number of civilian applications. With this increased interest, it is imperative that these UAVs are able to operate in shared airspace for enhanced efficiency. Multi-UAV systems are inherently safety-critical systems, which means that safety guarantees must be made to ensure no undesirable configurations, such as collisions, occur. This paper proposes a decentralized method based on a Partial Differential Equation (PDE) to generate collision-free 3D trajectories for multiple UAVs operating in a shared airspace. This method exploits the dynamical properties of multi-phase fluids flowing through a porous medium by modeling the porosity values as a function of the risk of collision. To highlight the feasibility for on-board implementation, we propose propose a machine learning technique for obtaining computationally efficient solutions of the PDE describing flow movements in porous medium. This method has been compared via a simulation study to two other path planning strategies, centralized and sequential planning, and the advantages of this method are presented. Furthermore, results from an experiment using three UAVs have been presented to demonstrate the applicability of the proposed method to real-world implementation.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.iot.2020.100205,Journal,Internet of Things (Netherlands),scopus,2020-09-01,sciencedirect,Multi-UAV Allocation Framework for Predictive Crime Deterrence and Data Acquisition,https://api.elsevier.com/content/abstract/scopus_id/85105803842,"The recent decline in the number of police and security force personnel has raised a serious security issue that could lead to reduced public safety and delayed response to crimes in urban areas. This may be alleviated in part by utilizing micro or small unmanned aerial vehicles (UAVs) and their high-mobility on-board sensors in conjunction with machine-learning techniques such as neural networks to offer better performance in predicting times and places that are high-risk and deterring crimes. The key to the success of such operation lies in the suitable placement of UAVs. This paper proposes a multi-UAV allocation framework for predictive crime deterrence and data acquisition that consists of the overarching methodology, a problem formulation, and an allocation method that work with a prediction model using a machine learning approach. In contrast to previous studies, our framework provides the most effective arrangement of UAVs for maximizing the chance to apprehend offenders whilst also acquiring data that will help improve the performance of subsequent crime prediction. This paper presents the system architecture assumed in this study, followed by a detailed description of the methodology, the formulation of the problem, and the UAV allocation method of the proposed framework. Our framework is tested using a real-world crime dataset to evaluate its performance with respect to the expected number of crimes deterred by the UAV patrol. Furthermore, to address the engineering practice of the proposed framework, we discuss the feasibility of the simulated deployment scenario in terms of energy consumption and the relationship between data analysis and crime prediction.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacsc.2020.100096,Journal,IFAC Journal of Systems and Control,scopus,2020-09-01,sciencedirect,A biologically-inspired reinforcement learning based intelligent distributed flocking control for Multi-Agent Systems in presence of uncertain system and dynamic environment,https://api.elsevier.com/content/abstract/scopus_id/85098394516,"In this paper, we investigate the real-time flocking control of Multi-Agent Systems (MAS) in the presence of system uncertainties and dynamic environment. To handle the impacts from system uncertainties and dynamic environment, a novel reinforcement learning technique, which is appropriate for real-time implementation, has been integrated with multi-agent flocking control in this paper. The Brain Emotional Learning Based Intelligent Controller (BELBIC) is a biologically-inspired reinforcement learning-based controller relying on a computational model of emotional learning in the mammalian limbic system. The learning capabilities, multi-objective properties, and low computational complexity of BELBIC make it a very promising learning technique for implementation in real-time applications. Firstly, a novel brain emotional learning-based flocking control structure is proposed. Then, the real-time update laws are developed to tune the emotional signals based on real-time operational data. It is important to note that this data-driven reinforcement learning approach relaxes the requirement for system dynamics and effectively handle the uncertain impacts of the environment. Using the tuned emotional signals, the optimal flocking control can be obtained. The Lyapunov analysis has been used to prove the convergence of the proposed design. The effectiveness of the proposed design is also demonstrated through numerical and experimental results based on the coordination of multiple Unmanned Aerial Vehicles (UAVs).",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2020.103799,Journal,Engineering Applications of Artificial Intelligence,scopus,2020-09-01,sciencedirect,Trajectory based lateral control: A Reinforcement Learning case study,https://api.elsevier.com/content/abstract/scopus_id/85087950678,"Reinforcement Learning (RL) has been employed in many applications of robotics and has steadily been gaining traction in the field of Autonomous Driving (AD). This paper proposes a Deep Reinforcement Learning based approach for lateral Vehicle Motion Control (VMC), and explores the generalization capabilities of the approach. The proposed methodology uses a sequence of waypoints generated from a planning module of an AD stack as the input. The network has been trained to predict accurate steering commands to follow the given trajectory. In this paper we detail our implementation and share our learning experience on real-vehicle deployment of the RL based controller. Our experiments yield promising results with an agent trained on less than 4 h of simulated driving experience without any real-world data. The trained agent is able to successfully complete unseen and more complex tracks using different unseen vehicle models. The agent safely reached up to 150km/h in simulation and up to 60km/h in a real-life Sport Utility Vehicle (SUV) weighing more than 2000kg.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.trc.2020.102649,Journal,Transportation Research Part C: Emerging Technologies,scopus,2020-08-01,sciencedirect,Differential variable speed limits control for freeway recurrent bottlenecks via deep actor-critic algorithm,https://api.elsevier.com/content/abstract/scopus_id/85086802562,"Variable speed limit (VSL) control is a flexible way to improve traffic conditions, increase safety, and reduce emissions. There is an emerging trend of using reinforcement learning methods for VSL control. Currently, deep learning is enabling reinforcement learning to develop autonomous control agents for problems that were previously intractable. In this paper, a more effective deep reinforcement learning (DRL) model is developed for differential variable speed limit (DVSL) control, in which dynamic and distinct speed limits among lanes can be imposed. The proposed DRL model uses a novel actor-critic architecture to learn a large number of discrete speed limits in a continuous action space. Different reward signals, such as total travel time, bottleneck speed, emergency braking, and vehicular emissions are used to train the DVSL controller, and a comparison between these reward signals is conducted. The proposed DRL-based DVSL controllers are tested on a freeway with a simulated recurrent bottleneck. The simulation results show that the DRL based DVSL control strategy is able to improve the safety, efficiency and environment-friendliness of the freeway. In order to verify whether the controller generalizes to real world implementation, we also evaluate the generalization of the controllers on environments with different driving behavior attributes. and the robustness of the DRL agent is observed from the results.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isatra.2020.02.012,Journal,ISA Transactions,scopus,2020-06-01,sciencedirect,Adaptive tracking control of an unmanned aerial system based on a dynamic neural-fuzzy disturbance estimator,https://api.elsevier.com/content/abstract/scopus_id/85080895180,"The main goal of this study is developing an adaptive controller which can solve the trajectory tracking for a class of quadcopter unmanned aerial system (UAS), namely a quadrotor. The control design introduces a new paradigm for adaptive controllers based on the implementation of a set of differential neural networks (DNNs) in the consequence section of a Takagi–Sugeno (T–S) fuzzy inference system. This dynamic fuzzy inference structure was used to approximate the UAS description. The particular form of interaction between neural networks and fuzzy inference systems proposed in the present work received the name of dynamic neural fuzzy system (DNFS). An adaptive controller based on this DNFS form was the main solution attained in this study. This DNFS controller was focused on the estimation and compensation of the uncertain section of the Quadrotor dynamics and then, forced the UAS to perform a hover flight while the tracking of desired angular positions succeeded, which results in tracking a desired trajectory in the X-Y plane. The control design methodology supported on the Lyapunov stability theory guaranteed ultimate boundedness of the estimation and tracking errors simultaneously. Several experimental tests in an outdoor environment by using a real Quadrotor platform was performed by using an RTK-GPS (Real Time Kinematic) system to determine the position of the vehicle in the X-Y plane. The experimental results confirmed the superior performance of the proposed algorithm based on the combination of DNNs and T–S techniques with respect to classical robust controllers.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.rse.2020.111717,Journal,Remote Sensing of Environment,scopus,2020-05-01,sciencedirect,Assessing the relationship between macro-faunal burrowing activity and mudflat geomorphology from UAV-based Structure-from-Motion photogrammetry,https://api.elsevier.com/content/abstract/scopus_id/85079899077,"Characterisation of the ecosystem functioning of mudflats requires insight on the morphology and facies of these coastal features, but also on biological processes that influence mudflat geomorphology, such as crab bioturbation and the formation of benthic biofilms, as well as their heterogeneity at cm or less scales. Insight into this fine scale of ecosystem functioning is also important as far as minimizing errors in upscaling are concerned. The realisation of high-resolution ground surveys of these mudflats without perturbing their surface is a real challenge. Here, we address this challenge using UAV-supported photogrammetry based on the Structure-from-Motion (SfM) workflow. We produced a Digital Surface Model (DSM) and an orthophotograph at 1 cm and 0.5 cm pixel resolutions, respectively, of a mudflat in French Guiana, and mapped and classed into different size ranges intricate morphological features, including crab burrow apertures, tidal drainage creeks and depressions. We also determined subtle facies and elevation changes and slopes, and the footprint of different degrees of benthic biofilm development. The results generated at this scale of photogrammetric analysis also enabled us to relate macrofaunal crab burrowing activity to various parameters, including mudflat elevation, spatial distribution and sizes of creeks and depressions, benthic biofilm distribution, and flooding duration. SfM photogrammetry offers interesting new perspectives in fine-scale characterisation of the geomorphology, benthic activity and degree of biofilm development of dynamic muddy intertidal environments that are generally difficult of access. The main shortcomings highlighted in this study are a drift of accuracy of the DSM outside areas of ground control points and the deployment of which perturb the mudflat morphology and biology, the water-logged or very wet surfaces which generate reconstruction artefacts through the sun glint effect, and the time-consuming task of manual interpretation of extraction of features such as crab burrow apertures. On-going developments in UAV positioning integrating RTK/PPK GPS solutions for image-georeferencing and precise orientation with high-quality inertial measurement units will limit the difficulties inherent to ground control points, while conduction of surveys during homogeneous cloudy conditions could reduce the sun-glint effect. Manual extraction of image features could be automated in the future through the use of deep-learning algorithms.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2020.103472,Journal,Robotics and Autonomous Systems,scopus,2020-04-01,sciencedirect,Deploying MAVs for autonomous navigation in dark underground mine environments,https://api.elsevier.com/content/abstract/scopus_id/85079573394,"Operating Micro Aerial Vehicles (MAVs) in subterranean environments is becoming more and more relevant in the field of aerial robotics. Despite the large spectrum of technological advances in the field, flying in such challenging environments is still an ongoing quest that requires the combination of multiple sensor modalities like visual/thermal cameras as well as 3D and 2D lidars. Nevertheless, there exist cases in subterranean environments where the aim is to deploy fast and lightweight aerial robots for area reckoning purposes after an event (e.g. blasting in production areas). This work proposes a novel baseline approach for the navigation of resource constrained robots, introducing the aerial underground scout, with the main goal to rapidly explore unknown areas and provide a feedback to the operator. The main proposed framework focuses on the navigation, control and vision capabilities of the aerial platforms with low-cost sensor suites, contributing significantly towards real-life applications. The merit of the proposed control architecture is that it considers the flying platform as a floating object, composing a velocity controller on the 
                        x
                     , 
                        y
                      axes and altitude control to navigate along the tunnel. Two novel approaches make up the cornerstone of the proposed contributions for the task of navigation: (1) a vector geometry method based on 2D lidar, and (2) a Deep Learning (DL) method through a classification process based on an on-board image stream, where both methods correct the heading towards the center of the mine tunnel. Finally, the framework has been evaluated in multiple field trials in an underground mine in Sweden.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.imavis.2020.103889,Journal,Image and Vision Computing,scopus,2020-03-01,sciencedirect,Unsupervised domain adaptation for mobile semantic segmentation based on cycle consistency and feature alignment,https://api.elsevier.com/content/abstract/scopus_id/85079673578,"The supervised training of deep networks for semantic segmentation requires a huge amount of labeled real world data. To solve this issue, a commonly exploited workaround is to use synthetic data for training, but deep networks show a critical performance drop when analyzing data with slightly different statistical properties with respect to the training set. In this work, we propose a novel Unsupervised Domain Adaptation (UDA) strategy to address the domain shift issue between real world and synthetic representations. An adversarial model, based on the cycle consistency framework, performs the mapping between the synthetic and real domain. The data is then fed to a MobileNet-v2 architecture that performs the semantic segmentation task. An additional couple of discriminators, working at the feature level of the MobileNet-v2, allows to better align the features of the two domain distributions and to further improve the performance. Finally, the consistency of the semantic maps is exploited. After an initial supervised training on synthetic data, the whole UDA architecture is trained end-to-end considering all its components at once. Experimental results show how the proposed strategy is able to obtain impressive performance in adapting a segmentation network trained on synthetic data to real world scenarios. The usage of the lightweight MobileNet-v2 architecture allows its deployment on devices with limited computational resources as the ones employed in autonomous vehicles.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.comcom.2020.02.009,Journal,Computer Communications,scopus,2020-03-01,sciencedirect,UAV monitoring and forecasting model in intelligent traffic oriented applications,https://api.elsevier.com/content/abstract/scopus_id/85079351564,"Intelligent transportation system is a traffic management system developed with the progress of society and traffic. Its idea is to integrate the real-time operation of people, vehicles, roads and traffic involved in the traffic. The purpose of this paper is to build a safe, reliable and efficient vehicle monitoring and forecasting model for IOT. Based on the Beidou satellite positioning technology and Lora communication technology, aiming at the problem that the deep learning detection method cannot meet the real-time requirements in processing the monitoring video, this paper proposes a method of using multiple single target trackers instead of some yolov3 detection tasks, and puts forward the design idea and specific implementation scheme of the vehicle monitoring and prediction model. The vehicle monitoring and prediction model is used to detect four kinds of targets, namely, small cars, buses, trucks and pedestrians. The multi-target trajectory tracking is used to carry out the traffic statistics of multi vehicle types, the detection of two kinds of abnormal behaviors of traffic targets is low speed and parking, and the capture of pedestrians. The experimental results show that the vehicle monitoring and prediction model has the highest accuracy of location and type recognition for four types of traffic objects, namely, small cars, trucks, buses and pedestrians, reaching 80%.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-823014-5.00001-6,Book,Handbook of Deep Learning in Biomedical Engineering: Techniques and Applications,scopus,2020-01-01,sciencedirect,Early detection and diagnosis using deep learning,https://api.elsevier.com/content/abstract/scopus_id/85113589982,"With the growth of technology in every sphere, an attempt of reducing the human effort and increasing the accuracy of the system is always focused, and with the introduction to artificial intelligence (AI), we have covered almost every single domain in real-world scenario. The applications of AI are endless, and its use has helped in transforming the daily life through different use cases; under it, some of the highlighted examples for which it is well known in every sphere are natural language processing, self-driving cars, image processing, and so on. As it covers different domains of each sector, it also plays a significant role in healthcare sector. Influencing the sector entirely, it focuses on the end number of things that are part of this sector. When it approaches our health, specifically in matter when life and death are involved, the potential of AI to advance the consequences is enthralling. According to a survey, it has found that AI, specifically deep learning (DL), has the power to replace the whole discipline of medicine and is able to generate new characters for doctors called as information specialists. DL compromises substantial potential for medical diagnostics. There were times when detecting diseases was a tough task, but now with DL at rescue, diseases can be not only cured but also predicted at earlier stages. Chronic diseases such as Alzhiemer's disease, cancer, tumor, and much more can only be predicted by the advanced DL methods of prediction. Medical image processing plays a major role in detection and pinpointing of the exact problem it is leading to. The systems developed are smart enough to learn through the real cases and train the model as per the requirement, leading to the better prediction, detection, or providing with the methods of curing. Not only early-stage diagnosis is focused, but AI-assisted surgeries have also started taking place and AI-assisted nurses are also deployed for each patient to reduce the chances of errors. Mostly every problem of the healthcare sector whether it is administrative or medical or technical is majorly solved or falls under the category of DL, and many more researches related to this are under process.
               This chapter will help in determining how DL helps in the early diagnosis of several diseases such as Alzheimer's disease, rheumatic diseases, autism spectrum disorder, and more. After expanding upon the basics of DL and biomedical engineering, this chapter explores more upon diagnostics using DL and discusses the early diagnosis of certain diseases.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.1387,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Modelling human driving behavior for constrained model predictive control in mixed traffic at intersections,https://api.elsevier.com/content/abstract/scopus_id/85105108834,"Safe autonomous passing of intersections with mixed traffic, including human drivers and autonomous vehicles, is challenging. We propose a tailored approach that provides guarantees despite uncertainties fusing learned models and model predictive control. A single autonomous vehicle is controlled by the predictive controller via acceleration and steering angle without assumption of a global controller. Each maneuver of the human behaviour is modeled with a neural network, which enters the predictive controller formulation as a constraint. As an example, we consider a single autonomous vehicle on an unsignalized intersection, which gives right-of-way to a human-driven vehicle. We show how human driving behavior can be modeled based on real recorded trajectory data and implemented in the proposed predictive control approach by dynamically changing the constraints of the optimization problem.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.1459,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Deep learning based segmentation of fish in noisy forward looking MBES images,https://api.elsevier.com/content/abstract/scopus_id/85105082300,"In this work, we investigate a Deep Learning (DL) approach to fish segmentation in a small dataset of noisy low-resolution images generated by a forward-looking multibeam echosounder (MBES). We build on recent advances in DL and Convolutional Neural Networks (CNNs) for semantic segmentation and demonstrate an end-to-end approach for a fish/non-fish probability prediction for all range-azimuth positions projected by an imaging sonar. We use self-collected datasets from the Danish Sound and the Faroe Islands to train and test our model and present techniques to obtain satisfying performance and generalization even with a low-volume dataset. We show that our model proves the desired performance and has learned to harness the importance of semantic context and take this into account to separate noise and non-targets from real targets. Furthermore, we present techniques to deploy models on low-cost embedded platforms to obtain higher performance fit for edge environments – where compute and power are restricted by size/cost – for testing and prototyping.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2020.12.2306,Conference Proceeding,IFAC-PapersOnLine,scopus,2020-01-01,sciencedirect,Path-following control of fish-like robots: A deep reinforcement learning approach,https://api.elsevier.com/content/abstract/scopus_id/85102821107,"In this paper, we propose a deep reinforcement learning (DRL) approach for path-following control of a fish-like robot. The desired path may be a randomly generated Bézier curve. First, to implement the locomotion control of the fish-like robot, we design a modified Central Pattern Generated (CPG) model, using which the fish achieves varied swimming behaviors just by adjusting a single control input. To reduce the reality gap between simulation and the physical system, using the experimental data of the real fish-like robot, we build a surrogate simulation environment, which also well balances the accuracy and the speed of training. Second, for the path-following control, we select the advantage actor-critic (A2C) approach and train the control policy in the surrogate simulation environment with a straight line as the desired path. Then the trained control policy is directly deployed on a physical fish-like robot to follow a randomly generated Bézier curve. The experimental results show that our proposed approach has good practical applicability in view of its efficiency and feasibility in controlling the physical fishlike robot. This work shows a novel and promising way to control biomimetic underwater robots in the real world.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2020.10.062,Conference Proceeding,Procedia Computer Science,scopus,2020-01-01,sciencedirect,Deep real-time anomaly detection for connected autonomous vehicles,https://api.elsevier.com/content/abstract/scopus_id/85099885335,"Connected and autonomous vehicles (CAV) are expected to change the landscape of the automotive market. They are autonomous decision-making systems that process streams of observations coming from different external and on-board sensors. CAV like any other cyber-physical objects are prone to signal interference, hardware deterioration, software errors, power instability, and cyber-attacks. To avoid these anomalies which can be fatal, it is mandatory to design a robust real-time technique to detect them and identify their sources. In this paper, we propose a deep learning approach which consists of hierarchic models to firstly extract the signal features using an LSTM auto-encoder, then perform an accurate classification of each signal sequence in real-time. In addition, we investigated the impact of the model parameter tuning on the anomaly detection and the advantage of channel boosting through three scenarios. The model achieves an accuracy of 95.5% and precision of 94.2%.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.trpro.2020.03.108,Conference Proceeding,Transportation Research Procedia,scopus,2020-01-01,sciencedirect,Estimating time of arrival of trains at level crossings for the provision of multimodal cooperative services,https://api.elsevier.com/content/abstract/scopus_id/85084662425,"While cooperative services have been almost fully deployed in the road sector and are already being implemented in various cities in Europe as a pre-requisite for the introduction of autonomous vehicles, few attempts have been made in the same direction for the rail sector. This study proposes a system that aims to improve safety and minimize risk in the meeting point between road and rail, known as level crossings, by monitoring the location of floating road vehicles via a mobile device application. A neural network predictive model for estimating time of arrival of trains is also utilized. The safety system has been implemented and tested under real life conditions in the city of Thessaloniki, Greece.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2019.10.115,Journal,Neurocomputing,scopus,2020-01-01,sciencedirect,A cascade adaboost and CNN algorithm for drogue detection in UAV autonomous aerial refueling,https://api.elsevier.com/content/abstract/scopus_id/85082518295,"To promote the combat capability of unmanned aerial vehicles (UAVs) in the future battlefield, the autonomous aerial refueling (AAR) technology becomes a challenging research issue. An accurate position relationship between the tanker and the receiver is significant for AAR. A novel drogue detection method is presented in this paper. The Adaptive boosting (Adaboost) and the convolutional neural networks (CNN) classifier with the improved focal loss (IFL) function are utilized to detect the drogue in complex environments. The sample imbalance during the training stage of the CNN classifier is solved by the IFL function. The PyTorch deep learning framework is employed to implement the software system with the graphics processing units (GPUs). Real scenario images with a mimetic drogue on the tanker are captured for training and testing dataset by the airborne camera on the receiver. The experimental results indicate that the presented algorithm can accelerate the detection speed and improve the detection accuracy.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ress.2019.106555,Journal,Reliability Engineering and System Safety,scopus,2019-11-01,sciencedirect,A cognitive architecture safety design for safety critical systems,https://api.elsevier.com/content/abstract/scopus_id/85068360978,"This research is presented as a safety analysis of a cognitive architecture with an intelligent decision support model (IDSM) that is embedded into an autonomous non-deterministic safety-critical system.
                  Cognitive technology is currently simulated within safety-critical systems in order to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves the system's cognitive performance. In this research, the safety of the architecture was analyzed on an actual safety-critical system, an unmanned surface vehicle (USV). The safety analysis was conducted in both a simulated and a real world nautical based environment. The objective was to define the safety design of a cognitive architecture. The input to the safety design was provided through an approach that identified and mitigated hazards associated with a USV controlled by a cognitive architecture. This analysis provided a structured, task-oriented approach for the dissemination of information concerning safety requirements. This approach was necessary to achieve a safe execution of the USV's capabilities through a design that reduces the potential for injury to personnel and damage to equipment.
                  Other real time applications that would benefit from advancing the safety of cognitive technologies are unmanned platforms, transportation technologies, and service robotics. The results will provide cognitive science researchers with a reference for safety engineering of artificially intelligent safety-critical systems.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.isprsjprs.2019.07.009,Journal,ISPRS Journal of Photogrammetry and Remote Sensing,scopus,2019-09-01,sciencedirect,Development and evaluation of a deep learning model for real-time ground vehicle semantic segmentation from UAV-based thermal infrared imagery,https://api.elsevier.com/content/abstract/scopus_id/85069676654,"Real-time unmanned aerial vehicles (UAVs)-based thermal infrared images processing, due to high spatial resolution and knowledge of the various infrared radiant energy level distribution of solid bodies, has important applications such as monitoring and control of the various phenomena in different natural situations. One of these applications is monitoring the ground vehicles in cities by using detection or semantic segmentation of them in the thermal images. In this research, our purpose is to improve the performance of deep learning combined model by using Gaussian-Bernoulli Restricted Boltzmann Machine (GB-RBM) specifications for the segmentation of the ground vehicles from UAV-based thermal infrared imagery. The proposed model is studied in three steps. First, designing the proposed model by using an encoder-decoder structure and addition of extracted features from convolutional layers and restricted Boltzmann machine in the network. Second, the implementation of the research goals on four sets of UAV-based thermal infrared imagery named NPU_CS_UAV_IR_DATA that was collected from some streets of China by using FLIR TAU2 thermal infrared sensor in 2017. Finally, analyzing the performance of the proposed model by using five state-of-the-art models in semantic segmentation. The results evaluated the performance of the proposed model as a robust model with the average precision and average processing time of approximately 0.97, and 19.73 s for all datasets, respectively.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.net.2018.12.020,Journal,Nuclear Engineering and Technology,scopus,2019-06-01,sciencedirect,Numerical evaluation of gamma radiation monitoring,https://api.elsevier.com/content/abstract/scopus_id/85059358720,"Airborne Gamma Ray Spectrometry (AGRS) with its important applications such as gathering radiation information of ground surface, geochemistry measuring of the abundance of Potassium, Thorium and Uranium in outer earth layer, environmental and nuclear site surveillance has a key role in the field of nuclear science and human life. The Broyden–Fletcher–Goldfarb–Shanno (BFGS), with its advanced numerical unconstrained nonlinear optimization in collaboration with Artificial Neural Networks (ANNs) provides a noteworthy opportunity for modern AGRS. In this study a new AGRS system empowered by ANN-BFGS has been proposed and evaluated on available empirical AGRS data. To that effect different architectures of adaptive ANN-BFGS were implemented for a sort of published experimental AGRS outputs. The selected approach among of various training methods, with its low iteration cost and non-diagonal scaling allocation is a new powerful algorithm for AGRS data due to its inherent stochastic properties. Experiments were performed by different architectures and trainings, the selected scheme achieved the smallest number of epochs, the minimum Mean Square Error (MSE) and the maximum performance in compare with different types of optimization strategies and algorithms. The proposed method is capable to be implemented on a cost effective and minimum electronic equipment to present its real-time process, which will let it to be used on board a light Unmanned Aerial Vehicle (UAV). The advanced adaptation properties and models of neural network, the training of stochastic process and its implementation on DSP outstands an affordable, reliable and low cost AGRS design. The main outcome of the study shows this method increases the quality of curvature information of AGRS data while cost of the algorithm is reduced in each iteration so the proposed ANN-BFGS is a trustworthy appropriate model for Gamma-ray data reconstruction and analysis based on advanced novel artificial intelligence systems.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2018.11.017,Journal,Robotics and Autonomous Systems,scopus,2019-05-01,sciencedirect,A real-time framework for kinodynamic planning in dynamic environments with application to quadrotor obstacle avoidance,https://api.elsevier.com/content/abstract/scopus_id/85062619374,"The objective of this paper is to present a full-stack, real-time motion planning framework for kinodynamic robots and then show how it is applied and demonstrated on a physical quadrotor system operating in a laboratory environment. The proposed framework utilizes an offline–online computation paradigm, neighborhood classification through machine learning, sampling-based motion planning with an optimal cost distance metric, and trajectory smoothing to achieve real-time planning for aerial vehicles. This framework accounts for dynamic obstacles with an event-based replanning structure and a locally reactive control layer that minimizes replanning events. The approach is demonstrated on a quadrotor navigating moving obstacles in an indoor space and stands as, arguably, one of the first demonstrations of full-online kinodynamic motion planning, with execution cycles of 3 Hz to 5 Hz. For the quadrotor, a simplified dynamics model is used during the planning phase to accelerate online computation. A trajectory smoothing phase, which leverages the differentially flat nature of quadrotor dynamics, is then implemented to guarantee a dynamically feasible trajectory.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.aap.2018.12.019,Journal,Accident Analysis and Prevention,scopus,2019-03-01,sciencedirect,Evaluating the safety impact of connected and autonomous vehicles on motorways,https://api.elsevier.com/content/abstract/scopus_id/85059327033,"Recent technological advancements bring the Connected and Autonomous Vehicles (CAVs) era closer to reality. CAVs have the potential to vastly improve road safety by taking the human driver out of the driving task. However, the evaluation of their safety impacts has been a major challenge due to the lack of real-world CAV exposure data. Studies that attempt to simulate CAVs by using either a single or integrating multiple simulation platforms have limitations, and in most cases, consider a small element of a network (e.g. a junction) and do not perform safety evaluations due to inherent complexity. This paper addresses this problem by developing a decision-making CAV control algorithm in the simulation software VISSIM, using its External Driver Model Application Programming Interface. More specifically, the developed CAV control algorithm allows a CAV, for the first time, to have longitudinal control, search adjacent vehicles, identify nearby CAVs and make lateral decisions based on a ruleset associated with motorway traffic operations. A motorway corridor within M1 in England is designed in VISSIM and employed to implement the CAV control algorithm. Five simulation models are created, one for each weekday. The baseline models (i.e. CAV market penetration: 0%) are calibrated and validated using real-world minute-level inductive loop detector data and also data collected from a radar-equipped vehicle. The safety evaluation of the proposed algorithm is conducted using the Surrogate Safety Assessment Model (SSAM). The results show that CAVs bring about compelling benefit to road safety as traffic conflicts significantly reduce even at relatively low market penetration rates. Specifically, estimated traffic conflicts were reduced by 12–47%, 50–80%, 82–92% and 90–94% for 25%, 50%, 75% and 100% CAV penetration rates respectively. Finally, the results indicate that the presence of CAVs ensured efficient traffic flow.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2018.12.013,Journal,Applied Soft Computing Journal,scopus,2019-03-01,sciencedirect,Online identification of a rotary wing Unmanned Aerial Vehicle from data streams,https://api.elsevier.com/content/abstract/scopus_id/85059117819,"Until now the majority of the neuro and fuzzy modeling and control approaches for rotary wing Unmanned Aerial Vehicles (UAVs), such as the quadrotor, have been based on batch learning techniques, therefore static in structure, and cannot adapt to rapidly changing environments. Implication of Evolving Intelligent System (EIS) based model-free data-driven techniques in fuzzy system are good alternatives, since they are able to evolve both their structure and parameters to cope with sudden changes in behavior, and performs perfectly in a single pass learning mode which is suitable for online real-time deployment. The Metacognitive Scaffolding Learning Machine (McSLM) is seen as a generalized version of EIS since the metacognitive concept enables the what-to-learn, how-to-learn, and when-to-learn scheme, and the scaffolding theory realizes a plug-and-play property which strengthens the online working principle of EISs. This paper proposes a novel online identification scheme, applied to a quadrotor using real-time experimental flight data streams based on McSLM, namely Metacognitive Scaffolding Interval Type 2 Recurrent Fuzzy Neural Network (McSIT2RFNN). Our proposed approach demonstrated significant improvements in both accuracy and complexity against some renowned existing variants of the McSLMs and EISs.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-815956-9.00006-5,Book,Technology in Supply Chain Management and Logistics: Current Practice and Future Applications,scopus,2019-01-01,sciencedirect,Emerging technologies in the health-care supply chain,https://api.elsevier.com/content/abstract/scopus_id/85082603106,"In this chapter, the background and organization of the health-care supply chain are reviewed, and the impact of emerging technologies is described. Maturing technologies, including optimization software, sensors/telematics, cloud computing, data warehouse systems, and automated storage and retrieval, are examined. Growth technologies, including mobility, wearable devices, data analytics, and social media, are examined as they potentially relate to the health-care supply chain. Emerging technologies, including 3D printing, drone delivery, and autonomous vehicles, are presented and examples provided on their use in the health-care supply chain. Exponential technologies, including blockchain, the Internet of Things, virtual/augmented reality, and artificial intelligence, are described with respect to potential applications in the health-care supply chain. Future changes in the external environment of health care, including decentralization, new competitors, and the increased use of telemedicine, are described with respect to impacts on the health-care supply chain.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/B978-0-12-815956-9.00002-8,Book,Technology in Supply Chain Management and Logistics: Current Practice and Future Applications,scopus,2019-01-01,sciencedirect,Technologies in supply chain management and logistics,https://api.elsevier.com/content/abstract/scopus_id/85082571600,"Until recently, technology has been considered an enabler for improvements in underlying supply chain and logistics operations. However, recent trends in society and business, such as mobile computing, social media, and online retailing, have significantly changed almost every aspect of the supply chain and logistics landscape. In this chapter the following technologies were found to have a pervasive role in altering this landscape:
               
                  
                     
                        •
                        Maturing technologies
                     
                     
                        •
                        Optimization software
                     
                     
                        •
                        Sensors/Telematics
                     
                     
                        •
                        Cloud computing
                     
                     
                        •
                        Data warehouse and integration
                     
                     
                        •
                        Automated storage and retrieval
                     
                     
                        •
                        Growth technologies
                     
                     
                        •
                        Mobility
                     
                     
                        •
                        Wearability
                     
                     
                        •
                        Data analytics
                     
                     
                        •
                        Social media
                     
                     
                        •
                        Emerging technologies
                     
                     
                        •
                        3D printing
                     
                     
                        •
                        Drones
                     
                     
                        •
                        Autonomous vehicles
                     
                     
                        •
                        Exponential
                     
                     
                        •
                        Blockchain
                     
                     
                        •
                        Internet of Things
                     
                     
                        •
                        Virtual reality
                     
                     
                        •
                        Machine learning
                     
                  
               
               Each of these technologies will be discussed along with videos illustrating their use.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2019.12.529,Conference Proceeding,IFAC-PapersOnLine,scopus,2019-01-01,sciencedirect,Autonomous Canal Following by a Micro-Aerial Vehicle Using Deep CNN,https://api.elsevier.com/content/abstract/scopus_id/85081052924,"Globally, large-scale irrigation canal networks serve as the backbone of agriculture in many important river basins. However, these water channels are in a constant threat of erosion, silt accumulation and structural damages over time which significantly reduces the water carrying capacity. Therefore, periodic inspections of the canals are required for critical operations and maintenance tasks. Due to the vast lengths of the channels and time-critical operations, automation has become a necessity. In this paper, we have proposed an aerial autonomous canal traversal system using ResNet50 inspired deep convolutional neural network. Given the uniqueness of our problem, we have generated our dataset for supervised learning and validation and later evaluated the proposed approach on a real canal. We have implemented our approach on a COTS micro-aerial vehicle. We have designed our system in such a way that it takes 200ms from perception to action thereby making the system real-time. We compare the superior performance of our Res Net 50 inspired network with other state-of-the-art CNNs trained on canal datasets.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.procs.2019.09.442,Conference Proceeding,Procedia Computer Science,scopus,2019-01-01,sciencedirect,Unmanned aerial vehicle in the machine learning environment,https://api.elsevier.com/content/abstract/scopus_id/85079097933,"Unmanned Aerial Vehicles and machine learning have started gaining attentions of academic and industrial research. The Unmanned Aerial Vehicles have extended the freedom to operate and monitor the activities from remote locations. This study retrieved and synthesized research on the use of Unmanned Aerial Vehicles along with machine learning and its algorithms in different areas and regions. The objective was to synthesize the scope and importance of machine learning models in enhancing Unmanned Aerial Vehicles capabilities, solutions to problems, and numerous application areas.
                  The machine learning implementation has reduced numbers of challenges to Unmanned Aerial Vehicles besides enhancing the capabilities and opening the door to the different sectors. The Unmanned Aerial Vehicles and machine learning association has resulted in fast and reliable outputs. The combination of Unmanned Aerial Vehicles and machine learning has helped in real time monitoring, data collection and processing, and prediction in the computer/wireless networks, smart cities, military, agriculture, and mining.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2018.10.010,Journal,Applied Soft Computing Journal,scopus,2019-01-01,sciencedirect,A hybridization of extended Kalman filter and Ant Colony Optimization for state estimation of nonlinear systems,https://api.elsevier.com/content/abstract/scopus_id/85056208326,"In this paper, a new nonlinear heuristic filter based on the hybridization of an extended Kalman filter and an ant colony estimator is proposed to estimate the states of a nonlinear system. In this filter, a group of virtual ants searches the state space stochastically and dynamically to find and track the best state estimation while the position of each ant is updated at the measurement time using the extended Kalman filter. The performance of the proposed filter is compared with well-known heuristic filters using a nonlinear benchmark problem. The statistical results show that this algorithm is able to provide promising and competitive results. Then, the new filter is tested on a nonlinear engineering problem with more than one state. The problem is to estimate simultaneously the states of an unmanned aerial vehicle as well as the wind disturbances, applied to the system. In this case, a processor-in-the-loop experiment is also performed to verify the implementation capability of the proposed approach. This paper also investigates the real-time implementation capability of the proposed filter in the attitude estimation of a three degrees of freedom experimental setup of a quadrotor to further investigate its effectiveness in practice.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.knosys.2018.04.015,Journal,Knowledge-Based Systems,scopus,2018-08-01,sciencedirect,Teaching a vehicle to autonomously drift: A data-based approach using Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/85045849458,"This paper presents a novel approach to teach a vehicle how to drift, in a similar manner that professional drivers do. Specifically, a hybrid structure formed by a Model Predictive Controller and feedforward Neural Networks is employed for this purpose. The novelty of this work lies in a) the adoption of a data-based approach to achieve autonomous drifting along a wide range of road radii and body slip angles, and b) in the implementation of a road terrain classifier to adjust the system actuation depending on the current friction characteristics. The presented drift control system is implemented in a multi-actuated ground vehicle equipped with active front steering and in-wheel electric motors and trained to drift by a real test driver using a driver-in-the-loop setup. Its performance is verified in the simulation environment IPG-CarMaker through different open loop and path following drifting manoeuvres.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.compind.2018.03.014,Journal,Computers in Industry,scopus,2018-06-01,sciencedirect,Real-time object detection in agricultural/remote environments using the multiple-expert colour feature extreme learning machine (MEC-ELM),https://api.elsevier.com/content/abstract/scopus_id/85044151304,"It is necessary for autonomous robotics in agriculture to provide real time feedback, but due to a diverse array of objects and lack of landscape uniformity this objective is inherently complex. The current study presents two implementations of the multiple-expert colour feature extreme learning machine (MEC-ELM). The MEC-ELM is a cascading algorithm that has been implemented along side a summed area table (SAT) for fast feature extraction and object classification, for a fully functioning object detection algorithm. The MEC-ELM is an implementation of the colour feature extreme learning machine (CF-ELM), which is an extreme learning machine (ELM) with a partially connected hidden layer; taking three colour bands as inputs. The colour implementation used with the SAT enable the MEC-ELM to find and classify objects quickly, with 84% precision and 91% recall in weed detection in the Y’UV colour space and in 0.5 s per frame. The colour implementation is however limited to low resolution images and for this reason a colour level co-occurrence matrix (CLCM) variant of the MEC-ELM is proposed. This variant uses the SAT to produce a CLCM and texture analyses, with texture values processed as an input to the MEC-ELM. This enabled the MEC-ELM to achieve 78–85% precision and 81–93% recall in cattle, weed and quad bike detection and in times between 1 and 2 s per frame. Both implementations were benchmarked on a standard i7 mobile processor. Thus the results presented in this paper demonstrated that the MEC-ELM with SAT grid and CLCM makes an ideal candidate for fast object detection in complex and/or agricultural landscapes.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2017.12.016,Journal,Neurocomputing,scopus,2018-03-22,sciencedirect,Use of human gestures for controlling a mobile robot via adaptive CMAC network and fuzzy logic controller,https://api.elsevier.com/content/abstract/scopus_id/85038844344,"Mobile robots with manipulators have been more and more commonly applied in extreme and hostile environments to assist or even replace human operators for complex tasks. In addition to autonomous abilities, mobile robots need to facilitate the human–robot interaction control mode that enables human users to easily control or collaborate with robots. This paper proposes a system which uses human gestures to control an autonomous mobile robot integrating a manipulator and a video surveillance platform. A human user can control the mobile robot just as one drives an actual vehicle in the vehicle’s driving cab. The proposed system obtains human’s skeleton joints information using a motion sensing input device, which is then recognized and interpreted into a set of control commands. This is implemented, based on the availability of training data set and requirement of in-time performance, by an adaptive cerebellar model articulation controller neural network, a finite state machine, a fuzzy controller and purposely designed gesture recognition and control command generation systems. These algorithms work together implement the steering and velocity control of the mobile robot in real-time. The experimental results demonstrate that the proposed approach is able to conveniently control a mobile robot using virtual driving method, with smooth manoeuvring trajectories in various speeds.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.artint.2017.12.001,Journal,Artificial Intelligence,scopus,2018-03-01,sciencedirect,Decentralized Reinforcement Learning of robot behaviors,https://api.elsevier.com/content/abstract/scopus_id/85038868982,"A multi-agent methodology is proposed for Decentralized Reinforcement Learning (DRL) of individual behaviors in problems where multi-dimensional action spaces are involved. When using this methodology, sub-tasks are learned in parallel by individual agents working toward a common goal. In addition to proposing this methodology, three specific multi agent DRL approaches are considered: DRL-Independent, DRL Cooperative-Adaptive (CA), and DRL-Lenient. These approaches are validated and analyzed with an extensive empirical study using four different problems: 3D Mountain Car, SCARA Real-Time Trajectory Generation, Ball-Dribbling in humanoid soccer robotics, and Ball-Pushing using differential drive robots. The experimental validation provides evidence that DRL implementations show better performances and faster learning times than their centralized counterparts, while using less computational resources. DRL-Lenient and DRL-CA algorithms achieve the best final performances for the four tested problems, outperforming their DRL-Independent counterparts. Furthermore, the benefits of the DRL-Lenient and DRL-CA are more noticeable when the problem complexity increases and the centralized scheme becomes intractable given the available computational resources and training time.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ssci.2017.10.020,Journal,Safety Science,scopus,2018-03-01,sciencedirect,Safety engineering of computational cognitive architectures within safety-critical systems,https://api.elsevier.com/content/abstract/scopus_id/85034114889,"This paper presents the integration of a cognitive architecture with an intelligent decision support model (IDSM) that is embedded into an autonomous non-deterministic safety critical system. The IDSM will integrate multi-criteria decision making via intelligent technologies like expert systems, fuzzy logic, machine learning and genetic algorithms.
                  Cognitive technology is currently simulated in safety–critical systems to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves a system’s cognitive performance. In this study, the IDSM is being applied to an actual safety–critical system, an unmanned surface vehicle (USV) with embedded artificial intelligence (AI) software. The USV’s safety performance is being researched in a simulated and a real world nautical based environment. The objective is to build a dynamically changing model to evaluate a cognitive architecture’s ability to ensure safe performance of an intelligent safety–critical system. The IDSM does this by finding a set of key safety performance parameters that can be critiqued via safety measurements, mechanisms and methodologies. The uniqueness of this research will be on bounding the decision making associated with the cognitive architecture’s key safety parameters (KSP).
                  Other real-time applications that could benefit from advancing the safety of cognitive technologies are unmanned platforms, transportation technologies, and service robotics. The results will provide cognitive science researchers a reference for safety engineering artificially intelligent safety–critical systems.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.bdr.2017.06.002,Journal,Big Data Research,scopus,2018-03-01,sciencedirect,Fast Deep Convolutional Face Detection in the Wild Exploiting Hard Sample Mining,https://api.elsevier.com/content/abstract/scopus_id/85025460499,"Face detection constitutes a key visual information analysis task in Machine Learning. The rise of Big Data has resulted in the accumulation of a massive volume of visual data which requires proper and fast analysis. Deep Learning methods are powerful approaches towards this task as training with large amounts of data exhibiting high variability has been shown to significantly enhance their effectiveness, but often requires expensive computations and leads to models of high complexity. When the objective is to analyze visual content in massive datasets, the complexity of the model becomes crucial to the success of the model. In this paper, a lightweight deep Convolutional Neural Network (CNN) is introduced for the purpose of face detection, designed with a view to minimize training and testing time, and outperforms previously published deep convolutional networks in this task, in terms of both effectiveness and efficiency. To train this lightweight deep network without compromising its efficiency, a new training method of progressive positive and hard negative sample mining is introduced and shown to drastically improve training speed and accuracy. Additionally, a separate deep network was trained to detect individual facial features and a model that combines the outputs of the two networks was created and evaluated. Both methods are capable of detecting faces under severe occlusion and unconstrained pose variation and meet the difficulties of large scale real-world, real-time face detection, and are suitable for deployment even in mobile environments such as Unmanned Aerial Vehicles (UAVs).",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2017.09.044,Journal,Neurocomputing,scopus,2018-01-31,sciencedirect,Transferring deep knowledge for object recognition in Low-quality underwater videos,https://api.elsevier.com/content/abstract/scopus_id/85030701582,"In recent years, underwater video technologies allow us to explore the ocean in scientific and noninvasive ways, such as environmental monitoring, marine ecology studies, and fisheries management. However the low-light and high-noise scenarios pose great challenges for the underwater image and video analysis. We here propose a CNN knowledge transfer framework for underwater object recognition and tackle the problem of extracting discriminative features from relatively low contrast images. Even with the insufficient training set, the transfer framework can well learn a recognition model for the special underwater object recognition task together with the help of data augmentation. For better identifying objects from an underwater video, a weighted probabilities decision mechanism is introduced to identify the object from a series of frames. The proposed framework can be implemented for real-time underwater object recognition on autonomous underwater vehicles and video monitoring systems. To verify the effectiveness of our method, experiments on a public dataset are carried out. The results show that the proposed method achieves promising results for underwater object recognition on both test image datasets and underwater videos.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2017.08.573,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Emotions Embodied in the SVC of an Autonomous Driver System,https://api.elsevier.com/content/abstract/scopus_id/85031823496,"A concept of embodied intelligence (EI) is considered. None of such implementations can be fully identified with artificial intelligence. Projects that dare to approach AI and EI should be based on both the AI concepts (symbolic and sub-symbolic), in solving real problems of perception and decision-making. Therefore, the EI, in this paper, is understood as a methodology that uses all available resources and algorithms from the analytic and synthetic approaches, in order to implement an intelligent and autonomous agent.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.ifacol.2017.08.1219,Conference Proceeding,IFAC-PapersOnLine,scopus,2017-07-01,sciencedirect,Implementation of Brain Emotional Learning-Based Intelligent Controller for Flocking of Multi-Agent Systems,https://api.elsevier.com/content/abstract/scopus_id/85031823232,"The Brain Emotional Learning Based Intelligent Controller (BELBIC) is a neurobiologically-motivated intelligent controller based on a computational model of emotional learning in mammalian limbic system. The learning capabilities, multi-objective properties, and low computational complexity of BELBIC make it a very promising tool for implementation in real-time applications.
                  Our research combines, in an original way, the BELBIC methodology with a flocking control strategy, in order to perform real-time coordination of multiple Unmanned Aircraft Systems (UAS). The characteristics of BELBIC fit well in this scenario, since almost always the dynamics of the autonomous agents are not fully known, and furthermore, since they operate in close proximity, they are subjected to aggressive external disturbances. Numerical and experimental results based on the coordination of multiple quad rotorcraft UAS platforms demonstrate the applicability and satisfactory performance of the proposed method.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2016.08.019,Journal,Engineering Applications of Artificial Intelligence,scopus,2017-06-01,sciencedirect,GPU-based parallel optimization of immune convolutional neural network and embedded system,https://api.elsevier.com/content/abstract/scopus_id/84995489085,"Up to now, the image recognition system has been utilized more and more widely in the security monitoring, the industrial intelligent monitoring, the unmanned vehicle, and even the space exploration. In designing the image recognition system, the traditional convolutional neural network has some defects such as long training time, easy over-fitting and high misclassification rate. In order to overcome these defects, we firstly used the immune mechanism to improve the convolutional neural network and put forward a novel immune convolutional neural network algorithm, after we analyzed the network structure and parameters of the convolutional neural network. Our algorithm not only integrated the location data of the network nodes and the adjustable parameters, but also dynamically adjusted the smoothing factor of the basis function. In addition, we utilized the NVIDIA GPU (Graphics Processing Unit) to accelerate the new immune convolutional neural network (ICNN) in parallel computing and built a real-time embedded image recognition system for this ICNN. The immune convolutional neural network algorithm was improved with CUDA programming and was tested with the sample data in the GPU-based environment. The GPU-based implementation of the novel immune convolutional neural network algorithm was made with the cuDNN, which was designed by NVIDIA for GPU-based accelerating of DNNs in machine learning. Experimental results show that our new immune convolutional neural network has higher recognition rate, more stable performance and faster computing speed than the traditional convolutional neural network.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jpowsour.2017.01.105,Journal,Journal of Power Sources,scopus,2017-01-01,sciencedirect,Adaptive prognosis of lithium-ion batteries based on the combination of particle filters and radial basis function neural networks,https://api.elsevier.com/content/abstract/scopus_id/85011392342,"Lithium-Ion rechargeable batteries are widespread power sources with applications to consumer electronics, electrical vehicles, unmanned aerial and spatial vehicles, etc. The failure to supply the required power levels may lead to severe safety and economical consequences. Thus, in view of the implementation of adequate maintenance strategies, the development of diagnostic and prognostic tools for monitoring the state of health of the batteries and predicting their remaining useful life is becoming a crucial task. Here, we propose a method for predicting the end of discharge of Li-Ion batteries, which stems from the combination of particle filters with radial basis function neural networks. The major innovation lies in the fact that the radial basis function model is adaptively trained on-line, i.e., its parameters are identified in real time by the particle filter as new observations of the battery terminal voltage become available. By doing so, the prognostic algorithm achieves the flexibility needed to provide sound end-of-discharge time predictions as the charge-discharge cycles progress, even in presence of anomalous behaviors due to failures or unforeseen operating conditions. The method is demonstrated with reference to actual Li-Ion battery discharge data contained in the prognostics data repository of the NASA Ames Research Center database.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.robot.2016.07.003,Journal,Robotics and Autonomous Systems,scopus,2016-10-01,sciencedirect,A practical approach for detection and classification of traffic signs using Convolutional Neural Networks,https://api.elsevier.com/content/abstract/scopus_id/84991736217,"Automatic detection and classification of traffic signs is an important task in smart and autonomous cars. Convolutional Neural Networks has shown a great success in classification of traffic signs and they have surpassed human performance on a challenging dataset called the German Traffic Sign Benchmark. However, these ConvNets suffer from two important issues. They are not computationally suitable for real-time applications in practice. Moreover, they cannot be used for detecting traffic signs for the same reason. In this paper, we propose a lightweight and accurate ConvNet for detecting traffic signs and explain how to implement the sliding window technique within the ConvNet using dilated convolutions. Then, we further optimize our previously proposed real-time ConvNet for the task of traffic sign classification and make it faster and more accurate. Our experiments on the German Traffic Sign Benchmark datasets show that the detection ConvNet locates the traffic signs with average precision equal to 
                        99.89
                        %
                     . Using our sliding window implementation, it is possible to process 37.72 high-resolution images per second in a multi-scale fashion and locate traffic signs. Moreover, single ConvNet proposed for the task of classification is able to classify 
                        99.55
                        %
                      of the test samples, correctly. Finally, our stability analysis reveals that the ConvNet is tolerant against Gaussian noise when 
                        σ
                        <
                        10
                     .",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.eswa.2016.03.024,Journal,Expert Systems with Applications,scopus,2016-10-01,sciencedirect,Building detection from orthophotos using a machine learning approach: An empirical study on image segmentation and descriptors,https://api.elsevier.com/content/abstract/scopus_id/84963795376,"Building detection from aerial images has many applications in fields like urban planning, real-estate management, and disaster relief. In the last two decades, a large variety of methods on automatic building detection have been proposed in the remote sensing literature. Many of these approaches make use of local features to classify each pixel or segment to an object label, therefore involving an extra step to fuse pixelwise decisions. This paper presents a generic framework that exploits recent advances in image segmentation and region descriptors extraction for the automatic and accurate detection of buildings on aerial orthophotos. The proposed solution is supervised in the sense that appearances of buildings are learnt from examples. For the first time in the context of building detection, we use the matrix covariance descriptor, which proves to be very informative and compact. Moreover, we introduce a principled evaluation that allows selecting the best pair segmentation algorithm-region descriptor for the task of building detection. Finally, we provide a performance evaluation at pixel level using different classifiers. This evaluation is conducted over 200 buildings using different segmentation algorithms and descriptors. The performance analysis quantifies the quality of both the image segmentation and the descriptor used. The proposed approach presents several advantages in terms of scalability, suitability and simplicity with respect to the existing methods. Furthermore, the proposed scheme (detection chain and evaluation) can be deployed for detecting multiple object categories that are present in images and can be used by intelligent systems requiring scene perception and parsing such as intelligent unmanned aerial vehicle navigation and automatic 3D city modeling.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.trpro.2016.05.240,Conference Proceeding,Transportation Research Procedia,scopus,2016-01-01,sciencedirect,Advanced Driver Assistance System for Road Environments to Improve Safety and Efficiency,https://api.elsevier.com/content/abstract/scopus_id/84991383337,"The advances in Information Technologies have led to more complex road safety applications. These systems provide multiple possibilities for improving road transport. The integrated system that this paper presents deals with two aspects that have been identified as key topics: safety and efficiency. To this end, the development and implementation of an integrated advanced driver assistance system (ADAS) for rural and intercity environments is proposed. The system focuses mainly on single-carriageways roads, given the complexity of these environments compared to motorways and the high number of severe and fatal accidents on them. The proposed system is based on advanced perception techniques, vehicle automation and communications between vehicles (V2V) and with the infrastructure (V2I). Sensor fusion architecture based on computer vision and laser scanner technologies are developed. It allows real time detection and classification of obstacles, and the identification of potential risks. The driver receives this information and some warnings generated by the system. In case, he does not react in a proper way, the vehicle could perform autonomous actions (both on speed control or steering maneuvers) to improve safety and/or efficiency. Furthermore, a multimodal V2V and V2I communication system, based on GeoNetworking, facilitates the flow of information between vehicles and assists in the detection and information broadcasting processes. All this, combined with vehicle positioning, detailed digital maps and advanced map-matching algorithms, establish the decision algorithms of different ADAS systems.
                  The applications developed include: adaptive cruise control with consumption optimization, overtaking assistance system in single-carriageways roads that takes into account appropriate speed evolution and identifies most suitable road stretches for the maneuver; assistance system in intersections with speed control during approximation maneuvers, and collision avoidance system with the possibility of evasive maneuvers. To this end, mathematical vehicle dynamics models have been used to ensure the stability, and propulsion system models are used to establish efficient patterns, Artificial Intelligence and simulation are used for experimentation and evaluation of algorithms to be implemented in the control unit. Finally, the system is designed to warn the driver if a risk is detected and, if necessary, to take control of the vehicle. The system has been implemented on a passenger car and has been tested in specific scenarios on a test track with satisfactory results.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.apor.2015.09.010,Journal,Applied Ocean Research,scopus,2015-10-01,sciencedirect,Neural adaptive robust control of underactuated marine surface vehicles with input saturation,https://api.elsevier.com/content/abstract/scopus_id/84945290024,"This paper proposes a saturated tracking controller for underactuated autonomous marine surface vehicles with limited torque. First, a second-order open-loop error dynamic model is developed in the actuated degrees of freedom to simplify the design procedure. Then, a saturated tracking controller is designed by utilizing generalized saturation functions to reduce the risk of actuator saturation. This, in turn, improves the transient performance of the control system. A multi-layer neural network and adaptive robust control techniques are also employed to preserve the controller robustness against unmodeled dynamics and environmental disturbances induced by waves and ocean currents. A Lyapunov stability analysis shows that all signals of the closed-loop system are bounded and tracking errors are semi-globally uniformly ultimately bounded. Finally, simulation results are provided for a hovercraft vehicle to illustrate the effectiveness of the proposed controller as a qualified candidate for real implementations in offshore applications.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.oceaneng.2015.06.026,Journal,Ocean Engineering,scopus,2015-07-14,sciencedirect,Leader-follower formation control of underactuated autonomous marine surface vehicles with limited torque,https://api.elsevier.com/content/abstract/scopus_id/84937210986,"This paper proposes a leader–follower formation tracking controller for underactuated autonomous marine surface vehicles with limited torque under environmental disturbances. A second-order formation dynamic model is developed in the actuated degrees of freedom of the followers to simplify the design procedure. Then, a formation tracking controller is designed by utilizing generalized saturation functions to reduce the risk of actuator saturation. Radial basis function neural network and adaptive robust control techniques are also adopted to preserve the controller robustness against unmodeled dynamics and environmental disturbances induced by waves and ocean currents. A Lyapunov-based stability analysis shows that all signals of the closed-loop system are bounded and tracking errors are semi-globally uniformly ultimately bounded. Finally, simulation results are provided for a group of surface vessels to illustrate the effectiveness of the proposed controller as a qualified candidate for real implementations in offshore applications.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.bica.2015.04.008,Journal,Biologically Inspired Cognitive Architectures,scopus,2015-01-01,sciencedirect,Automatic navigation of wall following mobile robot using Adaptive Resonance Theory of Type-1,https://api.elsevier.com/content/abstract/scopus_id/84960798237,"The automatic navigation of wall following robot is playing important role in various real world tasks such as underwater exploration, unmanned flight, and in automotive industries based on its computational complexity. In this work, a novel navigation approach based on biologically inspired neural network, known as “Adaptive Resonance Theory-1” which was proposed by Carpenter and Grossberg, has been implemented and investigated for navigation of wall following mobile robots. The proposed navigation algorithm is successfully tested with three sensor reading datasets obtained from clockwise navigation of SCITOS G5 mobile robot. Test decision accuracy (%), and simulation time were used as performance analysis parameters for the proposed algorithm and it has been found that the present work can achieve 99.59% of maximum decision accuracy.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neucom.2013.03.060,Journal,Neurocomputing,scopus,2014-05-20,sciencedirect,Autonomous UAV based search operations using constrained sampling evolutionary algorithms,https://api.elsevier.com/content/abstract/scopus_id/84896707806,"This paper introduces and studies the application of Constrained Sampling Evolutionary Algorithms in the framework of an UAV based search and rescue scenario. These algorithms have been developed as a way to harness the power of Evolutionary Algorithms (EA) when operating in complex, noisy, multimodal optimization problems and transfer the advantages of their approach to real time real world problems that can be transformed into search and optimization challenges. These types of problems are denoted as Constrained Sampling problems and are characterized by the fact that the physical limitations of reality do not allow for an instantaneous determination of the fitness of the points present in the population that must be evolved. A general approach to address these problems is presented and a particular implementation using Differential Evolution as an example of CS-EA is created and evaluated using teams of UAVs in search and rescue missions. The results are compared to those of a Swarm Intelligence based strategy in the same type of problem as this approach has been widely used within the UAV path planning field in different variants by many authors.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/C2012-0-02673-7,Book,Efficient Computation of Argumentation Semantics,scopus,2014-01-14,sciencedirect,Efficient Computation of Argumentation Semantics,https://api.elsevier.com/content/abstract/scopus_id/84941975883,Unknown,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.engappai.2014.01.007,Journal,Engineering Applications of Artificial Intelligence,scopus,2014-01-01,sciencedirect,Adaptive multi-objective reinforcement learning with hybrid exploration for traffic signal control based on cooperative multi-agent framework,https://api.elsevier.com/content/abstract/scopus_id/84894106219,"In this paper, we focus on computing a consistent traffic signal configuration at each junction that optimizes multiple performance indices, i.e., multi-objective traffic signal control. The multi-objective function includes minimizing trip waiting time, total trip time, and junction waiting time. Moreover, the multi-objective function includes maximizing flow rate, satisfying green waves for platoons traveling in main roads, avoiding accidents especially in residential areas, and forcing vehicles to move within moderate speed range of minimum fuel consumption. In particular, we formulate our multi-objective traffic signal control as a multi-agent system (MAS). Traffic signal controllers have a distributed nature in which each traffic signal agent acts individually and possibly cooperatively in a MAS. In addition, agents act autonomously according to the current traffic situation without any human intervention. Thus, we develop a multi-agent multi-objective reinforcement learning (RL) traffic signal control framework that simulates the driver's behavior (acceleration/deceleration) continuously in space and time dimensions. The proposed framework is based on a multi-objective sequential decision making process whose parameters are estimated based on the Bayesian interpretation of probability. Using this interpretation together with a novel adaptive cooperative exploration technique, the proposed traffic signal controller can make real-time adaptation in the sense that it responds effectively to the changing road dynamics. These road dynamics are simulated by the Green Light District (GLD) vehicle traffic simulator that is the testbed of our traffic signal control. We have implemented the Intelligent Driver Model (IDM) acceleration model in the GLD traffic simulator. The change in road conditions is modeled by varying the traffic demand probability distribution and adapting the IDM parameters to the adverse weather conditions. Under the congested and free traffic situations, the proposed multi-objective controller significantly outperforms the underlying single objective controller which only minimizes the trip waiting time (i.e., the total waiting time in the whole vehicle trip rather than at a specific junction). For instance, the average trip and waiting times are ≃8 and 6 times lower respectively when using the multi-objective controller.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jmsy.2011.09.002,Journal,Journal of Manufacturing Systems,scopus,2012-04-01,sciencedirect,Intelligent evaluation of supplier bids using a hybrid technique in distributed supply chains,https://api.elsevier.com/content/abstract/scopus_id/84858340427,"The main idea of this research is to devise the smart module to pick the best supplier bid(s) automatically. The hybrid model is composed of three useful tools: fuzzy logic, AHP, and QFD. The approach has been carefully implemented and verified via a real-world case study in a medium-to-large industry manufacturing vehicle tires and other rubber products. A collection of 12 assessment criteria classified into two categories have been considered. Eight factors are derived from customer suggestions and the other four are design specifications required to manufacture the product. The main outcomes are: a hybrid autonomous model to evaluate supplier bids without direct human intervention; devising a hybrid three-module method and overcoming complexity of computations in resulting algorithm by means of agents; outlining the best criteria to assess suppliers; evaluating the suppliers based on voice of customer during all stages of the process; and discussing analysis, design, and implementation issues of the evaluation agent. The paper includes implications for development of an integrated total system for supply chain coordination. The most important advantages of this work over earlier researches on supplier selection are: implementation of an autonomous assessment mechanism using intelligent agents for the first time, making the best out of three widely applied methodologies all at once, evaluation process mainly based on features of customer order, coordination of supply job based on a bidding system, and portal-mediated operation and control.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20120403-3-DE-3010.00065,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Towards gun- and aircraft - Launched MAVs: Embedded flight control system,https://api.elsevier.com/content/abstract/scopus_id/84866108968,"The actual paper presents an embedded flight control systems (EFCS) of two micro air vehicles (MAVs) concepts, the gun-launched MAV (GLMAV) and the aircraft-launched MAV (ALMAV), meant to rapidly reach a target zone further away from the launching point. The paper details the embedded architecture used by both vehicles at software and hardware level. The software layer contains the algorithms to process the onboard sensors information as well as to compute the control law. The hardware layer features the bidirectional communication link with the ground workstation and interfaces the control commands with the actuators. The key element of the overall embedded processing architecture is the Gumstix-COM which operates under the Xenomai real-time framework. Concerning the configuration of the airframe, we provide a descriptive study of the transition phase, covering the dynamic model and control scheme. The evaluation of the two concepts implies a comparative, regarding to identify similarities and differences of the configuration, i.e. advantages and drawbacks. Motivating simulations results were obtained in the evaluation of the flight controller within the transition phase. The experimental evaluation of the proposed EFCS during autonomous attitude-stabilized flight has returned promising results.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20120403-3-DE-3010.00041,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2012-01-01,sciencedirect,Embedded system for controlling a mini underwater vehicle in autonomous hover mode,https://api.elsevier.com/content/abstract/scopus_id/84866098847,"This work presents the development of a mini underwater vehicle (Triton-PR), the embedded system, and the experiments in real-time for autonomous hover operation. Artificial vision allows the vehicle to obtain the translational position and velocity. The main characteristic of the embedded system is the implementation of low cost devices and materials, besides the number and location of the thrusters was chosen in order to have enough power and generate the rotation and translation movements. The dynamical model of the (Triton-PR) is described by the classic Euler-Lagrange equations, and a PD controller based on saturation functions is proposed for providing autonomous attitude and position of the robot. Finally, the performance of the vehicle is shown in simulation and real-time experimental results.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.neunet.2009.06.032,Journal,Neural Networks,scopus,2009-07-01,sciencedirect,Extending the Evolutionary Robotics approach to flying machines: An application to MAV teams,https://api.elsevier.com/content/abstract/scopus_id/68149170032,"The work presented in this article focuses on the use of embodied neural networks–developed through Evolutionary Robotics and Multi-Agent Systems methodologies–as autonomous distributed controllers for Micro-unmanned Aerial Vehicle (MAV) teams. The main aim of the research is to extend the range of domains that could be successfully tackled by the Evolutionary Robotics approach. The flying robots realm is an area that has not been yet thoroughly investigated by this discipline. This is due to the lack of an affordable and reliable robotic platform to use for carrying out experiments, and to the difficulty and the high computational load involved in experiments based upon a realistic software simulator for aircraft. We believe that the most recent improvements to the state of the art now permit the investigation of this domain. For demonstrating this point, two different evolutionary computer simulation models are presented in this article. The first model, which uses a simplified 2D test environment, has resulted in controllers evolved with the following capabilities: (1) navigation through unknown environments, (2) obstacle-avoidance, (3) tracking of a movable target, and (4) execution of cooperative and coordinated behaviors based on implicit communication strategies. In order to improve the robustness of these results and their potential use in real MAV teams, a more sophisticated 3D model is presented herein. The results obtained so far using the two models demonstrate the feasibility of the chosen approach for further research on the design of autonomous controllers for MAVs.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.asoc.2007.07.009,Journal,Applied Soft Computing Journal,scopus,2008-03-01,sciencedirect,Direct adaptive neural flight control system for an unstable unmanned aircraft,https://api.elsevier.com/content/abstract/scopus_id/37249000905,"A direct adaptive controller design using neural network is proposed for an unstable unmanned research aircraft similar in configuration to combat aircraft. The control law to track the pitch rate command is developed based on system theory. Neural network with linear filters and back propagation through time learning algorithm is used to approximate the control law. The bounded signal requirement to develop the neural controller is circumvented using an off-line finite time training scheme, which provides the necessary stability and tracking performances. On-line learning scheme is implemented to compensate for uncertainties due to variation in aerodynamic coefficients, control surface failures and also variations in center of gravity position. The performance of the proposed control scheme is validated at different flight conditions. The disturbance rejection capability of the neural controller is analyzed in the presence of the realistic gust and sensor noises. Hardware-in-loop simulation is also carried out to study the behavior of control surface deflections in real-time.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/j.jnca.2004.10.005,Journal,Journal of Network and Computer Applications,scopus,2005-08-01,sciencedirect,A future framework for interfacing BDI agents in a real-time teaming environment,https://api.elsevier.com/content/abstract/scopus_id/29144487764,"Research and applications in human–machine teaming continue to evolve the role of the human from immediate (manual) operator into supervisory and televisory controller. In the supervisory control role, the human operator will be functionally removed from the system under control and in the televisory role, the human operator will be physically removed. Although unmanned systems and vehicles have become a technical reality that drives this change, they will not eliminate the importance of the human operator as the commanding and controlling element in-the-loop. This paper will argue that existing automation concepts remain equally valid with an even greater emphasis on the need for a human-centered automation approach. Intelligent agent technology has become mature and attractive enough to implement the automated components of the human–machine team. Agents that implement the Beliefs-Desire-Intention syntax will be discussed as being of particular interest for human–machine teaming applications. This paper proposes a theoretical framework for teaming human and intelligent agents. The teaming framework will be demonstrated in a real-time simulation environment using the commercial game called Unreal Tournament and its existing GameBot extension. The intelligent agents will be implemented based on the Belief-Desire-Intention (BDI) syntax and using JACK, a commercial BDI Agent development language. The requirements for follow-on research, such as human–agent teaming, human–agent coordination and agent learning will be highlighted.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.3182/20020721-6-es-1901.01303,Conference Proceeding,IFAC Proceedings Volumes (IFAC-PapersOnline),scopus,2002-01-01,sciencedirect,High-level control of autonomous robots using a behavior-based scheme and reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/84945581469,"This paper proposes a behavior-based scheme for high-level control of autonomous robots. Two main characteristics can be highlighted in the control scheme. Behavior coordination is done through a hybrid methodology, which takes in advantages of the robustness and modularity in competitive approaches, as well as optimized trajectories in cooperative ones. As a second feature, behavior state/action mapping is learnt by means of Reinforcement Learning (RL). A new continuous approach of the Q_learning algorithm, implemented with a multi-layer neural network, is used. The behavior-based scheme attempts to fulfill simple missions in which several behaviors/tasks compete for the vehicle's control. This paper is centered in the RL-based behaviors. In order to test the feasibility of the proposed Neural-Q_learning scheme, real experiments with the underwater robot ODIN in a target following behavior were done. Results showed the convergence of the behavior into an optimal state/action mapping. Discussion about the proposed approach is given, as well as an overall description of the high level control scheme.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/S0952-1976(01)00004-5,Journal,Engineering Applications of Artificial Intelligence,scopus,2001-01-01,sciencedirect,Parallel and diagonal parking in nonholonomic autonomous vehicles,https://api.elsevier.com/content/abstract/scopus_id/0035435172,"This paper considers the problem of parallel and diagonal parking in wheeled vehicles. A method to plan in real-time a set of collision-free manoeuvres is presented. Artificial intelligent techniques, namely fuzzy logic, play an important role in the practical application of the method. Thus, a fuzzy system is used to select the most suitable manoeuvre from the solution set according with the environment, dealing with optimality, path tracking performance and collision avoidance trade-off. This technique has been implemented in a fuzzy behaviour-based control architecture combining planning and reactivity. The efficiency of the proposed method is demonstrated using the nonholonomic mobile robot ROMEO-3R designed and built at the University of Seville.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/S0736-5845(99)00033-2,Journal,Robotics and Computer-Integrated Manufacturing,scopus,1999-01-01,sciencedirect,A new approach to vision-based unsupervised learning of unexplored indoor environment for autonomous land vehicle navigation,https://api.elsevier.com/content/abstract/scopus_id/0042538803,"A vision-based approach to unsupervised learning of the indoor environment for autonomous land vehicle (ALV) navigation is proposed. The ALV may, without human's involvement, self-navigate systematically in an unexplored closed environment, collect the information of the environment features, and then build a top-view map of the environment for later planned navigation or other applications. The learning system consists of three subsystems: a feature location subsystem, a model management subsystem, and an environment exploration subsystem. The feature location subsystem processes input images, and calculates the locations of the local features and the ALV by model matching techniques. To facilitate feature collection, two laser markers are mounted on the vehicle which project laser light on the corridor walls to form easily detectable line and corner features. The model management subsystem attaches the local model into a global one by merging matched corner pairs as well as line segment pairs. The environment exploration subsystem guides the ALV to explore the entire navigation environment by using the information of the learned model and the current ALV location. The guidance scheme is based on the use of a pushdown transducer derived from automata theory. A prototype learning system was implemented on a real vehicle, and simulations and experimental results in real environments show the feasibility of the proposed approach.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/S0921-8890(98)00005-0,Journal,Robotics and Autonomous Systems,scopus,1998-10-31,sciencedirect,A robust landmark-based system for vehicle location using low-bandwidth vision,https://api.elsevier.com/content/abstract/scopus_id/0032180335,"This paper presents novel computer algorithms, a system architecture, and the prototype implementation of a vision-based automatic vehicle location system. The objective of the vehicle location system is to keep track of the vehicle location for a human driver, and perhaps to provide the driver with real-time audio directions to his destination. The techniques developed here are equally applicable to autonomous robot navigation. The prototype system uses odometer readings and a skeleton map to perform dead reckoning, and uses low-bandwidth visual information and neural networks to recognize places for correcting cumulative dead reckoning errors. The visual information is also used to detect turns, for dead reckoning at intersections. The system is self-contained in the sense that it requires no infrastructure outside the vehicle, such as external beacons installed on roadways or satellites used by Global Positioning Systems (GPS). The system maintains a large number of location hypotheses and searches for a large number of landmarks stored in a database in real time. Hence the system is robustly able to recover the vehicle location after being lost for various reasons. The system has been tested, with success, in both day and night time, in all four seasons, and on roads in New York City, a regional highway, and on suburban streets.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/0925-2312(94)00018-N,Journal,Neurocomputing,scopus,1995-01-01,sciencedirect,Fast computation of optimal paths using a parallel Dijkstra algorithm with embedded constraints,https://api.elsevier.com/content/abstract/scopus_id/0029329099,"We have developed a new optimal path algorithm in which the paths are subjected to turning constraints. The restriction which we have incorporated is the next link in the path must not make an angle exceeding 45 ° in magnitude with the preceeding link. This algorithm has a natural implementation as an artificial neural system with either synchronous or asynchronous weight updating, and as an automata executing on a massively parallel array processor. At a given step in the path solution process our path planning artificial neural system keeps track of all constrained optimal paths flowing into the nodes of the network. This new algorithm has applications to any path planning problem where the vehicle traveling the path is subject to a limited turning capability. The ability of the network to solve for constrained paths is illustrated with both a graph theoretic example and a scenario involving an unmanned vehicle that must travel a constrained path through a real terrain area containing artificially generated keep out zones.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/0952-1976(94)90020-5,Journal,Engineering Applications of Artificial Intelligence,scopus,1994-01-01,sciencedirect,Curvature-optimal path planning and servoing for autonomous vehicles: A neural net implementation,https://api.elsevier.com/content/abstract/scopus_id/0028406446,"This paper shows how to implement curvature-optimal solutions of path-planning and trajectoryservoing problems for autonomous vehicles by using neural nets. The neural net is used to implement the nonlinear optimal state-feedback laws. The trained net can be used in real-time control. The net is trained by simulated data generated at random. No two-point boundary-value problems need to be solved. The method presented can, in general, be applied to a wide range of time-invariant terminal control or servo control problems.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1016/0165-1684(93)90040-H,Journal,Signal Processing,scopus,1993-01-01,sciencedirect,Determination of road directions using feedback neural nets,https://api.elsevier.com/content/abstract/scopus_id/0027588835,"Autonomous vehicles may be driven by image data of real-world scenes collected through a TV camera. Detecting the clues for safe navigation requires, among other things, the estimation of the path to be followed by the vehicle, which has proven to be a formidable task in outdoor scenes. In this paper, an innovative system for road direction detection is proposed which is composed of three specialized blocks performing edge extraction, image-segments detection and road estimation. The road direction estimation block is implemented as a feedback neural network and is not fed directly with image data but with higher-level image features which are extracted through the preprocessing stages. The use of feedback, while reducing the complexity of the network, improves the estimation robustness and the noise immunity. A novel algorithm is defined and employed for the training step and experimental results in outdoor scenes are reported.",autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
