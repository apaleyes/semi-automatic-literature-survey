doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database,query_name,query_value
10.1109/ICEEICT53905.2021.9667885,A Convolutional Neural Network for End to End Structural Prediction and Lane Detection for Autonomous Vehicle,IEEE,Conferences,"Real-time automated road lane recognition is an essential aspect of a smart vehicle safety system. The driver assistance system is the most significant advancement for intelligent automobiles. The vision-based driver assistance system of intelligent vehicles provides automatic road lane detection. This paper proposes a novel approach based on CNN for recognizing road lane and road border markings in the development of autonomous vehicle navigation. At first, it transforms the image of the RGB street scene to a gray image and then uses the flood-fill technique to identify the gray image’s related components. Following that, the road region, which is the largest connected component in the labeled image, is extracted using the maximum width and number of pixels. Finally, the outer region is excluded, and the road lane and road border markers are separated from the associated components. The numerical results prove the applicability of the proposed algorithm on both straight and slightly curved road scene photos under various daylight circumstances and the presence of road shadows. The driver support system consists of a camera support system that captures real-time pictures of the vehicle environs and displays pertinent information to the driver. As a result, intelligent vehicles automatically collect information about the road lane and the vehicle’s position corresponding to the lane. The system employed by smart vehicles thus offers the means of alerting drivers who swerve off the lane without using the blinker. Smart vehicles will therefore clearly improve traffic safety if they are extensively adopted.",https://ieeexplore.ieee.org/document/9667885/,2021 5th International Conference on Electrical Engineering and Information Communication Technology (ICEEICT),18-20 Nov. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSM49833.2020.9201644,A Preliminary Investigation of an Autonomous Vehicle Validation Infrastructure for Smart Cities,IEEE,Conferences,"The research and development of autonomous vehicle has entered the era of commercialization. While the vehicle self-driving technology has been growing rapidly, the validation for autonomous vehicle in terms of driving model, human factor model and traffic model is still maturing. Most of previous infrastructures are mainly focused on validation of those three models separately resorting either on real driving test at physical infrastructure or software simulation in virtualized infrastructure. However, neither the real driving test can cover all possible scenarios of autonomous driving and human factors, nor the virtualized software simulation can generate a feasible model for practical on/off-road driving. Furthermore, future autonomous transport in smart cities requires comprehensive validation. In order for autonomous vehicles to meet the autonomous transport in such complex traffic environment, an integrated testing and simulation infrastructure has been built targeting the systematic validation for autonomous vehicles: the Multi-User Environment for Autonomous Vehicle Innovation (MUEAVI). A preliminary investigation of a new autonomous vehicle validation infrastructure that can serve a multitude of research projects for smart city is presented.",https://ieeexplore.ieee.org/document/9201644/,2020 International Conference Mechatronic Systems and Materials (MSM),1-3 July 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1986.1087458,An architecture for reflexive autonomous vehicle control,IEEE,Conferences,"We describe a software architecture to support the planning and control requirements of an autonomous land vehicle. This architecture is designed specifically to handle diverse terrain with maximal speed, efficacy and versatility through the use of a library of reflexive strategies specialized to particular needs. A hierarchy of control is built in which lower level modules perform tasks requiring greatest immediacy while higher level modules perform tasks involving greater assimilation of sensor data. With all levels of the hierarchy operating in parallel, higher level modules perform selective activation and deactivation of lower level responses with limited interference from demands for immediacy, allowing the system to exploit the advantages of both immediate and assimilated data. The reflexive component of this system has been demonstrated both in a detailed real-time simulation and on a small indoor robotic vehicle.",https://ieeexplore.ieee.org/document/1087458/,Proceedings. 1986 IEEE International Conference on Robotics and Automation,7-10 April 1986,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2018.8500461,Autonomous Vehicle Testing and Validation Platform: Integrated Simulation System with Hardware in the Loop,IEEE,Conferences,"With the development of autonomous driving, offline testing remains an important process allowing low-cost and efficient validation of vehicle performance and vehicle control algorithms in multiple virtual scenarios. This paper aims to propose a novel simulation platform with hardware in the loop (HIL). This platform comprises of four layers: the vehicle simulation layer, the virtual sensors layer, the virtual environment layer and the Electronic Control Unit (ECU) layer for hardware control. Our platform has attained multiple capabilities: (1) it enables the construction and simulation of kinematic car models, various sensors and virtual testing fields; (2) it performs a closed-loop evaluation of scene perception, path planning, decision-making and vehicle control algorithms, whilst also having multi-agent interaction system; (3) it further enables rapid migrations of control and decision-making algorithms from the virtual environment to real self-driving cars. In order to verify the effectiveness of our simulation platform, several experiments have been performed with self-defined car models in virtual scenarios of a public road and an open parking lot and the results are substantial.",https://ieeexplore.ieee.org/document/8500461/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMPEUR.1992.218441,Developments in autonomous vehicle navigation,IEEE,Conferences,An approach to autonomous vehicle navigation using neural networks to imitate human driving behavior is presented. The results are based on the measured actions of a real human driver in a real car. The car environment and the information that is recorded during driving are described. The problem that should be solved by the neural network is discussed. The actual learning of the driving task is presented. The quality of the leaned driving behavior is reviewed. The experiments showed that medium-sized neural networks were able to approximate the driving behavior within a maximum error of 5%.<>,https://ieeexplore.ieee.org/document/218441/,CompEuro 1992 Proceedings Computer Systems and Software Engineering,4-8 May 1992,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAS51828.2021.9458488,Evaluation of Machine Learning-based Detection against Side-Channel Attacks on Autonomous Vehicle,IEEE,Conferences,"Autonomous vehicles are becoming increasingly popular, but their reliance on computer systems to sense and operate in the physical world has introduced new security risks. Recent studies have shown that using Cache-based Side-Channel Attacks (SCAs) could infer sensitive users' information (e.g., which route the user is taking) highlighting significant vulnerability posed to today's computer systems. As a result, it is crucial to propose effective detection mechanisms against emerging microarchitectural SCAs on autonomous driving systems. In response, we first identify the threat model and victim applications of autonomous driving systems in this work. Next, we explore the suitability of various machine learning-based classifiers trained by information collected from built-in hardware performance counter registers available in modern autonomous vehicle systems. To this end, various supervised machine learning models are implemented for cache-based SCAs detection and precisely compared and characterized in terms of detection accuracy, robustness, and latency of the detection. Our experiments conducted on an Intel Xeon, which Waymo autonomous driving vendor uses, demonstrate that J48 achieves 99.5% accuracy with the highest efficiency compared with other investigated models.",https://ieeexplore.ieee.org/document/9458488/,2021 IEEE 3rd International Conference on Artificial Intelligence Circuits and Systems (AICAS),6-9 June 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTCFall.2017.8288318,Impact to Longitude Velocity Control of Autonomous Vehicle from Human Driver's Distraction Behavior,IEEE,Conferences,"Driver distraction behaviors are usually blind to autonomous vehicles (AVs), leading to probable late preparation for AVs to take emergency measures. Hence, this paper aims to build a bridge between AV control and driver behavior detection, to assist AVs to predict the potential risk and avoid abnormal drivers carefully like experienced drivers. Our main contributions of this paper consist: i) put forward a practicable system framework integrating driver distraction monitoring, vehicle-to-vehicle communication and AV velocity control; ii) provide a real-time driver distraction monitoring implementation building on convolutional neural network trained offline; iii) propose a method of longitude velocity control of AV considering the risk of driver distraction behavior based on model predictive control strategy. Simulation results validate the effectiveness of our work.",https://ieeexplore.ieee.org/document/8288318/,2017 IEEE 86th Vehicular Technology Conference (VTC-Fall),24-27 Sept. 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE49875.2021.9637402,Learning Autonomous Driving in Tangible Practice: Development and On-Road Applications of a 1/10-Scale Autonomous Vehicle,IEEE,Conferences,"This Innovative Practice Work-In-Progress Paper presents a case of learning autonomous driving in tangible practice. As technology sustainably enhances the quality of life, intelligent systems continue to contribute solutions to some of the biggest challenges faced by humans. Autonomous vehicles offer humans the opportunity to increase transportation safety by reducing human errors on the road, preventing accidents, improving human productivity by reducing commuting time, and possibly mitigating air pollution. There is a critical shortage of educational and training programs in autonomous vehicles due to the high cost of full-size vehicles, computing and sensor equipment, and big lab space needed. To address this problem, we develop a 1/10-scale autonomous vehicle powered by pre-collision detection, lane tracking, and road sign recognition systems. The pre-collision system is built using ultrasonic sensors, and the Proportional-Integral-Derivative (PID) control is implemented to manipulate the vehicle's safety response. The Open-Source Computer Vision Library (OpenCV) is exploited to detect and process real-time on-road streaming video to enable lane-tracking and road sign recognition. AI techniques are utilized for the model training. Preliminary results of this work are presented and analyzed. We also discuss the future directions of this study.",https://ieeexplore.ieee.org/document/9637402/,2021 IEEE Frontiers in Education Conference (FIE),13-16 Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2010.71,Microcontroller Based Neural Network Controlled Low Cost Autonomous Vehicle,IEEE,Conferences,"In this paper, design of a low cost autonomous vehicle based on neural network for navigation in unknown environments is presented. The vehicle is equipped with four ultrasonic sensors for hurdle distance measurement, a wheel encoder for measuring distance traveled, a compass for heading information, a GPS receiver for goal position information, a GSM modem for changing destination place on run time and a nonvolatile RAM for storing waypoint data; all interfaced to a low cost AT89C52 microcontroller. The microcontroller processes the information acquired from the sensors and generates robot motion commands accordingly through neural network. The neural network running inside the microcontroller is a multilayer feed-forward network with back-propagation training algorithm. The network is trained offline with tangent-sigmoid as activation function for neurons and is implemented in real time with piecewise linear approximation of tangent-sigmoid function. Results have shown that upto twenty neurons can be implemented in hidden layer with this technique. The vehicle is tested with varying destination places in outdoor environments containing stationary as well as moving obstacles and is found to reach the set targets successfully.",https://ieeexplore.ieee.org/document/5460762/,2010 Second International Conference on Machine Learning and Computing,9-11 Feb. 2010,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2016.7734068,Monocular vision-based object recognition for autonomous vehicle driving in a real driving environment,IEEE,Conferences,"Nowadays, many attentions have been devoted to autonomous vehicles because the automation of driving technology has a large number of benefits, such as the minimization of risks, the improvement of mobility and ease of drivers. Among many technologies for autonomous driving, road environmental recognition is one of the key issues. In this paper, we present the test results of various object detection algorithms using single monocular camera for autonomous vehicle in real driving conditions. The vision recognition system tested in this paper has three main recognition parts: pedestrian detection, traffic sign and traffic light recognition. We use Histogram of Gradients (HOG) features and detect the pedestrians by Support Vector Machine (SVM). Also features of traffic signs are extracted by Principal Components Analysis (PCA) and canny edge detection is used for traffic lights. These two signals are classified by Neural Network (NN). Algorithms that we tested are implemented in General-Purpose computing on Graphics Processing Units (GPGPU). We show the effectiveness of these methods in real-time applications for autonomous driving.",https://ieeexplore.ieee.org/document/7734068/,2016 13th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),19-22 Aug. 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVE45908.2019.8965211,Optimizing coverage of simulated driving scenarios for the autonomous vehicle,IEEE,Conferences,"Self-driving cars and advanced driver-assistance systems are perceived as a game-changer in the future of road transportation. However, their validation is mandatory before industrialization; testing every component should be assessed intensively in order to mitigate potential failures and avoid unwanted problems on the road. In order to cover all possible scenarios, virtual simulations are used to complement real-test driving and aid in the validation process. This paper focuses on the validation of the command law during realistic virtual simulations. Its aim is to detect the maximum amount of failures while exploring the input search space of the scenarios. A key industrial restriction, however, is to launch simulations as little as possible in order to minimize computing power needed. Thus, a reduced model based on a random forest model helps in decreasing the number of simulations launched. It accompanies the algorithm in detecting the maximum amount of faulty scenarios everywhere in the search space. The methodology is tested on a tracking vehicle use case, which produces highly effective results.",https://ieeexplore.ieee.org/document/8965211/,2019 IEEE International Conference on Connected Vehicles and Expo (ICCVE),4-8 Nov. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICCS51141.2021.9432186,Pothole and Object Detection for an Autonomous Vehicle Using YOLO,IEEE,Conferences,"Object Detection is an key software and a fundamental task for an autonomous driving system that provides remarkable change in computer vision. İn recent years, company's are planning to launch autonomous vehicle in an full swing that's the most important ascpets for object detection and one of most challenging task for locating specific object from from multiple objects in a specific scenario. The computer vision and machine learning algorithm is the important tool for detecting objects in and around the environment. In this paper, which consists of two parts The first part is implemented on object detection in the surrounding with Yolo (You Only Look Once)Algorithm provides exact classification and position which is configured on newly created datasets for classes of object: a car, a person, a truck, a bus, traffic light, motorcycle, pothole, wetland uses the Convolutional Neural Network and max-polling layer for prediction that improves detecting of small target and these deep learning technique provides a high accuracy for detecting real world. Detecting potholes in Indian road help the autonomous vehicle to move smoothly without getting struck in the potholes. In part two of the proposed method is implemented on Raspberry pi4 a popular embedded computer board explores suitability for the running objects. That solves the real world problems and improves the impact on detecting objects. Knowing pothole and wetland detection for self-driving vehicle is needed badly to solve the road lay problems like: accident, slowing down the transport system these are solved by deep learning.",https://ieeexplore.ieee.org/document/9432186/,2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS),6-8 May 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC42927.2021.9500318,Reinforcement Learning for Autonomous Vehicle Movements in Wireless Sensor Networks,IEEE,Conferences,"In this work we use autonomous vehicles to improve the performance of Wireless Sensor Networks (WSNs). In contrast to other autonomous vehicle applications, WSNs have two metrics for performance evaluation. First, quality of information (QoI) which is used to measure the quality of sensed data (e.g., measurement uncertainties or signal strength). Second, quality of service (QoS) which is used to measure the network&#x2019;s performance for data forwarding (e.g., delay and packet losses). As a use case, we consider wireless acoustic sensor networks, where a group of speakers move inside a room and there are autonomous vehicles installed with microphones for streaming the audio data. We formulate the problem as a Markov decision problem (MDP) and solve it using Deep-Q-Networks (DQN). Additionally, we compare the performance of DQN solution to two different real-world implementations: speakers holding/passing microphones and microphones being preinstalled in fixed positions.We show using simulations that the performance of autonomous vehicles in terms of QoI and QoS is better than the real-world implementation in some scenarios. Moreover, we study the impact of the vehicles speed on the learning process of the DQN solution and show how low speeds degrade the performance. Finally, we compare the DQN solution to a heuristic one and provide theoretical analysis of the performance with respect to dynamic WSNs.",https://ieeexplore.ieee.org/document/9500318/,ICC 2021 - IEEE International Conference on Communications,14-23 June 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEIC54506.2022.9748810,Reinforcement Learning for Autonomous Vehicle using MPC in Highway Situation,IEEE,Conferences,"Path planning for Autonomous Vehicle(AV) is a challenging problem, as the vehicle is required to obey the traffic rules while avoiding the collision with the other vehicles. Model Predictive Control(MPC) is one of the popular approach for proposing a feasible and stable path by reflecting vehicle dynamics in solving objective function and constraining the expected future control input. However, one of the drawbacks with this approach is that the demanded computational power increases proportionally to the number of considered future inputs. This paper presents a path planning algorithm using Reinforcement Learning(RL). RL is similar to MPC in finding the optimal solution that maximizes the reward function which can be seen as intrinsic objective function. In that respect, adequate employment of MPC path in training resulted in improved efficiency and performance. Through the simulations, proposed method showed 98&#x0025; of similarity with path of MPC and reduced computation time by 91.13&#x0025; on average, thus it is qualified for real-time path planning.",https://ieeexplore.ieee.org/document/9748810/,"2022 International Conference on Electronics, Information, and Communication (ICEIC)",6-9 Feb. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISORC.2018.00025,Representative Safety Assessment of Autonomous Vehicle for Public Transportation,IEEE,Conferences,"The implementations and testing in real conditions of Autonomous Vehicles (AV) for private usage show important advances. However, a lack still exists in addressing the particularities of AVs for Public Transportation. Such particularities range from limited safety mechanisms aboard, risky situations associated to particular users and complex self-driving situations up to the limited passengers-vehicle interactions possible. Since, to our knowledge, no comprehensive safety assessment actually exists and the current automotive related standards do not address identified aspects, in this paper, we propose to conduct a minimal but representative safety assessment based upon a local but real autonomous vehicle implementation. To conduct our study, the Hazard Analysis and Risks Assessment introduced in the ISO 26262 standard is taken as a basis. Initial outcomes suggest that critical autonomy aspects, like machine learning of complex operational situations, the metrics for quantitative assessment of autonomy, and potential conflicts between autonomy principles and external safety fences can have critical safety impacts and demand further discussions.",https://ieeexplore.ieee.org/document/8421156/,2018 IEEE 21st International Symposium on Real-Time Distributed Computing (ISORC),29-31 May 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
,Road following for autonomous vehicle navigation using a concurrent neural classifier,IEEE,Conferences,"The paper presents an original approach for visual identification of road direction of an autonomous vehicle using a neural network classifier called Concurrent Self-Organizing Maps (CSOM), representing a winner-takes-all collection of neural modules. We present the experimental results obtained by computer simulation of our model. The path to be identified has been quantized in 5 output directions. For training and testing the neural model, we captured and labeled a road image data set which has been divided in two lots: 30 images for training and other 30 images for test. We have also performed, trained and tested a real time neural path follower based on CSOM model, implemented on a mobile robot (car toy).",https://ieeexplore.ieee.org/document/4699060/,2008 World Automation Congress,28 Sept.-2 Oct. 2008,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766659,Roadmap for development of skills in Artificial Intelligence by means of a Reinforcement Learning model using a DeepRacer autonomous vehicle,IEEE,Conferences,"Using Deepracer, through experimentation and simulation, theoretical concepts can be applied to a practical application of reinforcement learning (RL) in a real-life problem. It was considered as a highly useful tool to develop many direct and transversal competences that students need to work within the field of artificial intelligence (AI). Nowadays the combination of Hardware, Computer Vision methods and Machine Learning (ML) algorithms for the development of controllers for vehicle driving automation have facilitated the development of solutions for this problem. The intention of this work is to show a Roadmap that was formulated to learn AI, ML and RL competencies required to prepare undergraduate students for the industry of this area, following a structured mostly practical learning plan using Deepracer AWS platform and local alternatives for training; and a physical vehicle as primary tools that have made an incredibly compact setup process and reduced complexity in the educational researching field related to learning autonomous vehicles (AV) software development process. The AWS DeepRacer framework includes all needed hardware based on a two front-camera vehicle for stereo vision and a LiDAR sensor, besides it provides a powerful computation computer for high performance in scale autonomous vehicles. The roadmap was executed testing and comparing different RL models over the modalities that Deepracer provides (by comparing both local and AWS console training) documentation of the execution of the generated Roadmap is shown to ensure that should be considered as a stable learning system that could be followed by college programs. Finally, models were tested on a physical track built, and limitations, considerations and improvements for this Roadmap are explained as a contribution for future work.",https://ieeexplore.ieee.org/document/9766659/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TELFOR52709.2021.9653302,Sim-to-Real Autonomous Vehicle Lane Keeping using Vision,IEEE,Conferences,"This paper presents the analysis of the J-Net deep neural network used for autonomous vehicle lane keeping, and its verification in simulated and real conditions. The transition from the simulated world on the personal computer to the real world in laboratory conditions is tackled. In the presented solution, autonomous lane keeping is achieved by analyzing information from visual sensors using a deep neural network. J-Net was developed with an aim to be implemented on an autonomous vehicle platform with limited hardware performance in terms of computing power and memory capacity. Verification of autonomous driving using J-Net was achieved in simulated conditions, using an open-source simulator for autonomous driving, and in real-world conditions. For the verification in real-world conditions, an autonomous driving system was designed and implemented in the Laboratory of Electronics at the School of Electrical Engineering, University of Belgrade.",https://ieeexplore.ieee.org/document/9653302/,2021 29th Telecommunications Forum (TELFOR),23-24 Nov. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636018,Smart Pointers and Shared Memory Synchronisation for Efficient Inter-process Communication in ROS on an Autonomous Vehicle,IEEE,Conferences,"Despite the stringent requirements of a real-time system, the reliance of the Robot Operating System (ROS) on the loopback network interface imposes a considerable overhead on the transport of high bandwidth data, while the nodelet package, which is an efficient mechanism for intra-process communication, does not address the problem of efficient local inter-process communication (IPC). To remedy this, we propose a novel integration into ROS of smart pointers and synchronisation primitives stored in shared memory. These obey the same semantics and, more importantly, exhibit the same performance as their C++ standard library counterparts, making them preferable to other local IPC mechanisms. We present a series of benchmarks for our mechanism - which we call LOT (Low Overhead Transport) - and use them to assess its performance on realistic data loads based on Five’s Autonomous Vehicle (AV) system, and extend our analysis to the case where multiple ROS nodes are running in Docker containers. We find that our mechanism performs up to two orders of magnitude better than the standard IPC via local loopback. Finally, we apply industry-standard profiling techniques to explore the hotspots of code running in both user and kernel space, comparing our implementation against alternatives.",https://ieeexplore.ieee.org/document/9636018/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACMI53878.2021.9528185,Speed Bump &amp; Pothole Detection with Single Shot MultiBox Detector Algorithm &amp; Speed Control for Autonomous Vehicle,IEEE,Conferences,"The development of self-driving cars has always been an extensive research field for the automobile industry. To make a capable self-driving car, many challenges need to be resolved. Detection of the road condition is one of them. This paper focuses on a particular part-detection of speed bumps and potholes using a camera and analyzing the video feed with the help of artificial intelligence. To solve this problem a popular and lightweight algorithm, SSD (Single Shot Multibox Detector) is used. This is an optimal choice because of being lightweight and also accurate enough to run on mobile devices and to use in real-life situations. For detecting speed bumps and potholes, a dataset has been created based on the road structure of Bangladesh as the main priority of this system is to work on the local environment. Raspberry Pi has been used as the main processing unit because of being small but powerful. A warning system has been implemented so that it can warn the onboard driver about the upcoming pothole or speed bump. This system can also send a signal to the speed controller unit of the car to reduce the speed on detection to avoid accidents or damages to the car. The speed control unit is a microcontroller-based system that uses an ATmega328 microcontroller and L298 motor driver. This paper summarizes the combination of an artificial intelligence-based detection system injunction with a microcontroller-based speed control system in a cost-effective way that can be used in building self-driving cars.",https://ieeexplore.ieee.org/document/9528185/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IoTaIS53735.2021.9628587,System configuration of Human-in-the-loop Simulation for Level 3 Autonomous Vehicle using IPG CarMaker,IEEE,Conferences,"The increasingly automated vehicles (AV) have increased the complexity of the testing methods and number of driven miles required to demonstrate the vehicle system’s reliability. Most modern autonomous driving systems also used deep neural networks which requires a large amount of data to develop. Physical driving alone to collect driving data and test system’s safety is no longer suitable for development of AV as this is costly, time consuming and could harm the road users if the safety system failed. This paper proposed human-in-the-loop simulation testing for evaluation of an autonomous vehicle using a 3D virtual vehicle driving platform that can be used for safety assessment of autonomous vehicle. The aim of this study is to establish human-computer interaction platform that can be used as safety testing for Level 3 autonomous vehicle whereby an emergency takeover is required during critical driving conditions. The proposed platform make use of IPG CarMaker to provide 3D virtual environment with accurate vehicle dynamics model, sensor model and environment model. We are able to interface the IPG CarMaker with Simulink and successfully developed a Simulink model that can interface a steering and pedal driving hardware with the virtual vehicle in the simulation. We can also collect driving data and simulation data from the IPG CarMaker as well as accessing the variable in the IPG CarMaker in real-time using Python. The recorded data can be used to train and fine-tune autonomous system based on machine learning.",https://ieeexplore.ieee.org/document/9628587/,2021 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS),23-24 Nov. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASCON51689.2021.9563527,Visual Perception Stack for Autonomous Vehicle Using Semantic Segmentation and Object Detection,IEEE,Conferences,"An autonomous vehicle requires a reliable system for high vehicle precision and relative estimation of its state for the safety of humans during the autonomous movement of the vehicle in an environment dominated by human drivers. Such systems have a complex environment involving multiple sensors (e.g. Vision modules, Global Navigation Satellite System (GNSS), LIDAR, RADAR). Through this paper, environment perception stack for self-driving cars is proposed to improve the intelligence for decision making and improve the safety measures. Semantic image segmentation, based on Fully convolutional Network architecture is implemented and the output received from the model is then used for implementing 3D space estimation and lane estimation. Considering the real-time cooperation required between the autonomous vehicles and other vehicles in the frame, a 2D object detector is implemented on the stack to detect different classes of objects and their relative distances are calculated. The proposed system is then implemented on the CARLA simulation software and generated outcomes are further discussed in the paper.",https://ieeexplore.ieee.org/document/9563527/,2021 IEEE Madras Section Conference (MASCON),27-28 Aug. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ECC54610.2021.9655042,Weights-varying MPC for Autonomous Vehicle Guidance: a Deep Reinforcement Learning Approach,IEEE,Conferences,"Model Predictive Control (MPC) can achieve excellent results for complex control tasks like path-following of autonomous vehicles. However, its performance depends on the right choice of a cost function for its internal optimization problem. Optimizing the cost function to different objectives is challenging and time-consuming. In this paper, we propose to automatically learn context-dependent optimal weights for the cost function with Deep Reinforcement Learning and to adapt the weights online. We show that our approach outperforms the results of a human expert.",https://ieeexplore.ieee.org/document/9655042/,2021 European Control Conference (ECC),29 June-2 July 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCSII.2020.3011367,An Extensive Soft Error Reliability Analysis of a Real Autonomous Vehicle Software Stack,IEEE,Journals,"Automotive systems are integrating artificial intelligence and complex software stacks aiming to interpret the real world, make decisions, and perform actions without human input. The occurrence of soft errors in such systems can lead to wrong decisions, which might ultimately incur in life losses. This brief focuses on the soft error susceptibility assessment of a real automotive application running on top of unmodified Linux kernels, and considering two commercially available processors, and three cross-compilers. Results collected from more than 29 thousand simulation hours show that the occurrence of faults in critical functions may cause 2.16× more failures on the system.",https://ieeexplore.ieee.org/document/9146326/,IEEE Transactions on Circuits and Systems II: Express Briefs,Jan. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3125620,Autonomous Vehicle Evaluation: A Comprehensive Survey on Modeling and Simulation Approaches,IEEE,Journals,"In recent years, autonomous vehicles (AVs), which observe the driving environment and lead a few or all of the driving tasks, have garnered tremendous success. The field of AVs has been rapidly developing and has found many applications. As a safety requirement established by policymakers, these vehicles must be evaluated before their deployment. The evaluation process for AVs is challenging because crashes are rare events, and AVs can escape passing predefined test scenarios. Therefore, capturing crashes and creating real test scenarios should be considered in order to develop an evaluation approach that represents real-world scenarios. One evaluation approach is based on the naturalistic field operational test (N-FOT), in which prototype AVs are driven on roads by volunteers or test engineers. Unfortunately, this approach is time-consuming and costly because thousands of miles need to be driven to experience a police-reported collision and nearly millions of miles for a fatal crash. Another approach is the accelerated evaluation method. The core idea of the accelerated evaluation approach is to modify the statistics of naturalistic driving so that safety-critical events are emphasized. This paper presents a brief survey of the advances that have occurred in the area of the evaluation of partially or fully autonomous vehicles, starting with naturalistic field operational tests (N-FOTs). The review covers the test matrix evaluation, worst-case scenario evaluation (WCSE), Monte Carlo simulations, and accelerated evaluation (AE). We also present all the simulation-based and agent-based modeling approaches that do not follow any evaluation protocol listed above. This study provides a scientific analysis of each evaluation techniques, focusing on their advantages/disadvantages, inherent restrictions, practicability, and optimality. The results reveal that the accelerated evaluation approach outperforms naturalistic field operational tests (N-FOTs), test matrix evaluation, worst-case scenario evaluation (WCSE), and Monte Carlo simulation methods in some of the car-following and lane-change studies when using specific models. Moreover, the agent-based model and augmented and virtual reality approaches show promising results in AV evaluation. Furthermore, integrating machine and deep learning into the available AV evaluation methods can improve their performance and generate encouraging outcomes.",https://ieeexplore.ieee.org/document/9605690/,IEEE Access,2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2022.3141044,Spatiotemporal Scene-Graph Embedding for Autonomous Vehicle Collision Prediction,IEEE,Journals,"In autonomous vehicles (AVs), early warning systems rely on collision prediction to ensure occupant safety. However, state-of-the-art methods using deep convolutional networks either fail at modeling collisions or are too expensive/slow, making them less suitable for deployment on AV edge hardware. To address these limitations, we propose SG2VEC, a spatiotemporal <i>scene-graph</i> embedding methodology that uses the graph neural network (GNN) and long short-term memory (LSTM) layers to predict future collisions via visual scene perception. We demonstrate that SG2VEC predicts collisions 8.11&#x0025; more accurately and 39.07&#x0025; earlier than the state-of-the-art method on synthesized data sets, and 29.47&#x0025; more accurately on a challenging real-world collision data set. We also show that SG2VEC is better than the state of the art at transferring knowledge from synthetic data sets to real-world driving data sets. Finally, we demonstrate that SG2VEC performs inference <inline-formula> <tex-math notation=""LaTeX"">$9.3\times $ </tex-math></inline-formula> faster with an 88.0&#x0025; smaller model, 32.4&#x0025; less power, and 92.8&#x0025; less energy than the state-of-the-art method on the industry-standard Nvidia DRIVE PX 2 platform, making it more suitable for implementation on the edge.",https://ieeexplore.ieee.org/document/9672160/,IEEE Internet of Things Journal,"15 June15, 2022",ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2982963,Trajectory Tracking Control Algorithm for Autonomous Vehicle Considering Cornering Characteristics,IEEE,Journals,"Trajectory tracking control is a key technology in the research and development of autonomous vehicles. With the aim of addressing problems such as low control accuracy and poor real-time performance, which can occur easily when an autonomous vehicle avoids obstacles, this research focuses on the trajectory tracking control algorithm for autonomous vehicle considering cornering characteristics. First, the vehicle dynamics model and tire model are established through appropriate simplification. Then, based on the basic principle of model predictive control, a linear time-varying model predictive controller (LTV MPC) that considers the cornering characteristics is designed and optimized. Finally, using CarSim and MATLAB/Simulink software, a joint simulation model is established and the trajectory tracking performance of the controlled vehicle under different vehicle speeds and road adhesion conditions are tested through simulation experiments in combination with the double-shift line reference trajectory. The simulation results show the LTV MPC controller that considers cornering characteristics has good self-adaptability under complicated and severe working conditions, and no cases, such as car sideslip or track departure, were observed. Compared with other controllers and algorithms, the designed trajectory tracking controller has remarkable comprehensive performance, exhibits superior robustness and anti-interference ability, and significant improvements in the trajectory tracking control accuracy and real-time performance. The proposed control algorithm is of great importance in improving the tracking stability and driving safety of autonomous vehicles under complex extreme conditions and conducive to the further development and improvement of the technological level of intelligent vehicle driving assistance.",https://ieeexplore.ieee.org/document/9045954/,IEEE Access,2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV51971.2022.9827054,A Spatio-Temporal Multilayer Perceptron for Gesture Recognition,IEEE,Conferences,"Gesture recognition is essential for the interaction of autonomous vehicles with humans. While the current approaches focus on combining several modalities like image features, keypoints and bone vectors, we present neural network architecture that delivers state-of-the-art results only with body skeleton input data. We propose the spatio-temporal multilayer perceptron for gesture recognition in the context of autonomous vehicles. Given 3D body poses over time, we define temporal and spatial mixing operations to extract features in both domains. Additionally, the importance of each time step is re-weighted with Squeeze-and-Excitation layers. An extensive evaluation of the TCG and Drive&#x0026; Act datasets is provided to showcase the promising performance of our approach. Furthermore, we deploy our model to our autonomous vehicle to show its real-time capability and stable execution.",https://ieeexplore.ieee.org/document/9827054/,2022 IEEE Intelligent Vehicles Symposium (IV),4-9 June 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UUST.1989.754727,A Testbed Processor for Embedded Multi-Computing,IEEE,Conferences,"A 16-node array of transputers is being installed in an undersea electronics bottle. A passive backplane IBM-AT compatible processor was previously configured and is the host for the array. The Experimental Autonomous Vehicle - West (EAVE-West) and Free Swimming Mine Neutralization Vehicle (FSMNV) will both use this new processor. The array will provide extended capability for future versions of these systems and their follow-on efforts. Also, the testbed processor is expected to provide valuable insights concerning undersea application of embeddable multi-computing.",https://ieeexplore.ieee.org/document/754727/,"Proceedings of the 6th International Symposium on Unmanned Untethered Submersible Technology,",12-14 June 1989,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.649083,A mobile robot for service use: behaviour simulation system and intelligent control,IEEE,Conferences,"The structure of hardware and software of AI control system of a mobile robot for service use are described. Hardware of the mobile robot described include an autonomous wheel vehicle and a five degree of freedom manipulator. The software of the AI control system is based on soft computing including fuzzy control rules, fuzzy neural network and genetic algorithms. The intelligent control of cooperative motion between the autonomous vehicle and manipulator realises flexible operations such as navigation of a mobile robot in presence of static and dynamic obstacles, processes of opening door in rooms and pushing buttons of an elevator. New hierarchical structure of the AI control system includes direct human-robot communication line based on natural language and cognitive graphics, and a generator of virtual reality for simulation of artificial life conditions for the mobile service robot. Simulation and experimental results of navigation and technical operations with the manipulator mobile service robot used in office building are described.",https://ieeexplore.ieee.org/document/649083/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVES.2016.7548165,A new hopfield-type neural network approach to multi-goal vehicle navigation in unknown environments,IEEE,Conferences,"A Hopfield-type neural networks (HNN) algorithm associated with histogram navigation method is proposed in this paper for real-time map building and path planning for multiple goals applications. In real world applications such as rescue robots, service robots, mining mobile robots, and mine searching robots, etc., an autonomous vehicle needs to reach multiple goals with a shortest path that, in this paper, is capable of being implemented by a HNN method with minimized overall distance. Once a global trajectory is planned, a foraging-enabled trail is created to guide the vehicle to the multiple goals. A histogram-based local navigation algorithm is employed to plan a collision-free path along the trail planned by the global path planner. A re-planning-based algorithm aims to generate trajectory while an autonomous vehicle explores through a terrain with map building in unknown environments. In this paper, simulation and experimental results demonstrate that the real-time concurrent mapping and multi-goal navigation of an autonomous vehicle is successfully performed under unknown environments.",https://ieeexplore.ieee.org/document/7548165/,2016 IEEE International Conference on Vehicular Electronics and Safety (ICVES),10-12 July 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISI49825.2020.9280513,A virtual simulation environment using deep learning for autonomous vehicles obstacle avoidance,IEEE,Conferences,"Autonomous vehicles which are capable of operating independently will be commercially available in the near future. Autonomous driving systems are becoming more complicated and must be successfully checked before implementation. Within this framework, falls our research work. The key purpose of this paper is to implement a simulation environment for autonomous vehicles. We first created this environment which is a novel high fidelity driving simulator that can connect arbitrary interfaces, build simulated worlds consisting of scenarios and incidents experienced by drivers in real-world driving, and incorporate fully autonomous driving. The simulator makes possible to clone the behavior of human driver face as well as some complex situations such as obstacle avoidance maneuvers. The work consists in creating a virtual simulation environment to collect training data used to train vehicles on how to steer themselves. The simulator is, thus, like a video game of car racing. Indeed we used the scenes to make some driving experiences. After collecting the training data, we chose to use deep learning explicitly Convolutional Neural Networks to create a model for autonomous vehicles that avoid obstacles. Clearly, the true challenge for an autonomous vehicle is to navigate without the possibility of collision. This simulator is invested to assess the performance of an autonomous vehicle and to analyze its self-driving activities. In this method, the suggested solution proves to be feasible, efficient and reliable for autonomous vehicle simulation research.",https://ieeexplore.ieee.org/document/9280513/,2020 IEEE International Conference on Intelligence and Security Informatics (ISI),9-10 Nov. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEEIC/ICPSEurope49358.2020.9160705,ASDVC - A Self-Driving Vehicle Controller using Unsupervised Machine Learning,IEEE,Conferences,"ASDVC is a self-driving vehicle controller that uses unsupervised machine learning methods, namely clustering based k-means, hierarchical, Gaussian Matrix Model and self-organizing mapping to optimize the path the vehicle follows from the source to destination. The real-time optimal selection of the unsupervised machine learning based motion control algorithm could provide fast response times of under one microsecond during the lateral, longitudinal and angular motion control of the autonomous vehicle. However, it is shown that a simple selection of one of the machine learning methods may not guarantee the optimality of the results. The successful implementation of ASDVC controller in self-driving vehicles could have a significant contribution towards making mobility more reliable and sustainable for the future vehicular transportation systems.",https://ieeexplore.ieee.org/document/9160705/,2020 IEEE International Conference on Environment and Electrical Engineering and 2020 IEEE Industrial and Commercial Power Systems Europe (EEEIC / I&CPS Europe),9-12 June 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197024,Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving,IEEE,Conferences,In the autonomous driving area synthetic data is crucial for cover specific traffic scenarios which autonomous vehicle must handle. This data commonly introduces domain gap between synthetic and real domains. In this paper we deploy data augmentation to generate custom traffic scenarios with VRUs in order to improve pedestrian recognition. We provide a pipeline for augmentation of the Cityscapes dataset with virtual pedestrians. In order to improve augmentation realism of the pipeline we reveal a novel generative network architecture for adversarial learning of the data-set lighting conditions. We also evaluate our approach on the tasks of semantic and instance segmentation.,https://ieeexplore.ieee.org/document/9197024/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967743,Agent Prioritization for Autonomous Navigation,IEEE,Conferences,"In autonomous navigation, a planning system reasons about other agents to plan a safe and plausible trajectory. Before planning starts, agents are typically processed with computationally intensive models for recognition, tracking, motion estimation and prediction. With limited computational resources and a large number of agents to process in real time, it becomes important to efficiently rank agents according to their impact on the decision making process. This allows spending more time processing the most important agents. We propose a system to rank agents around an autonomous vehicle (AV) in real time. We automatically generate a ranking data set by running the planner in simulation on real-world logged data, where we can afford to run more accurate and expensive models on all the agents. The causes of various planner actions are logged and used for assigning ground truth importance scores. The generated data set can be used to learn ranking models. In particular, we show the utility of combining learned features, via a convolutional neural network, with engineered features designed to capture domain knowledge. We show the benefits of various design choices experimentally. When tested on real AVs, our system demonstrates the capability of understanding complex driving situations.",https://ieeexplore.ieee.org/document/8967743/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISEC52395.2021.9763986,An Autonomous Driving Simulation Platform as a Virtual HSAVC Competition Environment,IEEE,Conferences,"At ISEC 2018, Professor Marc E. Herniter presented the High School Autonomous Vehicle Competition (HSAVC), which introduces autonomous driving to high school students. The competition promotes STEM education by challenging participants to use MATLAB to create a vision-based track detection algorithm and Simulink to build a motor controller model. Following the COVID-19 global pandemic, many in-person STEM competitions were canceled, including HSAVC. The goal of the Autonomous Driving Simulation Platform is to replicate the physical conditions of HSAVC using simulation to allow students to continue the activity virtually. Using MATLAB, the Simulation Platform creates a real-time virtual environment for students to test their HSAVC track detection algorithms and motor controller models. The Simulation Platform consists of two MATLAB apps: a Track Generator and a Driving Simulator. The Track Generator application can create fixed tracks based on user inputs or randomized tracks based on user-defined lengths. The Track Generator utilizes a growth and mutation algorithm to create a track with three distinct track sections: straight, left curve, and right curve. The Track Generator&#x2019;s randomized track replicates the HSAVC&#x2019;s physical track, and the Driving Simulator replicates the HSAVC&#x2019;s 1:18 scale autonomous vehicle equipped with a linescan camera, two drive motors, and a servo motor with a vehicle model and a camera model. The Track Generator and Driving Simulator have been successfully designed and implemented with MATLAB App Designer. Users can create a track and test their algorithms and models through an intuitive interface, making it an effective tool for STEM education in any classroom. The Autonomous Driving Simulation Platform holds potential as a solution to continue the HSAVC during the pandemic and can increase student engagement in the HSAVC from high schools around the world like Amazon Web Services DeepRacer. Another benefit of the Simulation Platform is convenient and controlled virtual algorithm testing, which allows for repetitive experimentation to be simulated without risk of damaged materials. The simulation platform has broad potential as an educational tool, such as complementing high school robotics curriculums to teach motor control algorithms and training reinforced learning racing models. The successful virtual adaptation of HSAVC demonstrates how simulation can provide many educational benefits when borrowing the framework of STEM competition.",https://ieeexplore.ieee.org/document/9763986/,2021 IEEE Integrated STEM Education Conference (ISEC),13-13 March 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1986.1087518,Architecture and early experience with planning for the ALV,IEEE,Conferences,"This paper describes the software architecture and the initial algorithms that have proved to be effective for a real time robot planning system. The architecture is designed to incorporate planning technology from research on artificial intelligence while at the same time supporting the high performance decision making needed to control a fast-moving autonomous vehicle. The symbolic representation of the vehicle's plan is a key element in this architecture. Our initial algorithms use an especially efficient version of dynamic programming to find the best routes. The route is then translated into a symbolic plan. Replanning happens at several levels with the cost of replanning proportionate to the scope of the changes. This software is currently running in an environment which simulates the vehicle and perception systems, but it will be transferred to the DARPA Autonomous Land Vehicle built by Martin Marietta Denver Aerospace [Lowrie 86].",https://ieeexplore.ieee.org/document/1087518/,Proceedings. 1986 IEEE International Conference on Robotics and Automation,7-10 April 1986,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTCSpring.2019.8746507,Autonomous Driving without a Burden: View from Outside with Elevated LiDAR,IEEE,Conferences,"The current autonomous driving architecture places a heavy burden in signal processing for the graphics processing units (GPUs) in the car. This directly translates into battery drain and lower energy efficiency, crucial factors in electric vehicles. This is due to the high bit rate of the captured video and other sensing inputs, mainly due to Light Detection and Ranging (LiDAR) sensor at the top of the car which is an essential feature in autonomous vehicles. LiDAR is needed to obtain a high precision map for the vehicle AI to make relevant decisions. However, this is still a quite restricted view from the car. This is the same even in the case of cars without a LiDAR such as Tesla. The existing LiDARs and the cameras have limited horizontal and vertical fields of visions. In all cases it can be argued that precision is lower, given the smaller map generated. This also results in the accumulation of a large amount of data in the order of several TBs in a day, the storage of which becomes challenging. If we are to reduce the effort for the processing units inside the car, we need to uplink the data to edge or an appropriately placed cloud. However, the required data rates in the order of several Gbps are difficult to be met even with the advent of 5G. Therefore, we propose to have a coordinated set of LiDAR's outside at an elevation which can provide an integrated view with a much larger field of vision (FoV) to a centralized decision making body which then sends the required control actions to the vehicles with a lower bit rate in the downlink and with the required latency. The calculations we have based on industry standard equipment from several manufacturers show that this is not just a concept but a feasible system which can be implemented.The proposed system can play a supportive role with existing autonomous vehicle architecture and it is easily applicable in an urban area.",https://ieeexplore.ieee.org/document/8746507/,2019 IEEE 89th Vehicular Technology Conference (VTC2019-Spring),28 April-1 May 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ZINC.2019.8769392,Classification of Objects Detected by the Camera based on Convolutional Neural Network,IEEE,Conferences,"Nowadays, we are trying to achieve as much vehicle autonomy as possible by developing Advanced Driver-Assistance Systems (ADAS). For such a system to make decisions, it should have insight into the environment of the vehicle, e.g. the objects surrounding the vehicle. During forward driving, the information about the objects in front of the vehicle is usually obtained by a front view in-vehicle camera. This paper describes the image classification method of the objects in the front of the vehicle based on deep convolutional neural networks (CNN). Such CNN is supposed to be implemented in embedded system of an autonomous vehicle and the inference should satisfy real-time constraints. This means that the CNN should be structured to have fast inference by reducing the number of operations as much as possible, but still having satisfying accuracy. This can be achieved by reducing the number of parameters which also means that the resulting network has lower memory requirements. This paper describes the process of realizing such a network, from image dataset development up to the CNN structuring and training. The proposed CNN is compared to the state-of-the-art deep neural network in terms of classification accuracy, inference speed and memory requirements.",https://ieeexplore.ieee.org/document/8769392/,2019 Zooming Innovation in Consumer Technologies Conference (ZINC),29-30 May 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC48978.2021.9564566,Continual Unsupervised Domain Adaptation for Semantic Segmentation by Online Frequency Domain Style Transfer,IEEE,Conferences,"When deep neural networks are deployed in a highly automated vehicle for environment perception tasks in an unseen (target) domain that differs from the training (source) domain, the mismatch will result in decreased performance. Domain adaptation methods aim at overcoming this mismatch. Many recently investigated methods for unsupervised domain adaptation train a model using labeled source data and unlabeled target data at the same time. These methods assume that data from the target domain is available during the source domain training, which is not always the case in real applications. In this paper we present a way to perform an online style transfer for continual domain adaptation which improves performance on (multiple) unseen target domains using a given perception model. The approach is based on an image style transfer in the frequency domain and requires neither an adjustment of the given source-trained model parameters to the target domain, nor does it require any considerable amount of memory for storing its frequency domain representation of the source domain style, which is particularly important considering the hardware limitations in an autonomous vehicle.",https://ieeexplore.ieee.org/document/9564566/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2019.00127,Coverage-Guided Fuzzing for Feedforward Neural Networks,IEEE,Conferences,"Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc.",https://ieeexplore.ieee.org/document/8952279/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOSEC51865.2021.9591747,Deep Learning based Object Detection Model for Autonomous Driving Research using CARLA Simulator,IEEE,Conferences,"Autonomous vehicle research has grown exponentially over the years with researchers working on different object detection algorithms to realize safe and competent self-driving systems while legal authorities are simultaneously looking into the ways of mitigating the risks posed by fully autonomous vehicles. These advancements can result in a much safer commuting environment, reduced accidents and also eliminate the necessity for human driving. The creation of data and access to data for autonomous driving research is difficult challenge that research communities are facing. Hence, open source simulators such as the CARLA simulator (CAR Learning to Act) help us train and test models and to gain insights into autonomous driving with ease. This paper proposes the application of object detection algorithm on CARLA simulator to derive useful results for autonomous driving research. Further, the comparison of CARLA simulator with other available simulators, key players in the field of autonomous vehicle technology, state-of-the-art algorithms being used for autonomous driving, real time implementation challenges and future technologies are also discussed.",https://ieeexplore.ieee.org/document/9591747/,2021 2nd International Conference on Smart Electronics and Communication (ICOSEC),7-9 Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC51557.2021.9454613,Deep Learning-Based Automated Vehicle Steering,IEEE,Conferences,"Autonomous Vehicle applications are full of open challenges. Despite the advanced technologies, the lack of robust systems still exists due to the high complexity of the surrounded environments. The automated steering is one of the most complex autonomous driving system's application. Model predictive control is the most common control strategy used to implement the automated steering tasks due to its ability to solve an online quadratic optimization problem in the real-time, in addition to its efficiency in handling the constraints of the system's environments. MPC controller is used to drive the vehicle autonomously along the centerline of the road based on two main factors, the lateral deviation and relative yaw angle. Deep learning technology has been widely used in recent years because of the promising performance achieved in different applications and tasks. In this context, we suggested that the implementation of the Deep Neural Network (DNN) will provide a great improvement and it can be more computationally efficient than solving an online quadratic problem (QP), that will naturally lead to reduce the time, the complexity, and the computational loads of implementations. The main aims of this paper are to design a deep learning-based approach for automated vehicle steering based on the behaviour of the traditional MPC controller. In addition, to study the efficiency of the full replacement of the MPC controller by the suggested DNN model. The study is based on performing a comparison between the implementations of both controllers (MPC and DNN model) in terms of the performance and the execution time. The performance indicator is the ability of the controller to drive the decision variables (lateral deviation and yaw angle) to be close to zero in order to drive the vehicle autonomously along the desired path.",https://ieeexplore.ieee.org/document/9454613/,2021 22nd International Carpathian Control Conference (ICCC),31 May-1 June 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSIT48917.2020.9214125,Design & implementation of real time autonomous car by using image processing & IoT,IEEE,Conferences,"Because of the inaccessibility of Vehicle-to-Infrastructure correspondence in the present delivering frameworks, (TLD), Traffic Sign Detection and path identification are as yet thought to be a significant task in self-governing vehicles and Driver Assistance Systems (DAS) or Self Driving Car. For progressively exact outcome, businesses are moving to profound Neural Network Models Like Convolutional Neural Network (CNN) as opposed to Traditional models like HOG and so forth. Profound neural Network can remove and take in increasingly unadulterated highlights from the Raw RGB picture got from nature. In any case, profound neural systems like CNN have a highly complex calculation. This paper proposes an Autonomous vehicle or robot that can identify the diverse article in condition and group them utilizing CNN model and through this information can take some continuous choice which can be utilized in the Self Driving vehicle or Autonomous Car or Driving Assistant System (DAS).",https://ieeexplore.ieee.org/document/9214125/,2020 Third International Conference on Smart Systems and Inventive Technology (ICSSIT),20-22 Aug. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UUST.1987.1158593,Eave-west: A testbed for plan execution,IEEE,Conferences,"The Experimental Autonomous Vehicle - West (EAVE-West) submersible testbed has been configured for demonstrating a distributable software architecture for Autonomous Undersea Vehicle (AUV) plan execution. Instead of using a machine planner aboard the AUV, plans are represented and then downloaded to the vehicle. This technique obviates the problems associated with planning and, as a result, the real-time response of the AUV can potentially be improved. A review of the architecture is given and the EAVE-West demonstration system is discussed.",https://ieeexplore.ieee.org/document/1158593/,Proceedings of the 1987 5th International Symposium on Unmanned Untethered Submersible Technology,June 1987,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV48863.2021.9575135,End-to-End Intersection Handling using Multi-Agent Deep Reinforcement Learning,IEEE,Conferences,"Navigating through intersections is one of the main challenging tasks for an autonomous vehicle. However, for the majority of intersections regulated by traffic lights, the problem could be solved by a simple rule-based method in which the autonomous vehicle behavior is closely related to the traffic light states. In this work, we focus on the implementation of a system able to navigate through intersections where only traffic signs are provided. We propose a multi-agent system using a continuous, model-free Deep Reinforcement Learning algorithm used to train a neural network for predicting both the acceleration and the steering angle at each time step. We demonstrate that agents learn both the basic rules needed to handle intersections by understanding the priorities of other learners inside the environment, and to drive safely along their paths. Moreover, a comparison between our system and a rule-based method proves that our model achieves better results especially with dense traffic conditions. Finally, we test our system on real world scenarios using real recorded traffic data, proving that our module is able to generalize both to unseen environments and to different traffic conditions.",https://ieeexplore.ieee.org/document/9575135/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CiSt49399.2021.9357196,End-to-End Neural Network for Vehicle Dynamics Modeling,IEEE,Conferences,"Autonomous vehicles have to meet high safety standards in order to be commercially viable. Before real-world testing of an autonomous vehicle, extensive simulation is required to verify software functionality and to detect unexpected behavior. This incites the need for accurate models to match real system behavior as closely as possible. During driving, planing and control algorithms also need an accurate estimation of the vehicle dynamics in order to handle the vehicle safely. Until now, vehicle dynamics estimation has mostly been performed with physics-based models. Whereas these models allow specific effects to be implemented, accurate models need a variety of parameters. Their identification requires costly resources, e.g., expensive test facilities. Machine learning models enable new approaches to perform these modeling tasks without the necessity of identifying parameters. Neural networks can be trained with recorded vehicle data to represent the vehicle's dynamic behavior. We present a neural network architecture that has advantages over a physics-based model in terms of accuracy. We compare both models to real-world test data from an autonomous racing vehicle, which was recorded on different race tracks with high- and low-grip conditions. The developed neural network architecture is able to replace a single-track model for vehicle dynamics modeling.",https://ieeexplore.ieee.org/document/9357196/,2020 6th IEEE Congress on Information Science and Technology (CiSt),5-12 June 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7966404,Following the leader using a tracking system based on pre-trained deep neural networks,IEEE,Conferences,"In this work, we present a software architecture to solve, at some level, the follow the leader problem. This problem consists of an autonomous vehicle trying to track and follow a leader vehicle. To track the leader position in consecutive camera images, we employed the Generic Object Tracking Using Regression Networks (GOTURN). GOTURN is a pre-trained Deep Neural Network capable of tracking generic objects, without application-specific training or fine-tuning. The proposed software architecture was evaluated using a real autonomous vehicle, in four stretches of a University ring road. In all experiments, the autonomous vehicle was able to follow the leader's path with maximum root mean square error of 0.28m.",https://ieeexplore.ieee.org/document/7966404/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSA54496.2021.00042,Implementation of Autoware Application to real-world Services Based Adaptive Big Data Management System for Autonomous Driving,IEEE,Conferences,"World Connected autonomous driving not only enables transportation but also improves passenger safety, comfort and entertainment. Nowadays, fully autonomous driving is required. Passengers can achieve their goals and turn dead hours of driving into fruitful hours of learning, work, engagement, and relaxation. This must be done in real-time. Our goal in this project is to master connectivity in order to provide V2X (Vehicle-to-Everything) functions for fully autonomous driving. Big data services are changing the way we drive. With autonomous driving, information from various integrated sensors is processed and analyzed in milliseconds. In this case, machine vision in an autonomous vehicle means that all input data is captured by the vehicle's sensors or cameras, and this data is analyzed in real-time during operation. The purpose of our study is to design and implement by expanding the Autoware software stack to enable embedded computer functions. Based on high-performance GPU, from a functional safety perspective, this project focuses on the application of autoware to develop autonomous vehicles by converting window-based sources to ROS-based sources when starting point and point are specified of arrival. Another goal of this study is that the autonomous driving component is software that can be used to analyze autonomous driving with large amounts of data. By connecting to a network, smart cars not only pass information from all their sensors to the cloud but also react immediately to conditions. And the interfaces required at this layer to connect the standalone software defined service platform with the framework for physical management, connectivity, and spatial data in the standalone Oracle Cloud database. This paper proposes a UML diagram including the design of the Deep Learning Process Model Autonomous Driving Based on Big Data and software in it. The software is called embedded software. Model-based testing is a resolution for testing embedded software.",https://ieeexplore.ieee.org/document/9732368/,2021 21st International Conference on Computational Science and Its Applications (ICCSA),13-16 Sept. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMechS.2016.7813486,Intelligent adaptive precrash control for autonmous vehicle agents (CBR Engine & hybrid A path planner),IEEE,Conferences,"PreCrash problem of Intelligent Control of autonomous vehicles robot is a very complex problem, especially vehicle pre-crash scenariws and at points of intersections in real-time environmenta. This Paper presents a novel architecture of Intelligent adaptive control for autonomous vehicle agent that depends on Artificial Intelligence Techniques that applies case-based reasoning techniques, where Parallel CBR Engines are implemented for different scenarios' of PreCrash problem and sub-problems of intersection safety and collision avoidance, in the higher level of the controller and A∗ path planner for path planning and at lower-levels it also uses some features of autonomous vehicle dynamics. Moreover, the planner is enhanced by combination of Case-Based Planner. All modules are presented and discussed. Experimental results are conducted in the framework of Webots autonomous vehicle tool and overall results are good for the CBR Engine for Adaptive control and also for the hybrid Case-Based Planner, A∗ and D∗ motion planner along with conclusion and future work.",https://ieeexplore.ieee.org/document/7813486/,2016 International Conference on Advanced Mechatronic Systems (ICAMechS),30 Nov.-3 Dec. 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVCI54083.2021.9661121,Learning End-to-End Inertial-Wheel Odometry for Vehicle Ego-Motion Estimation,IEEE,Conferences,"We present an end-to-end learning architecture for wheel speeds and inertial fusion for real-time ego-motion estimation. In contrast to the previous work, our method utilizes the neural network to learn the nonlinear vehicle model and reduce the modeling error in motion propagation. The network inputs are the wheel speeds and inertial measurements while the outputs are the 2D pose increments in vehicle frames. To enhance the learning efficiency of the network, we propose a data enhancement strategy and employ a large amount of data for training. A diversity of practical scenario tests have shown that our proposed method outperforms traditional methods in terms of accuracy and robustness, achieving an average translation error of less than 0.4m per 100m, which meets the practical application requirements and the trained model has been deployed as a front-end module for other systems on an autonomous vehicle.",https://ieeexplore.ieee.org/document/9661121/,2021 5th CAA International Conference on Vehicular Control and Intelligence (CVCI),29-31 Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC48978.2021.9564899,Learning a Model for Inferring a Spatial Road Lane Network Graph using Self-Supervision,IEEE,Conferences,"Interconnected road lanes are a central concept for navigating urban roads. Currently, most autonomous vehicles rely on preconstructed lane maps as designing an algorithmic model is difficult. However, the generation and maintenance of such maps is costly and hinders large-scale adoption of autonomous vehicle technology. This paper presents the first self-supervised learning method to train a model to infer a spatially grounded lane-level road network graph based on a dense segmented representation of the road scene generated from onboard sensors. A formal road lane network model is presented and proves that any structured road scene can be represented by a directed acyclic graph of at most depth three while retaining the notion of intersection regions, and that this is the most compressed representation. The formal model is implemented by a hybrid neural and search-based model, utilizing a novel barrier function loss formulation for robust learning from partial labels. Experiments are conducted for all common road intersection layouts. Results show that the model can generalize to new road layouts, unlike previous approaches, demonstrating its potential for real-world application as a practical learning-based lane-level map generator.",https://ieeexplore.ieee.org/document/9564899/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2019.00220,Learning to Drive via Apprenticeship Learning and Deep Reinforcement Learning,IEEE,Conferences,"With the implementation of reinforcement learning (RL) algorithms, current state-of-art autonomous vehicle technology have the potential to get closer to full automation. However, most of the applications have been limited to game domains or discrete action space which are far from the real world driving. Moreover, it is very tough to tune the parameters of reward mechanism since the driving styles vary a lot among the different users. For instance, an aggressive driver may prefer driving with high acceleration whereas some conservative drivers prefer a safer driving style. Therefore, we propose an apprenticeship learning in combination with deep reinforcement learning approach that allows the agent to learn the driving and stopping behaviors with continuous actions. We use gradient inverse reinforcement learning (GIRL) algorithm to recover the unknown reward function and employ REINFORCE as well as Deep Deterministic Policy Gradient algorithm (DDPG) to learn the optimal policy. The performance of our method is evaluated in simulation-based scenario and the results demonstrate that the agent performs human like driving and even better in some aspects after training.",https://ieeexplore.ieee.org/document/8995417/,2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI),4-6 Nov. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/AEITAUTOMOTIVE50086.2020.9307387,LiDAR point-cloud processing based on projection methods: a comparison,IEEE,Conferences,"An accurate and rapid-response perception system is fundamental for autonomous vehicles to operate safely. 3D object detection methods handle point clouds given by LiDAR sensors to provide accurate depth and position information for each detection, together with its dimensions and classification. The information is then used to track vehicles and other obstacles in the surroundings of the autonomous vehicle, and also to feed control units that guarantee collision avoidance and motion planning. Nowadays, object detection systems can be divided into two main categories. The first ones are the geometric based, which retrieve the obstacles using geometric and morphological operations on the 3D points. The seconds are the deep learning-based, which process the 3D points, or an elaboration of the 3D point-cloud, with deep learning techniques to retrieve a set of obstacles. This paper presents a comparison between those two approaches, presenting one implementation of each class on a real autonomous vehicle. Accuracy of the estimates of the algorithms has been evaluated with experimental tests carried in the Monza ENI circuit. The positions of the ego vehicle and the obstacle are given by GPS sensors with real time kinematic (RTK) correction, which guarantees an accurate ground truth for the comparison. Both algorithms have been implemented on ROS and run on a consumer laptop.",https://ieeexplore.ieee.org/document/9307387/,2020 AEIT International Conference of Electrical and Electronic Technologies for Automotive (AEIT AUTOMOTIVE),18-20 Nov. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV48863.2021.9575925,LiDAR-based Object Detection Failure Tolerated Autonomous Driving Planning System,IEEE,Conferences,"A typical autonomous driving system usually relies on the detected objects from an environment perception module. Current research still cannot guarantee a perfect perception, and failure detections may cause collisions, leading to untrustworthy autonomous vehicles. This work proposes a trajectory planner to tolerate the detection failure of the LiDAR sensors. This method will plan the path relying on the detected objects as well as the raw sensor data. The overlapping and contradiction of both perception routes will be carefully addressed for safe and efficient driving. The object detector in this work uses a deep learning-based method, i.e., CNN-Segmentation neural network. The designed trajectory planner has multi-layers to handle the multi-resolution environment formed by different perception routes. The final system will dynamically adjust its attention to the detected objects or the point cloud to avoid collision due to detection failures. This method is implemented on a real autonomous vehicle to drive in an open urban area. The results show that when the autonomous vehicle fails to detect a surrounding object, e.g., vehicles or some undefined objects, the autonomous vehicles still can plan an efficient and safe trajectory. In the meantime, when the perception system works well, the A V will not be affected by the point clouds. This technology can make the autonomous vehicle trustworthy even with the black-box neural networks. The codes are open-source with our autonomous driving platform to help other researchers for A V development.",https://ieeexplore.ieee.org/document/9575925/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DELCON54057.2022.9753403,Machine Learning Based VoxelNet and LUNET architectures for Object Detection using LiDAR Cloud Points,IEEE,Conferences,"Autonomous vehicles are the epitome of technological transformation in this decade. Object detection is a primary task in an autonomous vehicle, is an important issue to evaluate to bring autonomous vehicles into reality. This paper examines 3D object detection using VoxelNet architecture and LUNet architecture from LiDAR-based cloud points. The aforementioned architecture addresses the problem of manual feature extraction, high computational costs, and memory constraints. The datasets that were utilized for training, as well as testing, have been taken out of the KITTI vehicle detection benchmarking software. Post-training and testing of the model, the results obtained have been used to evaluate the performance of this implementation. The LUNET fares better in terms of low loss whereas VoxelNet fares better with regard to low computational costs and ease of implementation.",https://ieeexplore.ieee.org/document/9753403/,2022 IEEE Delhi Section Conference (DELCON),11-13 Feb. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IntelliSys.2017.8324372,Machine learning and deep neural network — Artificial intelligence core for lab and real-world test and validation for ADAS and autonomous vehicles: AI for efficient and quality test and validation,IEEE,Conferences,"Autonomous vehicles are now the future of automobile industry. Human drivers can be completely taken out of the loop through the implementation of safe and intelligent autonomous vehicles. Although we can say that HW and SW development continues to play a large role in the automotive industry, test and validation of these systems is a must. The ability to test these vehicles thoroughly and efficiently will ensure their proper and flawless operation. When a large number of people with heterogeneous knowledge and skills try to develop an autonomous vehicle together, it is important to use a sensible engineering process. State of the art techniques for such development include Waterfall, Agile & V-model, where test & validation (T&V) process is an integral part of such a development cycle. This paper will propose a new methodology using machine learning & deep neural network (AI-core) for lab & real-world T&V for ADAS (Advanced driver assistance system) and autonomous vehicles. The methodology will initially connect T&V of individual systems in each level of development and that of complete system efficiently, by using the proposed phase methodology, in which autonomous driving functions are grouped under categories, special T&V processes are carried on simulation as well as in HIL systems. The complete transition towards AI in the field of T&V will be a sequence of steps. Initially the AI-core is fed with available test scenarios, boundary conditions for the test cases and scenarios, and examples, the AI-core will conduct virtual tests on simulation environment using available test scenarios and further generates new test cases and scenarios for efficient and precise tests. These test cases and scenarios are meant to cover all available cases and concentrate on the area where bugs or failures occur. The complete surrounding environment in the simulation is also controlled by the AI-core which means that the system can attain endless/all-possible combinations of the surrounding environment which is necessary. Results of the tests are sorted and stored, critical and important tests are again repeated in the real-world environment using automated cars with other real subsystems to depict the surrounding environment, which are all controlled by the AI-core, and meanwhile the AI-core is always in the loop and learning from each and every executed test case and its results/outcomes. The main goal is to achieve efficient and high quality test and validation of systems for automated driving, which can save precious time in the development process. As a future scope of this methodology, we can step-up to make most parts of test and validation completely autonomous.",https://ieeexplore.ieee.org/document/8324372/,2017 Intelligent Systems Conference (IntelliSys),7-8 Sept. 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2019.8917085,Machine learning method to ensure robust decision-making of AVs,IEEE,Conferences,"Replacing the human driver to perform the Dynamic Driving Task (DDT)[1] will require perception, complex analysis and assessment of traffic situation. The path leading to success the deployment of fully Autonomous Vehicle (AV) depends on the resolution of a lot of challenges. Both the safety and the security aspects of AV constitute the core of regulatory compliance and technical research. The Autonomous Driving System (ADS) should be designed to ensure a safe manoeuvre and a stable behaviour despite the technological limitations, the uncertainties and hazards which characterize the real traffic conditions. In fully Autonomous Driving situation, detecting all relevant objects and agents should be sufficient to generate a warning, however the ADS requires further complex data analysis steps to quantify and improve the safety of decision making. This paper aims to improve the robustness of decision-making in order to mimic human-like decision ability. The approach is based on machine learning to identify the criticality of the dynamic situation and enabling ADS to make appropriate decision and fulfil safe manoeuvre.",https://ieeexplore.ieee.org/document/8917085/,2019 IEEE Intelligent Transportation Systems Conference (ITSC),27-30 Oct. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC45102.2020.9294697,Monte Carlo Tree Search With Reinforcement Learning for Motion Planning,IEEE,Conferences,"Motion planning for an autonomous vehicle is most challenging for scenarios such as large, multi-lane, and unsignalized intersections in the presence of dense traffic. In such situations, the motion planner has to deal with multiple crossing-points to reach an objective in a safe, comfortable, and efficient way. In addition, motion planning challenges include real-time computation and scalability to complex scenes with many objects and different road geometries. In this work, we propose a motion planning system addressing these challenges. We enable real-time applicability of a Monte Carlo Tree Search algorithm with a deep-learning heuristic. We learn a fast evaluation function from accurate, but non real-time models. While using Deep Reinforcement Learning techniques we maintain a clear separation between making predictions and making decisions. We reduce the complexity of the search model and benchmark the proposed agent against multiple methods: rules-based, MCTS, $A^{*}$ search, deep learning, and Model Predictive Control. We show that our agent outperforms these other agents in a variety of challenging scenarios, where we benchmark safety, comfort and efficiency metrics.",https://ieeexplore.ieee.org/document/9294697/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAR.2017.7942721,Object detection on panoramic images based on deep learning,IEEE,Conferences,"Panoramic image can be widely used in many applications, such as virtual reality, visual surveillance and autonomous vehicle, because of its large field of view. However, the inherent distortion for panorama causes object detection to be a challenging task. This paper focuses on the multi-class objects detection in panoramic images using deep learning method. The proposed system uses three fisheye cameras to efficiently create panoramas and build a large dataset. A region based convolutional neutral network (R-CNN) is implemented to train and test on an indoor panoramic image dataset. Experiments show great improvement performance on ten categories of distorted indoor objects with a mean average precision of 68.7%.",https://ieeexplore.ieee.org/document/7942721/,"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)",24-26 April 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAA52185.2022.00010,On Using Real-Time Reachability for the Safety Assurance of Machine Learning Controllers,IEEE,Conferences,"Over the last decade, advances in machine learning and sensing technology have paved the way for the belief that safe, accessible, and convenient autonomous vehicles may be realized in the near future. Despite the prolific competencies of machine learning models for learning the nuances of sensing, actuation, and control, they are notoriously difficult to assure. The challenge here is that some models, such as neural networks, are &#x201C;black box&#x201D; in nature, making verification and validation difficult, and sometimes infeasible. Moreover, these models are often tasked with operating in uncertain and dynamic environments where design time assurance may only be partially transferable. Thus, it is critical to monitor these components at runtime. One approach for providing runtime assurance of systems with unverified components is the simplex architecture, where an unverified component is wrapped with a safety controller and a switching logic designed to prevent dangerous behavior. In this paper, we propose the use of a real-time reachability algorithm for the implementation of such an architecture for the safety assurance of a 1/10 scale open source autonomous vehicle platform known as F1/10. The reachability algorithm (a) provides provable guarantees of safety, and (b) is used to detect potentially unsafe scenarios. In our approach, the need to analyze the underlying controller is abstracted away, instead focusing on the effects of the controller&#x2019;s decisions on the system&#x2019;s future states. We demonstrate the efficacy of our architecture through experiments conducted both in simulation and on an embedded hardware platform.",https://ieeexplore.ieee.org/document/9763623/,2022 IEEE International Conference on Assured Autonomy (ICAA),22-24 March 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2002.1187941,Pattern matching as the nucleus for either autonomous driving or driver assistance systems,IEEE,Conferences,"Concerns autonomous vehicle driving by pattern matching combined with reinforcement learning. In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicle or obstacle but with the strict requirement to self-improve driving behaviour. A camera is used to build quickly an abstract complete description (ACSD) of vehicle's current situation. This combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an approximate nearest neighbour pattern matching algorithm with a O(n log m) characteristic compared to O(n/spl middot/m) for the conventional nearest neighbour calculation. In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.",https://ieeexplore.ieee.org/document/1187941/,"Intelligent Vehicle Symposium, 2002. IEEE",17-21 June 2002,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVCI54083.2021.9661188,Pedestrian Detection with YOLOv5 in Autonomous Driving Scenario,IEEE,Conferences,"Autonomous vehicle, with the attributes that ensuring driving safety and improving traffic efficiency, has been a research hotspot for a long time. In the modular developing pipeline of autonomous vehicles, pedestrian detection based on computer vision is a critical component of perception module. In this paper, we apply the newly proposed network structure YOLOv5 in pedestrian detection problem. After training in PASCAL VOC2012 dataset, the model realizes high detection accuracy and real-time efficiency. At the same time, the model owns competitive generalization ability which achieve high detection accuracy in KITTI dataset. With competitive detection accuracy and real-time efficiency, YOLOv5 have the potential to be deployed on autonomous vehicles.",https://ieeexplore.ieee.org/document/9661188/,2021 5th CAA International Conference on Vehicular Control and Intelligence (CVCI),29-31 Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS45731.2020.9180841,PointNet on FPGA for Real-Time LiDAR Point Cloud Processing,IEEE,Conferences,"LiDAR sensors have been widely used in many autonomous vehicle modalities, such as perception, mapping, and localization. This paper presents an FPGA-based deep learning platform for real-time point cloud processing targeted on autonomous vehicles. The software driver for the Velodyne LiDAR sensor is modified and moved into the on-chip processor system, while the programmable logic is designed as a customized hardware accelerator. As the state-of-art deep learning algorithm for point cloud processing, PointNet is successfully implemented on the proposed FPGA platform. Targeted on a Xilinx Zynq UltraScale+ MPSoC ZCU104 development board, the FPGA implementations of PointNet achieve the computing performance of 182.1 GOPS and 280.0 GOPS for classification and segmentation respectively. The proposed design can support an input up to 4096 points per frame. The processing time is 19.8 ms for classification and 34.6 ms for segmentation, which meets the real-time requirement for most of the existing LiDAR sensors.",https://ieeexplore.ieee.org/document/9180841/,2020 IEEE International Symposium on Circuits and Systems (ISCAS),12-14 Oct 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE43902.2021.00046,Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis,IEEE,Conferences,"Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA.",https://ieeexplore.ieee.org/document/9402064/,2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE),22-30 May 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561747,Pylot: A Modular Platform for Exploring Latency-Accuracy Tradeoffs in Autonomous Vehicles,IEEE,Conferences,"We present Pylot, a platform for autonomous vehicle (AV) research and development, built with the goal to allow researchers to study the effects of the latency and accuracy of their models and algorithms on the end-to-end driving behavior of an AV. This is achieved through a modular structure enabled by our high-performance dataflow system that represents AV software pipeline components (object detectors, motion planners, etc.) as a dataflow graph of operators which communicate on data streams using timestamped messages. Pylot readily interfaces with popular AV simulators like CARLA, and is easily deployable to real-world vehicles with minimal code changes.To reduce the burden of developing an entire pipeline for evaluating a single component, Pylot provides several state-of-the-art reference implementations for the various components of an AV pipeline. Using these reference implementations, a Pylot-based AV pipeline is able to drive a real vehicle, and attains a high score on the CARLA Autonomous Driving Challenge. We also present several case studies enabled by Pylot, including evidence of a need for context-dependent components, and per-component time allocation. Pylot is open source, with the code available at https://github.com/erdos-project/pylot.",https://ieeexplore.ieee.org/document/9561747/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NAECON46414.2019.9057988,Real-Time 3-D Segmentation on An Autonomous Embedded System: using Point Cloud and Camera,IEEE,Conferences,"Present day autonomous vehicle relies on several sensor technologies for it's autonomous functionality. The sensors based on their type and mounted-location on the vehicle, can be categorized as: line of sight and non-line of sight sensors and are responsible for the different level of autonomy. These line of sight sensors are used for the execution of actions related to localization, object detection and the complete environment understanding. The surrounding or environment understanding for an autonomous vehicle can be achieved by segmentation. Several traditional and deep learning related techniques providing semantic segmentation for an input from camera is already available, however with the advancement in the computing processor, the progression is on developing the deep learning application replacing traditional methods. This paper presents an approach to combine the input of camera and lidar for semantic segmentation purpose. The proposed model for outdoor scene segmentation is based on the frustum pointnet, and ResNet which utilizes the 3d point cloud and camera input for the 3d bounding box prediction across the moving and non-moving object and thus finally recognizing and understanding the scenario at the point-cloud or pixel level. For real time application the model is deployed on the RTMaps framework with Bluebox (an embedded platform for autonomous vehicle). The proposed architecture is trained with the CITYScpaes and the KITTI dataset.",https://ieeexplore.ieee.org/document/9057988/,2019 IEEE National Aerospace and Electronics Conference (NAECON),15-19 July 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EIT48999.2020.9208309,Real-Time Traffic Sign Detection and Classification Using Machine Learning and Optical Character Recognition,IEEE,Conferences,"Autonomous vehicle development is currently progressing at a very fast pace and traffic sign detection and classification has an important role in it. This paper analyzes a few possible approaches of doing this task in real-time using a portable system. The final solution uses a convolutional neural network for detection and classification combined with a custom optical character recognition algorithm for speed limit signs. The training and testing dataset is based on a combination of the Belgian Dataset, German Dataset, as well as images taken while driving in Illinois, United States.",https://ieeexplore.ieee.org/document/9208309/,2020 IEEE International Conference on Electro Information Technology (EIT),31 July-1 Aug. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2018.8500455,"Real-time Detection, Tracking, and Classification of Moving and Stationary Objects using Multiple Fisheye Images",IEEE,Conferences,"The ability to detect pedestrians and other moving objects is crucial for an autonomous vehicle. This must be done in real-time with minimum system overhead. This paper discusses the implementationof a surround view system to identify moving as well as static objects that are close to the ego vehicle. The algorithm works on 4 views captured by fisheye cameras which are merged into a single frame. The moving object detection and tracking solution uses minimal system overhead to isolate regions of interest (ROIs) containing moving objects. These ROIs are then analyzed using a deep neural network (DNN) to categorize the moving object. With deployment and testing on a real car in urban environments, we have demonstrated the practical feasibility of the solution.11The video demos of our algorithm have been uploaded to Youtube: https://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs",https://ieeexplore.ieee.org/document/8500455/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIPR.2016.8010547,Real-time detection and classification of traffic light signals,IEEE,Conferences,"Traffic light detection is an important part of Advanced Driver Assist as well as autonomous vehicle systems which ensures timely and appropriate reaction to traffic lights (TLs) in cross sections. In this paper we introduce a robust and realtime approach to detect TLs and recognize its status in complex traffic scenes solely based on image processing techniques. The proposed system uses color properties of the scene to detect TLs in real-time. An innovative technique has been developed to significantly decrease compute requirement for detection of TL color by using one Lookup Table independent of lighting conditions. Each candidate region is further analyzed, using features analysis, to segregate actual TL signals among all candidate regions. As in similar machine learning techniques, an unsupervised classifier using a set of significant features has been developed to accurately segregate circular, semi-circular, and arrow shaped TL signals without using a training dataset. The final C++ code has been implemented and optimized on intelplatform using 1920×1080 frame resolution to recognize the status of TLs during day-time and night-time scenes, achieving 95% precision and 94.7% recall at 30FPS.",https://ieeexplore.ieee.org/document/8010547/,2016 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),18-20 Oct. 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICTA49861.2020.9429073,Road Recognition System with Heuristic Method and Machine Learning,IEEE,Conferences,"Road recognition is one of essential information for determining an Autonomous Vehicle movement. Latest research has shown that machine learning could be used to obtain the information from images. Nevertheless, the system could be improved by effectivity and efficiency. This research proposed finding better feature combinations and using Artificial Neural Network algorithm to build higher accuracy road detection model for better effectivity. Region of Interest module using heuristic method also applied to reduce computation for better efficiency. These three new modules are implemented and combined with road recognition module to become road recognition system. The proposed method performance then tested and compared with the latest research. The experiment results shown that Artificial Neural Network cannot increase the system effectiveness. Nonetheless, with right feature and region of interest module, the proposed system successfully gives better performance. The prototype has accuracy increased from F1-score 0,94 to 0,95 and speed increased from 99 to 112 frames processed per second.",https://ieeexplore.ieee.org/document/9429073/,"2020 7th International Conference on Advance Informatics: Concepts, Theory and Applications (ICAICTA)",8-9 Sept. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC48978.2021.9564785,Safe Imitation Learning on Real-Life Highway Data for Human-like Autonomous Driving,IEEE,Conferences,"This paper presents a safe imitation learning approach for autonomous vehicle driving, with attention on real-life human driving data and experimental validation. In order to increase occupant's acceptance and gain drivers' trust, the autonomous driving function needs to provide a both safe and comfortable behavior such as risk-free and naturalistic driving. Our goal is to obtain such behavior via imitation learning of a planning policy from human driving data. In particular, we propose to incorporate barrier functions and smooth spline-based motion parametrization in the training loss function. The advantage is twofold: improving safety of the learning algorithm, while reducing the amount of needed training data. Moreover, the behavior is learned from highway driving data, which is collected consistently by a human driver and then processed towards a specific driving scenario. For development validation, a digital twin of the real test vehicle, sensors, and traffic scenarios are reconstructed toward high-fidelity and physics-based modeling technologies. These models are imported to simulation tools and co-simulated with the proposed algorithm for validation and further testing. Finally, we present experimental results and analyses, and compare with the conventional imitation learning technique (behavioral cloning) to justify the proposed development.",https://ieeexplore.ieee.org/document/9564785/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCAI53970.2022.9752493,Segmentation of Lidar Point Cloud Data using SOM,IEEE,Conferences,"Sensors play an important role in detecting and perceiving the environment. Many applications of designing a system to understand the environment semantically include vision for navigation, reverse engineering, Simultaneous Localization and Mapping (SLAM), modelling, autonomous vehicles, change detection, and autonomous robots. Accuracy concern, the Lidar sensor is now used in stunning systematic investigations. Lidar sensors generate data in various file formats such as LAS and point cloud data from the sensed environment. The ability to form simple various applications with error-free environment perception from the Lidar point cloud is critical. Navigation of an autonomous vehicle without colliding with obstacles and locating the vehicle on its own may be a difficult task, which is commonly referred to as SLAM. Creating an accurate map for local features in SLAM problems can be a time-consuming process. Many studies are concerned with the creation of a local map for a structured environment. This study will look into the implementation of a self-organized map for an unknown environment. The self-organized map (SOM) learns about the environment and acquires semantic knowledge from a local expert. This is taken as an input and further segments the region, especially unstructured environments like agricultural land, nonindustrial environments. Heretofore, the developed works for clustering and segmentation only for the roadside habitat and industrial purpose. In this paper, we implemented the real-time SOM to perceive any world environment regardless of illumination and is experimented. Implementation outputs are analysed for further rectification.",https://ieeexplore.ieee.org/document/9752493/,"2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",28-29 Jan. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVCI54083.2021.9661259,Sharpening Mixture of Experts Fusion of Infrared and Visible Images for Night Perception Enhancement,IEEE,Conferences,"Most object detection frameworks for the autonomous vehicle only focus on the perception task in good illumination conditions, ignoring the task at night time. This paper aims to develop an accurate and real-time object detection framework using the combined information of the visible and infrared cameras. A dual-modal dataset is firstly established using our designed vision acquisition platform, the image pairs are all captured in the local traffic environment. Then the sharpening mixture of experts fusion model based on the state-of-the-art network YOLOv5 is devised to adaptively learn the complementary information of the visible and infrared modalities. Finally, comprehensive comparative experiments are implemented, the results show that our proposed fusion framework outperforms the other fusion methods at night perception scenarios.",https://ieeexplore.ieee.org/document/9661259/,2021 5th CAA International Conference on Vehicular Control and Intelligence (CVCI),29-31 Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2014.6943162,Spatio-temporal motion features for laser-based moving objects detection and tracking,IEEE,Conferences,"This paper proposes a spatio-temporal motion feature detection and tracking method using range sensors working on a moving platform. The proposed spatio-temporal motion features are similar to optical flow but are extended on a moving platform with fusion of odometry and show much better classification accuracy with consideration of different uncertainties. In the proposal, the ego motion is compensated by odometry sensors and the laser scan points are accumulated and represented as space-time point clouds, from which the velocities and moving directions can be extracted. Based on these spatio-temporal features, a supervised learning technique is applied to classify the points as static or moving and Kalman filters are implemented to track the moving objects. A real experiment is performed during day and night on an autonomous vehicle platform and shows promising results in a crowded and dynamic environment.",https://ieeexplore.ieee.org/document/6943162/,2014 IEEE/RSJ International Conference on Intelligent Robots and Systems,14-18 Sept. 2014,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ChiCC.2019.8865420,Specialized Car Detector for Autonomous Driving,IEEE,Conferences,"Fast and accurate perception of the environment are important for autonomous vehicle research, especially the detection of other vehicles in the front. Different from object detection tasks in other fields where high accuracy and real-time detection speed are required, the model size of object detection system applied to autonomous vehicles should be as small as possible due to the limitations of vehicle-mounted embedded system. The object detection system of the autonomous vehicle takes too many pedestrians and other factors into account, which lead to a degradation in detection performance. In this work, we propose the so-called SpCarDet, a specialized car detector, which is a one-stage object detection network. Based on the Fire module, Stacked feature map and Shortcut connections, a new and small model size backbone network is built. We also present the bounding boxes generation network named AGnet that is optimized for car detection. The real-time accurate detection of the vehicle is achieved, and the model size of network is small to enable embedded system deployment. Experiments on the KITTI dataset demonstrate the effectiveness of our algorithm.",https://ieeexplore.ieee.org/document/8865420/,2019 Chinese Control Conference (CCC),27-30 July 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon48659.2022.9763965,Sub-Optimal Head Position Classification Using OpenVINO and SVM,IEEE,Conferences,"Improving our understanding of passenger-driver behavior during roadway driving is crucial for advancing the development of effective autonomous vehicle technologies. Understanding passenger behavior will allow for the development of appropriate control structures that would be able to tune responses to prevailing passenger preferences. This area, however, proves to be a challenge despite the prevalence and continued emergence of high quality technical products, because, we argue, there continues to be an incomplete understanding of how to enable machine interpretations of real-time passenger responses for such tuning behaviors. Using machine learning techniques and optimization algorithms to create a support vector machine (SVM) classifier, this paper investigates a method for using remotely-sensed passenger head position during drives in an attempt to understand the passengers&#x2019; behavior in relation with, and perhaps as a window into their judgements of, the driver and/or their behavior. Our results demonstrate that remote head position sensing is feasible and may be useful even at lower fidelity (trinary) classification levels, especially if resolvable in near realtime. In the case of this exploratory work, we observed that most of the passengers maintained a straight head position throughout the drive, which we take as validation of the potential for future applications using evolved versions of this or similar technologies, for instance, in leveraging such inferences across larger groups of vehicle occupants to enhance prediction accuracy.",https://ieeexplore.ieee.org/document/9763965/,SoutheastCon 2022,26 March-3 April 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.1994.639471,The development of a fully autonomous ground vehicle (FAGV),IEEE,Conferences,"As a first step toward the creation of a fully autonomous vehicle that operates in a real world environment, we are currently developing a prototype autonomous ground vehicle (AGV) for use in factories and other industrial/business sites based on behavior-based artificial intelligence (AI) control. This flexible and fully autonomous AGV (FAGV) is expected to operate efficiently in a normal industrial environment without any external guidance. The crucial technique employed is a non-Cartesian way of organizing software agents for the creation of a highly responsive control program. The resulting software is considerably reduced in size. Through numerous experiments using mobile robots we confirmed that these new control programs excel in functionality, efficiency, flexibility and robustness. The second key technique in the planning stage is evolutionary computation, of which genetic algorithms are a principal technique. An online, real-time evolution of the control program will be incorporated in later phases of the project to make FAGVs adaptable to any given operational environment after deployment. The first prototype FAGV has an active vision and behaviour-based control system.",https://ieeexplore.ieee.org/document/639471/,Proceedings of the Intelligent Vehicles '94 Symposium,24-26 Oct. 1994,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIMPAR.2018.8376285,The sleepwalker framework: Verification and validation of autonomous vehicles by mixed reality LiDAR stimulation,IEEE,Conferences,"Verification and validation of autonomous mobile systems, such as autonomous vehicles, is indispensable, since conflicts and serious incidents are rarely acceptable when human beings are involved. Although integrative simulation frameworks are commonly applied to test these systems, such simulations are usually too idealistic, while real world tests are both, expensive and not reproducible. To overcome this problem, we present the framework Sleepwalker for verifying and validating autonomous vehicles: Similar to a human sleepwalker, our framework stimulates the automated driving function at a sensor close level with virtual laserscans mixed with sensor data from the real environment. Thus, the autonomous driving function explicitely builds up a mixed reality environment model as a basis for the subsequent components and therefore enables an overall performance assessment. The instantiation of the framework is adaptable so it to can be balanced between the required result's plausibility and scenario criticality. We demonstrate the distinguished benefits of our framework by different instantiations stimulating an autonomous vehicle and conclude with further research questions.",https://ieeexplore.ieee.org/document/8376285/,"2018 IEEE International Conference on Simulation, Modeling, and Programming for Autonomous Robots (SIMPAR)",16-19 May 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00153,Three-Dimensional Simulation for Training Autonomous Vehicles in Smart City Environments,IEEE,Conferences,"This paper proposes a photorealistic 3D city simulation method for training autonomous vehicles. The proposed method incorporates human simulation, animal simulation, vehicle simulation, and traffic light simulation. To generate natural actions for humans and animals, a motivation-based approach is first applied; then the Q-Network is used to select optimal goals depending on the motivations, and action plans are made based on a hierarchical task network. For vehicles, affinity propagation, data augmentation, and convolutional neural network are employed to generate driver driving data for realistic vehicle movement simulation. A traffic light system is also implemented based on rules derived from real-life observation. The results of experiments in which a virtual city was created demonstrate that the proposed method can simulate city environments naturally. The proposed method can be applied to various smart city applications, such as autonomous vehicle training systems.",https://ieeexplore.ieee.org/document/8875271/,"2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",14-17 July 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSES52305.2021.9633959,Traffic Sign Detection using Deep Learning Techniques in Autonomous Vehicles,IEEE,Conferences,"Autonomous vehicle is an emerging topic for both researchers and the automobile industry as companies are still struggling to make fully functional autonomous vehicles. Driving a safe vehicle in a real world depends on different conditions, such as distance from other vehicles, pedestrians, animals, speed-breakers, traffic signals and other unpredictable dynamic environments. Autonomous vehicle can decrease vehicle crashes because software installed in the vehicle instructs the control system of the autonomous vehicle rather than human, and Software makes less error compare to human beings. Automated Traffic Sign Detection and Recognition (ATSDR) is an important task for a safe driving by an autonomous vehicle. Many researchers have used various deep learning-based models for in real-time ATSDR. Here in the present review, we have studied various deep learning models used for in real-time ATSDR. Our study suggested that YOLO and SSD can detect the traffic sign in real time and are superior models for ATSDR as compared to other deep learning methods as CNN, R-CNN, Fast R-CNN and Faster RCNN.",https://ieeexplore.ieee.org/document/9633959/,"2021 International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems (ICSES)",24-25 Sept. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2014.6856427,Traversability analysis using terrain mapping and online-trained Terrain type classifier,IEEE,Conferences,"Path estimation is a big challenge for autonomous vehicle navigation, especially in unknown, dynamic environments, when road characteristics change often. 3D terrain information (e.g. stereo cameras) can provide useful hints about the traversability cost of certain regions. However, when the terrain tends to be flat and uniform, it is difficult to identify a better path using 3D map solely. In this scenario the use of a priori knowledge on the expected road's visual characteristics can support detection, but it has the drawback of being not robust to environmental changes. This paper presents a path detection method that mixes together 3D mapping and visual classification, trying to learn, in real time, the actual road characteristics. An on-line learning of visual characteristics is implemented to feedback a terrain classifier, so that the road characteristics are updated as the vehicle moves. The feedback data are taken from a 3D traversability cost map, which provides some hints on traversable and non-traversable regions. After several re-training cycles the algorithm converges on a better separation of the path and non-path regions. The fusion of both 3D traversability cost and visual characteristics of the terrain yields a better estimation when compared with either of these methods solely.",https://ieeexplore.ieee.org/document/6856427/,2014 IEEE Intelligent Vehicles Symposium Proceedings,8-11 June 2014,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV45572.2020.9093332,Uncertainty-aware Short-term Motion Prediction of Traffic Actors for Autonomous Driving,IEEE,Conferences,"We address one of the crucial aspects necessary for safe and efficient operations of autonomous vehicles, namely predicting future state of traffic actors in the autonomous vehicle's surroundings. We introduce a deep learning-based approach that takes into account a current world state and produces raster images of each actor's vicinity. The rasters are then used as inputs to deep convolutional models to infer future movement of actors while also accounting for and capturing inherent uncertainty of the prediction task. Extensive experiments on real-world data strongly suggest benefits of the proposed approach. Moreover, following successful tests the system was deployed to a fleet of autonomous vehicles.",https://ieeexplore.ieee.org/document/9093332/,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1-5 March 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA52953.2021.00228,Using Generative Adversarial Networks and Non-Roadside Video Data to Generate Pedestrian Crossing Scenarios,IEEE,Conferences,"As fully autonomous driving is introduced on our roads, the safety of vulnerable road users is of the greatest importance. Available real-world data is limited and often lacks the variety required to ensure the safe deployment of new technologies. This paper builds on a novel generation method to generate pedestrian crossing scenarios for autonomous vehicle testing, known as the Ped-Cross GAN. While our previously developed Pedestrian Scenario dataset [1] is extremely detailed, there exist labels in the dataset where available data is severely imbalanced. In this paper, augmented non-roadside data is used to improve the generation results of pedestrians running at the roadside, increasing the classification accuracy from 20.95% to 82.56%, by increasing the training data by only 30%. This proves that researchers can generate rare, edge case scenarios using the Ped-Cross GAN, by successfully supplementing available data with additional non-roadside data. This will allow for adequate testing and greater test coverage when testing the performance of autonomous vehicles in pedestrian crossing scenarios. Ultimately, this will lead to fewer pedestrian casualties on our roads.",https://ieeexplore.ieee.org/document/9680112/,2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA),13-16 Dec. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTUCON53541.2021.9711733,Vehicle Distance and Speed Estimation Algorithm for Computer Vision Sensor System,IEEE,Conferences,"This study is devoted to solving the perception problem of autonomous transport system by Convolutional Neural Network (CNN) application. The goal of this paper is to develop the method for single camera based computer vision sensor for intelligent transport systems to detect and recognize objects in real-time, as well as estimate the speed of detected vehicle. This paper aims at evaluating the workability of this sensor in different weather conditions for autonomous vehicle applications. This sensor can be applied for different levels of abstraction like road safety, autonomous vehicle navigation, guiding system for disabled, animal survey, surveillance and other purposes.",https://ieeexplore.ieee.org/document/9711733/,2021 IEEE 62nd International Scientific Conference on Power and Electrical Engineering of Riga Technical University (RTUCON),15-17 Nov. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MICAI.2011.25,Velocity Control of an Electric Vehicle over a CAN Network,IEEE,Conferences,"Distributed control applications require a reliable network for information exchange. The network discussed on this paper uses CAN bus as a means of communication to control the speed of an electric vehicle. National Instruments Programmable Automation Controller, Compact RIO, based on Lab VIEW programming environment is used to execute one of two different speed control algorithms (PID or fuzzy logic) to test the performance of the implemented vehicle network and the control algorithm itself. It also acts as a human-machine interface via a personal computer. The proposed network provides robustness in terms of communication and opens the possibility of expansion to develop complete control architecture in order to successfully build a fully autonomous vehicle.",https://ieeexplore.ieee.org/document/6119003/,2011 10th Mexican International Conference on Artificial Intelligence,26 Nov.-4 Dec. 2011,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITST.2017.7972192,Virtual assistants and self-driving cars,IEEE,Conferences,"Self-driving cars are technologically a reality and in the next decade they are expected to reach the highest level of automation. While there is general agreement that an advanced human-autonomous vehicle (HAV) interaction is key to achieve the benefits of self-driving cars, it is less clear what role artificial intelligence (AI) should play in this context. While the scientific community is debating on the role and intersections of AI, autonomous vehicles and related issues, above all ethics, the automotive industry is already presenting AI-based products and services that may influence, in a direction or in another, our technological and societal futures. This paper focuses on virtual assistants, the personification of the car intelligence incorporating, among others, an algorithmic “brain”, a synthetic human “voice” and powerful sensor-based “senses”. Should virtual assistants just assist humans or replace them whenever necessary? Should their scope of action be limited to safety-related driving tasks or to any activity performed in the car or controlled from the car? Although at a very early stage of commercial development, the paper will review the state-of-the-art of in-car virtual assistants underlining their role and functions in the connected and automated driving ecosystem. By drawing from earlier reflections on automation, robots and intelligent agents, it will then identify a series of issues to be addressed by the scientific community, policy-makers and the automotive industry stakeholders.",https://ieeexplore.ieee.org/document/7972192/,2017 15th International Conference on ITS Telecommunications (ITST),29-31 May 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVT.2020.3027352,A Nash Q-Learning Based Motion Decision Algorithm With Considering Interaction to Traffic Participants,IEEE,Journals,"In order to improve the efficiency and comfort of autonomous vehicles while ensuring safety, the decision algorithm needs to interact with human drivers, infer the most probable behavior and then makes advantageous decision. This paper proposes a Nash-Q learning based motion decision algorithm to consider the interaction. First, the local trajectory of surrounding vehicle is predicted by kinematic constraints, which can reflect the short-term motion trend. Then, the future action space is built based the predicted local trajectory that consists of five basis actions. With that, the Nash-Q learning process can be implemented by the game between these basis actions. By elimination of strictly dominated actions and the Lemke-Howson method, the autonomous vehicle can decide the optimal action and infer the behavior of surrounding vehicle. Finally, the lane merging scenario is built to test the performance contrast to the existing methods. The driver in loop experiment is further designed to verify the interaction performance in multi-vehicle traffic. The results show that the Nash-Q learning based algorithm can improve the efficiency and comfort by 15.75% and 20.71% to the Stackelberg game and the no-interaction method respectively while the safety is ensured. It can also make real-time interaction with human drivers in multi-vehicle traffic.",https://ieeexplore.ieee.org/document/9207975/,IEEE Transactions on Vehicular Technology,Nov. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2964029,A New Vehicular Fog Computing Architecture for Cooperative Sensing of Autonomous Driving,IEEE,Journals,"The sensing coverage and accuracy of vehicles are vital for autonomous driving. However, the current sensing capability of a single autonomous vehicle is quite limited in the complicated road traffic environment, which leads to many sensing dead zones or frequent misdetection. In this paper, we propose to develop a Vehicular Fog Computing (VFC) architecture to implement cooperative sensing among multiple adjacent vehicles driving in the form of a platoon. Based on our VFC architecture greedy and Support Vector Machine (SVM) algorithms are adopted respectively to enhance the sensing coverage and accuracy in the platoon. Furthermore, the distributed deep learning is processed for trajectory prediction by applying the Light Gated Recurrent Unit (Li-GRU) neural network algorithm. Simulation results based on real-world traffic datasets indicate the sensing coverage and accuracy by the proposed algorithms can be significantly improved with low computational complexity.",https://ieeexplore.ieee.org/document/8950168/,IEEE Access,2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIA.2022.3164861,A Parallelized Self-Driving Vehicle Controller Using Unsupervised Machine Learning,IEEE,Journals,"In this article, a self-driving vehicle controller that optimizes the path a vehicle follows from its initial position to its destination is presented. The methods include clustering-based k-means, hierarchical, Gaussian matrix model, and self-organizing mapping. The real-time parallel implementation of the unsupervised machine learning algorithms could provide fast response times of under one microsecond during the lateral, longitudinal, and angular motion control of the autonomous vehicle. It was observed that a random selection of one of the machine learning methods may not always guarantee the optimality of the position and velocity variables as compared to the desired values. The proposed parallel implementation and optimization of the algorithms could have a significant contribution towards making transportation mobility more reliable and sustainable for future vehicular systems.",https://ieeexplore.ieee.org/document/9749840/,IEEE Transactions on Industry Applications,July-Aug. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TFUZZ.2004.832532,Automatic design of fuzzy controllers for car-like autonomous robots,IEEE,Journals,"This paper describes the design and implementation of a fuzzy control system for a car-like autonomous vehicle. The problem addressed is the diagonal parking in a constrained space, a typical problem in motion control of nonholonomic robots. The architecture proposed for the fuzzy controller is a hierarchical scheme which combines seven modules working in series and in parallel. The rules of each module employ the adequate fuzzy operators for its task (making a decision or generating a smoothly varying control output), and they have been obtained from heuristic knowledge and numerical data (with geometric information) depending on the module requirements (some of them are constrained to provide paths of near-minimal lengths). The computer-aided design tools of the environment Xfuzzy 3.0 (developed by some of the authors) have been employed to automate the different design stages: 1) translation of heuristic knowledge into fuzzy rules; 2) extraction of fuzzy rules from numerical data and their tuning to give paths of near-minimal lengths; 3) offline verification of the control system behavior; and 4) its synthesis to be implemented in a true robot and be verified on line. Real experiments with the autonomous vehicle ROMEO 4R (designed and built at the Escuela Superior de Ingenieros, University of Seville, Seville, Spain) demonstrate the efficiency of the described controller and of the methodology followed in its design.",https://ieeexplore.ieee.org/document/1321074/,IEEE Transactions on Fuzzy Systems,Aug. 2004,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIV.2020.3029853,Autonomous Wary Collision Avoidance,IEEE,Journals,"Handling of critical situations is an important part in the architecture of an autonomous vehicle. A controller for autonomous collision avoidance is developed based on a wary strategy that assumes the least tire-road friction for which the maneuver is still feasible. Should the friction be greater, the controller makes use of this, and performs better. The controller uses an acceleration-vector reference obtained from optimal control of a friction-limited particle, whose applicability is verified by using numerical optimization on a full vehicle model. By employing an analytical tire model of the tire-road friction limit, to determine slip references for steering, and body-slip control, the result is a controller where the computation of its output is explicit, and independent of the actual tire-road friction. When evaluated in real-time on a high-fidelity simulation model, the developed controller performs close to that achieved by offline numerical optimization.",https://ieeexplore.ieee.org/document/9219194/,IEEE Transactions on Intelligent Vehicles,June 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMC.2019.2892451,Deep CNN-Based Real-Time Traffic Light Detector for Self-Driving Vehicles,IEEE,Journals,"Due to the unavailability of Vehicle-to-Infrastructure (V2I) communication in current transportation systems, Traffic Light Detection (TLD) is still considered an important module in autonomous vehicles and Driver Assistance Systems (DAS). To overcome low flexibility and accuracy of vision-based heuristic algorithms and high power consumption of deep learning-based methods, we propose a lightweight and real-time traffic light detector for the autonomous vehicle platform. Our model consists of a heuristic candidate region selection module to identify all possible traffic lights, and a lightweight Convolution Neural Network (CNN) classifier to classify the results obtained. Offline simulations on the GPU server with the collected dataset and several public datasets show that our model achieves higher average accuracy and less time consumption. By integrating our detector module on NVidia Jetson TX1/TX2, we conduct on-road tests on two full-scale self-driving vehicle platforms (a car and a bus) in normal traffic conditions. Our model can achieve an average detection accuracy of 99.3 percent (mRttld) and 99.7 percent (Rttld) at 10Hz on TX1 and TX2, respectively. The on-road tests also show that our traffic light detection module can achieve <; + 1:5m errors at stop lines when working with other selfdriving modules.",https://ieeexplore.ieee.org/document/8611202/,IEEE Transactions on Mobile Computing,1 Feb. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2019.2962413,DenseLightNet: A Light-Weight Vehicle Detection Network for Autonomous Driving,IEEE,Journals,"In recent years, vehicle detectors built on deep convolutional neural network (DCNN) have been widely used in autonomous driving. Under the complex conditions of road traffic, the detector is expected to run in high speed and high accuracy. However, due to the limited computing power and storage space on the autonomous vehicle, the deployment of advanced DCNN detectors is often restricted. The design of lightweight and powerful detectors is in a great desire. Recently, group convolution, as a novel convolution algorithm, has been proposed to reduce the floating-point operations and make the detection network lighter and faster. However, in practice, it is found that the increase of group number does not always boost the detection speed, but sometimes leads to the performance degradation. In addition, the existing guidelines for network design do not indicate how to choose the group number in the group convolution in order to maximize the overall detection speed. To this end, in this paper, we propose three new guidelines to determine the valid range of the group number, and design a lightweight detection network-DenseLightNet-based on these new design criteria. The proposed detector runs at a speed of three times faster than the current real-time detector YoloV3, while holding a much smaller model size.",https://ieeexplore.ieee.org/document/8948288/,IEEE Transactions on Industrial Electronics,Dec. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LES.2020.3009910,Designing Neural Networks for Real-Time Systems,IEEE,Journals,"Artificial neural networks (ANNs) are increasingly being used within safety-critical cyber-physical systems (CPSs). It is important to validate both the timing and functional correctness of these systems. However, most approaches in the literature consider guaranteeing only the functionality of the ANN-based controllers. This issue stems largely from the implementation strategies used within common neural network frameworks-their underlying source code is often simply unsuitable for formal techniques such as static timing analysis. As a result, developers of safety-critical CPS must rely on informal techniques, such as measurement-based approaches, to prove correctness, techniques that provide weak guarantees at best. In this letter, we address this challenge. We propose a design pipeline whereby neural networks trained using the popular deep learning framework Keras are compiled to functionally equivalent C code. This C code is restricted to simple constructs that may be analyzed by the existing static timing analysis tools. As a result, if compiled to a suitable time-predictable platform, all execution bounds may be statically derived. To demonstrate the benefits of our approach, we execute an ANN trained to drive an autonomous vehicle around a race track. We compile the ANN to the Patmos time-predictable controller and show that we can derive the worst-case execution timings.",https://ieeexplore.ieee.org/document/9143184/,IEEE Embedded Systems Letters,Sept. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2022.3191174,Improving Worst Case Visual Localization Coverage via Place-Specific Sub-Selection in Multi-Camera Systems,IEEE,Journals,"6-DoF visual localization systems utilize principled approaches rooted in 3D geometry to perform accurate camera pose estimation of images to a map. Current techniques use hierarchical pipelines and learned 2D feature extractors to improve scalability and increase performance. However, despite gains in typical recall<inline-formula><tex-math notation=""LaTeX"">$@0.25\,{\rm{m}}$</tex-math></inline-formula> type metrics, these systems still have limited utility for real-world applications like autonomous vehicles because of their <i>worst</i> areas of performance - the locations where they provide insufficient recall at a certain required error tolerance. Here we investigate the utility of using <i>place specific configurations</i>, where a map is segmented into a number of places, each with its own configuration for modulating the pose estimation step, in this case selecting a camera within a multi-camera system. On the Ford AV benchmark dataset, we demonstrate substantially improved worst-case localization performance compared to using off-the-shelf pipelines - minimizing the percentage of the dataset which has low recall at a certain error tolerance, as well as improved overall localization performance. Our proposed approach is particularly applicable to the crowdsharing model of autonomous vehicle deployment, where a fleet of AVs are regularly traversing a known route.",https://ieeexplore.ieee.org/document/9830877/,IEEE Robotics and Automation Letters,Oct. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVT.2022.3150793,"Joint Optimization of Sensing, Decision-Making and Motion-Controlling for Autonomous Vehicles: A Deep Reinforcement Learning Approach",IEEE,Journals,"The three main modules of autonomous vehicles, i.e., sensing, decision making, and motion controlling, have been studied separately in most existing works on autonomous driving, which overlook the correlations among these modules, leading to a result of unsatisfactory performance. In this paper, we propose a novel scheme that first tactfully processes the sensing data, then jointly learns and optimizes the decision-making and motion-controlling using reinforcement learning (RL). Specifically, the proposed scheme designs a novel state representation mechanism, where the sensing data goes through the attention layer and the convolutional neural network (CNN) layer sequentially. The attention layer focuses on extracting the most important local information and then CNN layer takes a broad view to comprehensively consider the global information for a better representation. Furthermore, the proposed scheme jointly learns decision-making and motion-controlling, therefore, the relevance of these two modules is implicitly considered, which helps to achieve a better autonomous driving policy. Extensive simulation results show that the proposed scheme is better than classic control methods and some RL methods in terms of safety, velocity, etc. We also demonstrate the respective functions of the attention layer and the CNN layer through ablation studies. Finally, we construct a traffic scene with a real autonomous vehicle, and verified the feasibility of the proposed scheme.",https://ieeexplore.ieee.org/document/9712351/,IEEE Transactions on Vehicular Technology,May 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2970728,LoPECS: A Low-Power Edge Computing System for Real-Time Autonomous Driving Services,IEEE,Journals,"To simultaneously enable multiple autonomous driving services on affordable embedded systems, we designed and implemented LoPECS, a Low-Power Edge Computing System for real-time autonomous robots and vehicles services. The contributions of this paper are three-fold: first, we developed a Heterogeneity-Aware Runtime Layer to fully utilize vehicle's heterogeneous computing resources to fulfill the real-time requirement of autonomous driving applications; second, we developed a vehicle-edge Coordinator to dynamically offload vehicle tasks to edge cloudlet to further optimize user experience in the way of prolonged battery life; third, we successfully integrated these components into LoPECS system and implemented it on Nvidia Jetson TX1. To the best of our knowledge, this is the first complete edge computing system in a production autonomous vehicle. Our implementation on Nvidia Jetson demonstrated that it could successfully support multiple autonomous driving services with only 11 W of power consumption, and hence proves the effectiveness of the proposed LoPECS system.",https://ieeexplore.ieee.org/document/8977507/,IEEE Access,2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/41.704895,Modeling of ultrasonic range sensors for localization of autonomous mobile robots,IEEE,Journals,"This paper presents a probabilistic model of ultrasonic range sensors using backpropagation neural networks trained on experimental data. The sensor model provides the probability of detecting mapped obstacles in the environment, given their position and orientation relative to the transducer. The detection probability can be used to compute the location of an autonomous vehicle from those obstacles that are more likely to be detected. The neural network model is more accurate than other existing approaches, since it captures the typical multilobal detection pattern of ultrasonic transducers. Since the network size is kept small, implementation of the model on a mobile robot can be efficient for real-time navigation. An example that demonstrates how the credence could be incorporated into the extended Kalman filter (EKF) and the numerical values of the final neural network weights are provided in the appendices.",https://ieeexplore.ieee.org/document/704895/,IEEE Transactions on Industrial Electronics,Aug. 1998,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TITS.2020.3045096,N2C: Neural Network Controller Design Using Behavioral Cloning,IEEE,Journals,"Modern vehicles communicate data to and from sensors, actuators, and electronic control units (ECUs) using Controller Area Network (CAN) bus, which operates on differential signaling. An autonomous ECU responsible for the execution of decision commands to an autonomous vehicle is developed by assimilating the information from the CAN bus. The conventional way of parsing the decision commands is motion planning, which uses a path tracking algorithm to evaluate the decision commands. This study focuses on designing a robust controller using behavioral cloning and motion planning of autonomous vehicle using a deep learning framework. In the first part of this study, we explore the pipeline of parsing decision commands from the path tracking algorithm to the controller and proposed a neural network-based controller ( N2C) using behavioral cloning. The proposed network predicts throttle, brake, and torque when trained with the manual driving data acquired from the CAN bus. The efficacy of the proposed method is demonstrated by comparing the accuracy with the Proportional-Derivative-Integral (PID) controller in conjunction with the path tracking algorithm (pure pursuit and model predictive control based path follower). The second part of this study complements N2C, in which an end-to-end neural network for predicting the speed and steering angle is proposed with image data as an input. The performance of the proposed frameworks are evaluated in real-time and on the Udacity dataset, showing better metric scores in the former and reliable prediction in the later case when compared with the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9312433/,IEEE Transactions on Intelligent Transportation Systems,July 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVCG.2019.2899227,You or Me? Personality Traits Predict Sacrificial Decisions in an Accident Situation,IEEE,Journals,"Emergency situations during car driving sometimes force the driver to make a sudden decision. Predicting these decisions will have important applications in updating risk analyses in insurance applications, but also can give insights for drafting autonomous vehicle guidelines. Studying such behavior in experimental settings, however, is limited by ethical issues as it would endanger peoples' lives. Here, we employed the potential of virtual reality (VR) to investigate decision-making in an extreme situation in which participants would have to sacrifice others in order to save themselves. In a VR driving simulation, participants first trained to complete a difficult course with multiple crossroads in which the wrong turn would lead the car to fall down a cliff. In the testing phase, obstacles suddenly appeared on the “safe” turn of a crossroad: for the control group, obstacles consisted of trees, whereas for the experimental group, they were pedestrians. In both groups, drivers had to decide between falling down the cliff or colliding with the obstacles. Results showed that differences in personality traits were able to predict this decision: in the experimental group, drivers who collided with the pedestrians had significantly higher psychopathy and impulsivity traits, whereas impulsivity alone was to some degree predictive in the control group. Other factors like heart rate differences, gender, video game expertise, and driving experience were not predictive of the emergency decision in either group. Our results show that self-interest related personality traits affect decision-making when choosing between preservation of self or others in extreme situations and showcase the potential of virtual reality in studying and modeling human decision-making.",https://ieeexplore.ieee.org/document/8651471/,IEEE Transactions on Visualization and Computer Graphics,May 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2019.8803189,Recognizing Chinese Texts with 3D Convolutional Neural Network,IEEE,Conferences,"In this paper, we propose a deep learning system to localize and recognize Chinese texts in scenes with signage and road marks through 3D convolutional neural network. The proposed system adopts YOLO for detecting target location and exploits 3D convolutional neural network for recognizing the contents. The proposed design outperforms the existing designs based on LSTM and achieves real-time processing performance, which is feasible to be implemented on embedded platforms. The proposed system reaches over 90% accuracy in recognizing Chinese texts on bird's-eye viewing road marks in a self-driving vehicle equipped with a fisheye camera. In addition, this system can achieve 20 fps execution speed with NVIDIA DIGITS DevBox with 1080Ti GPU, which is fast enough for autonomous driving applications.",https://ieeexplore.ieee.org/document/8803189/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEM51511.2021.9445285,Simulation based vehicle movement tracking using Kalman Filter algorithm for Autonomous vehicles,IEEE,Conferences,"In the domain of Software automotive industry, one of the most widely used algorithms for performing analysis of driving operations is the Kalman filter algorithm. In today's world of advanced machine learning, the Kalman filter remains an important tool to fuse measurements from several sensors to estimate the real time state of robotics systems such as a self-driving vehicle. Kalman filter is able to update an estimate of evolving nature of continuously changing states of the common filters to take a probabilistic estimate. The driving scenario results are updated in real time using 2-steps update and correction method. In this paper, we have described the process of Kalman filter and its variant to estimate about the detection of moving object in a given traffic scenario using advance toolboxes of MATLAB. Results have been shown for multiple changing parameters.",https://ieeexplore.ieee.org/document/9445285/,2021 2nd International Conference on Intelligent Engineering and Management (ICIEM),28-30 April 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCSP49186.2020.9315223,Artificial Intelligence based Self-Driving Car,IEEE,Conferences,"The paper proposes a self-driving car model also called autonomous, robotic or driver-less car is one that operates and navigates using its intelligence. The basic idea behind the paper is to develop a 1/10 scale RC car to portray an automated car. The model consists of the following software and hardware components such as CNN (Convolutional neural network), Monocular vision algorithm, Haar cascade classifier, Raspberry Pi Board model B+, Pi camera, Arduino, and an Ultrasonic sensor. The Pi camera and ultrasonic sensor are attached to the raspberry pi board to collect input images along with sensor data to stream these data to the server which in our case is the laptop. The (CNN) convolutional neural network running on the server will be used to enable lane detection to provide steering predictions that are left, right, forward based on the input image. The haar cascade classifier will be used to detect signals and stop sign and monocular vision algorithms to calculate distance from them. The ultrasonic sensor will be used for front collision avoidance by stopping the car at a certain distance before the obstacle ahead. The navigation commands such as right, left, forward, stop will be sent to the car through Arduino which is connected to the RC car's remote, this will make the car drive autonomously based on neural network predictions and some hard coded rules. Thus this model will enable autonomous driving by self-navigation via lane detection, stopping at detection of stop signs, red lights and moving on green signal and front collision avoidance in a cost-effective manner. Hardcoded rules, obstacle avoidance mechanisms, machine learning models, and smart object discrimination will help the system follow traffic rules and navigate the car using artificial intelligence.",https://ieeexplore.ieee.org/document/9315223/,"2020 4th International Conference on Computer, Communication and Signal Processing (ICCCSP)",28-29 Sept. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2014.6974391,Predicting dynamic computational workload of a self-driving car,IEEE,Conferences,"This study aims at developing a method that predicts the CPU usage patterns of software tasks running on a self-driving car. To ensure safety of such dynamic systems, the worst-case-based CPU utilization analysis has been used; however, the nature of dynamically changing driving contexts requires more flexible approach for an efficient computing resource management. To better understand the dynamic CPU usage patterns, this paper presents an effort of designing a feature vector to represent the information of driving environments and of predicting, using regression methods, the selected tasks' CPU usage patterns given specific driving contexts. Experiments with real-world vehicle data show a promising result and validate the usefulness of the proposed method.",https://ieeexplore.ieee.org/document/6974391/,"2014 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2014,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GTSD.2018.8595590,Real-Time Self-Driving Car Navigation Using Deep Neural Network,IEEE,Conferences,"In this paper, a monocular vision-based self-driving car prototype using Deep Neural Network on Raspberry Pi is proposed. Self-driving cars are one of the most increasing interests in recent years as the definitely developing relevant hardware and software technologies toward fully autonomous driving capability with no human intervention. Level-3/4 autonomous vehicles are potentially turning into a reality in near future. Convolutional Neural Networks (CNNs) have been shown to achieve significant performance in various perception and control tasks in comparison to other techniques in the latest years. The key factors behind these impressive results are their ability to learn millions of parameters using a large amount of labeled data. In this work, we concentrate on finding a model that directly maps raw input images to a predicted steering angle as output using a deep neural network. The technical contributions of this work are two-fold. First, the CNN model parameters were trained by using data collected from vehicle platform built with a 1/10 scale RC car, Raspberry Pi 3 Model B computer and front-facing camera. The training data were road images paired with the time-synchronized steering angle generated by manually driving. Second, road tests the model on Raspberry to drive itself in the outdoor environment around oval-shaped and 8-shaped with traffic sign lined track. The experimental results demonstrate the effectiveness and robustness of autopilot model in lane keeping task. Vehicle's top speed is about 5-6km/h in a wide variety of driving conditions, regardless of whether lane markings are present or not.",https://ieeexplore.ieee.org/document/8595590/,2018 4th International Conference on Green Technology and Sustainable Development (GTSD),23-24 Nov. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSSE52999.2021.9538460,Steering Angle Estimation for Self-driving Car Based on Enhanced Semantic Segmentation,IEEE,Conferences,"Common approaches for semantic segmentation using Convolutional Neural Networks (CNN) based around conventional U-shapes architectures were widely used. However, failures to retrieve global context information and memory issues made such models unable to compete against modern architectures considering accuracy and real-time capability. In this paper, an efficient method maintaining equivalent accuracy of a previous segmentation network and skillfully making a model more light-weighted for real-time inference was proposed. More concretely, we managed to alleviate five out of 17 million trainable parameters, which effectively reduce the amount of computation of the original PSPNet by 30% using the backbone of CSPNet. Our proposed network implementation achieved 73 mIoU scores on our custom dataset and reached 15 fps regarding real-time inference. We deployed the trained model on the multifunctional hardware and then connected it to a golf car to jointly navigate the natural environment and traffic sign detection task. Accordingly, the STM32 board and servo motor were used for controlling the steering wheel through a track-and-wheel drive system. As for the traffic sign detection task, we employed a small-size Yolov5 trained on the TT100K dataset running around 60fps and attained real-time performance with sufficient accuracy.",https://ieeexplore.ieee.org/document/9538460/,2021 International Conference on System Science and Engineering (ICSSE),26-28 Aug. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00062,A Combination of Feedback Control and Vision-Based Deep Learning Mechanism for Guiding Self-Driving Cars,IEEE,Conferences,"The purpose of this paper is to develop an agent that can imitate the behavior of humans driving a car. When human beings driving a car, he/she majorly uses vision system to recognize the states of the car, including the position, velocity, and the surrounding environments. In this paper, we implemented a self-driving car which can drive itself on the track of a simulator. The self-driving car uses deep neural network as a computational framework to ""learn"" what is the position of the car related to the road. While the car understands the position of itself related to the track, it can use the information as a basis for feedback control.",https://ieeexplore.ieee.org/document/8613681/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPA/IUCC.2017.00167,A cGANs-Based Scene Reconstruction Model Using Lidar Point Cloud,IEEE,Conferences,"Road scene reconstruction is a fundamental and crucial module at the perception phase for autonomous vehicles, and will influence the later phase, such as object detection, motion planing and path planing. Traditionally, self-driving car uses Lidar, camera or fusion of the two kinds of sensors for sensing the environment. However, single Lidar or camera-based approaches will miss crucial information, and the fusion-based approaches often consume huge computing resources. We firstly propose a conditional Generative Adversarial Networks (cGANs)-based deep learning model that can rebuild rich semantic scene images from upsampled Lidar point clouds only. This makes it possible to remove cameras to reduce resource consumption and improve the processing rate. Simulation on the KITTI dataset also demonstrates that our model can reestablish color imagery from a single Lidar point cloud, and is effective enough for real time sensing on autonomous driving vehicles.",https://ieeexplore.ieee.org/document/8367397/,2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC),12-15 Dec. 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8851947,Bio-Inspired Foveated Technique for Augmented-Range Vehicle Detection Using Deep Neural Networks,IEEE,Conferences,"We propose a bio-inspired foveated technique to detect cars in a long range camera view using a deep convolutional neural network (DCNN) for the IARA self-driving car. The DCNN receives as input (i) an image, which is captured by a camera installed on IARA's roof; and (ii) crops of the image, which are centered in the waypoints computed by IARA's path planner and whose sizes increase with the distance from IARA. We employ an overlap filter to discard detections of the same car in different crops of the same image based on the percentage of overlap of detections' bounding boxes. We evaluated the performance of the proposed augmented-range vehicle detection system (ARVDS) using the hardware and software infrastructure available in the IARA self-driving car. Using IARA, we captured thousands of images of real traffic situations containing cars in a long range. Experimental results show that ARVDS increases the Average Precision (AP) of long range car detection from 29.51% (using a single whole image) to 63.15%.",https://ieeexplore.ieee.org/document/8851947/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIIP47207.2019.8985977,Comparative Analysis and Implementation of Different Human Detection Techniques,IEEE,Conferences,"Object Detection is used in making several smart surveillance applications which are used in detecting and tracking suspicious activities. Object Detection also plays a very crucial role in several latest inventions like the Google self-driving car. Over the past decade several image processing algorithms have evolved which have been used for human detection after preprocessing of the images. But in several cases these algorithms were found to be less accurate and more time taking. With the advent of the era of Machine Learning, the focus has gradually shifted towards computer vision and deep learning methodologies for human detection. Deep learning algorithms are far more accurate than the traditional methodologies as they employ feature extraction from the images followed by classification according to the dataset provided to them, thus enabling more accurate detection than their ancestors. Nevertheless its also true that some deep learning algorithms like the different versions of R-CNN and YOLO take much more processing time than the traditional methodologies. In our paper we have compared the performance of some traditional and some deep learning algorithms in different scenarios.",https://ieeexplore.ieee.org/document/8985977/,2019 Fifth International Conference on Image Information Processing (ICIIP),15-17 Nov. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3238147.3238187,DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems,IEEE,Conferences,"While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.",https://ieeexplore.ieee.org/document/9000040/,2018 33rd IEEE/ACM International Conference on Automated Software Engineering (ASE),3-7 Sept. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3180155.3180220,DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars,IEEE,Conferences,"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.",https://ieeexplore.ieee.org/document/8453089/,2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE),27 May-3 June 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin.2018.8576190,End to End Learning based Self-Driving using JacintoNet,IEEE,Conferences,"Automated driving functions, like highway driving and parking assist, are getting increasing deployed in high-end cars with the trend moving towards the self-driving car. With the advent of deep learning, many traditional computer vision techniques have been replaced by deep convolutional neural networks (CNN). End to end learning is one of the paradigm for self-driving, in which user provides a input images from the front facing camera to the given neural network and the network outputs the car control signals such as throttle, steering and braking. The paper proposes an embedded friendly convolutional neural network, `Jacintonet', to demonstrate self-driving using end to end learning paradigm in a virtual simulation environment. Paper discusses key learning during the training methodology and presents the results on embedded platform. Texas Instruments (TI) TDA2x System on Chip (SoC) is used as embedded platform for running `Jacintonet', real-time to demonstrate self-driving car in the virtual simulator.",https://ieeexplore.ieee.org/document/8576190/,2018 IEEE 8th International Conference on Consumer Electronics - Berlin (ICCE-Berlin),2-5 Sept. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SysCon48628.2021.9447097,Experimental Validation of a Steering Control System using an Adaptive Fuzzy Controller and Computer Vision,IEEE,Conferences,"This paper proposes an adaptive steering control strategy for self-driving cars based on a Fuzzy Expert System and Reinforcement Learning. Our objective consists in deriving an appropriate control law directly from a real vehicle that allows it to navigate on several types of lanes, by controlling the position in relation to the center of the tracks and also the translation speed of the vehicle. Using an on-line Reinforcement Learning approach, the Fuzzy expert controller is derived considering the coupling and non-linearity of the model on straight and winding tracks. To do this, an embedded camera captures the images and sends them to the computer vision algorithm responsible for performing tracks detection and recognition. From that, the control references which indicate the navigation path and direction on the lane are calculated. The main contribution of this work is to apply an online reinforcement learning approach to tune and optimize the fuzzy steering controller while the vehicle navigates through different routes. Using a real vehicle equipped with an embedded computer and also the implemented web user interface, the learning evolution of the adaptive fuzzy controller can be managed remotely during trial in actual environments. Experimental results showed that the learned fuzzy expert controller controls the self-driving car during the path tracking and precisely performs the execution of different maneuvers.",https://ieeexplore.ieee.org/document/9447097/,2021 IEEE International Systems Conference (SysCon),15 April-15 May 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCPS54341.2022.00027,Interpretable Detection of Distribution Shifts in Learning Enabled Cyber-Physical Systems,IEEE,Conferences,"The use of learning based components in cyber-physical systems (CPS) has created a gamut of possible avenues to use high dimensional real world signals generated from sensors like camera and LiDAR. The ability to process such signals can be largely attributed to the adoption of high-capacity function approximators like deep neural networks. However, this does not come without its potential perils. The pitfalls arise from possible over-fitting, and subsequent unsafe behavior when exposed to unknown environments. One challenge is that, in high dimensional input spaces it is almost impossible to experience enough training data in the design phase. What is required here, is an efficient way to flag out-of-distribution (OOD) samples that is precise enough to not raise too many false alarms. In addition, the system needs to be able to detect these in a computationally efficient manner at runtime. In this paper, our proposal is to build good representations for in-distribution data. We introduce the idea of a memory bank to store prototypical samples from the input space. We use these memories to compute probability density estimates using kernel density estimation techniques. We evaluate our technique on two challenging scenarios : a self-driving car setting implemented inside the simulator CARLA with image inputs, and an autonomous racing car navigation setting, with LiDAR inputs. In both settings, it was observed that a deviation from indistribution setting can potentially lead to deviation from safe behavior. An added benefit of using training samples as memories to detect out-of-distribution inputs is that the system is interpretable to a human operator. Explanation of this nature is generally hard to obtain from pure deep learning based alter-natives. Our code for reproducing the experiments is available at https://github.com/yangy96/interpretable_ood_detection.git",https://ieeexplore.ieee.org/document/9797620/,2022 ACM/IEEE 13th International Conference on Cyber-Physical Systems (ICCPS),4-6 May 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISQED.2018.8357325,Low cost and power CNN/deep learning solution for automated driving,IEEE,Conferences,"Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is “finding drivable free space”, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving.",https://ieeexplore.ieee.org/document/8357325/,2018 19th International Symposium on Quality Electronic Design (ISQED),13-14 March 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URTC45901.2018.9244807,Optimization of Four-Way Controlled Intersections with Autonomous and Human-Driven Vehicles,IEEE,Conferences,"Many car crashes occur at four-way stop sign intersections due to error-prone humans who misinterpret their surroundings. A solution is needed to prevent possible accidents and streamline traffic at four-way stop sign intersections. This paper proposes a software-based monitoring solution to eliminate this miscommunication that leads to accidents in a roadway setting where human-driven and autonomous cars are integrated. Relying on computer vision through an overhead camera and a convolutional neural network, the monitoring system is able to detect stopped vehicles at the intersection and direct traffic. Additionally, a self-driving car equipped with line-tracking was built along with a testing track to evaluate the efficacy of the monitoring software. Due to the project's limited resources and insufficient processing power, the software was unable to function as a real-time system. However, the system works with still images of various intersection scenarios with high accuracy.",https://ieeexplore.ieee.org/document/9244807/,2018 IEEE MIT Undergraduate Research Technology Conference (URTC),5-7 Oct. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMI50585.2021.9378655,Psychophysiological modelling of trust in technology: Comparative analysis of algorithm ensemble methods,IEEE,Conferences,"Measuring user's trust in technology in real-time using psychophysiological signals depends on the availability of stable, accurate, variance sensitive, and non-bias trust classifier model which can be achieved through ensembling several algorithms. Prior efforts resulted to fairly accurate but unstable models. This article investigates what ensemble method is most suitable for developing an ensemble trust classifier model for assessing users trust in technology with psychophysiological signals. Using a self-driving car game, a within subject four condition experiment was implemented. During which 31 participant were involved, and multimodal psychophysiological data (EEG, ECG, EDA, and Facial-EMG) were recorded. An exhaustive 172 features from time and frequency domain were extracted. Six carefully selected algorithms were combined for developing ensemble trust classifier models using each of the four ensemble methods (voting, bagging, stacking, boosting). The result indicated that the Stack ensemble method was more superior, despite voting method dominating prior studies.",https://ieeexplore.ieee.org/document/9378655/,2021 IEEE 19th World Symposium on Applied Machine Intelligence and Informatics (SAMI),21-23 Jan. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV51971.2022.9827302,Tackling Real-World Autonomous Driving using Deep Reinforcement Learning,IEEE,Conferences,"In the typical autonomous driving stack, planning and control systems represent two of the most crucial components in which data retrieved by sensors and processed by perception algorithms are used to implement a safe and comfortable self-driving behavior. In particular, the planning module predicts the path the autonomous car should follow taking the correct high-level maneuver, while control systems perform a sequence of low-level actions, controlling steering angle, throttle and brake. In this work, we propose a model-free Deep Reinforcement Learning Planner training a neural network that predicts both acceleration and steering angle, thus obtaining a single module able to drive the vehicle using the data processed by localization and perception algorithms on board of the self-driving car. In particular, the system that was fully trained in simulation is able to drive smoothly and safely in obstacle-free environments both in simulation and in a real-world urban area of the city of Parma, proving that the system features good generalization capabilities also driving in those parts outside the training scenarios. Moreover, in order to deploy the system on board of the real self-driving car and to reduce the gap between simulated and real-world performances, we also develop a module represented by a tiny neural network able to reproduce the real vehicle dynamic behavior during the training in simulation.",https://ieeexplore.ieee.org/document/9827302/,2022 IEEE Intelligent Vehicles Symposium (IV),4-9 June 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/Ubi-Media.2019.00014,The Matter of Deep Reinforcement Learning Towards Practical AI Applications,IEEE,Conferences,"Reinforcement Learning (RL) is an extraordinarily paradigm that aims to solve a complex problem. This technique leverages the traditional feedforward networks with temporal-difference learning to overcome supervised and unsupervised real-world problems. However, RL is one of state-of-the-art topic due to the opaque aspects in design and implementation. Also, in which situation we will get performance gain from RL is still unclear. Therefore, This study firstly examines the impact of Experience Replay in Deep Q-Learning agent with Self-Driving Car application. Secondly, The impact of Eligibility Trace in RNN A3C agents with Breakout AI game application is studied. Our results indicated that these two techniques enhance RL performance by more than 20 percent as compared with traditional RL methods.",https://ieeexplore.ieee.org/document/9049567/,2019 Twelfth International Conference on Ubi-Media Computing (Ubi-Media),5-8 Aug. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAMIMIA47173.2019.9223365,Autonomous Car Simulation Using Evolutionary Neural Network Algorithm,IEEE,Conferences,"Automation with artificial intelligence (AI) has widely implemented in robotics, transportation and manufacture. AI has become a powerful technology that change human life and help human more flexible doing something. In this paper, it will show a result of simulation from an autonomous car using the evolutionary neural network algorithm which combines genetic algorithm and neural network. The purpose of the simulation is to test the model that we develop to know the right direction based on the track, so the evolutionary neural network that implemented to the autonomous car be able to deliver the best solution before it implements in the real machine or car technology. Genetic algorithm combines with a neural network to reach an evolution condition. The evolution process is achieved through crossover, mutation and selection process, so the algorithm will give the best result from the iteration of the experiment. The result of our experiment shows that evolutionary neural network algorithm give the best result within 3 layer architecture, with iteration average is 14.5 reach finish point (check point) 3 in the track simulation. Based on the simulation, our car model can find out the right direction.",https://ieeexplore.ieee.org/document/9223365/,"2019 International Conference on Advanced Mechatronics, Intelligent Manufacture and Industrial Automation (ICAMIMIA)",9-10 Oct. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3097345,Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement Learning,IEEE,Journals,"Autonomous car racing is a challenging task in the robotic control area. Traditional modular methods require accurate mapping, localization and planning, which makes them computationally inefficient and sensitive to environmental changes. Recently, deep-learning-based end-to-end systems have shown promising results for autonomous driving/racing. However, they are commonly implemented by supervised imitation learning (IL), which suffers from the distribution mismatch problem, or by reinforcement learning (RL), which requires a huge amount of risky interaction data. In this work, we present a general deep imitative reinforcement learning approach (DIRL), which successfully achieves agile autonomous racing using visual inputs. The driving knowledge is acquired from both IL and model-based RL, where the agent can learn from human teachers as well as perform self-improvement by safely interacting with an offline world model. We validate our algorithm both in a high-fidelity driving simulation and on a real-world 1/20-scale RC-car with limited onboard computation. The evaluation results demonstrate that our method outperforms previous IL and RL methods in terms of sample efficiency and task performance. Demonstration videos are available at https://caipeide.github.io/autorace-dirl/.",https://ieeexplore.ieee.org/document/9488179/,IEEE Robotics and Automation Letters,Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV.2018.00024,A Hierarchical Deep Architecture and Mini-batch Selection Method for Joint Traffic Sign and Light Detection,IEEE,Conferences,"Traffic light and sign detectors on autonomous cars are integral for road scene perception. The literature is abundant with deep learning networks that detect either lights or signs, not both, which makes them unsuitable for real-life deployment due to the limited graphics processing unit (GPU) memory and power available on embedded systems. The root cause of this issue is that no public dataset contains both traffic light and sign labels, which leads to difficulties in developing a joint detection framework. We present a deep hierarchical architecture in conjunction with a mini-batch proposal selection mechanism that allows a network to detect both traffic lights and signs from training on separate traffic light and sign datasets. Our method solves the overlapping issue where instances from one dataset are not labelled in the other dataset. We are the first to present a network that performs joint detection on traffic lights and signs. We measure our network on the Tsinghua-Tencent 100K benchmark for traffic sign detection and the Bosch Small Traffic Lights benchmark for traffic light detection and show it outperforms the existing Bosch Small Traffic light state-of-the-art method. We focus on autonomous car deployment and show our network is more suitable than others because of its low memory footprint and real-time image processing time. Qualitative results can be viewed at https://youtu.be/ YmogPzBXOw.",https://ieeexplore.ieee.org/document/8575742/,2018 15th Conference on Computer and Robot Vision (CRV),8-10 May 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRC.2019.00073,Deep Grid Net (DGN): A Deep Learning System for Real-Time Driving Context Understanding,IEEE,Conferences,"Grid maps obtained from fused sensory information are nowadays among the most popular approaches for motion planning for autonomous driving cars. In this paper, we introduce Deep Grid Net (DGN), deep learning (DL) system designed for understanding the context in which an autonomous car is driving. DGN incorporates a learned driving environment representation based on Occupancy Grids (OG) obtained from raw Lidar data and constructed on top of the Dempster-Shafer (DS) theory. The predicted driving context is further used for switching between different driving strategies implemented within EB robinos, Elektrobit's Autonomous Driving (AD) software platform. Based on genetic algorithms (GAs), we also propose a neuroevolutionary approach for learning the tuning hyperparameters of DGN. The performance of the proposed deep network has been evaluated against similar competing driving context estimation classifiers.",https://ieeexplore.ieee.org/document/8675636/,2019 Third IEEE International Conference on Robotic Computing (IRC),25-27 Feb. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE.2018.8326285,Deep learning in low-power stereo vision accelerator for automotive,IEEE,Conferences,"Various forms of Convolutional Neural Network (CNN) architectures are used as Deep Learning (DL) tools for learning the similarity measure on video patches in order to run the stereo matching algorithm - the most computationally intensive stage of the pipeline for the stereo vision function used in designing an autonomous car. We propose a hybrid system implementation of the algorithm for real-time, low-power and high-temperature environment. The accelerator part of our system is a programmable many-core system with a Map-Reduce Architecture. The paper describes and evaluates the proposed accelerator for different versions of the stereo matching algorithm.",https://ieeexplore.ieee.org/document/8326285/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS48674.2020.9213884,Image-Based Sense and Avoid of Small Scale UAV Using Deep Learning Approach,IEEE,Conferences,"Distance detection of target object is an important information for obstacle avoidance in many fields, such as autonomous car. When the distance of the obstacle is calculated, one can determine the potential risk of collision. In this paper, a monocular camera was utilized to get the distance from an incoming unmanned aerial vehicle (UAV) using deep learning approach. The distance detection of an UAV using You Only Look Once (YOLO) object detector was proposed in this study. The region which contain the detected UAV was processed into 100 by 100 pixel and was input into the proposed model to estimate the distance of the target object. For the proposed model, a Convolutional Neural Network (CNN) was adopted to solve the regression problem. First, the feature extraction based on VGG network was performed, and then its results was applied to the distance network to estimate distance. Finally, Kalman filter was used to improve the object tracking when YOLO detector is not able to detect UAV and to smooth the estimated distance. The proposed model was trained only by using synthetic images from animation software and was validated by using both synthetic and real flight videos.",https://ieeexplore.ieee.org/document/9213884/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49033.2022.9700725,Mitigating Location-based Attacks Using Predication Models in Vehicular Ad-Hoc Networks,IEEE,Conferences,"The modern world is constantly in a state of technological revolution. Everyday new technological ideas, inventions, and threats emerge. With modern computer software and hardware advancements, we have the emergence of the Internet of Things (IoT). In conjunction, modern car companies have a push from public demand for a fully-autonomous car. To accomplish autonomy, small, and secure Vehicular Ad-Hoc Networks (VANETs) it is necessary to ensure that the systems that rely on connected vehicle data is reliable and accurate. In the event there is a malicious actor manipulating the data through replica and injection attacks or there is a hardware failure yielding inaccurate location information, it is necessary to explore efficient methods for predicting connected vehicles locations such that these systems, which rely on accurate information are not impacted. This study analyzes multiple clustering and prediction models to discover how effectively a multi-layered machine learning approach is able to meet the real-time requirement of future generation smart cities.",https://ieeexplore.ieee.org/document/9700725/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS50221.2020.9268343,Robust Traffic Light Detection and Classification Under Day and Night Conditions,IEEE,Conferences,"Recently, traffic light detection and classification systems have been studied and developed to build an autonomous car by many research institutes, universities, and companies. However, the results of existing traffic light detection systems are still not stable under day and night conditions. It is difficult to detect the location of traffic light due to their small size. Moreover, traffic lights&#x2019; shapes are also similar to advertisement lights in a city road. Therefore, this paper proposed a new approach to improve the performance of existing traffic light detection systems by using the benefits of hand-crafted features and deep learning techniques. Experimental results show that the proposed system obtained the detection rate of 80% under night conditions, while the color-based density method only got the detection rate of 50.43% under night conditions.",https://ieeexplore.ieee.org/document/9268343/,"2020 20th International Conference on Control, Automation and Systems (ICCAS)",13-16 Oct. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIS50930.2021.9396046,SSLA Based Traffic Sign and Lane Detection for Autonomous cars,IEEE,Conferences,"The Self-Driving Cars are also known as Autonomous Vehicles. This Car has the ability to sense around the environment. These sensed parameters are processed and according to it the different actuators in the car will work without any human involvement. An Autonomous car work like a normal car but without any human driver. Autonomous cars rely on sensors, actuators, machine learning algorithms and Software to perform all the Automated Functions. The Software part is very important for Autonomous vehicles. The Software architecture acts as a bridge between Hardware Components and Application. The Standardized Software for Automotive cars is AUTOSAR. The AUTOSAR is a Standardized Architecture between Application Software and Hardware. This Standardized Architecture provide all Communication Interfaces, Device Drivers, Basic Software and Run-Time Environment. There are two important modules in Self-Driving Cars. They are Lane Detection and Traffic Signal detection which works automatically without any Human Intervention. A Machine Learning Algorithm is proposed in this paper. This Algorithm is mainly used to train the shape models and helps to detect the shape for Traffic Sign detection and Lane Detection. These both tasks are programmed using python with Open cv2 library file, numpy library file and Hough Detection technique is used to detect the appropriate circles of the traffic signals.By using all these tools, all the shape models are trained using Supervised training Algorithm and the detection is performed in such a way to help Autonomous cars to detect the lane and traffic Sign.",https://ieeexplore.ieee.org/document/9396046/,2021 International Conference on Artificial Intelligence and Smart Systems (ICAIS),25-27 March 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INMIC.2018.8595684,Self-Driving Cars Using CNN and Q-Learning,IEEE,Conferences,"DrivingMatter is an experiment carried out to understand the deeper side of an autonomous car. In 1900s, idea was to drive car on Moon from Earth. This was initial motivation which grew from there and now expanding to complex system of roads in the real world. A book-sized Raspberry Pi based autonomous car is built to carry out the experiment on hardware. Software side was accomplished by developing a Python based library for controlling and communicating with car over a network or locally within the car. For environment learning two methodologies are practiced; Supervised learning: Drove the car on an environment/road and collected 3, 000+ data-points. Based on this a CNN model was trained which achieved 73 % test 89 % train accuracy. Reinforcement learning: Car is trained for three different road signs; Stop, No left, and Traffic light using DQN with existing CNN model. These road signs are detected in the environment using OpenCV cascade classifiers.",https://ieeexplore.ieee.org/document/8595684/,2018 IEEE 21st International Multi-Topic Conference (INMIC),1-2 Nov. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2015.291,Sensor-Based Control with Digital Maps Association for Global Navigation: A Real Application for Autonomous Vehicles,IEEE,Conferences,"This paper presents a sensor-based control strategy applied in the global navigation of autonomous vehicles in urban environments. Typically, sensor-based control performs local navigation tasks regarding some features perceived from the environment. However, when there is more than one possibility to go, like in road intersection, the vehicle control fails to accomplish its global navigation. In order to solve this problem, we propose the vehicle global navigation based on a topological representation of the environment using only digital maps and low cost sensors. The approach was developed for two main tasks: road following and road intersection maneuvers. The final solution was completely implemented in a real autonomous car and tested in a challenge circuit, showing the viability of our solution.",https://ieeexplore.ieee.org/document/7313383/,2015 IEEE 18th International Conference on Intelligent Transportation Systems,15-18 Sept. 2015,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UEMCON.2018.8796749,Smart City Software Revolution - Blackboard Systems for Smart City Solutions,IEEE,Conferences,"This paper examines the possibilities and requirements of using the blackboard software system in the context of a smart city. Using Unreal Engine 4, a virtual smart city was developed to test the capabilities and difficulties which would accompany the use of the blackboard. A number of simulations were run to generate insight regarding the viability of implementing the blackboard in a real-world setting. The tasks that the blackboard attempted addressed the following goals: a weather reaction system; a parking space availability system; and a daylight reaction system. An autonomous car running on preset routes was implemented in two separate ways to compare the performance between a blackboard system and blueprints.",https://ieeexplore.ieee.org/document/8796749/,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",8-10 Nov. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/InertialSensors.2018.8577143,Visual lnertial Hybridization Technique based on Beacons identified by Deep Learning,IEEE,Conferences,"Safran has been working for several years on autonomy of vehicles. Whether it is airborne with the UAV Patroller, or on the ground with the military vehicle eRider and with the civilian autonomous car in cooperation with Valeo. This paper focuses on the use of visual information to improve the localization of the car, more precisely, it presents, from a theoretical point of view, the hybridization of the inertial sensors measurement with the line of sight of known position visual beacons: road signs detected on the camera image thanks to deep learning. This fusion algorithm has been implemented in a prototype version of Safran's well know Epsilon 10 navigator and simulation results as well as first real-time results are presented.",https://ieeexplore.ieee.org/document/8577143/,2018 DGON Inertial Sensors and Systems (ISS),11-12 Sept. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3125503.3125568,Work-in-progress: testing autonomous cyber-physical systems using fuzzing features from convolutional neural networks,IEEE,Conferences,Autonomous cyber-physical systems rely on modern machine learning methods such as deep neural networks to control their interactions with the physical world. Testing of such intelligent cyberphysical systems is a challenge due to the huge state space associated with high-resolution visual sensory inputs. We demonstrate how fuzzing the input using patterns obtained from the convolutional filters of an unrelated convolutional neural network can be used to test computer vision algorithms implemented in intelligent cyber-physical systems. Our method discovers interesting counterexamples to a pedestrian detection algorithm implemented in the popular OpenCV library. Our approach also unearths counterexamples to the correct behavior of an autonomous car similar to NVIDIA's end-to-end self-driving deep neural net running on the Udacity open-source simulator.,https://ieeexplore.ieee.org/document/8094374/,2017 International Conference on Embedded Software (EMSOFT),15-20 Oct. 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2021.3063992,Real-Time Path Planning With Virtual Magnetic Fields,IEEE,Journals,"Humans and animals have learned or evolved to use magnetic fields for navigation. Knowing how to model and estimate these fields can be used for motion planning. However, computing the propagation of electromagnetic fields in a given environment requires solving complex differential equations with advanced numerical methods, and therefore it is not suitable for real-time decision making. In this latter, we present a real-time approximator for Maxwell's equations based on deep neural networks that predicts the distribution of a virtual magnetic field. We show how our approximator can be used to perform autonomous 2D navigation tasks, outperforming state-of-the-art navigation algorithms, ensuring completeness, and providing a near-optimal path up to 200 times per second without any post processing stage. We demonstrate the effectiveness of our method with physics-based simulations of an unmanned aerial vehicle, an autonomous car, as well as real-world experiments using a small off-road autonomous racing vehicle. Furthermore, we show how the approach can be applied to multi-robot systems, video game technology, and can be extended to 3D problems.",https://ieeexplore.ieee.org/document/9369851/,IEEE Robotics and Automation Letters,April 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSOSE.2017.7994953,Autonomous decision making for a driver-less car,IEEE,Conferences,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome.",https://ieeexplore.ieee.org/document/7994953/,2017 12th System of Systems Engineering Conference (SoSE),18-21 June 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/IConAC.2018.8748986,"Improvement of Driverless Cars' Passengers on Board Health and Safety, using Low-Cost Real-Time Heart Rate Monitoring System",IEEE,Conferences,"In this work, a real-time unobtrusive heart rate monitoring system is proposed and implemented. The proposed system aims to monitor the heart rate of the passengers by using a low-cost camera, which can be readily embedded in the car's rear-view mirror. Additionally, we integrate this system with the main system of our test driverless car, and we propose how driverless cars should act in response to serious medical emergency situations. Moreover, we investigate how this system can benefit from the promising features of Google I/O and Google AI. Our approach is based on Remote Photoplethysmography (rPPG), in which the heart rate is extracted from the subtle tiny changes occurring in the skin color of the face during every pulsation. The face is automatically detected and tracked, then the raw signal is calculated from each frame over a 10-seconds sliding window. After that, a series of signal processing techniques are implemented on the raw signals to recover the heart rate frequency. Finally, the resultant heart rate measurements are processed and stored, then we compare it with ground truth measurements values obtained using pulse oximeter.",https://ieeexplore.ieee.org/document/8748986/,2018 24th International Conference on Automation and Computing (ICAC),6-7 Sept. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AFRICON46755.2019.9134033,A Software Agent for Vehicle Driver Modeling,IEEE,Conferences,"The world is experiencing a paradigm shift towards intelligent agents in form of machine learning for modeling any given task or process. Human vehicle drivers are agents that operate under stochastic environments, full of other agents. Such environments are complex to perceive and model. This study explores how a utility-based agent could be used to model human vehicle drivers. The motivation behind the study was established on the assumption that a driver agent founded on GPS data, Mixture Models and probabilistic reasoning methodologies could effectively model human vehicle drivers. The data collected by GPS receivers was appropriately analysed to establish a driver behaviour dataset. The dataset was then divided into three sets: training, test and validation sets that were used to formulate the driver agent. The agent's successive actions were evaluated against sets of performance metrics to determine accuracy, precision and recall levels. The evaluation yielded over 50% successful performance rates at all levels. The significance of the study is four-fold: First, the function of the system could be extended to providing advisory services to drivers in real-time. Second, data gathered from the system could be used by road safety stakeholders to vet drivers and to diagnose causes of road accidents. Thirdly, the resulting knowledge-base could establish standards of rationality in driving and/or formulate rules for use in driverless vehicle control systems. Finally, the model could be used to build a dataset on driver behaviour for any given vehicle driver and type and nature of operational environment.",https://ieeexplore.ieee.org/document/9134033/,2019 IEEE AFRICON,25-27 Sept. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNC.2019.8886037,Pedestrian Detection for Autonomous Driving within Cooperative Communication System,IEEE,Conferences,"The ability to perceive and understand surrounding road-users behaviors is crucial for self-driving vehicles to correctly plan reliable reactions. Computer vision that relies mostly on machine learning techniques enables autonomous vehicles to perform several required tasks such as pedestrian detection. Furthermore, within a fully autonomous driving environment, driverless vehicle has to communicate and share perceived data with its neighboring vehicles for more safe navigation. In this context, our paper proposes a warning notification diffusion solution related to real-time pedestrian presence detection, through an inter-vehicle communication system. To achieve this purpose, pedestrian and vehicle recognition is required. Thus, we implemented intended detectors. We used Histogram of Oriented Gradients (HOG) descriptor with the linear Support Vector Machine (SVM) classifier for the pedestrian detector, and Haar feature-based cascade classifier to reach vehicle detection. The performance evaluation of our solution leads to fairly good detection accuracy around 90% for pedestrian and 88% for vehicle.",https://ieeexplore.ieee.org/document/8886037/,2019 IEEE Wireless Communications and Networking Conference (WCNC),15-18 April 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828057,Deep Reinforcement Learning Based Tracking Control of Unmanned Vehicle with Safety Guarantee,IEEE,Conferences,"It is well known that the development of efficient real-time path following strategy and collision avoidance mechanism is critical to the practical implementation of autonomous driving technique. Within this context, this paper presents a new kind of hybrid control strategy consisting of the robot Stanley's trajectory tracking algorithm [1] and deep reinforcement learning (DRL) technique to achieve the goal of tracking control of unmanned vehicle with safety guarantee. By introducing the DRL technique, the tracking accuracy of the robot Stanley's trajectory tracking algorithm is improved and a safe control algorithm with collision avoidance is obtained. Furthermore, the complexity of the learning algorithm involved in the tracking controller is significantly reduced by using the Stanley's trajectory tracking algorithm, which makes the learning converge fast. Finally, numerical simulations are performed to verify that the proposed tracking algorithm has obviously advantages on tracking accuracy and training efficiency over some existing ones.",https://ieeexplore.ieee.org/document/9828057/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/FRUCT52173.2021.9435505,Development of the Detecting System of the Landmark Tags to Increase the Navigation Accuracy of an Unmanned Vehicle in a Known Location,IEEE,Conferences,This paper proposes the landmark detection system for use in unmanned vehicle based on NVIDIA Jetson embedded device. The paper describes the synthetic dataset generation and training YOLOv4 neural network using data augmentation and transfer learning techniques to detect the landmark in incomplete data conditions. The real-time landmark detection module was implemented on the NVIDIA Jetson Xavier. The final mean average precision was resulted as 83.8%.,https://ieeexplore.ieee.org/document/9435505/,2021 29th Conference of Open Innovations Association (FRUCT),12-14 May 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACC.2016.7525529,A Supervised Adaptive Learning-based Fuzzy Controller for a non-linear vehicle system using Neural Network Identification,IEEE,Conferences,"In this paper, a Supervised Adaptive Learning-based Fuzzy Controller (ALFC) with Neural Network Identification and Convex Parameterization is designed to identify and control the unmanned vehicle in an autonomous parking system. The objective is to achieve robust learning and control while maintaining a low implementation cost. The proposed algorithm design incorporates the following learning and control theorems - non-linear system identification using neural network, fuzzy logic, supervised adaptive learning as well as multiple model based convex parameterization. To demonstrate the algorithm in a more straight forward manner, we are using a real nonlinear unmanned autonomous driving system as an example to apply the algorithm and showing the superior performance of controller. In the autonomous driving system, the proposed method can be used for both estimating and further controlling a desired vehicle speed and steering wheel turning. With a supervised adaptive learning-based method, robustness can be also assured under various operating environments regardless of unpredictable disturbances. The convex parameterization further improves the speed of convergence of the adaptive learning process for the Fuzzy controller by using the multiple models concept. Last but not least, comparative experiments have also demonstrated that systems equipped with the new algorithm are able to achieve faster and smoother convergence.",https://ieeexplore.ieee.org/document/7525529/,2016 American Control Conference (ACC),6-8 July 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2018.8396807,Learning safe recovery trajectories with deep neural networks for unmanned aerial vehicles,IEEE,Conferences,"Unmanned vehicles that use vision sensors for perception to aid autonomous flight are a highly popular area of research. However, these systems are often prone to failures that are often hard to model. Previous work has focused on using deep learning to detect these failures. In this work, we build on these failure detection systems and develop a pipeline that learns to identify the correct trajectory to execute that restores the vision system and the unmanned vehicle to a safe state. The key challenge with using a deep learning pipeline for this problem is the limited amount of training data available from a real world system. Ideally one requires millions of data points to sufficiently train a model from scratch. However, this is not feasible for an unmanned aerial vehicle. The dataset we operate with is limited to 400-500 points. To sufficiently learn from such a small dataset we leverage the idea of transfer learning and non linear dimensionality reduction. We deploy our pipeline on an unmanned aerial vehicle flying autonomously through outdoor clutter (in a GPS denied environment) and show that we are able to achieve long durations of safe autonomous flight.",https://ieeexplore.ieee.org/document/8396807/,2018 IEEE Aerospace Conference,3-10 March 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICCS53718.2022.9788210,Machine Learning in 6G: The Future of Wireless Communication,IEEE,Conferences,"The 6<sup>th</sup> Generation of wireless communication, 6G, is the future of wireless communication technology. 6G has the potential to support various pioneering applications. Post Covid-19 pandemic, the necessity of remote data-connectivity is recognized as a socio-economic requirement. Data-centric communication has been one of the key-features in 5G and the trend is expected to continue in near future. 6G proposes to support various cutting-edge applications, including remote healthcare, holographic transportation, twin body area network, unmanned vehicle, smart infrastructures and augmented and virtual reality. These applications require a superior data-rate and infrastructure beyond the current 5G facilities. Hence, currently, both industry and academia are exploring the possible technology solutions that can support the advanced communication requirements of near future. This research work intends to provide a comprehensive summary of the Key Performance Indicators (KPI) of 6G, possible 6G network architectures, use cases and enabling technologies. Additionally, an extensive summary of the Machine Learning (ML) algorithms implemented in 6G are provided to achieve various technology goals.",https://ieeexplore.ieee.org/document/9788210/,2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS),25-27 May 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCID.2016.1055,Path Planning for UUV in Dynamic Environment,IEEE,Conferences,"From naval operations to ocean science missions, the importance of autonomous vehicles is increasing with the advances in underwater robotics technology. Due to the dynamic and intermittent underwater environment and physical limitations of underwater unmanned vehicle (UUV), feasible and optimal path planning is crucial for autonomous underwater operations. According to different mission, the path planning method of UUV is divided into two categories: the point to point path planning and the complete coverage path planning. The objective of this thesis is to develop and demonstrate an efficient underwater path planning method that is adapted to complicated ocean environment. In this thesis, existing path planning method for the fields of ocean science and robotics are first reviewed, and then local dynamic obstacle avoidance method is proposed to avoid dynamic obstacles. Based on this again, the path planning of UUV in local dynamic environment can be efficiently implemented by adopting rolling window path planning method and local dynamic obstacle avoidance method. This method with the guide point strategy combines global path planning with local dynamic path planning, so that not only the requirements of real-time on-line path planning for UUV are met, the global optimality is also considered. A navigation route for UUV is planned in advance by using priori environmental information based on ant colony algorithm, so it provides the reference information for the selection of guide point. In order to solved the problem of area coverage search, a complete coverage path planning method is proposed by combining ant colony algorithm with biologically inspires neural network. In order to demonstrate underwater path planning method, all of the above ideas and methods developed were tested in simulation experiments.",https://ieeexplore.ieee.org/document/7830329/,2016 9th International Symposium on Computational Intelligence and Design (ISCID),10-11 Dec. 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CICA.2009.4982774,Tutorial CICA-T Computing with intelligence for identification and control of nonlinear systems,IEEE,Conferences,"System characterization and identification are fundamental problems in systems theory and play a major role in the design of controllers. System identification and nonlinear control has been proposed and implemented using intelligent systems such as neural networks, fuzzy logic, reinforcement learning, artificial immune system and many others using inverse models, direct/indirect adaptive, or cloning a linear controller. Adaptive Critic Designs (ACDs) are neural networks capable of optimization over time under conditions of noise and uncertainty. The ACD technique develops optimal control laws using two networks - critic and action. There are merits for each approach adopted will be presented. The primary aim of this tutorial is to provide control and system engineers/researchers from industry/academia, new to the field of computational intelligence with the fundamentals required to benefit from and contribute to the rapidly growing field of computational intelligence and its real world applications, including identification and control of power and energy systems, unmanned vehicle navigation, signal and image processing, and evolvable and adaptive hardware systems.",https://ieeexplore.ieee.org/document/4982774/,2009 IEEE Symposium on Computational Intelligence in Control and Automation,30 March-2 April 2009,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3061634,RnR: Retrieval and Reprojection Learning Model for Camera Localization,IEEE,Journals,"Camera localization is an essential technique in many applications, such as robot navigation, mixed reality, and unmanned vehicle. We are committed to solving the problem of predicting the 6-DoF pose of cameras from a single color image in a given three-Dimensional (3D) environment. In this paper, we proposed a robust learning model for it. Basically, our proposed methodology consists of two steps: image retrieval and space reprojection. The former is in charge of simultaneous localization and mapping based on pre-captured reference images that rely on the correspondence between pixel points and scene coordinates; whereas the latter carries out camera calibration between the 2D image plane and the 3D scene. Given a two-Dimensional (2D) image, the initial localization is accomplished rapidly by matching a reference image using Siamese networks. More precise localization is achieved by camera calibration between the 2D image and the 3D scene using a fully convolutional network. The experimental results on the public dataset show that our model is more robust and expandable than the previous methods. At the end of this paper, we also apply the system to Unmanned Aerial Vehicle (UAV) localization and achieve good results.",https://ieeexplore.ieee.org/document/9361658/,IEEE Access,2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC.2011.6030219,A redeployment strategy based on Unmanned Aerial Vehicle in wireless sensor network,IEEE,Conferences,"In reality, holes in large-scale sensor networks are exist due to various reasons, such as energy depletion and randomness deployment. These holes may degrade the detection performance of the entire sensor networks. Based on the probabilistic detection model with false alarm rate, this paper proposes a new redeployment method. A method of using an Unmanned Aerial Vehicle (UAV) was proposed for boundaries of holes detection. In particular, in this paper, the contour graph was used for redeploy the holes based on depth-first strategy. According to the simulation results, the proposed method can attain better coverage rate.",https://ieeexplore.ieee.org/document/6030219/,2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference,20-22 Aug. 2011,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE2T.2017.8215979,Application framework for forest surveillance and data acquisition using unmanned aerial vehicle system,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and targeted to be easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8215979/,2017 International Conference on Engineering Technology and Technopreneurship (ICE2T),18-20 Sept. 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UVS.2019.8658283,Automatic Fault Detection of Power Lines using Unmanned Aerial Vehicle (UAV),IEEE,Conferences,"Safety and automation are the two major challenges in the application of Unmanned Aerial Vehicle (UAV), commonly known as drone, to the power lines inspection and fault detection. While current state-of-the-art UAVs are equipped with collision avoidance features, there is less attention to the automatic and real-time fault detection of power lines using UAVs. This paper presents the architecture of three drone-oriented concept designs for automatic and real-time fault detection of power lines using UAVs. The proposed systems could be potential candidates for replacing traditional inspection methods of power lines, which are risky and costly. By incorporating a robust neural network, i.e., Artificial Intelligence (AI), and using appropriate and efficient sensors, the systems can automatically detect various faults and defects on power lines with high precision. We propose three concept design options comprised of different hardware/software components and their feasibility factors. For instance, FLIR Duo Pro R as a thermal sensor and Zenmuse XT for thermal vision have been proposed to be used in the concept designs. For data communication, the proposed designs use cloud-based virtual private network (VPN) for a secure connection between remote control (RC) of the UAV and the server. Based on the advantages and disadvantages of the three proposed design options, the most efficient design is also discussed. This design proposes a system with lightweight sensors, which could increase the flight time of the UAV. Further, the AI interface is coded on to the RC, making it economical, without any database for big data storage. The back-end of the neural network is stored in a cloud server. With the help of GSM antenna, the AI can run on the tablet if there is an available cellular network.",https://ieeexplore.ieee.org/document/8658283/,2019 1st International Conference on Unmanned Vehicle Systems-Oman (UVS),5-7 Feb. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon48659.2022.9764015,Biosensors Based Controller for Small Unmanned Aerial Vehicle Navigation,IEEE,Conferences,"This work describes a system that uses electromyography (EMG) signals obtained from muscle sensors and an Artificial Neural Network (ANN) for signal classification and pattern recognition that is used to control a small unmanned aerial vehicle using specific arm movements. The main objective of this endeavor is the development an intelligent interface that allows the user to control the flight of a drone beyond direct manual control. The biosensors used in this work were the MyoWare Muscle sensors which contain two EMG electrodes and were used to collect signals from the posterior (extensor) and anterior (flexor) forearm, and the bicep. The collection of the raw signals from each sensor were performed using an Arduino Uno. Data processing algorithms were developed with the purpose of classifying the signals generated by the arm&#x2019;s muscles when performing specific movements, namely: flexing, resting, arm-up, and arm motion from right to left. With these arm motions, roll control of the drone was achieved. MATLAB software was utilized to condition the signals and prepare them for the classification stage. To generate the input vector for the ANN and perform the classification, the root mean squared, and the standard deviation were processed for the signals from each electrode. The neuromuscular information was trained using an ANN with a single 10 neurons hidden layer to categorize the four targets. The result of the classification shows that an accuracy of 97.5% was obtained for a single user. Afterwards, classification results were used to generate the appropriate control signals from the computer to the drone through a Wi-Fi network connection. These procedures were successfully tested, where the drone responded successfully in real time to the commanded inputs.",https://ieeexplore.ieee.org/document/9764015/,SoutheastCon 2022,26 March-3 April 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2018.8628656,Brain Emotional Learning-Based Path Planning and Intelligent Control Co-Design for Unmanned Aerial Vehicle in Presence of System Uncertainties and Dynamic Environment,IEEE,Conferences,"This paper proposes a novel intelligent path planning and control co-design for Unmanned Aerial Vehicles (UAVs) in the presence of system uncertainties and dynamic environments. In order to simultaneously handle the uncertainties from both the UAV platform itself and from the environment, a novel biologically-inspired approach based on a computational model of emotional learning in mammalian limbic system is adopted. The methodology, known as Brain Emotional Learning (BEL), is implemented in this application for the first time. Making use of the multi-objective properties and the real-time learning capabilities of BEL, the path planning and control co-design are applied in a synthetic UAV path planning scenario, successfully dealing with the challenges caused by system uncertainties and dynamic environments. A Lyapunov analysis demonstrates the convergence of the co-design, and a set of numerical results illustrate the effectiveness of the proposed approach. Furthermore, it is shown that the low computational complexity of the method guarantees its implementation in real-time applications.",https://ieeexplore.ieee.org/document/8628656/,2018 IEEE Symposium Series on Computational Intelligence (SSCI),18-21 Nov. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TSCZh55469.2022.9802480,Comparative Analysis of Anomaly Detection Algorithms for Development of the Unmanned Aerial Vehicle Power Plant Digital Twin,IEEE,Conferences,"This article considers the capabilities of machine learning algorithms in the problem of anomaly detection, analysis of their effectiveness in simultaneous processing of several telemetric data streams. A variant of constructing an algorithm using the Hierarchical Temporal Memory for the development of the power plant digital twin of a medium-altitude long-endurance unmanned aerial vehicle is considered. In terms of their basic characteristics (geometric, weight) such aerial vehicle is considered correspond to the light aircraft class. In particular, a modification of the algorithm for analyzing of telemetric information received from sensors of the on-board measurement system is considered.",https://ieeexplore.ieee.org/document/9802480/,2022 XIX Technical Scientific Conference on Aviation Dedicated to the Memory of N.E. Zhukovsky (TSCZh),14-15 April 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.475,Double Unmanned Aerial Vehicle's Path Planning for Scout via Cross-Entropy Method,IEEE,Conferences,"Cross-entropy has been recently applied to combinatorial optimization problems with satisfying results. This paper introduce the cross-entropy method theory, the way of model making, real time and robust, and the application, in military scout , Unmanned Aerial Vehicle(UAV) is a special tool, with so many advantage in real time and global space. A new method, based on cross-entropy method, was used for the optimal way of double UAV's path planning.",https://ieeexplore.ieee.org/document/4287760/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2018.8679050,Ground Control Station for an Unmanned Aerial Vehicle Integrated in IoT,IEEE,Conferences,The paper presents the Murros Unmanned Aerial System (UAS) and its integration in IoT. Murros UAS is a multisensorial robotic system used for aerial monitoring of wireless sensors network in precision agriculture. This document details the software and hardware architecture of Murros UAS Ground Control Station and common situations when we are operating in an over-the-internet configuration. Experimental results are obtained from real missions.,https://ieeexplore.ieee.org/document/8679050/,"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",28-30 June 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE49325.2020.9200107,Inspection of Concrete Structures by a Computer Vision Technique and an Unmanned Aerial Vehicle,IEEE,Conferences,"We have proposed a visual inspection technique for concrete structures using deep learning and a hardware ecosystem, an Unmanned Aerial Vehicle (UAV). The UAV is a quadcopter that can fly to unreachable sections of a site which consists of a camera that captures images of the concrete surfaces via a mobile device and feed the real time images in the CNN model. The images taken from such remote locations may contain different types of surfaces, shadowed regions and surfaces with holes. The cracks are properly detected by the CNN `AlexNet' algorithm and masking with sliding window technique in such conditions due to variation in the image data set. The experimental results were simulated on a standard online data set of 40,000 images of Mendeley Data which is freely available and 3000 images have been chosen from the entire data set for this method. The classes have been divided into 2 categories of `crack' and `no crack' for the proposed method's data set. There are 1050 training images and 450 testing images for each category. Experimental results were achieved on Google Colab cloud service using Python Tensorflow API (Application Programming Interface). The proposed `AlexNet' CNN algorithm achieves 98.4 % accuracy and the model is deployed to a masking technique with sliding window to detect cracks in a 3008×2000 pixel resolution image by breaking the image into 227×227 pixel resolution image patches. The experimental results have proved that the proposed method handles noisy background such as cracks with shadows and stains, cracks on rusty and rough surfaces and minor dimension cracks with good efficiency.",https://ieeexplore.ieee.org/document/9200107/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTCC50638.2020.9259777,Integrated Fault Detection and Diagnosis of an Unmanned Aerial Vehicle using Time Difference of Arrival,IEEE,Conferences,"An integrated approach to the fault detection and diagnosis (FDD) of an unmanned aerial vehicle is presented. A novel approach using the Time Difference Of Arrival (TDOA) principle has been developed to detect, isolate and identify an incipient fault condition in the rotor dynamics. The requirements of a reconfigurable controller (RC) has been taken into account through the real-time implementation of a continuous forward algorithm (CFA) with a golden section search (GSS) combined with a meta-heuristic global optimization technique. The training and testing data for Radial Basis Function Neural Networks (RBF-NN) learning and prediction were supplied in discrete-time and its integration capacity validated through a Hardware-in-the-loop simulation (HILS) using a Teensy 3.6 microcontroller. The Pseudo real-time desktop simulation showed that the FDD algorithm was able to detect and isolate an incipient rotor fault and supply the RC a post-fault model and associated fault uncertainties. This method showed robustness towards prediction errors (bias and variance) and can be used in an integrated fault-tolerant control framework.",https://ieeexplore.ieee.org/document/9259777/,"2020 24th International Conference on System Theory, Control and Computing (ICSTCC)",8-10 Oct. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2017.7991337,Neural network based nonlinear model reference adaptive controller for an unmanned aerial vehicle,IEEE,Conferences,"This paper presents a neural network based model reference adaptive controller (MRAC) for the control of a fixed-wing unmanned aerial vehicle (UAV). An adaptive neural network is trained using the error between the UAV response and the desired response as given by the reference model. The design of a suitable reference model for the desired aircraft performance is investigated and developed. Unknown nonlinearities of the vehicle dynamics not accounted by the reference model are compensated in real-time by the adaptive neural network approximation, which also provides online adaptation during off-nominal flight conditions. The nonlinear dynamics of a twin-engine UAV are modeled in Simulink to test the controller in a software-in-the-loop simulation environment. Simulation results of maneuvers show the feasibility and performance of the proposed controller.",https://ieeexplore.ieee.org/document/7991337/,2017 International Conference on Unmanned Aircraft Systems (ICUAS),13-16 June 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECO.2016.7525773,Neural network implementation of a principal component analysis tasks on board the unmanned aerial vehicle information processing in real time,IEEE,Conferences,"The application of principal component analysis to simplify the classification of pixels of images obtained with the help of unmanned aerial vehicle for the organization received information processing in real time, and simplify the process of orientation of unmanned aerial vehicle on the ground. We consider the neural network of the principal component analysis. The results of experiments showing the effectiveness of such use of principal component analysis.",https://ieeexplore.ieee.org/document/7525773/,2016 5th Mediterranean Conference on Embedded Computing (MECO),12-16 June 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197071,On-board Deep-learning-based Unmanned Aerial Vehicle Fault Cause Detection and Identification,IEEE,Conferences,"With the increase in use of Unmanned Aerial Vehicles (UAVs)/drones, it is important to detect and identify causes of failure in real time for proper recovery from a potential crash-like scenario or post incident forensics analysis. The cause of crash could be either a fault in the sensor/actuator system, a physical damage/attack, or a cyber attack on the drone's software. In this paper, we propose novel architectures based on deep Convolutional and Long Short-Term Memory Neural Networks (CNNs and LSTMs) to detect (via Autoencoder) and classify drone mis-operations based on real-time sensor data. The proposed architectures are able to learn high-level features automatically from the raw sensor data and learn the spatial and temporal dynamics in the sensor data. We validate the proposed deep-learning architectures via simulations and realworld experiments on a drone. Empirical results show that our solution is able to detect (with over 90% accuracy) and classify various types of drone mis-operations (with about 99% accuracy (simulation data) and upto 85% accuracy (experimental data)).",https://ieeexplore.ieee.org/document/9197071/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1357,Small Unmanned Aerial Vehicle visual system for ground moving target positioning,IET,Conferences,"Recently, Small Unmanned Aerial Vehicles (SUAV) are used in a variety of reconnaissance, surveillance, combat applications and so on. Tracking and positioning ground moving targets using a camera mounted on SUAVs has important applications in military and civilian purposes. Although many approaches for a vision-based target positioning system have been developed, most researches are limited to target recognition algorithm or target state estimation. And hardware implementation schemes which can work well with the whole SUAV system are seldom discussed. In order to achieve an available systematic scheme, a complete visual system which incorporates a vision-based ground moving target positioning algorithm is presented in this paper. Experiment results are finally shown and commented. Experiments indicate that the system performs great real time response and high positioning precision.",https://ieeexplore.ieee.org/document/6492964/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IST55454.2022.9827665,Towards 3D printed modular unmanned aerial vehicle development: The landing safety paradigm,IEEE,Conferences,"Unmanned aerial vehicles (UAVs) are at the fore-front of this century&#x0027;s technological shift, becoming ubiquitous in research and market areas. Similarly, nowadays, 3D printing is a fast-emerging, widely used technology that allows individ-uals to design prototypes that fulfil their needs. This paper presents an autonomous UAV designed and implemented to be fully modular and 3D printable. Furthermore, suitable areas for landing are recognized using a lightweight deep learning architecture while a Gazebo model for simulation purposes is also given to the research community. Finally, its fly and surface recognition processes are evaluated exhaustively in real-world and simulation scenarios.",https://ieeexplore.ieee.org/document/9827665/,2022 IEEE International Conference on Imaging Systems and Techniques (IST),21-23 June 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC45564.2020.9147911,Unmanned Aerial Vehicle Angular Velocity Control via Reinforcement Learning in Dimension Reduced Search Spaces,IEEE,Conferences,"Search space dimension reduction strategies are studied for reinforcement learning based angular velocity control of multirotor unmanned aerial vehicles. Reinforcement learning approximates the value function iteratively over the state-action space, which is 6-dimensional in the case of multirotor angular velocity control. An inverse-dynamics approach is applied to reduce the 6-dimensional state-action space to a 3-dimensional state-only search space while estimating the uncertain model parameters. The search space dimension is further reduced when the state variables are only allowed to vary following either a motion camouflage strategy or a hyperbolic tangent path. Simulation results show that the modified reinforcement learning algorithms can be implemented in real time for multirotor angular velocity control.",https://ieeexplore.ieee.org/document/9147911/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC.2017.8190968,Unmanned aerial vehicle surveillance system (UAVSS) for forest surveillance and data acquisition,IEEE,Conferences,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture.",https://ieeexplore.ieee.org/document/8190968/,2017 International Conference on Information and Communication Technology Convergence (ICTC),18-20 Oct. 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2017.2764849,A Vision-Aided Approach to Perching a Bioinspired Unmanned Aerial Vehicle,IEEE,Journals,"This paper presents the implementation of a machine learning approach for replicating highly adaptive avian perching behavior. With full consideration of both the configuration of flying vehicles and perching principles, a bioinspired aerial robot comprising one flight subsystem and one perching subsystem is designed. Based on the real-time landing speed and attitude, a novel type of soft grasping mechanism for dexterous perching is proposed to provide adhesive force and absorb impact force. During the critical perching phase, the dynamics of the perching actuator change with the touchdown conditions and the type of perching target. A hybrid automation of a time-to-contact theory-based attitude controller and a robust self-localization system are utilized to regulate the desired perching maneuvers. The experimental results are provided to attest to the effectiveness of the proposed perching method.",https://ieeexplore.ieee.org/document/8074761/,IEEE Transactions on Industrial Electronics,May 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794446,A Reinforcement Learning Approach for Control of a Nature-Inspired Aerial Vehicle,IEEE,Conferences,"In this work, reinforcement learning is used to develop a position controller for an underactuated nature-inspired Unmanned Aerial Vehicle (UAV). This particular configuration of UAVs achieves lift by spinning its entire body contrary to standard multi-rotors or fixed-wing aircraft. Deep Deterministic Policy Gradients (DDPG) with Ape-X Distributed Prioritized Experience Replay was used to train neural network function approximators that were implemented as the final control policy. The reinforcement learning agent was trained in simulations and directly ported over to real-life hardware. Position control tests were performed on the learned control policy and compared to a baseline PID controller. The learned controller was found to exhibit better control over the inherent oscillations that arise from the non-linear dynamics of the platform.",https://ieeexplore.ieee.org/document/8794446/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2019.8798329,A Vision-based Unmanned Aircraft System for Autonomous Grasp & Transport,IEEE,Conferences,"The progress in sensor technologies, computer capabilities and artificial intelligence has endowed the unmanned aircraft system (UAS) with more autonomous abilities. Motivated by the 6th International Unmanned Aerial Vehicle Innovation Grand Prix (UAVGP), a UAS with high degree of autonomy was developed to perform the mission of building a simulated tower using prefabricated components. According to the requirement of the competition, the UAS was designed and implemented from the following four parts: 1) navigation and control, 2) recognition and location, 3) grasp and construction, and 4) task planning and scheduling. Different levels of autonomy have been given to the UAS based on these parts. The system hardware was developed on a quadrotor platform by integrating various components, including sensors, computers, power and grasp mechanism. Software which included precise navigation, mission planning, real-time perception and control was implemented and integrated with the developed UAS hardware. The performance in the test environment and actual competition showed that the UAS could perform the mission without human intervention with high autonomy and reliability. This paper addresses the major components and development process of the UAS and describes its application to the practical mission.",https://ieeexplore.ieee.org/document/8798329/,2019 International Conference on Unmanned Aircraft Systems (ICUAS),11-14 June 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/URSI-AT-RASC.2018.8471616,A Wireless Method for Drone Identification and Monitoring Using AIS Technology,IEEE,Conferences,"The fast growth of UAV (Unmanned Aerial Vehicle) technology in the last years has allowed to extend the use of these devices in many applications. However, the massive use of drones has alerted many governments about an inadequate usage, mainly in terms of security and terrorism. In regards to this problem, some laws about drone usage have been approved by some countries. Among these laws, it is mandatory that all drones are identified and monitored all the time. In this paper, a wireless prototype to identify drones is proposed. To that end, the AIS (Automatic Identification System) is used to transmit parameters as name, ID, speed or the course of drones. This proposed solution can solve the drone identification problem and even, allows to monitor this device in real time. A prototype of this method has been implemented and tested in a real environment, around many locations in the island of Gran Canaria.",https://ieeexplore.ieee.org/document/8471616/,2018 2nd URSI Atlantic Radio Science Meeting (AT-RASC),28 May-1 June 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AERO.2005.1559516,A scalable learning system for video recognition,IEEE,Conferences,"Learning has become an essential part of many image and video processing systems, but it is not often used as an end-to-end solution. Some of the most successful demonstrations of end-to-end learning have been with convolutional, or shared weight networks. We are interested in how this approach can scale and have developed a flexible framework for implementing and training large scale convolutional networks called Harpo. We present an overview of the Harpo framework and describe a multilevel learning strategy used to optimize convolutional networks for particular features of interest in video data streams. Harpo is designed to exploit reconfigurable hardware to accelerate massively parallel convolutional network components and achieve real-time processing speeds. In this paper, we present initial software experiments which use the system to segment exhaust plumes coming from military vehicles in unmanned aerial vehicle video data",https://ieeexplore.ieee.org/document/1559516/,2005 IEEE Aerospace Conference,5-12 March 2005,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00107,Aggregation Transfer Learning for Multi-Agent Reinforcement learning,IEEE,Conferences,"Multi-agent reinforcement learning is currently mainly used in many real-time strategy games. For example, StarCraft, UAV combat. Multi-agent reinforcement learning algorithms have attracted widespread attention. In large-scale multi-agent environment, there is still the problem of state space explosion. Especially in transfer training, since the network input size is fixed, the existing network structure is difficult to adapt to large-scale scenario transfer training. In this paper, we use aggregation transfer training for multi-agent combat problems from aerial unmanned aerial vehicle (UAV) combat scenarios to extend the small-scale learning to large-scale and complex scenarios. We combine the graph neural network (GNN) with the MADDPG algorithm to process the agent observation with aggregation function and take it as input. It starts training from a small-scale multi-UAV combat scenario, gradually increases the number of UAV. The experimental results indicate that MADDPG methods for multi-agent UAV combat problems trained via aggregation transfer learning are able to reach the target performance more quickly, provide superior performance, compared with ones trained without aggregation transfer learning. The versatility of the confrontation model has also been improved.",https://ieeexplore.ieee.org/document/9696029/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2016.7838775,An image matching system for autonomous UAV navigation based on neural network,IEEE,Conferences,"This paper proposes an image matching system using aerial images, captured in flight time, and aerial geo-referenced images to estimate the Unmanned Aerial Vehicle (UAV) position in a situation of Global Navigation Satellite System (GNSS) failure. The image matching system is based on edge detection in the aerial and geo-referenced image and posterior automatic image registration of these edge-images (position estimation of UAV). The edge detection process is performed by an Artificial Neural Network (ANN), with an optimal architecture. A comparison with Sobel and Canny edge extraction filters is also provided. The automatic image registration is obtained by a cross-correlation process. The ANN optimal architecture is set by the Multiple Particle Collision Algorithm (MPCA). The image matching system was implemented in a low cost/consumption portable computer. The image matching system has been tested on real flight-test data and encouraging results have been obtained. Results using real flight-test data will be presented.",https://ieeexplore.ieee.org/document/7838775/,"2016 14th International Conference on Control, Automation, Robotics and Vision (ICARCV)",13-15 Nov. 2016,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLBDBI54094.2021.00096,Architecture Design of Typical Target Detection and Tracking System in Battlefield Environment,IEEE,Conferences,"Aiming at the characteristics of small targets, few samples, many occlusions, and strong camouflage in the battlefield environment, a target detection and tracking system based on multi-source sensor data is designed. The system is mainly composed of three parts: the unmanned aerial vehicle (UAV) platform, the ground display control platform (GDCP), and the offline algorithm training platform. The offline-trained target detection and tracking algorithm is deployed in the UAV-borne AI processing chip, which can conduct the real-time detection and tracking of typical targets in the battlefield and send the results and attitude information of UAVs to the GDCP. The system can realize real-time detection of typical battlefield targets and tracking control through the GDCP.",https://ieeexplore.ieee.org/document/9730973/,"2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",3-5 Dec. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MED.2007.4433822,Attitude and heading refernce system I-AHRS for the EFIGENIA autonomous unmanned aerial vehicles UAV based on MEMS sensor and a neural network strategy for attitude estimation,IEEE,Conferences,"For the autonomous flight, navigation, guidance and control of the EFIGENIA unmanned aerial vehicle it is essential to have high performance 6-DOF attitude and heading reference system measurements. This paper presents the design and development of a real-time intelligent attitude and heading reference system I-AHRS, as in the hardware, as in the intelligent digital neural network software scheme, analysis, design and construction for the orientation calculation for the EFIGENIA EJ-1B MOZART and the EFIGENIA EJ-2B MARIA autonomous unmanned aerial vehicles UAVs. The EFIGENIA I-AHRS consists of three MEMS accelerometers, three MEMS rate-gyros and three magneto-resistive transducers that send its outputs to a digital neural network in which is possible to develop a strategy for attitude estimation. Additionally it is well known that the Kalman Filter is an option as multi-sensor data fusion and integration, however it has some adaptability limitations. In this paper, FPGA reconfigurable hardware digital neural network architecture is presented and utilized to replace the Kalman Filter in the integration of MEMS IMU inertial sensors signals and the Magneto resistive sensors.",https://ieeexplore.ieee.org/document/4433822/,2007 Mediterranean Conference on Control & Automation,27-29 June 2007,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NICS54270.2021.9701509,Autonomous Detection and Approach Tracking of Moving Ship on the Sea by VTOL UAV based on Deep Learning Technique through Simulated Real-time On-Air Image Acquisitions,IEEE,Conferences,"Real-time detection and tracking moving ships as well as locating the helipad on that ship is still a challenge in critical missions at sea. By using VTOL (Vertical Take-Off and Landing) UAV (Unmanned Aerial Vehicle) types, it allows both the ability to fly approaching the ship and also be able to land vertically on the helipad. This paper proposes a SITL (Software In The Loop) system to verify the automatic detection and tracking of ships moving at sea for the VTOL UAV during the mission and an algorithm to guide VTOL UAV mode selection in the process of approaching and landing on ships moving at sea. On-air images collected from the VTOL UAV&#x2019;s camera in the X-Plane 11 simulation environment are used to train Deep Learning computer vision algorithms. Real-time ship detection algorithm with up to 125 FPS and 96&#x0025; accuracy. From the results of the ship and helipad detection, we propose an algorithm to assist the transition of flight modes of VTOL UAV during the tracking and landing mission on a moving ship at sea.",https://ieeexplore.ieee.org/document/9701509/,2021 8th NAFOSTED Conference on Information and Computer Science (NICS),21-22 Dec. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WorldS4.2019.8904014,Autonomous Drone for Defence Machinery Maintenance and Surveillance,IEEE,Conferences,"This proposed research work focuses on the implementation of an autonomous unmanned aerial vehicle (UAV) which is controlled using a pix hawk flight controller. The Quad Copter is capable of navigating autonomously without any real-time input from the user and also programmed to follow a specified path autonomously. The algorithm enables a control technique by which quad copter is empowered to fly autonomously, trajectory tracking, graceful motion and accurate altitude hold performance. Surveillance and Machinery maintenance application are the primary applications designed for the Defense purposes in the Line Of Control and War-zones. This work is aimed to design a quad copter that will follow a command to fly through specified way points. The deep learning algorithm detects human motions and from data acquired camera and ultrasonic sensors to the cloud. On deviations from the standard protocol which is detected using a Sjcam 5000x elite Camera. Also, here a backup auxiliary mini drone is programmed to eject along with the data stored on the primary drone's memory on an unforeseen calamity or an attack that would damage the primary drone's ability to fly.",https://ieeexplore.ieee.org/document/8904014/,2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4),30-31 July 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UIC-ATC.2017.8397582,Autonomous UAV forced graffiti detection and removal system based on machine learning,IEEE,Conferences,"This paper proposes a smart graffiti clean-up system based on an autonomous Unmanned Aerial Vehicle (UAV) platform. This smart clean system is based on edge detection and machine learning algorithms to realize the detection and tracking of graffiti image in real time. In Graffiti detection, we aim to build a model to detect graffiti on walls which can help navigate the UAV to the correct coordinate and estimate the area of the graffiti. The data set which contain graffiti images are trained using machine learning techniques which will be used for the detections of the graffiti patterns. This will automate the process of detecting the location of the graffiti based on the edge detection technique and the model will be able to estimate the area of the graffiti. To achieve obstacle detection, and collision, a smart navigation approach is also proposed with the help of LiDAR and external camera. The overall graffiti cleanup system contains hardware and software that allow the user to use spray enamel with the reach and scale of an autonomous UAV.",https://ieeexplore.ieee.org/document/8397582/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIS54983.2021.00036,Building Detection in Remote Sensing Image Based on Improved YOLOV5,IEEE,Conferences,"Due to the different seismic design of high and low-rise buildings, the seismic levels are often different in the same intensity area. Using remote sensing to distinguish various height buildings, and then further make targeted measures to reduce earthquake losses at all seismic levels, which can reduce the secondary disasters of earthquakes. Traditional architectural target detection models use artificially designed target features based on architectural contours, which not only require a lot of time and manpower to process the data, but also cause low detection accuracy due to inconsistent manual standards. The target detection model based on deep learning has high detection accuracy. However, for unmanned aerial vehicle (UAV) remote sensing applications where real-time and accuracy requirements are relatively high, the acquired image information is less and the target features are not obvious, cause target detection accuracy to decrease. In order to improve the accuracy and speed of target detection, a building target detection model with Convolutional Block Attention Module (CBAM) is proposed based on the YOLO V5 model, which improves the target classification accuracy and detection speed of building detection. The model is deployed on an automatic disaster monitoring platform to conduct a house environment target detection experiment. The comprehensive detection accuracy can reach 88.5&#x0025;, and the detection speed is 26 frames/s, which meet the real-time and accuracy requirements of high-rise and low-rise building target detection.",https://ieeexplore.ieee.org/document/9701798/,2021 17th International Conference on Computational Intelligence and Security (CIS),19-22 Nov. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM42002.2020.9322556,Caching Placement and Resource Allocation for AR Application in UAV NOMA Networks,IEEE,Conferences,"The cache-enabling unmanned aerial vehicle (UAV) cellular networks with massive access capability supported by non-orthogonal multiple access (NOMA) are investigated in this paper. The delivery of multi-media contents for the mixed augmented reality (AR) and normal multi-media application is assisted by multiple mobile UAV base stations, which cache popular contents for wireless backhaul link traffic offloading. To cope with the dynamic content requests and mobility of users in practical scenarios, the dynamic optimization problem for user association, caching placement of UAVs, real-time deployment of UAVs, and power allocation of NOMA is modeled as a stackelberg game to minimize the long-term content delivery delay. Specifically, the game is decomposed into a leader level problem and a number of follower level problems. A correction mechanism is added in deep reinforcement learning (DRL) to optimize the user association in leader level. A meta actor network is proposed in DRL to jointly optimize the UAVs caching placement, real-time UAVs deployment and power allocation of NOMA in follower level. Then, a dynamic caching placement and resource allocation algorithm based on multi-agent meta deep reinforcement learning is proposed to minimize the long-term content delivery delay. Finally, we demonstrate that the considerable gains are achieved by the proposed algorithm.",https://ieeexplore.ieee.org/document/9322556/,GLOBECOM 2020 - 2020 IEEE Global Communications Conference,7-11 Dec. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DySPAN53946.2021.9677430,Colosseum: Large-Scale Wireless Experimentation Through Hardware-in-the-Loop Network Emulation,IEEE,Conferences,"Colosseum is an open-access and publicly-available large-scale wireless testbed for experimental research via virtualized and softwarized waveforms and protocol stacks on a fully programmable, &#x201C;white-box&#x201D; platform. Through 256 state-of-the-art software-defined radios and a massive channel emulator core, Colosseum can model virtually any scenario, enabling the design, development and testing of solutions at scale in a variety of deployments and channel conditions. These Colosseum radio-frequency scenarios are reproduced through high-fidelity FPGAbased emulation with finite-impulse response filters. Filters model the taps of desired wireless channels and apply them to the signals generated by the radio nodes, faithfully mimicking the conditions of real-world wireless environments. In this paper, we introduce Colosseum as a testbed that is for the first time open to the research community. We describe the architecture of Colosseum and its experimentation and emulation capabilities. We then demonstrate the effectiveness of Colosseum for experimental research at scale through exemplary use cases including prevailing wireless technologies (e.g., cellular and Wi-Fi) in spectrum sharing and unmanned aerial vehicle scenarios. A roadmap for Colosseum future updates concludes the paper.",https://ieeexplore.ieee.org/document/9677430/,2021 IEEE International Symposium on Dynamic Spectrum Access Networks (DySPAN),13-15 Dec. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC51459.2021.9415209,Controller module implementation to reduce interrupt in CNPC uplink,IEEE,Conferences,"The unmanned aerial vehicle is used in diverse area. In order to make more use of the unmanned aerial vehicle, reliable communication system is required. CNPC has been developed to standardize the communication system for unmanned aerial vehicle over 150kg. CNPC uplink should support diverse UAV in TDD. To implement CNPC in real world, operating system and FPGA should be used with the interface between the two. To reduce the use of interrupts in uplink implementations on FPGA, simple controller is designed to generate signals which act as the interrupts whenever other user message is needed. To implement this controller in FPGA, this paper deals with timing diagram for this module.",https://ieeexplore.ieee.org/document/9415209/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636680,DarkLighter: Light Up the Darkness for UAV Tracking,IEEE,Conferences,"Recent years have witnessed the fast evolution and promising performance of the convolutional neural network (CNN)-based trackers, which aim at imitating biological visual systems. However, current CNN-based trackers can hardly generalize well to low-light scenes that are commonly lacked in the existing training set. In indistinguishable night scenarios frequently encountered in unmanned aerial vehicle (UAV) tracking-based applications, the robustness of the state-of-the-art (SOTA) trackers drops significantly. To facilitate aerial tracking in the dark through a general fashion, this work proposes a low-light image enhancer namely DarkLighter, which dedicates to alleviate the impact of poor illumination and noise iteratively. A lightweight map estimation network, i.e., ME-Net, is trained to efficiently estimate illumination maps and noise maps jointly. Experiments are conducted with several SOTA trackers on numerous UAV dark tracking scenes. Exhaustive evaluations demonstrate the reliability and universality of DarkLighter, with high efficiency. Moreover, DarkLighter has further been implemented on a typical UAV system. Real-world tests at night scenes have verified its practicability and dependability.",https://ieeexplore.ieee.org/document/9636680/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTC2020-Fall49728.2020.9348616,Deep Q-Network Based Dynamic Movement Strategy in a UAV-Assisted Network,IEEE,Conferences,"Unmanned aerial vehicle (UAV)-assisted communications is a promising solution to improve the performance of future wireless networks, where UAVs are deployed as base stations for enhancing the quality of service (QoS) provided to ground users when traditional terrestrial base stations are unavailable or not sufficient. An effective framework is proposed in this paper to manage the dynamic movement of multiple unmanned aerial vehicles (UAVs) in response to ground user mobility, with the objective to maximize the sum data rate of the ground users. First, we discuss the relationship between the air-to-ground (A2G) path loss (PL) and the location of UAVs. Then a deep Q-network (DQN) based method is proposed to adjust the locations of UAVs to maximize the sum data rate of the user equipment (UE). Finally, simulation results show that the proposed method is capable of adjusting UAV locations in a real-time condition to improve the QoS of the entire network.",https://ieeexplore.ieee.org/document/9348616/,2020 IEEE 92nd Vehicular Technology Conference (VTC2020-Fall),18 Nov.-16 Dec. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SoftCOM50211.2020.9238313,Deep Semantic Image Segmentation for UAV-UGV Cooperative Path Planning: A Car Park Use Case,IEEE,Conferences,"Navigation of Unmanned Ground Vehicles (UGV) in unknown environments is an active area of research for mobile robotics. A main hindering factor for UGV navigation is the limited range of the on-board sensors that process only restricted areas of the environment at a time. In addition, most existing approaches process sensor information under the assumption of a static environment. This restrains the exploration capability of the UGV especially in time-critical applications such as search and rescue. The cooperation with an Unmanned Aerial Vehicle (UAV) can provide the UGV with an extended perspective of the environment which enables a better-suited path planning solution that can be adjusted on demand. In this work, we propose a UAV-UGV cooperative path planning approach for dynamic environments by performing semantic segmentation on images acquired from the UAV’s view via a deep neural network. The approach is evaluated in a car park scenario, with the goal of providing a path plan to an empty parking space for a ground-based vehicle. The experiments were performed on a created dataset of real-world car park images located in Croatia and Germany, in addition to images from a simulated environment. The segmentation results demonstrate the viability of the proposed approach in producing maps of the dynamic environment on demand and accordingly generating path plans for ground-based vehicles.",https://ieeexplore.ieee.org/document/9238313/,"2020 International Conference on Software, Telecommunications and Computer Networks (SoftCOM)",17-19 Sept. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAAI52226.2021.9508033,Development of Gasoline-Electric Hybrid Propulsion Surveillance and Reconnaissance VTOL UAV,IEEE,Conferences,"Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAV) have been a high potential topic in the aerospace industry during the last decades due to its multirotor and fixed-wing nature of the aircraft. Besides, having the ability to rapidly deploy from a tight airstrip and gathering Intelligence, Surveillance, and Reconnaissance (ISR) information is the best way to be one step ahead of the enemy. In this paper, we present the implementation and development of gasoline-electric hybrid propulsion VTOL Unmanned Aerial vehicle respectively. The Hybrid propulsion VTOL UAV offers image and real-time video transmission to the ground station with fully autonomous control to get the best view of the enemy from the sky. The gasoline-electric hybrid propulsion system provides long flight endurance with efficient power consumption. The fundamentals of the multirotor and the conventional fixed-wing aircraft present the theoretical background of the aircraft. The accomplished design consists of high-performance multirotor motors with an efficient gasoline engine. Furthermore, the control system architecture, avionics, and power distribution system presented with addressing cost-effective trending design techniques. The performance of the system has been improved using commercially off-the-shelf (COTS) hardware.",https://ieeexplore.ieee.org/document/9508033/,"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",21-23 April 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAECT50560.2020.9523700,Edge-Cloud Architectures Using UAVs Dedicated To Industrial IoT Monitoring And Control Applications,IEEE,Conferences,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud.",https://ieeexplore.ieee.org/document/9523700/,2020 International Symposium on Advanced Electrical and Communication Technologies (ISAECT),25-27 Nov. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOBECOM46510.2021.9685905,Energy-aware Control Of UAV-based Wireless Service Provisioning,IEEE,Conferences,"Unmanned aerial vehicle (UAV)-assisted communications have several promising advantages, such as the ability to facilitate on-demand deployment, high flexibility in network reconfiguration, and high chance of having line-of-sight (LoS) communication links. In this paper, we aim to optimize the UAV control for maximizing the UAV&#x0027;s energy efficiency, in which both aerodynamic energy and communication energy are considered while ensuring the communication requirements for each ground terminal (GT) and backhaul link between the UAV and the terrestrial base station (BS). The mobility of the UAV and GTs lead to time-varying channel conditions that make the environment dynamic. We formulate a nonconvex optimization for controlling the UAV considering the practical angle-dependent Rician fading channels between the UAV and GTs, and between the UAV and the terrestrial BS. Traditional optimization approaches are not able to handle the dynamic environment and high complexity of the problem in real-time. We propose to use the Trust Region Policy Optimization (TRPO) method that can improve the performance of the UAV compared to the Deep Deterministic Policy Gradient (DDPG) method in such a dynamic environment as in this paper.",https://ieeexplore.ieee.org/document/9685905/,2021 IEEE Global Communications Conference (GLOBECOM),7-11 Dec. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIPR50011.2020.9425341,Enhancing Network-edge Connectivity and Computation Security in Drone Video Analytics,IEEE,Conferences,"Unmanned Aerial Vehicle (UAV) systems with high-resolution video cameras are used for many operations such as aerial imaging, search and rescue, and precision agriculture. Multi-drone systems operating in Flying Ad Hoc Networks (FANETS) are inherently insecure and require efficient security schemes to defend against cyber-attacks such as e.g., Man-in-the-middle, Replay and Denial of Service attacks. In this paper, we propose a cloud-based, end-to-end security framework viz., ""DroneNet-Sec"" that provides secure network-edge connectivity, and computation security for drone video analytics to defend against common attack vectors in UAV systems. The DroneNet-Sec features a dynamic security scheme that uses machine learning to detect anomaly events and adopts countermeasures for computation security of containerized video analytics tasks. The security scheme comprises of a custom secure packet designed with MAVLink protocol for ensuring data privacy and integrity, without high degradation of the performance in a real-time FANET deployment. We evaluate DroneNet-Sec in a hybrid testbed that synergies simulation and emulation via an open-source network simulator (NS-3) and a research platform for mobile wireless networks (POWDER). Our performance evaluation experiments in our holistic hybrid-testbed show that DroneNet-Sec successfully detects learned anomaly events and effectively protects containerized tasks execution as well as communication in drones video analytics in a light-weight manner.",https://ieeexplore.ieee.org/document/9425341/,2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),13-15 Oct. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETA54173.2021.9726680,Estimating the height of a person from a video sequence,IEEE,Conferences,"The research described in this paper focuses on a development of experimental solution, which estimates human height from a video sequence. The main requirements include portability, low hardware requirements, and accuracy. The paper includes analysis of the algorithms employed for image processing, human detection and methods for estimating height of human. The experimental software solution is implemented in Python programming language. The SSD algorithm was selected to detect the person in the image and the output was later on utilized to calculate the real human height. A calibration object in a form of ArUco marker and its known dimensions became the foundation for calculating the real human height. The experimental testing confirmed the accuracy of the developed solution to be sufficient for the real-world application. The most accurate height, with just a deviation of 0.001 meters, was estimated at a distance of 5 and 8 meters. The approach presented in this paper is also to be used as a complement by newly proposed unmanned aerial vehicle security system.",https://ieeexplore.ieee.org/document/9726680/,2021 19th International Conference on Emerging eLearning Technologies and Applications (ICETA),11-12 Nov. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS48674.2020.9214045,Extensions of the open-source framework Aerostack 3.0 for the development of more interactive flights between UAVs,IEEE,Conferences,"The basis for properly verified R&D works is to provide reliable prototyping tools at three most important stages: computer simulation, laboratory tests and real-world experiments. In the laboratory-limited conditions, particular importance is attributed to the first two stages, especially in the context of the safety development of autonomous flights of unmanned aerial vehicle (UAV) groups in various missions. The open-source framework Aerostack support those needs and its effectiveness has been proven in the International Micro Air Vehicle Indoor Competitions (IMAV 2013, 2016, 2017) and Mohammed Bin Zayed International Robotics Challenge (MBZIRC 2020). In the paper, the exemplary functionalities for the new version of Aerostack Version 3.0 Distribution Sirocco (Aerial robotics framework for the industry), extended additionally with a library of new behaviors, are presented. The mission of UAVs can be developed fast and effectively in order to conduct test flights with real drones in lab, before one will decide to fly autonomously outdoor. The representative results obtained for low-cost AR.Drone 2.0 UAV models in two missions, are presented. The first mission is autonomous patrolling the area by a pair of UAVs, the second - intercepting the intruder in guarded area by the guard UAV.",https://ieeexplore.ieee.org/document/9214045/,2020 International Conference on Unmanned Aircraft Systems (ICUAS),1-4 Sept. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2019.8929613,Fish Detection and Tracking in Pisciculture Environment using Deep Instance Segmentation,IEEE,Conferences,"This study presents a novel approach in detecting and tracking of fish in pisciculture. Pisciculture in general involves challenging tasks of counting and monitoring fish in natural or nature like, man-made habitats such as inland fisheries for breeding, feeding and sorting purposes. These are presently achieved using conventional methods that are inefficient when implemented in large-scale commercial productions. To overcome such difficulties and improve the efficiency of the processes, images of fish and fish seeds are captured in natural murky water habitats through a vision sensor on board an unmanned aerial vehicle (UAV). In this research paper, a deep instance segmentation algorithm called Mask R-CNN along with GOTURN tracking algorithm is employed for real time fish detection and tracking. A comparison study is also carried out (i) fish detection on high resolution images (ii) fish detection on high resolution image multi-region parallel processing (iii) fish detection on high resolution image multi-region parallel processing with tracking. The results are found to be accurate with image multi-region parallel processing along with tracking, with an F1 score of 0.91 at 16 frames per seconds on in-land fishes environment.",https://ieeexplore.ieee.org/document/8929613/,TENCON 2019 - 2019 IEEE Region 10 Conference (TENCON),17-20 Oct. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TELECOM50385.2020.9299566,Forest Monitoring System for Early Fire Detection Based on Convolutional Neural Network and UAV imagery,IEEE,Conferences,"Forest fires are one of the main reasons for environmental degradation. In their early stages, the fires are hard to discover, so a faster and more accurate detection method can help minimize the amount of damage they can inflict. In this paper, we present an approach for autonomous early fire detection, which is based on a system with high degree of reliability and with no need of service or human interaction. To provide the autonomous capabilities to the proposed system, we have developed an object detection method, based on a convolutional neural network, which is presented in the main part of the paper. In order to have a better field of view over the observed area, instead of traditional lookout towers and satellite based monitoring, we use live video feed from an unmanned aerial vehicle (UAV), which patrols over the risky area. To make better predictions on the fire probability, we use not only the optical camera of the UAV, but also an on-board thermal camera. With the help of the software platform Node-RED, we have developed a web-based platform, which can present the acquired data in real-time and can notify the interested parties. The workflow for the development of the web-platform is also described in this paper.",https://ieeexplore.ieee.org/document/9299566/,2020 28th National Conference with International Participation (TELECOM),29-30 Oct. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2019.8762097,Green Mobility Management in UAV-Assisted IoT Based on Dueling DQN,IEEE,Conferences,"In most cases, the batteries of sensor nodes in the Internet of Things (IoT) are usually constrained by size and weight, and are difficult to recharge or replace. In traditional wireless sensor networks, data is transmitted in a multi-hop manner, which may cause the high data transmission delay and unbalanced traffic load. In this paper, an Unmanned Aerial Vehicle (UAV)-assisted IoT architecture is introduced, in which UAV is utilized to achieve low-latency and seamless-coverage acquisition of the sensing data. Furthermore, based on the recent advances on deep reinforcement learning algorithms, considering both data delay requirements and network energy consumption, a real-time flight path planning scheme of the UAV in the dynamic IoT sensor networks has been proposed based on dueling deep Q-network (DQN). Besides, the grid-based method is used to handle the network state modeling, which effectively reduces the complexity of the proposed scheme. Simulation results show that the proposed scheme significantly improves the network performance.",https://ieeexplore.ieee.org/document/8762097/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAICT50021.2020.9172031,Human Target Search and Detection using Autonomous UAV and Deep learning,IEEE,Conferences,"An Unmanned Aerial Vehicle (UAV) is an airborne system or pilotless aircraft which is remotely controlled by a human operator on ground or by an onboard computer such that the vehicle moves autonomously. The range of applications in which UAVs are used is very large. This paper describes the application of developing an autonomous surveillance system using an UAV to identify a given target and/or objects of interest in the terrain over which it flies. Such a system can be used in rescue operations, especially in remote areas where physical access is difficult. It can also be used for military operations, farming or any field where surveillance of a given land area is required. The UAV developed in this work is capable of object detection. A mounted camera is used to give visual feedback, and an onboard processing unit runs image recognition software to identify the target in real time. Optimal algorithms are used to search and find the target from the given search area. After recognition of the target, the UAV can either be used to hold its position so as to have a video feed of the target, or return to its base station once the coordinates have been estimated using GPS modules or relay the GPS location to the base station. This paper describes the implementation of the hardware and software components that lead to the realization of the UAV and the application of object detection. The details of a new search algorithm and an example of object detection is presented . The work presented in this paper is the first part in the attempt to develop a cluster of UAVs meant to work in collaboration to be deployed for search and rescue operations.",https://ieeexplore.ieee.org/document/9172031/,"2020 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",7-8 July 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECSYM.2018.8615503,Implementation of Victims Detection Framework on Post Disaster Scenario,IEEE,Conferences,"Disasters are prone to occur in Indonesia due to geographical factors, such as tectonic plate movements, which can cause an earthquake. Earthquakes are one of the most frequent disasters, they have broad impacts in a short time and are unpredictable. Thus, an extensive search process in a short time is highly critical to determine the victims location. In this paper, a victims detection framework is developed starting from acquiring images using an unmanned aerial vehicle and further processing using convolutional neural network (CNN) to locate victims robustly on post-disaster. Input images are then sent to victim detector dedicated ground station server for further high processing robustly locating the possibility of victims. A simulation system mimicking a real environment is developed to test our framework in real time. A transmission protocol is also developed for effectively transmitting data between the robot and the server. The treatment on the detection process of the victim is different from the normal human detection, some pre-processing stages are applied to increase the variation of the given dataset. An embedded system is used for taking images and additional sensors data, such as location and time using Global Navigation Satellite System.",https://ieeexplore.ieee.org/document/8615503/,2018 International Electronics Symposium on Engineering Technology and Applications (IES-ETA),29-30 Oct. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoPESA54515.2022.9754476,Insulator and Damage Detection and Location Based on YOLOv5,IEEE,Conferences,"In order to quickly and accurately detect and locate the insulator and its damage in the transmission line, by studying the YOLO (you only look once) series target detection algorithm model based on deep learning, a detection method based on YOLOv5 is proposed to achieve fast and accurate detection. By applying the network to the aerial insulator data set for training, the experimental results show that the highest AP (Average Precision) value based on YOLOv5 insulator detection is 96.47%, the highest AP value of insulator damage is 99.17%, and the overall m-AP (mean Average Precision) value is 97.82%. At the same time, YOLOv5s has a higher detection rate, and the real-time detection speed is 43.2FPS (Frames Per Second). The experimental results show that the target detection network based on YOLOv5 series has higher accuracy and faster calculation speed for transmission line insulator detection and damage identification under complex background, at the same time, the lightweight model of YOLOv5s is conducive to the deployment of UAV (Unmanned Aerial Vehicle) end model and improve inspection efficiency.",https://ieeexplore.ieee.org/document/9754476/,2022 International Conference on Power Energy Systems and Applications (ICoPESA),25-27 Feb. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASCC.2015.7244614,Joint unscented Kalman filter for dual estimation in a bifilar pendulum for a small UAV,IEEE,Conferences,"It has always been difficult to accurately estimate the moment of inertia of an object, e.g. an unmanned aerial vehicle (UAV). Whilst various offline estimation methods exist to allow accurate parametric estimation by minimizing an error cost function, they require large memory consumption, high computational effort, and a long convergence time. The initial estimate's accuracy is also vital in attaining convergence. In this paper, a new real time solution to the model identification problem is provided with the use of a Joint Unscented Kalman Filter for dual estimation. The identification procedures can be easily implemented using a microcontroller, a gyroscope sensor, and a simple bifilar pendulum setup. Accuracy, robustness, and convergence speed are achieved.",https://ieeexplore.ieee.org/document/7244614/,2015 10th Asian Control Conference (ASCC),31 May-3 June 2015,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCAI50917.2021.9447518,Lightweight Real-time Object Detection Model for UAV Platform,IEEE,Conferences,"Real-time detecting objects on captured images on UAV (Unmanned Aerial Vehicle) platforms, rather than barely transmitting images back to supporting equipment for post-processing, is a core requirement for advanced UAV applications. However, due to limited computing capacity and memory of UAV platforms, it is very challenging to deploy real-time detection models on them. In addition, there are more small objects in aerial images, which makes it more difficult to detect accurately. To solve these problems, this paper brings dense connection to Yolo(You Only Look Once)v3 network, and proposes Yolo-LiteDense model. The backbone of Yolo-LiteDense is densely connected, which improves the performance of feature extraction. Then, we enforce channel pruning to Yolo-LiteDense model by pruning less informative channels with less scaling factors. After pruning, parameters and weight size of the model are compressed significantly, and inference time is also shortened. Evaluation results on VisDrone2018-DET show that parameters and weight size of Yolo-LiteDense are 83% and inference time is 30% less than Yolov3-SPP with comparable average precision. In addition, this paper also proposes the lighter version of Yolo-LiteDense, Yolo-DenseNano. Parameters and weight size of Yolo-LiteDense are 70% less than Yolov3-tiny with 2.68 times greater average precision.",https://ieeexplore.ieee.org/document/9447518/,2021 International Conference on Computer Communication and Artificial Intelligence (CCAI),7-9 May 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIKE.2018.00015,Machine Learning Models to Enhance the Science of Cognitive Autonomy,IEEE,Conferences,"Intelligent Autonomous Systems (IAS) are highly cognitive, reflective, multitask-able, and effective in knowledge discovery. Examples of IAS include software systems that are capable of automatic reconfiguration, autonomous vehicles, network of sensors with reconfigurable sensory platforms, and an unmanned aerial vehicle (UAV) respecting privacy by deciding to turn off its camera when pointing inside a private residence. Research is needed to build systems that can monitor their environment and interactions, learn their capabilities and limitations, and adapt to meet the mission objectives with limited or no human intervention. The systems should be fail-safe and should allow for graceful degradations while continuing to meet the mission objectives. In this paper, we provide an overview of our proposed new methodologies and workflows, and survey the existing approaches and new ones that can advance the science of autonomy in smart systems through enhancements in real-time control, auto-reconfigurability, monitoring, adaptability, and trust. This paper also provides the theoretical framework behind IAS.",https://ieeexplore.ieee.org/document/8527447/,2018 IEEE First International Conference on Artificial Intelligence and Knowledge Engineering (AIKE),26-28 Sept. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2019.8761117,Machine Learning for Position Prediction and Determination in Aerial Base Station System,IEEE,Conferences,"A novel framework for dynamic 3-D deployment of unmanned aerial vehicle (UAV) in the aerial base station system (ABSS) that based on the machine learning algorithms is proposed. In the framework, the UAV is deployed as an aerial base station to serve a group of ground users and is placed based on the prediction of the users' mobility. The joint problem of prediction of users' track and 3-D deployment of the UAV is formulated for maximizing the sum transmit rate. A two-step approach is proposed for predicting the movement of users and for determining the dynamic 3-D placement of the UAV. Firstly, an echo state network (ESN) based prediction algorithm is utilized for predicting the future positions of users based on the real-world datasets collected from Twitter. Secondly, an iterative K-Means based algorithm is proposed for obtaining the optimal placement of UAV at each time slot based on the output of ESN model. Numerical results are illustrated for showing the superiority of the proposed algorithm over the prevalent algorithm on prediction tasks. The accuracy and efficiency of the proposed framework are also investigated. Additionally, compared with static placement of the UAV, the advantage of dynamic 3-D deployment is demonstrated.",https://ieeexplore.ieee.org/document/8761117/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2017.7991518,Nonlinear controller for a UAV using Echo State Network,IEEE,Conferences,"A nonlinear adaptive controller for an unmanned aerial vehicle (UAV) has been developed using Echo State Network (ESN), which is a form of three-layered recurrent neural network (RNN). Online learning is used to train the ESN in real-time starting from randomized weights. The ESN is integrated into ArduPilot, an open source autopilot, for complex flight simulations. Software-in-the-loop and hardware-in-the-loop simulations are performed using the FlightGear Flight Simulator. The response of the UAV using the controller based on the ESN has surpassed the performance of the traditional controllers. Noise and external disturbances are added to show the effectiveness of the controllers. A UAV test platform is designed and built to gather aircraft flight data and test the ESN.",https://ieeexplore.ieee.org/document/7991518/,2017 International Conference on Unmanned Aircraft Systems (ICUAS),13-16 June 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INERTIAL53425.2022.9787752,Q-Learning-Based Noise Covariance Adaptation in Kalman Filter for MARG Sensors Attitude Estimation,IEEE,Conferences,"The attitude estimation of a rigid body by magnetic, angular rate, and gravity (MARG) sensors is a research subject for a large variety of engineering applications. A standard solution for building up the observer is usually based on the Kalman filter and its different extensions for versatility and practical implementation. However, the performance of these observers has long suffered from the inaccurate process and measurement noise covariance matrices, which in turn entails tedious parameter turning procedures. To overcome the laborious noise covariance matrices regulation, we propose in this paper a Q-learning-based approach to autonomously adapt the values of process and measurement noise covariance matrices. The Q-learning method establishes a reinforcement learning mechanism that forces the noise covariance matrices pair with the least difference between predictions and measurements of output to be found in a predetermined candidate set of noise covariance matrices. The effectiveness of the Q-learning approach, applied to Extended Kalman filter-based attitude estimation, is validated through the Monte Carlo method that uses real flight data on an unmanned aerial vehicle.",https://ieeexplore.ieee.org/document/9787752/,2022 IEEE International Symposium on Inertial Sensors and Systems (INERTIAL),8-11 May 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoSE50414.2020.9130475,Real time object detection for aerial search and rescue missions for missing persons,IEEE,Conferences,"This paper introduces a solution to stand-alone system based, real-time object-detection, can efficiently facilitate the search for missing persons with an unmanned aerial vehicle. The challenge is the real-time implementation of the systems and training the given deep neural network for the desired task. The paper describes the methods and procedures currently in use, as well as the possible tools. Subsequently, the autonomous aircraft system, which carries a real-time detection system, is introduced. In the section about real-time detection, we will introduce the TensorFlow lite-based application, building on SSD topology, in detail, which was implemented on mobile phones. We will also introduce the dataset used for training, testing and the results achieved. In summary, the recall achieved is 65.4% and precision is 96.4%, besides the fact that the android-based application, using the phone’s camera, performs image analysis at a rate of 11 to 17 FPS in real-time, while continuously providing",https://ieeexplore.ieee.org/document/9130475/,2020 IEEE 15th International Conference of System of Systems Engineering (SoSE),2-4 June 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2009.4939663,Real-time Neural Network based Identification of a Rotary-Wing UAV dynamics for autonomous flight,IEEE,Conferences,"Real time flight implementation of a neural network based black-box identification (NNID) scheme to a rotary wing unmanned aerial vehicle (RUAV) is presented in this paper. The applicability of NNID scheme for real time identification of longitudinal and lateral dynamics of the RUAV is evaluated in flight. To show the efficacy of the method for real time applications, the identification results and error statistics are provided. The challenges involved in terms of hardware implementation, computational time requirements, and real time coding are investigated and reported. Results indicate that NNID is suitable for modeling the dynamics of the RUAV in real time.",https://ieeexplore.ieee.org/document/4939663/,2009 IEEE International Conference on Industrial Technology,10-13 Feb. 2009,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC46691.2019.8939564,Real-time UAV Detection based on Deep Learning Network,IEEE,Conferences,"This paper presents deep learning-based YOLO (You only look once), for the detection of an unmanned aerial vehicle (UAV). In common practice, the creation of own data set is an extensive and hectic task, that takes a long time because it requires proper resolution images from different angles. These issues make the data set creation an important task. Implementation of YOLOv2 and YOLOv3 is done on the own created data set for the real-time UAV's detection and to benchmark the performance of both models in terms of mean average precision (MAP) and accuracy. For the specifically created data set made, YOLOv3 is outperforming YOLOv2 both in MAP and accuracy.",https://ieeexplore.ieee.org/document/8939564/,2019 International Conference on Information and Communication Technology Convergence (ICTC),16-18 Oct. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2007.4413945,Real-time multi-network based identification with dynamic selection implemented for a low cost UAV,IEEE,Conferences,This paper describes a system identification technique based on dynamic selection of multiple neural networks for the Unmanned Aerial Vehicle (UAV). The UAV is a multi- input multi-output (MIMO) nonlinear system. The neural network models are based on the autoregressive technique. The multi-network dynamic selection method allows a combination of online and offline neural network models to be used in the architecture where the most suitable output is selected based on the given criteria. The online network uses a novel training scheme with memory retention. Flight test validation results for online and offline models are presented. Real-time hardware in the loop (HIL) simulation results show that the multi-net dynamic selection technique performs better than the individual models.,https://ieeexplore.ieee.org/document/4413945/,"2007 IEEE International Conference on Systems, Man and Cybernetics",7-10 Oct. 2007,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2019.8816557,Research on V-SLAM Methods,IEEE,Conferences,"With the development of intelligent mobile robots, SLAM, especially V-SLAM, as the basic technology of robot localization and navigation, has the advantages of strong adaptability, high precision and strong intelligence compared with the traditional localization technology. It is widely used in smart devices such as unmanned aerial vehicle, automatic driving and sweeping robots. According to different implementation methods, the visual SLAM is divided into: filter V-SLAM based on probability model, key frame BA-based V-SLAM using nonlinear optimization theory, direct tracking of V-SLAM under the assumption of luminosity invariance, space occupying V-SLAM that focuses on building three-dimensional dense maps. This paper focuses on representative systems of various V-SLAMs and gives their respective applicable scenarios and characteristics. Finally, this article forecasts the development of V-SLAM combining with multi-information fusion technology, semantic deep and learning technology.",https://ieeexplore.ieee.org/document/8816557/,2019 IEEE International Conference on Mechatronics and Automation (ICMA),4-7 Aug. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560756,Siamese Anchor Proposal Network for High-Speed Aerial Tracking,IEEE,Conferences,"In the domain of visual tracking, most deep learning-based trackers highlight the accuracy but casting aside efficiency. Therefore, their real-world deployment on mobile platforms like the unmanned aerial vehicle (UAV) is impeded. In this work, a novel two-stage Siamese network-based method is proposed for aerial tracking, i.e., stage-1 for high-quality anchor proposal generation, stage-2 for refining the anchor proposal. Different from anchor-based methods with numerous pre-defined fixed-sized anchors, our no-prior method can 1) increase the robustness and generalization to different objects with various sizes, especially to small, occluded, and fast-moving objects, under complex scenarios in light of the adaptive anchor generation, 2) make calculation feasible due to the substantial decrease of anchor numbers. In addition, compared to anchor-free methods, our framework has better performance owing to refinement at stage-2. Comprehensive experiments on three benchmarks have proven the superior performance of our approach, with a speed of &#x223C;200 frames/s.",https://ieeexplore.ieee.org/document/9560756/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECE.2008.4769331,Situational awareness based on neural control of an autonomous helicopter during hovering manoeuvres,IEEE,Conferences,"This paper focuses on a critical component of the situational awareness, the neural network control of autonomous vertical flight for an unmanned aerial vehicle. Application of the proposed two stage flight strategy which uses two autonomous adaptive neural dynamical feedback controllers was carried out for a nontrivial small-scale helicopter model comprising five states, two inputs and two outputs. This control strategy for chosen helicopter model has been verified by simulation of hovering manoeuvres using software package Simulink and demonstrated good performance for fast situational awareness in real-time search-and-rescue operations.",https://ieeexplore.ieee.org/document/4769331/,2008 International Conference on Electrical and Computer Engineering,20-22 Dec. 2008,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1387,Study on UAV flight based identification technology,IET,Conferences,"In order to verify the feasibility of applying identification technology to engineering control of such air vehicles as missiles, by taking into consideration the flight authenticity, repeatability, safety and reliability, the unmanned aerial vehicle (UAV) similar to missile flight control is selected as the controlled object. Based on identification theory and control guidance theory, this paper firstly makes theoretical analysis on UAV's identification technology, and then conducts flight practice to verify the theory. Firstly, The feasibility of UAV's closed-loop identification and the generation of excitation signals are studied. Secondly, identification experiments platform, namely UAV system platform, is constructed, including hardware platform and such software platforms as control guidance, signal transmission and data collection. Thirdly, multiple flight identification experiments are conducted on UAV, thus obtaining identification data. Finally, the mathematical model of UAV is obtained through analyzing and processing data, and this model is verified in time domain and frequency domain. Through a series of simplification method, a mathematical model, which is available in engineering, simplified and close to reality, is achieved, and its correctness is verified at last. The results show that identification's theory and ideology can be entirely applied to the engineering practice controlled by actual air vehicles.",https://ieeexplore.ieee.org/document/6492994/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANSKOBE.2018.8559324,Supervised vs Unsupervised Approaches for Real Time Hyperspectral Imaging Maritime Target Detection,IEEE,Conferences,This paper addresses the use of supervised and unsupervised methods for classification of hyperspectral imaging data in maritime border surveillance domain. In this work supervised (SVM) and unsupervised (HYDADE) approaches were implemented. An evaluation benchmark was performed in order to compare methods results using real hyperspectral imaging data taken from an Unmanned Aerial Vehicle in maritime border surveillance scenario.,https://ieeexplore.ieee.org/document/8559324/,2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO),28-31 May 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GHTC.2018.8601597,The EDNA Public Safety Drone: Bullet-Stopping Lifesaving,IEEE,Conferences,"Urban gun violence in cities across the world is a serious issue for public safety agencies and disaster management organizations. This led us to the development of the EDNA drone, an aerial robotics solution designed to equip first responders in high-risk settings with lifesaving-edge tools for situational awareness and non-lethal conflict resolution. The EDNA is an unmanned aerial vehicle (UAV) that delivers the patent-pending “Predictive Probable Cause” technology. The EDNA drone is designed to provide automated real-time analysis to assist teams entering high-risk situations where gun violence may occur. By leveraging machine learning, biometric sensors, and advanced materials in the field and routing feedback to an intuitive augmented-reality interface, the EDNA will provide autonomous threat detection and bullet-stopping capabilities wherever those features are needed--to groups such as Police and Sheriff's Departments, Fire Departments, and EMT and emergency rescue teams. Data from the EDNA drone's sensors is fed to machine learning algorithms running on the drone in real-time. Through a neural network trained on past data, the EDNA is able to detect the presence and location of firearms and explosives, even through walls or other obstacles. Through the use of advanced metal foams and composite materials, the armored drone can even stop bullets-functionality which has obvious benefits for humanitarian deployment.",https://ieeexplore.ieee.org/document/8601597/,2018 IEEE Global Humanitarian Technology Conference (GHTC),18-21 Oct. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00060,The Implementation of a Power Efficient BCNN-Based Object Detection Acceleration on a Xilinx FPGA-SoC,IEEE,Conferences,"This paper focuses on the power efficient design on the FPGA SoC for the object detection system based on Binary Convolutional Neural Network (BCNN). Especially, for the small IoT devices, such as an intelligent dash-cam, computer vision system installed on an unmanned aerial vehicle, the power consumption could be a significant factor of the performance and scalability. However, the optimized FPGA design has limitations to reduce the overall power consumption amount. We focus on the design of the FPGA Accelerator as well as the effective design of the peripherals including CPU. In our proposed FPGA-SoC design, it supports not only FPGA but also CPU and the peripheral component can be supported by additional virtual memory system for reducing the processing time. Overall customization including customized BCNN, virtual memories for CPU and FPGA part allows our testbed to achieve low power consumption without speed degradation. Our testbed is based on customized YOLOv2 which consists of applied binary and half precision convolution, and pipeline-based architecture with accelerated hardware design on the target device. The target device used in this paper is the Xilinx ZYNQ-SoC based PYNQ Z-1 board. Our proposed system achieves 15.15 frames per second (FPS) and 1.45 watts of power dissipation. Our result shows that our design technique is effective for real-time object detection and low power system.",https://ieeexplore.ieee.org/document/8875471/,"2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",14-17 July 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEIT53597.2021.00077,UAV Control in Smart City Based on Space-Air-Ground Integrated Network,IEEE,Conferences,"Unmanned Aerial Vehicle (UAV) is an important part of the wireless network system of the future smart city. As a difficult point in the large-scale application of UAV, UAV control gradually attracts people's attention. Aiming at the problems of UAV control in smart city application, a near real time online learning architecture of UAV control based on the software-defined space-air-ground integrated network (SSAG) was proposed. This architecture uses the two-layer software defined network (SDN) controller architecture of SSAG framework to separate UAV control. The upper-tier SDN controller is responsible for the scheduling of UAV configuration, while the lower-tier SDN controller is responsible for regional coordination of UAV. The upper-tier SDN controller updates the tendency of network states by acquiring network states information in time interval. By simulating the network state in the next time interval, the optimal strategy of UAV scheduling of the next time interval is obtained by using the strategy iteration algorithm. Finally, an example is given to verify that the near real-time online learning architecture can accurately predict the UAV requirement, and increase the throughput of the network system compared with the traditional approach.",https://ieeexplore.ieee.org/document/9526159/,"2021 International Conference on Internet, Education and Information Technology (IEIT)",16-18 April 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECICE50847.2020.9301968,UAV Landmark Detection Based on Convolutional Neural Network,IEEE,Conferences,"The extensive use application of visual perception technology in Unmanned Aerial Vehicle (UAV) has brought great changes to the application of UAV in various fields. It is challenge to detect in landmark images for UAV. During UAV flight in different environments, the performance of landmark detection to deteriorate seriously have been caused by the uncertainty of landmark orientation, the diversity of landmark types and the similarities. This paper presents landmark detection of UAV based on Convolutional Neural Network (CNN). Theoretical analysis and experimental results demonstrate landmark recognition with an accuracy of at least 96% to match deployed in UAV, and the proposed CNN can make a correct classification.",https://ieeexplore.ieee.org/document/9301968/,"2020 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)",23-25 Oct. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043987,UAV Path Planning Based on Biological Excitation Neural Network and Visual Odometer,IEEE,Conferences,"Unmanned aerial vehicle(UAV) have been widely used in military and civil fields due to their compact structure, flexible mobility, low cost and other advantages. With the development of artificial intelligence in recent years, more intelligent and advanced algorithms have appeared, in which machine vision, as an important branch in the field of artificial intelligence, has also been greatly developed. The limitation of space, load, endurance and computing capacity hinders the application of intelligent algorithms on UAV. In the paper a semi-autonomous control platform of the quadrotor UAV was developed and the upper and lower dual control core architecture is implemented. Based on the hardware platform, the improved visual inertia odometer (VIO) and the biological excitation neural network are used to improve the flight performance and the ability of autonomy. To solve the problem of the synchronization for VIO, a cubic spline interpolation function was employed. A biological excitation neural network was extended to solve UAV on-line path planning. It provides an on-board path planning approach for UAV in the 3D world considering the dynamic obstacles. Finally, the feasibility and stability of the designed system were verified by flight experiments.",https://ieeexplore.ieee.org/document/9043987/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CISTI.2017.7975750,UAV simulator for grown-up people quality of life enhancement,IEEE,Conferences,This paper presents the development of a virtual reality simulator for the management of a UAV (Unmanned Aerial Vehicle) focused on improving the quality of life of grown-up people. The present research has collected characteristics of gestures and physical movements from users made by other related research in order to study the same interaction within a virtual world. Through this research a smaller number of gestures were created improving the user learning curve without affecting the usability. The following implementation uses a client-server architecture composed of 2 Raspberry Pi devices and a Smartphone acting as a server the communication between them was achieved by employing Bluetooth Low Energy technology. The immersive virtual experience is accomplished by using Unity 3D and Google VR tools that allowed the design and display of a playful virtual environment as an approach to promote physical and cognitive skills such as spatial thinking and hand-eye coordination. By performing maneuvers through an aerial circuit filled with obstacles the proposed UAV simulator encourages motor and mental activity while the user is being entertained. The result is the improvement of the user quality of life by avoiding cognitive and physical sedentarism.,https://ieeexplore.ieee.org/document/7975750/,2017 12th Iberian Conference on Information Systems and Technologies (CISTI),21-24 June 2017,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC48513.2020.9065203,UAV-assisted Real-time Data Processing using Deep Q-Network for Industrial Internet of Things,IEEE,Conferences,"Industrial internet of things (IIoT) enables edge computing technology to provide communication between the machines that produce a large amount of data and locate at the edge network. A task scheduling is implemented in the edge node. Furthermore, the real-time data can achieve with the lowest latency that allowed by the edge node near the edge network. However, a mobile machine such as an autonomous guided vehicle can interfere in this situation. Because the vehicle also needs service by the edge node. Over that, quality of service (QoS) performance can decrease. Therefore, this paper deploys an unmanned aerial vehicle (UAV) as an edge node to provide service to the edge network through optimizing the trajectory of UAV, where the edge network request task using a Deep Q-Network (DQN) Learning. The result shows that using machine learning, notably the DQN algorithm, can increase the number of the machine that can be provided service. Subsequently, the real-time data can achieve either the interrupt occurs at the edge node.",https://ieeexplore.ieee.org/document/9065203/,2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),19-21 Feb. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2018.8665195,Unsupervised Feature Fusion Combined with Neural Network Applied to UAV Attitude Estimation,IEEE,Conferences,"In the field of an unmanned aerial vehicle (UAV), the navigation algorithm with high precision and easy implementation is a hot topic of research, and the key of UAV control is to obtain accurate and real-time attitude information. In this paper, a feature fusion algorithm based on unsupervised deep autoencoder (DAE) is proposed. It is used for data fusion of multiple sensors. The experimental results show that the unsupervised feature fusion algorithm can effectively improve the accuracy and has the potential to be applied to the data fusion of UAV sensors.",https://ieeexplore.ieee.org/document/8665195/,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),12-15 Dec. 2018,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2019.2905808,"Design, Implementation, and Evaluation of a Neural-Network-Based Quadcopter UAV System",IEEE,Journals,"In this paper, a quadcopter unmanned aerial vehicle (UAV) system based on neural-network enhanced dynamic inversion control is proposed for multiple real-world application scenarios. A sigma-pi neural network (SPNN) is used as the compensator to reduce the model error and improve the system performance in the presence of the uncertainties of UAV dynamics, payload, and environment. Besides, we present a technical framework for fast and robust implementation of multipurpose UAV systems and develop a testbed for the evaluation of UAV control system by using a high-precision optical motion capture system. Both simulation results and experiment results demonstrate that the SPNN can reduce the inversion errors related to UAV parameter uncertainties as well as tracking errors related to unknown disturbances and unmodeled dynamics. With the help of an online neural network (NN) learning mechanism, the entire system can achieve much higher accuracy in attitude and trajectory control than that achieved by conventional proportional-integral derivative based control systems under varying flight conditions.",https://ieeexplore.ieee.org/document/8676108/,IEEE Transactions on Industrial Electronics,March 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMECH.2021.3079935,Geometrical-Based Displacement Measurement With Pseudostereo Monocular Camera on Bidirectional Cascaded Linear Actuator,IEEE,Journals,"This article details the development of a geometrical-based displacement extraction framework capable of automatically extracting critical infrastructure measurements in one sequence. The framework is a novel rail viaduct bearing inspection pipeline implemented on Bearing Inspector for Narrow-space Observation Version 2 (BINOv2). BINOv2 is a tethered custom unmanned aerial vehicle system utilized to supplant labor-intensive pipelines and enhance inspection accuracy of infrastructure conditions in confined remote locations. The algorithm accepts stereoscopic images taken from a single monocular camera on a bidirectional cascaded linear actuator system in a rack-and-pinion configuration. A point cloud model generated from the image sets then runs through a hierarchical neural network for 3-D segmentation to extract targeted regions of interest. Our training pipeline generates and forms the full model's training dataset using only a small sample of real point clouds. The point cloud generated is inadequate to form the full bearing geometry profile. Therefore, the proposed framework projects best-fit circles based on the point cloud curvature to form the full bearing geometry profile so that the required displacement measurement is available for extraction. Several experiments were conducted on a mock-up and actual operational site to validate the proposed framework's accuracy, its robustness and comparison with other state-of-the-art alternatives.",https://ieeexplore.ieee.org/document/9430727/,IEEE/ASME Transactions on Mechatronics,Aug. 2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2021.3058192,Integrated Air-Ground Vehicles for UAV Emergency Landing Based on Graph Convolution Network,IEEE,Journals,"With unmanned aerial vehicle (UAV) technologies advanced rapidly, many applications have emerged in cities. However, those applications do not widely spread as the safety consideration hinders the UAV from integrating into the civilian environment. This work focuses on investigating the UAV emergency landing problem which is a critical safety functionality of UAV. This work proposed a graph convolution network (GCN)-based decision network to learn by imitating the human pilots&#x2019; landing strategy. To alleviate the needs of a large amount of real-world data for model training, the proposed model allows to be trained in a simulated environment and then transferred to the real-world scenario due to the separation of domain-specific terrain classes and domain-independent topological structures among down-looking camera images. The GCN-based decision network can be coupled with a topological heuristic to improve the performance of action prediction in an emergency situation. To evaluate the proposed method, this work implemented a simulation environment for collecting data and testing the UAV emergency landing. The empirical results in both simulated and real-world scenarios show that the proposed methods can outperform the state-of-the-art counterparts in terms of predictive accuracy and success landing rate.",https://ieeexplore.ieee.org/document/9351541/,IEEE Internet of Things Journal,"15 June15, 2022",ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TWC.2021.3121584,"Joint Resource, Deployment, and Caching Optimization for AR Applications in Dynamic UAV NOMA Networks",IEEE,Journals,"The cache-enabling unmanned aerial vehicle (UAV) non-orthogonal multiple access (NOMA) networks for mixture of augmented reality (AR) and normal multimedia applications are investigated, which is assisted by UAV base stations. The user association, power allocation of NOMA, deployment of UAVs and caching placement of UAVs are jointly optimized to minimize the content delivery delay. A branch and bound (BaB) based algorithm is proposed to obtain the per-slot optimization. To cope with the dynamic content requests and mobility of users in practical scenarios, the original optimization problem is transformed to a Stackelberg game. Specifically, the game is decomposed into a leader level user association sub-problem and a number of power allocation, UAV deployment and caching placement follower level sub-problems. The long-term minimization was further solved by a deep reinforcement learning (DRL) based algorithm. Simulation result shows that the content delivery delay of the proposed BaB based algorithm is much lower than benchmark algorithms, as the optimal solution in each time slot is achieved. Meanwhile, the proposed DRL based algorithm achieves a relatively low long-term content delivery delay in the dynamic environment with lower computation complexity than BaB based algorithm.",https://ieeexplore.ieee.org/document/9591297/,IEEE Transactions on Wireless Communications,May 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/JCN.2021.000026,Joint deployment and trajectory optimization in UAV-assisted vehicular edge computing networks,KICS,Journals,"As the general mobile edge computing (MEC) scheme cannot adequately handle the emergency communication requirements in vehicular networks, unmanned aerial vehicle (UAV)-assisted vehicular edge computing networks (VECNs) are envisioned as the reliable and cost-efficient paradigm for the mobility and flexibility of UAVs. UAVs can perform as the temporary base stations to provide edge services for road vehicles with heavy traffic. However, it takes a long time and huge energy consumption for the UAV to fly from the stay charging station to the mission areas disorderly. In this paper, we design a pre-dispatch UAV-assisted VECNs system to cope with the demand of vehicles in multiple traffic jams. We propose an optimal UAV flight trajectory algorithm based on the traffic situation awareness. The cloud computing center (CCC) server predicts the real-time traffic conditions, and assigns UAVs to different mission areas periodically. Then, a flight trajectory optimization problem is formulated to minimize the cost of UAVs, while both the UAV flying and turning energy costs are mainly considered. In addition, we propose a deep reinforcement learning(DRL)-based energy efficiency autonomous deployment strategy, to obtain the optimal hovering position of UAV at each assigned mission area. Simulation results demonstrate that our proposed method can obtain an optimal flight path and deployment of UAV with lower energy consumption.",https://ieeexplore.ieee.org/document/9541336/,Journal of Communications and Networks,Feb. 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3027825,Multi-Modal Data Fusion Using Deep Neural Network for Condition Monitoring of High Voltage Insulator,IEEE,Journals,"A novel Fusion Convolutional Network (FCN) is proposed in this research for potential real-time monitoring of insulators using unmanned aerial vehicle (UAV) edge devices. Precise airborne imaging of outdoor objects, such as high voltage insulators, suffers from varied object resolution, cluttered backgrounds, unclear or contaminated surfaces, and illumination conditions. Accurate information about the insulator surface condition is essential and is of a high priority since insulator breakdown is a leading cause of electrical failure. A multi-modal information fusion (MMIF) system is developed during this research to analyze and classify possible contaminations present on the electrical insulators. A novel system, referred to as FCN, consists of a Convolutional Neural Network (CNN) and a binary Multilayer Neural Network (MNN) sub-classifier. While constructing the MMIF dataset for training and testing the novel FCN, the image classification output of the CNN is combined with the leakage current values (LCV) obtained as the classification output of MNN. Each sample of the MMIF dataset is, therefore, represented as a series of fusions. Later, sub-classifiers, of the FCN, are trained to identify the contamination types in the fusion series by implementing a voting system of sub-classifiers which is trained to identify a given class. As a result of the implementation of the proposed FCN, the classification accuracy increased by 8.4%, i.e., from 92% to 99.76%. To compare and benchmark the performance of proposed FCN, conventional classification algorithms are also implemented on the fusion of features that are extracted employing the wavelet transform and PCA methods. State-of-the-art CNN architectures are also discussed on account of their time consumption and memory usage. The conceptualization of a potential hardware implementation of the proposed FCN, on emerging edge devices, is also provided for completeness of the discussion. Pertinent outcomes of this research can be further extended to other potential applications of airborne imaging.",https://ieeexplore.ieee.org/document/9210065/,IEEE Access,2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2020.3010730,Nonlinear MPC for Collision Avoidance and Control of UAVs With Dynamic Obstacles,IEEE,Journals,"This letter proposes a Novel Nonlinear Model Predictive Control (NMPC) for navigation and obstacle avoidance of an Unmanned Aerial Vehicle (UAV). The proposed NMPC formulation allows for a fully parametric obstacle trajectory, while in this letter we apply a classification scheme to differentiate between different kinds of trajectories to predict future obstacle positions. The trajectory calculation is done from an initial condition, and fed to the NMPC as an additional input. The solver used is the nonlinear, non-convex solver Proximal Averaged Newton for Optimal Control (PANOC) and its associated software OpEn (Optimization Engine), in which we apply a penalty method to properly consider the obstacles and other constraints during navigation. The proposed NMPC scheme allows for real-time solutions using a sampling time of 50 ms and a two second prediction of both the obstacle trajectory and the NMPC problem, which implies that the scheme can be considered as a local path-planner. This letter will present the NMPC cost function and constraint formulation, as well as the methodology of dealing with the dynamic obstacles. We include multiple laboratory experiments to demonstrate the efficacy of the proposed control architecture, and to show that the proposed method delivers fast and computationally stable solutions to the dynamic obstacle avoidance scenarios.",https://ieeexplore.ieee.org/document/9145644/,IEEE Robotics and Automation Letters,Oct. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMC.2020.3042925,Online Altitude Control and Scheduling Policy for Minimizing AoI in UAV-Assisted IoT Wireless Networks,IEEE,Journals,"This article considers unmanned aerial vehicle (UAV) assisted Internet of Things (IoT) networks, where low resource IoT devices periodically sample a stochastic process and need to upload more recent information to a Base Station (BS). Among the myriad of applications, there is a need for timely delivery of data (for example, status-updates) before the data becomes outdated and loses its value. Since transmission capabilities of IoT devices are limited, it may not always be feasible to transmit over one hop transmission to the BS. To address this challenge, UAVs with virtual queues are deployed as middle layer between IoT devices and the BS to relay recent information over unreliable channels. In the absence of channel conditions, the optimal online scheduling policy is investigated as well as dynamic UAV altitude control that maintains a fresh status of information at the BS. The objective of this paper is to minimize the Expected Weighted Sum Age of Information (EWSA) for IoT devices. First, the problem is formulated as an optimization problem that is however generally hard to solve. Second, an online model free Deep Reinforcement Learning (DRL) is proposed, where the deployed UAV obtains instantaneous channel state information (CSI) in real time along with any adjustment to its deployment altitude. Third, we formulate the online problem as a Markov Decision Process (MDP) and Proximal Policy Optimization (PPO) algorithm, which is a highly stable state-of-the-art DRL algorithm, is leveraged to solve the formulated problem. Finally, extensive simulations are conducted to verify findings and comprehensive comparisons with other baseline approaches are provided to demonstrate the effectiveness of the proposed design.",https://ieeexplore.ieee.org/document/9285215/,IEEE Transactions on Mobile Computing,1 July 2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3164505,Online Path Planning Framework for UAV in Rural Areas,IEEE,Journals,"A motion strategy plays an important role in supporting autonomous Unmanned Aerial Vehicle (UAV) movement. Many studies have been conducted to improve the motion frameworks in terms of its robustness, safety and performance. Most of them worked on the prior known maps scenario where the area information was collected by Global Positioning System (GPS) and satellite cameras. Even though the scheme can provide high quality map, the computation of motion planning remains dependent on the communication signal. In the rural areas such as forests and mountains, where communication signal does not perform well, unclear and noisy terrain maps can be generated and lead to mission failure. Therefore, it is significant that an alternative framework to enhance autonomous UAV motion performance in these certain conditions should be developed. Our work focuses on developing a high performance path planner for autonomous UAV motion when communication signal does not work well in rural areas. The search mission problem in forest terrain has been implemented in 3D simulation as an evaluation. By conducting a simulation process repeatedly with different test cases for positions, time constraints, flight speed (3-11 m/s) and flight range, our path planning framework can achieve completeness between 90-100&#x0025; and better performance compared to others.",https://ieeexplore.ieee.org/document/9748136/,IEEE Access,2022,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2900475,Pollution Source Localization Based on Multi-UAV Cooperative Communication,IEEE,Journals,"Harmful gas leakage accidents in chemical plants have occurred from time to time. The application of mobile robots to find odor source has become one of the hottest research topics. Compared to traditional robots, unmanned aerial vehicle (UAV) is more flexible and safer. Therefore, using multi-UAV to solve pollution source tracking is a meaningful study. In this paper, an air pollution source tracking algorithm based on artificial potential field and particle swarm optimization is proposed. The particle swarm optimization algorithm combined with artificial potential field method is used to guide the UAVs to track the plume and avoid the collisions among them. At the same time, adaptive inertia weights are used to help improve the convergence and the searchability of particles. We not only evaluated this algorithm in simulation experiments but also designed a multi-UAV pollution source tracking platform for real-world experiments. The experimental results show that the algorithm can accurately find the pollution source in a short time.",https://ieeexplore.ieee.org/document/8665856/,IEEE Access,2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3046499,Real-Time Energy Harvesting Aided Scheduling in UAV-Assisted D2D Networks Relying on Deep Reinforcement Learning,IEEE,Journals,"Unmanned aerial vehicle (UAV)-assisted device-to-device (D2D) communications can be deployed flexibly thanks to UAVs' agility. By exploiting the direct D2D interaction supported by UAVs, both the user experience and network performance can be substantially enhanced at public events. However, the continuous moving of D2D users, limited energy and flying time of UAVs are impediments to their applications in real-time. To tackle this issue, we propose a novel model based on deep reinforcement learning in order to find the optimal solution for the energy-harvesting time scheduling in UAV-assisted D2D communications. To make the system model more realistic, we assume that the UAV flies around a central point, the D2D users move continuously with random walk model and the channel state information encountered during each time slot is randomly time-variant. Our numerical results demonstrate that the proposed schemes outperform the existing solutions. The associated energy efficiency game can be solved in less than one millisecond by an off-the-shelf processor using trained neural networks. Hence our deep reinforcement learning techniques are capable of solving real-time resource allocation problems in UAV-assisted wireless networks.",https://ieeexplore.ieee.org/document/9303363/,IEEE Access,2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIM.2020.3001659,Real-Time Fault Detection for UAV Based on Model Acceleration Engine,IEEE,Journals,"With the wide applications of the unmanned aerial vehicle (UAV) in the civilian and military fields, its operational safety has drawn much attention. A series of fault detection methods are studied to avoid disasters. Due to the capabilities of strong feature extraction and massive flight data processing, the deep learning-based methods have received extensive attention. However, restricted by UAV airborne size, weight, and power consumption, a significant challenge is posed to deploy these complicated detection methods in the airborne application, which requires to run in real time. In this article, a fault detection model acceleration engine (FDMAE) for UAV real-time fault detection is realized under the airborne constraint. First, a high-performance detection model is designed based on stacked long short-term memory networks, and fault detection is achieved by a statistical threshold in this method. Second, a model pruning method based on principal component analysis is proposed to improve computing efficiency. Finally, the pruned fault detection method is optimized and integrated as a flexible acceleration engine through high-level synthesis and deployed on an airborne embedded computing platform based on a field-programmable gate array. Real UAV flight data are used to verify the proposed FDMAE. By comparing accuracy, the area under the receiver operating characteristic curve, speed, and power consumption, the effectiveness of the FDMAE is proven.",https://ieeexplore.ieee.org/document/9115090/,IEEE Transactions on Instrumentation and Measurement,Dec. 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TITS.2018.2797697,Real-Time Traffic Flow Parameter Estimation From UAV Video Based on Ensemble Classifier and Optical Flow,IEEE,Journals,"Recently, the availability of unmanned aerial vehicle (UAV) opens up new opportunities for smart transportation applications, such as automatic traffic data collection. In such a trend, detecting vehicles and extracting traffic parameters from UAV video in a fast and accurate manner is becoming crucial in many prospective applications. However, from the methodological perspective, several limitations have to be addressed before the actual implementation of UAV. This paper proposes a new and complete analysis framework for traffic flow parameter estimation from UAV video. This framework addresses the well-concerned issues on UAV's irregular ego-motion, low estimation accuracy in dense traffic situation, and high computational complexity by designing and integrating four stages. In the first two stages an ensemble classifier (Haar cascade + convolutional neural network) is developed for vehicle detection, and in the last two stages a robust traffic flow parameter estimation method is developed based on optical flow and traffic flow theory. The proposed ensemble classifier is demonstrated to outperform the state-of-the-art vehicle detectors that designed for UAV-based vehicle detection. Traffic flow parameter estimations in both free flow and congested traffic conditions are evaluated, and the results turn out to be very encouraging. The dataset with 20,000 image samples used in this study is publicly accessible for benchmarking at http://www.uwstarlab.org/research.html.",https://ieeexplore.ieee.org/document/8307444/,IEEE Transactions on Intelligent Transportation Systems,Jan. 2019,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2019.2945167,Sensor-Augmented Neural Adaptive Bitrate Video Streaming on UAVs,IEEE,Journals,"Recent advances in unmanned aerial vehicle (UAV) technology have revolutionized a broad class of civil and military applications. However, the designs of wireless technologies that enable real-time streaming of high-definition video between UAVs and ground clients present a conundrum. Most existing adaptive bitrate (ABR) algorithms are not optimized for the air-to-ground links, which usually fluctuate dramatically due to the dynamic flight states of the UAV. In this paper, we present SA-ABR, a new sensor-augmented system that generates ABR video streaming algorithms with the assistance of various kinds of inherent sensor data that are used to pilot UAVs. By incorporating the inherent sensor data with network observations, SA-ABR trains a deep reinforcement learning (DRL) model to extract salient features from the flight state information and automatically learn an ABR algorithm to adapt to the varying UAV channel capacity through the training process. SA-ABR does not rely on any assumptions or models about UAV's flight states or the environment, but instead, it makes decisions by exploiting temporal properties of past throughput through the long short-term memory (LSTM) to adapt itself to a wide range of highly dynamic environments. We have implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the results show that our system outperforms the best known existing ABR algorithm by 21.4% in terms of the average quality of experience (QoE) reward.",https://ieeexplore.ieee.org/document/8855031/,IEEE Transactions on Multimedia,June 2020,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3050522,Spectrum-Sharing UAV-Assisted Mission-Critical Communication: Learning-Aided Real-Time Optimisation,IEEE,Journals,"We propose an unmanned aerial vehicle (UAV) communications scheme with spectrum-sharing mechanism to provide mission-critical services such as disaster recovery and public safety. Specifically, the UAVs can serve as flying base stations to provide extended network coverage for the affected area under spectrum-sharing cognitive radio networks (CRNs). To cope with the effects of network destruction in a disaster, we propose a real-time optimisation framework for resource allocation (e.g., power and number of UAVs) for CRNs assisted by UAV relays. The proposed optimisation scheme aims at optimising the network throughput of primary and secondary networks under the stringent constraint of maximum tolerable interference impinged on the primary users. We also propose a deep neural network (DNN) model to significantly reduce the execution time under real-time solution of mixed-integer UAV deployment problems. For both primary and secondary networks, our real-time optimisation algorithms impose low computational complexity, hence, have a low execution time in solving throughput optimisation problems, which demonstrates the benefit of our approached proposed for spectrum-sharing UAV-assisted mission-critical services.",https://ieeexplore.ieee.org/document/9319135/,IEEE Access,2021,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
