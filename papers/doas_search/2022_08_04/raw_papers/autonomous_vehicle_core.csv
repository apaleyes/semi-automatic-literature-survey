id,datePublished,description,publisher,title,downloadUrl,journals,doi,database,query_name,query_value
219456675,2017-01-01T00:00:00,"The development of technologies for autonomous vehicle (AV) have seen rapid achievement in the recent years. Commercial carmakers are actively embedding this system in their production and are undergoing tremendous testing in the real world traffic environment. It is one of today’s most challenging topics in the intelligent transportation system (ITS) field in term of reliability as well as accelerating the world’s transition to a sustainable future. The utilization of current sensor technology however indicates some drawbacks where the complexity is high and the cost is extremely huge. This paper reviews the recent sensor technologies and their contributions in becoming part of the autonomous self-driving vehicle system. The ultimate focus is toward reducing the sensor count to just a single camera based on the single modality model. The capability of the sensor to detect and recognize on-the-road obstacles such as overtaking vehicle, pedestrians, signboards, bicycle, road lane marker and road curvature will be discussed. Different feature extraction approach will be reviewed further with the selection of the recent Artificial Intelligent (AI) methods that are being implemented. At the end of this review, the optimal techniques of processing information from single camera system will be discussed and summarized",Society of Automotive Engineers Malaysia,Single camera object detection for self-driving vehicle: a review,https://core.ac.uk/download/219456675.pdf,,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
479360462,2021-06-01T00:00:00,"Introduction. An important part of an automotive unmanned vehicle (UV) control system is the environment analysis module. This module is based on various types of sensors, e.g. video cameras, lidars and radars. The development of computer and video technologies makes it possible to implement an environment analysis module using a single video camera as a sensor. This approach is expected to reduce the cost of the entire module. The main task in video image processing is to analyse the environment as a 3D scene. The 3D trajectory of an object, which takes into account its dimensions, angle of view and movement vector, as well as the vehicle pose in a video image, provides sufficient information for assessing the real interaction of objects. A basis for constructing a 3D trajectory is vehicle pose estimation.Aim. To develop an automatic method for estimating vehicle pose based on video data analysis from a single video camera.Materials and methods. An automatic method for vehicle pose estimation from a video image was proposed based on a cascade approach. The method includes vehicle detection, key points determination, segmentation and vehicle pose estimation. Vehicle detection and determination of its key points were resolved via a neural network. The segmentation of a vehicle video image and its mask preparation were implemented by transforming it into a polar coordinate system and searching for the outer contour using graph theory.Results. The estimation of vehicle pose was implemented by matching the Fourier image of vehicle mask signatures and the templates obtained based on 3D models. The correctness of the obtained vehicle pose and angle of view estimation was confirmed by experiments based on the proposed method. The vehicle pose estimation had an accuracy of 89 % on an open Carvana image dataset.Conclusion. A new approach for vehicle pose estimation was proposed, involving the transition from end-to-end learning of neural networks to resolve several problems at once, e.g., localization, classification, segmentation, and angle of view, towards cascade analysis of information. The accuracy level of end-to-end learning requires large sets of representative data, which complicates the scalability of solutions for road environments in Russia. The proposed method makes it possible to estimate the vehicle pose with a high accuracy level, at the same time as involving no large costs for manual data annotation and training",'St. Petersburg Electrotechnical University LETI',Method for Automatic Determination of a 3D Trajectory of Vehicles in a Video Image,https://core.ac.uk/download/479360462.pdf,"[{'title': 'Journal of the Russian Universities Radioelectronics', 'identifiers': ['issn:1993-8985', '1993-8985', 'issn:2658-4794', '2658-4794']}]",10.32603/1993-8985-2021-24-3-49-59,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
441223432,2020-09-01T00:00:00,"The results of the development of hardware and software system (micromodule), which detects and classifies underlying surface images of the Earth are presented. The micromodule can be installed on board of a light unmanned aerial vehicle (drone). The device has the size 5.2×7.4×3.1 cm, the weight52 g, runs on a Raspberry Pi Zero Wireless single-board microcomputer and uses a convolutional neural network based on MobileNetV2 architecture for real-time image classification. When developing the micromodule, the authors aimed to achieve a real-time image classification on inexpensive mobile equipment with low computing power so that the classification quality is  comparable  to  popular  deep  convolutional  network  architectures. The provided information could be useful for engineers and researchers who are developing compact budget mobile systems for processing, analyzing and recognition of images",'United Institute of Informatics Problems of the National Academy of Sciences of Belarus',Recognition of underlying surface using a convolutional neural network on a single-board computer,https://core.ac.uk/download/441223432.pdf,"[{'title': 'Informatics', 'identifiers': ['1816-0301', 'issn:1816-0301']}]",10.37661/1816-0301-2020-17-3-36-43,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
286465896,2017-01-01T00:00:00,"Nel rilievo in ambito archeologico, per la presenza di elementi curvi e di parti aggettanti, è spesso necessario l’utilizzo di sistemi di acquisizione che permettano di ottenere una misura celere, dettagliata e di supporto alle tecniche tradizionali. L’utilizzo di sistemi di acquisizione fotogrammetrici da SAPR (Sistemi Aeromobili a Pilotaggio Remoto), unite all’utilizzo di algoritmi di Computer Vision, permettono la realizzazione di modelli tridimensionali ai quali vengono applicate texture foto-realistiche derivanti da immagini fotografiche acquisite in volo. Il caso studio che presentiamo è il rilievo fotogrammetrico del Ponte Rotto ad Apice (Bn). Per il rilievo aerofotogrammetrico è stato utilizzato un UAV (Unmanned Aerial Vehicle), con peso totale al decollo inferiore ai 2 kg (art. 12, Regolamento ENAC del 16 luglio 2015). Le immagini acquisite sono state elaborate con un software con tecnologia Structure From Motion; il modello è georeferito per mezzo di sei GCP (Ground Control Points), misurati con ricevitori GNSS (Global Navigation Satellite Systems) in modalità nRTK (Network Real Time Kinematic)",ASITA,L’uso di SAPR per la documentazione archeologica: il caso studio del Ponte Rotto ad Apice (Bn),,,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
233120453,2019-07-19T00:00:00,"Autonomous vehicle is a vehicle that can guide itself without human conduction. It is capable of sensing its environment and moving with little or no human input. This kind of vehicle has become a concrete reality and may pave the way for future systems where computers take over the art of driving. Advanced artificial intelligence control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant road signs. In this paper, we introduce an intelligent road signs classifier to help autonomous vehicles to recognize and understand road signs. The road signs classifier based on an artificial intelligence technique. In particular, a deep learning model is used, Convolutional Neural Networks (CNN). CNN is a widely used Deep Learning model to solve pattern recognition problems like image classification and object detection. CNN has successfully used to solve computer vision problems because of its methodology in processing images that are similar to the human brain decision making. The evaluation of the proposed pipeline was trained and tested using two different datasets. The proposed CNNs achieved high performance in road sign classification with a validation accuracy of 99.8% and a testing accuracy of 99.6%. The proposed method can be easily implemented for real time application",'Bilingual Publishing Co.',To Perform Road Signs Recognition for Autonomous Vehicles Using Cascaded Deep Learning Pipeline,https://core.ac.uk/download/233120453.pdf,,10.30564/aia.v1i1.569,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
227320072,2019-10-27T00:00:00,"International audienceReplacing the human driver to perform the Dynamic Driving Task (DDT)[1] will require perception, complex analysis and assessment of traffic situation. The path leading to success the deployment of fully Autonomous Vehicle (AV) depends on the resolution of a lot of challenges. Both the safety and the security aspects of AV constitute the core of regulatory compliance and technical research. The Autonomous Driving System (ADS) should be designed to ensure a safe manoeuvre and a stable behaviour despite the technological limitations, the uncertainties and hazards which characterize the real traffic conditions. In fully Autonomous Driving situation, detecting all relevant objects and agents should be sufficient to generate a warning, however the ADS requires further complex data analysis steps to quantify and improve the safety of decision making. This paper aims to improve the robustness of decision-making in order to mimic human-like decision ability. The approach is based on machine learning to identify the criticality of the dynamic situation and enabling ADS to make appropriate decision and fulfil safe manoeuvre",HAL CCSD,Machine learning method to ensure robust decision-making of AVs,https://core.ac.uk/download/227320072.pdf,,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
387259411,2021-01-01T00:00:00,"Handling of critical situations is an important part in the architecture of an autonomous vehicle. A controller for autonomous collision avoidance is developed based on a wary strategy that assumes the least tireroad friction for which the maneuver is still feasible. Should the friction be greater, the controller makes use of this and performs better. The controller uses an acceleration-vector reference obtained from optimal control of a friction-limited particle, whose applicability is verified by using numerical optimization on a full vehicle model. By employing an analytical tire model of the tireroad friction limit, to determine slip references for steering and body-slip control, the result is a controller where the computation of its output is explicit and independent of the actual tire-road friction. When evaluated in real-time on a high-fidelity simulation model, the developed controller performs close to that achieved by offline numerical optimization.Funding: Wallenberg AI, Autonomous Systems, and Software Program (WASP) - Knut and AliceWallenberg Foundation</p",'Institute of Electrical and Electronics Engineers (IEEE)',Autonomous Wary Collision Avoidance,,,10.1109/TIV.2020.3029853,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
301281319,2017-01-01T00:00:00,"Spatial perception, in which objects’ motion and positional relationship are recognized, is necessary for applications such as a walking robot and an autonomous car. One of the demanding features of spatial perception in real world applications is robustness. Neural network-based approaches, in which perception results are obtained by voting among a large number of neuronal activities, seem to be promising. We focused on a neural network model for motion stereo vision proposed by Kawakami et al. In this model, local motion in each small region of the visual field, which comprises optical flow, is detected by hierarchical neural network. Implementation of this model into a VLSI is required for real-time operation with low power consumption. In this study, we reduced the computational complexity of this model and showed cell responses of the reduced model by numerical simulation.Peer Reviewe",'Springer Science and Business Media LLC',Complexity reduction of neural network model for local motion detection in motion stereo vision,,,10.1007/978-3-319-70136-3,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
429089989,2020-12-21T00:00:00,"We present an unsupervised deep learning approach for post-disaster building damage detection that can transfer to different typologies of damage or geographical locations. Previous advances in this direction were limited by insufficient qualitative training data. We propose to use a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN) because it only requires pre-event imagery of buildings in their undamaged state. This approach aids the post-disaster response phase because the model can be developed in the pre-event phase and rapidly deployed in the post-event phase. We used the xBD dataset, containing pre-and post-event satellite imagery of several disaster-types, and a custom made Unmanned Aerial Vehicle (UAV) dataset, containing post-earthquake imagery. Results showed that models trained on UAV-imagery were capable of detecting earthquake-induced damage. The best performing model for European locations obtained a recall, precision and F1-score of 0.59, 0.97 and 0.74, respectively. Models trained on satellite imagery were capable of detecting damage on the condition that the training dataset was void of vegetation and shadows. In this manner, the best performing model for (wild)fire events yielded a recall, precision and F1-score of 0.78, 0.99 and 0.87, respectively. Compared to other supervised and/or multi-epoch approaches, our results are encouraging. Moreover, in addition to image classifications, we show how contextual information can be used to create detailed damage maps without the need of a dedicated multi-task deep learning framework. Finally, we formulate practical guidelines to apply this single-epoch and unsupervised method to real-world applications",'MDPI AG',Post-Disaster Building Damage Detection from Earth Observation Imagery using Unsupervised and Transferable Anomaly Detecting Generative Adversarial Networks,,,10.3390/rs12244193,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
477989584,2020-06-01T00:00:00,"[EN] Interactive spaces for education are emerging as a mechanism for fostering children's natural ways of learning by means of play and exploration in physical spaces. The advanced interactive modalities and devices for such environments need to be both motivating and intuitive for children. Among the wide variety of interactive mechanisms, robots have been a popular research topic in the context of educational tools due to their attractiveness for children. However, few studies have focused on how children would naturally interact and explore interactive environments with robots. While there is abundant research on full-body interaction and intuitive manipulation of robots by adults, no similar research has been done with children. This paper therefore describes a gesture elicitation study that identified the preferred gestures and body language communication used by children to control ground robots. The results of the elicitation study were used to define a gestural language that covers the different preferences of the gestures by age group and gender, with a good acceptance rate in the 6-12 age range. The study also revealed interactive spaces with robots using body gestures as motivating and promising scenarios for collaborative or remote learning activities.This work is funded by the European Development Regional Fund (EDRF-FEDER) and supported by the Spanish MINECO (TIN2014-60077-R). The work of Patricia Pons is supported by a national grant from the Spanish MECD (FPU13/03831). Special thanks are due to the children and teachers of the Col-legi Public Vicente Gaos for their valuable collaboration and dedication.Pons Tomás, P.; Jaén Martínez, FJ. (2020). Interactive spaces for children: gesture elicitation for controlling ground mini-robots. Journal of Ambient Intelligence and Humanized Computing. 11(6):2467-2488. https://doi.org/10.1007/s12652-019-01290-6S24672488116Alborzi H, Hammer J, Kruskal A et al (2000) Designing StoryRooms: interactive storytelling spaces for children. In: Proceedings of the conference on designing interactive systems processes, practices, methods, and techniques—DIS’00. ACM Press, New York, pp 95–104Antle AN, Corness G, Droumeva M (2009) What the body knows: exploring the benefits of embodied metaphors in hybrid physical digital environments. Interact Comput 21:66–75. https://doi.org/10.1016/j.intcom.2008.10.005Belpaeme T, Baxter PE, Read R et al (2013) Multimodal child–robot interaction: building social bonds. J Human-Robot Interact 1:33–53. https://doi.org/10.5898/JHRI.1.2.BelpaemeBenko H, Wilson AD, Zannier F, Benko H (2014) Dyadic projected spatial augmented reality. In: Proceedings of the 27th annual ACM symposium on user interface software and technology—UIST’14, pp 645–655Bobick AF, Intille SS, Davis JW et al (1999) The KidsRoom: a perceptually-based interactive and immersive story environment. Presence Teleoper Virtual Environ 8:367–391. https://doi.org/10.1162/105474699566297Bonarini A, Clasadonte F, Garzotto F, Gelsomini M (2015) Blending robots and full-body interaction with large screens for children with intellectual disability. In: Proceedings of the 14th international conference on interaction design and children—IDC’15. ACM Press, New York, pp 351–354Cauchard JR, E JL, Zhai KY, Landay JA (2015) Drone & me: an exploration into natural human–drone interaction. In: Proceedings of the 2015 ACM international joint conference on pervasive and ubiquitous computing—UbiComp’15. ACM Press, New York, pp 361–365Connell S, Kuo P-Y, Liu L, Piper AM (2013) A Wizard-of-Oz elicitation study examining child-defined gestures with a whole-body interface. In: Proceedings of the 12th international conference on interaction design and children—IDC’13. ACM Press, New York, pp 277–280Derboven J, Van Mechelen M, Slegers K (2015) Multimodal analysis in participatory design with children. In: Proceedings of the 33rd annual ACM conference on human factors in computing systems—CHI’15. ACM Press, New York, pp 2825–2828Dong H, Danesh A, Figueroa N, El Saddik A (2015) An elicitation study on gesture preferences and memorability toward a practical hand-gesture vocabulary for smart televisions. IEEE Access 3:543–555. https://doi.org/10.1109/ACCESS.2015.2432679Druin A (1999) Cooperative inquiry: developing new technologies for children with children. In: Proceedings of the SIGCHI conference on human factors computer system CHI is limit—CHI’99, vol 14, pp 592–599. https://doi.org/10.1145/302979.303166Druin A (2002) The role of children in the design of new technology. Behav Inf Technol 21:1–25. https://doi.org/10.1080/01449290110108659Druin A, Bederson B, Boltman A et al (1999) Children as our technology design partners. In: Druin A (ed) The design of children’s technology. Morgan Kaufman, San Francisco, pp 51–72Epps J, Lichman S, Wu M (2006) A study of hand shape use in tabletop gesture interaction. CHI’06 extended abstracts on human factors in computing systems—CHI EA’06. ACM Press, New York, pp 748–753Fender AR, Benko H, Wilson A (2017) MeetAlive : room-scale omni-directional display system for multi-user content and control sharing. In: Proceedings of the 2017 ACM international conference on interactive surfaces and spaces, pp 106–115Fernandez RAS, Sanchez-Lopez JL, Sampedro C et al (2016) Natural user interfaces for human–drone multi-modal interaction. In: 2016 international conference on unmanned aircraft systems (ICUAS). IEEE, New York, pp 1013–1022Garcia-Sanjuan F, Jaen J, Nacher V, Catala A (2015) Design and evaluation of a tangible-mediated robot for kindergarten instruction. In: Proceedings of the 12th international conference on advances in computer entertainment technology—ACE’15. ACM Press, New York, pp 1–11Garcia-Sanjuan F, Jaen J, Jurdi S (2016) Towards encouraging communication in hospitalized children through multi-tablet activities. In: Proceedings of the XVII international conference on human computer interaction, pp 29.1–29.4Gindling J, Ioannidou A, Loh J et al (1995) LEGOsheets: a rule-based programming, simulation and manipulation environment for the LEGO programmable brick. In: Proceedings of symposium on visual languages. IEEE Computer Society Press, New York, pp 172–179Gonzalez B, Borland J, Geraghty K (2009) Whole body interaction for child-centered multimodal language learning. In: Proceedings of the 2nd workshop on child, computer and interaction—WOCCI’09. ACM Press, New York, pp 1–5Grønbæk K, Iversen OS, Kortbek KJ et al (2007) Interactive floor support for kinesthetic interaction in children learning environments. In: Human–computer interaction—INTERACT 2007. Lecture notes in computer science, pp 361–375Guha ML, Druin A, Chipman G et al (2005) Working with young children as technology design partners. Commun ACM 48:39–42. https://doi.org/10.1145/1039539.1039567Hansen JP, Alapetite A, MacKenzie IS, Møllenbach E (2014) The use of gaze to control drones. In: Proceedings of the symposium on eye tracking research and applications—ETRA’14. ACM Press, New York, pp 27–34Henkemans OAB, Bierman BPB, Janssen J et al (2017) Design and evaluation of a personal robot playing a self-management education game with children with diabetes type 1. Int J Hum Comput Stud 106:63–76. https://doi.org/10.1016/j.ijhcs.2017.06.001Horn MS, Crouser RJ, Bers MU (2011) Tangible interaction and learning: the case for a hybrid approach. Pers Ubiquitous Comput 16:379–389. https://doi.org/10.1007/s00779-011-0404-2Hourcade JP (2015) Child computer interaction. CreateSpace Independent Publishing Platform, North CharlestonHöysniemi J, Hämäläinen P, Turkki L (2004) Wizard of Oz prototyping of computer vision based action games for children. Proceeding of the 2004 conference on interaction design and children building a community—IDC’04. ACM Press, New York, pp 27–34Höysniemi J, Hämäläinen P, Turkki L, Rouvi T (2005) Children’s intuitive gestures in vision-based action games. Commun ACM 48:44–50. https://doi.org/10.1145/1039539.1039568Hsiao H-S, Chen J-C (2016) Using a gesture interactive game-based learning approach to improve preschool children’s learning performance and motor skills. Comput Educ 95:151–162. https://doi.org/10.1016/j.compedu.2016.01.005Jokela T, Rezaei PP, Väänänen K (2016) Using elicitation studies to generate collocated interaction methods. In: Proceedings of the 18th international conference on human–computer interaction with mobile devices and services adjunct, pp 1129–1133. https://doi.org/10.1145/2957265.2962654Jones B, Benko H, Ofek E, Wilson AD (2013) IllumiRoom: peripheral projected illusions for interactive experiences. In: Proceedings of the SIGCHI conference on human factors in computing systems—CHI’13, pp 869–878Jones B, Shapira L, Sodhi R et al (2014) RoomAlive: magical experiences enabled by scalable, adaptive projector-camera units. In: Proceedings of the 27th annual ACM symposium on user interface software and technology—UIST’14, pp 637–644Kaminski M, Pellino T, Wish J (2002) Play and pets: the physical and emotional impact of child-life and pet therapy on hospitalized children. Child Heal Care 31:321–335. https://doi.org/10.1207/S15326888CHC3104_5Karam M, Schraefel MC (2005) A taxonomy of gestures in human computer interactions. In: Technical report in electronics and computer science, pp 1–45Kistler F, André E (2013) User-defined body gestures for an interactive storytelling scenario. Lect Notes Comput Sci (including subser Lect Notes Artif Intell Lect Notes Bioinform) 8118:264–281. https://doi.org/10.1007/978-3-642-40480-1_17Konda KR, Königs A, Schulz H, Schulz D (2012) Real time interaction with mobile robots using hand gestures. In: Proceedings of the seventh annual ACM/IEEE international conference on human–robot interaction—HRI’12. ACM Press, New York, pp 177–178Kray C, Nesbitt D, Dawson J, Rohs M (2010) User-defined gestures for connecting mobile phones, public displays, and tabletops. In: Proceedings of the 12th international conference on human computer interaction with mobile devices and services—MobileHCI’10. ACM Press, New York, pp 239–248Kurdyukova E, Redlin M, André E (2012) Studying user-defined iPad gestures for interaction in multi-display environment. In: Proceedings of the 2012 ACM international conference on intelligent user interfaces—IUI’12. ACM Press, New York, pp 93–96Lambert V, Coad J, Hicks P, Glacken M (2014) Social spaces for young children in hospital. Child Care Health Dev 40:195–204. https://doi.org/10.1111/cch.12016Lee S-S, Chae J, Kim H et al (2013) Towards more natural digital content manipulation via user freehand gestural interaction in a living room. In: Proceedings of the 2013 ACM international joint conference on pervasive and ubiquitous computing—UbiComp’13. ACM Press, New York, p 617Malinverni L, Mora-Guiard J, Pares N (2016) Towards methods for evaluating and communicating participatory design: a multimodal approach. Int J Hum Comput Stud 94:53–63. https://doi.org/10.1016/j.ijhcs.2016.03.004Mann HB, Whitney DR (1947) On a test of whether one of two random variables is stochastically larger than the other. Ann Math Stat 18:50–60. https://doi.org/10.1214/aoms/1177730491Marco J, Cerezo E, Baldassarri S et al (2009) Bringing tabletop technologies to kindergarten children. In: Proceedings of the 23rd British HCI Group annual conference on people and computers: celebrating people and technology, pp 103–111Michaud F, Caron S (2002) Roball, the rolling robot. Auton Robots 12:211–222. https://doi.org/10.1023/A:1014005728519Micire M, Desai M, Courtemanche A et al (2009) Analysis of natural gestures for controlling robot teams on multi-touch tabletop surfaces. In: Proceedings of the ACM international conference on interactive tabletops and surfaces—ITS’09. ACM Press, New York, pp 41–48Mora-Guiard J, Crowell C, Pares N, Heaton P (2016) Lands of fog: helping children with autism in social interaction through a full-body interactive experience. In: Proceedings of the 15th international conference on interaction design and children—IDC’16. ACM Press, New York, pp 262–274Morris MR (2012) Web on the wall: insights from a multimodal interaction elicitation study. In: Proceedings of the 2012 ACM international conference on interactive tabletops and surfaces. ACM Press, New York, pp 95–104Morris MR, Wobbrock JO, Wilson AD (2010) Understanding users’ preferences for surface gestures. Proc Graph Interface 2010:261–268Nacher V, Garcia-Sanjuan F, Jaen J (2016) Evaluating the usability of a tangible-mediated robot for kindergarten children instruction. In: 2016 IEEE 16th international conference on advanced learning technologies (ICALT). IEEE, New York, pp 130–132Nahapetyan VE, Khachumov VM (2015) Gesture recognition in the problem of contactless control of an unmanned aerial vehicle. Optoelectron Instrum Data Process 51:192–197. https://doi.org/10.3103/S8756699015020132Obaid M, Häring M, Kistler F et al (2012) User-defined body gestures for navigational control of a humanoid robot. In: Lecture notes in computer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics), pp 367–377Obaid M, Kistler F, Häring M et al (2014) A framework for user-defined body gestures to control a humanoid robot. Int J Soc Robot 6:383–396. https://doi.org/10.1007/s12369-014-0233-3Obaid M, Kistler F, Kasparavičiūtė G, et al (2016) How would you gesture navigate a drone?: a user-centered approach to control a drone. In: Proceedings of the 20th international academic Mindtrek conference—AcademicMindtrek’16. ACM Press, New York, pp 113–121Pares N, Soler M, Sanjurjo À et al (2005) Promotion of creative activity in children with severe autism through visuals in an interactive multisensory environment. In: Proceeding of the 2005 conference on interaction design and children—IDC’05. ACM Press, New York, pp 110–116Pfeil K, Koh SL, LaViola J (2013) Exploring 3D gesture metaphors for interaction with unmanned aerial vehicles. In: Proceedings of the 2013 international conference on intelligent user interfaces—IUI’13, pp 257–266. https://doi.org/10.1145/2449396.2449429Piaget J (1956) The child’s conception of space. Norton, New YorkPiaget J (1973) The child and reality: problems of genetic psychology. Grossman, New YorkPiumsomboon T, Clark A, Billinghurst M, Cockburn A (2013) User-defined gestures for augmented reality. CHI’13 extended abstracts on human factors in computing systems—CHI EA’13. ACM Press, New York, pp 955–960Pons P, Carrión A, Jaen J (2018) Remote interspecies interactions: improving humans and animals’ wellbeing through mobile playful spaces. Pervasive Mob Comput. https://doi.org/10.1016/j.pmcj.2018.12.003Puranam MB (2005) Towards full-body gesture analysis and recognition. University of Kentucky, LexingtonPyryeskin D, Hancock M, Hoey J (2012) Comparing elicited gestures to designer-created gestures for selection above a multitouch surface. In: Proceedings of the 2012 ACM international conference on interactive tabletops and surfaces—ITS’12. ACM Press, New York, pp 1–10Raffle HS, Parkes AJ, Ishii H (2004) Topobo: a constructive assembly system with kinetic memory. System 6:647–654. https://doi.org/10.1145/985692.985774Read JC, Markopoulos P (2013) Child–computer interaction. Int J Child-Comput Interact 1:2–6. https://doi.org/10.1016/j.ijcci.2012.09.001Read JC, Macfarlane S, Casey C (2002) Endurability, engagement and expectations: measuring children’s fun. In: Interaction design and children, pp 189–198Read JC, Markopoulos P, Parés N et al (2008) Child computer interaction. In: Proceeding of the 26th annual CHI conference extended abstracts on human factors in computing systems—CHI’08. ACM Press, New York, pp 2419–2422Robins B, Dautenhahn K (2014) Tactile interactions with a humanoid robot: novel play scenario implementations with children with autism. Int J Soc Robot 6:397–415. https://doi.org/10.1007/s12369-014-0228-0Robins B, Dautenhahn K, Te Boekhorst R, Nehaniv CL (2008) Behaviour delay and robot expressiveness in child–robot interactions: a user study on interaction kinesics. In: Proceedings of the 3rd ACMIEEE international conference on human robot interaction, pp 17–24. https://doi.org/10.1145/1349822.1349826Ruiz J, Li Y, Lank E (2011) User-defined motion gestures for mobile interaction. In: Proceedings of the 2011 annual conference on human factors in computing systems—CHI’11. ACM Press, New York, p 197Rust K, Malu M, Anthony L, Findlater L (2014) Understanding childdefined gestures and children’s mental models for touchscreen tabletop interaction. In: Proceedings of the 2014 conference on interaction design and children—IDC’14. ACM Press, New York, pp 201–204Salter T, Dautenhahn K, Te Boekhorst R (2006) Learning about natural human-robot interaction styles. Robot Auton Syst 54:127–134. https://doi.org/10.1016/j.robot.2005.09.022Sanghvi J, Castellano G, Leite I et al (2011) Automatic analysis of affective postures and body motion to detect engagement with a game companion. In: Proceedings of the 6th international conference on human–robot interaction—HRI’11. ACM Press, New York, pp 305–311Sanna A, Lamberti F, Paravati G, Manuri F (2013) A Kinect-based natural interface for quadrotor control. Entertain Comput 4:179–186. https://doi.org/10.1016/j.entcom.2013.01.001Sato E, Yamaguchi T, Harashima F (2007) Natural interface using pointing behavior for human–robot gestural interaction. IEEE Trans Ind Electron 54:1105–1112. https://doi.org/10.1109/TIE.2007.892728Schaper M-M, Pares N (2016) Making sense of body and space through full-body interaction design. In: Proceedings of the 15th international conference on interaction design and children—IDC’16. ACM Press, New York, pp 613–618Schaper M-M, Malinverni L, Pares N (2015) Sketching through the body: child-generated gestures in full-body interaction design. In: Proceedings of the 14th international conference on interaction design and children—IDC’15. ACM Press, New York, pp 255–258Seyed T, Burns C, Costa Sousa M et al (2012) Eliciting usable gestures for multi-display environments. In: Proceedings of the 2012 ACM international conference on interactive tabletops and surfaces—ITS’12. ACM Press, New York, p 41Shimon SSA, Morrison-Smith S, John N et al (2015) Exploring user-defined back-of-device gestures for mobile devices. In: Proceedings of the 17th international conference on human–computer interaction with mobile devices and services—MobileHCI’15. ACM Press, New York, pp 227–232Sipitakiat A, Nusen N (2012) Robo-blocks: a tangible programming system with debugging for children. In: Proceedings of the 11th international conference on interaction design and children—IDC’12. ACM Press, New York, p 98Soler-Adillon J, Ferrer J, Pares N (2009) A novel approach to interactive playgrounds: the interactive slide project. In: Proceedings of the 8th international conference on interaction design and children—IDC’09. ACM Press, New York, pp 131–139Stiefelhagen R, Fogen C, Gieselmann P et al (2004) Natural human–robot interaction using speech, head pose and gestures. In: 2004 IEEE/RSJ international conference on intelligent robots and systems (IROS) (IEEE Cat. No. 04CH37566). IEEE, New York, pp 2422–2427Subrahmanyam K, Greenfield PM (1994) Effect of video game practice on spatial skills in girls and boys. J Appl Dev Psychol 15:13–32. https://doi.org/10.1016/0193-3973(94)90004-3Sugiyama J, Tsetserukou D, Miura J (2011) NAVIgoid: robot navigation with haptic vision. In: SIGGRAPH Asia 2011 emerging technologies SA’11, vol 15, p 4503. https://doi.org/10.1145/2073370.2073378Takahashi T, Morita M, Tanaka F (2012) Evaluation of a tricycle-style teleoperational interface for children: a comparative experiment with a video game controller. In: 2012 IEEE RO-MAN: the 21st IEEE international symposium on robot and human interactive communication. IEEE, New York, pp 334–338Tanaka F, Takahashi T (2012) A tricycle-style teleoperational interface that remotely controls a robot for classroom children. In: Proceedings of the seventh annual ACM/IEEE international conference on human–robot interaction—HRI’12. ACM Press, New York, pp 255–256Tjaden L, Tong A, Henning P et al (2012) Children’s experiences of dialysis: a systematic review of qualitative studies. Arch Dis Child 97:395–402. https://doi.org/10.1136/archdischild-2011-300639Vatavu R-D (2012) User-defined gestures for free-hand TV control. In: Proceedings of the 10th European conference on interactive TV and video—EuroiTV’12. ACM Press, New York, pp 45–48Vatavu R-D (2017) Smart-Pockets: body-deictic gestures for fast access to personal data during ambient interactions. Int J Hum Comput Stud 103:1–21. https://doi.org/10.1016/j.ijhcs.2017.01.005Vatavu R-D, Wobbrock JO (2015) Formalizing agreement analysis for elicitation studies: new measures, significance test, and toolkit. In: Proceedings of the 33rd annual ACM conference on human factors in computing systems—CHI’15. ACM Press, New York, pp 1325–1334Vatavu R-D, Wobbrock JO (2016) Between-subjects elicitation studies: formalization and tool support. In: Proceedings of the 2016 CHI conference on human factors in computing systems—CHI’16. ACM Press, New York, pp 3390–3402Voyer D, Voyer S, Bryden MP (1995) Magnitude of sex differences in spatial abilities: a meta-analysis and consideration of critical variables. Psychol Bull 117:250–270. https://doi.org/10.1037/0033-2909.117.2.250Wainer J, Robins B, Amirabdollahian F, Dautenhahn K (2014) Using the humanoid robot KASPAR to autonomously play triadic games and facilitate collaborative play among children with autism. IEEE Trans Auton Ment Dev 6:183–199. https://doi.org/10.1109/TAMD.2014.2303116Wang Y, Zhang L (2015) A track-based gesture recognition algorithm for Kinect. Appl Mech Mater 738–7399:334–338. https://doi.org/10.4028/www.scientific.net/AMM.738-739.334",'Springer Science and Business Media LLC',Interactive spaces for children: gesture elicitation for controlling ground mini-robots,https://riunet.upv.es/bitstream/10251/169739/1/PonsJaen%20-%20Interactive%20spaces%20for%20children%20gesture%20elicitation%20for%20controlling%20ground%20mini-robots.pdf,,10.1007/s12652-019-01290-6,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
475073071,2021-06-01T00:00:00,"An Unmanned Aerial Vehicle (UAV) can greatly reduce manpower in the agricultural plant protection such as watering, sowing, and pesticide spraying. It is essential to develop a Decision-making Support System (DSS) for UAVs to help them choose the correct action in states according to the policy. In an unknown environment, the method of formulating rules for UAVs to help them choose actions is not applicable, and it is a feasible solution to obtain the optimal policy through reinforcement learning. However, experiments show that the existing reinforcement learning algorithms cannot get the optimal policy for a UAV in the agricultural plant protection environment. In this work we propose an improved Q-learning algorithm based on similar state matching, and we prove theoretically that there has a greater probability for UAV choosing the optimal action according to the policy learned by the algorithm we proposed than the classic Q-learning algorithm in the agricultural plant protection environment. This proposed algorithm is implemented and tested on datasets that are evenly distributed based on real UAV parameters and real farm information. The performance evaluation of the algorithm is discussed in detail. Experimental results show that the algorithm we proposed can efficiently learn the optimal policy for UAVs in the agricultural plant protection environment",'MDPI AG',Improved Q-Learning Algorithm Based on Approximate State Matching in Agricultural Plant Protection Environment,,"[{'title': 'Entropy', 'identifiers': ['1099-4300', 'issn:1099-4300']}]",10.3390/e23060737,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
395142972,2019-08-11T00:00:00,"[EN] Modelling the dynamic behaviour of heavy vehicles, such as buses or trucks, can be very useful for driving simulation and training, autonomous driving, crash analysis, etc. However, dynamic modelling of a vehicle is a difficult task because there are many subsystems and signals that affect its behaviour. In addition, it might be hard to combine data because available signals come at different rates, or even some samples might be missed due to disturbances or communication issues. In this paper, we propose a non-invasive data acquisition hardware/software setup to carry out several experiments with an urban bus, in order to collect data from one of the internal communication networks and other embedded systems. Subsequently, non-conventional sampling data fusion using a Kalman filter has been implemented to fuse data gathered from different sources, connected through a wireless network (the vehicle¿s internal CAN bus messages, IMU, GPS, and other sensors placed in pedals). Our results show that the proposed combination of experimental data gathering and multi-rate filtering algorithm allows useful signal estimation for vehicle identification and modelling, even when data samples are missing.This research was funded by the Spanish Ministry of Economy and European Union, grant DPI2016-81002-R, by Generalitat Valenciana, grant APOSTD/2017/055, and by the European Union too through the European Social Fund (ESF). The authors want to acknowledge public transport operators Autos Vallduxense S.L. and EMT Valencia for their support to the project by lending their vehicles and bus drivers. The authors are also thankful to Instituto de Diseño y Fabricación and SAFETRANS project (DPI2013-42302-R) research teams, for lending most of the electronic devices and sensors used in the experimentation.Girbés, V.; Hernández, D.; Armesto, L.; Dols Ruiz, JF.; Sala, A. (2019). Drive Force and Longitudinal Dynamics Estimation in Heavy-Duty Vehicles. Sensors. 19(16):1-19. https://doi.org/10.3390/s19163515S1191916Bloom, T., & Friedman, H. (2013). Classifying dogs’ (Canis familiaris) facial expressions from photographs. Behavioural Processes, 96, 1-10. doi:10.1016/j.beproc.2013.02.010Armesto, L., Arnal, L., Dols, J., Girbés, V., & Peris, J. C. (2016). Proyecto SAFEBUS: Sistemas Avanzados de Seguridad Integral en Autobuses. Revista Iberoamericana de Automática e Informática Industrial RIAI, 13(1), 103-114. doi:10.1016/j.riai.2015.04.006Girbes, V., Armesto, L., Dols, J., & Tornero, J. (2017). An Active Safety System for Low-Speed Bus Braking Assistance. IEEE Transactions on Intelligent Transportation Systems, 18(2), 377-387. doi:10.1109/tits.2016.2573921Balsa‐Barreiro, J., Valero‐Mora, P. M., Pareja Montoro, I., & Sánchez García, M. (2013). Geo‐referencing naturalistic driving data using a novel method based on vehicle speed. IET Intelligent Transport Systems, 7(2), 190-197. doi:10.1049/iet-its.2012.0152Törnros, J. (1998). Driving behaviour in a real and a simulated road tunnel—a validation study. Accident Analysis & Prevention, 30(4), 497-503. doi:10.1016/s0001-4575(97)00099-7Bella, F. (2008). Driving simulator for speed research on two-lane rural roads. Accident Analysis & Prevention, 40(3), 1078-1087. doi:10.1016/j.aap.2007.10.015Kemeny, A., & Panerai, F. (2003). Evaluating perception in driving simulation experiments. Trends in Cognitive Sciences, 7(1), 31-37. doi:10.1016/s1364-6613(02)00011-6Girbés, V., Armesto, L., & Tornero, J. (2014). Path following hybrid control for vehicle stability applied to industrial forklifts. Robotics and Autonomous Systems, 62(6), 910-922. doi:10.1016/j.robot.2014.01.004Yan, X., Abdel-Aty, M., Radwan, E., Wang, X., & Chilakapati, P. (2008). Validating a driving simulator using surrogate safety measures. Accident Analysis & Prevention, 40(1), 274-288. doi:10.1016/j.aap.2007.06.007Hidalgo, C. E., Marcano, M., Fernández, G., & Pérez, J. M. (2020). Maniobras cooperativas aplicadas a vehículos automatizados en entornos virtuales y reales. Revista Iberoamericana de Automática e Informática industrial, 17(1), 56. doi:10.4995/riai.2019.11155Hsu, L.-Y., & Chen, T.-L. (2012). Vehicle Dynamic Prediction Systems with On-Line Identification of Vehicle Parameters and Road Conditions. Sensors, 12(11), 15778-15800. doi:10.3390/s121115778Sun, R., Cheng, Q., Xue, D., Wang, G., & Ochieng, W. (2017). GNSS/Electronic Compass/Road Segment Information Fusion for Vehicle-to-Vehicle Collision Avoidance Application. Sensors, 17(12), 2724. doi:10.3390/s17122724Jeong, Y., Son, S., Jeong, E., & Lee, B. (2018). An Integrated Self-Diagnosis System for an Autonomous Vehicle Based on an IoT Gateway and Deep Learning. Applied Sciences, 8(7), 1164. doi:10.3390/app8071164Yang Jiansen, Guo Konghui, Ding Haitao, Zhang Jianwei, & Xiang Bin. (2010). The application of SAE J1939 protocol in Automobile Smart and Integrated Control System. 2010 International Conference on Computer, Mechatronics, Control and Electronic Engineering. doi:10.1109/cmce.2010.5610301Turk, E., & Challenger, M. (2018). An android-based IoT system for vehicle monitoring and diagnostic. 2018 26th Signal Processing and Communications Applications Conference (SIU). doi:10.1109/siu.2018.8404378Ozguner, U., Redmill, K. A., & Broggi, A. (s. f.). Team terramax and the DARPA grand challenge: a general overview. IEEE Intelligent Vehicles Symposium, 2004. doi:10.1109/ivs.2004.1336387Li, Y., & Ji, X. (2013). Controller Design for ISG Hybrid Electric Vehicle Based on SAE J1939 Protocol. Proceedings of the 2nd International Conference on Computer Science and Electronics Engineering (ICCSEE 2013). doi:10.2991/iccsee.2013.647Wang Dafang, Nan Jinrui, & Sun Fengchun. (2008). The application of CAN communication in distributed control system of electric city bus. 2008 IEEE Vehicle Power and Propulsion Conference. doi:10.1109/vppc.2008.4677533Hu, J., Li, G., Yu, X., & Liu, S. (2007). Design and Application of SAE J1939 Communication Database in City-Bus Information Integrated Control System Development. 2007 International Conference on Mechatronics and Automation. doi:10.1109/icma.2007.4304114Tomero, J., & Armesto, L. (s. f.). A general formulation for generating multi-rate models. Proceedings of the 2003 American Control Conference, 2003. doi:10.1109/acc.2003.1239742Tan, H., Shen, B., Liu, Y., Alsaedi, A., & Ahmad, B. (2017). Event-triggered multi-rate fusion estimation for uncertain system with stochastic nonlinearities and colored measurement noises. Information Fusion, 36, 313-320. doi:10.1016/j.inffus.2016.12.00",'MDPI AG',Drive Force and Longitudinal Dynamics Estimation in Heavy-Duty Vehicles,https://riunet.upv.es/bitstream/handle/10251/161981/Girb%c3%a9s%3bHern%c3%a1ndez%3bArmesto%20-%20Drive%20Force%20and%20Longitudinal%20Dynamics%20Estimation%20in%20Heavy-Duty%20Vehicles.pdf?sequence=1&isAllowed=y,,10.3390/s19163515,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
4744310,2006-05-01T00:00:00,"Published in Journal of Aerospace Computing, Information, and Communication, 3(5):187-213, May, 2006.Received 27 July 2005; revision received and accepted for publication 17 November 2005. Copyright 2005 by Henrik B.
Christophersen, R. Wayne. Pickell, James C. Neidhoefer, Adrian A. Koller, Suresh, K. Kannan and Eric N. Johnson. Published
by the American Institute of Aeronautics and Astronautics, Inc., with permission.The Flight Control System 20 (FCS20) is a compact, self-contained Guidance, Navigation,
and Control system that has recently been developed to enable advanced autonomous
behavior in a wide range of Unmanned Aerial Vehicles (UAVs). The FCS20 uses a floating
point Digital Signal Processor (DSP) for high level serial processing, a Field Programmable
Gate Array (FPGA) for low level parallel processing, and GPS and Micro Electro Mechanical
Systems (MEMS) sensors. In addition to guidance, navigation, and control functions, the
FCS20 is capable of supporting advanced algorithms such as automated reasoning, artificial
vision, and multi-vehicle interaction. The unique contribution of this paper is that it gives a
complete overview of the FCS20 GN&C system, including computing, communications, and
information aspects. Computing aspects of the FCS20 include details about the design process,
hardware components, and board configurations, and specifications. Communications
aspects of the FCS20 include descriptions of internal and external data flow. The information
section describes the FCS20 Operating System (OS), the Support Vehicle Interface Library
(SVIL) software, the navigation Extended Kalman Filter, and the neural network based
adaptive controller. Finally, simulation-based results as well as actual flight test results that
demonstrate the operation of the guidance, navigation, and control algorithms on a real
Unmanned Aerial Vehicle (UAV) are presented","American Institute of Aeronautics and Astronautics, Inc.","A Compact Guidance, Navigation, and Control System for Unmanned Aerial Vehicles",https://core.ac.uk/download/4744310.pdf,"[{'title': 'Journal of Aerospace Computing Information and Communication', 'identifiers': ['1542-9423', 'issn:1542-9423']}]",,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
212471168,2018-01-01T00:00:00,"Autonomous cooperative driving systems require the integration of research activities in the field of embedded systems, robotics, communication, control and artificial intelligence in order to create a secure and intelligent autonomous drivers behaviour patterns in the traffic. Beside autonomous vehicle management, an important research focus is on the cooperation behaviour management. In this paper, we propose hybrid automaton modelling to emulate flexible vehicle Platoon and vehicles cooperation interactions. We introduce novel coding function for Platoon cooperation behaviour profile generation in time, which depends of vehicles number in Platoon and behaviour types. As the behaviour prediction of transportation systems, one of the primarily used methods of artificial intelligence in Intelligent Transport Systems, we propose an approach towards NARX neural network prediction of Platoon cooperation behaviour profile. With incorporation of Platoon manoeuvres dynamic prediction, which is capable of analysing traffic behaviour, this approach would be useful for secure implementation of real autonomous vehicles cooperation",'Mechanical Engineering Faculty in Slavonski Brod',Hybrid Automaton Based Vehicle Platoon Modelling and Cooperation Behaviour Profile Prediction,https://core.ac.uk/download/212471168.pdf,,10.17559/TV-20170308230100,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
491013080,2021-11-01T00:00:00,"This paper presents the implementation of an autonomous electric vehicle (EV) project in the National Taiwan University of Science and Technology (NTUST) campus in Taiwan. The aim of this work was to integrate two important practices of realizing an autonomous vehicle in a campus environment, including vehicle positioning and path tracking. Such a project is helpful to the students to learn and practice key technologies of autonomous vehicles conveniently. Therefore, a laboratory-made EV was equipped with real-time kinematic GPS (RTK-GPS) to provide centimeter position accuracy. Furthermore, the model predictive control (MPC) was proposed to perform the path tracking capability. Nevertheless, the RTK-GPS exhibited some robust positioning concerns in practical application, such as a low update rate, signal obstruction, signal drift, and network instability. To solve this problem, a multisensory fusion approach using an unscented Kalman filter (UKF) was utilized to improve the vehicle positioning performance by further considering an inertial measurement unit (IMU) and wheel odometry. On the other hand, the model predictive control (MPC) is usually used to control autonomous EVs. However, the determination of MPC parameters is a challenging task. Hence, reinforcement learning (RL) was utilized to generalize the pre-trained datum value for the determination of MPC parameters in practice. To evaluate the performance of the RL-based MPC, software simulations using MATLAB and a laboratory-made, full-scale electric vehicle were arranged for experiments and validation. In a 199.27 m campus loop path, the estimated travel distance error was 0.82% in terms of UKF. The MPC parameters generated by RL also achieved a better tracking performance with 0.227 m RMSE in path tracking experiments, and they also achieved a better tracking performance when compared to that of human-tuned MPC parameters",'MDPI AG',Integrating Vehicle Positioning and Path Tracking Practices for an Autonomous Vehicle Prototype in Campus Environment,,"[{'title': 'Electronics', 'identifiers': ['issn:2079-9292', '2079-9292']}]",10.3390/electronics10212703,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
475072228,2021-06-01T00:00:00,"Unmanned aerial vehicle (UAV) imaging is a promising data acquisition technique for image-based plant phenotyping. However, UAV images have a lower spatial resolution than similarly equipped in field ground-based vehicle systems, such as carts, because of their distance from the crop canopy, which can be particularly problematic for measuring small-sized plant features. In this study, the performance of three deep learning-based super resolution models, employed as a pre-processing tool to enhance the spatial resolution of low resolution images of three different kinds of crops were evaluated. To train a super resolution model, aerial images employing two separate sensors co-mounted on a UAV flown over lentil, wheat and canola breeding trials were collected. A software workflow to pre-process and align real-world low resolution and high-resolution images and use them as inputs and targets for training super resolution models was created. To demonstrate the effectiveness of real-world images, three different experiments employing synthetic images, manually downsampled high resolution images, or real-world low resolution images as input to the models were conducted. The performance of the super resolution models demonstrates that the models trained with synthetic images cannot generalize to real-world images and fail to reproduce comparable images with the targets. However, the same models trained with real-world datasets can reconstruct higher-fidelity outputs, which are better suited for measuring plant phenotypes",'MDPI AG',Spatial Super Resolution of Real-World Aerial Images for Image-Based Plant Phenotyping,,"[{'title': 'Remote Sensing', 'identifiers': ['2072-4292', 'issn:2072-4292']}]",10.3390/rs13122308,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
