id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract
1,included,10.1109/isaect50560.2020.9523700,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9523700/,11/27/2020 0:00,edge-cloud architectures using uavs dedicated to industrial iot monitoring and control applications,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud."
2,included,10.1109/icccn52240.2021.9522281,2021 International Conference on Computer Communications and Networks (ICCCN),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e5db631b9237de584eadab60dd6529470438ad5d,1/1/2021 0:00,realization of an intrusion detection use-case in onap with acumos,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms."
3,included,10.1007/s44196-021-00040-x,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s44196-021-00040-x,11/20/2021,edge computing using embedded webserver with mobile device for diagnosis and prediction of metastasis in histopathological images,"Diagnosis of different breast cancer stages using histopathology whole slide images is the gold standard in grading the tissue metastasis. Traditional diagnosis involves labor intensive procedures and is prone to human errors. Computer aided diagnosis assists medical experts as a second opinion tool in early detection which prevents further proliferation. Computing facilities have emerged to an extent where algorithms can attain near human accuracy in prediction of diseases, offering better treatment to curb further proliferation. The work introduced in the paper provides an interface in mobile platform, which enables the user to input histopathology image and obtain the prediction results with its class probability through embedded web-server. The trained deep convolutional neural networks model is deployed into a microcomputer-based embedded system after hyper-parameter tuning, offering congruent performance. The implementation results show that the embedded platform with custom-trained CNN model is suitable for medical image classification, as it takes less execution time and mean prediction time. It is also noticed that customized CNN classifier model outperforms pre-trained models when used in embedded platforms for prediction and classification of histopathology images. This work also emphasizes the relevance of portable and flexible embedded device in real time clinical applications."
4,included,10.1007/s42979-021-00726-1,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s42979-021-00726-1,6/19/2021 0:00,towards regulatory-compliant mlops: oravizio’s journey from a machine learning experiment to a deployed certified medical product,"Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context."
5,included,10.1007/978-3-030-77070-9_10,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-77070-9_10,1/1/2021 0:00,smart and intelligent chatbot assistance for future industry 4.0,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers."
6,included,10.1109/rweek.2018.8473535,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8473535/,8/23/2018 0:00,framework for data driven health monitoring of cyber-physical systems,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features."
7,included,10.1109/access.2020.2970178,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8974224/,1/1/2020 0:00,a novel software engineering approach toward using machine learning for improving the efficiency of health systems,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications."
8,included,10.1109/iceccme52200.2021.9591113,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9591113/,10/8/2021 0:00,cobots for fintech,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space."
9,included,10.1109/icdmw.2019.00123,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8955523/,11/11/2019 0:00,implementation of mobile-based real-time heart rate variability detection for personalized healthcare,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework."
10,included,10.1109/qrs51102.2020.00018,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9282796/,12/14/2020 0:00,phm technology for memory anomalies in cloud computing for iaas,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier."
11,included,10.1109/itc-egypt52936.2021.9513888,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9513888/,7/15/2021 0:00,a proposed end to end telemedicine system based on embedded system and mobile application using cmos wearable sensors,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver."
12,included,10.1016/j.iot.2020.100185,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85086362688,9/1/2020,highly-efficient fog-based deep learning aal fall detection system,"Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement."
13,included,10.1109/syscon.2018.8369547,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8369547/,4/26/2018 0:00,an interactive architecture for industrial scale prediction: industry 4.0 adaptation of machine learning,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics."
14,included,10.1109/med.2017.7984310,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7984310/,7/6/2017 0:00,cloud computing for big data analytics in the process control industry,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0."
15,included,10.1109/isie45063.2020.9152441,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9152441/,6/19/2020 0:00,deployment of a smart and predictive maintenance system in an industrial case study,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions."
16,included,10.1016/j.adhoc.2019.102047,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076174369,3/1/2020,an intelligent edge-iot platform for monitoring livestock and crops in a dairy farming scenario,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud."
17,included,10.1109/mocast49295.2020.9200283,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9200283/,9/9/2020 0:00,a cloud based smart recycling bin for waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low."
18,included,10.1145/3326285.3329051,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9068647/,6/25/2019 0:00,leap: learning-based smart edge with caching and prefetching for adaptive video streaming,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4&#x0025; and reduces video rebuffering by 42.7&#x0025;, which leads to at least 15.9&#x0025; improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP."
19,included,10.1109/isc2.2016.7580798,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7580798/,9/15/2016 0:00,smartseal: a ros based home automation framework for heterogeneous devices interconnection in smart buildings,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it."
20,included,http://arxiv.org/abs/2009.10679v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2009.10679v1,9/22/2020 0:00,"an embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames."
21,included,10.1109/bigdata.2018.8621926,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8621926/,12/13/2018 0:00,harnessing the nature of spam in scalable online social spam detection,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management."
22,included,10.1109/icfec51620.2021.00018,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9458893/,5/13/2021 0:00,a privacy preserving system for ai-assisted video analytics,"The emerging Edge computing paradigm facilitates the deployment of distributed AI-applications and hardware, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits for parties in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, the widespread deployment of such mechanism in public areas are a growing cause of privacy and security concerns. Data protection strategies need to be appropriately designed and correctly implemented in order to mitigate the associated risks. Most existing approaches focus on privacy and security related operations of the video stream itself or protecting its transmission. In this paper, we propose a privacy preserving system for AI-assisted video analytics, that extracts relevant information from video data and governs the secure access to that information. The system ensures that applications leveraging extracted data have no access to the video stream. An attribute-based authorization scheme allows applications to only query a predefined subset of extracted data. We demonstrate the feasibility of our approach by evaluating an application motivated by the recent COVID-19 pandemic, deployed on typical edge computing infrastructure."
23,included,http://arxiv.org/abs/2008.05255v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2008.05255v1,8/12/2020 0:00,"identity-aware attribute recognition via real-time distributed inference
  in mobile edge clouds","With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods."
24,included,10.1109/compsac48688.2020.0-168,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9202848/,7/17/2020 0:00,an early warning system for hemodialysis complications utilizing transfer learning from hd iot dataset,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed."
25,included,10.1109/cbms.2019.00041,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8787393/,6/7/2019 0:00,action recognition in real homes using low resolution depth video data,"We report work in progress from interdisciplinary research on Assisted Living Technology in smart homes for older adults with mild cognitive impairments or dementia. We present our field trial, the set-up for collecting and storing data from real homes, and preliminary results on action recognition using low resolution depth video cameras. The data have been collected from seven apartments with one resident each over a period of two weeks. We propose a pre-processing of the depth videos by applying an Infinite Response Filter (IIR) for extracting the movements in the frames prior to classification. In this work we classify four actions: TV interaction (turn it on/ off and switch over), standing up, sitting down, and no movement. Our first results indicate that using the IIR filter for movement information extraction improves accuracy and can be an efficient method for recognizing actions. Our current implementation uses a convolutional long short-term memory (ConvLSTM) neural network, and achieved an average peak accuracy of 86%."
26,included,http://arxiv.org/abs/2004.05953v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2004.05953v1,4/13/2020 0:00,"software-defined network for end-to-end networked science at the
  exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence."
27,included,10.1184/r1/6710654.v1,,core,health,health' AND 'machine learning' AND ('real-world' AND 'deploy'),10.1184/r1/6710654.v1,6/30/2018 0:00,software and system health management for autonomous robotics missions,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time"
28,included,10.1109/wf-iot.2019.8767231,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8767231/,4/18/2019 0:00,efficient deployment of predictive analytics in edge gateways: fall detection scenario,"Ambient Assisted Living (AAL) represents the most promising Internet of Things (IoT) application due to its relevance in the elders healthcare and improvement of their quality of life. Recently, the AAL IoT ecosystem has been enriched with promising technologies such as edge computing, which has demonstrated to be the best approach to overcome the demanding requirements of AAL and healthcare services by providing a reduction of the amount of data to transfer to the cloud, an improvement of the response time, and quality of experience. Also, the deployment of Artificial Intelligence (AI) technologies at the edge provides intelligence to improve the decision making timely. However, this approach has been scarcely studied in AAL scenarios and the few proposals based on deploying machine learning models at the edge lack efficiency, security, mechanisms of resource management, service management, and deployment, as well as a real and experimental AAL scenario. For these reasons, this paper proposes an innovative edge gateway architecture to support the deployment of deep learning (DL) models in AAL and healthcare scenarios efficiently. To do so, we have added a predictive analytics module to deploy the models. Since AI technologies demand more resources, a container-based virtualization technology is employed on the edge gateway to manage the limited resources, and provide security and lifecycle services management. The edge gateway performance was evaluated deploying a DL-based fall detection application on it. As a result, our approach improves the inference time compared to that based on the cloud in 34 seconds and to similar approaches in 8 seconds."
29,included,http://arxiv.org/abs/1805.05491v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1805.05491v1,5/14/2018 0:00,"crowdbreaks: tracking health trends using public social media data and
  crowdsourcing","In the past decade, tracking health trends using social media data has shown
great promise, due to a powerful combination of massive adoption of social
media around the world, and increasingly potent hardware and software that
enables us to work with these new big data streams. At the same time, many
challenging problems have been identified. First, there is often a mismatch
between how rapidly online data can change, and how rapidly algorithms are
updated, which means that there is limited reusability for algorithms trained
on past data as their performance decreases over time. Second, much of the work
is focusing on specific issues during a specific past period in time, even
though public health institutions would need flexible tools to assess multiple
evolving situations in real time. Third, most tools providing such capabilities
are proprietary systems with little algorithmic or data transparency, and thus
little buy-in from the global public health and research community. Here, we
introduce Crowdbreaks, an open platform which allows tracking of health trends
by making use of continuous crowdsourced labelling of public social media
content. The system is built in a way which automatizes the typical workflow
from data collection, filtering, labelling and training of machine learning
classifiers and therefore can greatly accelerate the research process in the
public health domain. This work introduces the technical aspects of the
platform and explores its future use cases."
30,included,10.1007/978-3-030-28925-6_1,Springer,springer,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-28925-6_1,1/1/2020 0:00,cityflow: supporting spatial-temporal edge computing for urban machine learning applications,"A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities."
31,included,10.1109/tnsm.2019.2929511,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8765778/,9/1/2019 0:00,itelescope: softwarized network middle-box for real-time video telemetry and classification,"Video continues to dominate network traffic, yet operators today have poor visibility into the number, duration, and resolutions of the video streams traversing their domain. Current monitoring approaches are inaccurate, expensive, or unscalable, as they rely on statistical sampling, middle-box hardware, or packet inspection software. We present iTelescope, the first intelligent, inexpensive, and scalable softwarized network middle-box solution for identifying and classifying video flows in realtime. Our solution is novel in combining dynamic flow rules with telemetry and machine learning, and is built on commodity OpenFlow switches and open-source software. We develop a fully functional system, train it in the lab using multiple machine learning algorithms, and validate its performance to show over 95% accuracy in identifying and classifying video streams from many providers, including YouTube and Netflix. Lastly, we conduct tests to demonstrate its scalability to tens of thousands of concurrent streams, and deploy it live on a campus network serving several hundred real users. Our traffic monitoring system gives unprecedented fine-grained real-time visibility of video streaming performance to operators of enterprise and carrier networks at very low cost."
32,included,10.1016/j.jmsy.2021.04.005,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85106283308,7/1/2021,learningadd: machine learning based acoustic defect detection in factory automation,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability."
33,included,10.1109/icitr51448.2020.9310890,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9310890/,12/4/2020 0:00,hybrid approach and architecture to detect fake news on twitter in real-time using neural networks,"Fake news has been a key issue since the dawn of social media. Currently, we are at a stage where it is merely impossible to differentiate between real and fake news. This directly and indirectly affects people's decision patterns and makes us question the credibility of the news shared via social media platforms. Twitter is one of the leading social networks in the world by active users. There has been an exponential spread of fake news on Twitter in the recent past. In this paper, we will discuss the implementation of a browser extension which will identify fake news on Twitter using deep learning models with a focus on real-world applicability, architectural stability and scalability of such a solution. Experimental results show that the proposed browser extension has an accuracy of 86% accuracy in fake news detection. To the best of our knowledge, our work is the first of its kind to detect fake news on Twitter real-time using a hybrid approach and evaluate using real users."
34,included,10.1109/icac.2017.21,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8005354/,7/21/2017 0:00,ananke: a q-learning-based portfolio scheduler for complex industrial workflows,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns."
35,included,10.1109/robot.2004.1308781,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1308781/,5/1/2004 0:00,software approach for the autonomous inspection robot makro,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level."
36,included,10.1109/fdl53530.2021.9568376,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9568376/,9/10/2021 0:00,a container-based design methodology for robotic applications on kubernetes edge-cloud architectures,"Programming modern Robots&#x0027; missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub."
37,included,http://arxiv.org/abs/1804.09914v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1804.09914v1,4/26/2018 0:00,"itelescope: intelligent video telemetry and classification in real-time
  using software defined networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost."
38,included,http://arxiv.org/abs/2101.04930v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.04930v2,1/13/2021 0:00,"an empirical study on deployment faults of deep learning based mobile
  applications","Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices."
39,included,10.1109/aero50100.2021.9438232,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9438232/,3/13/2021 0:00,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations."
40,included,http://arxiv.org/abs/1802.08960v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1802.08960v2,2/25/2018 0:00,"bonnet: an open-source training and deployment framework for semantic
  segmentation in robotics using cnns","The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos."
41,included,10.5555/3154630.3154662,Symposium on Networked Systems Design and Implementation,semantic_scholar,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/96fff7583363c2faa1d7d65332be938ada896114,1/1/2017 0:00,pytheas enabling data driven quality of experience optimization using group based exploration exploitation,"Content providers are increasingly using data-driven mechanisms to optimize quality of experience (QoE). Many existing approaches formulate this process as a prediction problem of learning optimal decisions (e.g., server, bitrate, relay) based on observed QoE of recent sessions. While prediction-based mechanisms have shown promising QoE improvements, they are necessarily incomplete as they: (1) suffer from many known biases (e.g., incomplete visibility) and (2) cannot respond to sudden changes (e.g., load changes). Drawing a parallel from machine learning, we argue that data-driven QoE optimization should instead be cast as a real-time exploration and exploitation (E2) process rather than as a prediction problem. Adopting E2 in network applications, however, introduces key architectural (e.g., how to update decisions in real time with fresh data) and algorithmic (e.g., capturing complex interactions between session features vs. QoE) challenges. We present Pytheas, a framework which addresses these challenges using a group-based E2 mechanism. The insight is that application sessions sharing the same features (e.g., IP prefix, location) can be grouped so that we can run E2 algorithms at a per-group granularity. This naturally captures the complex interactions and is amenable to realtime control with fresh measurements. Using an end-to-end implementation and a proof-of-concept deployment in CloudLab, we show that Pytheas improves video QoE over a state-of-the-art prediction-based system by up to 31% on average and 78% on 90th percentile of persession QoE."
42,included,10.1109/aivr.2018.00018,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8613637/,12/12/2018 0:00,a compensation method of two-stage image generation for human-ai collaborated in-situ fashion design in augmented reality environment,"In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real human computer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design."
43,included,10.1109/iceeccot46775.2019.9114716,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9114716/,12/14/2019 0:00,facial recognition using machine learning algorithms on raspberry pi,"Facial recognition is a non-invasive method of biometric authentication and useful for numerous applications. The real time implementation of the algorithm with adequate accuracy is required, with hardware timing into consideration. This paper deals with the implementation of machine learning algorithm for real time facial image recognition. Two dominant methods out of many facial recognition methods are discussed, simulated and implemented using Raspberry Pi. A rigorous comparative analysis is presented considering various limitations which may be the case required for innumerable application which utilize facial recognition. The drawbacks and different use cases of each method is highlighted. The facial recognition software uses algorithms to compare a digital image captured through a camera, to the stored face print so as to authenticate a person's identity. The Haar-Cascade method was one of the first methods developed for facial recognition. The HOG (Histogram of Oriented Gradients) method has worked very effectively for object recognition and thus suitable for facial recognition also. Both the methods are compared with Eigen feature-based face recognition algorithm. Various important features are experimented like speed of operation, lighting condition, frontal face profile, side profiles, distance of image, size of image etc. The facial recognition model is implemented to detect and recognize faces in real-time by means of Raspberry Pi and Pi camera for the user defined database in addition to the available databases."
44,included,10.1109/cnna.2010.5430245,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5430245/,2/5/2010 0:00,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates."
45,included,10.1109/lars-sbr.2016.49,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7783535/,10/12/2016 0:00,integration of people detection and simultaneous localization and mapping systems for an autonomous robotic platform,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment."
46,included,10.23919/iconac.2019.8895095,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8895095/,9/7/2019 0:00,ant colony optimization algorithm for industrial robot programming in a digital twin,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention."
47,included,http://arxiv.org/abs/2007.02351v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2007.02351v1,11/18/2020 0:00,offline model guard: secure and private ml on mobile devices,"Performing machine learning tasks in mobile applications yields a challenging
conflict of interest: highly sensitive client information (e.g., speech data)
should remain private while also the intellectual property of service providers
(e.g., model parameters) must be protected. Cryptographic techniques offer
secure solutions for this, but have an unacceptable overhead and moreover
require frequent network interaction. In this work, we design a practically
efficient hardware-based solution. Specifically, we build Offline Model Guard
(OMG) to enable privacy-preserving machine learning on the predominant mobile
computing platform ARM - even in offline scenarios. By leveraging a trusted
execution environment for strict hardware-enforced isolation from other system
components, OMG guarantees privacy of client data, secrecy of provided models,
and integrity of processing algorithms. Our prototype implementation on an ARM
HiKey 960 development board performs privacy-preserving keyword recognition
using TensorFlow Lite for Microcontrollers in real time."
48,included,10.1109/iria53009.2021.9588707,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9588707/,9/22/2021 0:00,automatic license plate recognition system using ssd,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95&#x0025;."
49,included,10.1109/ic4me247184.2019.9036531,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9036531/,7/12/2019 0:00,object detection based security system using machine learning algorthim and raspberry pi,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally."
50,included,10.1109/icecce49384.2020.9179349,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9179349/,6/13/2020 0:00,a cloud based smart recycling bin for in-house waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations."
51,included,10.1109/ams.2017.22,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8424312/,12/6/2017 0:00,autonomous rover navigation using gps based path planning,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino)."
52,included,10.1016/j.micpro.2019.102960,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85077060597,3/1/2020,a novel hybrid optimized and adaptive reconfigurable framework for the implementation of hybrid bio-inspired classifiers for diagnosis,"Due to recent advances in IoT (Internet of Things) technologies, availability of reliable data and emergence of machine learning, bio-inspired learning and artificial intelligence, has demonstrated its ability to solve the large complex problems which is not possible before. In particular, machine learning and bio-inspired learning algorithms provides the effective solutions in image processing techniques. However, the implementation of the above-mentioned algorithms in the general CPU requires the intensive usage of bandwidth, area and power which makes the CPU unhealthy of usage and implementation. To overcome this problem, ASIC (application specific integrated circuits), GPU (Graphics Processing Unit) &FPGA (Field Programmable gate arrays) have been employed to improve the performance of the hybrid machine learning (ML) classifiers and deep learning algorithms. FPGA has been recently employed for an effective implementation and to achieve the high performance of the learning algorithms. But integrating the complex learning algorithms in FPGA still remains to be real challenge among the researchers. The paper proposes new reconfigurable architectures for bio- inspired classifiers to diagnosis the medical casualties which can be suitable for the tele health care applications. This paper aim is as follows (i) Design and implementation of Parallel Fusion of FSM and Reconfigurable shared Distributed Arithmetic for Bio-Inspired Classifiers (ii) Development of Accelerator Environment to test the performance of proposed architecture (iii) Performance evaluation of proposed architecture in terms of accuracy of detection in compared with MATLAB simulation iv) Implementation of proposed architectures in different ARtix-7 architectures and determination of power, throughput and area . Moreover, the proposed architecture has been tested with the and compared with the other existing architectures."
53,included,10.1109/ICAIIC.2019.8669047,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8669047,2/13/2019 0:00,a complete multi cpu fpga based design and prototyping methodology for autonomous vehicles multiple object detection and recognition case study,"Embedded smart systems are Hardware/Software (HW/SW) architectures integrated in new autonomous vehicles in order to increase their smartness. A key example of such applications are camera-based automatic parking systems. In this paper we introduce a fast prototyping perspective within a complete design methodology for these embedded smart systems. One of our main objective being to reduce development and prototyping time, compared to usual simulation approaches. Based on our previous work [1], a supervised machine learning approach, we propose a HW/SW algorithm implementation for objects detection and recognition around autonomous vehicles. We validate our real-time approach via a quick prototype on the top of a Multi-CPU/FPGA platform (ZYNQ). The main contribution of this current work is the definition of a complete design methodology for smart embedded vehicle applications which defines four main parts: specification & native software, hardware acceleration, machine learning software, and the real embedded system prototype. Toward a full automation of our methodology, several steps are already automated and presented in this work. Our hardware acceleration of point cloud-based data processing tasks is 300 times faster than a pure software implementation."
54,included,10.1109/ijcnn.2013.6706957,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6706957/,8/9/2013 0:00,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors."
55,included,10.1109/aiiot52608.2021.9454183,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9454183/,5/13/2021 0:00,image classification with knowledge-based systems on the edge for real-time danger avoidance in robots,"Mobile robots are increasingly common in society and are increasingly being used for complex and high-stakes tasks such as search and rescue. The growing requirements for these robots demonstrate a need for systems which can review and react in real time to environmental hazards, which will allow robots to handle environments that are both dynamic and dangerous. We propose and test a system which allows mobile robots to reclassify environmental objects during operation in conjunction with an edge system. We train an image classification model with 99 percent accuracy and deploy it in conjunction with an edge server and JSON-based ruleset to allow robots to react to and avoid hazards."
56,included,10.1109/vlsid51830.2021.00035,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9407373/,2/24/2021 0:00,binary neural network based real time emotion detection on an edge computing device to detect passenger anomaly,"Passenger safety in public transportation especially while riding in the form of shared cabs, and taxis are often ignored, and not much preventive protocols are devised. In the connected mobility world, emotion recognition from facial expressions is a possibility, however a faster processing and edge computing device to derive anomaly state inferences will be apt for further notifying about the safety of the passenger. FPGA implementation is a viable approach to not only implement in the embedded system automotive electronics, but also accelerate the inference results, hence making it as an ideal real time candidate for passenger anomaly state identification. For the same, a real time emotion detection system using facial features was implemented on FPGA. A Binary Neural Network (BNN) feeded by Local Binary Pattern (LBP) output was designed towards the development of an improved and faster emotion recognition system. LBP is configured as a preprocessing step to extract facial features that is passed on to the BNN layer for successful inference. The preprocessing method utilizes Viola-Jones (VJ) algorithm to extract facial data while removing other background information from the image. The LBP-BNN network is modelled using Facial Expression 2013 (FER-2013) data set for training. The custom hardware accelerator or the overlay is synthesized and the designed IP is implemented on FPGA for the inference. Inference is done using the trained model on FPGA to enable faster classified results. Emotion detection using facial expressions is classified to six states namely: angry, disgust, fear, happy, sad, and surprise. The LBP-BNN network is implemented in FPGA, to realize a real time facial emotion recognition by capturing the image of a person from a web camera interfaced to the FPGA acting as edge computing inference device, with acceptable accuracy. The image processing based emotion detection design is highly suitable for other applications including tracking of emotions for movement disorder patients in hospitals."
57,included,10.1109/tifs.2021.3131026,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9627681/,1/1/2022 0:00,poligraph: intrusion-tolerant and distributed fake news detection system,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally ( ${4\%}$ – ${7\%}$ ) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach."
58,included,http://arxiv.org/abs/2201.09550v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2201.09550v1,1/24/2022 0:00,crowd tracking and monitoring middleware via map-reduce,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques."
59,included,10.1109/bigdata.2015.7363884,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7363884/,11/1/2015 0:00,pairs: a scalable geo-spatial data analytics platform,"Geospatial data volume exceeds hundreds of Petabytes and is increasing exponentially mainly driven by images/videos/data generated by mobile devices and high resolution imaging systems. Fast data discovery on historical archives and/or real time datasets is currently limited by various data formats that have different projections and spatial resolution, requiring extensive data processing before analytics can be carried out. A new platform called Physical Analytics Integrated Repository and Services (PAIRS) is presented that enables rapid data discovery by automatically updating, joining, and homogenizing data layers in space and time. Built on top of open source big data software, PAIRS manages automatic data download, data curation, and scalable storage while being simultaneously a computational platform for running physical and statistical models on the curated datasets. By addressing data curation before data being uploaded to the platform, multi-layer queries and filtering can be performed in real time. In addition, PAIRS offers a foundation for developing custom analytics. Towards that end we present two examples with models which are running operationally: (1) high resolution evapo-transpiration and vegetation monitoring for agriculture and (2) hyperlocal weather forecasting driven by machine learning for renewable energy forecasting."
60,included,10.1109/icsssm.2016.7538620,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7538620/,6/26/2016 0:00,emotion-based social computing platform for streaming big-data: architecture and application,"Exploration of user generated content in the epoch of Web 2.0 brings unprecedented challenge to the social computing, which has to provide real-time solution in the circumstance of massive data volumes and evolving application scenarios. This paper presents an emotion-based social computing platform namely ESC for streaming big-data. The main aim of ESC is to provide sentiment analysis as the foundation of social computing and enable both real-time computation on streaming big-data and batch computation on off-line big-data with high performance and low risk. Different from conventional data processing technologies, ESC is designed as a scalable and QoS-optimized adaptive platform for developers to only focus on business models instead of being distracted by details of the computing infrastructure. In addition, continuous streaming computing is emphasized in ESC to keep tracking on long term dynamic evolution in social media, which can provide a valuable proxy for in-depth social analytics. The architecture of ESC is implemented by distributed storage, sentiment analysis, data parallelism and routing, real-time streaming computation, batch computation and distributed machine learning. And the evaluation results from real-time and batch computations testify the high performance and scalability of ESC. Moreover, a few applications based on it further demonstrates its usability in enacting on different streaming big-data and variety of social computations."
61,included,10.1016/j.procs.2020.03.044,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85085566175,1/1/2020,an artificial intelligence based crowdsensing solution for on-demand accident scene monitoring,"Road traffic crashes have a devastating impact on societies by claiming more than 1.35 million lives each year and causing up to 50 million injuries. Improving the efficiency of emergency management systems constitutes a key measure to reduce road traffic deaths and injuries. In this work, we propose a comprehensive crowdsensing-based solution for the real-time collection and the analysis of accident scene intelligence as a means to improve the efficiency of the emergency response process and help reduce road fatalities. The solution leverages sensory, mobile, and web technologies for the real-time monitoring of accident scenes, and employs Artificial Intelligence for the automatic analysis of the accident scene data, to allow the automatic generation of accident intelligence reports. Police officers and rescue teams can use those reports for fast and accurate situational assessment and effective response to emergencies. The proposed system was fully implemented and its operation was successfully tested using a variety of scenarios. This work gives interesting insights into the possibility of leveraging crowdsensing and artificial intelligence for offering emergency situational awareness and improving the efficiency of emergency response operations."
62,included,10.1109/lra.2020.2998414,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9103220/,7/1/2020 0:00,rilaas: robot inference and learning as a service,"Programming robots is complicated due to the lack of `plug-and-play' modules for skill acquisition. Virtualizing deployment of deep learning models can facilitate large-scale use/re-use of off-the-shelf functional behaviors. Deploying deep learning models on robots entails real-time, accurate and reliable inference service under varying query load. This letter introduces a novel Robot-Inference-and-Learning-as-a-Service (RILaaS) platform for low-latency and secure inference serving of deep models that can be deployed on robots. Unique features of RILaaS include: 1) low-latency and reliable serving with gRPC under dynamic loads by distributing queries over multiple servers on Edge and Cloud, 2) SSH based authentication coupled with SSL/TLS based encryption for security and privacy of the data, and 3) front-end REST API for sharing, monitoring and visualizing performance metrics of the available models. We report experiments to evaluate the RILaaS platform under varying loads of batch size, number of robots, and various model placement hosts on Cloud, Edge, and Fog for providing benchmark applications of object recognition and grasp planning as a service. We address the complexity of load balancing with a reinforcement learning algorithm that optimizes simulated profiles of networked robots; outperforming several baselines including round robin, least connections, and least model time with 68.30% and 14.04% decrease in round-trip latency time across models compared to the worst and the next best baseline respectively. Details and updates are available at: https://sites.google.com/view/rilaas."
63,included,10.1016/j.procs.2019.09.169,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076257910,1/1/2019,iassistme - adaptable assistant for persons with eye disabilities,"Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers."
64,included,10.22323/1.372.0041,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/946e25b009baa067d99f6c2397a5fed72de260db,1/1/2020 0:00,machine learning-based system for the availability and reliability assessment and management of critical infrastructures (caso),"A critical infrastructure is a complex interconnected system of systems providing basic and essential services to support the operation of particle accelerators but also industries and households for which they must guarantee high reliability of critical functions. 
Model-based approaches are usually adopted to provide an early identiﬁcation of failures and to reveal hidden dependencies among subsystems. System models are complex and require constant updating to be reactive to system changes and real operating conditions, wear and aging. The interconnections between the different systems and the functional dependencies between their components are in many cases modified at both physical and functional levels while their degraded performances impact the overall system availability and reliability. 
A novel approach is proposed which combines model-based and Big Data analytics by machine learning techniques to extract descriptive and predictive models directly from data. The objective is to foresee and react in time to failures to reduce downtimes as well as to optimize maintenance and operation costs. 
The Computer-Aided System for critical infrastructure Operation (CASO) is designed to significantly and efficiently enhance the quality, safety, reliability and availability of critical infrastructures. 
We report on the design of CASO, its implementation and on the preliminary results inferred on historical and live stream data recorded from CERN’s technical infrastructure. Proposal for the full deployment and expected long-term capabilities will also be discussed."
65,included,10.1016/j.procs.2022.01.300,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85127760892,1/1/2022,an i4.0 data intensive platform suitable for the deployment of machine learning models: a predictive maintenance service case study,"The Artificial Intelligence is one of the key enablers of the Industry 4.0. The building of learning models as well as their deployment in environments where the rate of data generation is high and their analysis must meet real time requirements lead to the need of selecting a big data platform suitable for this purpose. The heterogeneous and distributed nature of I4.0 environments where data becomes highly relevant requires the use of a data centric, distributed and scalable platform where the different applications are deployed as services. In this paper we present an I4.0 digital platform based on RAI4.0 reference architecture on which a predictive maintenance service has been built and deployed in Amazon Web Service cloud. Different strategies to build the predictor are described as well as the stages carried out for its construction. Finally, the predictor built with k-nearest algorithm is chosen because it is the fastest in producing an answer and its accuracy of 99.87% is quite close to the best model for our case study."
66,included,http://arxiv.org/abs/1909.13343v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.13343v2,9/29/2019 0:00,"isthmus: secure, scalable, real-time and robust machine learning
  platform for healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions."
67,included,10.1109/access.2022.3141913,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9676574/,1/1/2022 0:00,decentralized federated learning for healthcare networks: a case study on tumor segmentation,"Smart healthcare relies on artificial intelligence (AI) functions for learning and analysis of patient data. Since large and diverse datasets for training of Machine Learning (ML) models can rarely be found in individual medical centers, classical centralized AI requires moving privacy-sensitive data from medical institutions to data centers that process the fused information. Training on data centers thus requires higher communication resource/energy demands while violating privacy. This is considered today as a significant bottleneck in pursuing scientific collaboration across trans-national clinical medical research centers. Recently, federated learning (FL) has emerged as a distributed AI approach that enables the cooperative training of ML models, without the need of sharing patient data. This paper dives into the analysis of different FL methods and proposes a real-time distributed networking framework based on the Message Queuing Telemetry Transport (MQTT) protocol. In particular, we design a number of solutions for ML over networks, based on FL tools relying on a parameter server (PS) and fully decentralized paradigms driven by consensus methods. The proposed approach is validated in the context of brain tumor segmentation, using a modified version of the popular U-NET model with representative clinical datasets obtained from the daily clinical workflow. The FL process is implemented on multiple physically separated machines located in different countries and communicating over the Internet. The real-time test-bed is used to obtain measurements of training accuracy vs. latency trade-offs, and to highlight key operational conditions that affect the performance in real deployments."
68,included,10.1186/s40537-020-00303-y,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1186/s40537-020-00303-y,4/6/2020 0:00,extending reference architecture of big data systems towards machine learning in edge computing environments,"Background Augmented reality, computer vision and other (e.g. network functions, Internet-of-Things (IoT)) use cases can be realised in edge computing environments with machine learning (ML) techniques. For realisation of the use cases, it has to be understood how data is collected, stored, processed, analysed, and visualised in big data systems. In order to provide services with low latency for end users, often utilisation of ML techniques has to be optimized. Also, software/service developers have to understand, how to develop and deploy ML models in edge computing environments. Therefore, architecture design of big data systems to edge computing environments may be challenging. Findings The contribution of this paper is reference architecture (RA) design of a big data system utilising ML techniques in edge computing environments. An earlier version of the RA has been extended based on 16 realised implementation architectures, which have been developed to edge/distributed computing environments. Also, deployment of architectural elements in different environments is described. Finally, a system view is provided of the software engineering aspects of ML model development and deployment. Conclusions The presented RA may facilitate concrete architecture design of use cases in edge computing environments. The value of RAs is reduction of development and maintenance costs of systems, reduction of risks, and facilitation of communication between different stakeholders."
69,included,10.1109/itsc45102.2020.9294435,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1fcacf7fc81ff9366229c02440e1dc70c3ae28c1,1/1/2020 0:00,architecture design and development of an on-board stereo vision system for cooperative automated vehicles,"In a cooperative automated driving scenario like platooning, the ego vehicle needs reliable and accurate perception capabilities to autonomously follow the lead vehicle. This paper presents the architecture design and development of an on-board stereo vision system for cooperative automated vehicles. The input to the proposed system is stereo image pairs. It uses three deep neural networks to detect and classify objects, lane markings, and free space boundary simultaneously in front of the ego vehicle. The rectified left and right image frames of the stereo camera are used to compute a disparity map to estimate the detected object’s depth and radial distance. It also estimates the object’s relative velocity, azimuth, and elevation angle with respect to the ego vehicle. It sends the perceived information to the vehicle control system and displays the perceived information in a meaningful way on the human-machine interface. The system runs on both PC (x86_64 architecture) with Nvidia GPU, and the Nvidia Drive PX 2 (aarch64 architecture) automotive-grade compute platform. It is deployed and evaluated on Renault Twizy cooperative automated driving research platform. The presented results show that the stereo vision system works in real-time and is useful for cooperative automated vehicles."
70,included,10.1109/embc44109.2020.9175947,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9175947/,7/24/2020 0:00,aidex - an open-source platform for real-time forecasting sepsis and a case study on taking ml algorithms to production,"Sepsis, a dysregulated immune response to infection, has been the leading cause of morbidity and mortality in critically ill patients. Multiple studies have demonstrated improved survival outcomes when early treatment is initiated for septic patients. In our previous work, we developed a real-time machine learning algorithm capable of predicting onset of sepsis four to six hours prior to clinical recognition. In this work, we develop AIDEx, an open-source platform that consumes data as FHIR resources. It is capable of consuming live patient data, securely transporting it into a cloud environment, and monitoring patients in real-time. We build AIDEx as an EHR vendor-agnostic open-source platform that can be easily deployed in clinical environments. Finally, the computation of the sepsis risk scores uses a common design pattern that is seen in streaming clinical informatics and predictive analytics applications. AIDEx provides a comprehensive case study in the design and development of a production-ready ML platform that integrates with Healthcare IT systems."
71,included,10.1109/bigdata.2017.8258089,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8258089/,12/14/2017 0:00,"flux: groupon's automated, scalable, extensible machine learning platform","As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company."
72,included,10.3390/drones4020018,Drones,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/4f6624b0fe7735dd4f7b703a1b27020d6f6df839,1/1/2020 0:00,sharkeye: real-time autonomous personal shark alerting via aerial surveillance,"While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens."
73,included,http://arxiv.org/abs/2103.13452v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.13452v1,3/24/2021 0:00,"a portable, self-contained neuroprosthetic hand with deep learning-based
  finger control","Objective: Deep learning-based neural decoders have emerged as the prominent
approach to enable dexterous and intuitive control of neuroprosthetic hands.
Yet few studies have materialized the use of deep learning in clinical settings
due to its high computational requirements. Methods: Recent advancements of
edge computing devices bring the potential to alleviate this problem. Here we
present the implementation of a neuroprosthetic hand with embedded deep
learning-based control. The neural decoder is designed based on the recurrent
neural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a
compacted yet powerful edge computing platform for deep learning inference.
This enables the implementation of the neuroprosthetic hand as a portable and
self-contained unit with real-time control of individual finger movements.
Results: The proposed system is evaluated on a transradial amputee using
peripheral nerve signals (ENG) with implanted intrafascicular microelectrodes.
The experiment results demonstrate the system's capabilities of providing
robust, high-accuracy (95-99%) and low-latency (50-120 msec) control of
individual finger movements in various laboratory and real-world environments.
Conclusion: Modern edge computing platforms enable the effective use of deep
learning-based neural decoders for neuroprosthesis control as an autonomous
system. Significance: This work helps pioneer the deployment of deep neural
networks in clinical applications underlying a new class of wearable biomedical
devices with embedded artificial intelligence."
74,included,10.1109/access.2020.3010609,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9144582/,1/1/2020 0:00,a privacy-aware crowd management system for smart cities and smart buildings,"Cities are growing at a dizzying pace and they require improved methods to manage crowded areas. Crowd management stands for the decisions and actions taken to supervise and control densely populated spaces and it involves multiple challenges, from recognition and assessment to application of actions tailored to the current situation. To that end, Wi-Fi-based monitoring systems have emerged as a cost-effective solution for the former one. The key challenge that they impose is the requirement to handle large datasets and provide results in near real-time basis. However, traditional big data and event processing approaches have important shortcomings while dealing with crowd management information. In this paper, we describe a novel system architecture for real-time crowd recognition for smart cities and smart buildings that can be easily replicated. The described system proposes a privacy-aware platform that enables the application of artificial intelligence mechanisms to assess crowds' behavior in buildings employing sensed Wi-Fi traces. Furthermore, the present paper shows the implementation of the system in two buildings, an airport and a market, as well as the results of applying a set of classification algorithms to provide crowd management information."
75,included,10.1109/tpds.2019.2922205,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8735806/,12/1/2019 0:00,t-gaming: a cost-efficient cloud gaming system at scale,"Cloud gaming (CG) system could pursue both high-quality gaming experience via intensive computing, and ultimate convenience anywhere at anytime through any energy-constrained mobile devices. Despite the abundance of efforts devoted, state-of-the-art CG systems still suffer from multiple key limitations: expensive deployment cost, high bandwidth consumption and unsatisfied quality of experience (QoE). As a result, existing works are not widely adopted in reality. This paper proposes a Transparent Gaming framework called T-Gaming that allows users to play any popular high-end desktop/console games on-the-fly over the Internet. T-Gaming utilizes the off-the-shelf consumer GPUs without resorting to the expensive proprietary GPU virtualization (vGPU) technology to reduce the deployment cost. Moreover, it enables prioritized video encoding based on the human visual feature to reduce the bandwidth consumption without noticeable visual quality degradation. Last but not least, T-Gaming adopts adaptive real-time streaming based on deep reinforcement learning (RL) to improve user's QoE. To evaluate the performance of T-Gaming, we implement and test a prototype system in the real world. Compared with the existing cloud gaming systems, T-Gaming not only reduces the expense per user by 75 percent hardware cost reduction and 14.3 percent network cost reduction, but also improves the normalized average QoE by 3.6-27.9 percent."
76,included,10.1016/j.neucom.2020.11.066,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85098065075,3/7/2021,bayesuites: an open web framework for massive bayesian networks focused on neuroscience,"BayeSuites is the first web framework for learning, visualizing, and interpreting Bayesian networks (BNs) that can scale to tens of thousands of nodes while providing fast and friendly user experience. All the necessary features that enable this are reviewed in this paper; these features include scalability, extensibility, interoperability, ease of use, and interpretability. Scalability is the key factor in learning and processing massive networks within reasonable time; for a maintainable software open to new functionalities, extensibility and interoperability are necessary. Ease of use and interpretability are fundamental aspects of model interpretation, fairly similar to the case of the recent explainable artificial intelligence trend. We present the capabilities of our proposed framework by highlighting a real example of a BN learned from genomic data obtained from Allen Institute for Brain Science. The extensibility properties of the software are also demonstrated with the help of our BN-based probabilistic clustering implementation, together with another genomic-data example."
77,included,10.1109/dsaa.2019.00070,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8964147/,10/8/2019 0:00,"bighead: a framework-agnostic, end-to-end machine learning platform","With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing."
78,included,10.1109/snpd.2017.8022765,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8022765/,6/28/2017 0:00,recognizing adls of one person household based on non-intrusive environmental sensing,"Pervasive sensing technologies are promising for increasing one-person households (OPH), where the sensors monitor and assist the resident to maintain healthy life rhythm. Towards the practical use, the recognition of activities of daily living (ADL) is an important step. Many studies of the ADL recognition have been conducted so far, for real-life and human-centric applications such as eldercare and healthcare. However, most existing methods have limitations in deployment cost, privacy exposure, and inconvenience for residents. To cope with the limitations, this paper presents a new indoor ADL recognition system especially for OPH. To minimize the deployment cost as well as the intrusions to user and house, we exploit an IoT-based environment-sensing device, called Autonomous Sensor Box (SensorBox) which can autonomously measure 7 kinds of environment attributes. We apply machine-learning techniques to the collected data, and predicts 7 kinds of ADLs. We conduct an experiment within an actual apartment of a single user. The result shows that the proposed system achieves the average accuracy of ADL recognition with more than 88%, by carefully developing the features of environment attributes."
79,included,http://arxiv.org/abs/1604.04384v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1604.04384v2,4/15/2016 0:00,the strands project: long-term autonomy in everyday environments,"Thanks to the efforts of the robotics and autonomous systems community,
robots are becoming ever more capable. There is also an increasing demand from
end-users for autonomous service robots that can operate in real environments
for extended periods. In the STRANDS project we are tackling this demand
head-on by integrating state-of-the-art artificial intelligence and robotics
research into mobile service robots, and deploying these systems for long-term
installations in security and care environments. Over four deployments, our
robots have been operational for a combined duration of 104 days autonomously
performing end-user defined tasks, covering 116km in the process. In this
article we describe the approach we have used to enable long-term autonomous
operation in everyday environments, and how our robots are able to use their
long run times to improve their own performance."
80,included,10.1038/s41598-022-07764-6,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s41598-022-07764-6,3/8/2022 0:00,real-time infection prediction with wearable physiological monitoring and ai to aid military workforce readiness during covid-19,"Infectious threats, like the COVID-19 pandemic, hinder maintenance of a productive and healthy workforce. If subtle physiological changes precede overt illness, then proactive isolation and testing can reduce labor force impacts. This study hypothesized that an early infection warning service based on wearable physiological monitoring and predictive models created with machine learning could be developed and deployed. We developed a prototype tool, first deployed June 23, 2020, that delivered continuously updated scores of infection risk for SARS-CoV-2 through April 8, 2021. Data were acquired from 9381 United States Department of Defense (US DoD) personnel wearing Garmin and Oura devices, totaling 599,174 user-days of service and 201 million hours of data. There were 491 COVID-19 positive cases. A predictive algorithm identified infection before diagnostic testing with an AUC of 0.82. Barriers to implementation included adequate data capture (at least 48% data was needed) and delays in data transmission. We observe increased risk scores as early as 6 days prior to diagnostic testing (2.3 days average). This study showed feasibility of a real-time risk prediction score to minimize workforce impacts of infection."
81,included,10.1109/bigdata50022.2020.9377837,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9377837/,12/13/2020 0:00,a sentiment analysis service platform for streamed multilingual tweets,"Micro-blogging and social-media platforms are now prominent forums for disseminating information, opinions and commentaries. Among these, Twitter enjoys an in-excess of 330M base of users who continually produce and consume information snippets. Users collectively create a voluminous and multi-lingual corpus in a very broad range of topics on a daily basis. The discourse generated in the blogosphere is often of prime interest and importance to individuals, organizations, and companies. These actors would certainly like to periodically receive an overall assessment of demonstrated ""sentiments"" on specific issues by automatically classifying tweets expressed in different languages in conjunction with big-data analytics. In this paper, we propose a scalable service platform that employs multilingual sentiment analysis to classify streamed-tweets and yields analytics for selected topics in real-time. We discuss the main component of our Spark-enabled platform as we seek to offer an effective big-data service that can: 1) dynamically handle voluminous as well as high-rate tweettraffic through a multi-component application exploiting the latest software developments, 2) accurately identify messages originated by non-genuine user-accounts, and 3) utilize the Spark machine-learning library (MLib) to successfully classify streamed multi-lingual messages in real-time, using multiple potentially distributed executors. To empower our service platform, we have adopted training sets and developed sentiment analysis (SA) models for English, French, and Greek that help classify streamed tweetswith high accuracy. While experimenting with our distributed analytical platform, we establish both accurate and real-time classification for tweetsexpressed in the above European languages."
82,included,10.1038/s41591-022-01895-z,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s41591-022-01895-z,7/1/2022 0:00,factors driving provider adoption of the trews machine learning-based early warning system and its effects on sepsis treatment timing,"Machine learning-based clinical decision support tools for sepsis create opportunities to identify at-risk patients and initiate treatments at early time points, which is critical for improving sepsis outcomes. In view of the increasing use of such systems, better understanding of how they are adopted and used by healthcare providers is needed. Here, we analyzed provider interactions with a sepsis early detection tool (Targeted Real-time Early Warning System), which was deployed at five hospitals over a 2-year period. Among 9,805 retrospectively identified sepsis cases, the early detection tool achieved high sensitivity (82% of sepsis cases were identified) and a high rate of adoption: 89% of all alerts by the system were evaluated by a physician or advanced practice provider and 38% of evaluated alerts were confirmed by a provider. Adjusting for patient presentation and severity, patients with sepsis whose alert was confirmed by a provider within 3 h had a 1.85-h (95% CI 1.66–2.00) reduction in median time to first antibiotic order compared to patients with sepsis whose alert was either dismissed, confirmed more than 3 h after the alert or never addressed in the system. Finally, we found that emergency department providers and providers who had previous interactions with an alert were more likely to interact with alerts, as well as to confirm alerts on retrospectively identified patients with sepsis. Beyond efforts to improve the performance of early warning systems, efforts to improve adoption are essential to their clinical impact and should focus on understanding providers’ knowledge of, experience with and attitudes toward such systems. Prospective evaluation of a machine learning-based early warning system for sepsis, deployed at five hospitals, showed that healthcare providers interacted with the system at a high rate and that this interaction was associated with faster antibiotic ordering."
83,included,10.17863/cam.45198,Journal of Management in Engineering,core,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),,5/1/2020 0:00,developing a dynamic digital twin at building and city levels: a case study of the west cambridge campus,"A Digital Twin (DT) refers to a digital replica of physical assets, processes and systems. DTs integrate artificial intelligence, machine learning and data analytics to create living digital simulation models that are able to learn and update from multiple sources, and to represent and predict the current and future conditions of physical counterparts. However, the current activities related to DTs are still at an early stage with respect to buildings and other infrastructure assets from an architectural and engineering/construction point of view. Less attention has been paid to the operation & maintenance (O&M) phase, which is the longest time span in the asset life cycle. A systematic and clear architecture verified with practical use cases for constructing a DT would be the foremost step for effective operation and maintenance of buildings and cities. According to current research about multi-tier architectures, this paper presents a system architecture for DTs which is specifically designed at both the building and city levels. Based on this architecture, a DT demonstrator of the West Cambridge site of the University of Cambridge was developed, which integrates heterogeneous data sources, supports effective data querying and analysing, supports decision-making processes in O&M management, and further bridges the gap between human relationships with buildings/cities. This paper aims at going through the whole process of developing DTs in building and city levels from the technical perspective and sharing lessons learnt and challenges involved in developing DTs in real practices. Through developing this DT demonstrator, the results provide a clear roadmap and present particular DT research efforts for asset management practitioners, policymakers and researchers to promote the implementation and development of DT at the building and city levels.Centre for Digital Built Britain (Innovate UK); Centre for Smart Infrastructure and Construction (Innovate UK/EPSRC"
84,included,10.1109/icde51399.2021.00211,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9458658/,4/22/2021 0:00,catching them red-handed: real-time aggression detection on social media,"Aggression on social media has evolved into a major point of concern. However, recently proposed machine learning (ML) approaches to detect various types of aggressive behavior fall short, due to the fast and increasing pace of content generation as well as evolution of such behavior over time. This work introduces the first, practical, real-time framework for detecting aggression on Twitter via embracing the streaming ML paradigm. This method adapts its ML binary classifiers in an incremental fashion, while receiving new annotated examples, and achieves similar performance as batch-based ML models, with 82-93% accuracy, precision, and recall. Experimental analysis on real Twitter data reveals how this framework, implemented in Spark Streaming, easily scales to process millions of tweets in minutes."
85,included,http://arxiv.org/abs/1911.06633v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1911.06633v1,11/15/2019 0:00,"healthfog: an ensemble deep learning based smart healthcare system for
  automatic diagnosis of heart diseases in integrated iot and fog computing
  environments","Cloud computing provides resources over the Internet and allows a plethora of
applications to be deployed to provide services for different industries. The
major bottleneck being faced currently in these cloud frameworks is their
limited scalability and hence inability to cater to the requirements of
centralized Internet of Things (IoT) based compute environments. The main
reason for this is that latency-sensitive applications like health monitoring
and surveillance systems now require computation over large amounts of data
(Big Data) transferred to centralized database and from database to cloud data
centers which leads to drop in performance of such systems. The new paradigms
of fog and edge computing provide innovative solutions by bringing resources
closer to the user and provide low latency and energy-efficient solutions for
data processing compared to cloud domains. Still, the current fog models have
many limitations and focus from a limited perspective on either accuracy of
results or reduced response time but not both. We proposed a novel framework
called HealthFog for integrating ensemble deep learning in Edge computing
devices and deployed it for a real-life application of automatic Heart Disease
analysis. HealthFog delivers healthcare as a fog service using IoT devices and
efficiently manages the data of heart patients, which comes as user requests.
Fog-enabled cloud framework, FogBus is used to deploy and test the performance
of the proposed model in terms of power consumption, network bandwidth,
latency, jitter, accuracy and execution time. HealthFog is configurable to
various operation modes that provide the best Quality of Service or prediction
accuracy, as required, in diverse fog computation scenarios and for different
user requirements."
86,included,10.1109/bibm47256.2019.8983322,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8983322/,11/21/2019 0:00,openhi2 — open source histopathological image platform,"Transition from conventional to digital pathology requires a new category of biomedical informatic infrastructure which could facilitate delicate pathological routine. Pathological diagnoses are sensitive to many external factors and is known to be subjective. Only systems that can meet strict requirements in pathology would be able to run along pathological routines and eventually digitized the area, and the developed platform should comply with existing pathological routines and international standards. Currently, there are a number of available software tools which can perform histopathological tasks including virtual slide viewing, annotating, and basic image analysis, however, none of them can serve as a digital platform for pathology. Here we describe OpenHI2, an enhanced version Open Histopathological Image platform which is capable of supporting all basic pathological tasks and file formats; ready to be deployed in medical institutions on a standard server environment or cloud computing infrastructure. In this paper, we also describe the development decisions for the platform and propose solutions to overcome technical challenges including responsive region retrieval and viewing, virtual slide magnification, recording of diagnostic areas. These factors would promote OpenHI2 be used as a platform for histopathological images in real-world clinical settings. Furthermore, in research, OpenHI2 inherited the annotation functionality from the previous version, thus acquired annotations can be directly utilized by the newly added machine learning module which include popular machine learning models to perform tasks such as histology image classification and segmentation in the same environment. Addition can be made to the platform since each component is modularized and fully documented. OpenHI2 is free, open-source, and available at https://gitlab.com/BioAI/OpenHI."
87,included,10.1109/aero50100.2021.9438171,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9438171/,3/13/2021 0:00,considerations in the deployment of machine learning algorithms on spaceflight hardware,"Recent advances in artificial intelligence (AI) and machine learning (ML) have revolutionized many fields. ML has many potential applications in the space domain. Next generation space instruments are producing data at rates that exceed the capabilities of current spacecraft to store or transmit to ground stations. Deployment of ML algorithms onboard future spacecraft could perform processing of sensor data as it is gathered, reducing data volume and providing a dramatic increase in throughput of meaningful data. ML techniques may also be used to enhance the autonomy of space missions. However ML techniques have not yet been widely deployed in space environments, primarily due to limitations on the computational capabilities of spaceflight hardware. The need to verify that high-performance computational hardware can reliably operate in this environment delays the adoption of these technologies. Nevertheless, the availability of advanced processing capabilities onboard spacecraft is increasing. These platforms may not provide the processing power of terrestrial equivalents, but they do provide the resources necessary for deploying real-time execution of ML algorithms. In this paper, we present results exploring the implementation of ML techniques on computationally-constrained, high-reliability spacecraft hardware. We show two ML algorithms utilizing deep learning techniques which illustrate the utility of these approaches for space applications. We describe implementation considerations when tailoring these algorithms for execution on computationally-constrained hardware and present a workflow for performing these optimizations. We also present initial results on characterizing the trade space between algorithm accuracy, throughput, and reliability on a variety of hardware platforms with current and anticipated paths to spaceflight."
88,included,http://arxiv.org/abs/2011.09463v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.09463v3,11/18/2020 0:00,"easytransfer -- a simple and scalable deep transfer learning platform
  for nlp applications","The literature has witnessed the success of leveraging Pre-trained Language
Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural
Language Processing (NLP) applications, yet it is not easy to build an
easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the
EasyTransfer platform is designed to develop deep TL algorithms for NLP
applications. EasyTransfer is backended with a high-performance and scalable
engine for efficient training and inference, and also integrates comprehensive
deep TL algorithms, to make the development of industrial-scale TL applications
easier. In EasyTransfer, the built-in data and model parallelism strategies,
combined with AI compiler optimization, show to be 4.0x faster than the
community version of distributed training. EasyTransfer supports various NLP
models in the ModelZoo, including mainstream PLMs and multi-modality models. It
also features various in-house developed TL algorithms, together with the
AppZoo for NLP applications. The toolkit is convenient for users to quickly
start model training, evaluation, and online deployment. EasyTransfer is
currently deployed at Alibaba to support a variety of business scenarios,
including item recommendation, personalized search, conversational question
answering, etc. Extensive experiments on real-world datasets and online
applications show that EasyTransfer is suitable for online production with
cutting-edge performance for various applications. The source code of
EasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer)."
89,included,http://arxiv.org/abs/2003.02454v4,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.02454v4,3/5/2020 0:00,agl: a scalable system for industrial-purpose graph machine learning,"Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
  In this paper, we design AGL, a scalable, fault-tolerance and integrated
system, with fully-functional training and inference for GNNs. Our system
design follows the message passing scheme underlying the computations of GNNs.
We design to generate the $k$-hop neighborhood, an information-complete
subgraph for each node, as well as do the inference simply by merging values
from in-edge neighbors and propagating values to out-edge neighbors via
MapReduce. In addition, the $k$-hop neighborhood contains information-complete
subgraphs for each node, thus we simply do the training on parameter servers
due to data independency. Our system AGL, implemented on mature
infrastructures, can finish the training of a 2-layer graph attention network
on a graph with billions of nodes and hundred billions of edges in 14 hours,
and complete the inference in 1.2 hour."
90,included,10.1016/j.enbuild.2018.12.034,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85059816255,2/15/2019,"intellimav: a cloud computing measurement and verification 2.0 application for automated, near real-time energy savings quantification and performance deviation detection","Energy conservation measures (ECMs) are implemented in all sectors with the objective of improving the efficiency with which energy is consumed. Measurement and verification (M&V) is required to verify the performance of every ECM to ensure its successful implementation and operation. The methodologies implemented to achieve this are currently evolving to a more dynamic state, known as measurement and verification 2.0, through the use of automated and advanced analytics. The primary barrier to the adoption of M&V 2.0 practices are the tools available to practitioners. This paper aims to populate the knowledge gap in the industrial buildings sector by presenting a novel cloud computing-based application, IntelliMaV, that applies advanced machine learning techniques on large datasets to automatically verify the performance of ECMs in near real-time. Additionally, a performance deviation detection system is incorporated, ensuring persistence of savings beyond the typical period of analysis in M&V.
                  IntelliMaV allows M&V practitioners to quantify energy savings with minimum levels of uncertainty by applying powerful analytics to data readily available in industrial facilities. The use of a cloud computing-based architecture reduces the resources required on-site and decreases the time required to train the baseline energy model through the use of parallel processing. The robust nature of the application ensures it is applicable across the broad spectrum of ECMs in the industrial buildings sector. A case study carried out in a large biomedical manufacturing facility demonstrates the ease of use of the application and the benefits realised through its adoption. The energy savings from an ECM were calculated to be 2,353,225 kWh/yr with 25.5% uncertainty at a 90% confidence interval."
91,included,10.1109/avss.2018.8639168,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8639168/,11/30/2018 0:00,a practical person monitoring system for city security,"Recent progress in Deep Learning(DL) has brought many breakthroughs with incredible performance, which have not been achieved with traditional machine learning algorithms. In computer vision, DL-based methods have started to outperform humans in certain tasks and are going to impact our daily lives. We present our case study of an implementation and evaluation of our prototype real-time person-monitoring system using cutting-edge DL computer vision techniques. We used a fast and lightweight stream-processing engine for its flexibility and portability, packaged all of DL software stacks as docker containers for portability and ease of deployment, and evaluated our prototype's performance using realistic scenarios in which one hundred camera streams are gathered at centered GPU servers. We confirmed that our prototype system can monitor one hundred video streams in real-time. We also report lessons learned through our prototype implementation and discuss the future direction of person monitoring."
92,included,http://arxiv.org/abs/2107.13212v1,arxiv,arxiv,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2107.13212v1,7/28/2021 0:00,"the trip to the enterprise gourmet data product marketplace through a
  self-service data platform","Data Analytics provides core business reporting needs in many software
companies, acts as a source of truth for key information, and enables building
advanced solutions, e.g., predictive models, machine learning, real-time
recommendations, to grow the business.
  A self-service, multi-tenant, API-first, and scalable data platform is the
foundational requirement in creating an enterprise data marketplace, which
enables the creation, publishing, and exchange of data products. Such a
marketplace enables the exploration and discovery of data products, further
providing high-level data governance and oversight on marketplace contents. In
this paper, we describe our way to the gourmet data product marketplace. We
cover the design principles, the implementation details, technology choices,
and the journey to build an enterprise data platform that meets the above
characteristics. The platform consists of ingestion, streaming, storage,
transformation, schema generation, fail-safe, data sharing, access management,
PII data automatic identification, self-service storage optimization
recommendations, and CI/CD integration.
  We then show how the platform enables and operates the data marketplace,
facilitating the exchange of stable data products across users and tenants. We
motivate and show how we run scalable decentralized data governance. All of
this is built and run for Cimpress Technology (CT), which operates the Mass
Customization Platform for Cimpress and its businesses. The CT data platform
serves 1000s of users from different platform participants, with data sourced
from heterogeneous sources. Data is ingested at a rate of well over 1000
individual messages per second and serves more than 100k analytical queries
daily."
93,included,http://arxiv.org/abs/1606.03966v2,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1606.03966v2,6/13/2016 0:00,making contextual decisions with low technical debt,"Applications and systems are constantly faced with decisions that require
picking from a set of actions based on contextual information.
Reinforcement-based learning algorithms such as contextual bandits can be very
effective in these settings, but applying them in practice is fraught with
technical debt, and no general system exists that supports them completely. We
address this and create the first general system for contextual learning,
called the Decision Service.
  Existing systems often suffer from technical debt that arises from issues
like incorrect data collection and weak debuggability, issues we systematically
address through our ML methodology and system abstractions. The Decision
Service enables all aspects of contextual bandit learning using four system
abstractions which connect together in a loop: explore (the decision space),
log, learn, and deploy. Notably, our new explore and log abstractions ensure
the system produces correct, unbiased data, which our learner uses for online
learning and to enable real-time safeguards, all in a fully reproducible
manner.
  The Decision Service has a simple user interface and works with a variety of
applications: we present two live production deployments for content
recommendation that achieved click-through improvements of 25-30%, another with
18% revenue lift in the landing page, and ongoing applications in tech support
and machine failure handling. The service makes real-time decisions and learns
continuously and scalably, while significantly lowering technical debt."
94,included,http://arxiv.org/abs/1804.05839v4,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1804.05839v4,4/16/2018 0:00,bigdl: a distributed deep learning framework for big data,"This paper presents BigDL (a distributed deep learning framework for Apache
Spark), which has been used by a variety of users in the industry for building
deep learning applications on production big data platforms. It allows deep
learning applications to run on the Apache Hadoop/Spark cluster so as to
directly process the production data, and as a part of the end-to-end data
analysis pipeline for deployment and management. Unlike existing deep learning
frameworks, BigDL implements distributed, data parallel training directly on
top of the functional compute model (with copy-on-write and coarse-grained
operations) of Spark. We also share real-world experience and ""war stories"" of
users that have adopted BigDL to address their challenges(i.e., how to easily
build end-to-end data analysis and deep learning pipelines for their production
data)."
95,included,10.1109/tsc.2017.2777478,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8119814/,2/1/2021 0:00,solar: services-oriented deep learning architectures-deep learning as a service,"Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space exploration. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability."
96,included,10.1109/phm-paris.2019.00052,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8756426/,5/5/2019 0:00,a common service middleware for intelligent complex software system,"With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technology, various intelligent complex software systems (i-CSS) are increasingly popular, becoming one of the most important software system development paradigms. Its inherent growth construction and adaptive evolution properties pose new challenges to existing software design and development methods. Especially, how to achieve growth construction by quickly reusing existing excellent software resources, and how to establish data flow across system boundaries around the business flow to achieve adaptive evolution based on data intelligence. Facing the above challenges, this paper proposes novel data-oriented analysis and design method (DOAD), microservice and container-based mashup development method (SCMD). On this basis, the paper implements i-CSS common service middleware to support the above methods in engineering. In a real cloud-based PHM system and the other three industry projects, the proposed methods and middleware are used for application verification, the results show that they can greatly reduce the complexity of i-CSS design and development, reduce the ability threshold of the i-CSS development team, improve the development efficiency of the development team, reduce the team development workload by 31.5% on average, and help the i-CSS team effectively cope with the challenges of growth construction and adaptive evolution."
97,included,10.1109/icc40277.2020.9148684,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9148684/,6/11/2020 0:00,machine learning for predictive diagnostics at the edge: an iiot practical example,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure."
98,included,10.1109/i2ct54291.2022.9823974,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9823974/,4/9/2022 0:00,covid-tracker: surveillance of potential clusters using a wristband and location-based data,"COVID-19 is a global pandemic that has threatened the survival of humans and other living beings. COVID-19 causes illnesses varying from the very mild cold to serious health complications resulting in death. Most Information Technology based solutions have been implemented to prevent the COVID-19 pandemic while raising awareness in the public. However, there is a limited number of reliable and real-time applications of self-awareness on COVID-19. Currently, the globe is dealing with the COVID-19 epidemic, particularly in pursuit of economic growth in each country. Therefore, an accurate, efficient automatic method to raise self-awareness by avoiding risky contacts is useful for human survival. This paper describes the automatic detection of temperature using a wearable device and an automatic alerting mechanism to inform the users of potentially risky contacts with higher temperatures nearby within a considerable time frame. COVID-Tracker produces results with high accuracy and efficiency, this is beneficial to improve self-awareness among users, to visualize potential covid clusters, and also to improve the mental health of self-isolated people. The developed application consists of four main components namely: temperature measuring band, mobile application, prediction model-based visualization dashboard and an AI bot. Based on the results reported here, developed methods can help people to achieve self-awareness of COVID-19 by avoiding risk factors early and accurately."
99,included,http://arxiv.org/abs/1906.07391v3,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1906.07391v3,6/18/2019 0:00,"the breakthrough listen search for intelligent life: public data,
  formats, reduction and archiving","Breakthrough Listen is the most comprehensive and sensitive search for
extraterrestrial intelligence (SETI) to date, employing a collection of
international observational facilities including both radio and optical
telescopes. During the first three years of the Listen program, thousands of
targets have been observed with the Green Bank Telescope (GBT), Parkes
Telescope and Automated Planet Finder. At GBT and Parkes, observations have
been performed ranging from 700 MHz to 26 GHz, with raw data volumes averaging
over 1PB / day. A pseudo-real time software spectroscopy suite is used to
produce multi-resolution spectrograms amounting to approximately 400 GB hr^-1
GHz^-1 beam^-1. For certain targets, raw baseband voltage data is also
preserved. Observations with the Automated Planet Finder produce both
2-dimensional and 1-dimensional high resolution (R~10^5) echelle spectral data.
  Although the primary purpose of Listen data acquisition is for SETI, a range
of secondary science has also been performed with these data, including studies
of fast radio bursts. Other current and potential research topics include
spectral line studies, searches for certain kinds of dark matter, probes of
interstellar scattering, pulsar searches, radio transient searches and
investigations of stellar activity. Listen data are also being used in the
development of algorithms, including machine learning approaches to modulation
scheme classification and outlier detection, that have wide applicability not
just for astronomical research but for a broad range of science and
engineering.
  In this paper, we describe the hardware and software pipeline used for
collection, reduction, archival, and public dissemination of Listen data. We
describe the data formats and tools, and present Breakthrough Listen Data
Release 1.0 (BLDR 1.0), a defined set of publicly-available raw and reduced
data totalling 1 PB."
100,included,10.1016/j.puhe.2016.01.006,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84957716366,5/1/2016,id-viewer: a visual analytics architecture for infectious diseases surveillance and response management in pakistan,"Objectives
                  Globally, disease surveillance systems are playing a significant role in outbreak detection and response management of Infectious Diseases (IDs). However, in developing countries like Pakistan, epidemic outbreaks are difficult to detect due to scarcity of public health data and absence of automated surveillance systems. Our research is intended to formulate an integrated service-oriented visual analytics architecture for ID surveillance, identify key constituents and set up a baseline for easy reproducibility of such systems in the future.
               
                  Study design
                  This research focuses on development of ID-Viewer, which is a visual analytics decision support system for ID surveillance. It is a blend of intelligent approaches to make use of real-time streaming data from Emergency Departments (EDs) for early outbreak detection, health care resource allocation and epidemic response management.
               
                  Methods
                  We have developed a robust service-oriented visual analytics architecture for ID surveillance, which provides automated mechanisms for ID data acquisition, outbreak detection and epidemic response management. Classification of chief-complaints is accomplished using dynamic classification module, which employs neural networks and fuzzy-logic to categorize syndromes. Standard routines by Center for Disease Control (CDC), i.e. c1-c3 (c1-mild, c2-medium and c3-ultra), and spatial scan statistics are employed for detection of temporal and spatio-temporal disease outbreaks respectively. Prediction of imminent disease threats is accomplished using support vector regression for early warnings and response planning. Geographical visual analytics displays are developed that allow interactive visualization of syndromic clusters, monitoring disease spread patterns, and identification of spatio-temporal risk zones.
               
                  Results
                  We analysed performance of surveillance framework using ID data for year 2011–2015. Dynamic syndromic classifier is able to classify chief-complaints to appropriate syndromes with high classification accuracy. Outbreak detection methods are able to detect the ID outbreaks in start of epidemic time zones. Prediction model is able to forecast dengue trend for 20 weeks ahead with nominal normalized root mean square error of 0.29. Interactive geo-spatiotemporal displays, i.e. heat-maps, and choropleth are shown in respective sections.
               
                  Conclusion
                  The proposed framework will set a standard and provide necessary details for future implementation of such a system for resource-constrained regions. It will improve early outbreak detection attributable to natural and man-made biological threats, monitor spatio-temporal epidemic trends and provide assurance that an outbreak has, or has not occurred. Advanced analytics features will be beneficial in timely organization/formulation of health management policies, disease control activities and efficient health care resource allocation."
101,included,10.1109/pyhpc51966.2020.00010,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9307950/,11/13/2020 0:00,accelerating microstructural analytics with dask for volumetric x-ray images,"While X-ray microtomography has become indispensable in 3D inspections of materials, efficient processing of such volumetric datasets continues to be a challenge. This paper describes a computational environment for HPC to facilitate parallelization of algorithms in computer vision and machine learning needed for microstructure characterization and interpretation. The contribution is to accelerate microstructural analytics by employing Dask high-level parallel abstractions, which scales Numpy workflows to enable multi-dimensional image analysis of diverse specimens. We illustrate our results using an example from materials sciences, emphasizing the benefits of parallel execution of image-dependent tasks. Preliminary results show that the proposed environment configuration and scientific software stack deployed using JupyterLab at NERSC Cori enables near-real time analyses of complex, high-resolution experiments."
102,included,982248098bd6b1be2d72369ae3d083f551eefced,ArXiv,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/982248098bd6b1be2d72369ae3d083f551eefced,2021,"modeling live video streaming: real-time classification, qoe inference, and field evaluation","Social media, professional sports, and video games are driving rapid growth in live video streaming, on platforms such as Twitch and YouTube Live. Live streaming experience is very susceptible to short-time-scale network congestion since client playback buffers are often no more than a few seconds. Unfortunately, identifying such streams and measuring their QoE for network management is challenging, since content providers largely use the same delivery infrastructure for live and videoon-demand (VoD) streaming, and packet inspection techniques (including SNI/DNS query monitoring) cannot always distinguish between the two. In this paper, we design, build, and deploy ReCLive: a machine learning method for live video detection and QoE measurement based on network-level behavioral characteristics. Our contributions are four-fold: (1) We analyze about 23,000 video streams from Twitch and YouTube, and identify key features in their traffic profile that differentiate live and on-demand streaming. We release our traffic traces as open data to the public; (2) We develop an LSTM-based binary classifier model that distinguishes live from on-demand streams in real-time with over 95% accuracy across providers; (3) We develop a method that estimates QoE metrics of live streaming flows in terms of resolution and buffer stall events with overall accuracies of 93% and 90%, respectively; and (4) Finally, we prototype our solution, train it in the lab, and deploy it in a live ISP network serving more than 7,000 subscribers. Measurements from the field show that 99.8% of Twitch videos are streamed live, while this measure is only 2.3% for YouTube. Further, during peak hours as many as 15% of live video streams are played at low-definition resolution and about 7% of them experience a buffer stall. Our method provides ISPs with fine-grained visibility into live video streams, enabling them to measure and improve user experience."
103,included,6d9cd2d863d33b57b9bf435605a4e149d13e6d39,IEEE Access,semantic_scholar,citation,citation,https://www.semanticscholar.org/paper/6d9cd2d863d33b57b9bf435605a4e149d13e6d39,2019,ml4iot: a framework to orchestrate machine learning workflows on internet of things data,"Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT framework enables the implementation of several types of ML models, each one with a different workflow. These models can be easily configured and used through a simple pipeline. ML4IoT has been designed to use container-based components to enable training and deployment of various ML models in parallel. The results obtained suggest that the proposed framework can manage real-world IoT heterogeneous data by providing elasticity, robustness, and performance."
