id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract
1,included,10.1109/sysose.2017.7994953,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7994953/,6/21/2017 0:00,autonomous decision making for a driver-less car,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome."
2,unknown,10.1109/ictc.2017.8190968,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8190968/,10/20/2017 0:00,unmanned aerial vehicle surveillance system (uavss) for forest surveillance and data acquisition,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture."
3,included,10.1109/isaect50560.2020.9523700,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9523700/,11/27/2020 0:00,edge-cloud architectures using uavs dedicated to industrial iot monitoring and control applications,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud."
4,unknown,10.1109/isc246665.2019.9071705,2019 IEEE International Smart Cities Conference (ISC2),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1c2a98e580f7c222e052fd623cb6f8ceed49e110,1/1/2019 0:00,on the use of machine learning for state-of- charge forecasting in electric vehicles,"nowadays, it is well known that a main solution for pollution reduction in cities will be the introduction of electric and hybrid vehicles on transportation roads. Many research efforts have been dedicated to develop new technologies to further promote the use of this type of vehicles. However, their penetration on transportation roads faces some obstacles that have not yet been fully tackled. For instance, the development of intelligent battery management systems needs to be further investigated taking into consideration the uncertainty linked to how vehicles will perform in different scenarios, such as traffic situation, driver behavior, and road profile. The work presented in this study is towards developing a battery management system by investigating new approaches for accurate estimation and prediction of remaining charge, the expected lifetime of the batteries and the remaining driving rang. We focus mainly on the integration of predictive analytics techniques for forecasting the state-of-charge. We first deployed statistical- and machine learning based techniques in real-sitting scenarios (LSTM, ARIMA and XGBoost). Experiments have been conducted using an electric vehicle platform and results are reported to shed more light on their accuracy for multiple-horizon forecasts of battery’s state of charge."
5,unknown,10.1109/ispass48437.2020.00019,2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d7cc8603b7bdb2f8cb2c86778da3c803e308d7f9,1/1/2020 0:00,clan: continuous learning using asynchronous neuroevolution on commodity edge devices,"Recent advancements in machine learning algorithms, especially the development of Deep Neural Networks (DNNs) have transformed the landscape of Artificial Intelligence (AI). With every passing day, deep learning based methods are applied to solve new problems with exceptional results. The portal to the real world is the edge. The true impact of AI can only be fully realized if we can have AI agents continuously interacting with the real world and solving everyday problems. Unfortunately, high compute and memory requirements of DNNs acts a huge barrier towards this vision. Today we circumvent this problem by deploying special purpose inference hardware on the edge while procuring trained models from the cloud. This approach, however, relies on constant interaction with the cloud for transmitting all the data, training on massive GPU clusters, and downloading updated models. This is challenging for bandwidth, privacy, and constant connectivity concerns that autonomous agents may exhibit. In this paper we evaluate techniques for enabling adaptive intelligence on edge devices with zero interaction with any high-end cloud/server. We build a prototype distributed system of Raspberry Pis communicating via WiFi running NeuroEvolutionary (NE) learning and inference. We evaluate the performance of such a collaborative system and detail the compute/communication characteristics of different arrangements of the system that trade-off parallelism versus communication. Using insights from our analysis, we also propose algorithmic modifications to reduce communication by up to 3.6x during the learning phase to enhance scalability even further and match performance of higher end computing devices at scale. We believe that these insights will enable algorithm-hardware co-design efforts for enabling continuous learning on the edge."
6,included,10.1109/icccn52240.2021.9522281,2021 International Conference on Computer Communications and Networks (ICCCN),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e5db631b9237de584eadab60dd6529470438ad5d,1/1/2021 0:00,realization of an intrusion detection use-case in onap with acumos,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms."
7,unknown,10.23919/cycon49761.2020.9131724,2020 12th International Conference on Cyber Conflict (CyCon),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/abbd4715971471fb025bd063e8f97a8568a5b80c,1/1/2020 0:00,hacking the ai - the next generation of hijacked systems,"Within the next decade, the need for automation, intelligent data handling and pre-processing is expected to increase in order to cope with the vast amount of information generated by a heavily connected and digitalised world. Over the past decades, modern computer networks, infrastructures and digital devices have grown in both complexity and interconnectivity. Cyber security personnel protecting these assets have been confronted with increasing attack surfaces and advancing attack patterns. In order to manage this, cyber defence methods began to rely on automation and (artificial) intelligence supporting the work of humans. However, machine learning (ML) and artificial intelligence (AI) supported methods have not only been integrated in network monitoring and endpoint security products but are almost omnipresent in any application involving constant monitoring, complex or large volumes of data. Intelligent IDS, automated cyber defence, network monitoring and surveillance as well as secure software development and orchestration are all examples of assets that are reliant on ML and automation. These applications are of considerable interest to malicious actors due to their importance to society. Furthermore, ML and AI methods are also used in audio-visual systems utilised by digital assistants, autonomous vehicles, face-recognition applications and many others. Successful attack vectors targeting the AI of audio-visual systems have already been reported. These attacks range from requiring little technical knowledge to complex attacks hijacking the underlying AI.With the increasing dependence of society on ML and AI, we must prepare for the next generation of cyber attacks being directed against these areas. Attacking a system through its learning and automation methods allows attackers to severely damage the system, while at the same time allowing them to operate covertly. The combination of being inherently hidden through the manipulation made, its devastating impact and the wide unawareness of AI and ML vulnerabilities make attack vectors against AI and ML highly favourable for malicious operators. Furthermore, AI systems tend to be difficult to analyse post-incident as well as to monitor during operations. Discriminating a compromised from an uncompromised AI in real-time is still considered difficult.In this paper, we report on the state of the art of attack patterns directed against AI and ML methods. We derive and discuss the attack surface of prominent learning mechanisms utilised in AI systems. We conclude with an analysis of the implications of AI and ML attacks for the next decade of cyber conflicts as well as mitigations strategies and their limitations."
8,unknown,10.4043/29335-ms,"Day 2 Tue, May 07, 2019",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/095aeceba84ba188a2b3bb6e086d3755f11e824a,1/1/2019 0:00,"using augmented intelligence to automate subsea inspection data acquisition, processing, analysis, reporting and access","
 Augmented Intelligence (AI2) involves fusing Analyst Intuition with Artificial Intelligence to deliver an optimised combination of human-machine decision support.
 AI2 is being incorporated by i-Tech Services / Leidos into the physical inspection of offshore Oil, Gas, and Renewables assets, delivering valuable data driven insights that contribute to greater efficiency, enhanced condition monitoring, improved asset integrity and asset life extension.
 The deployment of vehicular and diver assets to obtain such inspection data, with associated support vessels, remains a major cost challenge for Operators.
 We believe the industry needs to approach this challenge from two key directions. Firstly, through the application of autonomous systems for data acquisition and delivery, reducing vessel reliance, and secondly through automating the acquisition and processing of data and maximising the insight provided by the data.
 This paper will examine the use of Augmented Intelligence to optimise the Subsea Inspection data workflow as a key use case, to demonstrate the principles.
 The historic paradigm consists of a fragmented evolving approach, with insufficient consideration and design across all the sensors, processing analytical engines and data visualisation. The approach being adopted is to closely link all aspects of the data workflow, within the context of delivering the data and beyond in terms of harvesting additional insight and value.
 To achieve the optimum workflow a number of developmental initiatives are being knitted into a modular platform, each element providing standalone value but the sum of the parts generates the most significant value and cost reduction.
 The elements being combined are automatic data quality control at acquisition source and through the full workflow, automated processing, machine vision for object recognition and reporting and machine learning to optimise the system intelligence. All of these are designed to augment the expertise of the analyst / user, detecting change to learnt parameters, by using real time data and critically by referencing large historical data sets and as-built data.
 The outputs from a system holistic approach will be improved data acquisition with more efficient high quality right first time data reporting. In addition layers of analytics, with smart, intuitive data access and retrieval will optimise delivery of key information within large data sets, together with maximising value and insight."
9,unknown,10.1109/wispnet.2017.8300034,"2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/5d449d17d0babccd02f8557b09622616b9b7af9a,1/1/2017 0:00,a reliable and secure approach for efficient car-to-car communication in intelligent transportation systems,"Intelligent Transportation System (ITS) is considered to be an integral part of the Smart City concept. As per the idea of ITS, intelligence incorporated in existing vehicles, both private and public, is expected to make vehicle management inexpensive and less complex. Some of the barriers in successful deployment of ITS have been its reliability, scalability, interoperability and security. Communication plays a big role in ITS and Wi-Fi/ cellular technologies have been used to facilitate Car-to-Server (C2S) and Server-to-Server (S2S) type communication. Car-to-Car (C2C) communication emerges as a more resource constrained and challenging model. Due to strict real-time, low delay and high security requirements, C2C presents several avenues for research. In this work, we investigate the network and security concerns of C2C architecture of ITS. Based on our study, we propose an effective approach for C2C communication using event-triggered broadcast of information. We use Public Key Infrastructure (PKI) based sender authentication for information verification. Furthermore, we provide Machine Learning based solutions to some common problems encountered during C2C communication."
10,unknown,10.1109/icodt255437.2022.9787424,2022 2nd International Conference on Digital Futures and Transformative Technologies (ICoDT2),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/0cd4639954db4c8bde3d28f8fa99f8804516fc36,1/1/2022 0:00,"machine learning based theoretical framework for failure prediction, detection, and correction of mission-critical flight software","Mission-critical flight software acts as the control mechanism for autonomous flights and lies at the heart of next-generation developments in the aviation industry. Most state-of-the-art technological evolution is realized through the use of contemporary software which implements the essentially required, novel, innovative, and featuring value additions. Real-time physical exposure and the data-driven flying nature of aerial vehicles make them vulnerable to an ever-evolving new threat spectrum of cyber security. Nation or state-sponsored cyber attacks through sensors’ data corruption, hardware Trojans, or counterfeit wireless signals may exploit dormant and residual software vulnerabilities. It may lead to severe and catastrophic consequences including but not limited to serious injury or death of the crew, extreme damage or loss to equipment and environment. We have proposed a machine learning based theoretical framework for real-time monitoring and failure analysis of autonomous flight software. It has been introduced to protect the mission-critical flight software from run-time data-driven semantic bugs and exploitation that may be caused by missing, jammed, or spoofed data values, due to malicious online cyber activities. The effectiveness of the proposed framework has been demonstrated by the evaluation of a real-world incident of grounding an aerial vehicle by the actors in their vicinity without the intent of the original equipment manufacturer (OEM). The results show that the reported undesired but successful cyber attack may has been avoided by the effective utilization of our proposed cyber defense approach, which is targeted at software failure prediction, detection, and correction for autonomous aerial vehicles."
11,unknown,10.2147/rmhp.s338186,Risk management and healthcare policy,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/86041f9ba7e725862af6c84126d6d0122be799e5,1/1/2022 0:00,a sustainable model for emergency medical services in developing countries: a novel approach using partial outsourcing and machine learning,"Introduction Unlike Western countries, many low- and middle-income countries (LMIC), like India, have a de-centralized emergency medical services (EMS) involving both semi-government and non-government organizations. It is alarming that due to the absence of a common ecosystem, the utilization of resources is inefficient, which leads to shortage of available vehicles and larger response time. Fragmentation of emergency supply chain resources motivates us to propose a new vehicle routing and scheduling model equipped with novel features to ensure minimal response time using existing resources. Materials and Methods The data set of medical and fire-related emergencies from January 2018 to May 2018 of Uttarakhand State in India was provided by GVK Emergency Management and Research Institute (GVK EMRI) also known as 108 EMSs was used in the study. The proposed model integrates all the available EMS vehicles including partial outsourcing to non-ambulatory vehicles like police vans, taxis, etc., using a novel two-echelon heuristic approach. In the first stage, an offline learning model is developed to yield the deployment strategy for EMS vehicles. Seven well researched machine learning (ML) algorithms were analyzed for parameter prediction namely random forest (RF), convolutional neural network (CNN), k-nearest neighbor (KNN), classification and regression tree (CART), support vector machine (SVM), logistic regression (LR), and linear discriminant analysis (LDA). In the second stage, a real-time routing model is proposed for EMS vehicle routing at the time of emergency, considering partial outsourcing. Results and Discussion The results indicate that the RF classifier outperforms the LR, LDA, SVM, CNN, CART and NB classifier in terms of both accuracy as well as F-1 score. The proposed vehicle routing and scheduling model for automated decision-making shows an improvement of 42.1%, 54%, 27.9% and 62% in vehicle assignment time, vehicle travel time from base to scene, travel time from scene to hospital, and total response time, respectively, in urban areas."
12,unknown,http://arxiv.org/abs/2110.15127v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2110.15127v1,10/28/2021 0:00,"lightweight mobile automated assistant-to-physician for global
  lower-resource areas","Importance: Lower-resource areas in Africa and Asia face a unique set of
healthcare challenges: the dual high burden of communicable and
non-communicable diseases; a paucity of highly trained primary healthcare
providers in both rural and densely populated urban areas; and a lack of
reliable, inexpensive internet connections. Objective: To address these
challenges, we designed an artificial intelligence assistant to help primary
healthcare providers in lower-resource areas document demographic and medical
sign/symptom data and to record and share diagnostic data in real-time with a
centralized database. Design: We trained our system using multiple data sets,
including US-based electronic medical records (EMRs) and open-source medical
literature and developed an adaptive, general medical assistant system based on
machine learning algorithms. Main outcomes and Measure: The application
collects basic information from patients and provides primary care providers
with diagnoses and prescriptions suggestions. The application is unique from
existing systems in that it covers a wide range of common diseases, signs, and
medication typical in lower-resource countries; the application works with or
without an active internet connection. Results: We have built and implemented
an adaptive learning system that assists trained primary care professionals by
means of an Android smartphone application, which interacts with a central
database and collects real-time data. The application has been tested by dozens
of primary care providers. Conclusions and Relevance: Our application would
provide primary healthcare providers in lower-resource areas with a tool that
enables faster and more accurate documentation of medical encounters. This
application could be leveraged to automatically populate local or national EMR
systems."
13,unknown,http://arxiv.org/abs/2107.07502v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2107.07502v2,7/15/2021 0:00,multibench: multiscale benchmarks for multimodal representation learning,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community."
14,unknown,http://arxiv.org/abs/2104.09876v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2104.09876v1,4/20/2021 0:00,"iiot-enabled health monitoring for integrated heat pump system using
  mixture slow feature analysis","The sustaining evolution of sensing and advancement in communications
technologies have revolutionized prognostics and health management for various
electrical equipment towards data-driven ways. This revolution delivers a
promising solution for the health monitoring problem of heat pump (HP) system,
a vital device widely deployed in modern buildings for heating use, to timely
evaluate its operation status to avoid unexpected downtime. Many HPs were
practically manufactured and installed many years ago, resulting in fewer
sensors available due to technology limitations and cost control at that time.
It raises a dilemma to safeguard HPs at an affordable cost. We propose a hybrid
scheme by integrating industrial Internet-of-Things (IIoT) and intelligent
health monitoring algorithms to handle this challenge. To start with, an IIoT
network is constructed to sense and store measurements. Specifically,
temperature sensors are properly chosen and deployed at the inlet and outlet of
the water tank to measure water temperature. Second, with temperature
information, we propose an unsupervised learning algorithm named mixture slow
feature analysis (MSFA) to timely evaluate the health status of the integrated
HP. Characterized by frequent operation switches of different HPs due to the
variable demand for hot water, various heating patterns with different heating
speeds are observed. Slowness, a kind of dynamics to measure the varying speed
of steady distribution, is properly considered in MSFA for both heating pattern
division and health evaluation. Finally, the efficacy of the proposed method is
verified through a real integrated HP with five connected HPs installed ten
years ago. The experimental results show that MSFA is capable of accurately
identifying health status of the system, especially failure at a preliminary
stage compared to its competing algorithms."
15,unknown,http://arxiv.org/abs/2101.10869v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.10869v2,1/23/2021 0:00,"a raspberry pi-based traumatic brain injury detection system for
  single-channel electroencephalogram","Traumatic Brain Injury (TBI) is a common cause of death and disability.
However, existing tools for TBI diagnosis are either subjective or require
extensive clinical setup and expertise. The increasing affordability and
reduction in size of relatively high-performance computing systems combined
with promising results from TBI related machine learning research make it
possible to create compact and portable systems for early detection of TBI.
This work describes a Raspberry Pi based portable, real-time data acquisition,
and automated processing system that uses machine learning to efficiently
identify TBI and automatically score sleep stages from a single-channel
Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and
verification of the system that can digitize EEG signal using an Analog to
Digital Converter (ADC) and perform real-time signal classification to detect
the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN)
and XGBoost based predictive models to evaluate the performance and demonstrate
the versatility of the system to operate with multiple types of predictive
models. We achieve a peak classification accuracy of more than 90% with a
classification time of less than 1 s across 16 s - 64 s epochs for TBI vs
control conditions. This work can enable development of systems suitable for
field use without requiring specialized medical equipment for early TBI
detection applications and TBI research. Further, this work opens avenues to
implement connected, real-time TBI related health and wellness monitoring
systems."
16,unknown,http://arxiv.org/abs/2101.03989v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.03989v2,1/11/2021 0:00,technology readiness levels for machine learning systems,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics."
17,unknown,http://arxiv.org/abs/2010.02715v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2010.02715v1,10/3/2020 0:00,"assessing automated machine learning service to detect covid-19 from
  x-ray and ct images: a real-time smartphone application case study","The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a
non interventional and sustainable AI solution. Lung disease remains a major
healthcare challenge with high morbidity and mortality worldwide. The
predominant lung disease was lung cancer. Until recently, the world has
witnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We
have experienced how viral infection of lung and heart claimed thousands of
lives worldwide. With the unprecedented advancement of Artificial Intelligence
in recent years, Machine learning can be used to easily detect and classify
medical imagery. It is much faster and most of the time more accurate than
human radiologists. Once implemented, it is more cost-effective and
time-saving. In our study, we evaluated the efficacy of Microsoft Cognitive
Service to detect and classify COVID19 induced pneumonia from other
Viral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the
implication and accuracy of the Automated ML-based Rapid Application
Development (RAD) environment in the field of Medical Image diagnosis. This
study will better equip us to respond with an ML-based diagnostic Decision
Support System(DSS) for a Pandemic situation like COVID19. After optimization,
the trained network achieved 96.8% Average Precision which was implemented as a
Web Application for consumption. However, the same trained network did not
perform the same like Web Application when ported to Smartphone for Real-time
inference. Which was our main interest of study. The authors believe, there is
scope for further study on this issue. One of the main goal of this study was
to develop and evaluate the performance of AI-powered Smartphone-based
Real-time Application. Facilitating primary diagnostic services in less
equipped and understaffed rural healthcare centers of the world with unreliable
internet service."
18,unknown,10.1007/s00170-021-07248-3,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00170-021-07248-3,8/1/2021 0:00,cognitive capabilities for the caai in cyber-physical production systems,"This paper presents the cognitive module of the Cognitive Architecture for Artificial Intelligence (CAAI) in cyber-physical production systems (CPPS). The goal of this architecture is to reduce the implementation effort of artificial intelligence (AI) algorithms in CPPS. Declarative user goals and the provided algorithm-knowledge base allow the dynamic pipeline orchestration and configuration. A big data platform (BDP) instantiates the pipelines and monitors the CPPS performance for further evaluation through the cognitive module. Thus, the cognitive module is able to select feasible and robust configurations for process pipelines in varying use cases. Furthermore, it automatically adapts the models and algorithms based on model quality and resource consumption. The cognitive module also instantiates additional pipelines to evaluate algorithms from different classes on test functions. CAAI relies on well-defined interfaces to enable the integration of additional modules and reduce implementation effort. Finally, an implementation based on Docker, Kubernetes, and Kafka for the virtualization and orchestration of the individual modules and as messaging technology for module communication is used to evaluate a real-world use case."
19,included,10.1007/s42979-021-00726-1,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s42979-021-00726-1,6/19/2021 0:00,towards regulatory-compliant mlops: oravizio’s journey from a machine learning experiment to a deployed certified medical product,"Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context."
20,unknown,10.1007/s00521-021-05726-z,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00521-021-05726-z,1/25/2021 0:00,towards design and implementation of industry 4.0 for food manufacturing,"Today’s factories are considered as smart ecosystems with humans, machines and devices interacting with each other for efficient manufacturing of products. Industry 4.0 is a suite of enabler technologies for such smart ecosystems that allow transformation of industrial processes. When implemented, Industry 4.0 technologies have a huge impact on efficiency, productivity and profitability of businesses. The adoption and implementation of Industry 4.0, however, require to overcome a number of practical challenges, in most cases, due to the lack of modernisation and automation in place with traditional manufacturers. This paper presents a first of its kind case study for moving a traditional food manufacturer, still using the machinery more than one hundred years old, a common occurrence for small- and medium-sized businesses, to adopt the Industry 4.0 technologies. The paper reports the challenges we have encountered during the transformation process and in the development stage. The paper also presents a smart production control system that we have developed by utilising AI, machine learning, Internet of things, big data analytics, cyber-physical systems and cloud computing technologies. The system provides novel data collection, information extraction and intelligent monitoring services, enabling improved efficiency and consistency as well as reduced operational cost. The platform has been developed in real-world settings offered by an Innovate UK-funded project and has been integrated into the company’s existing production facilities. In this way, the company has not been required to replace old machinery outright, but rather adapted the existing machinery to an entirely new way of operating. The proposed approach and the lessons outlined can benefit similar food manufacturing industries and other SME industries."
21,included,10.1007/978-3-030-77070-9_10,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-77070-9_10,1/1/2021 0:00,smart and intelligent chatbot assistance for future industry 4.0,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers."
22,unknown,10.1186/s40537-020-00340-7,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1186/s40537-020-00340-7,8/12/2020 0:00,big data architecture for intelligent maintenance: a focus on query processing and machine learning algorithms,"Exploiting available condition monitoring data of industrial machines for intelligent maintenance purposes has been attracting attention in various application fields. Machine learning algorithms for fault detection, diagnosis and prognosis are popular and easily accessible. However, our experience in working at the intersection of academia and industry showed that the major challenges of building an end-to-end system in a real-world industrial setting go beyond the design of machine learning algorithms. One of the major challenges is the design of an end-to-end data management solution that is able to efficiently store and process large amounts of heterogeneous data streams resulting from a variety of physical machines. In this paper we present the design of an end-to-end Big Data architecture that enables intelligent maintenance in a real-world industrial setting. In particular, we will discuss various physical design choices for optimizing high-dimensional queries, such as partitioning and Z-ordering, that serve as the basis for health analytics. Finally, we describe a concrete fault detection use case with two different health monitoring algorithms based on machine learning and classical statistics and discuss their advantages and disadvantages. The paper covers some of the most important aspects of the practical implementation of such an end-to-end solution and demonstrates the challenges and their mitigation for the specific application of laser cutting machines."
23,unknown,10.1109/ieem.2018.8607399,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8607399/,12/19/2018 0:00,a chatbot-supported smart wireless interactive healthcare system for weight control and health promotion,"People who are overweight and obese have a greater risk of developing serious diseases and health conditions. A steadily increasing trend of obesity is not only limited to developed countries, but to developing nations as well. As smartphones have rapidly gained mainstream popularity, mobile applications (apps) are used in public health as intervention to keep track of diets, activity as well as weight, which is deemed more accurate than relying on user's self-report measure, for the sake of weight management. A solution called “Smart Wireless Interactive Healthcare System” (SWITCHes) is developed to facilitate objective data reception and transmission in a real-time manner. Based on the user data acquired from SWITCHes app and the auxiliary data from medical instruments, not only SWITCHes app can engage user with tailored feedback in an interactive way, in terms of artificial intelligence-powered health chatbot, but the healthcare professional can provide the more accurate medical advice to user also. This paper presents an overview of development and implementation of SWITCHes."
24,unknown,10.1109/compsac48688.2020.00-35,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9202847/,7/17/2020 0:00,an intelligent health analysis approach to detecting potential threats with health data reuse,"Health care is becoming imperative in normal life because threats from several aspects could have negative influences on people's health. Along with machine learning development, some application software can be used for evaluating health level and provide health care reports for health supervising and management. However, it is rarely being used in common life for health managing and supervising. Meanwhile, most of the health data that are collected or processed can rarely be reused in any of applied systems. This paper is used for applying machine learning in health care management and supervision. The threats from circumstance could be detected by processing static data and dynamic data. Entire system is equipped with wearable equipment, such as cloth and shoes, which can be used to achieve real time supervising threats for the users. Machine learning can be used to achieve accurate calculation for the health data. Health management and threats supervision performance can be improved effectively. Distinguished subjects are combined to achieve new methods and integrated knowledge is used for achieving novel theory to support health care system establishment. Creative computing methods, knowledge combination is used for generating such integrated methods."
25,unknown,10.1109/tsp.2019.8768883,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8768883/,7/3/2019 0:00,edge-ai in lora-based health monitoring: fall detection system with fog computing and lstm recurrent neural networks,"Remote healthcare monitoring has exponentially grown over the past decade together with the increasing penetration of Internet of Things (IoT) platforms. IoT-based health systems help to improve the quality of healthcare services through real-time data acquisition and processing. However, traditional IoT architectures have some limitations. For instance, they cannot properly function in areas with poor or unstable Internet. Low power wide area network (LPWAN) technologies, including long-range communication protocols such as LoRa, are a potential candidate to overcome the lacking network infrastructure. Nevertheless, LPWANs have limited transmission bandwidth not suitable for high data rate applications such as fall detection systems or electrocardiography monitoring. Therefore, data processing and compression are required at the edge of the network. We propose a system architecture with integrated artificial intelligence that combines Edge and Fog computing, LPWAN technology, IoT and deep learning algorithms to perform health monitoring tasks. In particular, we demonstrate the feasibility and effectiveness of this architecture via a use case of fall detection using recurrent neural networks. We have implemented a fall detection system from the sensor node and Edge gateway to cloud services and end-user applications. The system uses inertial data as input and achieves an average precision of over 90% and an average recall over 95% in fall detection."
26,included,10.1109/rweek.2018.8473535,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8473535/,8/23/2018 0:00,framework for data driven health monitoring of cyber-physical systems,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features."
27,unknown,10.1109/fmec.2019.8795327,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8795327/,6/13/2019 0:00,health monitoring with low power iot devices using anomaly detection algorithm,"The healthcare industry is rapidly adopting new technologies such as the Internet of Things (IoT), which are dropping costs and improving healthcare outcomes. Such IoT systems typically include edge devices (glucose monitors, ventilators, pacemakers), gateway devices that aggregate the data from the edge devices and transmit it to the cloud, and cloud-based systems which analyze the device data to draw conclusions, display information, or direct the connected devices to take action. This process can lead to communication lags and delayed responses to patient conditions/treatment. The aim of this proposal is to overcome these delays with IoT technology and allow for prompt urgent treatment to patients. The solution proposed includes a model to monitor and process the data disseminated by wearable devices related to the patients' health issues and connect the data to IoT cloud platforms. Analysis of the patients' health data to identify anomalies will be performed at the device level by developing an offline machine learning model using specific algorithms for anomaly detection and deploying them on the IoT devices or IoT gateway. Processing of the real-time health data will be performed at the device level and the prediction of anomalous data will be sent to the third-party cloud for implementing any necessary actions."
28,unknown,10.1109/icoei.2019.8862754,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8862754/,4/25/2019 0:00,machine learning based health prediction system using ibm cloud as paas,"Adaptable Critical Patient Caring system is a key concern for hospitals in developing countries like Bangladesh. Most of the hospital in Bangladesh lack serving proper health service due to unavailability of appropriate, easy and scalable smart systems. The aim of this project is to build an adequate system for hospitals to serve critical patients with a real-time feedback method. In this paper, we propose a generic architecture, associated terminology and a classificatory model for observing critical patient's health condition with machine learning and IBM cloud computing as Platform as a service (PaaS). Machine Learning (ML) based health prediction of the patients is the key concept of this research. IBM Cloud, IBM Watson studio is the platform for this research to store and maintain our data and ml models. For our ml models, we have chosen the following Base Predictors: Naïve Bayes, Logistic Regression, KNeighbors Classifier, Decision Tree Classifier, Random Forest Classifier, Gradient Boosting Classifier, and MLP Classifier. For improving the accuracy of the model, the bagging method of ensemble learning has been used. The following algorithms are used for ensemble learning: Bagging Random Forest, Bagging Extra Trees, Bagging KNeighbors, Bagging SVC, and Bagging Ridge. We have developed a mobile application named “Critical Patient Management System - CPMS” for real-time data and information view. The system architecture is designed in such a way that the ml models can train and deploy in a real-time interval by retrieving the data from IBM Cloud and the cloud information can also be accessed through CPMS in a requested time interval. To help the doctors, the ml models will predict the condition of a patient. If the prediction based on the condition gets worse, the CPMS will send an SMS to the duty doctor and nurse for getting immediate attention to the patient. Combining with the ml models and mobile application, the project may serve as a smart healthcare solution for the hospitals."
29,unknown,10.1109/icesc51422.2021.9532913,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9532913/,8/6/2021 0:00,natural language processing based human assistive health conversational agent for multi-users,"Background: Most of the people are not medically qualified for studying or understanding the extremity of their diseases or symptoms. This is the place where natural language processing plays a vital role in healthcare. These chatbots collect patients' health data and depending on the data, these chatbot give more relevant data to patients regarding their body conditions and recommending further steps also. Purposes: In the medical field, AI powered healthcare chatbots are beneficial for assisting patients and guiding them in getting the most relevant assistance. Chatbots are more useful for online search that users or patients go through when patients want to know for their health symptoms. Methods: In this study, the health assistant system was developed using Dialogflow application programming interface (API) which is a Google's Natural language processing powered algorithm and the same is deployed on google assistant, telegram, slack, Facebook messenger, and website and mobile app. With this web application, a user can make health requests/queries via text message and might also get relevant health suggestions/recommendations through it. Results: This chatbot acts like an informative and conversational chatbot. This chatbot provides medical knowledge such as disease symptoms and treatments. Storing patients personal and medical information in a database for further analysis of the patients and patients get real time suggestions from doctors. Conclusion: In the healthcare sector AI-powered applications have seen a remarkable spike in recent days. This covid crisis changed the whole healthcare system upside down. So this NLP powered chatbot system reduced office waiting, saving money, time and energy. Patients might be getting medical knowledge and assisting ourselves within their own time and place."
30,unknown,10.1109/asmc49169.2020.9185239,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9185239/,8/26/2020 0:00,real-time tool health monitoring and defect inspection during epoxy dispense process,"We demonstrate a new real-time inspection system developed to monitor tool health and detect defects during the epoxy dispense process. The system includes both hardware and software components. The hardware was designed to be low-cost and fit into a small footprint within the existing tools. Our software contains a tool setup/calibration utility and a user interface for recipe creation and real-time inspection. The software also provides extensive logging of key results including tabulated data, annotated images, and live inspection results on the user interface. The algorithm uses a mixture of advanced machine learning and computer vision algorithms to identify unwanted process variation. The new system has provided excellent results, an order of magnitude below the qualification targets, while ensuring the throughput time targets are not impacted."
31,unknown,10.1109/iwcmc48107.2020.9148180,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9148180/,6/19/2020 0:00,real-time health monitoring system based on wearable devices,"This paper proposed a novel electrocardiogram (ECG) automatic diagnose system for health assistance and rescue related with cardiovascular diseases. This system consists of three parts: 1) Data acquisition subsystem, this subsystem acquires ECG data from wearable devices on users' body and transmit them to the cloud server. 2) Deep learning analysis subsystem, with the help of convolutional neural network, the important feature lied inside ECG signal can be extract for abnormal heart condition detection. Hierarchical residual modules provide the network the ability to see seconds of signal and make a decision through the combination of features. Meanwhile, the global max pooling layer on top of the network enables it to capture the most important feature across the whole ECG signal with periodicity. This subsystem is a crucial part for cardiac status based health caring. 3) Back-stage management subsystem, methodical data storage and management were conducted in this subsystem, which also provides the users an interface to access their healthy data and body status. Assembling these three parts of system, real-time ECG diagnose for people in need and timely medical rescue can be implemented."
32,unknown,10.1109/bigdata47090.2019.9005638,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9005638/,12/12/2019 0:00,speech emotion detection using iot based deep learning for health care,"Human emotions are essential to recognize the behavior and state of mind of a person. Emotion detection through speech signals has started to receive more attention lately. This paper proposes the method for detecting human emotions using speech signals and its implementation in real-time using the Internet of Things (IoT) based deep learning for the care of older adults in nursing homes. The research has two main contributions. First, we have implemented a real-time system based on audio IoT, where we have recorded human voice and predicted emotions via deep learning. Secondly, for advance classification, we have designed a model using data normalization and data augmentation techniques. Finally, we have created an integrated deep learning model, called Speech Emotion Detection (SED), using a 2D convolutional neural networks (CNN). The best accuracy that was reported by our method was approximately 95%, which outperformed all state-of-the-art approaches. We have further extended to apply the SED model to a live audio sentiment analysis system with IoT technologies for the care of older adults in nursing homes."
33,included,10.1109/access.2020.2970178,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8974224/,1/1/2020 0:00,a novel software engineering approach toward using machine learning for improving the efficiency of health systems,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications."
34,included,10.1109/compsac48688.2020.0-168,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9202848/,7/17/2020 0:00,an early warning system for hemodialysis complications utilizing transfer learning from hd iot dataset,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed."
35,unknown,10.1109/coginf.2011.6016132,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6016132/,8/20/2011 0:00,an intelligent fault recognizer for rotating machinery via remote characteristic vibration signal detection,"Monitoring industrial machine health in real-time is not only highly demanded but also significantly complicated and difficult. Possible reasons for this include: (a) Access to the machines on site is sometimes impracticable; and (b) The environment in which they operate is usually not human-friendly due to pollution, noise, hazardous wastes, etc. Despite the theoretically sound findings on developing intelligent solutions for machine condition based monitoring, there are few commercial tools in the market that can readily be used. This paper reports on the development of an intelligent fault recognition and monitoring system (Melvin I), which detects and diagnoses rotating machine conditions according to changes in fault frequency indicators. The signals and data are remotely collected from designated sections of machines via data acquisition cards. They are processed by a signal processor in order to extract characteristic vibration signals of ten key performance indicators (KPIs). A 3-layer neural network is designed to recognize and classify faults based on the set of KPIs. The system implemented in our laboratory and applied in the field can also incorporate new experiences into the knowledge base without overwriting previous training. Preliminary results have demonstrated that Melvin I is a smart tool for both system vibration analysts and industrial machine operators."
36,included,10.1109/iceccme52200.2021.9591113,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9591113/,10/8/2021 0:00,cobots for fintech,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space."
37,unknown,10.1109/etfa45728.2021.9613448,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9613448/,9/10/2021 0:00,computer vision based privacy protected fall detection and behavior monitoring system for the care of the elderly,"The elderly population constitutes a large percentage of the society hence making elderly care a top priority. Falls have been identified as a leading issue among major problems faced by them. Concerning this, many monitoring devices have been developed, most of them focusing solely on one specific health care aspect or related to fall detection, and are based on sensors and wearable devices which are usually uncomfortable for daily use. Considering these aspects, the solution proposed in this research is a real time computer vision-based system that monitors behavior and detects anomalies through deep learning. The monitoring is mainly focused on detecting unusual behavior including falls, and monitoring routine activities to detect deviations. A device approach is used to deploy the deep learning models and consists of IP camera-based monitoring which uses a special privacy protected procedure that ensures the detection is done based on meta data and therefore no camera image or footage is stored. The research is mainly focused on four major components which are user identification, fall detection, routine variance detection and device configuration."
38,unknown,10.1109/cns48642.2020.9162311,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9162311/,7/1/2020 0:00,heka: a novel intrusion detection system for attacks to personal medical devices,"Modern Smart Health Systems (SHS) involve the concept of connected personal medical devices. These devices significantly improve the patient's lifestyle as they permit remote monitoring and transmission of health data (i.e., telemedicine), lowering the treatment costs for both the patient and the healthcare providers. Although specific SHS communication standards (i.e., ISO/IEEE 11073) enable real-time plug-and-play interoperability and communication between different personal medical devices, they do not specify any features for secure communications. In this paper, we demonstrate how personal medical device communication is indeed vulnerable to different cyber attacks. Specifically, we show how an external attacker can hook into the personal medical device's communication and eavesdrop the sensitive health data traffic, and implement manin-the-middle, replay, false data injection, and denial-of-service attacks. Furthermore, we also propose an Intrusion Detection System (IDS), HEKA, to monitor personal medical device traffic and detect attacks on them. HEKA passively hooks into the personal medical traffic generated by medical devices to learn the contiguous sequence of packets information from the captured traffic and detects irregular traffic-flow patterns using an n-grambased approach and different machine learning techniques. We implemented HEKA in a testbed consisting of eight off-the-shelf personal medical devices and evaluated its performance against four different attacks. Our extensive evaluation shows that HEKA can effectively detect different attacks on personal medical devices with an accuracy of 98.4% and Fl-score of 98%."
39,included,10.1109/icdmw.2019.00123,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8955523/,11/11/2019 0:00,implementation of mobile-based real-time heart rate variability detection for personalized healthcare,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework."
40,unknown,10.1109/phm-besancon49106.2020.00009,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9115516/,5/7/2020 0:00,machine performance monitoring and fault classification using vibration frequency analysis,"Machine anomalies in manufacturing directly affect the production yield and factory operation efficiency if such anomalies cannot be detected in time. Real-time monitoring of machine health condition not only improves machine throughput by reducing unplanned downtime caused by machine failure but also saves cost for unnecessary routine maintenance. This paper presents a systematic approach for real-time or near real-time machine performance monitoring solution development from data collection, feature extraction, data analytics to real-time machine fault and machine status classification. Three data-driven machine-learning approaches using one vibration sensor data are proposed to detect two common machine failure modes during machine turning process. To evaluate the the performance of each approach, three machine-learning algorithms (Random Forest, K Nearest Neighborhood, and Support Vector Machine) are implemented and tested. Evaluation results on the actual machine data shows that a two-layered classification structure with random forest algorithm as the base has high classification accuracy on the machine status including machine fault detection. The developed data-driven machine health monitoring solution is deployed in the IoT device for real-time data collection and processing and results are sent data server for data visualization."
41,unknown,10.1109/snpdwinter52325.2021.00019,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9403457/,1/30/2021 0:00,machine learning based network intrusion detection for data streaming iot applications,"In recent years, Internet of Things (IoT) technologies have been widely used in many fields such as surveillance, health-care, smart metering and environment monitoring. This extensive usage leads to massive data management and a complexity in data analysis. A huge number of IoT sensors are deployed for monitoring task and send continuously their collected data to gateways. IoT applications are analyzing these data flows and making real time decisions about specific monitored events (fire, flood, terrorist attacks, etc.). Anomalies that may be related to sensor failures or network intrusions are affecting such decisions. Therefore, they should be detected and eliminated as soon as they arrive. This task requires real time data processing detectors for making accurate and fast predictions. In this paper, we design an architecture for a real time network intrusion detection system for IoT streaming data. The system was developed, deployed and tested with the two leading stream processing frameworks (Apache Flink and Apache Spark Streaming). We used two different public datasets and different machine learning algorithms. Results show considerable throughputs and high detection accuracy especially for Apache Flink."
42,included,10.1109/ijcnn.2013.6706957,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6706957/,8/9/2013 0:00,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors."
43,included,10.1109/qrs51102.2020.00018,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9282796/,12/14/2020 0:00,phm technology for memory anomalies in cloud computing for iaas,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier."
44,unknown,10.1109/bds/hpsc/ids18.2018.00045,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8552303/,5/5/2018 0:00,real-time intelligent air quality evaluation on a resource-constrained embedded platform,"Indoor air quality has a major impact on health and comfort of building occupants. Poor air quality may reduce productivity in offices and impair students learning in classes. In order to provide localized air pollution data and tailor it for individual, wearable air quality sensor is a promising solution. Furthermore, crowd sensing has emerged as an Internet-of-Things (IoT) solution, which is economical, scalable and easy to deploy and re-deploy as it uses the power of crowd data collection. The goal of our proposed system is to monitor indoor air quality through a crowd sensing system that will use a set of sensors to measure air quality, monitor the concentration of pollutants continuously, and make recommendations in real time for improved air quality. In this paper artificial network is developed to perform real-time indoor air quality control. Utilizing created neural network embedded into a smart controller comfort level of air quality parameters such as temperature, CO_2 air concentration and humidity could be estimated after every measurement and used for adapting air conditioning systems to adjust air quality. Neural network data preparation and training process are discussed along with deployment of trained network on a smart controller."
45,unknown,10.1109/fads.2017.8253198,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8253198/,10/25/2017 0:00,the potential of conversational agents to provide a rapid hiv counseling and testing services,"In low-and middle-income countries where demand for health services outstrips the available supply of skilled labor, advances in information and communication technologies have already been shown to hold promise. While much of the mHealth literature continues to explore mature technologies such as text message and web portals, continual advancement in machine learning opens innovative new areas of exploration for public health practitioners. This paper explores one such possibility, a conversational agent, able to guide users through an HIV counseling and testing session. Using commercially available software (http://api.ai), an agent was designed and built according to the Center for Disease Control's guidelines for the provision of HIV counseling and testing in a non-clinical setting. The agent was linked to the Telegram chat client (http://telegram.org) and 10 testers were invited to participate in a simulated HIV counseling interaction. Six testers found that talking to the agent felt natural, and equivalent to chatting to a human. Seven said they would feel comfortable taking a real HIV test with the agent. Key concerns with the current agent were the use of overly formal language, the speed at which the agent responded (too fast) and the agent either misunderstanding or not understanding the tester. Positive sentiment towards the agent included the fact that testers felt like the session was more private and anonymous, and avoided the need for them to visit a public health facility and stand in a long queue to get tested."
46,unknown,10.1109/iceee2.2017.7935834,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7935834/,4/10/2017 0:00,using lstm networks to predict engine condition on large scale data processing framework,"As the Internet of Things technology is developing rapidly, companies have an ability to observe the health of engine components and constructed systems through collecting signals from sensors. According to output of IoT sensors, companies can build systems to predict the conditions of components. Practically the components are required to be maintained or replaced before the end of life in performing their assigned task. Predicting the life condition of a component is so crucial for industries that have intent to grow in a fast paced technological environment. Recent studies on predictive maintenance help industries to create an alert before the components are corrupted. Thanks to prediction of component failures, companies have a chance to sustain their operations efficiently while reducing their maintenance cost by repairing components in advance. Since maintenance affects production capacity and the service quality directly, optimized maintenance is the key factor for organizations to have more revenue and stay competitive in developing industrialized world. With the aid of well-designed prediction system for understanding current situation of an engine, components could be taken out of active service before malfunction occurs. With the help of inspection, effective maintenance extends component life, improves equipment availability and keeps components in a proper condition while reducing costs. Real time data collected from sensors is a great source to model component deteriorations. Markov Chain models, Survival Analysis, Optimization algorithms and several machine learning approaches have been implemented in order to model predictive maintenance. In this paper Long Short Term Memory (LSTM) networks has been performed to predict the current situation of an engine. LSTM model deals with a sequential input data. Training process of LSTM networks has been performed on large-scale data processing engine with high performance. Since huge amount of data is flowing into the predictive model, Apache Spark which is offering a distributed clustering environment has been used. The output of the LSTM network is deciding the current life condition of components and offering the alerts for components before the end of their life. The proposed model also trained and tested on an open source data that is about an engine degradation simulation provided by the Prognostics CoE at NASA Ames."
47,unknown,10.1109/access.2021.3051583,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9324826/,1/1/2021 0:00,data-driven condition monitoring of mining mobile machinery in non-stationary operations using wireless accelerometer sensor modules,"This paper presents the development of an easy-to-deploy and smart monitoring IoT system that utilizes vibration measurement devices to assess real-time condition of bulldozers, power shovels and backhoes, in non-stationary operations in the mining industry. According to operating experience data and the type of mining machine, total loss failure rates per machine fleet can reach up to 30%. Vibration analysis techniques are commonly used for condition monitoring and early detection of unforeseen failures to generate predictive maintenance plans for heavy machinery. However, this maintenance strategy is intensively used only for stationary machines and/or mobile machinery in stationary operations. Today, there is a lack of proper solutions to detect and prevent critical failures for non-stationary machinery. This paper shows a cost-effective solution proposal for implementing a vibration sensor network with wireless communication and machine learning data-driven capabilities for condition monitoring of non-stationary heavy machinery in mining operations. During the machine operation, 3-axis accelerations were measured using two sensors deployed across the machine. The machine accelerations (amplitudes and frequencies) are measured in two different frequency spectrums to improve each sensing location's time resolution. Multiple machine learning algorithms use this machine data to assess conditions according to manufacturer recommendations and operational benchmarks Proposed data-driven machine learning models classify the machine condition in states according to the ISO 2372 standards for vibration severity: Good, Acceptable, Unsatisfactory, or Unacceptable. After performing field tests with bulldozers and backhoes from different manufacturers, the machine learning algorithms are able to classify machine health status with an accuracy between 85% - 95%. Moreover, the system allows early detection of “Unacceptable” states between 120 to 170 hours prior to critical failure. These results demonstrate that the proposed system will collect relevant data to generate predictive maintenance plans and avoid unplanned downtimes."
48,unknown,10.1109/access.2018.2883106,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8543563/,1/1/2018 0:00,intelligent and real-time data acquisition for medical monitoring in smart campus,"With the continuous development of information technology, people are gradually moving from the digital age to the intelligent era. As one of the most representative emerging technologies in this era, the artificial intelligence is quietly changing our lives at an unprecedented rate. At present, the Internet of Things has been formally included the five emerging strategic industries in the country. Its development route is from safe city, digital city, to the perception of China. As an important part of a safe city, the medical and health field is also moving toward to the intelligent era. Therefore, it is imperative to construct a smart campus hospital environment monitoring system based on Internet of Things technology. The most core part of home environment intelligent monitoring system is the design of data acquisition and display methods. This paper mainly designs and implements the system client, system data format conversion, and system data transmission. The main technical points involved are HL7 protocol, AMQP protocol, and RabbitMQ framework, besides, and the cache technology is applied to the server. Information is cached to provide data assembly for different clients. Finally, real-time monitoring and alarming of the environment are implemented in the PC client and the Android client. The paper monitors the smart campus hospital environment, then carries out real-time transmission, storage, display, and finally analyzes the data to intelligently identify the smart campus hospital environment."
49,unknown,10.1109/wimob.2019.8923286,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8923286/,10/23/2019 0:00,design and deployment of a wireless ban at the edge for reliable healthcare monitoring,"Body Area Networks (BANs) have attracted a lot of research interest in the last decades as also witnessed by standardization activities and European Commission fundings. Today, commercial devices, which implement simplified BAN monitoring have appeared, although they usually imply expensive subscription costs or need for a supporting device carried by the user (e.g. a smart phone) which, in case of elderly people, cannot be easily held. However, the integration between commercial devices and external sensors located on/in the body is still an open issue due to the limited processing capabilities of low cost commercial devices. Also, no tools for anomaly detection aimed at reliable healthcare monitoring are currently commercially available. In this paper, we focus on Cloud-Assisted BANs and evolve this vision according to the emerging paradigm of edge computing. We present design, implementation and experimentation of a wireless BAN system which performs data transmission using a commercial, cheap, off-the-shelf gateway smart watch. A mechanism for prompt anomaly detection at the edge node is also supported for the purpose of reliable healthcare monitoring as well as pre-filtering of the data at the smart device itself. Also, in order to reduce the overhead caused by propagation of useless and time-correlated data, and to guarantee a prompt action in case of emergency, edge network nodes located closer to the patient BAN are exploited since they can execute machine learning algorithms to process large amounts of data and activate potential alerts in a shorter time and without overloading the cloud. In this work we describe a real system and evaluate the effectiveness of the approach in terms of false alarm probability."
50,included,10.1109/itc-egypt52936.2021.9513888,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9513888/,7/15/2021 0:00,a proposed end to end telemedicine system based on embedded system and mobile application using cmos wearable sensors,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver."
51,included,10.1109/wf-iot.2019.8767231,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8767231/,4/18/2019 0:00,efficient deployment of predictive analytics in edge gateways: fall detection scenario,"Ambient Assisted Living (AAL) represents the most promising Internet of Things (IoT) application due to its relevance in the elders healthcare and improvement of their quality of life. Recently, the AAL IoT ecosystem has been enriched with promising technologies such as edge computing, which has demonstrated to be the best approach to overcome the demanding requirements of AAL and healthcare services by providing a reduction of the amount of data to transfer to the cloud, an improvement of the response time, and quality of experience. Also, the deployment of Artificial Intelligence (AI) technologies at the edge provides intelligence to improve the decision making timely. However, this approach has been scarcely studied in AAL scenarios and the few proposals based on deploying machine learning models at the edge lack efficiency, security, mechanisms of resource management, service management, and deployment, as well as a real and experimental AAL scenario. For these reasons, this paper proposes an innovative edge gateway architecture to support the deployment of deep learning (DL) models in AAL and healthcare scenarios efficiently. To do so, we have added a predictive analytics module to deploy the models. Since AI technologies demand more resources, a container-based virtualization technology is employed on the edge gateway to manage the limited resources, and provide security and lifecycle services management. The edge gateway performance was evaluated deploying a DL-based fall detection application on it. As a result, our approach improves the inference time compared to that based on the cloud in 34 seconds and to similar approaches in 8 seconds."
52,unknown,10.1109/tencon.2019.8929612,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8929612/,10/20/2019 0:00,lung nodule detection from low dose ct scan using optimization on intel xeon and core processors with intel distribution of openvino toolkit,"With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance."
53,unknown,10.1109/bibm47256.2019.8983322,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8983322/,11/21/2019 0:00,openhi2 — open source histopathological image platform,"Transition from conventional to digital pathology requires a new category of biomedical informatic infrastructure which could facilitate delicate pathological routine. Pathological diagnoses are sensitive to many external factors and is known to be subjective. Only systems that can meet strict requirements in pathology would be able to run along pathological routines and eventually digitized the area, and the developed platform should comply with existing pathological routines and international standards. Currently, there are a number of available software tools which can perform histopathological tasks including virtual slide viewing, annotating, and basic image analysis, however, none of them can serve as a digital platform for pathology. Here we describe OpenHI2, an enhanced version Open Histopathological Image platform which is capable of supporting all basic pathological tasks and file formats; ready to be deployed in medical institutions on a standard server environment or cloud computing infrastructure. In this paper, we also describe the development decisions for the platform and propose solutions to overcome technical challenges including responsive region retrieval and viewing, virtual slide magnification, recording of diagnostic areas. These factors would promote OpenHI2 be used as a platform for histopathological images in real-world clinical settings. Furthermore, in research, OpenHI2 inherited the annotation functionality from the previous version, thus acquired annotations can be directly utilized by the newly added machine learning module which include popular machine learning models to perform tasks such as histology image classification and segmentation in the same environment. Addition can be made to the platform since each component is modularized and fully documented. OpenHI2 is free, open-source, and available at https://gitlab.com/BioAI/OpenHI."
54,unknown,10.1109/iccca49541.2020.9250902,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9250902/,10/31/2020 0:00,smart accident recognition and alerting system for edge devices,"The rate of road accidents has been rising over the years and the high fatalities in these accidents is a matter of great concern. In accidents like these, every second matters. Many times the medical services are unable to make it in time resulting in an unfortunate loss of life, making road accident deaths an integral issue. Also, chances of accidents are equally high during late hours. In an emergency at odd hours, one cannot guarantee if anyone will be present around to inform hospitals, or the police. This paper proposes an End-to-End Deep Learning solution to automate accident recognition and send real-time alerts to emergency services, that is the nearest hospitals and police stations. The proposed system is aimed to be deployed on edge devices attached to roadside CCTV cameras. For this purpose, an optimization step is performed on the deep learning models using the Intel's OpenVINO toolkit which improves performance on edge devices."
55,unknown,10.1109/access.2019.2919736,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8728285/,1/1/2019 0:00,federated learning-based computation offloading optimization in edge computing-supported internet of things,"Recently, smart cities, smart homes, and smart medical systems have challenged the functionality and connectivity of the large-scale Internet of Things (IoT) devices. Thus, with the idea of offloading intensive computing tasks from them to edge nodes (ENs), edge computing emerged to supplement these limited devices. Benefit from this advantage, IoT devices can save more energy and still maintain the quality of the services they should provide. However, computational offload decisions involve federation and complex resource management and should be determined in the real-time face to dynamic workloads and radio environments. Therefore, in this work, we use multiple deep reinforcement learning (DRL) agents deployed on multiple edge nodes to indicate the decisions of the IoT devices. On the other hand, with the aim of making DRL-based decisions feasible and further reducing the transmission costs between the IoT devices and edge nodes, federated learning (FL) is used to train DRL agents in a distributed fashion. The experimental results demonstrate the effectiveness of the decision scheme and federated learning in the dynamic IoT system."
56,included,10.1109/icac.2017.21,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8005354/,7/21/2017 0:00,ananke: a q-learning-based portfolio scheduler for complex industrial workflows,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns."
57,unknown,10.1016/j.scs.2021.103215,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85111854275,11/1/2021,ioht-enabled gliomas disease management using fog computing computing for sustainable societies,"The proliferation of sensor-based applications in healthcare has given rise to Internet of Health Things (IoHT) that improves patient safety, staff morale, and operational efficiency. Edge-fog computing has seen significant development in recent years and supports the association of various intelligent things with sensors for establishing smooth data transfer. However, it becomes challenging for edge-fog computing to tackle diverse IoHT settings such as efficient disease management, emergency response management, etc. The key limitation of existing architectures is the restricted scalability and inability to meet the demands of hierarchical computing environments for IoHT. This is because latency-sensitive applications often require large quantities of data to be measured and transferred to the data centers, which causes delay and reduced output. This research proposes a novel edge-fog computing framework for the convergence of machine learning ensemble with edge-fog computing. The proposed architecture delivers healthcare as a fog system that handles data from different sources to manage the diseases effectively. The proposed framework is used for the real-life implementation and automatic detection of gliomas diseases. Glioma is a kind of tumor, which ensues in the spinal cord and a portion of the brain. Glioma instigates in the glial cells that surround the nerve cells. The proposed edge-fog framework efficiently manages the real-time data related to gliomas. This framework is configured for specific operating modes including diverse edge-fog scenarios, different user requirements, quality of service, precision, and predictive accuracy. The proposed framework is evaluated using real-time datasets from various sources and experimentally tested with reliable datasets that disclose the effectiveness of the proposed architecture. The performance of the proposed model is evaluated in terms of power consumption, latency, accuracy, and execution time, respectively."
58,unknown,10.1016/j.eswa.2021.114951,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85104365062,9/1/2021,providing music service in ambient intelligence: experiments with gym users,"Ambient Intelligence (AmI) is an interdisciplinary research area of ICT which has evolved since the 90s, taking great advantage from the advent of the Internet of Things (IoT). AmI creates, by using Artificial Intelligence (AI), an intelligent ecosystem in which computers, sensors, lighting, music, personal devices, and distributed services, work together to improve the user experience through the support of natural and intuitive user interfaces. Nowadays, AmI is used in various contexts, e.g., for building smart homes and smart cities, providing healthcare, and creating an adequate atmosphere in retail and public environments.
                  In this paper, we propose a novel AmI system for gym environments, named Gym Intelligence, able to provide adequate music atmosphere, according to the users’ physical effort during the training. The music is taken from Spotify and is classified according to some music features, as provided by Spotify itself. The system is based on a multi-agent computational intelligence model built on two main components: 
                        
                           (
                           i
                           )
                        
                      machine learning methods that forecast appropriate values for the Spotify music features, and 
                        
                           (
                           ii
                           )
                        
                      a multi-objective dynamic genetic algorithm that selects a specific Spotify music track, according to such values. Gym Intelligence is built by sensing the ambient with a minimal, low-cost, and non-intrusive set of sensors, and it has been designed considering the outcome of a preliminary analysis in real gyms, involving real users. We have considered well-known regression methods and we have validated them using a collected data 
                        
                           (
                           i
                           )
                        
                      about the users’ physical effort, through the sensors, and 
                        
                           (
                           ii
                           )
                        
                      about the users’ music preferences, through an Android app that the users have used during the training. Among the regression methods considered, the one that provided the best results is the Random Forest, which predicted Spotify music features with a mean absolute error of 0.02 and a root mean squared error of 0.05. We have implemented Gym Intelligence and deployed it in five real gyms. We have evaluated it conducting several experiments. The experiments show how, with the help of Gym Intelligence, the users’ satisfaction about the provided background music, rose from 3.05 to 4.91 (on a scale from 1 to 5, where 5 is the maximum score)."
59,unknown,10.1016/j.micpro.2020.103301,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85094168107,2/1/2021,iot enabled cancer prediction system to enhance the authentication and security using cloud computing,"In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results."
60,unknown,10.1016/j.procs.2021.03.025,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85106733954,1/1/2021,lightweight photoplethysmography quality assessment for real-time iot-based health monitoring using unsupervised anomaly detection,"Real-time remote health monitoring is dramatically growing, revolutionizing healthcare delivery and outcome in everyday settings. Such remote services enable monitoring individuals anywhere and anytime, allowing diseases early detection and prevention. Photoplethysmography (PPG) is a non-invasive and convenient technique that enables tracking vital signs such as heart rate, heart rate variability, respiration rate, and blood oxygen saturation. PPG is broadly used in various clinical and commercial wearable devices, as it is easy-to-implement and low-cost. However, the technique is highly susceptible to motion artifacts and environmental noises, which distort the collected signals. Therefore, the signal quality needs to be investigated, and unreliable signals should be discarded. In the literature, rule-based and machine learning-based PPG quality assessment methods have been investigated in several studies. However, the rule-based methods are mostly inaccurate in remote health monitoring, where users engage in different physical activities. The supervised machine learning-based methods –including deep learning–are also infeasible for real-time monitoring applications since they are slow and are dependent on a massive pool of annotated data to train the model. In this paper, we introduce a PPG quality assessment method, enabled by an elliptical envelope, which requires low computational resources. The method clusters the PPG signals into two groups as “reliable” and “unreliable.” We also investigate various features extracted from the PPG signals. Five features with the highest scoring values are selected to be fed to the elliptical envelope model. Moreover, we assess the performance of the proposed method in terms of accuracy and execution time, using data collected in free-living conditions via an Internet-of-Things-based health monitoring system enabled by smart wristbands. The method is evaluated in comparison to a state-of-the-art PPG quality assessment method. We also provide the model implemented in Python for the community to be used in their solutions."
61,unknown,10.1016/j.ipm.2020.102340,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85087504158,11/1/2020,a topic modeling framework for spatio-temporal information management,"Real-time processing and learning of conflicting data, especially messages coming from different ideas, locations, and time, in a dynamic environment such as Twitter is a challenging task that recently gained lots of attention. This paper introduces a framework for managing, processing, analyzing, detecting, and tracking topics in streaming data. We propose a model selector procedure with a hybrid indicator to tackle the challenge of online topic detection. In this framework, we built an automatic data processing pipeline with two levels of cleaning. Regular and deep cleaning are applied using multiple sources of meta knowledge to enhance data quality. Deep learning and transfer learning techniques are used to classify health-related tweets, with high accuracy and improved F1-Score. In this system, we used visualization to have a better understanding of trending topics. To demonstrate the validity of this framework, we implemented and applied it to health-related twitter data from users originating in the USA over nine months. The results of this implementation show that this framework was able to detect and track the topics at a level comparable to manual annotation. To better explain the emerging and changing topics in various locations over time the result is graphically displayed on top of the United States map."
62,included,10.1016/j.iot.2020.100185,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85086362688,9/1/2020,highly-efficient fog-based deep learning aal fall detection system,"Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement."
63,unknown,10.1016/j.eswa.2020.113251,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85079340111,7/1/2020,integrating complex event processing and machine learning: an intelligent architecture for detecting iot security attacks,"The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives."
64,unknown,10.1016/j.iot.2019.100130,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85077314197,3/1/2020,an iot based device-type invariant fall detection system,"As the world elderly population is increasing rapidly, the use of technology for the development of accurate and fast automatic fall detection systems has become a necessity. Most of the fall detection systems are developed for specific devices which reduces the versatility of the fall detection system. This paper proposes a centralized unobtrusive IoT based device-type invariant fall detection and rescue system for monitoring of a large population in real-time. Any type of devices such as Smartphones, Raspberry Pi, Arduino, NodeMcu, and Custom Embedded Systems can be used to monitor a large population in the proposed system. The devices are placed into the users’ left or right pant pocket. The accelerometer data from the devices are continuously sent to a multithreaded server which hosts a pre-trained machine learning model that analyzes the data to determine whether a fall has occurred or not. The server sends the classification results back to the corresponding devices. If a fall is detected, the server notifies the mediator of the user's location via an SMS. As a failsafe, the corresponding device alerts nearby individuals by sounding the buzzer and contacts emergency medical services and mediators via SMS for immediate medical assistance, thus saving the user's life. The proposed system achieved 99.7% accuracy, 96.3% sensitivity, and 99.6% specificity. Finally, the proposed system can be implemented on a variety of devices and used to reliably monitor a large population with low false alarm rate, without obstructing the users’ daily living, as no external connections are required."
65,included,10.1016/j.micpro.2019.102960,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85077060597,3/1/2020,a novel hybrid optimized and adaptive reconfigurable framework for the implementation of hybrid bio-inspired classifiers for diagnosis,"Due to recent advances in IoT (Internet of Things) technologies, availability of reliable data and emergence of machine learning, bio-inspired learning and artificial intelligence, has demonstrated its ability to solve the large complex problems which is not possible before. In particular, machine learning and bio-inspired learning algorithms provides the effective solutions in image processing techniques. However, the implementation of the above-mentioned algorithms in the general CPU requires the intensive usage of bandwidth, area and power which makes the CPU unhealthy of usage and implementation. To overcome this problem, ASIC (application specific integrated circuits), GPU (Graphics Processing Unit) &FPGA (Field Programmable gate arrays) have been employed to improve the performance of the hybrid machine learning (ML) classifiers and deep learning algorithms. FPGA has been recently employed for an effective implementation and to achieve the high performance of the learning algorithms. But integrating the complex learning algorithms in FPGA still remains to be real challenge among the researchers. The paper proposes new reconfigurable architectures for bio- inspired classifiers to diagnosis the medical casualties which can be suitable for the tele health care applications. This paper aim is as follows (i) Design and implementation of Parallel Fusion of FSM and Reconfigurable shared Distributed Arithmetic for Bio-Inspired Classifiers (ii) Development of Accelerator Environment to test the performance of proposed architecture (iii) Performance evaluation of proposed architecture in terms of accuracy of detection in compared with MATLAB simulation iv) Implementation of proposed architectures in different ARtix-7 architectures and determination of power, throughput and area . Moreover, the proposed architecture has been tested with the and compared with the other existing architectures."
66,unknown,10.1016/j.cmpb.2019.105277,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076945151,2/1/2020,an extensible software platform for interdisciplinary cardiovascular imaging research,"Background and objective
                  Cardiovascular imaging is an exponentially growing field with aspects ranging from image acquisition and analysis to disease characterization, and evaluation of therapy approaches.The transfer of innovative new technological and algorithmic solutions into clinical practice is still slow. In addition to the verification of solutions, their integration in the clinical processing workflow must be enabled for the assessment of clinical impact and risks. The goal of our software platform for cardiac image processing – CAIPI – is to support researchers from different specialties such as imaging physics, computer science, and medicine by a common extensible platform to address typical challenges and hurdles in interdisciplinary cardiovascular imaging research. It provides an integrated solution for method comparison, integrated analysis, and validation in the clinical context. The interface concept enables a combination with existing frameworks that address specific aspects of the pipeline, such as modeling (e.g., OpenCMISS, CARP) or image reconstruction (Gadgetron).
               
                  Methods
                  In our platform, we developed a concept for import, integration, and management of cardiac image data. The integration approach considers the spatiotemporal properties of the beating heart through a specific data model. The solution is based on MeVisLab and provides functionalities for data retrieval and storage. Two types of plugins can be added. While ToolPlugins usually provide processing algorithms such as image correction and segmentation, AnalysisPlugins enable interactive data exploration and reporting. GUI integration concepts are presented for both plugin types. We developed domain-specific reporting and visualization tools (e.g., AHA segment model) to enable validation studies by clinical experts. The platform offers plugins for calculating and reporting quantitative parameters such as cardiac function, which can be used to, e.g., evaluate the effect of processing algorithms on clinical parameters. Export functionalities include quantitative measurements to Excel, image data to PACS, and STL models to modeling and simulation tools.
               
                  Results
                  To demonstrate the applicability of this concept both for method development and clinical application, we present use cases representing different problems along the innovation chain in cardiac MR imaging.
                  Validation of an image reconstruction method (MRI T1 mapping)
                  Validation of an image correction method for real-time 2D-PC MRI
                  Comparison of quantification methods for blood flow analysis
                  Training and integration of machine learning solutions with expert annotations
                  Clinical studies with new imaging techniques (flow measurements in the carotid arteries and peripheral veins as well as cerebral spinal fluid).
               
                  Conclusion
                  The presented platform can be used in interdisciplinary teams, in which engineers or data scientists perform the method validation, followed by clinical research studies in patient collectives. The demonstrated use cases show how it enables the transfer of innovations through validation in the cardiovascular application context."
67,unknown,10.1016/j.gie.2019.03.019,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065917454,7/1/2019,quality assurance of computer-aided detection and diagnosis in colonoscopy,"Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field “deep learning,” have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice—polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing."
68,unknown,10.1016/j.compeleceng.2017.03.009,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85015331423,1/1/2018,applying spark based machine learning model on streaming big data for health status prediction,"Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action."
69,unknown,10.1016/j.ajo.2017.03.026,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85017447877,6/1/2017,using electronic health records to build an ophthalmologic data warehouse and visualize patients' data,"Purpose
                  To develop a near-real-time data warehouse (DW) in an academic ophthalmologic center to gain scientific use of increasing digital data from electronic medical records (EMR) and diagnostic devices.
               
                  Design
                  Database development.
               
                  Methods
                  Specific macular clinic user interfaces within the institutional hospital information system were created. Orders for imaging modalities were sent by an EMR-linked picture-archiving and communications system to the respective devices. All data of 325 767 patients since 2002 were gathered in a DW running on an SQL database. A data discovery tool was developed. An exemplary search for patients with age-related macular degeneration, performed cataract surgery, and at least 10 intravitreal (excluding bevacizumab) injections was conducted.
               
                  Results
                  Data related to those patients (3 142 204 diagnoses [including diagnoses from other fields of medicine], 720 721 procedures [eg, surgery], and 45 416 intravitreal injections) were stored, including 81 274 optical coherence tomography measurements. A web-based browsing tool was successfully developed for data visualization and filtering data by several linked criteria, for example, minimum number of intravitreal injections of a specific drug and visual acuity interval. The exemplary search identified 450 patients with 516 eyes meeting all criteria.
               
                  Conclusions
                  A DW was successfully implemented in an ophthalmologic academic environment to support and facilitate research by using increasing EMR and measurement data. The identification of eligible patients for studies was simplified. In future, software for decision support can be developed based on the DW and its structured data. The improved classification of diseases and semiautomatic validation of data via machine learning are warranted."
70,unknown,10.1016/j.procs.2016.09.052,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84992317444,1/1/2016,designing and testing healthtracker for activity recognition and energy expenditure estimation within the daphne platform,"This paper describes the design and evaluation of a mobile software library, HealthTracker, which aims to produce activity and energy expenditure estimations in real-time from accelerometer and gyroscope data provided by wearable sensors. Using feature extraction together with a classifier trained using machine learning, the system will automatically and periodically send all the produced estimations to a cloud-based platform that will allow later evaluation by both the user and a physician or caretaker. The system is presented within the DAPHNE platform, an ICT ecosystem designed to provide a means for remote health and lifestyle monitoring and guidance between physicians and their patients."
71,included,10.1184/r1/6710654.v1,,core,health,health' AND 'machine learning' AND ('real-world' AND 'deploy'),10.1184/r1/6710654.v1,6/30/2018 0:00,software and system health management for autonomous robotics missions,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time"
72,unknown,10.2118/204794-ms,"Day 4 Wed, December 01, 2021",semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d11c7341d3bcc4b0c73076048dcd10b35748f355,1/1/2021 0:00,satellite fields digitalization & als optimization with edge & advance analytics application,"
 Data monitoring in remote satellite field without any DOF platform is a challenging task but critical for ALS monitoring and optimization. In SRP wells the VFD data collection is important for analysis of downhole pump behavior and system health. SRP maintenance crew collects data from VFDs daily, but it is time consuming and can target only few wells in a day. The steps from requirement of dyna to final decision taken for ALS optimization are mobilizing team, permits approvals, download data, e-mail dynacards, dyna visualization, final decision.
 The problems with above process were: -
 Insufficient and discrete data for any post-failure analysis or ALS-optimization Minimal data to investigate the pre failure events
 The lack of real time monitoring was resulting in well downtime and associated production loss. The combination of IOT, Cloud Computing and Machine learning was implemented to shift from the reactive to proactive approach which helped in ALS Optimization and reduced production loss.
 The data was transmitted to a Cloud server and further it was transmitted to web-based app. Since thousands of Dynacards are generated in a day, hence it requires automated classification using computer driven pattern recognition techniques. The real time data is used for analysis involving basic statistic and Machine learning algorithms. The critical pump signatures were identified using machine learning libraries and email is generated for immediate action. Several informative dashboards were developed which provide quick analysis of ALS performance. The types of dashboard are as below
 Well Operational Status Dynacards Interpretation module SRP parameters visualization Machine Learning model calibration module Pump Performance Statistics
 After collection of enough data and creation of analytical dashboards on the three wells using domain knowledge the gained insights were used for ALS optimization. To keep the model in an evergreen high-confidence prediction state, inputs from domain experts are often required. After regular fine-tuning the prediction accuracy of the ML model increased to 80-85 %. In addition, system was made flexible so that a new algorithm can be deployed when required. Smart Alarms were generated involving statistic and Machine Learning by the system which gives alerts by e-mail if an abnormal behavior or erratic dynacards were identified. This helped in reduction of well downtime in some events which were treated instinctively before.
 The integration of domain knowledge and digitalization enables an engineer to take informed and effective decisions. The techniques discussed above can be implemented in marginal fields where DOF implementation is logistically and economically challenged. EDGE along with advanced analytics will gain more technological advances and can be used in other potential domains as well in near future."
73,unknown,10.3390/ijerph18137087,International journal of environmental research and public health,semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/176d31f73bc65e07dc6e650b75404c31666dcdbb,1/1/2021 0:00,design of a spark big data framework for pm2.5 air pollution forecasting,"In recent years, with rapid economic development, air pollution has become extremely serious, causing many negative effects on health, environment and medical costs. PM2.5 is one of the main components of air pollution. Therefore, it is necessary to know the PM2.5 air quality in advance for health. Many studies on air quality are based on the government’s official air quality monitoring stations, which cannot be widely deployed due to high cost constraints. Furthermore, the update frequency of government monitoring stations is once an hour, and it is hard to capture short-term PM2.5 concentration peaks with little warning. Nevertheless, dealing with short-term data with many stations, the volume of data is huge and is calculated, analyzed and predicted in a complex way. This alleviates the high computational requirements of the original predictor, thus making Spark suitable for the considered problem. This study proposes a PM2.5 instant prediction architecture based on the Spark big data framework to handle the huge data from the LASS community. The Spark big data framework proposed in this study is divided into three modules. It collects real time PM2.5 data and performs ensemble learning through three machine learning algorithms (Linear Regression, Random Forest, Gradient Boosting Decision Tree) to predict the PM2.5 concentration value in the next 30 to 180 min with accompanying visualization graph. The experimental results show that our proposed Spark big data ensemble prediction model in next 30-min prediction has the best performance (R2 up to 0.96), and the ensemble model has better performance than any single machine learning model. Taiwan has been suffering from a situation of relatively poor air pollution quality for a long time. Air pollutant monitoring data from LASS community can provide a wide broader monitoring, however the data is large and difficult to integrate or analyze. The proposed Spark big data framework system can provide short-term PM2.5 forecasts and help the decision-maker to take proper action immediately."
74,unknown,http://arxiv.org/abs/2207.03066v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2207.03066v1,7/7/2022 0:00,device-cloud collaborative recommendation via meta controller,"On-device machine learning enables the lightweight deployment of
recommendation models in local clients, which reduces the burden of the
cloud-based recommenders and simultaneously incorporates more real-time user
features. Nevertheless, the cloud-based recommendation in the industry is still
very important considering its powerful model capacity and the efficient
candidate generation from the billion-scale item pool. Previous attempts to
integrate the merits of both paradigms mainly resort to a sequential mechanism,
which builds the on-device recommender on top of the cloud-based
recommendation. However, such a design is inflexible when user interests
dramatically change:
  the on-device model is stuck by the limited item cache while the cloud-based
recommendation based on the large item pool do not respond without the new
re-fresh feedback.
  To overcome this issue, we propose a meta controller to dynamically manage
the collaboration between the on-device recommender and the cloud-based
recommender, and introduce a novel efficient sample construction from the
causal perspective to solve the dataset absence issue of meta controller. On
the basis of the counterfactual samples and the extended training, extensive
experiments in the industrial recommendation scenarios show the promise of meta
controller in the device-cloud collaboration."
75,unknown,http://arxiv.org/abs/2101.07831v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.07831v1,1/19/2021 0:00,"multi-task network pruning and embedded optimization for real-time
  deployment in adas","Camera-based Deep Learning algorithms are increasingly needed for perception
in Automated Driving systems. However, constraints from the automotive industry
challenge the deployment of CNNs by imposing embedded systems with limited
computational resources. In this paper, we propose an approach to embed a
multi-task CNN network under such conditions on a commercial prototype
platform, i.e. a low power System on Chip (SoC) processing four surround-view
fisheye cameras at 10 FPS.
  The first focus is on designing an efficient and compact multi-task network
architecture. Secondly, a pruning method is applied to compress the CNN,
helping to reduce the runtime and memory usage by a factor of 2 without
lowering the performances significantly. Finally, several embedded optimization
techniques such as mixed-quantization format usage and efficient data transfers
between different memory areas are proposed to ensure real-time execution and
avoid bandwidth bottlenecks. The approach is evaluated on the hardware
platform, considering embedded detection performances, runtime and memory
bandwidth. Unlike most works from the literature that focus on classification
task, we aim here to study the effect of pruning and quantization on a compact
multi-task network with object detection, semantic segmentation and soiling
detection tasks."
76,unknown,http://arxiv.org/abs/2011.09463v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.09463v3,11/18/2020 0:00,"easytransfer -- a simple and scalable deep transfer learning platform
  for nlp applications","The literature has witnessed the success of leveraging Pre-trained Language
Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural
Language Processing (NLP) applications, yet it is not easy to build an
easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the
EasyTransfer platform is designed to develop deep TL algorithms for NLP
applications. EasyTransfer is backended with a high-performance and scalable
engine for efficient training and inference, and also integrates comprehensive
deep TL algorithms, to make the development of industrial-scale TL applications
easier. In EasyTransfer, the built-in data and model parallelism strategies,
combined with AI compiler optimization, show to be 4.0x faster than the
community version of distributed training. EasyTransfer supports various NLP
models in the ModelZoo, including mainstream PLMs and multi-modality models. It
also features various in-house developed TL algorithms, together with the
AppZoo for NLP applications. The toolkit is convenient for users to quickly
start model training, evaluation, and online deployment. EasyTransfer is
currently deployed at Alibaba to support a variety of business scenarios,
including item recommendation, personalized search, conversational question
answering, etc. Extensive experiments on real-world datasets and online
applications show that EasyTransfer is suitable for online production with
cutting-edge performance for various applications. The source code of
EasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer)."
77,unknown,http://arxiv.org/abs/2003.06700v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.06700v3,3/14/2020 0:00,"cocopie: making mobile ai sweet as pie --compression-compilation
  co-design goes a long way","Assuming hardware is the major constraint for enabling real-time mobile
intelligence, the industry has mainly dedicated their efforts to developing
specialized hardware accelerators for machine learning and inference. This
article challenges the assumption. By drawing on a recent real-time AI
optimization framework CoCoPIE, it maintains that with effective
compression-compiler co-design, it is possible to enable real-time artificial
intelligence on mainstream end devices without special hardware. CoCoPIE is a
software framework that holds numerous records on mobile AI: the first
framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,
language models, and so on; the fastest DNN pruning and acceleration framework,
up to 180X faster compared with current DNN pruning on other frameworks such as
TensorFlow-Lite; making many representative AI applications able to run in
real-time on off-the-shelf mobile devices that have been previously regarded
possible only with special hardware support; making off-the-shelf mobile
devices outperform a number of representative ASIC and FPGA solutions in terms
of energy efficiency and/or performance."
78,unknown,http://arxiv.org/abs/2003.02454v4,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.02454v4,3/5/2020 0:00,agl: a scalable system for industrial-purpose graph machine learning,"Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
  In this paper, we design AGL, a scalable, fault-tolerance and integrated
system, with fully-functional training and inference for GNNs. Our system
design follows the message passing scheme underlying the computations of GNNs.
We design to generate the $k$-hop neighborhood, an information-complete
subgraph for each node, as well as do the inference simply by merging values
from in-edge neighbors and propagating values to out-edge neighbors via
MapReduce. In addition, the $k$-hop neighborhood contains information-complete
subgraphs for each node, thus we simply do the training on parameter servers
due to data independency. Our system AGL, implemented on mature
infrastructures, can finish the training of a 2-layer graph attention network
on a graph with billions of nodes and hundred billions of edges in 14 hours,
and complete the inference in 1.2 hour."
79,unknown,http://arxiv.org/abs/1911.05771v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1911.05771v1,11/13/2019 0:00,"machine learning based network vulnerability analysis of industrial
  internet of things","It is critical to secure the Industrial Internet of Things (IIoT) devices
because of potentially devastating consequences in case of an attack. Machine
learning and big data analytics are the two powerful leverages for analyzing
and securing the Internet of Things (IoT) technology. By extension, these
techniques can help improve the security of the IIoT systems as well. In this
paper, we first present common IIoT protocols and their associated
vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the
utilization of machine learning in countering these susceptibilities. Following
that, a literature review of the available intrusion detection solutions using
machine learning models is presented. Finally, we discuss our case study, which
includes details of a real-world testbed that we have built to conduct
cyber-attacks and to design an intrusion detection system (IDS). We deploy
backdoor, command injection, and Structured Query Language (SQL) injection
attacks against the system and demonstrate how a machine learning based anomaly
detection system can perform well in detecting these attacks. We have evaluated
the performance through representative metrics to have a fair point of view on
the effectiveness of the methods."
80,unknown,http://arxiv.org/abs/1908.08998v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1908.08998v2,8/13/2019 0:00,aibench: an industry standard internet service ai benchmark suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html."
81,unknown,http://arxiv.org/abs/1807.00139v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1807.00139v1,6/30/2018 0:00,harnessing constrained resources in service industry via video analytics,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance."
82,unknown,10.1007/s11042-022-13514-7,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11042-022-13514-7,7/26/2022 0:00,bamcloud: a cloud based mobile biometric authentication framework,"There has been an exponential increase in the number of users switching to mobile banking. Therefore, various countries are adopting biometric solutions as security measures. Biometric technologies provide the potential security framework to make banking more convenient and secure than it has ever been. These technologies are gaining much popularity because of the ease in capturing biometric data in real-time using one’s mobile phone. At the same time, the exponential growth of enrollment in the biometric system produces a massive amount of high-dimensional data. To overcome performance-related issues arising due to the resulting data deluge, this paper aims to propose a distributed mobile biometric system based on a high-performance cluster Cloud. In this paper, a Cloud-based mobile biometric authentication framework (BAMCloud) is proposed that uses dynamic signatures for authentication. The process flow of the BAMCloud system involves capturing data using any handheld mobile device, followed by its storage, preprocessing, and training of the system in a distributed manner over the Cloud. MapReduce has been implemented on the Hadoop platform to reduce the processing time. For model training, The Levenberg-Marquardt backpropagation neural network has been used. It achieves a speed of 8.5 times the original speed and performance of 96.23%. Furthermore, the cost-benefit analysis of the implemented system shows that the cost of implementation and execution of the system is less than the existing ones. The experiments demonstrate that better performance is achieved by implementing the proposed framework as compared to other methods used in recent literature."
83,unknown,10.1007/978-981-16-5847-1_6,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-981-16-5847-1_6,1/1/2022 0:00,an architecture for quality centric crop production system,"Artificial Intelligence (AI) can be applied to find the patterns in historical data and help finding real-time predictions for making data-driven decisions. Digital transformation and automation are required for industries for better growth. AI-predictive analytics software solution that at its core, delivers the ability to recommend or select farm fields, practices, timing and inputs that have a high probability of delivering crops that meet an optimized set of quality attributes. These attributes allow the client to produce food with specific characteristics for competitive advantage and/or financial benefit. The system will allow growers to benefit by allowing “what-if” modeling of farming practices to anticipate desirable crop attributes. In this chapter, we proposed an architecture for developing a recommendation system for helping farmers produce crops with specific quality attributes which are required by the specific consumer. The recommendations can be generated by using the endogenous and exogenous data captured through IoT sensors and AI modeling. The architecture is generic and solutions can be designed to work for a range of foods/crops. The architecture is designed to develop solutions that deliver food producer’s a focused value proposition. It uses an Agcentric methodology in creating the capability to recommend the fields for crop production."
84,included,10.1109/syscon.2018.8369547,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8369547/,4/26/2018 0:00,an interactive architecture for industrial scale prediction: industry 4.0 adaptation of machine learning,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics."
85,included,10.1109/med.2017.7984310,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7984310/,7/6/2017 0:00,cloud computing for big data analytics in the process control industry,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0."
86,unknown,10.1109/ains47559.2019.8968698,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8968698/,11/21/2019 0:00,cyber security risk assessment on industry 4.0 using ics testbed with ai and cloud,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures."
87,unknown,10.1109/icsima47653.2019.9057343,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9057343/,8/29/2019 0:00,real-time wireless monitoring for three phase motors in industry: a cost-effective solution using iot,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan."
88,unknown,10.1109/wrc-sara.2019.8931920,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8931920/,8/22/2019 0:00,software-defined cloud manufacturing in the context of industry 4.0,"In the practice of &#x201C;Cloud Manufacturing (CMfg)&#x201D; or &#x201C;Industrial Internet&#x201D;, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &#x0026; upgrading."
89,unknown,10.1109/phm-paris.2019.00052,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8756426/,5/5/2019 0:00,a common service middleware for intelligent complex software system,"With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technology, various intelligent complex software systems (i-CSS) are increasingly popular, becoming one of the most important software system development paradigms. Its inherent growth construction and adaptive evolution properties pose new challenges to existing software design and development methods. Especially, how to achieve growth construction by quickly reusing existing excellent software resources, and how to establish data flow across system boundaries around the business flow to achieve adaptive evolution based on data intelligence. Facing the above challenges, this paper proposes novel data-oriented analysis and design method (DOAD), microservice and container-based mashup development method (SCMD). On this basis, the paper implements i-CSS common service middleware to support the above methods in engineering. In a real cloud-based PHM system and the other three industry projects, the proposed methods and middleware are used for application verification, the results show that they can greatly reduce the complexity of i-CSS design and development, reduce the ability threshold of the i-CSS development team, improve the development efficiency of the development team, reduce the team development workload by 31.5% on average, and help the i-CSS team effectively cope with the challenges of growth construction and adaptive evolution."
90,unknown,10.1109/csci49370.2019.00084,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9071016/,12/7/2019 0:00,a real-time based intelligent system for predicting equipment status,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time."
91,unknown,10.1109/icmla.2019.00115,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8999203/,12/19/2019 0:00,an edge computing visual system for vegetable categorization,"In self-service supermarket and retail industry, efforts to reduce customer wait time using automatic grocery item identification are challenged by low recognition accuracy, long response time and substantial requirement for equipment. In this paper, we propose a novel edge computing system named EdgeVegfru for vegetable and fruit image classification. While existing work on Vegfru dataset shows excellent performance, few of them have been deployed in real-world applications. We adopt an edge computing paradigm, design, implement and evaluate the whole system on the Android devices. The proposed deep learning model and quantization algorithm reduce the model size and inference time significantly. Our system has shown out-standing accuracy within limited time and computation resources, compared with other machine learning methods(such as Support Vector Machine(SVM), Random Forest(RF)), thus providing the potential path for automatic recognition and pricing in self-service retail stores."
92,unknown,10.1109/bigdata.2016.7840859,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7840859/,12/8/2016 0:00,building a research data science platform from industrial machines,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions."
93,unknown,10.1109/ehb50910.2020.9280165,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9280165/,10/30/2020 0:00,drivers’ drowsiness detection and warning systems for critical infrastructures,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events."
94,unknown,10.1109/csit49958.2020.9321954,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9321954/,9/26/2020 0:00,eco-friendly home automation system implemented using machine learning algorithms,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution."
95,unknown,10.1109/bigdata.2018.8622583,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8622583/,12/13/2018 0:00,ensemble machine learning systems for the estimation of steel quality control,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out."
96,unknown,10.1109/dcoss.2019.00079,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8804483/,5/31/2019 0:00,middleware for real-time event detection andpredictive analytics in smart manufacturing,"Industry 4.0 is a recent trend of automation for manufacturing technologies and represents the fourth industrial revolution which transforms current industrial processes with the use of technologies such as automation, data analytics, cyber-physical systems, IoT, artificial intelligence, etc. The vision of Industry 4.0 is to build an end-to-end industrial transformation with the support of digitization. Data analytics plays a key role to get a better understanding of business processes and to design intelligent decision support systems. However, a key challenge faced by industry is to integrate multiple autonomous processes, machines and businesses to get an integrated view for data analytics activities. Another challenge is to develop methods and mechanisms for real-time data acquisition and analytics on-the-fly. In this paper, we propose a semantically interoperable framework for historical data analysis combined with real-time data acquisition, event detection, and real-time data analytics for very precise production forecasting within a manufacturing unit. Besides historical data analysis techniques, our middleware is capable of collecting data from diverse autonomous applications and operations in real time using various IoT devices, analyzing the collected data on the fly, and evaluating the impact of any detected unexpected events. Using semantic technologies we integrate multiple autonomous systems (e.g. production system, supply chain management and open data). The outcome of real-time data analytics is used in combination with machine learning models trained over historical data in order to precisely forecast production in a manufacturing unit in real time. We also present our key findings and challenges faced while deploying our solution in real industrial settings for a large manufacturing unit."
97,included,10.23919/iconac.2019.8895095,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8895095/,9/7/2019 0:00,ant colony optimization algorithm for industrial robot programming in a digital twin,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention."
98,included,10.1109/isie45063.2020.9152441,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9152441/,6/19/2020 0:00,deployment of a smart and predictive maintenance system in an industrial case study,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions."
99,unknown,10.1109/ro-man50785.2021.9515431,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9515431/,8/12/2021 0:00,simplifying the a.i. planning modeling for human-robot collaboration,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism."
100,unknown,10.1109/his.2011.6122180,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6122180/,12/8/2011 0:00,neural network prognostics model for industrial equipment maintenance,"This paper presents a new prognostics model based on neural network technique for supporting industrial maintenance decision. In this study, the probabilities of failure based on the real condition equipment are initially calculated by using logistic regression method. The failure probabilities are subsequently utilized as input for prognostics model to predict the future value of failure condition and then used to estimate remaining useful lifetime of equipment. By having a time series of predicted failure probability, the failure distribution can be generated and used in the maintenance cost model to decide the optimal time to do maintenance. The proposed prognostic model is implemented in the industrial equipment known as autoclave burner. The result from the model reveals that it can give prior warnings and indication to the maintenance department to take an appropriate decision instead of dealing with the failures while the autoclave burner is still operating. This significant contribution provides new insights into the maintenance strategy which enables the use of existing condition data from industrial equipment and prognostics approach."
101,unknown,10.1109/rtsi50628.2021.9597339,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9597339/,9/9/2021 0:00,towards graph machine learning for smart grid knowledge graphs in industrial scenarios,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG."
102,included,10.1109/fdl53530.2021.9568376,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9568376/,9/10/2021 0:00,a container-based design methodology for robotic applications on kubernetes edge-cloud architectures,"Programming modern Robots&#x0027; missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub."
103,unknown,10.1109/icct46805.2019.8947193,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8947193/,10/19/2019 0:00,edge ai for heterogeneous and massive iot networks,"By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud."
104,unknown,10.1109/ijcnn52387.2021.9533808,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9533808/,7/22/2021 0:00,end-to-end federated learning for autonomous driving vehicles,"In recent years, with the development of computation capability in devices, companies are eager to investigate and utilize suitable ML/DL methods to improve their service quality. However, with the traditional learning strategy, companies need to first build up a powerful data center to collect and analyze data from the edge and then perform centralized model training, which turns out to be inefficient. Federated Learning has been introduced to solve this challenge. Because of its characteristics such as model-only exchange and parallel training, the technique can not only preserve user data privacy but also accelerate model training speed. The method can easily handle real-time data generated from the edge without taking up a lot of valuable network transmission resources. In this paper, we introduce an approach to end-to-end on-device Machine Learning by utilizing Federated Learning. We validate our approach with an important industrial use case in the field of autonomous driving vehicles, the wheel steering angle prediction. Our results show that Federated Learning can significantly improve the quality of local edge models and also reach the same accuracy level as compared to the traditional centralized Machine Learning approach without its negative effects. Furthermore, Federated Learning can accelerate model training speed and reduce the communication overhead, which proves that this approach has great strength when deploying ML/DL components to various real-world embedded systems."
105,unknown,10.1109/iciss.2010.5656975,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5656975/,10/24/2010 0:00,research and implementation of the temperature control system of heat treatment based on .net and rs-485 bus,"RS-485 is a widely used industrial field bus. The successful application of AIBUS protocol in AI series display control instrument, make the AIDCS system's cost significantly lower than traditional DCS system. The paper successfully developed a prototype system based on RS-485 bus for high precision temperature control system of heat treatment, and based on the .net and AIBUS protocol, developed it' s system management software. This system have the characteristics of good real-time, high control-precision, high degree of automation, and friendly human-machine interface."
106,unknown,10.23919/aeit.2018.8577226,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8577226/,10/5/2018 0:00,smart farms for a sustainable and optimized model of agriculture,"Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The `smart farm' model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business."
107,unknown,10.1016/j.elerap.2021.101098,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85119699964,11/1/2021,an intelligent knowledge-based chatbot for customer service,"This study proposes an intelligent knowledge-based conversational agent system architecture to support customer services in e-commerce sales and marketing. A pilot implementation of a chatbot for customer services is reported in a leading women’s intimate apparel manufacturing firm. The proposed system incorporates various emerging technologies, including web crawling, natural language processing, knowledge bases, and artificial intelligence. In this study, a prototype system is built in a real-world setting. The results of the system prototype evaluation are satisfactory and support the contention that the system is effective. The study also discusses the challenges and lessons learned during system implementation and the theoretical and managerial implications of this study."
108,unknown,10.1016/j.asoc.2021.107465,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105315919,9/1/2021,click-event sound detection in automotive industry using machine/deep learning,"In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25,000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from 
                        
                           −
                           16
                           .
                           67
                        
                      dB to 
                        
                           −
                           12
                           .
                           87
                        
                      dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 
                        
                           94
                           .
                           55
                           ±
                           0
                           .
                           83
                        
                      %. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations."
109,included,10.1016/j.jmsy.2021.04.005,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85106283308,7/1/2021,learningadd: machine learning based acoustic defect detection in factory automation,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability."
110,included,10.1016/j.enconman.2021.113856,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85101129959,4/1/2021,the mutual benefits of renewables and carbon capture: achieved by an artificial intelligent scheduling strategy,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system."
111,unknown,10.1016/j.procs.2021.08.095,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85116946450,1/1/2021,soda: a real-time simulation framework for object detection and analysis in smart manufacturing,"For modern manufacturing firms, automation has already become a norm but constantly needs to be improved as firms still face strong demand to increase their productivity. This can be achieved by reducing dependability on manpower, reaching lean and even unmanned production and this is where some of the standards of Industry 4.0 come in useful, not to mention: Machine Vision, Image Recognition or Machine Learning. In our paper, we present SODA – our approach to build a flexible ML and AI enabled framework for object detection, analysis, and simulation. The framework is designed to support a development process of solutions requiring real-time analysis of images of different types of moving objects on a conveyor belt. In our work we discuss architectural challenges of the developed framework as well as the basic components of the system. We do also provide information on how to use the framework and present a sample implementation of an actual system employing some of the machine learning methods."
112,unknown,10.1016/j.compind.2020.103329,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85092922057,12/1/2020,a middleware platform for intelligent automation: an industrial prototype implementation,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms."
113,unknown,10.1016/j.compind.2020.103244,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85084401966,9/1/2020,machine learning for predictive scheduling and resource allocation in large scale manufacturing systems,"The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud.
                  Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data."
114,included,10.1016/j.adhoc.2019.102047,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076174369,3/1/2020,an intelligent edge-iot platform for monitoring livestock and crops in a dairy farming scenario,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud."
115,unknown,10.1016/j.procs.2020.03.044,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85085566175,1/1/2020,an artificial intelligence based crowdsensing solution for on-demand accident scene monitoring,"Road traffic crashes have a devastating impact on societies by claiming more than 1.35 million lives each year and causing up to 50 million injuries. Improving the efficiency of emergency management systems constitutes a key measure to reduce road traffic deaths and injuries. In this work, we propose a comprehensive crowdsensing-based solution for the real-time collection and the analysis of accident scene intelligence as a means to improve the efficiency of the emergency response process and help reduce road fatalities. The solution leverages sensory, mobile, and web technologies for the real-time monitoring of accident scenes, and employs Artificial Intelligence for the automatic analysis of the accident scene data, to allow the automatic generation of accident intelligence reports. Police officers and rescue teams can use those reports for fast and accurate situational assessment and effective response to emergencies. The proposed system was fully implemented and its operation was successfully tested using a variety of scenarios. This work gives interesting insights into the possibility of leveraging crowdsensing and artificial intelligence for offering emergency situational awareness and improving the efficiency of emergency response operations."
116,unknown,10.1016/j.aei.2020.101044,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85078852726,1/1/2020,iot edge computing-enabled collaborative tracking system for manufacturing resources in industrial park,"In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability."
117,unknown,10.1016/j.aei.2019.101013,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85075778987,1/1/2020,guidelines for applied machine learning in construction industry—a case of profit margins estimation,"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects."
118,unknown,10.1016/j.cie.2019.106031,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85071975175,11/1/2019,machine learning based concept drift detection for predictive maintenance,"In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope."
119,unknown,10.1016/j.compind.2019.04.010,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065732680,8/1/2019,managing workflow of customer requirements using machine learning,"Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored.
                  The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier.
                  The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system.
                  Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow."
120,unknown,10.1016/j.promfg.2018.12.026,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85072561400,1/1/2019,hybrid artificial intelligence system for the design of highly-automated production systems,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype."
121,unknown,10.1016/j.procir.2019.02.101,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065424368,1/1/2019,autonomous order dispatching in the semiconductor industry using reinforcement learning,"Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system."
122,unknown,10.1016/j.procir.2018.03.022,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85049587790,1/1/2018,fostering robust human-robot collaboration through ai task planning,"Recent advances in Artificial Intelligence (AI) are facilitating the deployment of intelligent systems in manufacturing. In Human-Robot Collaboration (HRC), industrial robots offer accuracy and efficiency while humans guarantee both experience and specialized and not replaceable skills. The seamless coordination of such different abilities constitutes one of the current challenges. This paper presents a dynamic task sequencing system for robust HRC developed within a EU-funded project. The proposed solution uses AI techniques to deal with the temporal variance entailed by the active presence of humans as well as to dynamically adapt task plans according to actual behavior of the pair human-worker/robot. The tool has been deployed in a real pilot plant."
123,unknown,http://arxiv.org/abs/2104.04076v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2104.04076v1,4/1/2021 0:00,"an artificial intelligence and internet of things based automated
  irrigation system","It is not hard to see that the need for clean water is growing by considering
the decrease of the water sources day by day in the world. Potable fresh water
is also used for irrigation, so it should be planned to decrease freshwater
wastage. With the development of technology and the availability of cheaper and
more effective solutions, the efficiency of irrigation increased and the water
loss can be reduced. In particular, Internet of things (IoT) devices has begun
to be used in all areas. We can easily and precisely collect temperature,
humidity and mineral values from the irrigation field with the IoT devices and
sensors. Most of the operations and decisions about irrigation are carried out
by people. For people, it is hard to have all the real-time data such as
temperature, moisture and mineral levels in the decision-making process and
make decisions by considering them. People usually make decisions with their
experience. In this study, a wide range of information from the irrigation
field was obtained by using IoT devices and sensors. Data collected from IoT
devices and sensors sent via communication channels and stored on MongoDB. With
the help of Weka software, the data was normalized and the normalized data was
used as a learning set. As a result of the examinations, a decision tree (J48)
algorithm with the highest accuracy was chosen and an artificial intelligence
model was created. Decisions are used to manage operations such as starting,
maintaining and stopping the irrigation. The accuracy of the decisions was
evaluated and the irrigation system was tested with the results. There are
options to manage, view the system remotely and manually and also see the
system s decisions with the created mobile application."
124,unknown,http://arxiv.org/abs/2103.13997v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.13997v1,3/25/2021 0:00,real-time low-resource phoneme recognition on edge devices,"While speech recognition has seen a surge in interest and research over the
last decade, most machine learning models for speech recognition either require
large training datasets or lots of storage and memory. Combined with the
prominence of English as the number one language in which audio data is
available, this means most other languages currently lack good speech
recognition models.
  The method presented in this paper shows how to create and train models for
speech recognition in any language which are not only highly accurate, but also
require very little storage, memory and training data when compared with
traditional models. This allows training models to recognize any language and
deploying them on edge devices such as mobile phones or car displays for fast
real-time speech recognition."
125,unknown,http://arxiv.org/abs/2102.12165v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.12165v1,2/24/2021 0:00,"efficient low-latency dynamic licensing for deep neural network
  deployment on edge devices","Along with the rapid development in the field of artificial intelligence,
especially deep learning, deep neural network applications are becoming more
and more popular in reality. To be able to withstand the heavy load from
mainstream users, deployment techniques are essential in bringing neural
network models from research to production. Among the two popular computing
topologies for deploying neural network models in production are
cloud-computing and edge-computing. Recent advances in communication
technologies, along with the great increase in the number of mobile devices,
has made edge-computing gradually become an inevitable trend. In this paper, we
propose an architecture to solve deploying and processing deep neural networks
on edge-devices by leveraging their synergy with the cloud and the
access-control mechanisms of the database. Adopting this architecture allows
low-latency DNN model updates on devices. At the same time, with only one model
deployed, we can easily make different versions of it by setting access
permissions on the model weights. This method allows for dynamic model
licensing, which benefits commercial applications."
126,unknown,http://arxiv.org/abs/2102.02638v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.02638v1,2/2/2021 0:00,"autodidactic neurosurgeon: collaborative deep inference for mobile edge
  intelligence via online learning","Recent breakthroughs in deep learning (DL) have led to the emergence of many
intelligent mobile applications and services, but in the meanwhile also pose
unprecedented computing challenges on resource-constrained mobile devices. This
paper builds a collaborative deep inference system between a
resource-constrained mobile device and a powerful edge server, aiming at
joining the power of both on-device processing and computation offloading. The
basic idea of this system is to partition a deep neural network (DNN) into a
front-end part running on the mobile device and a back-end part running on the
edge server, with the key challenge being how to locate the optimal partition
point to minimize the end-to-end inference delay. Unlike existing efforts on
DNN partitioning that rely heavily on a dedicated offline profiling stage to
search for the optimal partition point, our system has a built-in online
learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn
the optimal partition point on-the-fly. Therefore, ANS is able to closely
follow the changes of the system environment by generating new knowledge for
adaptive decision making. The core of ANS is a novel contextual bandit learning
algorithm, called $\mu$LinUCB, which not only has provable theoretical learning
performance guarantee but also is ultra-lightweight for easy real-world
implementation. We implement our system on a video stream object detection
testbed to validate the design of ANS and evaluate its performance. The
experiments show that ANS significantly outperforms state-of-the-art benchmarks
in terms of tracking system changes and reducing the end-to-end inference
delay."
127,included,http://arxiv.org/abs/2101.04930v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.04930v2,1/13/2021 0:00,"an empirical study on deployment faults of deep learning based mobile
  applications","Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices."
128,included,http://arxiv.org/abs/2008.05255v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2008.05255v1,8/12/2020 0:00,"identity-aware attribute recognition via real-time distributed inference
  in mobile edge clouds","With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods."
129,unknown,http://arxiv.org/abs/2002.11045v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2002.11045v1,2/22/2020 0:00,"deep learning for ultra-reliable and low-latency communications in 6g
  networks","In the future 6th generation networks, ultra-reliable and low-latency
communications (URLLC) will lay the foundation for emerging mission-critical
applications that have stringent requirements on end-to-end delay and
reliability. Existing works on URLLC are mainly based on theoretical models and
assumptions. The model-based solutions provide useful insights, but cannot be
directly implemented in practice. In this article, we first summarize how to
apply data-driven supervised deep learning and deep reinforcement learning in
URLLC, and discuss some open problems of these methods. To address these open
problems, we develop a multi-level architecture that enables device
intelligence, edge intelligence, and cloud intelligence for URLLC. The basic
idea is to merge theoretical models and real-world data in analyzing the
latency and reliability and training deep neural networks (DNNs). Deep transfer
learning is adopted in the architecture to fine-tune the pre-trained DNNs in
non-stationary networks. Further considering that the computing capacity at
each user and each mobile edge computing server is limited, federated learning
is applied to improve the learning efficiency. Finally, we provide some
experimental and simulation results and discuss some future directions."
130,unknown,http://arxiv.org/abs/1901.04985v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1901.04985v1,1/12/2019 0:00,"nnstreamer: stream processing paradigm for neural networks, toward
  efficient development and execution of on-device ai applications","We propose nnstreamer, a software system that handles neural networks as
filters of stream pipelines, applying the stream processing paradigm to neural
network applications. A new trend with the wide-spread of deep neural network
applications is on-device AI; i.e., processing neural networks directly on
mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy
issues, data transmission costs, and operational costs signifies the need for
on-device AI especially when a huge number of devices with real-time data
processing are deployed. Nnstreamer efficiently handles neural networks with
complex data stream pipelines on devices, improving the overall performance
significantly with minimal efforts. Besides, nnstreamer simplifies the neural
network pipeline implementations and allows reusing off-shelf multimedia stream
filters directly; thus it reduces the developmental costs significantly.
Nnstreamer is already being deployed with a product releasing soon and is open
source software applicable to a wide range of hardware architectures and
software platforms."
131,unknown,http://arxiv.org/abs/1808.07647v4,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1808.07647v4,8/23/2018 0:00,"machine learning at the edge: a data-driven architecture with
  applications to 5g cellular networks","The fifth generation of cellular networks (5G) will rely on edge cloud
deployments to satisfy the ultra-low latency demand of future applications. In
this paper, we argue that such deployments can also be used to enable advanced
data-driven and Machine Learning (ML) applications in mobile networks. We
propose an edge-controller-based architecture for cellular networks and
evaluate its performance with real data from hundreds of base stations of a
major U.S. operator. In this regard, we will provide insights on how to
dynamically cluster and associate base stations and controllers, according to
the global mobility patterns of the users. Then, we will describe how the
controllers can be used to run ML algorithms to predict the number of users in
each base station, and a use case in which these predictions are exploited by a
higher-layer application to route vehicular traffic according to network Key
Performance Indicators (KPIs). We show that the prediction accuracy improves
when based on machine learning algorithms that rely on the controllers' view
and, consequently, on the spatial correlation introduced by the user mobility,
with respect to when the prediction is based only on the local data of each
single base station."
132,unknown,http://arxiv.org/abs/1806.07761v3,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1806.07761v3,6/20/2018 0:00,"quick and plenty: achieving low delay and high rate in 802.11ac edge
  networks","We consider transport layer approaches for achieving high rate, low delay
communication over edge paths where the bottleneck is an 802.11ac WLAN. We
first show that by regulating send rate so as to maintain a target aggregation
level it is possible to realise high rate, low delay communication over
802.11ac WLANs. We then address two important practical issues arising in
production networks, namely that (i) many client devices are non-rooted mobile
handsets/tablets and (ii) the bottleneck may lie in the backhaul rather than
the WLAN, or indeed vary between the two over time. We show that both these
issues can be resolved by use of simple and robust machine learning techniques.
We present a prototype transport layer implementation of our low delay rate
allocation approach and use this to evaluate performance under real radio
conditions."
133,unknown,http://arxiv.org/abs/1604.04384v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1604.04384v2,4/15/2016 0:00,the strands project: long-term autonomy in everyday environments,"Thanks to the efforts of the robotics and autonomous systems community,
robots are becoming ever more capable. There is also an increasing demand from
end-users for autonomous service robots that can operate in real environments
for extended periods. In the STRANDS project we are tackling this demand
head-on by integrating state-of-the-art artificial intelligence and robotics
research into mobile service robots, and deploying these systems for long-term
installations in security and care environments. Over four deployments, our
robots have been operational for a combined duration of 104 days autonomously
performing end-user defined tasks, covering 116km in the process. In this
article we describe the approach we have used to enable long-term autonomous
operation in everyday environments, and how our robots are able to use their
long run times to improve their own performance."
134,included,10.1007/978-3-030-28925-6_1,Springer,springer,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-28925-6_1,1/1/2020 0:00,cityflow: supporting spatial-temporal edge computing for urban machine learning applications,"A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities."
135,unknown,10.1109/access.2020.3010609,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9144582/,1/1/2020 0:00,a privacy-aware crowd management system for smart cities and smart buildings,"Cities are growing at a dizzying pace and they require improved methods to manage crowded areas. Crowd management stands for the decisions and actions taken to supervise and control densely populated spaces and it involves multiple challenges, from recognition and assessment to application of actions tailored to the current situation. To that end, Wi-Fi-based monitoring systems have emerged as a cost-effective solution for the former one. The key challenge that they impose is the requirement to handle large datasets and provide results in near real-time basis. However, traditional big data and event processing approaches have important shortcomings while dealing with crowd management information. In this paper, we describe a novel system architecture for real-time crowd recognition for smart cities and smart buildings that can be easily replicated. The described system proposes a privacy-aware platform that enables the application of artificial intelligence mechanisms to assess crowds' behavior in buildings employing sensed Wi-Fi traces. Furthermore, the present paper shows the implementation of the system in two buildings, an airport and a market, as well as the results of applying a set of classification algorithms to provide crowd management information."
136,unknown,10.1109/atc52653.2021.9598291,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9598291/,10/16/2021 0:00,an edge-ai heterogeneous solution for real-time parking occupancy detection,"In the digital era, building smart cities is a highly desired goal that every country strives to achieve. With the advancement of technology, many smart city systems have been developed at a rapid rate of which Smart Parking is emerging as one of the core components. Smart Parking promises to automate the parking process, thereby saving time, resources and effort for searching an optimal parking space as well as reducing traffic congestion and population. As one of the newly emerging and disrupting technology, Artificial Intelligence, Machine Learning and Deep Learning (AI/ML/DL) are being utilized in many aspects of developing a Smart Parking system. In this paper, we propose an solution for accelerating AI/ML/DL algorithms deployed on low-cost System-on-Chip platforms (SoCs), which are often used as edge devices in Smart Parking system. In particular, we leverage Binary Neural Network (BNN), one of the most advanced deep learning models, to build a heterogeneous algorithm for real-time identifying parking occupancy based on the integration of SoCs and existing surveillance systems. The proposed solution is implemented and evaluated in Zynq UltraScale+ MPSoC with high accuracy (approx. 87%), low latency (avg. 16ms) and high frame per second (FPS) rate."
137,unknown,10.1109/iotais.2018.8600904,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8600904/,11/3/2018 0:00,smart air quality monitoring system with lorawan,"Nowadays, cities all over the globe are transforming into smart cities. Smart cities initiatives need to address environmental concerns such as air pollution to provide clean air. A scalable and cost-effective air monitoring system is imperative to monitor and control air pollution for smart city development. Air pollution has notable effects on the well-being of the population a whole, global atmosphere, and worldwide economy. This paper presents a scalable smart air quality monitoring system with low-cost sensors and long-range communication protocol. The sensors collect four parameters, temperature, humidity, dust and carbon dioxide in the air. The proposed end-to-end system has been implemented and deployed in Yangon, the business capital of Myanmar, as a case study since Jun 2018. The system allows the users to log in to an online dashboard to monitor the real-time status. In addition, based the collected air quality parameters for the past two months, a machine learning model has been trained to make predictions of parameters such that proactive actions can be taken to alleviate the impacts from air pollution."
138,unknown,10.1109/isc2.2016.7580869,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7580869/,9/15/2016 0:00,urbansense: an urban-scale sensing platform for the internet of things,"A critical step towards smarter and safer cities is to endow them with the abilities to massively gather a wide variety of data sets and to automatically feed those data to decision support tools and applications that leverage artificial intelligence. We present UrbanSense, a platform deployed on the streets of a mid-size European city (Porto, Portugal) to collect key environmental data. The main innovations of UrbanSense are (1) design for affordability and extensibility, (2) its ability to leverage heterogeneous networks to send the data to the cloud (using both real-time and delay-tolerant communications), and (3) its Internet of Things integration to expose the data streams to smart city tools and applications. Beyond discussing the design choices, we present operational results for 6 months of operation and give a detailed account of the challenges faced by the successful deployment of urban sensing technologies in the wild."
139,unknown,10.1109/tenconspring.2017.8070078,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8070078/,7/16/2017 0:00,identifying uncollected garbage in urban areas using crowdsourcing and machine learning,"Waste management in Urban Cities of India is a serious concern with growing amounts of uncollected garbage on the streets. In this paper, we present a mobile application based solution to empower the citizens to report instances of uncollected garbage and draw the attention of authorities. Our application has been successfully deployed and has seen more than a million complaints registered across many Indian cities. One of the challenges we have had to address on deployment of the application was the presence of a number of spurious complaints, such complaints result in unnecessary work by the municipal authorities. We tackled the challenge by using machine learning techniques to identify spurious complaints. We have been able to achieve an accuracy of over 85% in segregating the spurious complaints from actual ones by analyzing the image uploaded along with the complaint. The application can be used in real time once the model is trained."
140,unknown,10.1109/vtcfall.2017.8288311,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8288311/,9/27/2017 0:00,towards an application for real-time travel mode detection in urban centers,"Context-aware applications in intelligent transportation systems have a growing need for travel mode detection systems. However, few applications allow real-time travel mode detection through the use of smartphones. In this paper, we propose a real-time travel mode detection application based on GPS traces using a data mining technique through which these traces are preprocessed, grouped in motion segments and classified by supervised machine learning algorithms. An application prototype was implemented on the Android platform, used by smartphones, for movement data collection and user travel mode detection using the WEKA API in Java. Finally, to evaluate the performance of the application in a real environment, field tests were carried out with dozens of volunteers in the metropolitan area of Rio de Janeiro. Therefore, 1338 travel mode inferences were obtained by four machine learning techniques and the results were evaluated and compared by the indicators of the confusion matrix. Thus, through the performance evaluation carried out, it was possible to verify that the proposed application is useful for real-time travel mode detection in urban centers."
141,unknown,10.1109/access.2020.3015655,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9163328/,1/1/2020 0:00,a two-layer water demand prediction system in urban areas based on micro-services and lstm neural networks,"In recent years, scarce water resources became one of the main problems that endanger human species existence and the advancement of any nation. In this research, smart water meters were implemented, distributed, and installed in a regional area in Cairo while data were collected at uniform intervals then sent to the cloud instantly. The solution paradigm uses an Internet of Things (IoT) based on micro-services and containers. The design incorporates real-time streaming and infrastructure performance optimization to store data. A second layer to analyze the acquired data was used to model water consumption using Long Short-Term Memory (LSTM). The designed LSTM is validated and tested to be utilized in the forecast of future water demand. Moreover, two alternative machine learning methods, namely Support Vector Regression and Random Forest commonly utilized in time series forecasting applications, were used for a comparative analysis of which LSTM has proven to be superior. The proper integration of the system elements is the key to the proposed system success. Based on the success of the designed system, it can be applicable on a national scale. That can enable the optimal management of consumers' demand and improve water infrastructure utilization. The proposed paradigm presents a testbed for various scenarios that can be used in water resources management."
142,included,10.1109/icecce49384.2020.9179349,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9179349/,6/13/2020 0:00,a cloud based smart recycling bin for in-house waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations."
143,included,10.1109/mocast49295.2020.9200283,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9200283/,9/9/2020 0:00,a cloud based smart recycling bin for waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low."
144,unknown,10.1109/icac51239.2020.9357161,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9357161/,12/11/2020 0:00,deepfake audio detection: a deep learning based solution for group conversations,"The recent advancements in deep learning and other related technologies have led to improvements in various areas such as computer vision, bio-informatics, and speech recognition etc. This research mainly focuses on a problem with synthetic speech and speaker diarization. The developments in audio have resulted in deep learning models capable of replicating natural-sounding voice also known as text-to-speech (TTS) systems. This technology could be manipulated for malicious purposes such as deepfakes, impersonation, or spoofing attacks. We propose a system that has the capability of distinguishing between real and synthetic speech in group conversations.We built Deep Neural Network models and integrated them into a single solution using different datasets, including but not limited to Urban-Sound8K (5.6GB), Conversational (12.2GB), AMI-Corpus (5GB), and FakeOrReal (4GB). Our proposed approach consists of four main components. The speech-denoising component cleans and preprocesses the audio using Multilayer- Perceptron and Convolutional Neural Network architectures, with 93% and 94% accuracies accordingly. The speaker diarization was implemented using two different approaches, Natural Language Processing for text conversion with 93% accuracy and Recurrent Neural Network model for speaker labeling with 80% accuracy and 0.52 Diarization-Error-Rate. The final component distinguishes between real and fake audio using a CNN architecture with 94 % accuracy. With these findings, this research will contribute immensely to the domain of speech analysis."
145,unknown,10.1109/tensymp52854.2021.9550904,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9550904/,8/25/2021 0:00,deep learning based smart parking for a metropolitan area,"In this study, we have introduced a method for utilizing the maximum parking space available for a metropolitan city. This will result in much lesser traffic congestion due to street-side parking. Furthermore, it will also decrease the hassle drivers face when they have to leave their vehicles on the side of the road to do other activities. The method introduces a Deep Learning based system where parking spaces are detected using Data Capturing Units (DCU). These DCUs feed data into our database which can be accessed by the users from our mobile application. The users can book parking spaces accordingly. All these data are saved in real-time and can be accessed through the mobile application. A vehicle classification system has also been designed that achieves an accuracy of 77% from multiple vehicle classes. Furthermore, a number plate recognition system has been used for the identification and safety protocols of the vehicles in parking sites. The number plate identification system is very precise and achieves an accuracy of over 90% for each digit. To the best of our knowledge, no other system of this kind has been implemented for the city of Dhaka before this. On top of that, successful implementation in a hectic city like Dhaka implies that it can be applied anywhere in the world. We believe this system can have a huge impact in reducing traffic congestions and can save an endless measure of time and money for citizens in a metropolitan area."
146,unknown,10.1109/icws.2017.76,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8029817/,6/30/2017 0:00,early air pollution forecasting as a service: an ensemble learning approach,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system."
147,included,10.1109/vlsid51830.2021.00035,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9407373/,2/24/2021 0:00,binary neural network based real time emotion detection on an edge computing device to detect passenger anomaly,"Passenger safety in public transportation especially while riding in the form of shared cabs, and taxis are often ignored, and not much preventive protocols are devised. In the connected mobility world, emotion recognition from facial expressions is a possibility, however a faster processing and edge computing device to derive anomaly state inferences will be apt for further notifying about the safety of the passenger. FPGA implementation is a viable approach to not only implement in the embedded system automotive electronics, but also accelerate the inference results, hence making it as an ideal real time candidate for passenger anomaly state identification. For the same, a real time emotion detection system using facial features was implemented on FPGA. A Binary Neural Network (BNN) feeded by Local Binary Pattern (LBP) output was designed towards the development of an improved and faster emotion recognition system. LBP is configured as a preprocessing step to extract facial features that is passed on to the BNN layer for successful inference. The preprocessing method utilizes Viola-Jones (VJ) algorithm to extract facial data while removing other background information from the image. The LBP-BNN network is modelled using Facial Expression 2013 (FER-2013) data set for training. The custom hardware accelerator or the overlay is synthesized and the designed IP is implemented on FPGA for the inference. Inference is done using the trained model on FPGA to enable faster classified results. Emotion detection using facial expressions is classified to six states namely: angry, disgust, fear, happy, sad, and surprise. The LBP-BNN network is implemented in FPGA, to realize a real time facial emotion recognition by capturing the image of a person from a web camera interfaced to the FPGA acting as edge computing inference device, with acceptable accuracy. The image processing based emotion detection design is highly suitable for other applications including tracking of emotions for movement disorder patients in hospitals."
148,included,10.1109/camad.2018.8515001,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8515001/,9/19/2018 0:00,uncertainty management for wearable iot wristband sensors using laplacian-based matrix completion,"Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems."
149,unknown,10.1016/j.procs.2021.01.245,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105671943,1/1/2021,water wise - a digital water solution for smart cities and water management entities,"Efficient water management of the urban water cycle is one of the current concerns with the increase of (peri) urban areas due to the population growth, economic development, and possibility of water scarcity due to climate change. To face the increase of the water demand it is imperative the creation of digital water solutions to provide a real-time monitoring, decision support system, to manage the water supply network efficiently and optimize the water-energy nexus. This paper presents a Water Wise System – W2S, results from a R&D project supported by an EU and Portuguese Government Grant. The paper provides a preliminary study of an architecture solution to Water Wise System software, focuses on the water challenges, present technology, digital water, IoT and the future of smart cities. The solution aims to support a paradigm shift in the management of water distribution networks, with predictive and analytical convergence supported in Machine Learning, Deep Learning and an integration with SCADA, GIS and EPANET."
150,unknown,10.1016/j.glt.2020.09.004,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85102077498,1/1/2020,development of an iot based real-time traffic monitoring system for city governance,"A significant amount of research work carried out on traffic management systems, but intelligent traffic monitoring is still an active research topic due to the emerging technologies such as the Internet of Things (IoT) and Artificial Intelligence (AI). The integration of these technologies will facilitate the techniques for better decision making and achieve urban growth. However, the existing traffic prediction methods mostly dedicated to highway and urban traffic management, and limited studies focused on collector roads and closed campuses. Besides, reaching out to the public, and establishing active connections to assist them in decision-making is challenging when the users are not equipped with any smart devices. This research proposes an IoT based system model to collect, process, and store real-time traffic data for such a scenario. The objective is to provide real-time traffic updates on traffic congestion and unusual traffic incidents through roadside message units and thereby improve mobility. These early-warning messages will help citizens to save their time, especially during peak hours. Also, the system broadcasts the traffic updates from the administrative authorities. A prototype is implemented to evaluate the feasibility of the model, and the results of the experiments show good accuracy in vehicle detection and a low relative error in road occupancy estimation. The study is part of the Omani-funded research project, investigating Real-Time Feedback for Adaptive Traffic Signals."
151,unknown,10.1016/j.procs.2020.09.009,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85093365315,1/1/2020,passenger bibo detection with iot support and machine learning techniques for intelligent transport systems,"The present article discusses the issue of automation of the CICO (Check-In/Check-Out) process for public transport fare collection systems, using modern tools forming part of the Internet of Things, such as Beacon and Smartphone. It describes the concept of an integrated passenger identification model applying machine learning technology in order to reduce or eliminate the risks associated with the incorrect classification of a smartphone user as a vehicle passenger. This will allow for the construction of an intelligent fare collection system, operating in the BIBO (Be-In/Be-Out) model, implementing the ""hands-free"" and ""pay-as-you-go"" approach. The article describes the architecture of the research environment, and the implementation of the elaborated model in the Bad.App4 proprietary solution. We also presented the complete process of concept verification under real-life conditions. Research results were described and supplemented with commentary."
152,unknown,10.1016/b978-0-12-814601-9.00020-1,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85072183737,11/16/2018,wearable systems for improving tourist experience,"In this chapter we present original approaches for the development of a smart audio-guide that adapts to the actions and interests of visitors of cultural heritage sites and exhibitions either in indoor or outdoor scenarios. The guide is capable of perceiving the context. It understands what the user is looking at, if he is moving or is inattentive (e.g. talking with someone), in order to provide relevant information at the appropriate timing. Automatic recognition of artworks is performed with different approaches depending on the scenario, i.e. indoor and outdoor. These approaches are, respectively, based on Convolutional Neural Network (CNN) and SIFT descriptors, performing, when appropriate, object localization and classification. The computer-vision system works in real time on the mobile device, exploiting also a fusion of audio and motion sensors. Configurable interfaces to ease interaction and fruition of multimedia insights are provided for both scenarios. The audio-guide has been deployed on a NVIDIA Jetson TX1 and a NVIDIA Shield Tablet K1, tested in a real world environment (Bargello Museum of Florence and the historical city center of Florence), and evaluated with regard to system usability."
153,unknown,10.1016/j.jnca.2017.09.001,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85032219805,2/1/2018,an intelligent power distribution service architecture using cloud computing and deep learning techniques,"Smart management of power consumption for green living is important for sustainable development. Existing approaches could not provide a complete solution for both smart monitoring of electricity consumption, and also intelligent processing of the collected data effectively. This paper presents a cloud-based intelligent power distribution service architecture, where an intelligent electricity box (IEB) is designed using Zigbee and Raspberry Pi, and a standard MQTT (Message Queuing Telemetry Transport) protocol is used to transfer monitored data to the backend Cloud computing infrastructure using open source software packages. The IEB provides cloud services of real-time electricity information checking, power consumption monitoring, and remote control of switches. The current and historical data are stored in HBase and analyzed using Long Short Term Memory (LSTM). Evaluations and practical usage show that our proposed solution is very efficient in terms of availability, performance, and the deep learning based approach has better prediction accuracy than that of both classical SVR based approach and the latest XGBoost approach."
154,unknown,10.1109/percomworkshops51409.2021.9431061,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e9201bea3bd785e962697f5c3ddd3a30348cbc48,1/1/2021 0:00,ultra-fast machine learning classifier execution on iot devices without sram consumption,"With the introduction of edge analytics, IoT devices are becoming smart and ready for AI applications. A few modern ML frameworks are focusing on the generation of small-size ML models (often in kBs) that can directly be flashed and executed on tiny IoT devices, particularly the embedded systems. Edge analytics eliminates expensive device-to-cloud communications, thereby producing intelligent devices that can perform energy-efficient real-time offline analytics. Any increase in the training data results in a linear increase in the size and space complexity of the trained ML models, making them unable to be deployed on IoT devices with limited memory. To alleviate the memory issue, a few studies have focused on optimizing and fine-tuning existing ML algorithms to reduce their complexity and size. However, such optimization is usually dependent on the nature of IoT data being trained. In this paper, we presented an approach that protects model quality without requiring any alteration to the existing ML algorithms. We propose SRAM-optimized implementation and efficient deployment of widely used standard/stable ML-frameworks classifier versions (e.g., from Python scikit-learn). Our initial evaluation results have demonstrated that ours is the most resource-friendly approach, having a very limited memory footprint while executing large and complex ML models on MCU-based IoT devices, and can perform ultra-fast classifications while consuming 0 bytes of SRAM. When we tested our approach by executing it on a variety of MCU-based devices, the majority of models ported and executed produced 1-4x times faster inference results in comparison with the models ported by the sklearn-porter, m2cgen, and emlearn libraries."
155,unknown,10.1109/isce.2018.8408911,2018 International Symposium on Consumer Technologies (ISCT),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/f6ff5cb091a3e27e67893744cd2995ad6fef3407,1/1/2018 0:00,low-cost real-time non-intrusive appliance identification and controlling through machine learning algorithm,"The existing power generation sources are unable to meet the hyper escalating electricity demand. Common solution is to install new generation plants to fulfill the electricity demand which it is not cost-effective. A cheap and effective solution is to monitor all the appliances running inside a building and to use them efficiently. Non-intrusive load monitoring (NILM) is one of the economical techniques to identify the appliances on the basis of their unique load signatures. In this paper, a machine learning based technique is presented to identify the devices for monitoring purposes. A low-cost hardware setup, called Appliance Identification and Management System (AIMS) is developed to identify and control the appliances remotely. The appliance identification algorithm is developed in Python and deployed on Raspberry Pi, coupled with Arduino. The hardware setup furnishes consumers with the real-time status of all home appliances on their smartphone and web server. Controlling module is also integrated with the identification hardware to provide smart access to consumer for the remote control of home appliances."
156,unknown,10.1109/jiot.2018.2828144,IEEE Internet of Things Journal,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/cc19ef2b057bd6a4c3137dc348d9e5adf866c1a7,1/1/2018 0:00,implemented iot-based self-learning home management system (shms) for singapore,"Internet of Things makes deployment of smart home concept easy and real. Smart home concept ensures residents to control, monitor, and manage their energy consumption without any wastage. This paper presents a self-learning home management system. In the proposed system, a home energy management system, demand side management system, and supply side management system were developed and integrated for real time operation of a smart home. This integrated system has some capabilities such as price forecasting, price clustering, and power alert system to enhance its functions. These enhancing capabilities were developed and implemented using computational and machine learning technologies. In order to validate the proposed system, real-time power consumption data was collected from a Singapore smart home and a realistic experimental case study was carried out. The case study has shown that the developed system has performed well and created energy awareness to the residents. This proposed system also displays its ability to customize the model for different types of environments compared to traditional smart home models."
157,unknown,10.3390/s21020405,Sensors,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/644b6322c111a291d46041366ec3436eebc329bc,1/1/2021 0:00,"dolars, a distributed on-line activity recognition system by means of heterogeneous sensors in real-life deployments—a case study in the smart lab of the university of almería","Activity Recognition (AR) is an active research topic focused on detecting human actions and behaviours in smart environments. In this work, we present the on-line activity recognition platform DOLARS (Distributed On-line Activity Recognition System) where data from heterogeneous sensors are evaluated in real time, including binary, wearable and location sensors. Different descriptors and metrics from the heterogeneous sensor data are integrated in a common feature vector whose extraction is developed by a sliding window approach under real-time conditions. DOLARS provides a distributed architecture where: (i) stages for processing data in AR are deployed in distributed nodes, (ii) temporal cache modules compute metrics which aggregate sensor data for computing feature vectors in an efficient way; (iii) publish-subscribe models are integrated both to spread data from sensors and orchestrate the nodes (communication and replication) for computing AR and (iv) machine learning algorithms are used to classify and recognize the activities. A successful case study of daily activities recognition developed in the Smart Lab of The University of Almería (UAL) is presented in this paper. Results present an encouraging performance in recognition of sequences of activities and show the need for distributed architectures to achieve real time recognition."
158,unknown,10.1109/ic3i44769.2018.9007294,2018 3rd International Conference on Contemporary Computing and Informatics (IC3I),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/db2783813c1071e47f55385454f13cc55dc8fb16,1/1/2018 0:00,iot based precision horticulture in north india,"Horticulture incorporates a major impact on economy of the country. It is a subdivision of agriculture which deals with plant gardening under controlled environment. Heap of analysis has been dole out in automating the irrigation system by using wireless device and mobile computing. Conjointly analysis has been done in applying machine learning in Horticultural system too. Machine to Machine (M2M) communication is a growing technology that permits devices, objects to speak among one another and send knowledge to Server or Cloud through the Core Network. Therefore, consequently we tend to have developed a Brainy IOT primarily based machine-controlled Irrigation system. Wherever device knowledge is touching wet soil, temperature is captured and consequently machine learning algorithmic is deployed for analysing the device knowledge for prediction towards irrigating the soil with water or switching ON/OFF fan to control temperature. This can be a totally machine-controlled wherever devices communicate among themselves and apply the intelligence in real time monitoring & analysis. Making data available online through Cloud to Scientists to remotely make smart decisions on Precision Horticulture. System has been developed using low value embedded devices like Raspberry Pi3, Arduino and successfully implemented in Delhi."
159,unknown,10.7717/peerj-cs.787,PeerJ Comput. Sci.,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/7c095c8b997e9419465656e34a656e800235d478,1/1/2021 0:00,detecting security attacks in cyber-physical systems: a comparison of mule and wso2 intelligent iot architectures,"The Internet of Things (IoT) paradigm keeps growing, and many different IoT devices, such as smartphones and smart appliances, are extensively used in smart industries and smart cities. The benefits of this paradigm are obvious, but these IoT environments have brought with them new challenges, such as detecting and combating cybersecurity attacks against cyber-physical systems. This paper addresses the real-time detection of security attacks in these IoT systems through the combined used of Machine Learning (ML) techniques and Complex Event Processing (CEP). In this regard, in the past we proposed an intelligent architecture that integrates ML with CEP, and which permits the definition of event patterns for the real-time detection of not only specific IoT security attacks, but also novel attacks that have not previously been defined. Our current concern, and the main objective of this paper, is to ensure that the architecture is not necessarily linked to specific vendor technologies and that it can be implemented with other vendor technologies while maintaining its correct functionality. We also set out to evaluate and compare the performance and benefits of alternative implementations. This is why the proposed architecture has been implemented by using technologies from different vendors: firstly, the Mule Enterprise Service Bus (ESB) together with the Esper CEP engine; and secondly, the WSO2 ESB with the Siddhi CEP engine. Both implementations have been tested in terms of performance and stress, and they are compared and discussed in this paper. The results obtained demonstrate that both implementations are suitable and effective, but also that there are notable differences between them: the Mule-based architecture is faster when the architecture makes use of two message broker topics and compares different types of events, while the WSO2-based one is faster when there is a single topic and one event type, and the system has a heavy workload."
160,unknown,10.3929/ethz-b-000347534,,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/478928b128cc8cd15515d348b644746bb0197b98,1/1/2018 0:00,"first analyses of rainfall patterns retrieved by a newly installed x-band radar over the metropolitan area of cagliari (sardinia, italy)","The growing urbanization and aggregation of metropolitan territorial communities, sustainable development, citizen engagement, economic and cultural attractiveness and governance are among the most important issues for modern cities. The increasing complexity of these problems and technological development are leading to an urgent need and the opportunity to radically rethink the way we build and manage our cities. The recent institution of the Metropolitan area of Cagliari, that counts more than 500 thousands inhabitants, stimulated the government of the Sardinian region to fund an innovative project (Tessuto Digitale Metropolitano), that will be developed jointly between the Center for advanced studies, research and development in Sardinia and the University of Cagliari. Specifically, the project aims at studying and developing innovative methods and technologies to offer new smart solutions to improve the attractiveness of the city, the management of resources and the safety and quality of life of citizens. These objectives can be pursued through the synergic use and experimentation of advanced communication infrastructures and widespread sensors, and the development of innovative vertical solutions. Among the others, the improvement of citizens’ safety against environmental risks is a priority objective, with a special regard to the development of monitoring and prediction systems of extreme precipitation events. In general, common characteristic of these phenomena is that their occurrence cannot be predicted with sufficient accuracy using traditional weather forecasting methods nor monitored by punctual traditional tipping buckets raingauges. This introduces the need for rainfall monitoring continuously in time and space, both to check their evolution in real time, and to dispose the necessary measures of civil protection. At the same time, the analysis of past observations, in terms of patterns and principal directions, allow to forecast (nowcasting) the occurrence of similar phenomena 30 minutes1hour ahead the rain hits the ground. Following these premises, the Department of Civil, Environmental Engineering and Architecture (DICAAR) of the University of Cagliari installed a weather radar (figure 1, left panel) over the towershaft elevator of a building of the Faculty of Engineering and Architecture, University of Cagliari (Lon 9.108720°, Lat 39.228991°). The radar is the SuperGauge model, produced by Envisens Technologies: it is an X-band radar characterized by a single elevation and single polarization, with 1 minute resolution in time and 60 m resolution in range. The radar can monitor an area within a radius of 30 km, with an azimuth resolution linearly increasing with distance up to 1500 m at the maximum distance (30 km). Hence, from the current position the instrument can monitor the whole Cagliari metropolitan area. Each scan is then processed to return the retrieved rainfall field in a regular grid with 60x60 meter grid-cells every minute. UrbanRain18 11 International Workshop on Precipitation in Urban Areas Fig 1: Left: Radar installation site. Right: Rainfall field observed during the 02-05-2018 event. The radar position was decided in order to limit electromagnetic interferences and minimize the ground clutter effects, which in turns are due to morphology and surrounding buildings. Initially, the radar was set with 0° elevation for the antenna. The first instrument run was during the rain events occurred throughout Sardinia at the beginning of May 2018, which showed high rainfall rates and precipitation volumes. Meteorological models correctly forecasted the storm occurrences and the civil protection issued several warnings of severe weather conditions; as a consequence, several damages were registered. A snapshot of the rainfall field as recorded by the radar during the event of 02-05-2018 is reported in figure 1 (right panel), showing some areas where the rainfall rate exceeds 40 mm/h. The comparison between the above observations and those collected by the National Radar Network supports the correct functioning of the instrument, at least in terms of registered rainfall patterns. Some adjustments and calibration are still needed: first, in order to minimize the ground clutter, hence improving the quality of the measurements, the elevation angle will be increased up to 3°. Second, rainfall observations inferred by the radar will be accurately adjusted taking the advantage of the Sardinia’s rain gauges network. When retrievals of other events will be collected and available, some nowcasting procedures will be implemented in order to use radar observations also to issue real time warnings. Traditional methods, based on cell tracking, area tracking, and stochastic algorithms will be compared to innovative methods, based on machine learning. Finally, radar and rain gauge data will be integrated with the sensor network envisaged by the abovementioned project, aimed at monitoring multiple environmental parameters (temperature, water level, wind speed, relative humidity). This complete data set will improve the forecast reliability, not only in terms of precipitation fields but also for many other quantities related to environmental security. Acknowledgments: This research was supported under the ROP Sardegna ERDF Action 1.2.2 (project “Tessuto Digitale Metropolitano”) and by Sardinian Regional Authorities."
161,included,http://arxiv.org/abs/2201.09550v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2201.09550v1,1/24/2022 0:00,crowd tracking and monitoring middleware via map-reduce,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques."
162,unknown,http://arxiv.org/abs/2103.11052v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.11052v1,3/19/2021 0:00,"a first step towards automated species recognition from camera trap
  images of mammals using ai in a european temperate forest","Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices."
163,included,http://arxiv.org/abs/2011.03630v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.03630v1,11/6/2020 0:00,"unmasking communication partners: a low-cost ai solution for digitally
  removing head-mounted displays in vr-based telepresence","Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions."
164,unknown,http://arxiv.org/abs/2010.00432v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2010.00432v1,10/1/2020 0:00,"the rfml ecosystem: a look at the unique challenges of applying deep
  learning to radio frequency applications","While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces."
165,included,http://arxiv.org/abs/2009.10679v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2009.10679v1,9/22/2020 0:00,"an embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames."
166,unknown,http://arxiv.org/abs/2004.05740v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2004.05740v2,4/13/2020 0:00,"deep-edge: an efficient framework for deep learning model update on
  heterogeneous edge","Deep Learning (DL) model-based AI services are increasingly offered in a
variety of predictive analytics services such as computer vision, natural
language processing, speech recognition. However, the quality of the DL models
can degrade over time due to changes in the input data distribution, thereby
requiring periodic model updates. Although cloud data-centers can meet the
computational requirements of the resource-intensive and time-consuming model
update task, transferring data from the edge devices to the cloud incurs a
significant cost in terms of network bandwidth and are prone to data privacy
issues. With the advent of GPU-enabled edge devices, the DL model update can be
performed at the edge in a distributed manner using multiple connected edge
devices. However, efficiently utilizing the edge resources for the model update
is a hard problem due to the heterogeneity among the edge devices and the
resource interference caused by the co-location of the DL model update task
with latency-critical tasks running in the background. To overcome these
challenges, we present Deep-Edge, a load- and interference-aware,
fault-tolerant resource management framework for performing model update at the
edge that uses distributed training. This paper makes the following
contributions. First, it provides a unified framework for monitoring,
profiling, and deploying the DL model update tasks on heterogeneous edge
devices. Second, it presents a scheduler that reduces the total re-training
time by appropriately selecting the edge devices and distributing data among
them such that no latency-critical applications experience deadline violations.
Finally, we present empirical results to validate the efficacy of the framework
using a real-world DL model update case-study based on the Caltech dataset and
an edge AI cluster testbed."
167,unknown,http://arxiv.org/abs/1909.06526v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.06526v1,9/14/2019 0:00,ffdl : a flexible multi-tenant deep learning platform,"Deep learning (DL) is becoming increasingly popular in several application
domains and has made several new application features involving computer
vision, speech recognition and synthesis, self-driving automobiles, drug
design, etc. feasible and accurate. As a result, large scale on-premise and
cloud-hosted deep learning platforms have become essential infrastructure in
many organizations. These systems accept, schedule, manage and execute DL
training jobs at scale.
  This paper describes the design, implementation and our experiences with
FfDL, a DL platform used at IBM. We describe how our design balances
dependability with scalability, elasticity, flexibility and efficiency. We
examine FfDL qualitatively through a retrospective look at the lessons learned
from building, operating, and supporting FfDL; and quantitatively through a
detailed empirical evaluation of FfDL, including the overheads introduced by
the platform for various deep learning models, the load and performance
observed in a real case study using FfDL within our organization, the frequency
of various faults observed including unanticipated faults, and experiments
demonstrating the benefits of various scheduling policies. FfDL has been
open-sourced."
168,unknown,http://arxiv.org/abs/1905.07082v6,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1905.07082v6,5/17/2019 0:00,"the audio auditor: user-level membership inference in internet of things
  voice services","With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy."
169,included,http://arxiv.org/abs/1804.09914v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1804.09914v1,4/26/2018 0:00,"itelescope: intelligent video telemetry and classification in real-time
  using software defined networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost."
170,included,http://arxiv.org/abs/1802.08960v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1802.08960v2,2/25/2018 0:00,"bonnet: an open-source training and deployment framework for semantic
  segmentation in robotics using cnns","The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos."
171,unknown,10.1109/access.2019.2926206,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8752358/,1/1/2019 0:00,a fusion-based framework for wireless multimedia sensor networks in surveillance applications,"Multimedia sensors enable monitoring applications to obtain more accurate and detailed information. However, the development of efficient and lightweight solutions for managing data traffic over wireless multimedia sensor networks (WMSNs) has become vital because of the excessive volume of data produced by multimedia sensors. As part of this motivation, this paper proposes a fusion-based WMSN framework that reduces the amount of data to be transmitted over the network by intra-node processing. This framework explores three main issues: (1) the design of a wireless multimedia sensor (WMS) node to detect objects using machine learning techniques; (2) a method for increasing the accuracy while reducing the amount of information transmitted by the WMS nodes to the base station, and; (3) a new cluster-based routing algorithm for the WMSNs that consumes less power than the currently used algorithms. In this context, a WMS node is designed and implemented using commercially available components. In order to reduce the amount of information to be transmitted to the base station and thereby extend the lifetime of a WMSN, a method for detecting and classifying objects on three different layers has been developed. A new energy-efficient cluster-based routing algorithm is developed to transfer the collected information/data to the sink. The proposed framework and the cluster-based routing algorithm are applied to our WMS nodes and tested experimentally. The results of the experiments clearly demonstrate the feasibility of the proposed WMSN architecture in the real-world surveillance applications."
172,unknown,10.1109/csci49370.2019.00077,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9070974/,12/7/2019 0:00,"embedded or integrated, autonomous intelligent monitoring architecture and framework for training assistance and automation","A key element to readiness lies in the ability to provide qualified trainers and role player personnel, a challenge in a budget constrained environment. The goal of this effort is to employ practical artificial intelligence (AI) and automation techniques to minimize staffing requirements for training, and to improve the quality and pace of training. This paper outlines implementation experiments and results of these techniques to interoperate multiple AI algorithm types in an architecture and framework to automate certain aspects of training systems. A core capability the ability to integrate into a variety of training systems, from embedded and virtual reality simulations to constructive wargames, via a user-friendly system of APIs."
173,included,10.1109/aivr.2018.00018,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8613637/,12/12/2018 0:00,a compensation method of two-stage image generation for human-ai collaborated in-situ fashion design in augmented reality environment,"In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real human computer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design."
174,unknown,10.1109/iwcmc55113.2022.9825089,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9825089/,6/3/2022 0:00,real-time application for recognition and visualization of arabic words with vowels based dl and ar,"Text is difficult to read in some cases due to text orientation, writing style, very light colors, etc. Visually impaired or visually impaired people have difficulty reading text in all of these situations. The architecture proposed in this work is intended to detect and identify Arabic characters with vowels in natural environments. This architecture can help visually impaired or blind people to read the text correctly. It allows users to read the text in a better and more immersive way by combining augmented reality with digital material. The approach uses both deep learning and more specifically the VGG 19 model and augmented reality to improve the efficiency, clarity, and accuracy of text reading. For text detection and identification we use the VGG 19 model, and for text visualization, we use augmented reality. The implementation technique presented in this research for an augmented reality interactive virtual assistant system is for users to use their smartphone&#x0027;s camera to receive enhanced text information via a text image and a three-dimensional image to understand the displayed text. It offers an interesting way to understand their environment. The use of augmented reality to better display recognized text in 3D is a fantastic feature. User research studies are conducted to assess usability and user satisfaction."
175,included,10.1109/aero50100.2021.9438232,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9438232/,3/13/2021 0:00,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations."
176,included,10.1109/icfec51620.2021.00018,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9458893/,5/13/2021 0:00,a privacy preserving system for ai-assisted video analytics,"The emerging Edge computing paradigm facilitates the deployment of distributed AI-applications and hardware, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits for parties in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, the widespread deployment of such mechanism in public areas are a growing cause of privacy and security concerns. Data protection strategies need to be appropriately designed and correctly implemented in order to mitigate the associated risks. Most existing approaches focus on privacy and security related operations of the video stream itself or protecting its transmission. In this paper, we propose a privacy preserving system for AI-assisted video analytics, that extracts relevant information from video data and governs the secure access to that information. The system ensures that applications leveraging extracted data have no access to the video stream. An attribute-based authorization scheme allows applications to only query a predefined subset of extracted data. We demonstrate the feasibility of our approach by evaluating an application motivated by the recent COVID-19 pandemic, deployed on typical edge computing infrastructure."
177,included,10.1109/cnna.2010.5430245,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5430245/,2/5/2010 0:00,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates."
178,included,10.1109/cbms.2019.00041,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8787393/,6/7/2019 0:00,action recognition in real homes using low resolution depth video data,"We report work in progress from interdisciplinary research on Assisted Living Technology in smart homes for older adults with mild cognitive impairments or dementia. We present our field trial, the set-up for collecting and storing data from real homes, and preliminary results on action recognition using low resolution depth video cameras. The data have been collected from seven apartments with one resident each over a period of two weeks. We propose a pre-processing of the depth videos by applying an Infinite Response Filter (IIR) for extracting the movements in the frames prior to classification. In this work we classify four actions: TV interaction (turn it on/ off and switch over), standing up, sitting down, and no movement. Our first results indicate that using the IIR filter for movement information extraction improves accuracy and can be an efficient method for recognizing actions. Our current implementation uses a convolutional long short-term memory (ConvLSTM) neural network, and achieved an average peak accuracy of 86%."
179,included,10.1145/3326285.3329051,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9068647/,6/25/2019 0:00,leap: learning-based smart edge with caching and prefetching for adaptive video streaming,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4&#x0025; and reduces video rebuffering by 42.7&#x0025;, which leads to at least 15.9&#x0025; improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP."
180,unknown,10.1109/ijcnn.2015.7280718,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7280718/,7/17/2015 0:00,real-time video object recognition using convolutional neural network,"A convolutional neural network (CNN) is implemented on a field-programmable gate array (FPGA) and used for recognizing objects in real-time video streams. In this system, an image pyramid is constructed by successively down-scaling the input video stream. Image blocks are extracted from the image pyramid and classified by the CNN core. The detected parts are then marked on the output video frames. The CNN core is composed of six hardware neurons and two receptor units. The hardware neurons are designed as fully-pipelined digital circuits synchronized with the system clock, and are used to compute the model neurons in a time-sharing manner. The receptor units scan the input image for local receptive fields and continuously supply data to the hardware neurons as inputs. The CNN core module is controlled according to the contents of a table describing the sequence of computational stages and containing the system parameters required to control each stage. The use of this table makes the hardware system more flexible, and various CNN configurations can be accommodated without re-designing the system. The system implemented on a mid-range FPGA achieves a computational speed greater than 170,000 classifications per second, and performs scale-invariant object recognition from a 720×480 video stream at a speed of 60 fps. This work is a part of a commercial project, and the system is targeted for recognizing any pre-trained objects with a small physical volume and low power consumption."
181,unknown,10.1109/icdcsw53096.2021.00009,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9545916/,7/10/2021 0:00,towards understanding the adaptation space of ai-assisted data protection for video analytics at the edge,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy."
182,included,10.1109/tnsm.2019.2929511,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8765778/,9/1/2019 0:00,itelescope: softwarized network middle-box for real-time video telemetry and classification,"Video continues to dominate network traffic, yet operators today have poor visibility into the number, duration, and resolutions of the video streams traversing their domain. Current monitoring approaches are inaccurate, expensive, or unscalable, as they rely on statistical sampling, middle-box hardware, or packet inspection software. We present iTelescope, the first intelligent, inexpensive, and scalable softwarized network middle-box solution for identifying and classifying video flows in realtime. Our solution is novel in combining dynamic flow rules with telemetry and machine learning, and is built on commodity OpenFlow switches and open-source software. We develop a fully functional system, train it in the lab using multiple machine learning algorithms, and validate its performance to show over 95% accuracy in identifying and classifying video streams from many providers, including YouTube and Netflix. Lastly, we conduct tests to demonstrate its scalability to tens of thousands of concurrent streams, and deploy it live on a campus network serving several hundred real users. Our traffic monitoring system gives unprecedented fine-grained real-time visibility of video streaming performance to operators of enterprise and carrier networks at very low cost."
183,included,10.1109/iria53009.2021.9588707,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9588707/,9/22/2021 0:00,automatic license plate recognition system using ssd,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95&#x0025;."
184,unknown,10.1109/3ict.2019.8910276,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8910276/,9/23/2019 0:00,ground operations management using a data governance dashboard,"An incident involving the use of chemical, biological, radiological, and nuclear (CBRN) materials might represent a significant challenge for crime scene investigators. The paper presents a full system architecture to assess the hazardous situations resulting from CBRN materials. This issue is crucial since there were a number of incidents that occurred in which forensics people could not reach the location either due to being unreachable or due to harmful emissions. The proposed solution integrates various inputs including data from sensors, video streaming, geo-data along with using Artificial Intelligence (AI) for good decision-making and data analysis. A geo-dashboard was also designed to demonstrate, in real-time, the collected data from several angles and according to various queries. It also monitors the performance in real-time. The topic is not new however the novelty of the proposed solution is the integration of multiple sources of data, applying deep neural nets and projecting the data and data analytics in real-time on a dashboard that displays the analysis and data from different perspectives considering the viewpoint of the individuals who will use that system. The paper also presents how the ROCSAFE multidisciplinary research project addresses the identified scenario. The project combines topics from robotics, sensor technology, analytical and situation awareness software, transforming data into knowledgeable insights to support the decision-making process."
185,unknown,10.1109/iccci50826.2021.9402701,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9402701/,1/29/2021 0:00,iot based two way safety enabled intelligent stove with age verification using machine learning,"Smart embedded systems have become a core component in the latest technologies, and IoT based smart embedded system is the trendiest field in the research area. In our research, we are proposing an IoT based smart stove. Any accident might occur at any time from a stove. So we are designing a two-way safety enabled stove with a child lock system and gas leakage detection feature. The intelligent stove will try to ensure safety and will detect age from real-time video streaming. Our main focus is a child would not be able to turn the stove on. As well as, the stove can entitle safety via gas detection alarm. We are using a Raspberry Pi and Gas Detection Module with a buzzer for the hardware implementation. Also, we are applying a Machine Learning object detection algorithm (Haar Cascade) and a deep learning architecture (CNN) for the system execution. Since our stove is IoT-based, the stove is ensuring safety remotely as well as manually which will try to prevent accidental occurrences."
186,included,10.1109/ic4me247184.2019.9036531,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9036531/,7/12/2019 0:00,object detection based security system using machine learning algorthim and raspberry pi,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally."
187,unknown,10.1109/csci.2017.81,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8560838/,12/16/2017 0:00,object movement detection by real-time deep learning for security surveillance camera,Developing a smart Web Video Player application connected to a security surveillance camera to keep track of the object of interest is an ongoing research. This paper presents a methodology to real time data mining of the sequence of frames from a live stream collected by security camera by processing trajectories of an object of interest. Two classifiers and a clustering method are implemented all working in real-time. Real-time Deep Learning and Support Vector Machines (SVM) machine learning algorithms are implemented on a local server without the use of cloud computing. This is a popular architecture for many buildings and industries who want to have an in-house smart security camera application.
188,unknown,10.1109/infocom41043.2020.9155467,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9155467/,7/9/2020 0:00,rldish: edge-assisted qoe optimization of http live streaming with reinforcement learning,"Recent years have seen a rapidly increasing traffic demand for HTTP-based high-quality live video streaming. The surging traffic demand, as well as the real-time property of live videos, make it challenging for content delivery networks (CDNs) to guarantee the Quality-of-Experiences (QoE) of viewers. The initial video segment (IVS) of live streaming plays an important role in the QoE of live viewers, particularly when users require fast join time and smooth view experience. State-of-the-art research on this regard estimates network throughput for each viewer and thus may incur a large overhead that offsets the benefit. To tackle the problem, we propose Rldish, a scheme deployed at the edge CDN server, to dynamically select a suitable IVS for new live viewers based on Reinforcement Learning (RL). Rldish is transparent to both the client and the streaming server. It collects the real-time QoE observations from the edge without any client-side assistance, then uses these QoE observations as real-time rewards in RL. We deploy Rldish as a virtualized network function (VNF) in a real HTTP cache server, and evaluate its performance using streaming servers distributed over the world. Our experiments show that Rldish improves the state- of-the-art IVS selection scheme w.r.t. the average QoE of live viewers by up to 22%."
189,unknown,10.1109/icnnb.2005.1614810,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1614810/,10/15/2005 0:00,security assurance using face recognition & detection system based on neural networks,"In this paper, we have proposed a new method of implementing an assurance system using the facial information of the people, this is a different approach to the conventional security system which uses biometric information or cryptography for assurance, here we use an efficient self-scaling face recognition system supported with a face detection system, the system is capable enough to extract the human faces from a real time video and to recognize the people using a face recognition system, we are designing the framework for face recognition system with a hybrid RBF neural network, the real advantage of the system lies in its capability to inculcate some basic features of the self organizing map (SOM) so that the system can scale on its own and it doesn't get outdated with time, for the face detection system we use a content based face detection algorithm, the facial feature so detected is inputted into the face recognition system, if the person's information is already present in the system, authentication can be accomplished, this system can be used in public places like airports and supermarkets, the information of criminals can be stored in the system and in case any of the criminals are detected by the system, the security personnel can be signaled, this system can also be implemented in robotics, the system can help the computer to identify individual users distinctly, our facial recognition system has been tested and found to be persistent in recognizing the individual even if the input facial image is of different gesture or holds some extra lineament like beard, moustache or spectacles so we can definitely state that the system is reliable and efficient"
190,unknown,10.1109/iccci50826.2021.9402589,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9402589/,1/29/2021 0:00,smart staff attendance system using convolutional neural network,"In this paper we have implemented Deep Learning model Convolutional Neural Network architecture for face detection to build a smart attendance system that will detect the faces of all the staff members and the attendance is marked automatically. This is a real time application which comes with day-to-day activities of handling attendance system in an institution. The process involves recognizing the face of staff members from the video taken through surveillance camera kept at different locations in the institution and other information technologies associated with the system. The proposed system will be able to find and recognize staff member faces fast and precisely with an accuracy of 90%. In addition, various data augmentation techniques are employed in the proposed system that improves the system accuracy further from 90% to 96%"
191,included,10.1109/tnsm.2021.3085097,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9444314/,9/1/2021 0:00,seta++: real-time scalable encrypted traffic analytics in multi-gbps networks,"The security and privacy of the end-users are a few of the most important components of a communication network. Though end-to-end encryption (e.g., TLS/SSL) fulfils this requirement, it makes inspecting network traffic with legacy solutions such as Deep Packet Inspection difficult. Recent Machine Learning techniques have shown outstanding performance in encrypted traffic classification. Nevertheless, such approaches require efficient flow sampling at real enterprise-scale networks due to the sheer volume of transferred data. Through this paper, we propose a holistic architecture to extract flow information of encrypted data at multi Gbps line rate using sampling and sketching mechanisms, enabling network operators to estimate flow size distribution accurately and understand the behavior of VPN-obfuscated traffic. Using over 6000 video traffic traces, under three main evaluation scenarios based on trace duration and starting time point, we show that it is possible to achieve 99% accuracy for service provider classification and over 90% accuracy for content classification for a given service provider in the best case. We also deploy our solution at an operational enterprise-scale network leveraging kernel bypassing to demonstrate its capability to efficiently sample live traffic for analytics."
192,included,10.1109/iceeccot46775.2019.9114716,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9114716/,12/14/2019 0:00,facial recognition using machine learning algorithms on raspberry pi,"Facial recognition is a non-invasive method of biometric authentication and useful for numerous applications. The real time implementation of the algorithm with adequate accuracy is required, with hardware timing into consideration. This paper deals with the implementation of machine learning algorithm for real time facial image recognition. Two dominant methods out of many facial recognition methods are discussed, simulated and implemented using Raspberry Pi. A rigorous comparative analysis is presented considering various limitations which may be the case required for innumerable application which utilize facial recognition. The drawbacks and different use cases of each method is highlighted. The facial recognition software uses algorithms to compare a digital image captured through a camera, to the stored face print so as to authenticate a person's identity. The Haar-Cascade method was one of the first methods developed for facial recognition. The HOG (Histogram of Oriented Gradients) method has worked very effectively for object recognition and thus suitable for facial recognition also. Both the methods are compared with Eigen feature-based face recognition algorithm. Various important features are experimented like speed of operation, lighting condition, frontal face profile, side profiles, distance of image, size of image etc. The facial recognition model is implemented to detect and recognize faces in real-time by means of Raspberry Pi and Pi camera for the user defined database in addition to the available databases."
193,unknown,10.1109/cds49703.2020.00012,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9275963/,8/2/2020 0:00,welding seam recognition robots based on edge computing,"In order to meet the requirements of the accuracy and real-time performance during the working process of underwater welding robots, a scheme of welding seam recognition robots system based on the edge computing is proposed in this paper. A number of pre-processing methods for capturing welding seam image were designed, including Thresholding, Filtering and Edge Detect. A Convolutional Neural Network(CNN) model for welding seam recognition was also created. In the experiments, the image pre-processing and CNN algorithms were integrated in and deployed to the robots, and the learning and training algorithms of the CNN were deployed to the cloud servers. The image pre-processing methods filtered the interference in underwater operations and achieved the image compression and feature extraction. The cloud servers fulfilled the training and parameter optimization of the CNN, which improved the accuracy of welding seam image recognition."
194,unknown,10.1109/ct.1997.617707,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/617707/,8/28/1997 0:00,the intelligent room project,"At the MIT Artificial Intelligence Laboratory, we have been working on technologies for an Intelligent Room. Rather than pull people into the virtual world of the computer, we are trying to pull the computer out into the real world of people. To do this, we are combining robotics and vision technology with speech understanding systems and agent-based architectures to provide ready-at-hand computation and information services for people engaged in day-to-day activities, both on their own and in conjunction with others. We have built a layered architecture where, at the bottom level, vision systems track people and identify their activities and gestures, and, through word spotting, decide whether people in the room are talking to each other or to the room itself. At the next level, an agent architecture provides a uniform interface to such specially-built systems, and to other off-the-shelf software, such as Web browsers, etc. At the highest level, we are able to build application systems that provide occupants of the room with specialized services; examples we have built include systems for command-and-control situations rooms and as a room for giving presentations."
195,unknown,10.1016/j.future.2020.06.017,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85087196994,11/1/2020,ppcensor: architecture for real-time pornography detection in video streaming,"Convolutional neural network (CNN) models are typically composed of several gigabytes of data, requiring dedicated hardware and significant processing capabilities for proper handling. In addition, video-detection tasks are typically performed offline, and each video frame is analyzed individually, meaning that the video’s categorization (class assignment) as normal or pornographic is only complete after all the video frames have been evaluated. This paper proposes the Private Parts Censor (PPCensor), a CNN-based architecture for transparent and near real-time detection and obfuscation of pornographic video frame regions. Our contribution is two-fold. First, the proposed architecture is the first that addresses the detection of pornographic content as an object detection problem. The objective is to apply user-friendly content filtering such that an inevitable false positive will obfuscate only regions (objects) within the video frames instead of blocking the entire video. Second, the PPCensor architecture is deployed on dedicated hardware, and real-time detection is deployed using a video-oriented streaming proxy. If a pornographic video frame is identified in the video, the system can hide pornographic content (private parts) in real time without user interaction or additional processing on the user’s device. Based on more than 50,000 objects labeled manually, the evaluation results show that the PPCensor is capable of detecting private parts in near real time for video streaming. Compared to cutting-edge CNN architectures for image classification, PPCensor achieved similar results, but operated in real time. In addition, when deployed on a desktop computer, PPCensor handled up to 35 simultaneous connections without the need for additional processing on the end-user device."
196,unknown,10.1016/j.isatra.2020.02.023,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85080060395,7/1/2020,strengthening the perception of the virtual worlds in a virtual reality environment,"Virtual reality is becoming more and more improved primarily due to numerous applications and the powers of mobile devices. Using various sensors, precise displays and high computing powers smartphone are becoming devices that make the boost in technology. Now it is necessary to efficiently use various sensors without affecting system operation and improve control abilities for various purposes. Especially in practical applications received by mass users such as games and any kind of experience. In this article, we propose a system that allows to extend the perception of the virtual world by conveying information about the user’s movements in reality into the supervised model. The system retrieves data from several sources, quickly analyzes them using artificial intelligence techniques, and returns information to the mobile phone about the activity that is being processed. The concept extends the understanding of today’s virtual reality by allowing the user to move and perform simple gestures in a specially designed room. Moreover, we propose multiplayer mode in virtual reality, where players are in different places. The proposed architecture of the system has been tested on simple applications, and the results show high potential for implementations in various apps by achieving almost 90% efficiency in changing player direction in real time and only 7.5% of collision cases."
197,unknown,10.1016/j.procs.2020.09.269,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85093364102,1/1/2020,"alexa, what classes do i have today? the use of artificial intelligence via smart speakers in education","Looking back to the rumours from the early 2000’s, when the world of technology bloomed together with the curiosity towards what was next to come, by 2020, robots should have assisted and supported almost every task from our daily life. While this may seem as a Sci-Fi movie scenario, it is partially a tangible reality, that we quickly got used to, thanks to the introduction of smart speakers.
                  As the world changes, so does the future of our students. In this respects, the evolution of the technology comes up with specific environments for educational purpose. Building smart learning environments supported by e-learning platforms is an important area of research in education domain within our days. The evolution of these smart learning environments is justified by some events (Covid19) that force students to learn remotely.
                  The paper proposes a software application component using Alexa smart speaker, that integrates different services (Amazon Web Services, Microsoft Services) for a proper virtual environment platform, for both students and teachers. It addresses the main concerns of the current educational system, and provides a smart solution through the use of Artificial Intelligence based tools. The proposed approach not only achieves unifying data and knowledge-share mechanisms in a remotely mode, but it brings also a good learning experience, increasing the effectiveness and the efficiency of the learning process."
198,unknown,10.11591/ijece.v12i1.pp331-338,'Institute of Advanced Engineering and Science',core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/478033462.pdf,2/1/2022 0:00,real-time traffic sign detection and recognition using raspberry pi,"Nowadays, the number of road accident in Malaysia is increasing expeditiously. One of the ways to reduce the number of road accident is through the development of the advanced driving assistance system (ADAS) by professional engineers. Several ADAS system has been proposed by taking into consideration the delay tolerance and the accuracy of the system itself. In this work, a traffic sign recognition system has been developed to increase the safety of the road users by installing the system inside the car for driver’s awareness. TensorFlow algorithm has been considered in this work for object recognition through machine learning due to its high accuracy. The algorithm is embedded in the Raspberry Pi 3 for processing and analysis to detect the traffic sign from the real-time video recording from Raspberry Pi camera NoIR. This work aims to study the accuracy, delay and reliability of the developed system using a Raspberry Pi 3 processor considering several scenarios related to the state of the environment and the condition of the traffic signs. A real-time testbed implementation has been conducted considering twenty different traffic signs and the results show that the system has more than 90% accuracy and is reliable with an acceptable delay"
199,unknown,http://arxiv.org/abs/2110.13041v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2110.13041v1,10/25/2021 0:00,applications and techniques for fast machine learning in science,"In this community review report, we discuss applications and techniques for
fast machine learning (ML) in science -- the concept of integrating power ML
methods into the real-time experimental data processing loop to accelerate
scientific discovery. The material for the report builds on two workshops held
by the Fast ML for Science community and covers three main areas: applications
for fast ML across a number of scientific domains; techniques for training and
implementing performant and resource-efficient ML algorithms; and computing
architectures, platforms, and technologies for deploying these algorithms. We
also present overlapping challenges across the multiple scientific domains
where common solutions can be found. This community report is intended to give
plenty of examples and inspiration for scientific discovery through integrated
and accelerated ML solutions. This is followed by a high-level overview and
organization of technical advances, including an abundance of pointers to
source material, which can enable these breakthroughs."
200,included,http://arxiv.org/abs/2004.05953v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2004.05953v1,4/13/2020 0:00,"software-defined network for end-to-end networked science at the
  exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence."
201,unknown,http://arxiv.org/abs/1909.08703v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.08703v1,9/18/2019 0:00,"deep complex networks for protocol-agnostic radio frequency device
  fingerprinting in the wild","Researchers have demonstrated various techniques for fingerprinting and
identifying devices. Previous approaches have identified devices from their
network traffic or transmitted signals while relying on software or operating
system specific artifacts (e.g., predictability of protocol header fields) or
characteristics of the underlying protocol (e.g.,frequency offset). As these
constraints can be a hindrance in real-world settings, we introduce a
practical, generalizable approach that offers significant operational value for
a variety of scenarios, including as an additional factor of authentication for
preventing impersonation attacks. Our goal is to identify artifacts in
transmitted signals that are caused by a device's unique hardware
""imperfections"" without any knowledge about the nature of the signal. We
develop RF-DCN, a novel Deep Complex-valued Neural Network (DCN) that operates
on raw RF signals and is completely agnostic of the underlying applications and
protocols. We present two DCN variations: (i) Convolutional DCN (CDCN) for
modeling full signals, and (ii) Recurrent DCN (RDCN) for modeling time series.
Our system handles raw I/Q data from open air captures within a given spectrum
window, without knowledge of the modulation scheme or even the carrier
frequencies. While our experiments demonstrate the effectiveness of our system,
especially under challenging conditions where other neural network
architectures break down, we identify additional challenges in signal-based
fingerprinting and provide guidelines for future explorations. Our work lays
the foundation for more research within this vast and challenging space by
establishing fundamental directions for using raw RF I/Q data in novel
complex-valued networks."
202,unknown,http://arxiv.org/abs/1907.12817v2,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1907.12817v2,7/30/2019 0:00,"increasing scalability of process mining using event dataframes: how
  data structure matters","Process Mining is a branch of Data Science that aims to extract
process-related information from event data contained in information systems,
that is steadily increasing in amount. Many algorithms, and a general-purpose
open source framework (ProM 6), have been developed in the last years for
process discovery, conformance checking, machine learning on event data.
However, in very few cases scalability has been a target, prioritizing the
quality of the output over the execution speed and the optimization of
resources. This is making progressively more difficult to apply process mining
with mainstream workstations on real-life event data with any open source
process mining framework. Hence, exploring more scalable storage techniques,
in-memory data structures, more performant algorithms is a strictly incumbent
need. In this paper, we propose the usage of mainstream columnar storages and
dataframes to increase the scalability of process mining. These can replace the
classic event log structures in most tasks, but require completely different
implementations with regards to mainstream process mining algorithms.
Dataframes will be defined, some algorithms on such structures will be
presented and their complexity will be calculated."
203,unknown,10.1109/fccm.2017.58,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7966655/,5/2/2017 0:00,accelerating large-scale graph analytics with fpga and hmc,"Graph analytics that explores the relationship among interconnected entities is becoming increasingly important due to its broad applicability from machine learning to social science. However, one major challenge for graph processing systems is the irregular data access pattern of graph computation which can significantly degrade the performance. The algorithms, software, and hardware that have been tailored for mainstream parallel applications are, as a result, generally not effective for massive-scale sparse graphs from the real world due to their complexity and irregularity. To address the performance issues in large-scale graph analytics, we combine the emerging Hybrid Memory Cube (HMC) with a modern FPGA in order to achieve exceptional random access performance without any loss of flexibility or efficiency in computation. In particular, we develop collaborative software/hardware techniques to perform a level-synchronized breadth first search (BFS) on the FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that fully exploits the platform's capability to improve data locality and memory access efficiency. For each input graph, this algorithm provides an efficient data layout that allows the FPGA to coalesce memory requests into the largest possible HMC payload requests so that the number of memory requests, which is the primary factor in runtime, can be minimized. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by adding a merging unit. The merging unit takes the best advantage of the increased data locality resulting from graph clustering. We evaluated the performance of our BFS implementation using the AC-510 development kit from Micron over a set of benchmarks from a wide range of applications. We observed that the combination of the clustering algorithm and the merging hardware achieved 2.8 × average performance improvement compared to the latest FPGA-HMC based graph processing system."
204,unknown,10.1109/fie44824.2020.9273981,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9273981/,10/24/2020 0:00,machine learning for middle-schoolers: children as designers of machine-learning apps,"This Research to Innovative Practice Full Paper presents a multidisciplinary, design-based research study that aims to develop and study pedagogical models and tools for integrating machine-learning (ML) topics into education. Although children grow up with ML systems, few theoretical or empirical studies have focused on investigating ML and data-driven design in K-12 education to date. This paper presents the theoretical grounds for a design-oriented pedagogy and the results from exploring and implementing those theoretical ideas in practice through a case study conducted in Finland. We describe the overall process in which middle-schoolers (N = 34) co-designed and made ML applications for solving meaningful, everyday problems. The qualitative content analysis of the pre-and post-tests, student interviews, and the students' own ML design ideas indicated that co-designing real-life applications lowered the barriers for participating in some of the core practices of computer science. It also supported children in exploring abstract ML concepts and workflows in a highly personalized and embodied way. The article concludes with a discussion on pedagogical insights for supporting middle-schoolers in becoming innovators and software designers in the age of ML."
205,included,10.1109/bigdata.2018.8621926,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8621926/,12/13/2018 0:00,harnessing the nature of spam in scalable online social spam detection,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management."
206,unknown,10.1109/trustcom50675.2020.00243,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9343128/,1/1/2021 0:00,monitoring social media for vulnerability-threat prediction and topic analysis,"Publicly available software vulnerabilities and exploit code are often abused by malicious actors to launch cyberattacks to vulnerable targets. Organizations not only have to update their software to the latest versions, but do effective patch management and prioritize security-related patching as well. In addition to intelligence sources such as Computer Emergency Response Team (CERT) alerts, cybersecurity news, national vulnerability database (NBD), and commercial cybersecurity vendors, social media is another valuable source that facilitates early stage intelligence gathering. To early detect future cyber threats based on publicly available resources on the Internet, we propose a dynamic vulnerability-threat assessment model to predict the tendency to be exploited for vulnerability entries listed in Common Vulnerability Exposures, and also to analyze social media contents such as Twitter to extract meaningful information. The model takes multiple aspects of vulnerabilities gathered from different sources into consideration. Features range from profile information to contextual information about these vulnerabilities. For the social media data, this study leverages machine learning techniques specially for Twitter which helps to filter out non-cybersecurity-related tweets and also label the topic categories of each tweet. When applied to predict the vulnerabilities exploitation and analyzed the real-world social media discussion data, it showed promising prediction accuracy with purified social media intelligence. Moreover, the AI-enabling modules have been deployed into a threat intelligence platform for further applications."
207,unknown,10.1109/asonam.2012.66,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6425738/,8/29/2012 0:00,semi-supervised policy recommendation for online social networks,"Fine grain policy settings in social network sites is becoming a very important requirement for managing user's privacy. Incorrect privacy policy settings can easily lead to leaks in private and personal information. At the same time, being too restrictive would reduce the benefits of online social networks. This is further complicated with the growing adoption of social networks and with the rapid growth in information uploading and sharing. The problem of facilitating policy settings has attracted numerous access control, and human computer interaction researchers. The solutions proposed range from usable interfaces for policy settings to automated policy settings. We propose a fine grained policy recommendation system that is based on an iterative semi-supervised learning approach that uses the social graph propagation properties. Active learning and social graph properties were used to detect the most informative instances to be labeled as training sets. We implemented and tested our approach using real Facebook dataset. We compared our proposed approach to supervised learning and random walk approaches. Our proposed approaches provided high accuracy and precision when compared to the other approaches."
208,included,10.1109/icitr51448.2020.9310890,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9310890/,12/4/2020 0:00,hybrid approach and architecture to detect fake news on twitter in real-time using neural networks,"Fake news has been a key issue since the dawn of social media. Currently, we are at a stage where it is merely impossible to differentiate between real and fake news. This directly and indirectly affects people's decision patterns and makes us question the credibility of the news shared via social media platforms. Twitter is one of the leading social networks in the world by active users. There has been an exponential spread of fake news on Twitter in the recent past. In this paper, we will discuss the implementation of a browser extension which will identify fake news on Twitter using deep learning models with a focus on real-world applicability, architectural stability and scalability of such a solution. Experimental results show that the proposed browser extension has an accuracy of 86% accuracy in fake news detection. To the best of our knowledge, our work is the first of its kind to detect fake news on Twitter real-time using a hybrid approach and evaluate using real users."
209,unknown,10.1109/icmla.2015.152,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7424435/,12/11/2015 0:00,mlaas: machine learning as a service,"The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time."
210,included,10.1109/tifs.2021.3131026,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9627681/,1/1/2022 0:00,poligraph: intrusion-tolerant and distributed fake news detection system,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally ( ${4\%}$ – ${7\%}$ ) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach."
211,unknown,10.1109/isrcs.2013.6623773,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6623773/,8/15/2013 0:00,scalable machine learning framework for behavior-based access control,"Today's activities in cyber space are more connected than ever before, driven by the ability to dynamically interact and share information with a changing set of partners over a wide variety of networks. The success of approaches aimed at securing the infrastructure has changed the threat profile to point where the biggest threat to the US cyber infrastructure is posed by targeted cyber attacks. The Behavior-Based Access Control (BBAC) effort has been investigating means to increase resilience against these attacks. Using statistical machine learning, BBAC (a) analyzes behaviors of insiders pursuing targeted attacks and (b) assesses trustworthiness of information to support real-time decision making about information sharing. The scope of this paper is to describe the challenge of processing disparate cyber security information at scale, together with an architecture and work-in-progress prototype implementation for a cloud framework supporting a strategic combination of stream and batch processing."
212,unknown,10.1016/j.chemolab.2021.104329,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105292476,7/15/2021,a novel approach for water quality classification based on the integration of deep learning and feature extraction techniques,"Water quality monitoring plays a vital role in the protection of water resources, environmental management, and decision-making. Artificial intelligence (AI) based on machine learning techniques has been widely used to evaluate and classify water quality for the last two decades. However, traditional machine learning techniques face many limitations, the most important of which is the inability to apply these techniques with big data generated by smart water quality monitoring stations to improve the prediction. Real-time water quality monitoring with high accuracy and efficiency for intelligent water quality monitoring stations requires new and sophisticated techniques based on machine and deep learning techniques. For this purpose, we propose a novel approach based on the integration of deep learning and feature extraction techniques to improve water quality classification. In this paper, was chosen the Tilesdit dam in Bouira (Algeria) as a case study. Moreover, we implemented the advanced deep learning method - Long Short Term Memory Recurrent Neural Networks (LSTM RNNs) to construct an intelligent model for drinking water quality classification. Furthermore, principal component analysis (PCA), linear discriminant analysis (LDA) and independent component analysis (ICA) techniques were used for features extraction and data reduction from original features. Additionally, we used three methods of cross-validation and two methods of the out-of-sample test to estimate the performance of LSTM RNNs model. From the results we found that the integration of LSTM RNNs with LDA, and LSTM RNNs with ICA yields an accuracy of 99.72%, using Random-Holdout technique."
213,unknown,10.1016/j.neucom.2020.11.066,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85098065075,3/7/2021,bayesuites: an open web framework for massive bayesian networks focused on neuroscience,"BayeSuites is the first web framework for learning, visualizing, and interpreting Bayesian networks (BNs) that can scale to tens of thousands of nodes while providing fast and friendly user experience. All the necessary features that enable this are reviewed in this paper; these features include scalability, extensibility, interoperability, ease of use, and interpretability. Scalability is the key factor in learning and processing massive networks within reasonable time; for a maintainable software open to new functionalities, extensibility and interoperability are necessary. Ease of use and interpretability are fundamental aspects of model interpretation, fairly similar to the case of the recent explainable artificial intelligence trend. We present the capabilities of our proposed framework by highlighting a real example of a BN learned from genomic data obtained from Allen Institute for Brain Science. The extensibility properties of the software are also demonstrated with the help of our BN-based probabilistic clustering implementation, together with another genomic-data example."
214,unknown,10.1021/acs.accounts.0c00736,Accounts of chemical research,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/2efecdfa2c6d7459b2b941d714e24bd3e64e6bc5,1/1/2021 0:00,automated experimentation powers data science in chemistry.,"ConspectusData science has revolutionized chemical research and continues to break down barriers with new interdisciplinary studies. The introduction of computational models and machine learning (ML) algorithms in combination with automation and traditional experimental techniques has enabled scientific advancement across nearly every discipline of chemistry, from materials discovery, to process optimization, to synthesis planning. However, predictive tools powered by data science are only as good as their data sets and, currently, many of the data sets used to train models suffer from several limitations, including being sparse, limited in scope and requiring human curation. Likewise, computational data faces limitations in terms of accurate modeling of nonideal systems and can suffer from low translation fidelity from simulation to real conditions. The lack of diverse data and the need to be able to test it experimentally reduces both the accuracy and scope of the predictive models derived from data science. This Account contextualizes the need for more complex and diverse experimental data and highlights how the seamless integration of robotics, machine learning, and data-rich monitoring techniques can be used to access it with minimal human labor.We propose three broad categories of data in chemistry: data on fundamental properties, data on reaction outcomes, and data on reaction mechanics. We highlight flexible, automated platforms that can be deployed to acquire and leverage these data. The first platform combines solid- and liquid-dosing modules with computer vision to automate solubility screening, thereby gathering fundamental data that are necessary for almost every experimental design. Using computer vision offers the additional benefit of creating a visual record, which can be referenced and used to further interrogate and gain insight on the data collected. The second platform iteratively tests reaction variables proposed by a ML algorithm in a closed-loop fashion. Experimental data related to reaction outcomes are fed back into the algorithm to drive the discovery and optimization of new materials and chemical processes. The third platform uses automated process analytical technology to gather real-time data related to reaction kinetics. This system allows the researcher to directly interrogate the reaction mechanisms in granular detail to determine exactly how and why a reaction proceeds, thereby enabling reaction optimization and deployment."
215,unknown,10.1109/saci.2007.375494,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/4262496/,5/18/2007 0:00,fpga parallel implementation of cmac type neural network with on chip learning,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor."
216,included,10.1109/isc2.2016.7580798,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7580798/,9/15/2016 0:00,smartseal: a ros based home automation framework for heterogeneous devices interconnection in smart buildings,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it."
217,unknown,10.1109/icsys47076.2019.8982469,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8982469/,9/19/2019 0:00,fpga-enabled binarized convolutional neural networks toward real-time embedded object recognition system for service robots,"In this presentation, we report the results of applying a binarized Convolutional Neural Network (CNN) and a Field Programmable Gate Array (FPGA) for image-based object recognition. While the demand rises for robots with robust object recognition implemented with Neural Networks, a tradeoff between data processing rate and power consumption persists. Some applications utilise Graphics Processing Units (GPU), which results in high power consumption, thus undesirable for embedded systems, while the others communicate with cloud computers to minimise computational resources at the clients' side, i.e. robots, raising another concern that the robots are unable to perform object recognition without the servers and network connections. To overcome these difficulties, we propose an embedded object recognition system implemented with a binarized CNN and an FPGA. FPGAs consist of a matrix of reconfigurable logic gates allowing parallel computing which befit most image processing algorithms such as the CNN. We train the binarized CNN on one of our datasets that contain images of several kinds of food and beverages. The results of the experiments show that the binarized CNN with an FPGA maintains high accuracy as well as real-time computation, suggesting that the proposed system is suitable for robots to perform their tasks in a real-world environment without needing to communicate with a server."
218,unknown,10.1109/fuzz48607.2020.9177654,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9177654/,7/24/2020 0:00,ai-fml agent for robotic game of go and aiot real-world co-learning applications,"In this paper, we propose an AI-FML agent for robotic game of Go and AIoT real-world co-learning applications. The fuzzy machine learning mechanisms are adopted in the proposed model, including fuzzy markup language (FML)-based genetic learning (GFML), eXtreme Gradient Boost (XGBoost), and a seven-layered deep fuzzy neural network (DFNN) with backpropagation learning, to predict the win rate of the game of Go as Black or White. This paper uses Google AlphaGo Master sixty games as the dataset to evaluate the performance of the fuzzy machine learning, and the desired output dataset were predicted by Facebook AI Research (FAIR) ELF Open Go AI bot. In addition, we use IEEE 1855 standard for FML to describe the knowledge base and rule base of the Open Go Darkforest (OGD) prediction platform in order to infer the win rate of the game. Next, the proposed AI-FML agent publishes the inferred result to communicate with the robot Kebbi Air based on MQTT protocol to achieve the goal of human and smart machine co-learning. From Sept. 2019 to Jan. 2020, we introduced the AI-FML agent into the teaching and learning fields in Taiwan. The experimental results show the robots and students can co-learn AI tools and FML applications effectively. In addition, XGBoost outperforms the other machine learning methods but DFNN has the most obvious progress after learning. In the future, we hope to deploy the AI-FML agent to more available robot and human co-learning platforms through the established AI-FML International Academy in the world."
219,included,10.1109/ams.2017.22,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8424312/,12/6/2017 0:00,autonomous rover navigation using gps based path planning,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino)."
220,unknown,10.1109/ieeeconf49454.2021.9382607,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9382607/,1/14/2021 0:00,teaching system for multimodal object categorization by human-robot interaction in mixed reality,"As service robots are becoming essential to support aging societies, teaching them how to perform general service tasks is still a major challenge preventing their deployment in daily-life environments. In addition, developing an artificial intelligence for general service tasks requires bottom-up, unsupervised approaches to let the robots learn from their own observations and interactions with the users. However, compared to the top-down, supervised approaches such as deep learning where the extent of the learning is directly related to the amount and variety of the pre-existing data provided to the robots, and thus relatively easy to understand from a human perspective, the learning status in bottom-up approaches is by their nature much harder to appreciate and visualize. To address these issues, we propose a teaching system for multimodal object categorization by human-robot interaction through Mixed Reality (MR) visualization. In particular, our proposed system enables a user to monitor and intervene in the robot&#x2019;s object categorization process based on Multimodal Latent Dirichlet Allocation (MLDA) to solve unexpected results and accelerate the learning. Our contribution is twofold by 1) describing the integration of a service robot, MR interactions, and MLDA object categorization in a unified system, and 2) proposing an MR user interface to teach robots through intuitive visualization and interactions."
221,included,10.1109/robot.2004.1308781,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1308781/,5/1/2004 0:00,software approach for the autonomous inspection robot makro,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level."
222,included,10.1109/lars-sbr.2016.49,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7783535/,10/12/2016 0:00,integration of people detection and simultaneous localization and mapping systems for an autonomous robotic platform,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment."
223,unknown,10.1016/j.enggeo.2020.105817,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85090411791,12/5/2020,"successful implementations of a real-time and intelligent early warning system for loess landslides on the heifangtai terrace, china","Real-time monitoring and intelligent early warning system are crucial and significant to take mitigation measures and reduce casualties and property losses related to landslides. It is difficult to obtain entire monitoring data in the accelerated deformation phase in a landslide event, and hard to issue early warning information using a traditional monitoring approach with fixed and low sampling frequency. Displacement increments of loess landslides induced by agriculture irrigation on the Heifangtai terrace could be sudden and extremely rapid. Typical landslide types include loess flowslides and loess falls. It is of practical significance to develop a self-adaptive data acquisition monitoring technique and establish a real-time landslide early warning system (LEWS) to meet the needs for risk mitigation of rapid sliding slopes on the Heifangtai terrace. The monitoring technique can wirelessly transmit displacement data and the LEWS was devised using the new artificial intelligence. The LEWS could automatically release the warning information in advance of the event once the early warning parameters exceed default thresholds. In this study, the early warning procedures, real-time monitoring approach, intelligent LEWS, a multiple criteria warning model, warning release and emergency mitigation measures, and performance are introduced in detail. Six loess landslides at Heifangtai and eight landslides in other regions of China have been successfully warned since its implementation in 2012. This study proposed an effective and practical solution for the early warning of loess landslides at Heifangtai. Two typical loess landslides that had successful early warnings at Heifangtai were presented. The successful implementation could serve as a reference for global rapid slope failure cases, considering the complex nature of landslide behaviors and failure mechanisms."
224,unknown,10.1016/j.simpat.2019.102015,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85074521783,5/1/2020,a fog computing model for implementing motion guide to visually impaired,"A guide dog robot system for visually impaired often needs to process many kinds of information, such as image, voice and other sensor information. Information processing methods based on deep neural network can achieve better results. However, it requires expensive computing and communication resources to meet the real-time requirement. Fog computing has emerged as a promising solution for applications that are data-intensive and delay-sensitive. We propose a fog computing framework named PEN (Phone + Embedded board + Neural compute stick) for the guide dog robot system. The robot’s functions in PEN are wrapped as services and deployed on the appropriate devices. Services are combined as an application in a visual programming language environment. Neural compute stick accelerates image processing speed at low power consumption. A simulation environment and a prototype are built on the framework. The simulated guide dog system is developed for operating in a miniature environment, including a small robot dog, a small wheelchair, model cars, traffic lights, and traffic blockage. The prototype is a full-sized portable guide system that can be used by a visually impaired person in a real environment. Simulation and experiments show that the framework can meet the functional and performance requirements for implementing the guide systems for visually impaired."
225,included,10.1016/j.robot.2018.02.010,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85044145526,6/1/2018,visual attention and object naming in humanoid robots using a bio-inspired spiking neural network,"Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred’ orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors."
226,unknown,10.1016/0950-7051(94)90024-8,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/38149147114,1/1/1994,"multi-paradigm software environment for the real-time processing of sound, music and multimedia","The paper introduces a system and a software architecture for the representation and real-time processing of sound, music, and multimedia based on artificial intelligence techniques. This system, called WinProcne/HARP, is able to represent objects in a two-fold formalism—symbolic and analogical—at different levels of abstraction, and to carry out plans according to the user's goals. It also provides both formal and informal analysis capabilities for extracting information. In WinProcne/HARP the user can build, update, browse, and merge various knowledge bases of sound, music, and multimedia material, as well as enter queries, start and manage real time performance, using a high-level graphical user interface. The system is currently used by researchers and composers in various experiments, including (a) advanced robotics projects, in which the system is used as a tool for interacting, cotrolling and simulating robot movements, and (b) theatrical automation, where the system is delegated to manage and integrate sound, music, and three-dimensional computer animations of humanoid figures. The paper explicitly refers to some applications in the music field."
227,unknown,10.1109/spices.2017.8091310,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8091310/,8/10/2017 0:00,implementation of a self-adaptive real time recommendation system using spark machine learning libraries,"Real time recommendation systems have become an essential component of e-commerce web applications. With increasing volume and velocity of data handled by these applications, known as the bigdata problem, traditional recommendation systems that analyze data and update models at regular time intervals would not be able to satisfy this requirement. With the evolution of technologies for processing bigdata in real time, it has become fairly easy to implement real time recommendation systems. Stream-computing is a new computing paradigm for handling the velocity attribute of bigdata which makes it possible to develop real time bigdata applications. This paper gives the details of implementation of a real time recommendation system using Apache Spark, a widely used platform for stream computing. This system is implemented for recommending TV channels to viewers in real time. This becomes a challenging task due to continuous changes in the set of available channels and the context dependent preference of viewers. In channel recommendation scenario, characterized by its dynamic nature, volume of data, and tight time constraints, traditional approaches cannot be used. We have implemented a highly scalable TV channel recommendation system optimized for the processing of real-time data streams originating from set-top boxes. The proposed system implements a self-adaptive approach for model building. The system effectively uses distributed processing power of Apache Spark to make recommendations in real time with scalability to meet the real time constraints with increasing load. The Spark Machine Learning Libraries (Spark MLLib) provide several algorithms which were used for developing the proposed recommendation system. The large amount of data in the system is efficiently managed by the data processing method of Lambda Architecture."
228,unknown,10.1007/978-3-030-60884-2_6,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-60884-2_6,2020-01-01 00:00:00,implementation of a svm on an embedded system: a case study on fall detection,"Edge Computing seeks to bring Machine Learning as close as possible to the source events of interest, providing an almost instant interpretation to data acquired by sensors giving sense to raw data while addressing concerns of particular applications such as latency, privacy and server stress relieve. Due to a lack of research on this particular type of application, we are faced with difficulties both in software and hardware as embedded systems are known to possess serious limitations on its available processing resources. To address this, we make use of the concepts of edge computing and offline programming to accomplish a reliable machine learning model deployment on the microprocessor. By studying real case problem, we can get measurements on the resources required by such an application as well as its performance. In this study, we address the implementation of such an application in an embedded system focusing on the detection of human falls."
229,unknown,10.1145/3004056,ACM Trans. Cyber Phys. Syst.,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/39e0091d8e83f72346eeff751089a8f2507697d1,2017-01-01 00:00:00,a cloud-based black-box solar predictor for smart homes,"The popularity of rooftop solar for homes is rapidly growing. However, accurately forecasting solar generation is critical to fully exploiting the benefits of locally generated solar energy. In this article, we present two machine-learning techniques to predict solar power from publicly available weather forecasts. We use these techniques to develop SolarCast, a cloud-based web service that automatically generates models that provide customized site-specific predictions of solar generation. SolarCast utilizes a “black box” approach that requires only (1) a site’s geographic location and (2) a minimal amount of historical generation data. Since we intend SolarCast for small rooftop deployments, it does not require detailed site- and panel-specific information, which owners may not know, but instead automatically learns these parameters for each site. We evaluate the accuracy of SolarCast’s different algorithms on two publicly available datasets, each containing over 100 rooftop deployments with a variety of attributes (e.g., climate, tilt, orientation, etc.). We show that SolarCast learns a more accurate model using much less data (∼1 month) than prior SVM-based approaches, which require ∼3 months of data. SolarCast also provides a programmatic API, enabling developers to integrate its predictions into energy efficiency applications. Finally, we present two case studies of using SolarCast to demonstrate how real-world applications can leverage its predictions. We first evaluate a “sunny” load scheduler, which schedules a dryer’s energy usage to maximally align with a home’s solar generation. We then evaluate a smart solar-powered charging station, which can optimally charge the maximum number of electric vehicles (EVs) on a given day. Our results indicate that a representative home is capable of reducing its grid demand up to 40% by providing a modest amount of flexibility (of ∼5 hours) in the dryer’s start time with opportunistic load scheduling. Further, our charging station uses SolarCast to provide EV owners the amount of energy they can expect to receive from solar energy sources."
230,unknown,http://arxiv.org/abs/1801.03002v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1801.03002v2,2018-01-08 00:00:00,deepstyle: multimodal search engine for fashion and interior design,"In this paper, we propose a multimodal search engine that combines visual and
textual cues to retrieve items from a multimedia database aesthetically similar
to the query. The goal of our engine is to enable intuitive retrieval of
fashion merchandise such as clothes or furniture. Existing search engines treat
textual input only as an additional source of information about the query image
and do not correspond to the real-life scenario where the user looks for 'the
same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those
shortcomings by using a joint neural network architecture to model contextual
dependencies between features of different modalities. We prove the robustness
of this approach on two different challenging datasets of fashion items and
furniture where our DeepStyle engine outperforms baseline methods by 18-21% on
the tested datasets. Our search engine is commercially deployed and available
through a Web-based application."
231,unknown,10.1016/j.procs.2022.01.318,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85127782163,2022-01-01,"predictive maintenance on sensorized stamping presses by time series segmentation, anomaly detection, and classification algorithms","Sheet metal forming tools, like stamping presses, play an ubiquitous role in the manufacture of several products. With increasing requirements of quality and efficiency, ensuring maximum uptime of these tools is fundamental to marketplace competitiveness. Using anomaly detection and predictive maintenance techniques, it is possible to develop lower risk and more intelligent approaches to maintenance scheduling, however, industrial implementations of these methods remain scarce due to the difficulties of obtaining acceptable results in real-world scenarios, making applications of such techniques in stamping processes seldom found. In this work, we propose a combination of two distinct approaches: (a) time segmentation together with feature dimension reduction and anomaly detection; and (b) machine learning classification algorithms, for effective downtime prediction. The approach (a)+(b) allows for an improvement rate up to 22.971% of the macro F1-score, when compared to sole approach (b). A ROC AUC index of 96% is attained by using Randomized Decision Trees, being the best classifier of twelve tested. An use case with a decentralized predictive maintenance architecture for the downtime forecasting of a stamping press, which is a critical machine in the manufacturing facilities of Bosch Thermo Technology, is discussed."
232,unknown,10.15835/buasvmcn-hort:2021.0030,'AcademicPres (EAP) Publishing House',core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/490608362.pdf,2021-11-29 00:00:00,bayesian ridge algorithm for brix prediction in  industrial tomato,"Tomato is one of the most significant vegetables in the world. Specifically, for the industrial tomato cultivation, the product is harvested when °Brix are at their peak. Technological advancements nowadays have made Decision Support Systems, based on Machine Learning Algorithms more applicable in a daily basis. Sustainable agriculture is evolving since farmers could be advised by this technology in order to take the best decision for their crops. Farmers who adopt this kind of technology will be able to know the quality of tomatoes. The implementation of a Decision Support System capable to predict the °Brix was conducted, based on various data from previous years, such as quality characteristics, the tomato hybrid used, weather conditions and soil data from the selected fields. Data came from fields from 6 different regions in Peloponnese, Greece over 3 cultivation periods. 12 different algorithms were tested in order to find which is the best one in terms of efficiency. Results of this research showed that the predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. The use of this DSS using real time weather data as an input will be a valuable tool for the farmers"
233,unknown,10.1007/978-3-030-86230-5_53,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-86230-5_53,2021-01-01 00:00:00,cloud based decision making for multi-agent production systems,"The use of multi-agent systems (MAS) as a distributed control method for shop-floor manufacturing control applications has been extensively researched. MAS provides new implementation solutions for smart manufacturing requirements such as the high dynamism and flexibility required in modern manufacturing applications. MAS in smart manufacturing is becoming increasingly important to achieve increased automation of machines and other components. Emerging technologies like artificial intelligence, cloud-based infrastructures, and cloud computing can also provide systems with intelligent, autonomous, and more scalable solutions. In the current work, a decision-making framework is proposed based on the combination of MAS cloud computing, agent technology, and machine learning. The framework is demonstrated in a quality control use case with vision inspection and agent-based control. The experiment utilizes a cloud-based machine learning pipeline for part classification and agent technology for routing. The results show the applicability of the framework in real-world scenarios bridging cloud service-oriented architecture with agent technology for production systems."
234,unknown,10.1109/icphm.2018.8448431,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8448431/,2018-06-13 00:00:00,industrial ai enabled prognostics for high-speed railway systems,"The vision of pervasive applications of artificial intelligence (AI) and the fact that hardware is becoming more portable and computationally powerful has encouraged the development of industrial AI. High-speed railway (HSR) transportation is an area of focus for industrial AI, since it demands maximized safety, reliability, availability, and minimized cost. Given the system complexity, the predictive maintenance for high-speed railway could hardly sustain with a raw experience-based system. This paper presents a framework for industrial AI enabled PHM system for HSR, which creates cyber twins of physical key subsystems and components to improve the condition transparency and decision efficiency. Enabled by advanced signal processing and machine learning with domain insights on historical data, cyber twins monitor real-time performance and predict potential faults to prevent unexpected downtime and support optimized decisions. Instead of performing analytics with large amount of raw data on the cloud, the cyber railway transportation system leverages the advantage of edge computing for real-time feature extraction and anomaly detection. The proposed framework introduces the general approach of implementing industrial AI. The paper also discusses the key methods of data-driven solutions for selected critical subsystems."
235,unknown,10.2118/203037-ms,,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/2a0dc87e45ebc4647cae0a3845d2dd8e44887bcc,2020-01-01 00:00:00,artificial intelligence aims to save lives in offshore marine vessels,"
 In ADNOC Oil and Gas 4.0 mission, we are committed to empower people with the needed capabilities and Artificial Intelligence (AI) technologies to fuel innovation, efficiency and more importantly achieve and sustain a 100% HSE, by transforming the way of handling HSE events by moving from reactive to proactive approach. The ultimate objective is to save lives, empower the vessel Captains to immediately identify and respond to violators, improve the HSE culture of the crew, and automatically generate live data analytics and statistics with the aim of improving safety in operations. The implemented AI use cases are; deviation for not wearing Protective Safety equipment in designated areas, violation of not utilizing safety passages, alert when no watchman in muster station, alarm when man overboard incident, alarm when man fell on stairs, and live Personnel on board each weather-deck. When introduced the Artificial Intelligence cameras, our marine vessels will adopt a smarter automated response and reporting culture, which will in turn, lead to increased safety oversight of our critical offshore operations. Therefore, with the advent of the AI technology, many common business processes have been automated thus enabling personnel to increase their focus on more important tasks while technologies like the AI System can handle many of the time consuming tasks.
 The solution components consists of Artificial Intelligence platform, high definition cameras, local server, wide-range WiFi access point, network infrastructure and a tablet. On the tablet device, the captain have full coverage of the vessel weather decks, working areas and restricted zones with a feature to generate alerts when detecting an emergency situation. This was provided to empower the vessel Captain to acknowledge and respond to violations as well as take a proactive action to prevent incidents from happening. The Machine Learning algorithm has been trained on actual scenarios and will be continuously improved by adding more recorded event to retrain the initial model. Currently, the prediction model is performing on the vessel operation mode and recording events with high rate of accuracy. In case of automatically detecting an alerting or non-compliance event, the captain would be notified, beacon lights and sound, and log recorded in the local and central system with a photo and a short video clip of the incident. The process of identifying HSE deviations are becoming digitally transformed by deploying AI capabilities on real-time video streams. The AI-based camera system leverages Computer Vision features that enables machines to get and analyze visual information and take action. The whole process of identifying HSE violation events has been digitally transformed by deploying an artificial intelligence solution to perform real time video analytics."
236,unknown,10.1109/access.2018.2879117,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8590712/,2019-01-01 00:00:00,a framework to estimate the nutritional value of food in real time using deep learning techniques,"There has been a rapid increase in dietary ailments during the last few decades, caused by unhealthy food routine. Mobile-based dietary assessment systems that can record real-time images of the meal and analyze it for nutritional content can be very handy and improve the dietary habits and, therefore, result in a healthy life. This paper proposes a novel system to automatically estimate food attributes such as ingredients and nutritional value by classifying the input image of food. Our method employs different deep learning models for accurate food identification. In addition to image analysis, attributes and ingredients are estimated by extracting semantically related words from a huge corpus of text, collected over the Internet. We performed experiments with a dataset comprising 100 classes, averaging 1000 images for each class to acquire top 1 classification rate of up to 85%. An extension of a benchmark dataset Food-101 is also created to include sub-continental foods. Results show that our proposed system is equally efficient on the basic Food-101 dataset and its extension for sub-continental foods. The proposed system is implemented as a mobile app that has its application in the healthcare sector."
237,unknown,10.1016/j.ipm.2018.04.011,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85048575075,2019-05-01,real-time processing of social media with sentinel: a syndromic surveillance system incorporating deep learning for health classification,"Interest in real-time syndromic surveillance based on social media data has greatly increased in recent years. The ability to detect disease outbreaks earlier than traditional methods would be highly useful for public health officials. This paper describes a software system which is built upon recent developments in machine learning and data processing to achieve this goal. The system is built from reusable modules integrated into data processing pipelines that are easily deployable and configurable. It applies deep learning to the problem of classifying health-related tweets and is able to do so with high accuracy. It has the capability to detect illness outbreaks from Twitter data and then to build up and display information about these outbreaks, including relevant news articles, to provide situational awareness. It also provides nowcasting functionality of current disease levels from previous clinical data combined with Twitter data.
                  The preliminary results are promising, with the system being able to detect outbreaks of influenza-like illness symptoms which could then be confirmed by existing official sources. The Nowcasting module shows that using social media data can improve prediction for multiple diseases over simply using traditional data sources."
238,unknown,10.1109/electr.1991.718282,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/718282/,1991-04-18 00:00:00,imaging and controls for mars robots with neural networks,"Two aspects of the design of space robots is covered implemented by neural networks and by hybrid approach with artificial intelligence. One is a neurocontroller for a real-time autonomous system. An optical control system developed saves the time for the image processing that analyzes an image sensor through the environment and induces a transformation over the sensor array. A prototype of the neurocontroller is able to learn and control by itself. The second aspect deals with the design of a Servo Control System for a Robot with the capability of ""learning in Unanticipated Situations"" incorporated in the system. The robot is assumed to be employed to perform useful tasks in an alien evironment. The model developed is shown to provide the robot with the capability to recover from unanticipated situations that can lead to the disruption of its normal operation, and to learn to avoid such situations in the future. These two aspects will be integrated for a design of a very intelligent autonomous space robot."
239,unknown,10.1007/s00607-020-00897-4,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00607-020-00897-4,2021-01-06 00:00:00,a novel indoor localization system using machine learning based on bluetooth low energy with cloud computing,"In this paper, we propose a novel indoor localization system in a multi-indoor environment using cloud computing. Prior studies show that there are always concerns about how to avoid signal occlusion and interference in the single indoor environment. However, we find some general rules to support our system being immune to interference generated by occlusion in the multi-indoor environment. A convenient way is measured to deploy Bluetooth low energy devices, which mainly collect large information to assist localization. A neural network-based classification is proposed to improve localization accuracy, compared with several algorithms and their performance comparison is discussed. We also design a distributed data storage structure and establish a platform considering the storage load with Redis. Our real experimental validation shows that our system will meet the four aspects of performance requirements, which are higher accuracy, less power consumption, and increased levels of system magnitude and deployment efficiency."
240,unknown,10.1109/itsc45102.2020.9294435,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1fcacf7fc81ff9366229c02440e1dc70c3ae28c1,2020-01-01 00:00:00,architecture design and development of an on-board stereo vision system for cooperative automated vehicles,"In a cooperative automated driving scenario like platooning, the ego vehicle needs reliable and accurate perception capabilities to autonomously follow the lead vehicle. This paper presents the architecture design and development of an on-board stereo vision system for cooperative automated vehicles. The input to the proposed system is stereo image pairs. It uses three deep neural networks to detect and classify objects, lane markings, and free space boundary simultaneously in front of the ego vehicle. The rectified left and right image frames of the stereo camera are used to compute a disparity map to estimate the detected object’s depth and radial distance. It also estimates the object’s relative velocity, azimuth, and elevation angle with respect to the ego vehicle. It sends the perceived information to the vehicle control system and displays the perceived information in a meaningful way on the human-machine interface. The system runs on both PC (x86_64 architecture) with Nvidia GPU, and the Nvidia Drive PX 2 (aarch64 architecture) automotive-grade compute platform. It is deployed and evaluated on Renault Twizy cooperative automated driving research platform. The presented results show that the stereo vision system works in real-time and is useful for cooperative automated vehicles."
241,unknown,10.1109/tie.2021.3068681,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9390342/,2022-03-01 00:00:00,degradation estimation and prediction of electronic packages using data-driven approach,"Recent trends in automotive electronics such as automated driving will increase the number and complexity of electronics used in safety-relevant applications. Applications in logistics or ridesharing will require a specific year of service rather than the conventional mileage usage. Reliable operations of the electronic systems must be assured at all times, regardless of the usage condition. A more dynamic and on-demand way of assuring the system availability will have to be developed. This article proposes a thermomechanical stress-based prognostics method as a potential solution. The goal is achieved by several novel advancements. On the experimental front, a key microelectronics package is developed to directly apply the prognostics and health management concept using a piezoresistive silicon-based stress sensor. Additional hardware for safe and secure data transmission and data processing is also developed, which is critically required for recording in situ and real-time data. On the data management front, proper data-driven approaches have to be identified to handle the unique dataset from the stress sensor employed in this study. The approaches effectively handle the massive amount of data that reveals the important information and automation of the prognostic process and thus to be able to detect, classify, locate, and predict the failure. The statistical techniques for diagnostics and the machine learning algorithms for health assessment and prognostics are also determined to implement the approaches in a simple, fast, but accurate way within the capacity of limited computing power. The proposed prognostics approach is implemented with actual microelectronics packages subjected to harsh accelerated testing conditions. The results corroborate the validity of the proposed prognostics approach."
242,unknown,10.1109/tps.2013.2276537,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6582663/,2013-09-01 00:00:00,space-varying templates for real-time applications of cellular nonlinear networks to pattern recognition in nuclear fusion,"In this paper, a new methodology consisting of space-varying templates in cellular nonlinear networks (CNNs) for real-time visual pattern recognition in nuclear fusion devices is presented. The development of space-varying templates is a new upgrade, driven by the need to process different parts of the images in different ways. The new approach has been applied to the identification in real time of various objects present in the Joint European Torus videos of both infrared (IR) and visible cameras. IR videos are here used to detect hot spots and the regions of the walls in which dangerously high temperatures are reached, whereas visible cameras provide information about multifaceted asymmetric radiations from the edge, which are dangerous instabilities that can lead to disruptions. Their identification is particularly difficult because of their movement and their shape which is similar to other objects present in the frames. Therefore, in addition to space-varying template CNNs, quite sophisticated morphological operators have to be deployed and their outputs processed by machine learning tools, such as support vector machines. The implementation of the whole methodology was performed in a field-programmable gate array board, obtaining, in both applications, a final success rate close to 100% and a frame rate higher than 200 frames/s."
243,unknown,http://arxiv.org/abs/2005.05287v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2005.05287v2,2020-05-11 00:00:00,"using computer vision to enhance safety of workforce in manufacturing in
  a post covid world","The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks."
244,unknown,10.1109/ccnc49033.2022.9700676,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9700676/,2022-01-11 00:00:00,qos-aware priority-based task offloading for deep learning services at the edge,"Emerging Edge Computing (EC) technology has shown promise for many delay-sensitive Deep Learning (DL) based applications of smart cities in terms of improved Quality-of-Service (QoS). EC requires judicious decisions which jointly consider the limited capacity of the edge servers and provided QoS of DL-dependent services. In a smart city environment, tasks may have varying priorities in terms of when and how to serve them; thus, priorities of the tasks have to be considered when making resource management decisions. In this paper, we focus on finding optimal offloading decisions in a three-tier user-edge-cloud architecture while considering different priority classes for the DL-based services and making a trade-off between a task&#x2019;s completion time and the provided accuracy by the DL-based service. We cast the optimization problem as an Integer Linear Program (ILP) where the objective is to maximize a function called gain of system (GoS) defined based on provided QoS and priority of the tasks. We prove the problem is NP-hard. We then propose an efficient offloading algorithm, called PGUS, that is shown to achieve near-optimal results in terms of the provided GoS. Finally, we compare our proposed algorithm, PGUS, with heuristics and a state-of-the-art algorithm, called GUS, using both numerical analysis and real-world implementation. Our results show that PGUS outperforms GUS by a factor of 45% in average in terms of serving the top 25% higher priority classes of the tasks while still keeping the overall percentage of the dropped tasks minimal and the overall gain of system maximized."
245,unknown,10.1007/s11704-015-4404-7,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11704-015-4404-7,2015-12-01 00:00:00,non-intrusive sleep pattern recognition with ubiquitous sensing in elderly assistive environment,"The quality of sleep may be a reflection of an elderly individual’s health state, and sleep pattern is an important measurement. Recognition of sleep pattern by itself is a challenge issue, especially for elderly-care community, due to both privacy concerns and technical limitations. We propose a novelmulti-parametric sensing system called sleep pattern recognition system (SPRS). This system, equipped with a combination of various non-invasive sensors, can monitor an elderly user’s sleep behavior. It accumulates the detecting data from a pressure sensor matrix and ultra wide band (UWB) tags. Based on these two types of complementary sensing data, SPRS can assess the user’s sleep pattern automatically via machine learning algorithms. Compared to existing systems, SPRS operateswithout disrupting the users’ sleep. It can be used in normal households with minimal deployment. Results of tests in our real assistive apartment at the Smart Elder-care Lab are also presented in this paper."
246,unknown,10.23919/eecsi53397.2021.9624311,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9624311/,2021-10-21 00:00:00,strawberry fruit quality assessment for harvesting robot using ssd convolutional neural network,"Strawberry has a tremendous economic value as well as being visually appealing. Therefore, strawberry farmers need to ensure that they only harvest good quality strawberries. However, assessing the quality of strawberries is not an easy problem, especially for local plantations which do not have enough human resources. As robotics becomes accessible and widely used for agriculture work such as harvesting fruit, the real-time embedded system computation power becomes much more powerful nowadays. This paper discusses the harvesting robot&#x0027;s ability to distinguish the quality of strawberries in realtime detection using computer vision technology in the form of object detection by utilizing a deep neural network in a single board computer (SBC). The robot software is built on Robot Operating System (ROS) framework. The proposed method is tested on a robot equipped with a monocular camera. The learning process shows that the robot can detect and differentiate between good and bad quality strawberries with 90&#x0025; accuracy and maintain a high frame rate."
247,unknown,10.1109/icarm.2017.8273193,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8273193/,2017-08-31 00:00:00,development of a mobile app for the operation monitoring and health management system of a steam turbine,"In order to realize the real-time monitoring of the operation state and predict the possible faults of a steam turbine, a mobile APP based on the steam turbine operation monitoring and health management system is proposed in this paper. Firstly, some common faults which will be encountered in the operation of steam turbine are analyzed and the mechanism and characteristics of the faults are studied. Secondly, the hardware and software platform on the PC are built to diagnose the fault and analyze the trend by combining the methods of wavelet packet decomposition and BP neural network. Finally, the APP on the mobile terminal for the real-time operation monitoring of the steam turbine is developed. Furthermore, the test results of the mobile APP demonstrate that the developed mobile APP can effectively diagnose the fault and predict the trend. It also proved that the developed mobile APP can make the operation monitoring and health management system of the steam turbine run stably and meet the practical application requirements."
248,unknown,10.1016/b978-0-323-85117-6.00007-8,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85129907981,2021-01-01,livestock health monitoring using a smart iot-enabled neural network recognition system,"A poultry farm monitoring system along with a livestock monitoring system is a principal system for investigating the status of bird health by collecting biological traits such as their uttered sounds. This theme combines the Internet of Things (IoT) with nonintrusive and reliable wearable sensing technology. In recent developments, machine learning (ML) or artificial neural networks (ANNs) have been well applied and recognized as an effective tool for a range of complicated scenario analyses in real time, including healthcare sector applications. This will help to alleviate problems typically suffered and faced by medical researchers in these fields by saving time for practitioners by providing unbiased results. In this chapter we discuss the utilization of analytics learning and neural network usage toward clinical concerns in health care using the IoT. ANN is a prime research domain with recent deployment of computational sophistication in hardware and software in several application domains with highly complicated computing scenarios. The healthcare sector is one area which is capable of automation to save time and that is subjective by nature. Therefore, ML- and ANN-based simulations generate unbiased outcomes. This chapter describes an IoT-structured wearable sensing platform with the inclusion of an audio feature and temperature of the livestock. In particular, the secure audio-wellbeing features are incorporated into the platform to spontaneously examine and conclude using voice information from the livestock for recognition of diseased birds. One month of long-term recognition experimentation analysis was performed where the recognition accuracy of the onset of disease bird was about 91% using a spiking neural network (SNN).The recognition accuracy of SNN in this regard is better than the performance of an ANN. A sequence of steps was taken in connection with a specific event that occurred and involved in examining the interrelationship across the central monitoring unit and the local monitoring unit using the IoT by utilizing the bird voice features, bird temperature, and room temperature and humidity."
249,unknown,http://arxiv.org/abs/1907.00594v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1907.00594v1,2019-07-01 00:00:00,"fingerprint-based localization using commercial lte signals: a
  field-trial study","Wireless localization for mobile device has attracted more and more interests
by increasing the demand for location based services. Fingerprint-based
localization is promising, especially in non-Line-of-Sight (NLoS) or rich
scattering environments, such as urban areas and indoor scenarios. In this
paper, we propose a novel fingerprint-based localization technique based on
deep learning framework under commercial long term evolution (LTE) systems.
Specifically, we develop a software defined user equipment to collect the real
time channel state information (CSI) knowledge from LTE base stations and
extract the intrinsic features among CSI observations. On top of that, we
propose a time domain fusion approach to assemble multiple positioning
estimations. Experimental results demonstrated that the proposed localization
technique can significantly improve the localization accuracy and robustness,
e.g. achieves Mean Distance Error (MDE) of 0.47 meters for indoor and of 19.9
meters for outdoor scenarios, respectively."
250,unknown,10.1007/978-3-319-95273-4_5,Springer,springer,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-319-95273-4_5,2019-01-01 00:00:00,applying sound-based analysis at porsche production: towards predictive maintenance of production machines using deep learning and internet-of-things technology,"(a) Situation faced : All mechanical and mechatronic devices are subject to wear, tear and breakdown. Failure of such devices can cause significant costs, e.g., in automotive factories. Established predictive maintenance approaches usually require deep integration with the specific machine. Such approaches are not practically feasible because of technical, legal and financial restrictions. A non-intrusive, lightweight and generic solution approach is desired. (b) Action taken : A solution concept was developed which, at its heart, is based on deep learning algorithms that monitor sound sequences captured from a microphone, analyze them and return classification results for use in further steps of a control loop, such as planning actions and execution steps. We named this approach the ‘Sound Detective’ and it was evaluated by retrofitting a coffee machine using simple microphones to capture production sounds. The sound sequences are subsequently analyzed using neural networks developed in Keras and TensorFlow. During prototyping, multiple kinds of neural networks and architectures were tested and the experiment was realized with two different kinds of coffee machines to validate the generalizability of the solution to different platforms. (c) Results achieved : The prototype can analyze sounds produced by a mechanical machine and classify different states. The technical realization relies on cheap commodity hardware and open-source software, demonstrating the applicability of existing technologies and the feasibility of the implementation. Especially, it was described that the proposed approach can be applied to solve predictive maintenance tasks. (d) Lessons learned : The present work demonstrates the feasibility of the Sound Detective’s reference architecture and discusses challenges and learnings during implementation. Specifically, key learnings include the importance of data quality, preprocessing and consistency, influences of the experimental setup on real-world prediction performance and the relevance of microcomputers, the target hardware and type of the programming language for complex analyses."
251,unknown,10.1109/hpsr54439.2022.9831399,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9831399/,2022-06-08 00:00:00,a novel personnel counting method based on wifi perception,"In the Internet of Things (IoT) era, WiFi is now commonly implemented worldwide as a convenient wireless data transmission technology and brings much convenience to people&#x2019;s life. Personnel counting, which plays an indispensable role in many areas, such as indoor crowd control, public safety, and marketing analytics, is also important in some special events. In emergencies, such as bank robberies, where police need to capture the number of robbers and hostages in a robbed bank, counting the number of people within the region using WiFi perception technology is more reliable and safer than traditional counting methods based on video streaming. In the early work of others, researchers used received signal strength indicator (RSSI) from the MAC layer for personnel counting studies. In recent years, it has been found that the channel state information (CSI) from the physical layer is more stable and the personnel counting method based on CSI has a higher accuracy rate. In their excellent works, however, researchers do not take into consideration the unpredictability of crowd behavior in real life. Meanwhile, the experimental accuracy needs to be greatly improved. This paper, therefore, focuses on proposing a novel CSI-based WiFi perception personnel counting method using the Long Short Term Memory (LSTM) algorithm that is useful for hidden counting the number of people in different situations. We conducted people counting experiments under three real experimental scenarios. In addition, we have compared three other algorithms for machine learning and optimized some parameters to achieve an overall accuracy of over 97% for our method."
252,unknown,10.1109/metroind4.0iot51437.2021.9488447,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9488447/,2021-06-09 00:00:00,iot data-driven experimental process optimisation for kevlar fiberglass components for aeronautic,"This paper describes the work carried out during the PROOF experiment (IOT data-driven experimental PROcess Optimization for kevlar Fiberglass components for aeronautic), winner of the second open call of the MIDIH EU project. The main objectives of the experiment are the integration of smart sensing devices with the Energy@Work IoT gateways and the development of cloudified innovative data-driven methodologies and data analytics tools to support process optimization in the production of hybrid composite material parts for the aeronautical sector. Collection of real-time production-data from multiple sensors with several industrial protocols and data transfer to the MIDIH project platform has been performed adopting the IoT gateway developed by Energy@Work, following MIDIH reference architecture for advanced data processing and visualization (e.g., Fiware Orion Context Broker, Apache Flink and Fiware Knowage) by using MQTT protocol. Then, historical and new acquired data has been analysed using advanced clustering techniques and trends, with the purpose to allow a novel CPS-based predictive system on the production process. Machine-Learning algorithms and visualisations (GUI based on Fiware Knowage) in real operating conditions have been used to validate the performance and assess the outcome. Finally, thanks to the implementation of specific optimization rules, able to process data gathered from the sensor network, a framework for distributed processing engine has been exploited by (i) generating tips for energy efficiency and process optimization and (ii) providing different type of alarms based on expected consumptions, resulting in concrete support to production managers for the improvement of the whole production value chain."
253,unknown,10.1109/jiot.2020.3020911,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9184072/,2015-02-15 20:21:00,achieving democracy in edge intelligence: a fog-based collaborative learning scheme,"The emergence of fog computing has brought unprecedented opportunities to the Internet-of-Things (IoT) field, and it is now feasible to incorporate deep learning at the edge of the IoT network to provide a wide range of highly tailored services. In this article, we present a fog-based democratically collaborative learning scheme in which fog nodes collaborate on the model training process even without the support of the cloud, contributing to the advances of IoT in terms of realizing a more intelligent edge. To achieve that, we design a voting strategy so that a fog node could be elected as the coordinator node based on both distance and computational power metrics to coordinate the training process. Also, a collaborative learning algorithm is proposed to generalize the training of different deep learning models in the fog-enabled IoT environment. We then implement two popular use cases, including a user trajectory prediction and a distributed image recognition, to demonstrate the feasibility, practicality, and effectiveness of the scheme. More importantly, the experiments on both use cases are conducted through a real world, in-door fog deployment. The result shows that the scheme can utilize fog to obtain a well-performing deep learning model in the cloudless IoT environment while mitigating the data locality issue for each fog node."
254,unknown,10.1109/icscds53736.2022.9760986,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9760986/,2022-04-09 00:00:00,enterprise email server data protection system using geo-fence technology and machine learning,"Data sharing and protection is becoming an increasingly important element of end users&#x0027; daily life as they access various systems, services, and apps. Data disclosure often occurs in real-world email services. Verification and copyright protection of multimedia content has always been a huge problem for secure data transfer media. The problem is exacerbated by the increasing use of the Internet and digital technology. But, implementing copyright protection is very complex and difficult. Digital watermarking emerged as a best solution to the problem of copyright protection. In the proposed method using Geofence technology both the Watermarking and Encryption method is used to share active content. A geofence is a virtual boundary around a physical location. A Geofence could be built dynamically in the form of a radius around a spot. Watermarking is used to hide information such as hiding private information on digital media such as photos. Encryption techniques are used to provide data safety and security. By encryption, encrypted information prevents unauthorized access and unauthorized persons from reading it. Finally, the authorized user can extract the encryption key with the help of the entered data entry process. Unauthorized access can be detected, where user information does not match the embedded information. This proposed application helps to track unauthorized access and prevent the distribution of content in the email environment Also provide group data sharing based on legal process using machine learning algorithm and provide approval for the post delivery system. suppose any unauthorized access to the information is identified, a notification to the admin regarding the forgery will be sent."
255,unknown,10.1109/isqed.2018.8357325,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8357325/,2018-03-14 00:00:00,low cost and power cnn/deep learning solution for automated driving,"Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is “finding drivable free space”, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving."
256,unknown,https://core.ac.uk/download/pdf/144848064.pdf,A Low-cost Approach for Detecting Activities of Daily Living using Audio Information: A Use Case on Bathroom Activity Monitoring,core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),,2016-04-21 00:00:00,10.5220/0005803700260032,"In this paper, we present an architecture for recognizing events related to activities of daily living in the context of a health monitoring environment. The proposed approach explores the integration of a Raspberry PI single-board PC both as an audio acquisition and analysis unit. A set of real-time feature extraction and classification procedures has been implemented and integrated on the Raspberry PI device, in order to provide continuous and online audio event recognition. In addition, a tuning and calibration workflow is presented, according to which the technicians installing the device in a fast ans user-friendly manner, without any requirements for machine learning expertise. The proposed approach has been evaluated against a particular scenario that is rather important in the context of any healthcare monitoring system for the elder, namely the ""bathroom scenario"" according to which a single microphone installed on a Raspberry PI device is used to monitor bathroom activity in a 24/7 basis. Experimental results indicate a satisfactory performance rate on the classification process (around 70% for five bathroom-related audio classes) even when less than two minutes of annotated data are used for training in the installation procedure. This makes the whole procedure non demanding in terms of time and effort needed to be calibrated by the technician"
257,unknown,10.1016/j.enbuild.2022.111988,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85125646504,2022-04-15,a digital twin predictive maintenance framework of air handling units based on automatic fault detection and diagnostics,"The building industry consumes the most energy globally, making it a priority in energy efficiency initiatives. Heating, ventilation, and air conditioning (HVAC) systems create the heart of buildings. Stable air handling unit (AHU) functioning is vital to ensuring high efficiency and extending the life of HVAC systems. This research proposes a Digital Twin predictive maintenance framework of AHU to overcome the limitations of facility maintenance management (FMM) systems now in use in buildings. Digital Twin technology, which is still at an initial stage in the facility management industry, use Building Information Modeling (BIM), Internet of things (IoT) and semantic technologies to create a better maintenance strategy for building facilities. Three modules are implemented to perform a predictive maintenance framework: operating fault detection in AHU based on the APAR (Air Handling Unit Performance Assessment Rules) method, condition prediction using machine learning techniques, and maintenance planning. Furthermore, the proposed framework was tested in a real-world case study with data between August 2019 and October 2021 for an educational building in Norway to validate that the method was feasible. Inspection information and previous maintenance records are also obtained through the FM system. The results demonstrate that the continually updated data combined with APAR and machine learning algorithms can detect faults and predict the future state of Air Handling Unit (AHU) components, which may assist in maintenance scheduling. Removing the detected operating faults resulted in annual energy savings of several thousand dollars due to eliminating the identified operating faults."
258,unknown,http://arxiv.org/abs/2109.07846v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2109.07846v1,2021-09-16 00:00:00,"telehealthcare and covid-19: a noninvasive & low cost invasive, scalable
  and multimodal real-time smartphone application for early diagnosis of
  sars-cov-2 infection","The global coronavirus pandemic overwhelmed many health care systems,
enforcing lockdown and encouraged work from home to control the spread of the
virus and prevent overrunning of hospitalized patients. This prompted a sharp
widespread use of telehealth to provide low-risk care for patients.
Nevertheless, a continuous mutation into new variants and widespread
unavailability of test kits, especially in developing countries, possess the
challenge to control future potential waves of infection. In this paper, we
propose a novel Smartphone application-based platform for early diagnosis of
possible Covid-19 infected patients. The application provides three modes of
diagnosis from possible symptoms, cough sound, and specific blood biomarkers.
When a user chooses a particular setting and provides the necessary
information, it sends the data to a trained machine learning (ML) model
deployed in a remote server using the internet. The ML algorithm then predicts
the possibility of contracting Covid-19 and sends the feedback to the user. The
entire procedure takes place in real-time. Our machine learning models can
identify Covid-19 patients with an accuracy of 100%, 95.65%, and 77.59% from
blood parameters, cough sound, and symptoms respectively. Moreover, the ML
sensitivity for blood and sound is 100%, which indicates correct identification
of Covid positive patients. This is significant in limiting the spread of the
virus. The multimodality offers multiplex diagnostic methods to better classify
possible infectees and together with the instantaneous nature of our technique,
demonstrates the power of telehealthcare as an easy and widespread low-cost
scalable diagnostic solution for future pandemics."
259,unknown,10.1364/jocn.403205,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9275288/,2021-01-01 00:00:00,open whitebox architecture for smart integration of optical networking and data center technology [invited],"In this paper, we identify challenges in developing future optical network infrastructure for new services based on technologies such as 5G, virtual reality, and artificial intelligence, and we suggest approaches to handling these challenges that include a business model, architecture, and diversity. Through activities in multiservice agreement and de facto standard organizations, we have shown how the hardware abstraction layer interfaces of optical transceivers are implemented for multivendor and heterogeneous environments, coherent digital signal processor interoperability, and optical transport whiteboxes. We have driven the effort to define the transponder abstraction interface with partners. The feasibility of such implementation was verified through demonstrations and trials. In addition, we are constructing an open-transport platform by combining existing open-source software and implementing software components that automate and enhance operations. An open architecture maintains a healthy ecosystem for industry and allows for a flexible, operator-driven network."
260,unknown,10.1016/j.procs.2020.04.199,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85086630682,2020-01-01,perspective vehicle license plate transformation using deep neural network on genesis of cpnet,"Recent development in vehicular industries and increased number of cars in modern society leads the people to pay more attention on Vehicle License Plate Recognition (V-LPR). V-LPR plays a major role in traffic related application such as road traffic monitoring, vehicle parking lots access control etc. Existing state of the art V-LPR systems in real world deployment works under restricted conditions, such as static illumination, fixed background etc. Most of them fails to work when any of the above given conditions are violated. Hence to address this issue, a novel V-LPR system is designed using modern deep learning framework called ""Capsule Network"". The proposed system is robust and works fine in any condition. Further, the proposed method aims to improve the processing time by integrating the segmentation process within the CN framework which involves the training and recognizing of entire license plate cropped region. Moreover, the feature extraction is performed by CN framework over a segmented alphanumeric character. Finally, Data augmentation technique is also used as a supplement to the CN framework to strengthen the process of training with various orientations like rotation, shift and flip for improving the recognition task."
261,unknown,10.1109/gucon48875.2020.9231114,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9231114/,2020-10-04 00:00:00,a novel machine learning based wearable belt for fall detection,"Falls are a major cause of hip fractures in elderly people. Such fractures take a lot of time to heal even after a surgery is performed. In addition to this, the majority of such injuries prove to be fatal due to the lack of quick communication and immediate medical response. This situation is pretty common in today's world where the elderly are most of the time unattended at home. Owing to this need, we have designed a system to detect such falls leveraging machine learning and signal processing algorithms deployed over a simple 32-bit micro-controller. To achieve higher accuracy, we prepared our custom dataset of various types of fall as well as other daily routine activities. Our device informs the close relatives/family via a GSM Module when a fall is detected. The main purpose of our system is to detect a fall and trigger the alert system and take immediate action to minimize the impact of the fall. Our system has been able to detect a fall within 0.25 seconds with high accuracy. This can further be used to develop a real-time safety mechanism gear to minimize the injury in case of a fall."
262,unknown,10.1109/icit.2017.7915520,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7915520/,2017-03-25 00:00:00,an intelligent maintenance planning framework prototype for production systems,"The Intelligent Maintenance Planner (IMP) is designed to automate and improve maintenance processes in industrial applications. The system tracks the entire process cycle beginning with data acquisition and management, it then detects and classifies failure states, initializes maintenance cases, and selects and assigns the required resources. IMP guides maintenance work processes, by automatically providing instructions and augmented reality information. Subsequent feedback of the maintenance process and new or updated information is added to the system and used to train selection algorithms. A prototype of IMP was implemented based on an industrial SCADA system and cloud solutions for storage and machine learning capabilities. This report explains the stages of the maintenance process and provides an outline of the implementation and project results."
263,unknown,10.1109/tase.2020.3048056,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9326384/,2022-04-01 00:00:00,intelligent fault diagnosis for large-scale rotating machines using binarized deep neural networks and random forests,"Recently, deep neural network (DNN) models work incredibly well, and edge computing has achieved great success in real-world scenarios, such as fault diagnosis for large-scale rotational machinery. However, DNN training takes a long time due to its complex calculation, which makes it difficult to optimize and retrain models. To address such an issue, this work proposes a novel fault diagnosis model by combining binarized DNNs (BDNNs) with improved random forests (RFs). First, a BDNN-based feature extraction method with binary weights and activations in a training process is designed to reduce the model runtime without losing the accuracy of feature extraction. Its generated features are used to train an RF-based fault classifier to relieve the information loss caused by binarization. Second, considering the possible classification accuracy reduction resulting from those very similar binarized features of two instances with different classes, we replace a Gini index with ReliefF as the attribute evaluation measure in training RFs to further enhance the separability of fault features extracted by BDNN and accordingly improve the fault identification accuracy. Third, an edge computing-based fault diagnosis mode is proposed to increase diagnostic efficiency, where our diagnosis model is deployed distributedly on a number of edge nodes close to the end rotational machines in distinct locations. Extensive experiments are conducted to validate the proposed method on the data sets from rolling element bearings, and the results demonstrate that, in almost all cases, its diagnostic accuracy is competitive to the state-of-the-art DNNs and even higher due to a form of regularization in some cases. Benefited from the relatively lower computing and storage requirements of BDNNs, it is easy to be deployed on edge nodes to realize real-time fault diagnosis concurrently. <i>Note to Practitioners</i>&#x2014;Rotating machines, such as engines and motors, are the cornerstones of the modern industry. Edge computing is an emerging computing paradigm where computation is performed on the edges of networks rather than on the central cloud, thereby reducing system response time, transmission overhead, storage space, and computation resources of the cloud. Motivated by the high demand on computation for deploying DNN models and lower computation complexity for running BDNN models and easiness for large-scale deployment of BDNNs, an edge computing-based method for real-time fault diagnosis of rotating machines is proposed. First, we design a BDNN-based feature extractor to decrease the amount of computation and speed up a diagnosis processes. Then, the resulting binary features are fed to train an RF-based classifier, where we use ReliefF instead of Gini index when training a random forest model to further improve the proposed method&#x2019;s diagnostic accuracy. Finally, a novel cloud-edge collaborative computing-based fault diagnostic mode is presented, where the model trained from the central cloud is deployed on the edge computing devices distributed in large-scale scenarios to realize real-time fault diagnosis. Experiment results show that the proposed method can maintain the desired accuracy but greatly enhance the diagnosis speed when deployed on the edge nodes near end physical machines. It is easily extended and used for fault detection in many industrial sectors."
264,unknown,10.1109/iemtronics55184.2022.9795771,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9795771/,2022-06-04 00:00:00,a real-time parking space occupancy detection using deep learning model,"A camera is a tool to record visual footage in the form of photographs, film or in video format. However, a smart camera can be recognized as a device to retrieve application-specific information from the recorded footage. In this paper, we have proposed a solution to detect parking lot occupancy status using deep learning model and commercially used CCTV cameras in real time. Our implemented solution is decentralized and efficient in terms of light-weight deployment to low powered devices like Raspberry Pi. Our proposed solution is compared with the existing approaches. Our deep learning model is also tested on other datasets having images taking from multiple CCTV camera implemented in different height. Along with this, we have tested our model on indoor both outdoor parking garages in low light conditions during day and evening. Result of the performed experiments shows that our model is operable in low-powered embedded devices with effective accuracy."
265,unknown,http://arxiv.org/abs/2205.10635v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2205.10635v1,2022-05-21 00:00:00,"splitplace: ai augmented splitting and placement of large-scale neural
  networks in mobile edge environments","In recent years, deep learning models have become ubiquitous in industry and
academia alike. Deep neural networks can solve some of the most complex
pattern-recognition problems today, but come with the price of massive compute
and memory requirements. This makes the problem of deploying such large-scale
neural networks challenging in resource-constrained mobile edge computing
platforms, specifically in mission-critical domains like surveillance and
healthcare. To solve this, a promising solution is to split resource-hungry
neural networks into lightweight disjoint smaller components for pipelined
distributed processing. At present, there are two main approaches to do this:
semantic and layer-wise splitting. The former partitions a neural network into
parallel disjoint models that produce a part of the result, whereas the latter
partitions into sequential models that produce intermediate results. However,
there is no intelligent algorithm that decides which splitting strategy to use
and places such modular splits to edge nodes for optimal performance. To combat
this, this work proposes a novel AI-driven online policy, SplitPlace, that uses
Multi-Armed-Bandits to intelligently decide between layer and semantic
splitting strategies based on the input task's service deadline demands.
SplitPlace places such neural network split fragments on mobile edge devices
using decision-aware reinforcement learning for efficient and scalable
computing. Moreover, SplitPlace fine-tunes its placement engine to adapt to
volatile environments. Our experiments on physical mobile-edge environments
with real-world workloads show that SplitPlace can significantly improve the
state-of-the-art in terms of average response time, deadline violation rate,
inference accuracy, and total reward by up to 46, 69, 3 and 12 percent
respectively."
266,unknown,10.1364/jocn.449009,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9721710/,2022-04-01 00:00:00,scalability analysis of machine learning qot estimators for a cloud-native sdn controller on a wdm over sdm network,"Maintaining a good quality of transmission (QoT) in optical transport networks is key to maintaining the service level agreement between the user and the service provider. QoT prediction techniques have been used to assure the quality of new lightpaths as well as that of the previously provisioned ones. Traditionally, two different approaches have been used: analytical methods, which take into account most physical impairments that are accurate but complex, and high margin formulas, which require much less computational resources at the cost of high margins. With the recent progress of machine learning (ML) together with software defined networking (SDN), ML has been considered as another option that could be both accurate and that does not consume as many resources as analytical methods. SDN architectures are difficult to scale because they are usually centralized; this is even worse with QoT predictors using ML. In this paper, a solution to this issue is presented using a cloud-native architecture, and its scalability is evaluated using three different ML QoT predictors and experimentally validated in a real wavelength-division multiplexing (WDM) over spatial-division multiplexing (SDM) testbed."
267,unknown,10.1109/oceanse.2019.8867398,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8867398/,2019-06-20 00:00:00,ocean of things : affordable maritime sensors with scalable analysis,"DARPA's Ocean of Things (OoT) program enables persistent maritime situational awareness over large ocean areas by deploying thousands of low-cost, intelligent floats that drift as a distributed sensor network. Each float manages a suite of commercially available sensors to collect environmental data such as sea surface temperature, sea state, and location as well as activity data about vessels and marine mammals moving across the ocean. The floats periodically transmit processed data, or immediately report events based on internal prioritization schemes. Messages travel via commercial satellite to a government cloud for storage and real-time analysis. Cloud-based data analytics feature machine learning aimed at discovering emergent features and behaviors from sparse data. The multiple performers manufacturing floats and developing software are being led by a government management team to employ commercial design methodology and agile best practices. At-sea float deployments are planned in two phases over 2019 (1-month) and 2020 (3-month). Program benefits include ocean environmental products derived from high-density, in-situ measurements and analytical applications, which can simultaneously provide users a range of outputs to include ocean circulation prediction, vessel and marine mammal tracking, and dynamic ocean resource management."
268,unknown,10.1109/bibm52615.2021.9669704,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9669704/,2021-12-12 00:00:00,web platform for medical deep learning services,"In the last decade, deep learning has been transforming the healthcare scenario with the provision of new computer-assisted diagnosis tools in an unprecedented way. The spread of specialized hardware and software has been supporting this reality. The development of such predictive models, however, requires expertise and time. In this context, this article proposes and describes the implementation of a no-coding software framework for the automation of deep learning training processes, aiming to support the development of medical image diagnostic classifiers by healthcare professionals with limited expertise in deep learning. It is a web solution that allows the easy management of data, models, and training processes associated with user requests, where the benefits extend to non-experts in the machine learning field. It is an intuitive web solution that contains a set of additional modern tools like Automatic Machine Learning (AutoML) to aid design models and a ranking system to help improve and share them."
269,unknown,10.1109/iccworkshops49005.2020.9145434,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9145434/,2020-06-11 00:00:00,an inter-disciplinary modelling approach in industrial 5g/6g and machine learning era,"Unlike conventional cellular systems, the fifth generation (5G) and beyond includes intrinsic support for vertical industries with diverse service requirements. Industrial process automation with autonomous fault detection and prediction, optimised operations and proactive control can be considered as one of the key verticals of 5G and beyond. Such applications enable equipping industrial plants with a reasoning sixth sense for optimised operations and fault avoidance. In this direction, we introduce an inter-disciplinary approach integrating wireless sensor networks with machine learning-enabled industrial plants to build a step towards developing this sixth sense technology, i.e., the reasoning ability. We develop a modular-based system that can be adapted to the vertical-specific elements. Without loss of generalisation, exemplary use cases are developed and presented including a fault detection/prediction scheme in a wireless communication network with sensors and actuators to enable the sixth sense technology with guaranteed service load requirements. The proposed schemes and modelling approach are implemented in a real chemical plant for testing purposes, and a high fault detection and prediction accuracy is achieved coupled with optimised sensor density analysis."
270,unknown,10.1109/uemcon51285.2020.9298178,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9298178/,2020-10-31 00:00:00,anomaly detection and identification using visual techniques in streaming video,"There are many intelligent systems and tools which uses highly efficient processing models to identify different anomalies with high accuracy. The anomaly detection is of high importance and mostly will come as an absolute requirement at high risk environments and situations. The amount of processing involved in quick decision taking systems bare high deployment costs which restricts the anomaly detection only to a selected few who are capable of building such resource centered systems. Modern world uses drones and other video feeds in order to find and keep track of any anomalous events around a specific area. But most such detection requires absolute manual attention as well as processing power to keep up with real time detection and recognition. The proposed research solution aims to automate this process and includes a two-step anomaly detection system which gives a quicker anomaly detection in an average processing unit time with an advanced recognition model with up to 90% accuracy. The deep learning model (VGG 16) together with alert system and comparison techniques on videos leads into unsupervised anomaly detection of a landscape. The system generates alerts and recognizes anomalies on the alerted video frames. The proposed solution can also be used by any source and does not require high capacity of capability system to get the optimal output. Moreover, the solution brings a simple yet sophisticated technique to address modern anomaly detection and quick alerting system."
271,unknown,10.1109/ism52913.2021.00039,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9666077/,2021-12-01 00:00:00,combining linked open data and multimedia knowledge base for digital cultural heritage robotic applications,"The current trends in society evolution are showing rapid changes in our habitual environments and consequently affecting human interactions with them. Among the key factors of this process we can certainly cite the growth of BigData favored by the knowledge digitalization, the dissemination of sensors in environments and the advancements of connectivity capabilities. At the same time, the progress in artificial intelligence and cognitive robotics has lead to the production of sophisticated humanoid robots, which are progressively spreading to a wide public. In this way, research efforts are needed for a proper knowledge management and knowledge acquisition by machines, in order to have more natural and friendly human-robot interactions in daily tasks, usually performed by human beings. In this paper we show an approach related to a human-robot interaction in cultural heritage context, simulating a digital ecosystem where a robot plays the role of a guide for tourists and it is able to proactively interact with its interlocutors by combining both semantic and visual information. The proposed approach, its implementation and experimental results on a real robotic platform are shown and discussed."
272,unknown,10.1109/icstcee54422.2021.9708587,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9708587/,2021-12-17 00:00:00,logistic regression versus xgboost: machine learning for counterfeit news detection,"In this age of globalization, the unstoppable spreading of fake news via the internet is unstoppable. The spread of false news cannot be supported due to the negative consequences. Society is extremely concerning. In addition, itleads to more serious problems and possible threats, like confusion, misunderstandings, defamation and falsehoods that induce users to share inflammatory content. With the convenience and tremendous increase in information gathering on social networks, it is becoming difficult to differentiate between what is false and what is real. Information can be easily disseminated through sharing, which has contributed to the exponential growth of their forgeries. Machine learning played an important role, in classifying information, although there are some limitations. This article explores various machine learning techniques used to detect fake and fabricated messages. The limitations are discussed using deep learning implementation. In this project, the methodology used is model development and Logistic Regression classifier is considered to detect false news. Based on previous research, this classifier performed well in classification tasks. In this approach, TF-IDF feature is used for the construction of this fake news model to get higher accuracy. The goal of this project is to detect false news using NLP and Machine Learning based on the news content of the article. Following the development of the appropriate Machine Learning model to detect fake/true news, it is deployed into a web interface using Python Flask."
273,unknown,10.1109/icrito48877.2020.9198018,IEEE,ieeexplore,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9198018/,2020-06-05 00:00:00,augmenting banking and fintech with intelligent internet of things technology,"Banks and Financial institutions play a crucial role in economic development of country albeit slow to embrace IoT technology. IoT has the potential to revolutionize financial services by enabling a deeper understanding of economic trends, customer preferences and capturing their needs and preferences in real time. Integrating Artificial Intelligence in IoT enhances IoT performance by analysing the information and extracting knowledge therefore making them more intelligent. The FinTech(Financial Technology) needs to be amalgamated with Intelligent IoT to improve customer services, gain customer's perspicacity and to improve efficiency. This paper proffers a cloud centric Intelligent Internet of Things conceptual architecture for banking and financial sector. Furthermore a case study of implementing Intelligent IoT for vehicle loans has also been presented."
274,unknown,10.1016/j.compchemeng.2012.06.021,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84869501412,2012-12-20,smartgantt - an interactive system for generating and updating rescheduling knowledge using relational abstractions,"Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy."
275,unknown,10.1007/s11761-020-00292-z,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11761-020-00292-z,2020-09-01 00:00:00,could or could not of grid-loc: grid ble structure for indoor localisation system using machine learning,"Indoor localisation and its various applications have received significant attention in recent years. The state-of-the-art systems include a large number of complex hardware structures and algorithms making the system not suitable for practical applications. In this paper, we integrate a localisation system that consists of device development, model deployment, data collection and localisation algorithm to explore the localisation accuracy in a special static indoor environment (i.e. a meeting room or a parking lot). Compared with previous studies, the significance of our work is to find out a more convenient and practical way to deploy devices with a simple algorithm (e.g. machine learning algorithm) in such a scenario. Besides, it is meaningful to explore the technology of indoor localisation based on the application scenario. We propose a Grid-Loc system that presents a grid structure of Bluetooth low-energy devices to collect data assisting localisation. The system is easy to deploy for reducing the signal attenuation caused by the objects’ occlusion. Meanwhile, the system applies an algorithm that combines adaptive boosting with a support vector machine algorithm to support the system. In our deployed localisation scenario, we also compare localisation performances for several algorithms; the result shows the Grid-Loc system achieves the accuracy of 91.2%, computing time within 3 s in real time and a low cost. The system is also robust and scalable under the same indoor environments."
276,unknown,10.1038/s41598-022-07764-6,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s41598-022-07764-6,2022-03-08 00:00:00,real-time infection prediction with wearable physiological monitoring and ai to aid military workforce readiness during covid-19,"Infectious threats, like the COVID-19 pandemic, hinder maintenance of a productive and healthy workforce. If subtle physiological changes precede overt illness, then proactive isolation and testing can reduce labor force impacts. This study hypothesized that an early infection warning service based on wearable physiological monitoring and predictive models created with machine learning could be developed and deployed. We developed a prototype tool, first deployed June 23, 2020, that delivered continuously updated scores of infection risk for SARS-CoV-2 through April 8, 2021. Data were acquired from 9381 United States Department of Defense (US DoD) personnel wearing Garmin and Oura devices, totaling 599,174 user-days of service and 201 million hours of data. There were 491 COVID-19 positive cases. A predictive algorithm identified infection before diagnostic testing with an AUC of 0.82. Barriers to implementation included adequate data capture (at least 48% data was needed) and delays in data transmission. We observe increased risk scores as early as 6 days prior to diagnostic testing (2.3 days average). This study showed feasibility of a real-time risk prediction score to minimize workforce impacts of infection."
277,unknown,10.1109/iccmst54943.2021.00014,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9784593/,2021-12-18 00:00:00,internet of things-based middleware against cyber-attacks on smart homes using software-defined networking and deep learning,"Internet of Things (IoT) devices are expected to number about 3.5 billion by 2023.; a tremendous amount of Internet of Things (IoT) data that is generating (IoT) devices is estimated to exceed 79.4 zettabytes by 2025. Security challenges will become an increasingly significant issue, especially in smart homes that depend entirely on IoT devices. Due to the weak infrastructure of the IoT, it is vulnerable to various types of cyber-attacks. The most common IoT attacks are distributed denial of service (DDoS). Most traditional security solutions, like intrusion detection systems (IDS), cannot detect most attacks. Complexity is being hidden by a new paradigm that recently arose, called the software-defined system that is brings a significant change to the networking industry, a great solution for mitigation of attacks that can adopt deep learning technique to encounter cyber-attacks based on the attack behavior and by filtering normal and attack traffic by using well-defined rules. This paper proposed a system by suggesting middleware that can help mitigate or prevent various attacks on IoT on smart home environment. Machine learning has included in the middleware to provide automatic protection against cyber-attacks on IoT networks. A promising approach to protecting real-time, highly accurate attacks on SDN-managed IoT networks has been proposed. This middleware allows IoT devices to efficiently handle evolving security threats dynamically and adaptively without impacting the IoT devices."
278,unknown,10.1109/i2ct54291.2022.9825287,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9825287/,2022-04-09 00:00:00,real time protein crystal monitoring system,"High-resolution crystal structures of the biological macromolecules (protein, DNA, RNA or their complexes) are essential for understanding their biological functions and causes of diseases at a molecular level. X-ray crystallographic technique gives the atomic positions of these biological macromolecules and decodes the intermolecular interactions between protein-drug, protein-protein, protein-nucleic acids which are crucial for rational drug design. To aid the macromolecular crystallization experiments, we have indigenously developed automated system, called Real Time Protein Crystal Monitoring System (RT-PCMS) for crystal imaging and to monitor the progress of protein crystal growth during crystallization. RT-PCMS images crystallization drops at frequent time intervals in 24- or 96-well crystallization plate formats. The system consists of precise robotics motion and a custom designed motorized microscopic imaging system, capturing multi-focus composite images of protein crystals in droplets in multiple wells. In a typical successful trial, the crystallization drop comprises protein crystals of size 20 to 300 microns located at the different regions of an elliptical droplet. We have implemented powerful image processing and deep learning algorithms in this system, fusing multi-depth crystal images within a crystallization drop and classification into different crystallization stages. The system is controlled through a personal computer that provides a custom developed graphical user interface software for visualization of all captured images at different crystallization stages and record the results of crystal formation."
279,unknown,10.1016/j.procs.2019.09.169,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076257910,2019-01-01,iassistme - adaptable assistant for persons with eye disabilities,"Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers."
280,unknown,http://arxiv.org/abs/2102.08936v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.08936v2,2021-02-17 00:00:00,"deep learning anomaly detection for cellular iot with applications in
  smart logistics","The number of connected Internet of Things (IoT) devices within
cyber-physical infrastructure systems grows at an increasing rate. This poses
significant device management and security challenges to current IoT networks.
Among several approaches to cope with these challenges, data-based methods
rooted in deep learning (DL) are receiving an increased interest. In this
paper, motivated by the upcoming surge of 5G IoT connectivity in industrial
environments, we propose to integrate a DL-based anomaly detection (AD) as a
service into the 3GPP mobile cellular IoT architecture. The proposed
architecture embeds autoencoder based anomaly detection modules both at the IoT
devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing
between the system responsiveness and accuracy. We design, integrate,
demonstrate and evaluate a testbed that implements the above service in a
real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT)
mobile operator network."
281,unknown,10.1016/j.matpr.2021.01.072,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85116453644,2021-01-01,flash flood risk management modeling in indian cities using iot based reinforcement learning,"Each year, flash floods in India have affected life, infrastructure and the economy of the country and there is a need for a systemic model of real-time flash flood management. So, to mitigate these losses, we proposed a flash flood management model focused on reinforcement learning. Based on their severity, the flash flood data is collected and rewards are distributed and this data is compared to the data collected from smart IoT devices deployed in the region impacted by the flood. We allocate the state-based reward values once the comparison is completed. The evacuation of flash flood water through the contour of the flash flood is carried out. This bypass has gates at different points that mean that, depending on the incentives, the gates are opened to remove flash flood water. The proposed solution was evaluated to be a faster, more effective and more accurate real-time flash flood management method."
282,unknown,10.1109/platcon53246.2021.9680756,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9680756/,2021-08-25 00:00:00,design and implementation of real-time bio signals management system based on hl7 fhir for healthcare services,"Recently, attention has been focused on services that combine medical technology with ICT technologies such as artificial intelligence, big data, Internet of Things, and block chains. In addition, Research on healthcare services that can collect bio signal data through wearable sensors using IoT technology and monitor and manage health based on the collected data is increasing significantly. In particular, in a situation where the world is entering a rapidly aging society, health care services are being researched and developed in the direction of preventing diseases in advance and maintaining a healthy life. Healthcare services are bringing important changes in the pandemic era caused by covid-19. There is a need for a system capable of efficiently sharing and exchanging information of heterogeneous services to prevent emergencies and support optimal medical services. In this paper, we designed and developed a system that can collect, convert, and store bio-signals from various wearable sensors into international standard data to develop such healthcare services. HL7 (health level seven) FHIR (fast healthcare interoperability resources) applied mutandis in this paper is a standard protocol for data exchange between medical information systems of real-time collected bio signals. In this paper, we implement an interface module that converts bio signals such as EEG (electroencephalography), ECG (electrocardiogram), EMG (electromyography), and PPG (photoplethysmography) collected in real time from a wearable sensor into a message structure defined by HL7 FHIR. The interface module consists of a client part and a server part. The client part generates a variety of signal data from the healthcare service user and delivers the message to the server part. The server part is designed and implemented to parse the received message by segment field unit and transmit whether the message is abnormal or not to the client part. The system designed and implemented in this paper will be utilized as a technology that can mutually share and exchange medical information in a customized healthcare service that reflects the needs of various customers and a telemedicine system."
283,unknown,10.1109/besc.2014.7059529,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7059529/,2014-11-01 00:00:00,comuptional reasoning and learning for smart manufacturing under realistic conditions,"Smart manufacturing has increasingly become a prominent research topic across the academia and industry during recent years. However, in the real world, most factories' conditions are normally insufficient for implementing full scale smart manufacturing. In order to increase smartness for manufacturing systems under different situations, one may observe that computational reasoning and learning, including latest machine learning methodology and traditional rule based systems, are able to offer potential powerful theoretical foundations as well as technical tools for enabling such smarter systems. Unfortunately, few studies exist to propose a practical yet systematic procedure that implements computational reason and learning to implement smartness for typical manufacturing systems. This paper proposes a general computational reasoning and learning framework to describe the key functions in smart manufacturing under the ""three Fs in one"" system framework, namely a system of interconnected data, integrated automation, and intelligent information. Among three Fs, the intelligent information plays the most important role in working towards smartness by connecting the other two Fs. Furthermore, to achieve it, a learning enabled comprehensive multi-agent decision model is developed. In particular, we first design a computational learning based architecture for analyzing support information for manufacturing processes. Then we provide an optimization architecture that enables realtime learning for a manufacturing process. At last, we employ a rule based learning system to integrate these two architectures to facilitate self-evolution of the manufacturing system. The advantages of our procedure include adaptive responses to dynamic environment, efficient computations, and abilities to fulfill complex manufacturing processes, which are demonstrated by a specific modeling procedure."
284,unknown,10.2118/205465-ms,"Day 4 Fri, September 10, 2021",semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/25ee6b6cffcc3ff756bc53c30c3400d44d0fff1a,2021-01-01 00:00:00,industry first ai-powered fully automated safety observation system deployed to global offshore fleet,"
 As the oil and gas industry is facing tumultuous challenges, adoption of cutting-edge digital technologies has been accelerated to deliver safer, more efficient operations with less impact on the environment.
 While advanced AI and other digital technologies have been rapidly evolving in many fields in the industry, the HSE sector is playing catch-up. With the increasing complexity of risks and safety management processes, the effective application of data-driven technologies has become significantly harder, particularly for international organizations with varying levels of digital readiness across diverse global operations. Leaders are more cautious to implement solutions that are not fit-for purpose, due to concerns over inconsistencies in rolling out the program across international markets and the impact this may have on ongoing operations.
 This paper describes how the effective application of Artificial intelligence (AI) and Machine Learning (ML) technologies have been used to engineer a solution that fully digitizes and automates the end-to-end offshore behavior-based safety program across a global offshore fleet; optimizing a critical safety process used by many leading oil & gas organization to drive positive workplace safety culture. The complex safety program has been transformed into clear, efficient and automated workflow, with real-time analytics and live transparent dashboards which detail critical safety indicators in real time, aiding decision-making and improving operational performance.
 The novel behavior-based safety digital solution, referred to as 3C observation tool within Noble drilling, has been built to be fully aligned with the organization's safety management system requirements and procedures, using modern and agile tools and applications for fully scalability and easy deployment. It has been critical in sharpening the offshore safety observation program across global operations, resulting in a boost of the workforce engagement by 30%, and subsequently increasing safety awareness skill set attainment; improving overall offshore safety culture, all while reducing operating costs by up to 70% and cutting carbon footprint through the elimination of 15,000 manhours and half a million paper cards each year, when compared to previously used methods and workflows"
285,unknown,10.1109/access.2022.3141913,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9676574/,2022-01-01 00:00:00,decentralized federated learning for healthcare networks: a case study on tumor segmentation,"Smart healthcare relies on artificial intelligence (AI) functions for learning and analysis of patient data. Since large and diverse datasets for training of Machine Learning (ML) models can rarely be found in individual medical centers, classical centralized AI requires moving privacy-sensitive data from medical institutions to data centers that process the fused information. Training on data centers thus requires higher communication resource/energy demands while violating privacy. This is considered today as a significant bottleneck in pursuing scientific collaboration across trans-national clinical medical research centers. Recently, federated learning (FL) has emerged as a distributed AI approach that enables the cooperative training of ML models, without the need of sharing patient data. This paper dives into the analysis of different FL methods and proposes a real-time distributed networking framework based on the Message Queuing Telemetry Transport (MQTT) protocol. In particular, we design a number of solutions for ML over networks, based on FL tools relying on a parameter server (PS) and fully decentralized paradigms driven by consensus methods. The proposed approach is validated in the context of brain tumor segmentation, using a modified version of the popular U-NET model with representative clinical datasets obtained from the daily clinical workflow. The FL process is implemented on multiple physically separated machines located in different countries and communicating over the Internet. The real-time test-bed is used to obtain measurements of training accuracy vs. latency trade-offs, and to highlight key operational conditions that affect the performance in real deployments."
286,unknown,10.1109/icmla.2013.142,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6786127/,2013-12-07 00:00:00,epg content recommendation in large scale: a case study on interactive tv platform,"Recommender systems in TV applications mostly focusing on the recommendation of video-on-demand (VOD) content, though the major part of users' content consumption is realized on linear channel programs, termed EPG content. In this case study we present how we tackled the EPG recommendation task, which exhibits several differences compared to the VOD scenario, including the lack of explicit user feedbacks, the magnitude of cold start problem, as well as data cleaning and feature selection necessary to be applied on raw consumption data. We provide both offline and online model validation. First we showcase the typical approach in machine learning by evaluating models against recall in an offline setting. Then, we investigate in depth the real-world results of the recommendation app using the pre-trained models, and analyze how personalized recommendation influence users watching behavior. The experimentation results are based on our recommender system deployed at a Canadian IPTV service provider using Microsoft Media room middleware."
287,unknown,http://arxiv.org/abs/1909.13343v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.13343v2,2019-09-29 00:00:00,"isthmus: secure, scalable, real-time and robust machine learning
  platform for healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions."
288,unknown,10.1109/icasi55125.2022.9774490,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9774490/,2022-04-23 00:00:00,application of augmented reality for aviation equipment inspection and maintenance training,"In the procedure of aviation equipment maintenance, a healthy status and prior knowledge are essential to every person. Making the newcomers get familiar with their tasks rapidly and reducing human error are important issues. Augmented reality (AR) integrated with the popular artificial intelligence (AI) technology can provide smart system inspections and train maintenance professionals on how to perform important maintenance procedures effectively and accurately. Utilization of the AR not only replaces classic manual training but also provides high mobility pre-training opportunities. In other words, AR is capable of taking equipment maintenance to the next level. In this paper, an application for aviation equipment inspection is developed. By using AR and AI, traditional standard operation procedures (SOP) can be visualized and standardized. Therefore, people who are not familiar with the maintenance procedure can still finish the standard inspections. The developed software gives a great contribution to human resources for maintenance training. Demonstrations including turbofan and landing gear are provided by using HoloLens 2 to illustrate the application novelty. Finally, pros and cons regarding maintenance using AR are also discussed."
289,unknown,10.1109/snpd.2017.8022765,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8022765/,2017-06-28 00:00:00,recognizing adls of one person household based on non-intrusive environmental sensing,"Pervasive sensing technologies are promising for increasing one-person households (OPH), where the sensors monitor and assist the resident to maintain healthy life rhythm. Towards the practical use, the recognition of activities of daily living (ADL) is an important step. Many studies of the ADL recognition have been conducted so far, for real-life and human-centric applications such as eldercare and healthcare. However, most existing methods have limitations in deployment cost, privacy exposure, and inconvenience for residents. To cope with the limitations, this paper presents a new indoor ADL recognition system especially for OPH. To minimize the deployment cost as well as the intrusions to user and house, we exploit an IoT-based environment-sensing device, called Autonomous Sensor Box (SensorBox) which can autonomously measure 7 kinds of environment attributes. We apply machine-learning techniques to the collected data, and predicts 7 kinds of ADLs. We conduct an experiment within an actual apartment of a single user. The result shows that the proposed system achieves the average accuracy of ADL recognition with more than 88%, by carefully developing the features of environment attributes."
290,unknown,10.1109/iecbes48179.2021.9398735,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9398735/,2021-03-03 00:00:00,high-intensity interval training exercise recognition using smartwatch,"The use of a smartwatches to enable human activity recognition has brought forth immersive applications. This paper presents an end-to-end approach using deep learning to recognise physical exercises from a commercially available smartwatch. The exercises are recognised based on two different settings namely; constrained and unconstrained workouts in the form of High-Intensity Interval Training. The model reported a 97.35% accuracy for constrained exercise recognition, and a 82.29% accuracy for unconstrained exercise recognition. This method is capable of recognising 18 High-Intensity Interval Training exercises. The model was deployed to Google Cloud Platform to recognise exercises in real-time settings. The method will be further expanded to operate as a real-time “Fitness Coach”, which could automatically suggest optimal workout plans for users and monitor their health conditions during workout sessions."
291,unknown,10.3390/s21020434,Sensors,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/4fe978ebae006e1203d7254aa9d70ada7ff00758,2021-01-01 00:00:00,image-based automatic watermeter reading under challenging environments,"With the rapid development of artificial intelligence and fifth-generation mobile network technologies, automatic instrument reading has become an increasingly important topic for intelligent sensors in smart cities. We propose a full pipeline to automatically read watermeters based on a single image, using deep learning methods to provide new technical support for an intelligent water meter reading. To handle the various challenging environments where watermeters reside, our pipeline disentangled the task into individual subtasks based on the structures of typical watermeters. These subtasks include component localization, orientation alignment, spatial layout guidance reading, and regression-based pointer reading. The devised algorithms for orientation alignment and spatial layout guidance are tailored to improve the robustness of our neural network. We also collect images of watermeters in real scenes and build a dataset for training and evaluation. Experimental results demonstrate the effectiveness of the proposed method even under challenging environments with varying lighting, occlusions, and different orientations. Thanks to the lightweight algorithms adopted in our pipeline, the system can be easily deployed and fully automated."
292,unknown,10.1109/isscs.2013.6651227,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6651227/,2013-07-12 00:00:00,a face recognition system based on a kinect sensor and windows azure cloud technology,"The aim of this paper is to build a system for human detection based on facial recognition. The state-of-the-art face recognition algorithms obtain high recognition rates base on demanding costs - computational, energy and memory. The use of these classical algorithms on an embedded system cannot achieve such performances due to the existing constrains: computational power and memory. Our objective is to develop a cheap, real time embedded system able to recognize faces without any compromise on system's accuracy. The system is designed for automotive industry, smart house application and security systems. To achieve superior performance (higher recognition rates) in real time, an optimum combination of new technologies was used for detection and classification of faces. The face detection system uses skeletal-tracking feature of Microsoft Kinect sensor. The face recognition, more precisely - the training of neural network, the most computing-intensive part of the software, is achieved based on the Windows Azures cloud technology."
293,unknown,10.1016/j.heliyon.2022.e09634,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85131417211,2022-06-01,smart deployment of iot-telosb service care streamrobot using software-defined reliability optimisation design,"Intelligent service care robots have increasingly been developed in mission-critical sectors such as healthcare systems, transportation, manufacturing, and environmental applications. The major drawbacks include the open-source Internet of Things (IoT) platform vulnerabilities, node failures, computational latency, and small memory capacity in IoT sensing nodes. This article provides reliable predictive analytics with the optimisation of data transmission characteristics in StreamRobot. Software-defined reliable optimisation design is applied in the system architecture. For the IoT implementation, the edge system model formulation is presented with a focus on edge cluster log-normality distribution, reliability, and equilibrium stability considerations. A real-world scenario for accurate data streams generation from in-built TelosB sensing nodes is converged at a sink-analytic dashboard. Two-phase configurations, namely off-taker and on-demand, link-state protocols are mapped for deterministic data stream offloading. An orphan reconnection trigger mechanism is used for reliable node-to-sink resilient data transmissions. Data collection is achieved, using component-based programming in the experimental testbed. Measurement parameters are derived with TelosB IoT nodes. Reliability validations on remote monitoring and prediction processes are studied considering neural constrained software-defined networking (SDN) intelligence. An OpenFlow-SDN construct is deployed to offload traffic from the edge to the fog layer. At the core, fog detection-to-cloud predictive machine learning (FD-CPML) is used to predict real-time data streams. Prediction accuracy is validated with decision tree, logistic regression, and the proposed FD-CPML. The data streams latency gave 40.00%, 33.33%, and 26.67%, respectively. Similarly, linear predictive scalability behaviour on the network plane gave 30.12%, 33.73%, and 36.15% respectively. The results show satisfactory responses in terms of reliable communication and intelligent monitoring of node failures."
294,unknown,10.1109/wf-iot.2019.8767291,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8767291/,2019-04-18 00:00:00,mountain pine beetle monitoring with iot,"Outbreaks of forest pests cause large-scale damages, which lead to significant impact on the ecosystem as well as the forestry industry. Current methods of monitoring pest outbreaks involve field, aerial and remote sensing surveys. These methods only provide partial spatial coverage and can detect outbreaks only after they have substantially progressed across wide geographic areas. This paper presents an IoT system for real-time insect infestation detection using bioacoustic recognition via machine learning techniques. Specifically, we focus on detecting the Mountain Pine Beetle (MPB), which is the most destructive insect of mature pines in western North American forests. We present the design of the system and describe its various hardware and software components. Experimental results collected from a prototype implementation of the system are presented, which show that the system can detect MPB with 82% accuracy. We also demonstrate the applicability of our system in other noise monitoring applications, and report our experimental results on urban noise detection and classification."
295,unknown,10.1109/comcas52219.2021.9629097,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9629097/,2021-11-03 00:00:00,using machine learning and virtual reality for orthopedic treatment and abnormality detection based on multivariate time series data,"In this work we present a virtual reality machine-learning system for telehealth orthopedic treatment. Our system can recognize orthopedic abnormalities and the presence of pain. It is based on a widely used virtual reality system, combined with its sensors. We implemented an algorithm that can identify very accurately wrist and neck pain and can serve as a real-time remote system for rehabilitation doctors or physical therapists, as part of a virtual reality telehealth treatment program. Our algorithms synchronize the patient’s movement data with a dedicated data server. The system has an easy-to-use interface for analysis of the collected data. We achieved more than 90% success rates evaluating the presence of neck pain and wrist pain across given exercises for each of our volunteers. Our system can serve as the basis for a real-world telehealth, clinically operative machine."
296,unknown,http://arxiv.org/abs/2205.11267v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2205.11267v1,2022-05-23 00:00:00,"fed-dart and fact: a solution for federated learning in a production
  environment","Federated Learning as a decentralized artificial intelligence (AI) solution
solves a variety of problems in industrial applications. It enables a
continuously self-improving AI, which can be deployed everywhere at the edge.
However, bringing AI to production for generating a real business impact is a
challenging task. Especially in the case of Federated Learning, expertise and
resources from multiple domains are required to realize its full potential.
Having this in mind we have developed an innovative Federated Learning
framework FACT based on Fed-DART, enabling an easy and scalable deployment,
helping the user to fully leverage the potential of their private and
decentralized data."
297,unknown,http://arxiv.org/abs/1606.03966v2,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1606.03966v2,2016-06-13 00:00:00,making contextual decisions with low technical debt,"Applications and systems are constantly faced with decisions that require
picking from a set of actions based on contextual information.
Reinforcement-based learning algorithms such as contextual bandits can be very
effective in these settings, but applying them in practice is fraught with
technical debt, and no general system exists that supports them completely. We
address this and create the first general system for contextual learning,
called the Decision Service.
  Existing systems often suffer from technical debt that arises from issues
like incorrect data collection and weak debuggability, issues we systematically
address through our ML methodology and system abstractions. The Decision
Service enables all aspects of contextual bandit learning using four system
abstractions which connect together in a loop: explore (the decision space),
log, learn, and deploy. Notably, our new explore and log abstractions ensure
the system produces correct, unbiased data, which our learner uses for online
learning and to enable real-time safeguards, all in a fully reproducible
manner.
  The Decision Service has a simple user interface and works with a variety of
applications: we present two live production deployments for content
recommendation that achieved click-through improvements of 25-30%, another with
18% revenue lift in the landing page, and ongoing applications in tech support
and machine failure handling. The service makes real-time decisions and learns
continuously and scalably, while significantly lowering technical debt."
298,unknown,http://arxiv.org/abs/1901.01632v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1901.01632v2,2019-01-07 00:00:00,"exploiting network loss for distributed approximate computing with
  netapprox","Many data center applications such as machine learning and big data analytics
can complete their analysis without processing the complete set of data. While
extensive approximate-aware optimizations have been proposed at hardware,
programming language, and application levels. However, to date, the approximate
computing optimizations have ignored the network layer.
  We propose NetApprox, which to the best of our knowledge, is the first
approximate-aware network layer comprising transport-layer protocol, network
resource allocation schemes, and scheduling/priority-assignment policies.
Building on the observation that approximate applications can tolerate loss,
NetApprox's main insights are to aggressively send approximate traffic (which
improves the performance of approximate applications) and to minimize the
network resources allocated to approximate traffic (which simultaneously limits
the impact of aggressive approximate traffic while freeing up resources that,
in turn, improve non-approximate applications' performance). We ported Flink,
Kafka, Spark, and PyTorch to NetApprox and evaluated NetApprox with both
large-scale simulation and real implementation. Our evaluation results show
that NetApprox improves job completion times by up to 80% compared to
network-oblivious approximation solutions, and improves the performance of
co-running non-approximate workloads by 79%."
299,unknown,10.1109/icmac54080.2021.9678242,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9678242/,2021-12-22 00:00:00,ai-based real-time classification of human activity using software defined radios,"Real-time monitoring is an essential part in the development of healthcare monitoring systems. Research has shown that human movement affects the propagation of radio frequencies, as signals will reflect off the human body. Machine Learning techniques have been used in research to classify patterns observed in the signal propagation. This paper makes use of universal software radio peripheral devices to create a wireless communication link where the signal propagation data, known as channel state information, is collected while a user moves or remains still. A machine learning model which achieved an accuracy result of 93.25 % is used to classify between movement and no activity. Inference is then used to decide if the human position is sitting or standing and detected movements are used to differentiate between the two positions. The testbed implements cloud storage and a web-interface to present a visualisation of the human position."
300,unknown,http://arxiv.org/abs/2105.01058v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2105.01058v2,2021-05-03 00:00:00,"a dataset and system for real-time gun detection in surveillance video
  using deep learning","Gun violence is a severe problem in the world, particularly in the United
States. Deep learning methods have been studied to detect guns in surveillance
video cameras or smart IP cameras and to send a real-time alert to security
personals. One problem for the development of gun detection algorithms is the
lack of large public datasets. In this work, we first publish a dataset with
51K annotated gun images for gun detection and other 51K cropped gun chip
images for gun classification we collect from a few different sources. To our
knowledge, this is the largest dataset for the study of gun detection. This
dataset can be downloaded at www.linksprite.com/gun-detection-datasets. We
present a gun detection system using a smart IP camera as an embedded edge
device, and a cloud server as a manager for device, data, alert, and to further
reduce the false positive rate. We study to find solutions for gun detection in
an embedded device, and for gun classification on the edge device and the cloud
server. This edge/cloud framework makes the deployment of gun detection in the
real world possible."
301,unknown,10.1109/access.2021.3121254,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9582812/,2021-01-01 00:00:00,deepdespy: a deep learning-based wireless spy camera detection system,"Spy cameras planted in various private places, such as motels, hotels, homestays (i.e., Airbnb), and restrooms, have raised immense privacy concerns. Wi-Fi spy cameras are used extensively by various adversaries because of easy installability, followed by size reduction. To prevent invasions of privacy, most studies have detected wireless cameras based on video traffic analysis and require additional synchronous data from external sensors or stimulus hardware to confirm the user’s motion. Such supplements make the users uncomfortable, requiring extra effort and time for setting. This paper proposes an effective spy camera detection system called DeepDeSpy to detect the recording of a spy camera with no effort from the user. The core idea is using the channel state information (CSI) and the network traffic from the camera to detect whether the wireless camera records the movements of the user. The CSI signal is prone to motion, and detecting motion from an enormous amount of CSI data in real-time is challenging. This was handled by leveraging the convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) deep learning methods. Such synergistic CNN and BiLSTM deep learning models enable instant and accurate detection by automatically extracting meaningful features from the sequential raw CSI data. The feasibility of DeepDeSpy was verified by implementing it on both a PC and a smartphone and evaluating it in real-life scenarios (e.g., various room sizes and user physical activities). The average accuracy achieved in different real-life settings was approximately 96%, reaching 98.9% with intensive physical activity in the large-size room. Moreover, the ability to achieve instant detection on a smartphone within only a one-second response time makes it workable for real-time applications."
302,unknown,10.1109/icirca51532.2021.9544776,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9544776/,2021-09-04 00:00:00,software implementation architecture design for online display of animation works based on vr technology,"The design of the online display system of virtual reality animation is at the intersection of art, design, and VR technology. VR technology is a fusion of computer artificial intelligence, simulation technology, display technology, sensor technology and other technologies. In order to solve the shortcomings of the traditional animation display system, this paper introduces VR technology and designs a software architecture that can display animation works online. First, it introduces the research status and application prospects of VR technology. Then I learned about the software architecture of the animation display system. Finally, an animation display system is designed."
303,unknown,10.1109/roman.2009.5326235,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5326235/,2009-10-02 00:00:00,gestural teleoperation of a mobile robot based on visual recognition of sign language static handshapes,"This paper presents results achieved in the frames of a national research project (titled ldquoDIANOEMArdquo), where visual analysis and sign recognition techniques have been explored on Greek Sign Language (GSL) data. Besides GSL modelling, the aim was to develop a pilot application for teleoperating a mobile robot using natural hand signs. A small vocabulary of hand signs has been designed to enable desktopbased teleoperation at a high-level of supervisory telerobotic control. Real-time visual recognition of the hand images is performed by training a multi-layer perceptron (MLP) neural network. Various shape descriptors of the segmented hand posture images have been explored as inputs to the MLP network. These include Fourier shape descriptors on the contour of the segmented hand sign images, moments, compactness, eccentricity, and histogram of the curvature. We have examined which of these shape descriptors are best suited for real-time recognition of hand signs, in relation to the number and choice of hand postures, in order to achieve maximum recognition performance. The hand-sign recognizer has been integrated in a graphical user interface, and has been implemented with success on a pilot application for real-time desktop-based gestural teleoperation of a mobile robot vehicle."
304,unknown,10.1109/access.2017.2756069,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8049520/,2017-01-01 00:00:00,digital twin shop-floor: a new shop-floor paradigm towards smart manufacturing,"With the developments and applications of the new information technologies, such as cloud computing, Internet of Things, big data, and artificial intelligence, a smart manufacturing era is coming. At the same time, various national manufacturing development strategies have been put forward, such as Industry 4.0, Industrial Internet, manufacturing based on Cyber-Physical System, and Made in China 2025. However, one of specific challenges to achieve smart manufacturing with these strategies is how to converge the manufacturing physical world and the virtual world, so as to realize a series of smart operations in the manufacturing process, including smart interconnection, smart interaction, smart control and management, etc. In this context, as a basic unit of manufacturing, shop-floor is required to reach the interaction and convergence between physical and virtual spaces, which is not only the imperative demand of smart manufacturing, but also the evolving trend of itself. Accordingly, a novel concept of digital twin shopfloor (DTS) based on digital twin is explored and its four key components are discussed, including physical shop-floor, virtual shop-floor, shop-floor service system, and shop-floor digital twin data. What is more, the operation mechanisms and implementing methods for DTS are studied and key technologies as well as challenges ahead are investigated, respectively."
305,unknown,10.1109/mwscas.2018.8624056,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8624056/,2018-08-08 00:00:00,emg-based hand gesture control system for robotics,"In this paper, a Electromyogram (EMG) based hand gesture control system is developed. A wearable human machine interface (HMI) device is designed for an in-home assistance service robot. An EMG-based control system utilizes MyoWave muscle sensor to acquire and amplify EMG signal. A microcontroller system is used to an artificial neural network (ANN) to classify the EMG signal. Based on different hand movements, commands are sent through WiFi to control the motor in a service robot. The on-board Camera system mounted the robot can capture video real-time. In addition, a web server is implemented to provide live video feedback for robot navigation and user instructions."
306,unknown,http://arxiv.org/abs/2003.04987v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.04987v1,2020-02-17 00:00:00,a financial service chatbot based on deep bidirectional transformers,"We develop a chatbot using Deep Bidirectional Transformer models (BERT) to
handle client questions in financial investment customer service. The bot can
recognize 381 intents, and decides when to say ""I don't know"" and escalates
irrelevant/uncertain questions to human operators. Our main novel contribution
is the discussion about uncertainty measure for BERT, where three different
approaches are systematically compared on real problems. We investigated two
uncertainty metrics, information entropy and variance of dropout sampling in
BERT, followed by mixed-integer programming to optimize decision thresholds.
Another novel contribution is the usage of BERT as a language model in
automatic spelling correction. Inputs with accidental spelling errors can
significantly decrease intent classification performance. The proposed approach
combines probabilities from masked language model and word edit distances to
find the best corrections for misspelled words. The chatbot and the entire
conversational AI system are developed using open-source tools, and deployed
within our company's intranet. The proposed approach can be useful for
industries seeking similar in-house solutions in their specific business
domains. We share all our code and a sample chatbot built on a public dataset
on Github."
307,unknown,10.1016/j.asoc.2014.10.018,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84912132496,2015-01-01,performance assessment of heat exchanger using intelligent decision making tools,"Process and manufacturing industries today are under pressure to deliver high quality outputs at lowest cost. The need for industry is therefore to implement cost savings measures immediately, in order to remain competitive. Organizations are making strenuous efforts to conserve energy and explore alternatives. This paper explores the development of an intelligent system to identify the degradation of heat exchanger system and to improve the energy performance through online monitoring system. The various stages adopted to achieve energy performance assessment are through experimentation, design of experiments and online monitoring system. Experiments are conducted as per full factorial design of experiments and the results are used to develop artificial neural network models. The predictive models are used to predict the overall heat transfer coefficient of clean/design heat exchanger. Fouled/real system value is computed with online measured data. Overall heat transfer coefficient of clean/design system is compared with the fouled/real system and reported. It is found that neural net work model trained with particle swarm optimization technique performs better comparable to other developed neural network models. The developed model is used to assess the performance of heat exchanger with the real/fouled system. The performance degradation is expressed using fouling factor, which is derived from the overall heat transfer coefficient of design system and real system. It supports the system to improve the performance by asset utilization, energy efficient and cost reduction in terms of production loss. This proposed online energy performance system is implemented into the real system and the adoptability is validated."
308,unknown,10.23919/splitech.2019.8783034,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8783034/,2019-06-21 00:00:00,episense: towards a smart solution for epileptic patients’ care,"Epilepsy is a chronic neurological brain disorder that affects 50 million people globally. There are several challenges associated with the care of epileptic patients, including: 1) the timely and accurate diagnosis of the condition; 2) the long-term non-intrusive monitoring and detection of epileptic seizures in real time for suitable interventions; 3) alleviating the mental health issues associated with epilepsy, such as anxiety and depression; and 4) the lack of availability of large scale datasets related to epileptic patients with different profiles, needed to advance research in epilepsy. In this work, we propose EpiSense - a smart healthcare solution for epileptic patients' care. EpiSense leverages sensory, mobile, and web technologies, as well as machine learning techniques for the real-time detection of epileptic seizures. As part of the system, a patient's mobile app. is provided to allow the detection of seizures' occurrence in real time and the sending of alarm notifications to care takers, for appropriate actions. Moreover, a web portal enables doctors to view the progress of their patients and get notified about seizures' occurrence and statistics. The EpiSense system was designed and implemented, and three machine learning models were tested for real-time epileptic seizure detection. This work gives interesting insights about the possibility of using sensory technologies and data analytics for the improvement of epileptic patients' care, and offers the possibility of personalized healthcare management."
309,unknown,http://arxiv.org/abs/1812.01813v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1812.01813v1,2018-12-05 00:00:00,"machine-learned epidemiology: real-time detection of foodborne illness
  at scale","Machine learning has become an increasingly powerful tool for solving complex
problems, and its application in public health has been underutilized. The
objective of this study is to test the efficacy of a machine-learned model of
foodborne illness detection in a real-world setting. To this end, we built
FINDER, a machine-learned model for real-time detection of foodborne illness
using anonymous and aggregated web search and location data. We computed the
fraction of people who visited a particular restaurant and later searched for
terms indicative of food poisoning to identify potentially unsafe restaurants.
We used this information to focus restaurant inspections in two cities and
demonstrated that FINDER improves the accuracy of health inspections;
restaurants identified by FINDER are 3.1 times as likely to be deemed unsafe
during the inspection as restaurants identified by existing methods.
Additionally, FINDER enables us to ascertain previously intractable
epidemiological information, for example, in 38% of cases the restaurant
potentially causing food poisoning was not the last one visited, which may
explain the lower precision of complaint-based inspections. We found that
FINDER is able to reliably identify restaurants that have an active lapse in
food safety, allowing for implementation of corrective actions that would
prevent the potential spread of foodborne illness."
310,unknown,10.1016/j.trpro.2020.03.108,scopus,sciencedirect,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85084662425,2020-01-01,estimating time of arrival of trains at level crossings for the provision of multimodal cooperative services,"While cooperative services have been almost fully deployed in the road sector and are already being implemented in various cities in Europe as a pre-requisite for the introduction of autonomous vehicles, few attempts have been made in the same direction for the rail sector. This study proposes a system that aims to improve safety and minimize risk in the meeting point between road and rail, known as level crossings, by monitoring the location of floating road vehicles via a mobile device application. A neural network predictive model for estimating time of arrival of trains is also utilized. The safety system has been implemented and tested under real life conditions in the city of Thessaloniki, Greece."
311,unknown,10.1109/iscsic54682.2021.00059,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9644395/,2021-11-14 00:00:00,near-real time quality prediction in a plastic injection molding process using apache spark,"The automotive industry is undergoing wide scope transformation. Industry 4.0 has both expanded the possibilities of digital transformation in automotive, increased its importance to all mobility ecosystem and being driven by continued digitization of the entire value chain. Manufacturing data which is unceasingly flow during serial production is one of the great sources towards Industry 4.0 goal to fully automatizing complex human dependent processes. However, there are few challenges to consider such as collecting and filtering various data from shop floor in given production cycle time range and make them ready for real time analytics as well as constructing efficient data pipeline to reach useful outcomes which is reliable enough to meet customer expectations. In this study, we will extract meaningful relation between injection machine parameters from Farplas Automotive Company's shop floor and describe their effects on the product quality. We will train and test machine learning models with different hyperparameters and test model performance to identify defected products. Finally, we will show implementation of streaming data pipeline using Kafka and Spark to be able to analyze injection machine data and effectively predict plastic injection product's OK-NOK condition real time even before human operator reaches the product itself. Consequently, detecting defected products will be independent from human attention which makes production areas one step closer to dark factory."
312,unknown,10.3390/electronics10020182,'MDPI AG',core,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/523305217.pdf,2021-01-01 00:00:00,design and implementation of deep learning based contactless authentication system using hand gestures,"Hand gestures based sign language digits have several contactless applications. Applications include communication for impaired people, such as elderly and disabled people, health-care
applications, automotive user interfaces, and security and surveillance. This work presents the design
and implementation of a complete end-to-end deep learning based edge computing system that
can verify a user contactlessly using ‘authentication code’. The ‘authentication code’ is an ‘n’ digit
numeric code and the digits are hand gestures of sign language digits. We propose a memory-efficient
deep learning model to classify the hand gestures of the sign language digits. The proposed deep
learning model is based on the bottleneck module which is inspired by the deep residual networks.
The model achieves classification accuracy of 99.1% on the publicly available sign language digits
dataset. The model is deployed on a Raspberry pi 4 Model B edge computing system to serve as an
edge device for user verification. The edge computing system consists of two steps, it first takes input
from the camera attached to it in real-time and stores it in the buffer. In the second step, the model
classifies the digit with the inference rate of 280 ms, by taking the first image in the buffer as input.publishedVersio"
313,unknown,10.1109/isie45063.2020.9152407,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9152407/,2020-06-19 00:00:00,modeling and predicting an industrial process using a neural network and automation data,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools."
314,unknown,10.1109/icitr54349.2021.9657405,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9657405/,2021-12-03 00:00:00,vision based intelligent shelf-management system,"Currently supermarkets are more popular, and the local stores are leaving the competition. when people go to supermarkets, they find various items stocked on seemingly unlimited shelves. Supermarket shelves needed to be filled with the items accordingly. The most common problems in the supermarkets are identifying the empty shelves, on-shelf availability, and future sales. The labors cannot always track the empty shelves and on shelf availability levels due to their workloads. Moreover, it is a time-consuming method for the labors which can affect the customer satisfaction and business profit. Every month, supermarkets buy the required number of products from related manufacturing companies by analyzing the previously purchased products and their sales. This is usually done manually by managing excel sheets which is also time consuming and not reliable. Especially during the seasonal times or pandemic situations they cannot use the manual method which must also be done as fast as possible. Therefore, this system can be used to assist in empty shelf detection, percentage of on-shelf availability and in the prediction of future sales. The implementation of on-shelves percentage detection service is done using machine learning. Machine learning processes are carried out for implementing the necessary functionalities and algorithms. Initially, the camera captures clear and real time images regularly. Then the system processes and detects the image similar to the threshold percentage or detect the empty shelves. When the system detects the threshold percentage or empty shelves, the system will provide an alert to the labors. The Implementation of the predicting the future supply and demands is done using time series analysis using several existing machine learning algorithms by utilizing historical data. In this research the prediction of future sales and demand in the supermarkets is done by considering the customers&#x0027; behavior, the variety of product groups they buy and seasonal changes. These predictions are made on the assumption of a constant per capital supply of products and demand in our system."
315,unknown,10.1049/cp.2018.1540,IET,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8726965/,2018-11-14 00:00:00,convolutional neural networks for facial emotion recognition towards the development of automatic pain quantifier,"Facial Expression Recognition (FER) has been one of the mainstream topics in the areas of computer vision. Numerous databases of facial expressions are available to the research community and are used as fundamental tools for the evaluation of a wide range of algorithms for FER. With the continuous evolution of deep learning in computer vision, FER has enticed many researchers within the area of Artificial Intelligence, applying powerful machine learnings to enhance the overall user experience in solving many real-world problems. In this study, we review the state of the art of Convolutional Neural Networks (CNNs) as a computational model for image classification, accentuating the algorithmic structure and its functional impact. We implemented CNN building framework to validate a real-time system that is applied to automatically classify key human emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutrality using FER2013 emotion datasets. The study showed that CNN model yields a high-efficiency result in achieving a high-level performance of automatically recognizing emotions from dynamic facial expressions. Our paradigm extends to the development of an artificially intelligent system capable of recognizing pain and its intensity and can diagnose a convincing and reliable outcome that speaks the same language from a medical perspective."
316,unknown,10.1109/w-ficloud.2018.00026,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8488186/,2018-08-08 00:00:00,embedded fatigue detection using convolutional neural networks with mobile integration,"Fatigued or drowsy drivers pose a significant risk of causing life-threatening accidents. Yet, many sleep-deprived drivers are behind the wheels exposing lives to danger. In this paper, we propose a low-cost and real-time embedded system for fatigue detection using convolutional neural networks (CNN). Our system starts by spatially processing the video signal using a real-time face detection algorithm to establish a region of interest and reduce computations. The video signal comes from a camera module mounted on the car dashboard connected to an embedded Linux board set to monitor the driver's eyes. Detected faces are then passed to an optimized fatigue recognition CNN binary classifier to detect the event of fatigued or normal driving. When temporally persistent fatigue is detected, alerts are sent to the driver's smart phone, and to possibly others, for prevention measures to be taken before accidents happen. Our testing shows that the system can robustly detect fatigue and can effectively be deployed to address the problem."
317,unknown,10.1109/iciea49774.2020.9102083,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9102083/,2020-04-21 00:00:00,a rapid deployment indoor positioning architecture based on image recognition,"With the rapid development of information, the immediacy, interoperability, and portability of information long been essential elements in present society. Technology has also continued to develop from these elements and creating more and more information equipment. In recent years, as new technologies continue to be developed, they have been mainly divided into two major technologies, namely “deep learning” and “augmented reality. These are including face recognition, image recognition, and augmented reality (AR), virtual reality (VR) and so on which are the key development trends now whether on mobile devices or web pages. In this study, the emerging mainstream “deep learning” and “augmented reality” are used to solve the problem of Google Map that cannot perform indoor navigation. The image recognition technology in deep learning is used to achieve the positioning function and the algorithm is used to calculate the best route to your destination, then indoor navigation presented. Finally, augmented reality (AR) technology is used to set up an exclusive AR floating electronic bulletin board at a specific location to display the relevant information and content of the specific location point for users. This research has the characteristics of rapid deployment which can be launched quickly in an indoor environment."
318,unknown,10.1016/b978-0-12-817356-5.00014-0,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85123886383,2019-01-01,combining predictive analytics and artificial intelligence with human intelligence in iot-based image-guided surgery,"When analyzing the history of medical surgery, we can see that humans have developed and refined instruments for surgical procedures. Evolution in medical advancements is on a par with that observed in the disease-causing agents and viruses. Starting from a medical era during which invasive surgeries were performed without anesthesia, the need for painless, safe, hygienic, and successful surgeries has led to the modern era of surgery, which has a relatively small mortality rate. The practical use of minimally invasive approaches that result in fewer wound-related complications, quick organ function return, and shorter hospitalizations has led to higher acceptance of image-guided surgical procedures. In our proposed system, we present an IoT-based surgical model that involves a virtual reality-based (VR-based) user interface, predictive analytics that predict the “what-ifs” for an activity to be performed during the surgery based on the data collected during similar previous surgeries, and artificial intelligence that learns from the same dataset used by predictive analytics to assist surgeons during the surgery. Virtual reality refers to an environment artificially created by the support of computer software. When humans access such an environment, they believe that they are actually there in the artificially created environment. Headsets and other navigators can be used along with the VR set-up in order to provide a more immersive environment. The experience obtained through the power of VR is no less than the actual reality. For a more immersive experience, the Oculus Rift VR system is to be used as part of the proposed model. The VR system is to be connected to the SAP IoT interface of the SAP Cloud system. Predictive analytics is one of the latest software paradigms that enables analysis of large datasets and prediction of future outcomes and behavior. It involves building a predictive analytics model by using big data and IoT sensors to uncover hidden risks, explore unforeseen opportunities, and reach a better understanding. In our proposed model, SAP predictive analytics is to be deployed to build a surgical predictive model that could provide real-time predictions during surgery based on the data collected during previous surgeries. Artificial intelligence is a computing paradigm used to create systems that automate intelligent processes. AI can be used for learning, problem solving, and decision-making processes and can work with a speed and precision that humans are able to achieve only with great effort. AI is to be deployed in our proposed surgical model to minimize the surgeons’ efforts in navigating the noninvasive surgical equipment and the camera. Often there is a difference between the intention with which a surgeon navigates and the actual navigation which happens internally during the surgery. The surgeon has to put in more effort to overcome this mismatch. In order to address this issue, formulating AI in the surgical internal device navigation could result in reduced manual effort by the surgeon, which in turn would help him in concentrating on comparatively more important surgical thoughts and activities. The SAP Leonardo Machine Learning feature is to be deployed in the proposed model."
319,unknown,10.1109/icpst.2002.1047521,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1047521/,2002-10-17 00:00:00,a study and implementation of an intelligent load forecast support system,"Computer decision support systems have been more widely applied in the planning of urban power systems, especially the distribution systems. A load forecast is a basic stage in the process of the distribution system planning. Hence, a load forecast support system (LFSS) is a basic sub-system in the package of a decision support system for urban power systems planning. The knowledge and experience of planners play a vital part in the practical forecasting process. Thus, it is urgent for researchers to find an approach to accumulate planners' knowledge and experience during load forecasting and reuse them in the further forecasting. The combination of artificial intelligence (AI) technology and LFSS provides an effective approach to solve this problem. This paper has proposes a model and an implementation frame of an intelligent load-forecast support system (ILFSS), which provides a total solution to store and reusing load forecasting knowledge. This approach has applied the AI technology to the whole process of load forecasting, including model definition, model selecting, result adjusting and decision making. Some AI technologies like rule-based reasoning and case-based reasoning are involved in the implementation of ILFSS, which cooperate with conventional load forecasting models. Up to now, some of the proposed architecture has been implemented in a real decision support system for urban power system planning called ""CNP"", which is widely used in China."
320,unknown,10.1109/ies53407.2021.9594038,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9594038/,2021-09-30 00:00:00,design and implementation of real-time pothole detection using convolutional neural network for iot smart environment,"Road is a transportation infrastructure that has a very important role in supporting the economic, social and culture as well as various other aspects of community life. Various community activities are affected by road conditions. Roads that are in good condition will provide comfort and facilitate economic activity in an area. However, in reality there are still many roads that are not in good condition, such as potholes. Various attempts have been made to detect potholes automatically, especially with the two-dimensional imaging method. In this paper, we propose real-time potholes detection using the Convolutional Neural Network (CNN) method based on the Edge Tensor Processing Unit (TPU) with the MobileNet SSD v2. Our system was implemented on Jetson Nano with several additional sensors such as a camera and GPS. The accuracy of the system is verified through experiments using the testbed."
321,unknown,10.1109/miltechs.2017.7988861,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7988861/,2017-06-02 00:00:00,multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform,"This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment."
322,unknown,10.1109/icde51399.2021.00313,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9458701/,2021-04-22 00:00:00,floravision: a spatial crowd-based learning system for california native plants,"With the availability of massive amounts of visual data covering wide geographical regions, various image learning applications have emerged, including classifying the street cleanliness level, detecting forest fires or road hazards. Such applications share similar characteristics as they need to 1) detect specific objects or events (what), 2) associate the detected object with a location (where), and 3) know the time that the event happened (when). Advancements in image-based machine learning (ML) benefit these applications as they can automate the detection of objects of interest. Along with the edge computing (EC) paradigm, the processing cost is offloaded to the devices, hence reducing latency and communication cost. Moreover, sensors on the edge devices (e.g., GPS) enrich the collected data with metadata. However, a shortcoming of existing approaches is that they rely on pre-trained ""static"" models. Nonetheless, crowdsourced data at diverse locations can be leveraged to iteratively improve the robustness of a model. We refer to the aforementioned strategy as ""spatial crowd-based learning"".To showcase this class of applications, we present FloraVision, an end-to-end system that integrates ML, crowdsourcing, and EC to automate the detection, mapping, and exploration of California Native Plants. FloraVision implements a pipeline to collect and clean publicly available image data, train a lightweight MobileNet-based classification model, and then deploy the model on mobile devices. It leverages spatial crowd-based learning to iteratively evolve the initial model from crowdsourced data. Its mobile application facilitates detecting plants and mapping their geolocations. Finally, it allows end-users to submit ad hoc spatio-temporal nearest neighbor queries and visualizes the results in an augmented reality user interface. Although our application focuses on plants, several other applications follow similar architectural patterns."
323,unknown,10.1109/jsyst.2019.2921867,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8746591/,2020-06-01 00:00:00,a smart optimization of fault diagnosis in electrical grid using distributed software-defined iot system,"Electrical power demands have increased significantly over the last years due to rapid increase in air conditioning units and home appliances per domestic unit, particularly in Iraq. Having an uninterrupted power supply is essential for the continuity of power-generated home services and industrial platforms. Currently, in Iraq, electrical power interruption has become a big concern to the utility suppliers. Despite successive attempts to put an end to this dilemma, the issue still prevails. One of the main factors in power outages in local zones is persistent faults in distribution transformers (DTs). DT is considered one of the main elements in the electrical network that is essential for the reliability of the grid supply. Due to the internal lack of monitoring system and periodic maintenance, DT is relentlessly subject to faults due to high overhead utilization. Therefore, in order to enhance the grid reliability, transformer health check, and maintenance practices, we propose a remote condition Internet of Things monitoring and fault prediction system that is based on a customized software-defined networking (SDN) technology. This approach is a transition to smart grid implementation by fusing the power grid with efficient and real-time wireless communication architecture. The SDN implementation is considered in two phases: one is a controller installed per local zone and the other is the main controller that is installed between zones and connected to the core network. The core network consists of redundant links to recover from any future fails. Furthermore, we propose a prediction system based on an artificial neural network algorithm, called distribution transformer fault prediction, that is installed in the management plane for periodic prediction based on real-time sensor traffic to our proposed cloud. Moreover, we propose a communication protocol in the local zone called local SDN-sense. The SDN-sense ensures a reliable communication and local node selection to relay DT sensor data to the main controller. Our proposed architecture showcases an efficient approach to handle future interruption and faults in power grid using cost-effective and reliable infrastructure that can predict and provide real-time health monitoring indices for the Iraqi grid network with minimal power interruptions. After extensive testing, the prediction accuracy was about 96.1%."
324,unknown,http://arxiv.org/abs/1902.02236v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1902.02236v1,2019-02-06 00:00:00,dynamic pricing for airline ancillaries with customer context,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system."
325,unknown,10.1109/fabs52071.2021.9702559,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9702559/,2021-12-22 00:00:00,real time alert system to prevent car accident,"The use of artificial intelligence to solve a complex problem and to solve a complex issue such as foreseeing and avoiding accidents on roads implies a robust and proven procedure. An effective and proven method is needed to predict and prevent accidents on the road. Artificial vision is an area of artificial intelligence to explain and understand the visual world. With machine vision and Intelligent Transportation Systems (ITS), we can provide drivers with a safety net. These technologies help reduce human error in the automotive industry and provide riders with tools and features to help them avoid serious mistakes and accidents. In this paper we present a Drowsiness Detection System which indicates the drowsiness of the car driver by detecting the drivers face in real time video and considering the various eye landmarks indicates the ratio of each eye telling how much the driver&#x2019;s eyes are open or close. This system is developed using the state-of-art Computer Vision technology making it easy to implement highly effective. This system is implemented as an Android Application which uses android mobile back camera for recording car driver&#x2019;s live video and implementing the above developed model to calculate the eye aspect ratio and indicate it along with the eye in the video."
326,unknown,10.1145/3448016.3457550,SIGMOD Conference,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,2021-01-01 00:00:00,milvus: a purpose-built vector data management system,"Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus."
327,unknown,10.1016/j.cie.2021.107824,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85122422552,2022-01-01,new perspectives and results for smart operators in industry 4.0: a human-centered approach,"Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators.
                  To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept.
                  Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework."
328,unknown,10.1109/icate49685.2021.9465023,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9465023/,2021-05-29 00:00:00,pv monitoring system using industrial internet of things technologies based on graphical programming,"Photovoltaic Panel Systems are already a commodity, the widespread use of such systems being no longer a pioneering topic. Monitoring these power generation systems is not a trivial task since the hunger of real-time data is growing continuously. The advent of IoT technologies, the ubiquitous presence of communication technologies, including remote areas, are making these solutions easier to develop, deploy and use. This will constitute the nurturing bed for optimization processes, which would eventually rely on Artificial Intelligence. This paper is describing how to design and deploy such data collecting system, emphasizing on its Software Architecture based on graphical programming technologies. Ready-to-run virtual instruments are facilitating real-time measurements and analysis, process data being securely stored and visualized using cloud technologies."
329,unknown,10.1109/iraset52964.2022.9737786,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9737786/,2022-03-04 00:00:00,smart energy management system: oil immersed power transformer failure prediction and classification techniques based on dga data,"The power transformer is the key element in the electrical grid. The failures of the power transformer impact critically the grid, can cause energy loss and blackouts. In energy production, transmission, distribution, and industrial applications, the oil immersed power transformer is the most used. The maintenance of this key equipment is highly important which can be done with different techniques such as thermal and vibration analysis, frequency analysis using wavelet transform and dissolved gas analysis. The application of predictive maintenance of the power transformer represents an important feature of the Smart Energy Management System in micro grids, that can reduce the percentage of failure occurrence while increasing the availability of the power transformer and prevents blackouts. This paper represents different failure classification techniques based on dissolved gas analysis data mainly logistic regression, multiclass jungle, multiclass decision tree and artificial neural network. The application can diagnosis the power transformer failures based on the parts-per-million of the different gas generated in the oil. The results of applying different types of classification algorithms shows the best technique to be part of a bigger system of monitoring and diagnostic of different installed equipment in a micro grid. The implementation of such application in real time energy management system requires different type of sensors and the interaction of offline database, the paper also shows the steps to integrate the algorithm in the Smart Energy Management System."
330,unknown,10.1155/2021/7906047,'Hindawi Limited',core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-01-01 00:00:00,optimization of the rapid design system for arts and crafts based on big data and 3d technology,"In this paper, to solve the problem of slow design of arts and crafts and to improve design efficiency and aesthetics, the existing big data and 3D technology are used to conduct an in-depth analysis of the optimization of the rapid design system of arts and crafts machine salt baking. In the system requirement analysis, the functional modules of this system are identified as nine functional modules such as design terminology management system and external information import function according to the actual usage requirements. In the system design, the overall structure design, database design, and functional module design of the system are comprehensively elaborated, and the key issues such as 3D display and home layout generation algorithm based on reinforcement learning are analyzed and designed. In the implementation part of the system, the overall construction of the system and the composition of functional modules are introduced in detail and the main functional modules of the system are presented with interface diagrams. In the system implementation part, the overall system construction and functional module composition are introduced in detail, the main functional modules of the system are shown with interface diagrams, codes, and algorithms, and the specific implementation process of 3D display and soft layout algorithms are also explained in detail. The process of Surface Mount Technology (SMT) big data processing and analysis is designed, and the design of SMT production line data collection scheme and real-time data processing architecture is completed. Based on the characteristics of SMT production line data, the K-means algorithm is used to detect data outliers and verify the accuracy of the method; also, the Spark-based association rule printing parameter recommendation model is designed, and the efficiency of the Apriori algorithm is significantly improved by parallelization"
331,unknown,10.1109/access.2020.2988735,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9072143/,2020-01-01 00:00:00,a framework for in-network qoe monitoring of encrypted video streaming,"With the amount of global network traffic steadily increasing, mainly due to video streaming services, network operators are faced with the challenge of efficiently managing their resources while meeting customer demands and expectations. A prerequisite for such Quality-of-Experience-driven (QoE) network traffic management is the monitoring and inference of application-level performance in terms of video Key Performance Indicators (KPIs) that directly influence end-user QoE. Given the persistent adoption of end-to-end encryption, operators lack direct insights into video quality metrics such as start-up delays, resolutions, or stalling events, which are needed to adequately estimate QoE and drive resource management decisions. Numerous solutions have been proposed to tackle this challenge on individual use-cases, most of them relying on machine learning (ML) for inferring KPIs from observable traffic patterns and statistics. In this paper, we summarize the key findings in state-of-the-art research on the topic. Going beyond previous work, we devise the concept of a generic framework for ML-based QoE/KPI monitoring of HTTP adaptive streaming (HAS) services, including model training, deployment, and re- evaluation. Components of the framework are designed in a generic way, independent of a particular streaming service and platform. The methodology for applying different framework components is discussed across various use-cases. In particular, we demonstrate framework applicability in a concrete use-case involving the YouTube service delivered to smartphones via the mobile YouTube app, as this presents one of the most prominent examples of accessing YouTube. We tackle both QoE/KPI estimation on a per-video-session level (utilizing the validated ITU-T P.1203 QoE model), as well as “real-time” KPI estimation over short time intervals. Obtained results provide important insights and challenges related to the deployment of a generic in-network QoE monitoring framework for encrypted video streams."
332,unknown,10.1016/j.cogsys.2020.11.002,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85097539429,2021-03-01,earthquake disaster avoidance learning system using deep learning,"The popularity of deep learning has influenced the field of surveillance and human safety. We adopt the advantages of deep learning techniques to recognize potentially harmful objects inside living rooms, offices, and dining rooms during earthquakes. In this study, we propose an educational system to teach earthquake risks using indoor object recognition based on deep learning algorithms. The system is based on the You Look Only Once (YOLO) deployed on our cloud-based server named Earthquake Situation Learning System (ESLS) for the detection of harmful objects associated with risk tags. ESLS is trained on our own indoor images dataset. The user interacts with the ESLS server through video or image files, and the object detection algorithm using YOLO recognizes the indoor objects with associated risk tags. Results show that the service time of ESLS is low enough to serve it to users in 0.8 s on average, including processing and communication times. Furthermore, the accuracy of the harmful object detection is 96% in the general indoor lighting situation. The results show that the proposed ESLS is applicable to real service for teaching the earthquake disaster avoidance."
333,unknown,10.32628/ijsrset229240,"International Journal of Scientific Research in Science, Engineering and Technology",semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/9656acc489a092e7d4c93c86ac45ccd411214da7,2022-01-01 00:00:00,machine learning based data analytics for iot enabled industry automation,"The main aims of this projects to the replacement of old communication that uses wired links with new communication that is wireless communication.The main reason to move to wireless communication is to improve the mobility, reduce the deployment cost, reduce cable damage and to improve the scalability.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence. The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The second aim of this project is to connect devices to IOT so as to improve theaccessibility of the industry from anywhere in the world. These services are known as Best Effort services."
334,unknown,http://arxiv.org/abs/2011.10823v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.10823v2,2020-11-21 00:00:00,"a system for automatic rice disease detection from rice paddy images
  serviced via a chatbot","A LINE Bot System to diagnose rice diseases from actual paddy field images
was developed and presented in this paper. It was easy-to-use and automatic
system designed to help rice farmers improve the rice yield and quality. The
targeted images were taken from the actual paddy environment without special
sample preparation. We used a deep learning neural networks technique to detect
rice diseases from the images. We developed an object detection model training
and refinement process to improve the performance of our previous research on
rice leave diseases detection. The process was based on analyzing the model's
predictive results and could be repeatedly used to improve the quality of the
database in the next training of the model. The deployment model for our LINE
Bot system was created from the selected best performance technique in our
previous paper, YOLOv3, trained by refined training data set. The performance
of the deployment model was measured on 5 target classes and found that the
Average True Positive Point improved from 91.1% in the previous paper to 95.6%
in this study. Therefore, we used this deployment model for Rice Disease LINE
Bot system. Our system worked automatically real-time to suggest primary
diagnosis results to the users in the LINE group, which included rice farmers
and rice disease specialists. They could communicate freely via chat. In the
real LINE Bot deployment, the model's performance was measured by our own
defined measurement Average True Positive Point and was found to be an average
of 78.86%. The system was fast and took only 2-3 s for detection process in our
system server."
335,unknown,10.1016/j.ssci.2019.06.025,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85067872085,2019-12-01,securing instant messaging based on blockchain with machine learning,"Instant Messaging (IM) offers real-time communications between two or more participants on Internet. Nowadays, most IMs take place on mobile applications, such as WhatsApp, WeChat, Viber and Facebook Messenger, which have more users than social networks, such as Twitter and Facebook. Among the applications of IMs, online shopping has become a part of our everyday life, primarily those who are busiest. However, transaction disputes are often occurred online shopping. Since most IMs are centralized and message history is not stored in the center, the messaging between users and owners of online shops are not reliable and traceable. In China, online shopping sales have soared from practically zero in 2003 to nearly 600 hundred million dollars last year, and now top those in the United States. It is very crucial to secure the instant messaging in online shopping in China. We present techniques to exploit blockchain and machine learning algorithms to secure instant messaging. Since the cryptography of Chinese national standard is encouraged to adopt in security applications of China, we propose a blockchain-based IM scheme with the Chinese cryptographic bases. First, we design a message authentication model based on SM2 to avoid the counterfeit attack and replay attack. Second, we design a cryptographic hash mode based on SM3 to verify the integrity of message. Third, we design a message encryption model based on SM4 to protect the privacy of users. Besides, we propose a method based on machine learning algorithms to monitor the activity on blockchain to detect anomaly. To prove and verify the blockchain-based IM scheme, a blockchain-based IM system has been designed on Linux platforms. The implementation result shows that it is a practical and secure IM system, which can be applied to a variety of instant messaging applications directly."
336,unknown,10.1016/j.imu.2020.100335,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85084287220,2020-01-01,spark architecture for deep learning-based dose optimization in medical imaging,"Background and objectives
                  Deep Learning (DL) and Machine Learning (ML) have brought several breakthroughs to biomedical image analysis by making available more consistent and robust tools for the identification, classification, reconstruction, denoising, quantification, and segmentation of patterns in biomedical images. Recently, some applications of DL and ML in Computed Tomography (CT) scans for low dose optimization were developed. Nowadays, DL algorithms are used in CT to perform replacement of missing data (processing technique) such as low dose to high dose, sparse view to full view, low resolution to high resolution, and limited angle to full angle. Thus, DL comes with a new vision to process biomedical data imagery from CT scan. It becomes important to develop architectures and/or methods based on DL algorithms for minimizing radiation during a CT scan exam thanks to reconstruction and processing techniques.
               
                  Methods
                  This paper describes DL for CT scan low dose optimization, shows examples described in the literature, briefly discusses new methods used in CT scan image processing, and offers conclusions. We based our study on the literature and proposed a pipeline for low dose CT scan image reconstruction. Our proposed pipeline relies on DL and the Spark Framework using MapReduce programming. We discuss our proposed pipeline with those proposed in the literature to conclude the efficiency and importance.
               
                  Results
                  An architecture for low dose optimization using CT imagery is suggested. We used the Spark Framework to design the architecture. The proposed architecture relies on DL, and permits us to develop efficient and appropriate methods to process dose optimization with CT scan imagery. The real implementation of our pipeline for image denoising shows that we can reduce the radiation dose, and use our proposed pipeline to improve the quality of the captured image.
               
                  Conclusion
                  The proposed architecture based on DL is complete and enables faster processing of biomedical CT imagery as compared with prior methods described in the literature."
337,unknown,10.1109/iciba52610.2021.9688086,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9688086/,2021-12-19 00:00:00,research on recommendation algorithm based on e-commerce user behavior sequence,"With the rapid rise and development of the Internet, e-commerce has become one of the most promising and fastest-growing industries. we propose an architecture of a personalized recommendation system based on offline mining, real-time mining, and deep learning technology. First, through the Flume + Kafka + Spark Streaming system, user behavior data is collected and stored in the business database after data preprocessing First, through the Flume + Kafka + Spark Streaming system, user behavior data is collected and stored in the business database after data preprocessing, which is the work preparing for the next step of data mining. Data mining includes offline mining and real-time mining. Offline mining mainly runs MapReduce on the Hadoop platform, there are also some calculations on the Spark platform, and the calculation results are stored in the business database; The implementation of real-time mining mainly is based on the message subscription of the Kafka cluster, and get real-time consumption statistics through Spark clusters; Deeplearning4j, a deep learning framework, runs on a multi-GPU Spark distributed cluster, which can be optimized the algorithm model online and generate recommendation results to push to the real-time business database and give feedback to the users.; It can also extract features from these data for training automatically, which is conducive to improving the quality and efficiency of users' shopping decisions, and improve the platform's cross-selling capabilities, shorten the user's shopping path, increase traffic conversion rates."
338,unknown,10.1007/s12652-017-0571-8,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s12652-017-0571-8,2018-10-01 00:00:00,fuzzy association rule mining for recognising daily activities using kinect sensors and a single power meter,"The recognition of activities of daily living (ADLs) by home monitoring systems can be helpful in order to objectively assess the health-related living behaviour and functional ability of older adults. Many ADLs involve human interactions with household electrical appliances (HEAs) such as toasters and hair dryers. Advances in sensor technology have prompted the development of intelligent algorithms to recognise ADLs via inferential information provided from the use of HEAs. The use of robust unsupervised machine learning techniques with inexpensive and retrofittable sensors is an ongoing focus in the ADL recognition research. This paper presents a novel unsupervised activity recognition method for elderly people living alone. This approach exploits a fuzzy-based association rule-mining algorithm to identify the home occupant’s interactions with HEAs using a power sensor, retrofitted at the house electricity panel, and a few Kinect sensors deployed at various locations within the home. A set of fuzzy rules is learned automatically from unlabelled sensor data to map the occupant’s locations during ADLs to the power signatures of HEAs. The fuzzy rules are then used to classify ADLs in new sensor data. Evaluations in real-world settings in this study demonstrated the potential of using Kinect sensors in conjunction with a power meter for the recognition of ADLs. This method was found to be significantly more accurate than just using power consumption data. In addition, the evaluation results confirmed that, owing to the use of fuzzy logic, the proposed method tolerates real-life variations in ADLs where the feature values in new sensor data differ slightly from those in the learning patterns."
339,unknown,10.1109/access.2020.2996214,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9097876/,2020-01-01 00:00:00,a machine learning security framework for iot systems,"Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges."
340,unknown,10.1109/jiot.2021.3096637,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9481251/,2001-02-01 20:22:00,an integrated framework for health state monitoring in a smart factory employing iot and big data techniques,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues."
341,unknown,10.1109/idt52577.2021.9497540,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9497540/,2021-06-24 00:00:00,phishing detection for secure operations of uavs,"Unmanned Aerial Vehicles (UAV) or drones are used in various domains more and more, including military operations, monitoring, rescue of victims, and transport. Often, UAV resources are developed as web services so that they can be accessed anywhere on the Internet through the World Wide Web. However, this makes them vulnerable to phishing activities of criminals who may try to access these resources and other sensitive information. Therefore, the development of a phishing detection tool based on data mining is presented in this paper. It consists of a browser extension monitoring visited webpages and a backend communicating with the browser extension for the purposes of executing some specific tasks. The browser extension is implemented in JavaScript and the ReactJS framework, and it contains an implementation of classifications with a Bayesian network, decision tree, nearest neighbor classifier and neural network. The backend uses PHP, Python scripts and the Apache HTTP Server. In addition, a browser extension is implemented so that data about webpages can be collected and this data is used for the creation of data mining models. Experimental validation with 10-fold cross-validation and through the browsing of real-world websites show promising results in phishing detection."
342,unknown,10.1109/igcc.2016.7892604,2016 Seventh International Green and Sustainable Computing Conference (IGSC),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/8ad5ecbd6b5003bc8ecd8ee14add64030eaf3aa0,2016-01-01 00:00:00,autoplug: an automated metadata service for smart outlets,"Low-cost network-connected smart outlets are now available for monitoring, controlling, and scheduling the energy usage of electrical devices. As a result, such smart outlets are being integrated into automated home management systems, which remotely control them by analyzing and interpreting their data. However, to effectively interpret data and control devices, the system must know the type of device that is plugged into each smart outlet. Existing systems require users to manually input and maintain the outlet metadata that associates a device type with a smart outlet. Such manual operation is time-consuming and error-prone: users must initially inventory all outlet-to-device mappings, enter them into the management system, and then update this metadata every time a new device is plugged in or moves to a new outlet. Inaccurate metadata may cause systems to misinterpret data or issue incorrect control actions. To address the problem, we propose AutoPlug, a system that automatically identifies and tracks the devices plugged into smart outlets in real time without user intervention. AutoPlug combines machine learning techniques with time-series analysis of device energy data in real time to accurately identify and track devices on startup, and as they move from outlet-to-outlet. We show that AutoPlug achieves ∼90% identification accuracy on real data collected from 13 distinct device types, while also detecting when a device changes outlets with an accuracy >90%. We implement an AutoPlug prototype on a Raspberry Pi and deploy it live in a real home for a period of 20 days. We show that its performance enables it to monitor up to 25 outlets, while detecting new devices or changes in devices with <50s latency."
343,unknown,10.1109/i-smac49090.2020.9243544,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9243544/,2020-10-09 00:00:00,scalable iot solution using cloud services – an automobile industry use case,"The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence."
344,unknown,http://arxiv.org/abs/2001.06202v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2001.06202v1,2020-01-17 00:00:00,"fedvision: an online visual object detection platform powered by
  federated learning","Visual object detection is a computer vision-based artificial intelligence
(AI) technique which has many practical applications (e.g., fire hazard
monitoring). However, due to privacy concerns and the high cost of transmitting
video data, it is highly challenging to build object detection models on
centrally stored large training datasets following the current approach.
Federated learning (FL) is a promising approach to resolve this challenge.
Nevertheless, there currently lacks an easy to use tool to enable computer
vision application developers who are not experts in federated learning to
conveniently leverage this technology and apply it in their systems. In this
paper, we report FedVision - a machine learning engineering platform to support
the development of federated learning powered computer vision applications. The
platform has been deployed through a collaboration between WeBank and Extreme
Vision to help customers develop computer vision-based safety monitoring
solutions in smart city applications. Over four months of usage, it has
achieved significant efficiency improvement and cost reduction while removing
the need to transmit sensitive data for three major corporate customers. To the
best of our knowledge, this is the first real application of FL in computer
vision-based tasks."
345,unknown,10.1109/bigdatacongress.2019.00032,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8818217/,2019-07-13 00:00:00,"premises, a scalable data-driven service to predict alarms in slowly-degrading multi-cycle industrial processes","In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack."
346,unknown,10.1109/access.2020.2996576,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9098886/,2020-01-01 00:00:00,an embedded system for collection and real-time classification of a tactile dataset,"Tactile perception of the material properties in real-time using tiny embedded systems is a challenging task and of grave importance for dexterous object manipulation such as robotics, prosthetics and augmented reality. As the psychophysical dimensions of the material properties cover a wide range of percepts, embedded tactile perception systems require efficient signal feature extraction and classification techniques to process signals collected by tactile sensors in real-time. For this purpose, we developed two embedded systems, one that served as a vibrotactile stimulator system and one that recorded and classified the vibrotactile signals collected by its sensors. The quality of the collected data was first verified offline using Fourier transform for feature extraction and then applying powerful machine learning classifiers such as support vector machines and neural networks. We implemented the proposed memory-less signal feature extraction method in order to achieve real-time processing as the data is being collected. The experimental results have shown that the proposed method significantly reduces the computational complexity of feature extraction and still has led to high classification accuracy even when fed to the less complex classifiers such as random forests that can be easily implemented on embedded systems. Finally, we have also shown that low-cost, highly accurate, and real-time tactile texture classification can be achieved using the proposed approach with an ensemble of sensors."
347,unknown,10.1145/3292500.3330723,KDD,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/7436d86cd6b572a3f52caa7820c07e7bfcf16f86,2019-01-01 00:00:00,gmail smart compose: real-time assisted writing,"In this paper, we present Smart Compose, a novel system for generating interactive, real-time suggestions in Gmail that assists users in writing mails by reducing repetitive typing. In the design and deployment of such a large-scale and complicated system, we faced several challenges including model selection, performance evaluation, serving and other practical issues. At the core of Smart Compose is a large-scale neural language model. We leveraged state-of-the-art machine learning techniques for language model training which enabled high-quality suggestion prediction, and constructed novel serving infrastructure for high-throughput and real-time inference. Experimental results show the effectiveness of our proposed system design and deployment approach. This system is currently being served in Gmail."
348,unknown,10.1016/j.promfg.2020.04.017,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85085527469,2020-01-01,ppe compliance detection using artificial intelligence in learning factories,"This project demonstrates the application of Artificial Intelligence (AI) and machine vision for the identification of Personal Protective Equipment (PPE), particularly safety glasses in zones of the Learning Factory, where safety risks exist. The objective is to design and implement an automated system for ensuring the safety of personnel when they are in the vicinity of machinery that presents potential risks to the eyes. Microsoft Azure Custom Vision AI and Intelligent AI Services, in conjunction with low-cost vision devices with lightweight onboard AI capability, provide a platform for a deep learning neural network model using publicly available images under the Creative Commons License. A combination of cloud-based and on-premises AI is used in this proof of concept system to provide a real-time vision-based safety system capable of detecting and recording potential safety breaches, promoting compliance, and ultimately preventing accidents before they happen. This system can be used to initiate different control actions in the event of safety violations and can detect multiple forms of protective wear. The flexibility of the system offers multiple benefits to learning factories and manufacturing organizations such as improved user safety, reduced insurance costs, and better detection and recording of safety violations. The hybrid AI architecture approach allows for flexibility in training and deployment based on the capability of local computing resources."
349,unknown,10.1016/j.eswa.2017.02.022,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85013790845,2017-07-15,improvement of newborn screening using a fuzzy inference system,"This paper presents a decision support system (DSS) called DSScreening to rapidly detect inborn errors of metabolism (IEMs) in newborn screening (NS). The system has been created using the Aide-DS framework, which uses techniques imported from model-driven software engineering (MDSE) and soft computing, and it is available through eGuider, a web portal for the enactment of computerised clinical practice guidelines and protocols.
                  MDSE provides the context and techniques to build new software artefacts based on models which conform to a specific metamodel. It also offers separation of concern, to disassociate medical from technological knowledge, thus allowing changes in one domain without affecting the other. The changes might include, for instance, the addition of new disorders to the DSS or new measures to the computation related to a disorder. Artificial intelligence and soft computing provide fuzzy logic to manage uncertainty and ambiguous situations. Fuzzy logic is embedded in an inference system to build a fuzzy inference system (FIS); specifically, a single-input rule modules connected zero-order Takagi-Sugeno FIS. The automatic creation of FISs is performed by the Aide-DS framework, which is capable of embedding the generated FISs in computerized clinical guidelines. It can also create a desktop application to execute the FIS. Technologically, it supports the addition of new target languages for the desktop applications and the inclusion of new ways of acquiring data.
                  DSScreening has been tested by comparing its predictions with the results of 152 real analyses from two groups: (1) NS samples and (2) clinical samples belonging to individuals of all ages with symptoms that do not necessarily correspond to an IEM. The system has reduced the time needed by 98.7% when compared to the interpretation time spent by laboratory professionals. Besides, it has correctly classified 100% of the NS samples and obtained an accuracy of 70% for samples belonging to individuals with clinical symptoms."
350,unknown,10.1049/ic:19960180,IET,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/578417/,1996-02-19 00:00:00,intelligent agents for distributed patient care,"The provision of medical care typically involves a number of individuals, located in a number of different institutions, whose decisions and actions need to be coordinated if the care is to be effective and efficient. To facilitate this decision making and to ensure the coordination process runs smoothly, the use of software support is becoming increasingly widespread. To this end, this paper describes an agent-based system which was developed to help manage the care process in real-world settings. The agents themselves are implemented using a layered architecture, called AADCare (Agent Architecture for Distributed Care), which combines a number of AI and agent techniques. The utility of this approach is demonstrated through the development of an application prototype for the clinical process of cancer treatment."
351,unknown,10.1109/bigdata.2017.8258089,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8258089/,2017-12-14 00:00:00,"flux: groupon's automated, scalable, extensible machine learning platform","As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company."
352,unknown,10.3390/drones4020018,Drones,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/4f6624b0fe7735dd4f7b703a1b27020d6f6df839,2020-01-01 00:00:00,sharkeye: real-time autonomous personal shark alerting via aerial surveillance,"While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens."
353,unknown,10.1109/ocit53463.2021.00036,2021 19th OITS International Conference on Information Technology (OCIT),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1c81ce769ea84598a5a94dd540cff152b21e5e09,2021-01-01 00:00:00,intelligent edge detection of attacks on ip-based iot deployments,"Edge-based computing and intelligence is a promising approach to reduce backhaul traffic load. This technology when applied to Intrusion Detection Systems (IDS), has the potential of detecting the edge-network intrusions in real-time, with minimal latency as compared to centralized cloud-based approaches. IP-based deployment of IoT has become common choice because of the wide availability of IP-based smart devices for building and home automation. In this paper, a novel intelligent edge-based architecture using machine learning, for classification, in order to detect intrusions in IP based IoT deployments is explored. The focus of this paper is to practically realize the proposed architecture as a prototype, rather than the application of machine learning techniques on available datasets. The paper presents the design details of this architecture. The results encompassing the functional and performance tests of this system prototype, as applied for the SYN-flood attack are shown. The engineering details namely, i) Feature extraction ii) Machine learning techniques used iii) Azure edge porting and usage on Raspberry Pi3 as edge device, which generates alerts to a cloud-based client, are detailed. The classification techniques XGBoost, Support Vector Method, and Logistic Regression are used and their performance is compared."
354,unknown,10.1109/access.2021.3074319,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9408578/,2021-01-01 00:00:00,hawk-eye: an ai-powered threat detector for intelligent surveillance cameras,"With recent advances in both AI and IoT capabilities, it is possible than ever to implement surveillance systems that can automatically identify people who might represent a potential security threat to the public in real-time. Imagine a surveillance camera system that can detect various on-body weapons, masked faces, suspicious objects and traffic. This system could transform surveillance cameras from passive sentries into active observers which would help in preventing a possible mass shooting in a school, stadium or mall. In this paper, we present a prototype implementation of such systems, Hawk-Eye, an AI-powered threat detector for smart surveillance cameras. Hawk-Eye can be deployed on centralized servers hosted in the cloud, as well as locally on the surveillance cameras at the network edge. Deploying AI-enabled surveillance applications at the edge enables the initial analysis of the captured images to take place on-site, which reduces the communication overheads and enables swift security actions. At the cloud side, we built a Mask R-CNN model that can detect suspicious objects in an image captured by a camera at the edge. The model can generate a high-quality segmentation mask for each object instance in the image, along with the confidence percentage and classification time. The camera side used a Raspberry Pi 3 device, Intel Neural Compute Stick 2 (NCS 2), and Logitech C920 webcam. At the camera side, we built a CNN model that can consume a stream of images directly from an on-site webcam, classify them, and displays the results to the user via a GUI-friendly interface. A motion detection module is developed to capture images automatically from the video when a new motion is detected. Finally, we evaluated our system using various performance metrics such as classification time and accuracy. Our experimental results showed an average overall prediction accuracy of 94% on our dataset."
355,unknown,10.1109/iciafs52090.2021.9605823,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9605823/,2021-08-13 00:00:00,deep learning & computer vision for iot based intelligent driver assistant system,"With the exponential increment of vehicles, roadside accidents also have been increasing rapidly where approximately 80% of these accidents were caused by human error. Therefore, the automobile industry and the government authorities are more focused on accident prevention by introducing improved road safety systems for the general public. Driver assistance system (DAS) is an intelligent road safety development where it senses the surrounding of a moving vehicle which will assist the driver to avoid the dangers and also warn the drivers of immediate dangers. With the current technological advancements, the automobile industry is equipped with the internet of things (IoT) based data transfer mechanisms, wherewith the concept of ‘connected car’ the passengers and the other interconnected vehicles with the internet can share data with back end applications. The data is consisting of the current location, the distance travelled by the vehicle, whether the vehicle requires urgent service. This study is mainly focused on the development of an intelligent driver assistance system based on computer vision and deep learning, which can prevent accidents with early detection of drowsiness, harmful objects and also by alerting the driver with the signboards and the road lines. The system is capable of passing the emergency messages to the driver as well as the other interconnected vehicles through the website by communicating the real-time road map generated within the system. The proposed system has been implemented and tested with multiple detection scenarios where machine learning has been employed to improve the accuracy of the results."
356,unknown,10.1016/j.envsci.2021.06.011,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85111282536,2021-10-01,a big data and artificial intelligence framework for smart and personalized air pollution monitoring and health management in hong kong,"All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries."
