id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract
1,unknown,10.1109/ispass48437.2020.00019,2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d7cc8603b7bdb2f8cb2c86778da3c803e308d7f9,2020-01-01 00:00:00,clan: continuous learning using asynchronous neuroevolution on commodity edge devices,"Recent advancements in machine learning algorithms, especially the development of Deep Neural Networks (DNNs) have transformed the landscape of Artificial Intelligence (AI). With every passing day, deep learning based methods are applied to solve new problems with exceptional results. The portal to the real world is the edge. The true impact of AI can only be fully realized if we can have AI agents continuously interacting with the real world and solving everyday problems. Unfortunately, high compute and memory requirements of DNNs acts a huge barrier towards this vision. Today we circumvent this problem by deploying special purpose inference hardware on the edge while procuring trained models from the cloud. This approach, however, relies on constant interaction with the cloud for transmitting all the data, training on massive GPU clusters, and downloading updated models. This is challenging for bandwidth, privacy, and constant connectivity concerns that autonomous agents may exhibit. In this paper we evaluate techniques for enabling adaptive intelligence on edge devices with zero interaction with any high-end cloud/server. We build a prototype distributed system of Raspberry Pis communicating via WiFi running NeuroEvolutionary (NE) learning and inference. We evaluate the performance of such a collaborative system and detail the compute/communication characteristics of different arrangements of the system that trade-off parallelism versus communication. Using insights from our analysis, we also propose algorithmic modifications to reduce communication by up to 3.6x during the learning phase to enhance scalability even further and match performance of higher end computing devices at scale. We believe that these insights will enable algorithm-hardware co-design efforts for enabling continuous learning on the edge."
2,unknown,http://arxiv.org/abs/2101.03989v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.03989v2,2021-01-11 00:00:00,technology readiness levels for machine learning systems,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics."
3,unknown,10.1109/bibm47256.2019.8983322,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8983322/,2019-11-21 00:00:00,openhi2 — open source histopathological image platform,"Transition from conventional to digital pathology requires a new category of biomedical informatic infrastructure which could facilitate delicate pathological routine. Pathological diagnoses are sensitive to many external factors and is known to be subjective. Only systems that can meet strict requirements in pathology would be able to run along pathological routines and eventually digitized the area, and the developed platform should comply with existing pathological routines and international standards. Currently, there are a number of available software tools which can perform histopathological tasks including virtual slide viewing, annotating, and basic image analysis, however, none of them can serve as a digital platform for pathology. Here we describe OpenHI2, an enhanced version Open Histopathological Image platform which is capable of supporting all basic pathological tasks and file formats; ready to be deployed in medical institutions on a standard server environment or cloud computing infrastructure. In this paper, we also describe the development decisions for the platform and propose solutions to overcome technical challenges including responsive region retrieval and viewing, virtual slide magnification, recording of diagnostic areas. These factors would promote OpenHI2 be used as a platform for histopathological images in real-world clinical settings. Furthermore, in research, OpenHI2 inherited the annotation functionality from the previous version, thus acquired annotations can be directly utilized by the newly added machine learning module which include popular machine learning models to perform tasks such as histology image classification and segmentation in the same environment. Addition can be made to the platform since each component is modularized and fully documented. OpenHI2 is free, open-source, and available at https://gitlab.com/BioAI/OpenHI."
4,unknown,10.1016/j.micpro.2020.103301,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85094168107,2021-02-01,iot enabled cancer prediction system to enhance the authentication and security using cloud computing,"In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results."
5,unknown,http://arxiv.org/abs/1911.05771v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1911.05771v1,2019-11-13 00:00:00,"machine learning based network vulnerability analysis of industrial
  internet of things","It is critical to secure the Industrial Internet of Things (IIoT) devices
because of potentially devastating consequences in case of an attack. Machine
learning and big data analytics are the two powerful leverages for analyzing
and securing the Internet of Things (IoT) technology. By extension, these
techniques can help improve the security of the IIoT systems as well. In this
paper, we first present common IIoT protocols and their associated
vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the
utilization of machine learning in countering these susceptibilities. Following
that, a literature review of the available intrusion detection solutions using
machine learning models is presented. Finally, we discuss our case study, which
includes details of a real-world testbed that we have built to conduct
cyber-attacks and to design an intrusion detection system (IDS). We deploy
backdoor, command injection, and Structured Query Language (SQL) injection
attacks against the system and demonstrate how a machine learning based anomaly
detection system can perform well in detecting these attacks. We have evaluated
the performance through representative metrics to have a fair point of view on
the effectiveness of the methods."
6,unknown,10.1007/978-981-15-5784-2_16,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-981-15-5784-2_16,2021-01-01 00:00:00,automatic classification of rotating machinery defects using machine learning (ml) algorithms,"Electric machines and motors have been the subject of enormous development. New concepts in design and control allow expanding their applications in different fields. The vast amount of data have been collected almost in any domain of interest. They can be static; that is to say, they represent real-world processes at a fixed point of time. Vibration analysis and vibration monitoring, including how to detect and monitor anomalies in vibration data are widely used techniques for predictive maintenance in high-speed rotating machines. However, accurately identifying the presence of a bearing fault can be challenging in practice, especially when the failure is still at its incipient stage, and the signal-to-noise ratio of the monitored signal is small. The main objective of this work is to design a system that will analyze the vibration signals of a rotating machine, based on recorded data from sensors, in the time/frequency domain. As a consequence of such substantial interest, there has been a dramatic increase of interest in applying Machine Learning (ML) algorithms to this task. An ML system will be used to classify and detect abnormal behavior and recognize the different levels of machine operation modes (normal, degraded, and faulty). The proposed solution can be deployed as predictive maintenance for Industry 4.0."
7,unknown,10.1109/icmla.2019.00115,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8999203/,2019-12-19 00:00:00,an edge computing visual system for vegetable categorization,"In self-service supermarket and retail industry, efforts to reduce customer wait time using automatic grocery item identification are challenged by low recognition accuracy, long response time and substantial requirement for equipment. In this paper, we propose a novel edge computing system named EdgeVegfru for vegetable and fruit image classification. While existing work on Vegfru dataset shows excellent performance, few of them have been deployed in real-world applications. We adopt an edge computing paradigm, design, implement and evaluate the whole system on the Android devices. The proposed deep learning model and quantization algorithm reduce the model size and inference time significantly. Our system has shown out-standing accuracy within limited time and computation resources, compared with other machine learning methods(such as Support Vector Machine(SVM), Random Forest(RF)), thus providing the potential path for automatic recognition and pricing in self-service retail stores."
8,unknown,10.1109/bigdata.2018.8622583,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8622583/,2018-12-13 00:00:00,ensemble machine learning systems for the estimation of steel quality control,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out."
9,unknown,10.1109/ijcnn52387.2021.9533808,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9533808/,2021-07-22 00:00:00,end-to-end federated learning for autonomous driving vehicles,"In recent years, with the development of computation capability in devices, companies are eager to investigate and utilize suitable ML/DL methods to improve their service quality. However, with the traditional learning strategy, companies need to first build up a powerful data center to collect and analyze data from the edge and then perform centralized model training, which turns out to be inefficient. Federated Learning has been introduced to solve this challenge. Because of its characteristics such as model-only exchange and parallel training, the technique can not only preserve user data privacy but also accelerate model training speed. The method can easily handle real-time data generated from the edge without taking up a lot of valuable network transmission resources. In this paper, we introduce an approach to end-to-end on-device Machine Learning by utilizing Federated Learning. We validate our approach with an important industrial use case in the field of autonomous driving vehicles, the wheel steering angle prediction. Our results show that Federated Learning can significantly improve the quality of local edge models and also reach the same accuracy level as compared to the traditional centralized Machine Learning approach without its negative effects. Furthermore, Federated Learning can accelerate model training speed and reduce the communication overhead, which proves that this approach has great strength when deploying ML/DL components to various real-world embedded systems."
10,unknown,10.1109/icws.2017.76,2017 IEEE International Conference on Web Services (ICWS),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/ca5fd02a1afb57b7dfd0c267dae923bf9d25f17c,2017-01-01 00:00:00,early air pollution forecasting as a service: an ensemble learning approach,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system."
11,unknown,http://arxiv.org/abs/2010.00432v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2010.00432v1,2020-10-01 00:00:00,"the rfml ecosystem: a look at the unique challenges of applying deep
  learning to radio frequency applications","While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces."
12,unknown,10.1109/trustcom50675.2020.00243,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9343128/,2021-01-01 00:00:00,monitoring social media for vulnerability-threat prediction and topic analysis,"Publicly available software vulnerabilities and exploit code are often abused by malicious actors to launch cyberattacks to vulnerable targets. Organizations not only have to update their software to the latest versions, but do effective patch management and prioritize security-related patching as well. In addition to intelligence sources such as Computer Emergency Response Team (CERT) alerts, cybersecurity news, national vulnerability database (NBD), and commercial cybersecurity vendors, social media is another valuable source that facilitates early stage intelligence gathering. To early detect future cyber threats based on publicly available resources on the Internet, we propose a dynamic vulnerability-threat assessment model to predict the tendency to be exploited for vulnerability entries listed in Common Vulnerability Exposures, and also to analyze social media contents such as Twitter to extract meaningful information. The model takes multiple aspects of vulnerabilities gathered from different sources into consideration. Features range from profile information to contextual information about these vulnerabilities. For the social media data, this study leverages machine learning techniques specially for Twitter which helps to filter out non-cybersecurity-related tweets and also label the topic categories of each tweet. When applied to predict the vulnerabilities exploitation and analyzed the real-world social media discussion data, it showed promising prediction accuracy with purified social media intelligence. Moreover, the AI-enabling modules have been deployed into a threat intelligence platform for further applications."
13,unknown,http://arxiv.org/abs/1908.08998v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1908.08998v2,2019-08-13 00:00:00,aibench: an industry standard internet service ai benchmark suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html."
14,unknown,http://arxiv.org/abs/2004.05740v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2004.05740v2,2020-04-13 00:00:00,"deep-edge: an efficient framework for deep learning model update on
  heterogeneous edge","Deep Learning (DL) model-based AI services are increasingly offered in a
variety of predictive analytics services such as computer vision, natural
language processing, speech recognition. However, the quality of the DL models
can degrade over time due to changes in the input data distribution, thereby
requiring periodic model updates. Although cloud data-centers can meet the
computational requirements of the resource-intensive and time-consuming model
update task, transferring data from the edge devices to the cloud incurs a
significant cost in terms of network bandwidth and are prone to data privacy
issues. With the advent of GPU-enabled edge devices, the DL model update can be
performed at the edge in a distributed manner using multiple connected edge
devices. However, efficiently utilizing the edge resources for the model update
is a hard problem due to the heterogeneity among the edge devices and the
resource interference caused by the co-location of the DL model update task
with latency-critical tasks running in the background. To overcome these
challenges, we present Deep-Edge, a load- and interference-aware,
fault-tolerant resource management framework for performing model update at the
edge that uses distributed training. This paper makes the following
contributions. First, it provides a unified framework for monitoring,
profiling, and deploying the DL model update tasks on heterogeneous edge
devices. Second, it presents a scheduler that reduces the total re-training
time by appropriately selecting the edge devices and distributing data among
them such that no latency-critical applications experience deadline violations.
Finally, we present empirical results to validate the efficacy of the framework
using a real-world DL model update case-study based on the Caltech dataset and
an edge AI cluster testbed."
15,unknown,10.1109/icecce49384.2020.9179349,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9179349/,2020-06-13 00:00:00,a cloud based smart recycling bin for in-house waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations."
16,included,10.1109/mocast49295.2020.9200283,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9200283/,2020-09-09 00:00:00,a cloud based smart recycling bin for waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low."
17,unknown,10.1016/j.engappai.2021.104316,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85108677220,2021-09-01,deep replacement: reinforcement learning based constellation management and autonomous replacement,"The Deep Reinforcement Learning (DRL) algorithm, Proximal Policy Optimization (PPO2), is deployed on a custom spacecraft (S/C) build and loss model to determine if an Artificial Intelligence (AI) can learn to monitor satellite constellation health and determine an optimal replacement strategy. A custom environment is created to simulate how S/C are built, launched, generate revenue, and finally decay. The reinforcement learning agent successfully learned an optimal policy for two models: a Simplified Model where the financial cost of actions is ignored; and an Advanced Model where the financial cost of actions is a major element. In both models the AI monitors the constellations and takes multiple strategic and tactical actions to replace satellites to maintain constellation performance. The Simplified Model showed that the PPO2 algorithm was able to converge on an optimal solution after 
                        ∼
                     200,000 simulations. The Advanced Model was much more difficult for the AI to learn, and thus, the performance drops during the early episodes, but eventually converges to an optimal policy at 
                        ∼
                     25,000,000 simulations. With the Advanced Model, the AI is taking actions that are successfully providing strategies for constellation management and satellite replacements which include these actions’ financial implications. Thus, the methods in this paper provide initial research developments towards a real-world tool and an AI application that can aid various Aerospace businesses in managing Low Earth Orbit (LEO) constellations. This type of AI application may become imperative for deploying and maintaining small satellite mega-constellations."
18,unknown,10.1016/s2589-7500(21)00272-7,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85126685401,2022-04-01,real-world evaluation of rapid and laboratory-free covid-19 triage for emergency care: external validation and pilot deployment of artificial intelligence driven screening,"Background
                  Uncertainty in patients' COVID-19 status contributes to treatment delays, nosocomial transmission, and operational pressures in hospitals. However, the typical turnaround time for laboratory PCR remains 12–24 h and lateral flow devices (LFDs) have limited sensitivity. Previously, we have shown that artificial intelligence-driven triage (CURIAL-1.0) can provide rapid COVID-19 screening using clinical data routinely available within 1 h of arrival to hospital. Here, we aimed to improve the time from arrival to the emergency department to the availability of a result, do external and prospective validation, and deploy a novel laboratory-free screening tool in a UK emergency department.
               
                  Methods
                  We optimised our previous model, removing less informative predictors to improve generalisability and speed, developing the CURIAL-Lab model with vital signs and readily available blood tests (full blood count [FBC]; urea, creatinine, and electrolytes; liver function tests; and C-reactive protein) and the CURIAL-Rapide model with vital signs and FBC alone. Models were validated externally for emergency admissions to University Hospitals Birmingham, Bedfordshire Hospitals, and Portsmouth Hospitals University National Health Service (NHS) trusts, and prospectively at Oxford University Hospitals, by comparison with PCR testing. Next, we compared model performance directly against LFDs and evaluated a combined pathway that triaged patients who had either a positive CURIAL model result or a positive LFD to a COVID-19-suspected clinical area. Lastly, we deployed CURIAL-Rapide alongside an approved point-of-care FBC analyser to provide laboratory-free COVID-19 screening at the John Radcliffe Hospital (Oxford, UK). Our primary improvement outcome was time-to-result, and our performance measures were sensitivity, specificity, positive and negative predictive values, and area under receiver operating characteristic curve (AUROC).
               
                  Findings
                  72 223 patients met eligibility criteria across the four validating hospital groups, in a total validation period spanning Dec 1, 2019, to March 31, 2021. CURIAL-Lab and CURIAL-Rapide performed consistently across trusts (AUROC range 0·858–0·881, 95% CI 0·838–0·912, for CURIAL-Lab and 0·836–0·854, 0·814–0·889, for CURIAL-Rapide), achieving highest sensitivity at Portsmouth Hospitals (84·1%, Wilson's 95% CI 82·5–85·7, for CURIAL-Lab and 83·5%, 81·8–85·1, for CURIAL-Rapide) at specificities of 71·3% (70·9–71·8) for CURIAL-Lab and 63·6% (63·1–64·1) for CURIAL-Rapide. When combined with LFDs, model predictions improved triage sensitivity from 56·9% (51·7–62·0) for LFDs alone to 85·6% with CURIAL-Lab (81·6–88·9; AUROC 0·925) and 88·2% with CURIAL-Rapide (84·4–91·1; AUROC 0·919), thereby reducing missed COVID-19 cases by 65% with CURIAL-Lab and 72% with CURIAL-Rapide. For the prospective deployment of CURIAL-Rapide, 520 patients were enrolled for point-of-care FBC analysis between Feb 18 and May 10, 2021, of whom 436 received confirmatory PCR testing and ten (2·3%) tested positive. Median time from arrival to a CURIAL-Rapide result was 45 min (IQR 32–64), 16 min (26·3%) sooner than with LFDs (61 min, 37–99; log-rank p<0·0001), and 6 h 52 min (90·2%) sooner than with PCR (7 h 37 min, 6 h 5 min to 15 h 39 min; p<0·0001). Classification performance was high, with sensitivity of 87·5% (95% CI 52·9–97·8), specificity of 85·4% (81·3–88·7), and negative predictive value of 99·7% (98·2–99·9). CURIAL-Rapide correctly excluded infection for 31 (58·5%) of 53 patients who were triaged by a physician to a COVID-19-suspected area but went on to test negative by PCR.
               
                  Interpretation
                  Our findings show the generalisability, performance, and real-world operational benefits of artificial intelligence-driven screening for COVID-19 over standard-of-care in emergency departments. CURIAL-Rapide provided rapid, laboratory-free screening when used with near-patient FBC analysis, and was able to reduce the number of patients who tested negative for COVID-19 but were triaged to COVID-19-suspected areas.
               
                  Funding
                  The Wellcome Trust, University of Oxford Medical and Life Sciences Translational Fund."
19,unknown,10.1109/icra48506.2021.9561747,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9561747/,2021-06-05 00:00:00,pylot: a modular platform for exploring latency-accuracy tradeoffs in autonomous vehicles,"We present Pylot, a platform for autonomous vehicle (AV) research and development, built with the goal to allow researchers to study the effects of the latency and accuracy of their models and algorithms on the end-to-end driving behavior of an AV. This is achieved through a modular structure enabled by our high-performance dataflow system that represents AV software pipeline components (object detectors, motion planners, etc.) as a dataflow graph of operators which communicate on data streams using timestamped messages. Pylot readily interfaces with popular AV simulators like CARLA, and is easily deployable to real-world vehicles with minimal code changes.To reduce the burden of developing an entire pipeline for evaluating a single component, Pylot provides several state-of-the-art reference implementations for the various components of an AV pipeline. Using these reference implementations, a Pylot-based AV pipeline is able to drive a real vehicle, and attains a high score on the CARLA Autonomous Driving Challenge. We also present several case studies enabled by Pylot, including evidence of a need for context-dependent components, and per-component time allocation. Pylot is open source, with the code available at https://github.com/erdos-project/pylot."
20,unknown,10.1016/j.cmpb.2018.04.030,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85046622771,2018-08-01,computerized decision support for beneficial home-based exercise rehabilitation in patients with cardiovascular disease,"Background
                  Exercise-based rehabilitation plays a key role in improving the health and quality of life of patients with Cardiovascular Disease (CVD). Home-based computer-assisted rehabilitation programs have the potential to facilitate and support physical activity interventions and improve health outcomes.
               
                  Objectives
                  We present the development and evaluation of a computerized Decision Support System (DSS) for unsupervised exercise rehabilitation at home, aiming to show the feasibility and potential of such systems toward maximizing the benefits of rehabilitation programs.
               
                  Methods
                  The development of the DSS was based on rules encapsulating the logic according to which an exercise program can be executed beneficially according to international guidelines and expert knowledge. The DSS considered data from a prescribed exercise program, heart rate from a wristband device, and motion accuracy from a depth camera, and subsequently generated personalized, performance-driven adaptations to the exercise program. Communication interfaces in the form of RESTful web service operations were developed enabling interoperation with other computer systems.
               
                  Results
                  The DSS was deployed in a computer-assisted platform for exercise-based cardiac rehabilitation at home, and it was evaluated in simulation and real-world studies with CVD patients. The simulation study based on data provided from 10 CVD patients performing 45 exercise sessions in total, showed that patients can be trained within or above their beneficial HR zones for 67.1 ± 22.1% of the exercise duration in the main phase, when they are guided with the DSS. The real-world study with 3 CVD patients performing 43 exercise sessions through the computer-assisted platform, showed that patients can be trained within or above their beneficial heart rate zones for 87.9 ± 8.0% of the exercise duration in the main phase, with DSS guidance.
               
                  Conclusions
                  Computerized decision support systems can guide patients to the beneficial execution of their exercise-based rehabilitation program, and they are feasible."
21,unknown,http://arxiv.org/abs/2011.09463v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.09463v3,2020-11-18 00:00:00,"easytransfer -- a simple and scalable deep transfer learning platform
  for nlp applications","The literature has witnessed the success of leveraging Pre-trained Language
Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural
Language Processing (NLP) applications, yet it is not easy to build an
easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the
EasyTransfer platform is designed to develop deep TL algorithms for NLP
applications. EasyTransfer is backended with a high-performance and scalable
engine for efficient training and inference, and also integrates comprehensive
deep TL algorithms, to make the development of industrial-scale TL applications
easier. In EasyTransfer, the built-in data and model parallelism strategies,
combined with AI compiler optimization, show to be 4.0x faster than the
community version of distributed training. EasyTransfer supports various NLP
models in the ModelZoo, including mainstream PLMs and multi-modality models. It
also features various in-house developed TL algorithms, together with the
AppZoo for NLP applications. The toolkit is convenient for users to quickly
start model training, evaluation, and online deployment. EasyTransfer is
currently deployed at Alibaba to support a variety of business scenarios,
including item recommendation, personalized search, conversational question
answering, etc. Extensive experiments on real-world datasets and online
applications show that EasyTransfer is suitable for online production with
cutting-edge performance for various applications. The source code of
EasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer)."
22,unknown,10.1016/j.cose.2015.04.002,scopus,sciencedirect,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84940722662,2015-09-01,banksealer: a decision support system for online banking fraud analysis and investigation,"The significant growth of online banking frauds, fueled by the underground economy of malware, raised the need for effective fraud analysis systems. Unfortunately, almost all of the existing approaches adopt black box models and mechanisms that do not give any justifications to analysts. Also, the development of such methods is stifled by limited Internet banking data availability for the scientific community. In this paper we describe BankSealer, a decision support system for online banking fraud analysis and investigation. During a training phase, BankSealer builds easy-to-understand models for each customer's spending habits, based on past transactions. First, it quantifies the anomaly of each transaction with respect to the customer historical profile. Second, it finds global clusters of customers with similar spending habits. Third, it uses a temporal threshold system that measures the anomaly of the current spending pattern of each customer, with respect to his or her past spending behavior. With this threefold profiling approach, it mitigates the under-training due to the lack of historical data for building well-trained profiles, and the evolution of users' spending habits over time. At runtime, BankSealer supports analysts by ranking new transactions that deviate from the learned profiles, with an output that has an easily understandable, immediate statistical meaning.
                  Our evaluation on real data, based on fraud scenarios built in collaboration with domain experts that replicate typical, real-world attacks (e.g., credential stealing, banking trojan activity, and frauds repeated over time), shows that our approach correctly ranks complex frauds. In particular, we measure the effectiveness, the computational resource requirements and the capabilities of BankSealer to mitigate the problem of users that performed a low number of transactions. Our system ranks frauds and anomalies with up to 98% detection rate and with a maximum daily computation time of 4 min. Given the good results, a leading Italian bank deployed a version of BankSealer in their environment to analyze frauds."
23,included,10.1109/icccn52240.2021.9522281,2021 International Conference on Computer Communications and Networks (ICCCN),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e5db631b9237de584eadab60dd6529470438ad5d,2021-01-01 00:00:00,realization of an intrusion detection use-case in onap with acumos,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms."
24,included,10.1109/isie45063.2020.9152441,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9152441/,2020-06-19 00:00:00,deployment of a smart and predictive maintenance system in an industrial case study,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions."
25,unknown,10.1016/j.compind.2020.103244,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85084401966,2020-09-01,machine learning for predictive scheduling and resource allocation in large scale manufacturing systems,"The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud.
                  Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data."
26,unknown,10.1109/ieeeconf49454.2021.9382607,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9382607/,2021-01-14 00:00:00,teaching system for multimodal object categorization by human-robot interaction in mixed reality,"As service robots are becoming essential to support aging societies, teaching them how to perform general service tasks is still a major challenge preventing their deployment in daily-life environments. In addition, developing an artificial intelligence for general service tasks requires bottom-up, unsupervised approaches to let the robots learn from their own observations and interactions with the users. However, compared to the top-down, supervised approaches such as deep learning where the extent of the learning is directly related to the amount and variety of the pre-existing data provided to the robots, and thus relatively easy to understand from a human perspective, the learning status in bottom-up approaches is by their nature much harder to appreciate and visualize. To address these issues, we propose a teaching system for multimodal object categorization by human-robot interaction through Mixed Reality (MR) visualization. In particular, our proposed system enables a user to monitor and intervene in the robot&#x2019;s object categorization process based on Multimodal Latent Dirichlet Allocation (MLDA) to solve unexpected results and accelerate the learning. Our contribution is twofold by 1) describing the integration of a service robot, MR interactions, and MLDA object categorization in a unified system, and 2) proposing an MR user interface to teach robots through intuitive visualization and interactions."
27,unknown,http://arxiv.org/abs/2102.12165v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.12165v1,2021-02-24 00:00:00,"efficient low-latency dynamic licensing for deep neural network
  deployment on edge devices","Along with the rapid development in the field of artificial intelligence,
especially deep learning, deep neural network applications are becoming more
and more popular in reality. To be able to withstand the heavy load from
mainstream users, deployment techniques are essential in bringing neural
network models from research to production. Among the two popular computing
topologies for deploying neural network models in production are
cloud-computing and edge-computing. Recent advances in communication
technologies, along with the great increase in the number of mobile devices,
has made edge-computing gradually become an inevitable trend. In this paper, we
propose an architecture to solve deploying and processing deep neural networks
on edge-devices by leveraging their synergy with the cloud and the
access-control mechanisms of the database. Adopting this architecture allows
low-latency DNN model updates on devices. At the same time, with only one model
deployed, we can easily make different versions of it by setting access
permissions on the model weights. This method allows for dynamic model
licensing, which benefits commercial applications."
28,unknown,10.1109/ictc.2017.8190968,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8190968/,2017-10-20 00:00:00,unmanned aerial vehicle surveillance system (uavss) for forest surveillance and data acquisition,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture."
29,unknown,10.1109/isc246665.2019.9071705,2019 IEEE International Smart Cities Conference (ISC2),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1c2a98e580f7c222e052fd623cb6f8ceed49e110,2019-01-01 00:00:00,on the use of machine learning for state-of- charge forecasting in electric vehicles,"nowadays, it is well known that a main solution for pollution reduction in cities will be the introduction of electric and hybrid vehicles on transportation roads. Many research efforts have been dedicated to develop new technologies to further promote the use of this type of vehicles. However, their penetration on transportation roads faces some obstacles that have not yet been fully tackled. For instance, the development of intelligent battery management systems needs to be further investigated taking into consideration the uncertainty linked to how vehicles will perform in different scenarios, such as traffic situation, driver behavior, and road profile. The work presented in this study is towards developing a battery management system by investigating new approaches for accurate estimation and prediction of remaining charge, the expected lifetime of the batteries and the remaining driving rang. We focus mainly on the integration of predictive analytics techniques for forecasting the state-of-charge. We first deployed statistical- and machine learning based techniques in real-sitting scenarios (LSTM, ARIMA and XGBoost). Experiments have been conducted using an electric vehicle platform and results are reported to shed more light on their accuracy for multiple-horizon forecasts of battery’s state of charge."
30,unknown,10.4043/29335-ms,"Day 2 Tue, May 07, 2019",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/095aeceba84ba188a2b3bb6e086d3755f11e824a,2019-01-01 00:00:00,"using augmented intelligence to automate subsea inspection data acquisition, processing, analysis, reporting and access","
 Augmented Intelligence (AI2) involves fusing Analyst Intuition with Artificial Intelligence to deliver an optimised combination of human-machine decision support.
 AI2 is being incorporated by i-Tech Services / Leidos into the physical inspection of offshore Oil, Gas, and Renewables assets, delivering valuable data driven insights that contribute to greater efficiency, enhanced condition monitoring, improved asset integrity and asset life extension.
 The deployment of vehicular and diver assets to obtain such inspection data, with associated support vessels, remains a major cost challenge for Operators.
 We believe the industry needs to approach this challenge from two key directions. Firstly, through the application of autonomous systems for data acquisition and delivery, reducing vessel reliance, and secondly through automating the acquisition and processing of data and maximising the insight provided by the data.
 This paper will examine the use of Augmented Intelligence to optimise the Subsea Inspection data workflow as a key use case, to demonstrate the principles.
 The historic paradigm consists of a fragmented evolving approach, with insufficient consideration and design across all the sensors, processing analytical engines and data visualisation. The approach being adopted is to closely link all aspects of the data workflow, within the context of delivering the data and beyond in terms of harvesting additional insight and value.
 To achieve the optimum workflow a number of developmental initiatives are being knitted into a modular platform, each element providing standalone value but the sum of the parts generates the most significant value and cost reduction.
 The elements being combined are automatic data quality control at acquisition source and through the full workflow, automated processing, machine vision for object recognition and reporting and machine learning to optimise the system intelligence. All of these are designed to augment the expertise of the analyst / user, detecting change to learnt parameters, by using real time data and critically by referencing large historical data sets and as-built data.
 The outputs from a system holistic approach will be improved data acquisition with more efficient high quality right first time data reporting. In addition layers of analytics, with smart, intuitive data access and retrieval will optimise delivery of key information within large data sets, together with maximising value and insight."
31,unknown,10.1109/wispnet.2017.8300034,"2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/5d449d17d0babccd02f8557b09622616b9b7af9a,2017-01-01 00:00:00,a reliable and secure approach for efficient car-to-car communication in intelligent transportation systems,"Intelligent Transportation System (ITS) is considered to be an integral part of the Smart City concept. As per the idea of ITS, intelligence incorporated in existing vehicles, both private and public, is expected to make vehicle management inexpensive and less complex. Some of the barriers in successful deployment of ITS have been its reliability, scalability, interoperability and security. Communication plays a big role in ITS and Wi-Fi/ cellular technologies have been used to facilitate Car-to-Server (C2S) and Server-to-Server (S2S) type communication. Car-to-Car (C2C) communication emerges as a more resource constrained and challenging model. Due to strict real-time, low delay and high security requirements, C2C presents several avenues for research. In this work, we investigate the network and security concerns of C2C architecture of ITS. Based on our study, we propose an effective approach for C2C communication using event-triggered broadcast of information. We use Public Key Infrastructure (PKI) based sender authentication for information verification. Furthermore, we provide Machine Learning based solutions to some common problems encountered during C2C communication."
32,unknown,10.2147/rmhp.s338186,Risk management and healthcare policy,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/86041f9ba7e725862af6c84126d6d0122be799e5,2022-01-01 00:00:00,a sustainable model for emergency medical services in developing countries: a novel approach using partial outsourcing and machine learning,"Introduction Unlike Western countries, many low- and middle-income countries (LMIC), like India, have a de-centralized emergency medical services (EMS) involving both semi-government and non-government organizations. It is alarming that due to the absence of a common ecosystem, the utilization of resources is inefficient, which leads to shortage of available vehicles and larger response time. Fragmentation of emergency supply chain resources motivates us to propose a new vehicle routing and scheduling model equipped with novel features to ensure minimal response time using existing resources. Materials and Methods The data set of medical and fire-related emergencies from January 2018 to May 2018 of Uttarakhand State in India was provided by GVK Emergency Management and Research Institute (GVK EMRI) also known as 108 EMSs was used in the study. The proposed model integrates all the available EMS vehicles including partial outsourcing to non-ambulatory vehicles like police vans, taxis, etc., using a novel two-echelon heuristic approach. In the first stage, an offline learning model is developed to yield the deployment strategy for EMS vehicles. Seven well researched machine learning (ML) algorithms were analyzed for parameter prediction namely random forest (RF), convolutional neural network (CNN), k-nearest neighbor (KNN), classification and regression tree (CART), support vector machine (SVM), logistic regression (LR), and linear discriminant analysis (LDA). In the second stage, a real-time routing model is proposed for EMS vehicle routing at the time of emergency, considering partial outsourcing. Results and Discussion The results indicate that the RF classifier outperforms the LR, LDA, SVM, CNN, CART and NB classifier in terms of both accuracy as well as F-1 score. The proposed vehicle routing and scheduling model for automated decision-making shows an improvement of 42.1%, 54%, 27.9% and 62% in vehicle assignment time, vehicle travel time from base to scene, travel time from scene to hospital, and total response time, respectively, in urban areas."
33,unknown,10.1109/icoei.2019.8862754,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8862754/,2019-04-25 00:00:00,machine learning based health prediction system using ibm cloud as paas,"Adaptable Critical Patient Caring system is a key concern for hospitals in developing countries like Bangladesh. Most of the hospital in Bangladesh lack serving proper health service due to unavailability of appropriate, easy and scalable smart systems. The aim of this project is to build an adequate system for hospitals to serve critical patients with a real-time feedback method. In this paper, we propose a generic architecture, associated terminology and a classificatory model for observing critical patient's health condition with machine learning and IBM cloud computing as Platform as a service (PaaS). Machine Learning (ML) based health prediction of the patients is the key concept of this research. IBM Cloud, IBM Watson studio is the platform for this research to store and maintain our data and ml models. For our ml models, we have chosen the following Base Predictors: Naïve Bayes, Logistic Regression, KNeighbors Classifier, Decision Tree Classifier, Random Forest Classifier, Gradient Boosting Classifier, and MLP Classifier. For improving the accuracy of the model, the bagging method of ensemble learning has been used. The following algorithms are used for ensemble learning: Bagging Random Forest, Bagging Extra Trees, Bagging KNeighbors, Bagging SVC, and Bagging Ridge. We have developed a mobile application named “Critical Patient Management System - CPMS” for real-time data and information view. The system architecture is designed in such a way that the ml models can train and deploy in a real-time interval by retrieving the data from IBM Cloud and the cloud information can also be accessed through CPMS in a requested time interval. To help the doctors, the ml models will predict the condition of a patient. If the prediction based on the condition gets worse, the CPMS will send an SMS to the duty doctor and nurse for getting immediate attention to the patient. Combining with the ml models and mobile application, the project may serve as a smart healthcare solution for the hospitals."
34,unknown,10.1109/phm-besancon49106.2020.00009,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9115516/,2020-05-07 00:00:00,machine performance monitoring and fault classification using vibration frequency analysis,"Machine anomalies in manufacturing directly affect the production yield and factory operation efficiency if such anomalies cannot be detected in time. Real-time monitoring of machine health condition not only improves machine throughput by reducing unplanned downtime caused by machine failure but also saves cost for unnecessary routine maintenance. This paper presents a systematic approach for real-time or near real-time machine performance monitoring solution development from data collection, feature extraction, data analytics to real-time machine fault and machine status classification. Three data-driven machine-learning approaches using one vibration sensor data are proposed to detect two common machine failure modes during machine turning process. To evaluate the the performance of each approach, three machine-learning algorithms (Random Forest, K Nearest Neighborhood, and Support Vector Machine) are implemented and tested. Evaluation results on the actual machine data shows that a two-layered classification structure with random forest algorithm as the base has high classification accuracy on the machine status including machine fault detection. The developed data-driven machine health monitoring solution is deployed in the IoT device for real-time data collection and processing and results are sent data server for data visualization."
35,unknown,10.1109/snpdwinter52325.2021.00019,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9403457/,2021-01-30 00:00:00,machine learning based network intrusion detection for data streaming iot applications,"In recent years, Internet of Things (IoT) technologies have been widely used in many fields such as surveillance, health-care, smart metering and environment monitoring. This extensive usage leads to massive data management and a complexity in data analysis. A huge number of IoT sensors are deployed for monitoring task and send continuously their collected data to gateways. IoT applications are analyzing these data flows and making real time decisions about specific monitored events (fire, flood, terrorist attacks, etc.). Anomalies that may be related to sensor failures or network intrusions are affecting such decisions. Therefore, they should be detected and eliminated as soon as they arrive. This task requires real time data processing detectors for making accurate and fast predictions. In this paper, we design an architecture for a real time network intrusion detection system for IoT streaming data. The system was developed, deployed and tested with the two leading stream processing frameworks (Apache Flink and Apache Spark Streaming). We used two different public datasets and different machine learning algorithms. Results show considerable throughputs and high detection accuracy especially for Apache Flink."
36,unknown,10.1016/j.eswa.2021.114951,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85104365062,2021-09-01,providing music service in ambient intelligence: experiments with gym users,"Ambient Intelligence (AmI) is an interdisciplinary research area of ICT which has evolved since the 90s, taking great advantage from the advent of the Internet of Things (IoT). AmI creates, by using Artificial Intelligence (AI), an intelligent ecosystem in which computers, sensors, lighting, music, personal devices, and distributed services, work together to improve the user experience through the support of natural and intuitive user interfaces. Nowadays, AmI is used in various contexts, e.g., for building smart homes and smart cities, providing healthcare, and creating an adequate atmosphere in retail and public environments.
                  In this paper, we propose a novel AmI system for gym environments, named Gym Intelligence, able to provide adequate music atmosphere, according to the users’ physical effort during the training. The music is taken from Spotify and is classified according to some music features, as provided by Spotify itself. The system is based on a multi-agent computational intelligence model built on two main components: 
                        
                           (
                           i
                           )
                        
                      machine learning methods that forecast appropriate values for the Spotify music features, and 
                        
                           (
                           ii
                           )
                        
                      a multi-objective dynamic genetic algorithm that selects a specific Spotify music track, according to such values. Gym Intelligence is built by sensing the ambient with a minimal, low-cost, and non-intrusive set of sensors, and it has been designed considering the outcome of a preliminary analysis in real gyms, involving real users. We have considered well-known regression methods and we have validated them using a collected data 
                        
                           (
                           i
                           )
                        
                      about the users’ physical effort, through the sensors, and 
                        
                           (
                           ii
                           )
                        
                      about the users’ music preferences, through an Android app that the users have used during the training. Among the regression methods considered, the one that provided the best results is the Random Forest, which predicted Spotify music features with a mean absolute error of 0.02 and a root mean squared error of 0.05. We have implemented Gym Intelligence and deployed it in five real gyms. We have evaluated it conducting several experiments. The experiments show how, with the help of Gym Intelligence, the users’ satisfaction about the provided background music, rose from 3.05 to 4.91 (on a scale from 1 to 5, where 5 is the maximum score)."
37,included,10.1016/j.iot.2020.100185,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85086362688,2020-09-01,highly-efficient fog-based deep learning aal fall detection system,"Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement."
38,unknown,10.1016/j.compeleceng.2017.03.009,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85015331423,2018-01-01,applying spark based machine learning model on streaming big data for health status prediction,"Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action."
39,unknown,10.1109/access.2021.3051583,IEEE Access,semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/69fb4c8b9bd2ea88c0ba24979c715e9bb580d363,2021-01-01 00:00:00,data-driven condition monitoring of mining mobile machinery in non-stationary operations using wireless accelerometer sensor modules,"This paper presents the development of an easy-to-deploy and smart monitoring IoT system that utilizes vibration measurement devices to assess real-time condition of bulldozers, power shovels and backhoes, in non-stationary operations in the mining industry. According to operating experience data and the type of mining machine, total loss failure rates per machine fleet can reach up to 30%. Vibration analysis techniques are commonly used for condition monitoring and early detection of unforeseen failures to generate predictive maintenance plans for heavy machinery. However, this maintenance strategy is intensively used only for stationary machines and/or mobile machinery in stationary operations. Today, there is a lack of proper solutions to detect and prevent critical failures for non-stationary machinery. This paper shows a cost-effective solution proposal for implementing a vibration sensor network with wireless communication and machine learning data-driven capabilities for condition monitoring of non-stationary heavy machinery in mining operations. During the machine operation, 3-axis accelerations were measured using two sensors deployed across the machine. The machine accelerations (amplitudes and frequencies) are measured in two different frequency spectrums to improve each sensing location’s time resolution. Multiple machine learning algorithms use this machine data to assess conditions according to manufacturer recommendations and operational benchmarks Proposed data-driven machine learning models classify the machine condition in states according to the ISO 2372 standards for vibration severity: Good, Acceptable, Unsatisfactory, or Unacceptable. After performing field tests with bulldozers and backhoes from different manufacturers, the machine learning algorithms are able to classify machine health status with an accuracy between 85% - 95%. Moreover, the system allows early detection of “Unacceptable” states between 120 to 170 hours prior to critical failure. These results demonstrate that the proposed system will collect relevant data to generate predictive maintenance plans and avoid unplanned downtimes."
40,unknown,10.1109/fmec.2019.8795327,2019 Fourth International Conference on Fog and Mobile Edge Computing (FMEC),semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/363906abc17b21b3fed028edd6387b7b4cffd8cc,2019-01-01 00:00:00,health monitoring with low power iot devices using anomaly detection algorithm,"The healthcare industry is rapidly adopting new technologies such as the Internet of Things (IoT), which are dropping costs and improving healthcare outcomes. Such IoT systems typically include edge devices (glucose monitors, ventilators, pacemakers), gateway devices that aggregate the data from the edge devices and transmit it to the cloud, and cloud-based systems which analyze the device data to draw conclusions, display information, or direct the connected devices to take action. This process can lead to communication lags and delayed responses to patient conditions/treatment. The aim of this proposal is to overcome these delays with IoT technology and allow for prompt urgent treatment to patients. The solution proposed includes a model to monitor and process the data disseminated by wearable devices related to the patients’ health issues and connect the data to IoT cloud platforms. Analysis of the patients’ health data to identify anomalies will be performed at the device level by developing an offline machine learning model using specific algorithms for anomaly detection and deploying them on the IoT devices or IoT gateway. Processing of the real-time health data will be performed at the device level and the prediction of anomalous data will be sent to the third-party cloud for implementing any necessary actions."
41,unknown,10.2118/204794-ms,"Day 4 Wed, December 01, 2021",semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d11c7341d3bcc4b0c73076048dcd10b35748f355,2021-01-01 00:00:00,satellite fields digitalization & als optimization with edge & advance analytics application,"
 Data monitoring in remote satellite field without any DOF platform is a challenging task but critical for ALS monitoring and optimization. In SRP wells the VFD data collection is important for analysis of downhole pump behavior and system health. SRP maintenance crew collects data from VFDs daily, but it is time consuming and can target only few wells in a day. The steps from requirement of dyna to final decision taken for ALS optimization are mobilizing team, permits approvals, download data, e-mail dynacards, dyna visualization, final decision.
 The problems with above process were: -
 Insufficient and discrete data for any post-failure analysis or ALS-optimization Minimal data to investigate the pre failure events
 The lack of real time monitoring was resulting in well downtime and associated production loss. The combination of IOT, Cloud Computing and Machine learning was implemented to shift from the reactive to proactive approach which helped in ALS Optimization and reduced production loss.
 The data was transmitted to a Cloud server and further it was transmitted to web-based app. Since thousands of Dynacards are generated in a day, hence it requires automated classification using computer driven pattern recognition techniques. The real time data is used for analysis involving basic statistic and Machine learning algorithms. The critical pump signatures were identified using machine learning libraries and email is generated for immediate action. Several informative dashboards were developed which provide quick analysis of ALS performance. The types of dashboard are as below
 Well Operational Status Dynacards Interpretation module SRP parameters visualization Machine Learning model calibration module Pump Performance Statistics
 After collection of enough data and creation of analytical dashboards on the three wells using domain knowledge the gained insights were used for ALS optimization. To keep the model in an evergreen high-confidence prediction state, inputs from domain experts are often required. After regular fine-tuning the prediction accuracy of the ML model increased to 80-85 %. In addition, system was made flexible so that a new algorithm can be deployed when required. Smart Alarms were generated involving statistic and Machine Learning by the system which gives alerts by e-mail if an abnormal behavior or erratic dynacards were identified. This helped in reduction of well downtime in some events which were treated instinctively before.
 The integration of domain knowledge and digitalization enables an engineer to take informed and effective decisions. The techniques discussed above can be implemented in marginal fields where DOF implementation is logistically and economically challenged. EDGE along with advanced analytics will gain more technological advances and can be used in other potential domains as well in near future."
42,unknown,10.3390/ijerph18137087,International journal of environmental research and public health,semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/176d31f73bc65e07dc6e650b75404c31666dcdbb,2021-01-01 00:00:00,design of a spark big data framework for pm2.5 air pollution forecasting,"In recent years, with rapid economic development, air pollution has become extremely serious, causing many negative effects on health, environment and medical costs. PM2.5 is one of the main components of air pollution. Therefore, it is necessary to know the PM2.5 air quality in advance for health. Many studies on air quality are based on the government’s official air quality monitoring stations, which cannot be widely deployed due to high cost constraints. Furthermore, the update frequency of government monitoring stations is once an hour, and it is hard to capture short-term PM2.5 concentration peaks with little warning. Nevertheless, dealing with short-term data with many stations, the volume of data is huge and is calculated, analyzed and predicted in a complex way. This alleviates the high computational requirements of the original predictor, thus making Spark suitable for the considered problem. This study proposes a PM2.5 instant prediction architecture based on the Spark big data framework to handle the huge data from the LASS community. The Spark big data framework proposed in this study is divided into three modules. It collects real time PM2.5 data and performs ensemble learning through three machine learning algorithms (Linear Regression, Random Forest, Gradient Boosting Decision Tree) to predict the PM2.5 concentration value in the next 30 to 180 min with accompanying visualization graph. The experimental results show that our proposed Spark big data ensemble prediction model in next 30-min prediction has the best performance (R2 up to 0.96), and the ensemble model has better performance than any single machine learning model. Taiwan has been suffering from a situation of relatively poor air pollution quality for a long time. Air pollutant monitoring data from LASS community can provide a wide broader monitoring, however the data is large and difficult to integrate or analyze. The proposed Spark big data framework system can provide short-term PM2.5 forecasts and help the decision-maker to take proper action immediately."
43,unknown,http://arxiv.org/abs/2207.03066v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2207.03066v1,2022-07-07 00:00:00,device-cloud collaborative recommendation via meta controller,"On-device machine learning enables the lightweight deployment of
recommendation models in local clients, which reduces the burden of the
cloud-based recommenders and simultaneously incorporates more real-time user
features. Nevertheless, the cloud-based recommendation in the industry is still
very important considering its powerful model capacity and the efficient
candidate generation from the billion-scale item pool. Previous attempts to
integrate the merits of both paradigms mainly resort to a sequential mechanism,
which builds the on-device recommender on top of the cloud-based
recommendation. However, such a design is inflexible when user interests
dramatically change:
  the on-device model is stuck by the limited item cache while the cloud-based
recommendation based on the large item pool do not respond without the new
re-fresh feedback.
  To overcome this issue, we propose a meta controller to dynamically manage
the collaboration between the on-device recommender and the cloud-based
recommender, and introduce a novel efficient sample construction from the
causal perspective to solve the dataset absence issue of meta controller. On
the basis of the counterfactual samples and the extended training, extensive
experiments in the industrial recommendation scenarios show the promise of meta
controller in the device-cloud collaboration."
44,included,10.1109/med.2017.7984310,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7984310/,2017-07-06 00:00:00,cloud computing for big data analytics in the process control industry,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0."
45,unknown,10.23919/aeit.2018.8577226,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8577226/,2018-10-05 00:00:00,smart farms for a sustainable and optimized model of agriculture,"Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The `smart farm' model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business."
46,unknown,10.1016/j.ijpe.2021.108296,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85114948077,2021-12-01,an integrated delphi-mcdm-bayesian network framework for production system selection,"Several attempts are needed to choose the most compatible production system for achieving the desired manufacturing outputs. The significant role of manufacturing strategy deployment is selecting the production system best suited for a manufacturing firm. The appropriately chosen production system (strategic process choice) facilitates a firm to produce “order winning” outputs and provides a production competence to achieve business success. This research presents a novel framework to determine the compatible production system for a manufacturing firm. An integrated three-stage Delphi-MCDM-Bayesian Network (BN) framework has been proposed. The process choice criteria (PCC) considered for deciding production systems are identified through an in-depth literature review and then validated by experts through a Delphi method in the first stage. It resulted in the determination of twenty-six PCC. In the second stage, the multi-criteria decision-making (MCDM) based voting analytical hierarchy process (VAHP) method is adopted to determine each criterion's relative importance for a firm. The relative weights obtained are then used as input for the machine learning (ML) technique- Bayesian network (BN) in the third stage. The BN model quantifies the selection probability of production systems. The proposed Delphi-MCDM-BN framework is demonstrated using a real-life case of a “hydraulic and pneumatic valve” manufacturing firm to select a suitable production system. The three-stage framework is a novel contribution to the literature, which can be used by researchers, practitioners, and manufacturing strategists to choose an appropriate production system for any manufacturing firm."
47,unknown,10.1016/j.aei.2019.101013,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85075778987,2020-01-01,guidelines for applied machine learning in construction industry—a case of profit margins estimation,"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects."
48,unknown,10.1109/icccnt45670.2019.8944586,"2019 10th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/6d313954e6b7e5f83eb49dc05d68934aa1a627c8,2019-01-01 00:00:00,blockchain and anomaly detection based monitoring system for enforcing wastewater reuse,"Industries, household communities consume a lot of water on regular basis, thereby increasing water crisis. There is continuous increase in water consumption by industries. Reusing wastewater can reduce water withdrawals from local water sources thus increasing water availability, lowering wastewater discharges and their pollutant load, reducing thermal energy consumption and, potentially, processing cost. Various ways have been implemented for recycling the generated wastewater. Wastewater must be reused for the benefit of mankind. In this paper we propose a wastewater recycle control system to efficiently manage the wastewater and coordinate it among the industries and the government. Blockchain technology has been deployed for storing data and developing an incentive model to encourage wastewater reuse. Tokens are provided to industries in proportion to quantity and quality of reused wastewater. Rules for issue and trade of these tokens are written as a smart contract. Unfortunately, providing such incentives also provides a motive for tampering the data on which these tokens are awarded. Anomaly detection algorithms are used to detect the potential frauds which take place in the system upon IoT meter data tampering. The system uses IoT meters that measure volume of wastewater generated and reused, along with quality metrics such as pH, hardness and oil content. Multiple machine learning algorithms are used to detect tampering- polynomial regression, DBSCAN, autoencoders and LSTM networks. Their performance has then been compared. A first implementation of this system and an evaluation of the system's performance are also presented."
49,unknown,http://arxiv.org/abs/2103.13997v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.13997v1,2021-03-25 00:00:00,real-time low-resource phoneme recognition on edge devices,"While speech recognition has seen a surge in interest and research over the
last decade, most machine learning models for speech recognition either require
large training datasets or lots of storage and memory. Combined with the
prominence of English as the number one language in which audio data is
available, this means most other languages currently lack good speech
recognition models.
  The method presented in this paper shows how to create and train models for
speech recognition in any language which are not only highly accurate, but also
require very little storage, memory and training data when compared with
traditional models. This allows training models to recognize any language and
deploying them on edge devices such as mobile phones or car displays for fast
real-time speech recognition."
50,unknown,http://arxiv.org/abs/1808.07647v4,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1808.07647v4,2018-08-23 00:00:00,"machine learning at the edge: a data-driven architecture with
  applications to 5g cellular networks","The fifth generation of cellular networks (5G) will rely on edge cloud
deployments to satisfy the ultra-low latency demand of future applications. In
this paper, we argue that such deployments can also be used to enable advanced
data-driven and Machine Learning (ML) applications in mobile networks. We
propose an edge-controller-based architecture for cellular networks and
evaluate its performance with real data from hundreds of base stations of a
major U.S. operator. In this regard, we will provide insights on how to
dynamically cluster and associate base stations and controllers, according to
the global mobility patterns of the users. Then, we will describe how the
controllers can be used to run ML algorithms to predict the number of users in
each base station, and a use case in which these predictions are exploited by a
higher-layer application to route vehicular traffic according to network Key
Performance Indicators (KPIs). We show that the prediction accuracy improves
when based on machine learning algorithms that rely on the controllers' view
and, consequently, on the spatial correlation introduced by the user mobility,
with respect to when the prediction is based only on the local data of each
single base station."
51,unknown,10.1109/atc52653.2021.9598291,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9598291/,2021-10-16 00:00:00,an edge-ai heterogeneous solution for real-time parking occupancy detection,"In the digital era, building smart cities is a highly desired goal that every country strives to achieve. With the advancement of technology, many smart city systems have been developed at a rapid rate of which Smart Parking is emerging as one of the core components. Smart Parking promises to automate the parking process, thereby saving time, resources and effort for searching an optimal parking space as well as reducing traffic congestion and population. As one of the newly emerging and disrupting technology, Artificial Intelligence, Machine Learning and Deep Learning (AI/ML/DL) are being utilized in many aspects of developing a Smart Parking system. In this paper, we propose an solution for accelerating AI/ML/DL algorithms deployed on low-cost System-on-Chip platforms (SoCs), which are often used as edge devices in Smart Parking system. In particular, we leverage Binary Neural Network (BNN), one of the most advanced deep learning models, to build a heterogeneous algorithm for real-time identifying parking occupancy based on the integration of SoCs and existing surveillance systems. The proposed solution is implemented and evaluated in Zynq UltraScale+ MPSoC with high accuracy (approx. 87%), low latency (avg. 16ms) and high frame per second (FPS) rate."
52,unknown,10.1109/iotais.2018.8600904,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8600904/,2018-11-03 00:00:00,smart air quality monitoring system with lorawan,"Nowadays, cities all over the globe are transforming into smart cities. Smart cities initiatives need to address environmental concerns such as air pollution to provide clean air. A scalable and cost-effective air monitoring system is imperative to monitor and control air pollution for smart city development. Air pollution has notable effects on the well-being of the population a whole, global atmosphere, and worldwide economy. This paper presents a scalable smart air quality monitoring system with low-cost sensors and long-range communication protocol. The sensors collect four parameters, temperature, humidity, dust and carbon dioxide in the air. The proposed end-to-end system has been implemented and deployed in Yangon, the business capital of Myanmar, as a case study since Jun 2018. The system allows the users to log in to an online dashboard to monitor the real-time status. In addition, based the collected air quality parameters for the past two months, a machine learning model has been trained to make predictions of parameters such that proactive actions can be taken to alleviate the impacts from air pollution."
53,unknown,10.1109/tenconspring.2017.8070078,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8070078/,2017-07-16 00:00:00,identifying uncollected garbage in urban areas using crowdsourcing and machine learning,"Waste management in Urban Cities of India is a serious concern with growing amounts of uncollected garbage on the streets. In this paper, we present a mobile application based solution to empower the citizens to report instances of uncollected garbage and draw the attention of authorities. Our application has been successfully deployed and has seen more than a million complaints registered across many Indian cities. One of the challenges we have had to address on deployment of the application was the presence of a number of spurious complaints, such complaints result in unnecessary work by the municipal authorities. We tackled the challenge by using machine learning techniques to identify spurious complaints. We have been able to achieve an accuracy of over 85% in segregating the spurious complaints from actual ones by analyzing the image uploaded along with the complaint. The application can be used in real time once the model is trained."
54,unknown,10.1109/isce.2018.8408911,2018 International Symposium on Consumer Technologies (ISCT),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/f6ff5cb091a3e27e67893744cd2995ad6fef3407,2018-01-01 00:00:00,low-cost real-time non-intrusive appliance identification and controlling through machine learning algorithm,"The existing power generation sources are unable to meet the hyper escalating electricity demand. Common solution is to install new generation plants to fulfill the electricity demand which it is not cost-effective. A cheap and effective solution is to monitor all the appliances running inside a building and to use them efficiently. Non-intrusive load monitoring (NILM) is one of the economical techniques to identify the appliances on the basis of their unique load signatures. In this paper, a machine learning based technique is presented to identify the devices for monitoring purposes. A low-cost hardware setup, called Appliance Identification and Management System (AIMS) is developed to identify and control the appliances remotely. The appliance identification algorithm is developed in Python and deployed on Raspberry Pi, coupled with Arduino. The hardware setup furnishes consumers with the real-time status of all home appliances on their smartphone and web server. Controlling module is also integrated with the identification hardware to provide smart access to consumer for the remote control of home appliances."
55,unknown,10.23919/splitech.2019.8783138,2019 4th International Conference on Smart and Sustainable Technologies (SpliTech),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/41fa513e87ab7ccede01f16245c7d4b74000193f,2019-01-01 00:00:00,a context agnostic air quality service to exploit data in the ioe era,"The upcoming IoE paradigm is taking the IoT era to a new shift, and that because of the natural inter-connection of processes, people, devices and stakeholders. From the smart city perspective, the main goal is to make well-informed decisions, on the base of a variable number of sensors and sources, exposing different data with different protocols and structures. The urban contexts may change, and with them, the number of sensors deployed. The novel smart city service must go beyond an integration strategy, it needs an exploitation model to optimally retrieve useful and highly contextualized information. In this paper we focus on the development of a model which fuses together the IoE potential and machine learning techniques for the cognitive smart city: retrive useful intelligent information, optimally exploiting the infrastructure the specific physical context may offer. We propose an approach and related techniques for realizing context agnostic services, namely services that do not depend on the enabling infrastructure beneath. The purpose is to create an IoE-based self-contextualizing service, which potentially consider the entire range of data that is being collected in smart cities and use such data to provide highly-personalized information about each environment, i.e., information that best suits the context of each Smart City. To prove the proposed context agnostic service, we take into account the air quality observation issue: we provide two high-contextualized informative services to leverage data related to two different physical environments, thus building location awareness for different geographic areas and stakeholders. But still managed by the same application which can adapts itself. Finally we present the evaluation of this prototype to illustrate the benefits of our solution and the future work."
56,unknown,10.3390/s21020405,Sensors,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/644b6322c111a291d46041366ec3436eebc329bc,2021-01-01 00:00:00,"dolars, a distributed on-line activity recognition system by means of heterogeneous sensors in real-life deployments—a case study in the smart lab of the university of almería","Activity Recognition (AR) is an active research topic focused on detecting human actions and behaviours in smart environments. In this work, we present the on-line activity recognition platform DOLARS (Distributed On-line Activity Recognition System) where data from heterogeneous sensors are evaluated in real time, including binary, wearable and location sensors. Different descriptors and metrics from the heterogeneous sensor data are integrated in a common feature vector whose extraction is developed by a sliding window approach under real-time conditions. DOLARS provides a distributed architecture where: (i) stages for processing data in AR are deployed in distributed nodes, (ii) temporal cache modules compute metrics which aggregate sensor data for computing feature vectors in an efficient way; (iii) publish-subscribe models are integrated both to spread data from sensors and orchestrate the nodes (communication and replication) for computing AR and (iv) machine learning algorithms are used to classify and recognize the activities. A successful case study of daily activities recognition developed in the Smart Lab of The University of Almería (UAL) is presented in this paper. Results present an encouraging performance in recognition of sequences of activities and show the need for distributed architectures to achieve real time recognition."
57,unknown,10.1109/rtsi50628.2021.9597339,2021 IEEE 6th International Forum on Research and Technology for Society and Industry (RTSI),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/81cc3b8587d27f55341a7d5c92baf46d22a3a98a,2021-01-01 00:00:00,towards graph machine learning for smart grid knowledge graphs in industrial scenarios,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG."
58,unknown,10.1109/ic3i44769.2018.9007294,2018 3rd International Conference on Contemporary Computing and Informatics (IC3I),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/db2783813c1071e47f55385454f13cc55dc8fb16,2018-01-01 00:00:00,iot based precision horticulture in north india,"Horticulture incorporates a major impact on economy of the country. It is a subdivision of agriculture which deals with plant gardening under controlled environment. Heap of analysis has been dole out in automating the irrigation system by using wireless device and mobile computing. Conjointly analysis has been done in applying machine learning in Horticultural system too. Machine to Machine (M2M) communication is a growing technology that permits devices, objects to speak among one another and send knowledge to Server or Cloud through the Core Network. Therefore, consequently we tend to have developed a Brainy IOT primarily based machine-controlled Irrigation system. Wherever device knowledge is touching wet soil, temperature is captured and consequently machine learning algorithmic is deployed for analysing the device knowledge for prediction towards irrigating the soil with water or switching ON/OFF fan to control temperature. This can be a totally machine-controlled wherever devices communicate among themselves and apply the intelligence in real time monitoring & analysis. Making data available online through Cloud to Scientists to remotely make smart decisions on Precision Horticulture. System has been developed using low value embedded devices like Raspberry Pi3, Arduino and successfully implemented in Delhi."
59,included,http://arxiv.org/abs/1804.09914v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1804.09914v1,2018-04-26 00:00:00,"itelescope: intelligent video telemetry and classification in real-time
  using software defined networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost."
60,unknown,http://arxiv.org/abs/2110.13041v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2110.13041v1,2021-10-25 00:00:00,applications and techniques for fast machine learning in science,"In this community review report, we discuss applications and techniques for
fast machine learning (ML) in science -- the concept of integrating power ML
methods into the real-time experimental data processing loop to accelerate
scientific discovery. The material for the report builds on two workshops held
by the Fast ML for Science community and covers three main areas: applications
for fast ML across a number of scientific domains; techniques for training and
implementing performant and resource-efficient ML algorithms; and computing
architectures, platforms, and technologies for deploying these algorithms. We
also present overlapping challenges across the multiple scientific domains
where common solutions can be found. This community report is intended to give
plenty of examples and inspiration for scientific discovery through integrated
and accelerated ML solutions. This is followed by a high-level overview and
organization of technical advances, including an abundance of pointers to
source material, which can enable these breakthroughs."
61,unknown,http://arxiv.org/abs/2004.05953v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2004.05953v1,2020-04-13 00:00:00,"software-defined network for end-to-end networked science at the
  exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence."
62,unknown,10.1145/3286490.3286560,DIDL@Middleware,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/4d7ba7116cc8b9fbf5638b3d6ce6092d5ca7dcdc,2018-01-01 00:00:00,distributed c++-python embedding for fast predictions and fast prototyping,"Python has evolved to become the most popular language for data science. It sports state-of-the-art libraries for analytics and machine learning, like Sci-Kit Learn. However, Python lacks the computational performance that a industrial system requires for high frequency real time predictions. Building upon a year long research project heavily based on SciKit Learn (sklearn), we faced performance issues in deploying to production. Replacing sklearn with a better performing framework would require re-evaluating and tuning hyperparameters from scratch. Instead we developed a python embedding in a C++ based server application that increased performance by up to 20x, achieving linear scalability up to a point of convergence. Our implementation was done for mainstream cost effective hardware, which means we observed similar performance gains on small as well as large systems, from a laptop to an Amazon EC2 instance to a high-end server."
63,unknown,10.1007/978-981-16-7498-3_8,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-981-16-7498-3_8,2022-01-01 00:00:00,application of ai/iot for smart renewable energy management in smart cities,"A city is considered to be smart when the application of Artificial Intelligence (AI) and the Internet of Things (IoT) is integrated with it. This enables the collection of data from people, devices, and buildings, then analyses are performed to optimize control over infrastructure, traffic, energy, etc. A smart city is a collective framework with the integration of Information and Communication Technologies (ICT) and Cloud that makes interaction easily with one another. In this chapter, smart energy infrastructure is studied to monitor energy utilization in the city and to reduce costs and carbon emissions. Energy usage has recently shifted focus to renewable energy sources with minimal carbon emissions, emphasizing the necessity for ongoing environmental and human health preservation. Renewable energy is becoming more abundant, and the issue is to recognize and understand it in meeting the increasing demand for clean, affordable energy. Customers, distributors, and government bodies are all concerned about cost and the climate. Artificial intelligence proclaimed a new age in technology as well as in sustainable development. So, in this chapter, an implication of AI is presented and analyzed for RE research in smart environments. Along with that, an analytical study is also presented with the application of AI or IoT for smart energy management for smart cities. The main aim is to focus on and explore the efficiency level of ML/IoT techniques. This work will also provide an in-depth analysis of innovative development, deployment, analysis, and management of smart energy in smart cities."
64,unknown,10.1109/tencon.2019.8929612,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8929612/,2019-10-20 00:00:00,lung nodule detection from low dose ct scan using optimization on intel xeon and core processors with intel distribution of openvino toolkit,"With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance."
65,unknown,10.1109/percomworkshops51409.2021.9431061,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e9201bea3bd785e962697f5c3ddd3a30348cbc48,2021-01-01 00:00:00,ultra-fast machine learning classifier execution on iot devices without sram consumption,"With the introduction of edge analytics, IoT devices are becoming smart and ready for AI applications. A few modern ML frameworks are focusing on the generation of small-size ML models (often in kBs) that can directly be flashed and executed on tiny IoT devices, particularly the embedded systems. Edge analytics eliminates expensive device-to-cloud communications, thereby producing intelligent devices that can perform energy-efficient real-time offline analytics. Any increase in the training data results in a linear increase in the size and space complexity of the trained ML models, making them unable to be deployed on IoT devices with limited memory. To alleviate the memory issue, a few studies have focused on optimizing and fine-tuning existing ML algorithms to reduce their complexity and size. However, such optimization is usually dependent on the nature of IoT data being trained. In this paper, we presented an approach that protects model quality without requiring any alteration to the existing ML algorithms. We propose SRAM-optimized implementation and efficient deployment of widely used standard/stable ML-frameworks classifier versions (e.g., from Python scikit-learn). Our initial evaluation results have demonstrated that ours is the most resource-friendly approach, having a very limited memory footprint while executing large and complex ML models on MCU-based IoT devices, and can perform ultra-fast classifications while consuming 0 bytes of SRAM. When we tested our approach by executing it on a variety of MCU-based devices, the majority of models ported and executed produced 1-4x times faster inference results in comparison with the models ported by the sklearn-porter, m2cgen, and emlearn libraries."
66,unknown,10.1109/etfa45728.2021.9613448,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9613448/,2021-09-10 00:00:00,computer vision based privacy protected fall detection and behavior monitoring system for the care of the elderly,"The elderly population constitutes a large percentage of the society hence making elderly care a top priority. Falls have been identified as a leading issue among major problems faced by them. Concerning this, many monitoring devices have been developed, most of them focusing solely on one specific health care aspect or related to fall detection, and are based on sensors and wearable devices which are usually uncomfortable for daily use. Considering these aspects, the solution proposed in this research is a real time computer vision-based system that monitors behavior and detects anomalies through deep learning. The monitoring is mainly focused on detecting unusual behavior including falls, and monitoring routine activities to detect deviations. A device approach is used to deploy the deep learning models and consists of IP camera-based monitoring which uses a special privacy protected procedure that ensures the detection is done based on meta data and therefore no camera image or footage is stored. The research is mainly focused on four major components which are user identification, fall detection, routine variance detection and device configuration."
67,unknown,10.1109/iccca49541.2020.9250902,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9250902/,2020-10-31 00:00:00,smart accident recognition and alerting system for edge devices,"The rate of road accidents has been rising over the years and the high fatalities in these accidents is a matter of great concern. In accidents like these, every second matters. Many times the medical services are unable to make it in time resulting in an unfortunate loss of life, making road accident deaths an integral issue. Also, chances of accidents are equally high during late hours. In an emergency at odd hours, one cannot guarantee if anyone will be present around to inform hospitals, or the police. This paper proposes an End-to-End Deep Learning solution to automate accident recognition and send real-time alerts to emergency services, that is the nearest hospitals and police stations. The proposed system is aimed to be deployed on edge devices attached to roadside CCTV cameras. For this purpose, an optimization step is performed on the deep learning models using the Intel's OpenVINO toolkit which improves performance on edge devices."
68,unknown,http://arxiv.org/abs/2101.07831v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.07831v1,2021-01-19 00:00:00,"multi-task network pruning and embedded optimization for real-time
  deployment in adas","Camera-based Deep Learning algorithms are increasingly needed for perception
in Automated Driving systems. However, constraints from the automotive industry
challenge the deployment of CNNs by imposing embedded systems with limited
computational resources. In this paper, we propose an approach to embed a
multi-task CNN network under such conditions on a commercial prototype
platform, i.e. a low power System on Chip (SoC) processing four surround-view
fisheye cameras at 10 FPS.
  The first focus is on designing an efficient and compact multi-task network
architecture. Secondly, a pruning method is applied to compress the CNN,
helping to reduce the runtime and memory usage by a factor of 2 without
lowering the performances significantly. Finally, several embedded optimization
techniques such as mixed-quantization format usage and efficient data transfers
between different memory areas are proposed to ensure real-time execution and
avoid bandwidth bottlenecks. The approach is evaluated on the hardware
platform, considering embedded detection performances, runtime and memory
bandwidth. Unlike most works from the literature that focus on classification
task, we aim here to study the effect of pruning and quantization on a compact
multi-task network with object detection, semantic segmentation and soiling
detection tasks."
69,unknown,http://arxiv.org/abs/1807.00139v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1807.00139v1,2018-06-30 00:00:00,harnessing constrained resources in service industry via video analytics,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance."
70,unknown,http://arxiv.org/abs/2101.04930v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.04930v2,2021-01-13 00:00:00,"an empirical study on deployment faults of deep learning based mobile
  applications","Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices."
71,unknown,http://arxiv.org/abs/2001.00048v1,arxiv,arxiv,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2001.00048v1,2019-12-31 00:00:00,"mir-vehicle: cost-effective research platform for autonomous vehicle
  applications","This paper illustrates the MIR (Mobile Intelligent Robotics) Vehicle: a
feasible option of transforming an electric ride-on-car into a modular Graphics
Processing Unit (GPU) powered autonomous platform equipped with the capability
that supports test and deployment of various intelligent autonomous vehicles
algorithms. To use a platform for research, two components must be provided:
perception and control. The sensors such as incremental encoders, an Inertial
Measurement Unit (IMU), a camera, and a LIght Detection And Ranging (LIDAR)
must be able to be installed on the platform to add the capability of
environmental perception. A microcontroller-powered control box is designed to
properly respond to the environmental changes by regulating drive and steering
motors. This drive-by-wire capability is controlled by a GPU powered laptop
computer where high-level perception algorithms are processed and complex
actions are generated by various methods including behavior cloning using deep
neural networks. The main goal of this paper is to provide an adequate and
comprehensive approach for fabricating a cost-effective platform that would
contribute to the research quality from the wider community. The proposed
platform is to use a modular and hierarchical software architecture where the
lower and simpler motor controls are taken care of by microcontroller programs,
and the higher and complex algorithms are processed by a GPU powered laptop
computer. The platform uses the Robot Operating System (ROS) as middleware to
maintain the modularity of the perceptions and decision-making modules. It is
expected that the level three and above autonomous vehicle systems and Advanced
Driver Assistance Systems (ADAS) can be tested on and deployed to the platform
with a decent real-time system behavior due to the capabilities and
affordability of the proposed platform."
72,unknown,10.1109/ijcnn.2013.6706957,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6706957/,2013-08-09 00:00:00,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors."
73,unknown,10.1109/bds/hpsc/ids18.2018.00045,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8552303/,2018-05-05 00:00:00,real-time intelligent air quality evaluation on a resource-constrained embedded platform,"Indoor air quality has a major impact on health and comfort of building occupants. Poor air quality may reduce productivity in offices and impair students learning in classes. In order to provide localized air pollution data and tailor it for individual, wearable air quality sensor is a promising solution. Furthermore, crowd sensing has emerged as an Internet-of-Things (IoT) solution, which is economical, scalable and easy to deploy and re-deploy as it uses the power of crowd data collection. The goal of our proposed system is to monitor indoor air quality through a crowd sensing system that will use a set of sensors to measure air quality, monitor the concentration of pollutants continuously, and make recommendations in real time for improved air quality. In this paper artificial network is developed to perform real-time indoor air quality control. Utilizing created neural network embedded into a smart controller comfort level of air quality parameters such as temperature, CO_2 air concentration and humidity could be estimated after every measurement and used for adapting air conditioning systems to adjust air quality. Neural network data preparation and training process are discussed along with deployment of trained network on a smart controller."
74,unknown,10.1109/icct46805.2019.8947193,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8947193/,2019-10-19 00:00:00,edge ai for heterogeneous and massive iot networks,"By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud."
75,unknown,http://arxiv.org/abs/1901.04985v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1901.04985v1,2019-01-12 00:00:00,"nnstreamer: stream processing paradigm for neural networks, toward
  efficient development and execution of on-device ai applications","We propose nnstreamer, a software system that handles neural networks as
filters of stream pipelines, applying the stream processing paradigm to neural
network applications. A new trend with the wide-spread of deep neural network
applications is on-device AI; i.e., processing neural networks directly on
mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy
issues, data transmission costs, and operational costs signifies the need for
on-device AI especially when a huge number of devices with real-time data
processing are deployed. Nnstreamer efficiently handles neural networks with
complex data stream pipelines on devices, improving the overall performance
significantly with minimal efforts. Besides, nnstreamer simplifies the neural
network pipeline implementations and allows reusing off-shelf multimedia stream
filters directly; thus it reduces the developmental costs significantly.
Nnstreamer is already being deployed with a product releasing soon and is open
source software applicable to a wide range of hardware architectures and
software platforms."
76,included,10.1145/3326285.3329051,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9068647/,2019-06-25 00:00:00,leap: learning-based smart edge with caching and prefetching for adaptive video streaming,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4&#x0025; and reduces video rebuffering by 42.7&#x0025;, which leads to at least 15.9&#x0025; improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP."
77,unknown,10.1016/j.future.2020.06.017,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85087196994,2020-11-01,ppcensor: architecture for real-time pornography detection in video streaming,"Convolutional neural network (CNN) models are typically composed of several gigabytes of data, requiring dedicated hardware and significant processing capabilities for proper handling. In addition, video-detection tasks are typically performed offline, and each video frame is analyzed individually, meaning that the video’s categorization (class assignment) as normal or pornographic is only complete after all the video frames have been evaluated. This paper proposes the Private Parts Censor (PPCensor), a CNN-based architecture for transparent and near real-time detection and obfuscation of pornographic video frame regions. Our contribution is two-fold. First, the proposed architecture is the first that addresses the detection of pornographic content as an object detection problem. The objective is to apply user-friendly content filtering such that an inevitable false positive will obfuscate only regions (objects) within the video frames instead of blocking the entire video. Second, the PPCensor architecture is deployed on dedicated hardware, and real-time detection is deployed using a video-oriented streaming proxy. If a pornographic video frame is identified in the video, the system can hide pornographic content (private parts) in real time without user interaction or additional processing on the user’s device. Based on more than 50,000 objects labeled manually, the evaluation results show that the PPCensor is capable of detecting private parts in near real time for video streaming. Compared to cutting-edge CNN architectures for image classification, PPCensor achieved similar results, but operated in real time. In addition, when deployed on a desktop computer, PPCensor handled up to 35 simultaneous connections without the need for additional processing on the end-user device."
78,unknown,10.1109/cds49703.2020.00012,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9275963/,2020-08-02 00:00:00,welding seam recognition robots based on edge computing,"In order to meet the requirements of the accuracy and real-time performance during the working process of underwater welding robots, a scheme of welding seam recognition robots system based on the edge computing is proposed in this paper. A number of pre-processing methods for capturing welding seam image were designed, including Thresholding, Filtering and Edge Detect. A Convolutional Neural Network(CNN) model for welding seam recognition was also created. In the experiments, the image pre-processing and CNN algorithms were integrated in and deployed to the robots, and the learning and training algorithms of the CNN were deployed to the cloud servers. The image pre-processing methods filtered the interference in underwater operations and achieved the image compression and feature extraction. The cloud servers fulfilled the training and parameter optimization of the CNN, which improved the accuracy of welding seam image recognition."
79,unknown,10.1016/j.simpat.2019.102015,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85074521783,2020-05-01,a fog computing model for implementing motion guide to visually impaired,"A guide dog robot system for visually impaired often needs to process many kinds of information, such as image, voice and other sensor information. Information processing methods based on deep neural network can achieve better results. However, it requires expensive computing and communication resources to meet the real-time requirement. Fog computing has emerged as a promising solution for applications that are data-intensive and delay-sensitive. We propose a fog computing framework named PEN (Phone + Embedded board + Neural compute stick) for the guide dog robot system. The robot’s functions in PEN are wrapped as services and deployed on the appropriate devices. Services are combined as an application in a visual programming language environment. Neural compute stick accelerates image processing speed at low power consumption. A simulation environment and a prototype are built on the framework. The simulated guide dog system is developed for operating in a miniature environment, including a small robot dog, a small wheelchair, model cars, traffic lights, and traffic blockage. The prototype is a full-sized portable guide system that can be used by a visually impaired person in a real environment. Simulation and experiments show that the framework can meet the functional and performance requirements for implementing the guide systems for visually impaired."
80,unknown,10.1109/access.2019.2919736,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8728285/,2019-01-01 00:00:00,federated learning-based computation offloading optimization in edge computing-supported internet of things,"Recently, smart cities, smart homes, and smart medical systems have challenged the functionality and connectivity of the large-scale Internet of Things (IoT) devices. Thus, with the idea of offloading intensive computing tasks from them to edge nodes (ENs), edge computing emerged to supplement these limited devices. Benefit from this advantage, IoT devices can save more energy and still maintain the quality of the services they should provide. However, computational offload decisions involve federation and complex resource management and should be determined in the real-time face to dynamic workloads and radio environments. Therefore, in this work, we use multiple deep reinforcement learning (DRL) agents deployed on multiple edge nodes to indicate the decisions of the IoT devices. On the other hand, with the aim of making DRL-based decisions feasible and further reducing the transmission costs between the IoT devices and edge nodes, federated learning (FL) is used to train DRL agents in a distributed fashion. The experimental results demonstrate the effectiveness of the decision scheme and federated learning in the dynamic IoT system."
81,unknown,10.1109/infocom41043.2020.9155467,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9155467/,2020-07-09 00:00:00,rldish: edge-assisted qoe optimization of http live streaming with reinforcement learning,"Recent years have seen a rapidly increasing traffic demand for HTTP-based high-quality live video streaming. The surging traffic demand, as well as the real-time property of live videos, make it challenging for content delivery networks (CDNs) to guarantee the Quality-of-Experiences (QoE) of viewers. The initial video segment (IVS) of live streaming plays an important role in the QoE of live viewers, particularly when users require fast join time and smooth view experience. State-of-the-art research on this regard estimates network throughput for each viewer and thus may incur a large overhead that offsets the benefit. To tackle the problem, we propose Rldish, a scheme deployed at the edge CDN server, to dynamically select a suitable IVS for new live viewers based on Reinforcement Learning (RL). Rldish is transparent to both the client and the streaming server. It collects the real-time QoE observations from the edge without any client-side assistance, then uses these QoE observations as real-time rewards in RL. We deploy Rldish as a virtualized network function (VNF) in a real HTTP cache server, and evaluate its performance using streaming servers distributed over the world. Our experiments show that Rldish improves the state- of-the-art IVS selection scheme w.r.t. the average QoE of live viewers by up to 22%."
82,unknown,http://arxiv.org/abs/2104.09876v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2104.09876v1,2021-04-20 00:00:00,"iiot-enabled health monitoring for integrated heat pump system using
  mixture slow feature analysis","The sustaining evolution of sensing and advancement in communications
technologies have revolutionized prognostics and health management for various
electrical equipment towards data-driven ways. This revolution delivers a
promising solution for the health monitoring problem of heat pump (HP) system,
a vital device widely deployed in modern buildings for heating use, to timely
evaluate its operation status to avoid unexpected downtime. Many HPs were
practically manufactured and installed many years ago, resulting in fewer
sensors available due to technology limitations and cost control at that time.
It raises a dilemma to safeguard HPs at an affordable cost. We propose a hybrid
scheme by integrating industrial Internet-of-Things (IIoT) and intelligent
health monitoring algorithms to handle this challenge. To start with, an IIoT
network is constructed to sense and store measurements. Specifically,
temperature sensors are properly chosen and deployed at the inlet and outlet of
the water tank to measure water temperature. Second, with temperature
information, we propose an unsupervised learning algorithm named mixture slow
feature analysis (MSFA) to timely evaluate the health status of the integrated
HP. Characterized by frequent operation switches of different HPs due to the
variable demand for hot water, various heating patterns with different heating
speeds are observed. Slowness, a kind of dynamics to measure the varying speed
of steady distribution, is properly considered in MSFA for both heating pattern
division and health evaluation. Finally, the efficacy of the proposed method is
verified through a real integrated HP with five connected HPs installed ten
years ago. The experimental results show that MSFA is capable of accurately
identifying health status of the system, especially failure at a preliminary
stage compared to its competing algorithms."
83,unknown,10.1109/icsima47653.2019.9057343,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9057343/,2019-08-29 00:00:00,real-time wireless monitoring for three phase motors in industry: a cost-effective solution using iot,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan."
84,unknown,10.1109/ro-man50785.2021.9515431,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9515431/,2021-08-12 00:00:00,simplifying the a.i. planning modeling for human-robot collaboration,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism."
85,included,10.1016/j.adhoc.2019.102047,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076174369,2020-03-01,an intelligent edge-iot platform for monitoring livestock and crops in a dairy farming scenario,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud."
86,unknown,http://arxiv.org/abs/1604.04384v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1604.04384v2,2016-04-15 00:00:00,the strands project: long-term autonomy in everyday environments,"Thanks to the efforts of the robotics and autonomous systems community,
robots are becoming ever more capable. There is also an increasing demand from
end-users for autonomous service robots that can operate in real environments
for extended periods. In the STRANDS project we are tackling this demand
head-on by integrating state-of-the-art artificial intelligence and robotics
research into mobile service robots, and deploying these systems for long-term
installations in security and care environments. Over four deployments, our
robots have been operational for a combined duration of 104 days autonomously
performing end-user defined tasks, covering 116km in the process. In this
article we describe the approach we have used to enable long-term autonomous
operation in everyday environments, and how our robots are able to use their
long run times to improve their own performance."
87,unknown,10.1109/isc2.2016.7580869,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7580869/,2016-09-15 00:00:00,urbansense: an urban-scale sensing platform for the internet of things,"A critical step towards smarter and safer cities is to endow them with the abilities to massively gather a wide variety of data sets and to automatically feed those data to decision support tools and applications that leverage artificial intelligence. We present UrbanSense, a platform deployed on the streets of a mid-size European city (Porto, Portugal) to collect key environmental data. The main innovations of UrbanSense are (1) design for affordability and extensibility, (2) its ability to leverage heterogeneous networks to send the data to the cloud (using both real-time and delay-tolerant communications), and (3) its Internet of Things integration to expose the data streams to smart city tools and applications. Beyond discussing the design choices, we present operational results for 6 months of operation and give a detailed account of the challenges faced by the successful deployment of urban sensing technologies in the wild."
88,unknown,10.11591/ijra.v9i4.pp244-250,ICRA 2020,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/5566df06316fea4167600aff8e393c64e3b1c0b5,2020-01-01 00:00:00,development of a real-time framework for farm monitoring using drone technology,"This work developed a cost-effective framework for agriculturists to regularly monitor their crops against intruding rodents and other security concerns using modern drone technology through configuration and deployment of an autonomous UAV which also functions as a remotely piloted vehicle. This was done by configuring a quadcopter capable of causing a disturbance when a rodent is observed through an inbuilt alarm system whose sound is amplified to be loud enough to cause the animals to leave the farm area. A framework for real-time image and live video transmission from the farm to a designated remote base station was developed. This was achieved through programming codes that configured the drone to operate an intelligent alarm and object tracking systems which enables a live feed from the UAV using Arduino IDE and Mission Planner for autonomous flight control. The requisite algorithms were developed using the framework of tracking, learning and detection (TLD) in the OpenCV software. The drone movement is equally controlled remotely over a Wi-Fi network using an ESP8266 Wi-Fi module for redirection and controlling of the drone movement to monitor specific locations."
89,unknown,10.1109/icesc51422.2021.9532913,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9532913/,2021-08-06 00:00:00,natural language processing based human assistive health conversational agent for multi-users,"Background: Most of the people are not medically qualified for studying or understanding the extremity of their diseases or symptoms. This is the place where natural language processing plays a vital role in healthcare. These chatbots collect patients' health data and depending on the data, these chatbot give more relevant data to patients regarding their body conditions and recommending further steps also. Purposes: In the medical field, AI powered healthcare chatbots are beneficial for assisting patients and guiding them in getting the most relevant assistance. Chatbots are more useful for online search that users or patients go through when patients want to know for their health symptoms. Methods: In this study, the health assistant system was developed using Dialogflow application programming interface (API) which is a Google's Natural language processing powered algorithm and the same is deployed on google assistant, telegram, slack, Facebook messenger, and website and mobile app. With this web application, a user can make health requests/queries via text message and might also get relevant health suggestions/recommendations through it. Results: This chatbot acts like an informative and conversational chatbot. This chatbot provides medical knowledge such as disease symptoms and treatments. Storing patients personal and medical information in a database for further analysis of the patients and patients get real time suggestions from doctors. Conclusion: In the healthcare sector AI-powered applications have seen a remarkable spike in recent days. This covid crisis changed the whole healthcare system upside down. So this NLP powered chatbot system reduced office waiting, saving money, time and energy. Patients might be getting medical knowledge and assisting ourselves within their own time and place."
90,unknown,10.1109/srii.2012.76,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6311047/,2012-07-27 00:00:00,an innovative decision support service for improving pharmaceutical acquisition capabilities,"The cost of pharmaceutical products is one of the largest contributors of operating costs in providing healthcare services in Thailand. As drug prices change according to shifting market forces, the distribution of purchase prices for each drug varies according to the type of medication and the purchasing power of the healthcare provider. These changes can have significant impact on how well healthcare providers can effectively provide services. PAC-DSS (Pharmaceutical Acquisition Capability Decision Support Service) is an innovative decision support service that enables hospitals to pool together and share information to better understand current market prices and run analytics on drug prices in order to improve their operating costs. Our service allows users to interact with real pricing data to dissect various factors that can contribute to acquisition capabilities of individual drugs such as specific brand names or groups of drugs such as all brands of a given generic drug, without sacrificing individual providers' privacy as we do not disclose individual purchase prices. In developing PAC-DSS, we have had to address a range of technical challenges such as data privacy and alignment of disparate drug ontologies. In this paper, we describe PAC-DSS's service architecture, analytic services, and the benefit of PAC-DSS on improving healthcare services by lowering operating cost without sacrificing service quality. We also discuss the initial benefits from deployment of the service currently hosted by the Ministry of Public Health."
91,unknown,10.1109/icccnt51525.2021.9579722,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9579722/,2021-07-08 00:00:00,architecture design of ai and iot based system to identify covid-19 protocol violators in public places,"The World Health Organization (WHO) describes COVID-19 as a pandemic that is causing a worldwide health disaster. Wearing a face mask in public places is the most effective method to curb the spread of the virus. The Internet of Things is emerging as one of the most significant innovations and playing a vital role during the pandemic. Affordable remote health monitoring devices help doctors to track quarantined patients. Our government is trying its best to control the spread of the virus. Citizens who do not follow the protocols serve as the reason for these widespread infections. Our work proposes a system to identify protocol violators in real-time. Our system consists of a face mask detection module, a social distance monitoring module, and a non-contact temperature monitoring module. We intend to deploy our proposed approach in public places such as airports, schools, and hospitals. These modules depend on video feeds from general security cameras and IoT sensor nodes deployed around public areas. Health care and security officials can subscribe to real-time data feeds to track public behaviour."
92,included,10.1109/qrs51102.2020.00018,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9282796/,2020-12-14 00:00:00,phm technology for memory anomalies in cloud computing for iaas,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier."
93,unknown,10.23919/splitech49282.2020.9243735,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9243735/,2020-09-26 00:00:00,padl: a language for the operationalization of distributed analytical pipelines over edge/fog computing environments,"In this paper we introduce PADL, a language for modeling and deploying data-based analytical pipelines. The novelty of this language relies on its independence from both the infrastructure and the technologies used on it. Specifically, this descriptive language aims at embracing all the particularities and constraints of high-demanding deployment models, such as critical restrictions regarding latency, privacy and performance, by providing fully-compliant schemas for implementing data analytical workloads. The adoption of PADL provides means for the operationalization of these pipelines in a reproducible and resilient fashion. In addition, PADL is able to fully utilize the benefits of Edge and Fog computing layers. The feasibility of the language has been validated with an analytical pipeline deployed over an Edge computing environment to solve an Industry 4.0 use case. The promising results obtained therefrom pave the way towards the widespread adoption of our proposed language when deploying data analytical pipelines over real application scenarios."
94,unknown,10.1109/is.2018.8710554,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8710554/,2018-09-27 00:00:00,exploiting the digital twin in the assessment and optimization of sustainability performances,"Digitalization has shown the potential to disrupt industrial value chains by supporting real-time, risk-free and inexpensive inputs to decision making towards enhanced companies' productivity and value networks flexibility. Developing a reliable and robust digital replica of the physical systems of the value chain is one of the most advanced (and challenging) approaches to digitalization, condensed in the concept of Digital Twin (DT). DT plays a fundamental role in creating a data-rich environment where simulation and optimization procedures can be run. With DT expected to become a commodity in the coming years, simulation and optimization become therefore a more accessible instrument for the improvement of manufacturing and business processes also in small enterprises with limited investment capacity. While scientific literature has analysed the adoption of DT in the optimization of products lifecycle, no contributions have yet focused on the exploitation of DT to improve the sustainability performances of whole value chains. In this paper we propose a reference framework where DTs built upon process and system data gathered from the field, allow to quickly assess the sustainability performances of both existing and planned production mixes and to compare achievable impacts with changing processes and technologies, thus enabling advisory features for sustainability-aware decision making in structured, multi-entity value networks. Internal validation will be deployed referring to real case studies."
95,unknown,10.1109/access.2021.3082641,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9438728/,2021-01-01 00:00:00,real-time facial expression recognition based on edge computing,"In recent years, many large-scale information systems in the Internet of Things (IoT) can be converted into interdependent sensor networks, such as smart cities, smart medical systems, and industrial Internet systems. The successful application of edge computing in the IoT will make our algorithms faster, more convenient, lower overall costs, providing better business practices, and enhance sustainability. Facial action unit (AU) detection recognizes facial expressions by analyzing cues about the movement of certain atomic muscles in the local facial area. According to the detected facial feature points, we could calculate the values of AU, and then use classification algorithms for emotion recognition. In edge devices, using optimized and custom algorithms to directly process the raw image data from each camera, the detected emotions can be more easily transmitted to the end-user. Due to the tremendous network overhead of transferring the facial action unit feature data, it poses challenges of a real-time facial expression recognition system being deployed in a distributed manner while running in production. Therefore, we designed a lightweight edge computing-based distributed system using Raspberry Pi tailed for this need, and we optimized the data transfer and components deployment. In the vicinity, the front-end and back-end processing modes are separated to reduce round-trip delay, thereby completing complex computing tasks and providing high-reliability, large-scale connection services. For IoT or smart city applications and services, they can be made into smart sensing systems that can be deployed anywhere with network connections."
96,unknown,10.1109/icdcsw53096.2021.00009,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9545916/,2021-07-10 00:00:00,towards understanding the adaptation space of ai-assisted data protection for video analytics at the edge,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy."
97,unknown,10.1109/icarsc.2015.19,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7101621/,2015-04-10 00:00:00,testing a fully autonomous robotic salesman in real scenarios,"Over the past decades, the number of robots deployed in museums, trade shows and exhibitions have grown steadily. This new application domain has become a key research topic in the robotics community. Therefore, new robots are designed to interact with people in these domains, using natural and intuitive channels. Visual perception and speech processing have to be considered for these robots, as they should be able to detect people in their environment, recognize their degree of accessibility and engage them in social conversations. They also need to safely navigate around dynamic, uncontrolled environments. They must be equipped with planning and learning components, that allow them to adapt to different scenarios. Finally, they must attract the attention of the people, be kind and safe to interact with. In this paper, we describe our experience with Gualzru, a salesman robot endowed with the cognitive architecture RoboCog. This architecture synchronizes all previous processes in a social robot, using a common inner representation as the core of the system. The robot has been tested in crowded, public daily life environments, where it interacted with people that had never seen it before nor had a clue about its functionality. Experimental results presented in this paper demonstrate the capabilities of the robot and its limitations in these real scenarios, and define future improvement actions."
98,included,10.1016/j.cie.2019.06.040,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85067600850,2019-09-01,"bernard, an energy intelligent system for raising residential users awareness","Energy efficiency is still a hot topic today. Coming roughly the 25% of the energy consumption in EU from the residential sector, very few cheap and simple tools to promote energy efficiency in home users have been developed. The purpose of this paper is to present Bernard, a concept proof designed for filling this gap. This aims that householders become aware of their energy habits and have useful information that help them to redirect their consumption pattern. To achieve these goals, Bernard offers, through a mobile application, the home energy consumption monitoring in real time, the energy price forecast for the next hour and the appliances which are switched on, among others. Furthermore, it is important to highlight that the system has been designed with the premises of being cheap, non-intrusive, reliable and easily scalable, in order that utilities can gradually deploy and provide it to their customers, gaining at the same time valuable information for decision making and improving its corporate social image. Therefore, the adopted solution is based on a real time streaming data architecture suitable for handling huge volumes of data and applying predictive techniques on a cloud-computing environment. The paper provides a detailed description of the system and experimental results evaluating the performance of the predictive modules built. As case study, REFIT and REDD datasets were used."
99,unknown,10.1109/icra.2019.8793690,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8793690/,2019-05-24 00:00:00,a fog robotics approach to deep robot learning: application to object recognition and grasp planning in surface decluttering,"The growing demand of industrial, automotive and service robots presents a challenge to the centralized Cloud Robotics model in terms of privacy, security, latency, bandwidth, and reliability. In this paper, we present a `Fog Robotics' approach to deep robot learning that distributes compute, storage and networking resources between the Cloud and the Edge in a federated manner. Deep models are trained on non-private (public) synthetic images in the Cloud; the models are adapted to the private real images of the environment at the Edge within a trusted network and subsequently, deployed as a service for low-latency and secure inference/prediction for other robots in the network. We apply this approach to surface decluttering, where a mobile robot picks and sorts objects from a cluttered floor by learning a deep object recognition and a grasp planning model. Experiments suggest that Fog Robotics can improve performance by sim-to-real domain adaptation in comparison to exclusively using Cloud or Edge resources, while reducing the inference cycle time by 4× to successfully declutter 86% of objects over 213 attempts."
100,included,10.1109/aiiot52608.2021.9454183,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9454183/,2021-05-13 00:00:00,image classification with knowledge-based systems on the edge for real-time danger avoidance in robots,"Mobile robots are increasingly common in society and are increasingly being used for complex and high-stakes tasks such as search and rescue. The growing requirements for these robots demonstrate a need for systems which can review and react in real time to environmental hazards, which will allow robots to handle environments that are both dynamic and dangerous. We propose and test a system which allows mobile robots to reclassify environmental objects during operation in conjunction with an edge system. We train an image classification model with 99 percent accuracy and deploy it in conjunction with an edge server and JSON-based ruleset to allow robots to react to and avoid hazards."
101,included,10.1109/isaect50560.2020.9523700,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9523700/,2020-11-27 00:00:00,edge-cloud architectures using uavs dedicated to industrial iot monitoring and control applications,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud."
102,unknown,10.1007/s00521-021-05726-z,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00521-021-05726-z,2021-01-25 00:00:00,towards design and implementation of industry 4.0 for food manufacturing,"Today’s factories are considered as smart ecosystems with humans, machines and devices interacting with each other for efficient manufacturing of products. Industry 4.0 is a suite of enabler technologies for such smart ecosystems that allow transformation of industrial processes. When implemented, Industry 4.0 technologies have a huge impact on efficiency, productivity and profitability of businesses. The adoption and implementation of Industry 4.0, however, require to overcome a number of practical challenges, in most cases, due to the lack of modernisation and automation in place with traditional manufacturers. This paper presents a first of its kind case study for moving a traditional food manufacturer, still using the machinery more than one hundred years old, a common occurrence for small- and medium-sized businesses, to adopt the Industry 4.0 technologies. The paper reports the challenges we have encountered during the transformation process and in the development stage. The paper also presents a smart production control system that we have developed by utilising AI, machine learning, Internet of things, big data analytics, cyber-physical systems and cloud computing technologies. The system provides novel data collection, information extraction and intelligent monitoring services, enabling improved efficiency and consistency as well as reduced operational cost. The platform has been developed in real-world settings offered by an Innovate UK-funded project and has been integrated into the company’s existing production facilities. In this way, the company has not been required to replace old machinery outright, but rather adapted the existing machinery to an entirely new way of operating. The proposed approach and the lessons outlined can benefit similar food manufacturing industries and other SME industries."
103,unknown,http://arxiv.org/abs/2003.02454v4,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.02454v4,2020-03-05 00:00:00,agl: a scalable system for industrial-purpose graph machine learning,"Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
  In this paper, we design AGL, a scalable, fault-tolerance and integrated
system, with fully-functional training and inference for GNNs. Our system
design follows the message passing scheme underlying the computations of GNNs.
We design to generate the $k$-hop neighborhood, an information-complete
subgraph for each node, as well as do the inference simply by merging values
from in-edge neighbors and propagating values to out-edge neighbors via
MapReduce. In addition, the $k$-hop neighborhood contains information-complete
subgraphs for each node, thus we simply do the training on parameter servers
due to data independency. Our system AGL, implemented on mature
infrastructures, can finish the training of a 2-layer graph attention network
on a graph with billions of nodes and hundred billions of edges in 14 hours,
and complete the inference in 1.2 hour."
104,unknown,10.1109/access.2019.2926206,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8752358/,2019-01-01 00:00:00,a fusion-based framework for wireless multimedia sensor networks in surveillance applications,"Multimedia sensors enable monitoring applications to obtain more accurate and detailed information. However, the development of efficient and lightweight solutions for managing data traffic over wireless multimedia sensor networks (WMSNs) has become vital because of the excessive volume of data produced by multimedia sensors. As part of this motivation, this paper proposes a fusion-based WMSN framework that reduces the amount of data to be transmitted over the network by intra-node processing. This framework explores three main issues: (1) the design of a wireless multimedia sensor (WMS) node to detect objects using machine learning techniques; (2) a method for increasing the accuracy while reducing the amount of information transmitted by the WMS nodes to the base station, and; (3) a new cluster-based routing algorithm for the WMSNs that consumes less power than the currently used algorithms. In this context, a WMS node is designed and implemented using commercially available components. In order to reduce the amount of information to be transmitted to the base station and thereby extend the lifetime of a WMSN, a method for detecting and classifying objects on three different layers has been developed. A new energy-efficient cluster-based routing algorithm is developed to transfer the collected information/data to the sink. The proposed framework and the cluster-based routing algorithm are applied to our WMS nodes and tested experimentally. The results of the experiments clearly demonstrate the feasibility of the proposed WMSN architecture in the real-world surveillance applications."
105,unknown,10.1109/icmla.2015.152,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7424435/,2015-12-11 00:00:00,mlaas: machine learning as a service,"The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time."
106,unknown,10.1016/j.asoc.2021.107465,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105315919,2021-09-01,click-event sound detection in automotive industry using machine/deep learning,"In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25,000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from 
                        
                           −
                           16
                           .
                           67
                        
                      dB to 
                        
                           −
                           12
                           .
                           87
                        
                      dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 
                        
                           94
                           .
                           55
                           ±
                           0
                           .
                           83
                        
                      %. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations."
107,unknown,http://arxiv.org/abs/2002.11045v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2002.11045v1,2020-02-22 00:00:00,"deep learning for ultra-reliable and low-latency communications in 6g
  networks","In the future 6th generation networks, ultra-reliable and low-latency
communications (URLLC) will lay the foundation for emerging mission-critical
applications that have stringent requirements on end-to-end delay and
reliability. Existing works on URLLC are mainly based on theoretical models and
assumptions. The model-based solutions provide useful insights, but cannot be
directly implemented in practice. In this article, we first summarize how to
apply data-driven supervised deep learning and deep reinforcement learning in
URLLC, and discuss some open problems of these methods. To address these open
problems, we develop a multi-level architecture that enables device
intelligence, edge intelligence, and cloud intelligence for URLLC. The basic
idea is to merge theoretical models and real-world data in analyzing the
latency and reliability and training deep neural networks (DNNs). Deep transfer
learning is adopted in the architecture to fine-tune the pre-trained DNNs in
non-stationary networks. Further considering that the computing capacity at
each user and each mobile edge computing server is limited, federated learning
is applied to improve the learning efficiency. Finally, we provide some
experimental and simulation results and discuss some future directions."
108,unknown,http://arxiv.org/abs/1905.07082v6,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1905.07082v6,2019-05-17 00:00:00,"the audio auditor: user-level membership inference in internet of things
  voice services","With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy."
109,unknown,10.1109/icsys47076.2019.8982469,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8982469/,2019-09-19 00:00:00,fpga-enabled binarized convolutional neural networks toward real-time embedded object recognition system for service robots,"In this presentation, we report the results of applying a binarized Convolutional Neural Network (CNN) and a Field Programmable Gate Array (FPGA) for image-based object recognition. While the demand rises for robots with robust object recognition implemented with Neural Networks, a tradeoff between data processing rate and power consumption persists. Some applications utilise Graphics Processing Units (GPU), which results in high power consumption, thus undesirable for embedded systems, while the others communicate with cloud computers to minimise computational resources at the clients' side, i.e. robots, raising another concern that the robots are unable to perform object recognition without the servers and network connections. To overcome these difficulties, we propose an embedded object recognition system implemented with a binarized CNN and an FPGA. FPGAs consist of a matrix of reconfigurable logic gates allowing parallel computing which befit most image processing algorithms such as the CNN. We train the binarized CNN on one of our datasets that contain images of several kinds of food and beverages. The results of the experiments show that the binarized CNN with an FPGA maintains high accuracy as well as real-time computation, suggesting that the proposed system is suitable for robots to perform their tasks in a real-world environment without needing to communicate with a server."
110,included,10.1109/icdmw.2019.00123,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8955523/,2019-11-11 00:00:00,implementation of mobile-based real-time heart rate variability detection for personalized healthcare,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework."
111,unknown,http://arxiv.org/abs/2110.15127v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2110.15127v1,2021-10-28 00:00:00,"lightweight mobile automated assistant-to-physician for global
  lower-resource areas","Importance: Lower-resource areas in Africa and Asia face a unique set of
healthcare challenges: the dual high burden of communicable and
non-communicable diseases; a paucity of highly trained primary healthcare
providers in both rural and densely populated urban areas; and a lack of
reliable, inexpensive internet connections. Objective: To address these
challenges, we designed an artificial intelligence assistant to help primary
healthcare providers in lower-resource areas document demographic and medical
sign/symptom data and to record and share diagnostic data in real-time with a
centralized database. Design: We trained our system using multiple data sets,
including US-based electronic medical records (EMRs) and open-source medical
literature and developed an adaptive, general medical assistant system based on
machine learning algorithms. Main outcomes and Measure: The application
collects basic information from patients and provides primary care providers
with diagnoses and prescriptions suggestions. The application is unique from
existing systems in that it covers a wide range of common diseases, signs, and
medication typical in lower-resource countries; the application works with or
without an active internet connection. Results: We have built and implemented
an adaptive learning system that assists trained primary care professionals by
means of an Android smartphone application, which interacts with a central
database and collects real-time data. The application has been tested by dozens
of primary care providers. Conclusions and Relevance: Our application would
provide primary healthcare providers in lower-resource areas with a tool that
enables faster and more accurate documentation of medical encounters. This
application could be leveraged to automatically populate local or national EMR
systems."
112,unknown,http://arxiv.org/abs/2010.02715v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2010.02715v1,2020-10-03 00:00:00,"assessing automated machine learning service to detect covid-19 from
  x-ray and ct images: a real-time smartphone application case study","The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a
non interventional and sustainable AI solution. Lung disease remains a major
healthcare challenge with high morbidity and mortality worldwide. The
predominant lung disease was lung cancer. Until recently, the world has
witnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We
have experienced how viral infection of lung and heart claimed thousands of
lives worldwide. With the unprecedented advancement of Artificial Intelligence
in recent years, Machine learning can be used to easily detect and classify
medical imagery. It is much faster and most of the time more accurate than
human radiologists. Once implemented, it is more cost-effective and
time-saving. In our study, we evaluated the efficacy of Microsoft Cognitive
Service to detect and classify COVID19 induced pneumonia from other
Viral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the
implication and accuracy of the Automated ML-based Rapid Application
Development (RAD) environment in the field of Medical Image diagnosis. This
study will better equip us to respond with an ML-based diagnostic Decision
Support System(DSS) for a Pandemic situation like COVID19. After optimization,
the trained network achieved 96.8% Average Precision which was implemented as a
Web Application for consumption. However, the same trained network did not
perform the same like Web Application when ported to Smartphone for Real-time
inference. Which was our main interest of study. The authors believe, there is
scope for further study on this issue. One of the main goal of this study was
to develop and evaluate the performance of AI-powered Smartphone-based
Real-time Application. Facilitating primary diagnostic services in less
equipped and understaffed rural healthcare centers of the world with unreliable
internet service."
113,unknown,10.1109/cns48642.2020.9162311,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9162311/,2020-07-01 00:00:00,heka: a novel intrusion detection system for attacks to personal medical devices,"Modern Smart Health Systems (SHS) involve the concept of connected personal medical devices. These devices significantly improve the patient's lifestyle as they permit remote monitoring and transmission of health data (i.e., telemedicine), lowering the treatment costs for both the patient and the healthcare providers. Although specific SHS communication standards (i.e., ISO/IEEE 11073) enable real-time plug-and-play interoperability and communication between different personal medical devices, they do not specify any features for secure communications. In this paper, we demonstrate how personal medical device communication is indeed vulnerable to different cyber attacks. Specifically, we show how an external attacker can hook into the personal medical device's communication and eavesdrop the sensitive health data traffic, and implement manin-the-middle, replay, false data injection, and denial-of-service attacks. Furthermore, we also propose an Intrusion Detection System (IDS), HEKA, to monitor personal medical device traffic and detect attacks on them. HEKA passively hooks into the personal medical traffic generated by medical devices to learn the contiguous sequence of packets information from the captured traffic and detects irregular traffic-flow patterns using an n-grambased approach and different machine learning techniques. We implemented HEKA in a testbed consisting of eight off-the-shelf personal medical devices and evaluated its performance against four different attacks. Our extensive evaluation shows that HEKA can effectively detect different attacks on personal medical devices with an accuracy of 98.4% and Fl-score of 98%."
114,unknown,10.1109/iceee2.2017.7935834,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7935834/,2017-04-10 00:00:00,using lstm networks to predict engine condition on large scale data processing framework,"As the Internet of Things technology is developing rapidly, companies have an ability to observe the health of engine components and constructed systems through collecting signals from sensors. According to output of IoT sensors, companies can build systems to predict the conditions of components. Practically the components are required to be maintained or replaced before the end of life in performing their assigned task. Predicting the life condition of a component is so crucial for industries that have intent to grow in a fast paced technological environment. Recent studies on predictive maintenance help industries to create an alert before the components are corrupted. Thanks to prediction of component failures, companies have a chance to sustain their operations efficiently while reducing their maintenance cost by repairing components in advance. Since maintenance affects production capacity and the service quality directly, optimized maintenance is the key factor for organizations to have more revenue and stay competitive in developing industrialized world. With the aid of well-designed prediction system for understanding current situation of an engine, components could be taken out of active service before malfunction occurs. With the help of inspection, effective maintenance extends component life, improves equipment availability and keeps components in a proper condition while reducing costs. Real time data collected from sensors is a great source to model component deteriorations. Markov Chain models, Survival Analysis, Optimization algorithms and several machine learning approaches have been implemented in order to model predictive maintenance. In this paper Long Short Term Memory (LSTM) networks has been performed to predict the current situation of an engine. LSTM model deals with a sequential input data. Training process of LSTM networks has been performed on large-scale data processing engine with high performance. Since huge amount of data is flowing into the predictive model, Apache Spark which is offering a distributed clustering environment has been used. The output of the LSTM network is deciding the current life condition of components and offering the alerts for components before the end of their life. The proposed model also trained and tested on an open source data that is about an engine degradation simulation provided by the Prognostics CoE at NASA Ames."
115,unknown,10.1016/j.procs.2021.03.025,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85106733954,2021-01-01,lightweight photoplethysmography quality assessment for real-time iot-based health monitoring using unsupervised anomaly detection,"Real-time remote health monitoring is dramatically growing, revolutionizing healthcare delivery and outcome in everyday settings. Such remote services enable monitoring individuals anywhere and anytime, allowing diseases early detection and prevention. Photoplethysmography (PPG) is a non-invasive and convenient technique that enables tracking vital signs such as heart rate, heart rate variability, respiration rate, and blood oxygen saturation. PPG is broadly used in various clinical and commercial wearable devices, as it is easy-to-implement and low-cost. However, the technique is highly susceptible to motion artifacts and environmental noises, which distort the collected signals. Therefore, the signal quality needs to be investigated, and unreliable signals should be discarded. In the literature, rule-based and machine learning-based PPG quality assessment methods have been investigated in several studies. However, the rule-based methods are mostly inaccurate in remote health monitoring, where users engage in different physical activities. The supervised machine learning-based methods –including deep learning–are also infeasible for real-time monitoring applications since they are slow and are dependent on a massive pool of annotated data to train the model. In this paper, we introduce a PPG quality assessment method, enabled by an elliptical envelope, which requires low computational resources. The method clusters the PPG signals into two groups as “reliable” and “unreliable.” We also investigate various features extracted from the PPG signals. Five features with the highest scoring values are selected to be fed to the elliptical envelope model. Moreover, we assess the performance of the proposed method in terms of accuracy and execution time, using data collected in free-living conditions via an Internet-of-Things-based health monitoring system enabled by smart wristbands. The method is evaluated in comparison to a state-of-the-art PPG quality assessment method. We also provide the model implemented in Python for the community to be used in their solutions."
116,unknown,10.1016/j.iot.2019.100130,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85077314197,2020-03-01,an iot based device-type invariant fall detection system,"As the world elderly population is increasing rapidly, the use of technology for the development of accurate and fast automatic fall detection systems has become a necessity. Most of the fall detection systems are developed for specific devices which reduces the versatility of the fall detection system. This paper proposes a centralized unobtrusive IoT based device-type invariant fall detection and rescue system for monitoring of a large population in real-time. Any type of devices such as Smartphones, Raspberry Pi, Arduino, NodeMcu, and Custom Embedded Systems can be used to monitor a large population in the proposed system. The devices are placed into the users’ left or right pant pocket. The accelerometer data from the devices are continuously sent to a multithreaded server which hosts a pre-trained machine learning model that analyzes the data to determine whether a fall has occurred or not. The server sends the classification results back to the corresponding devices. If a fall is detected, the server notifies the mediator of the user's location via an SMS. As a failsafe, the corresponding device alerts nearby individuals by sounding the buzzer and contacts emergency medical services and mediators via SMS for immediate medical assistance, thus saving the user's life. The proposed system achieved 99.7% accuracy, 96.3% sensitivity, and 99.6% specificity. Finally, the proposed system can be implemented on a variety of devices and used to reliably monitor a large population with low false alarm rate, without obstructing the users’ daily living, as no external connections are required."
117,unknown,10.1016/j.ajo.2017.03.026,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85017447877,2017-06-01,using electronic health records to build an ophthalmologic data warehouse and visualize patients' data,"Purpose
                  To develop a near-real-time data warehouse (DW) in an academic ophthalmologic center to gain scientific use of increasing digital data from electronic medical records (EMR) and diagnostic devices.
               
                  Design
                  Database development.
               
                  Methods
                  Specific macular clinic user interfaces within the institutional hospital information system were created. Orders for imaging modalities were sent by an EMR-linked picture-archiving and communications system to the respective devices. All data of 325 767 patients since 2002 were gathered in a DW running on an SQL database. A data discovery tool was developed. An exemplary search for patients with age-related macular degeneration, performed cataract surgery, and at least 10 intravitreal (excluding bevacizumab) injections was conducted.
               
                  Results
                  Data related to those patients (3 142 204 diagnoses [including diagnoses from other fields of medicine], 720 721 procedures [eg, surgery], and 45 416 intravitreal injections) were stored, including 81 274 optical coherence tomography measurements. A web-based browsing tool was successfully developed for data visualization and filtering data by several linked criteria, for example, minimum number of intravitreal injections of a specific drug and visual acuity interval. The exemplary search identified 450 patients with 516 eyes meeting all criteria.
               
                  Conclusions
                  A DW was successfully implemented in an ophthalmologic academic environment to support and facilitate research by using increasing EMR and measurement data. The identification of eligible patients for studies was simplified. In future, software for decision support can be developed based on the DW and its structured data. The improved classification of diseases and semiautomatic validation of data via machine learning are warranted."
118,unknown,10.1109/csci49370.2019.00084,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9071016/,2019-12-07 00:00:00,a real-time based intelligent system for predicting equipment status,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time."
119,unknown,10.1109/csit49958.2020.9321954,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9321954/,2020-09-26 00:00:00,eco-friendly home automation system implemented using machine learning algorithms,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution."
120,unknown,10.1016/j.compind.2019.04.010,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065732680,2019-08-01,managing workflow of customer requirements using machine learning,"Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored.
                  The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier.
                  The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system.
                  Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow."
121,unknown,10.1109/vtcfall.2017.8288311,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8288311/,2017-09-27 00:00:00,towards an application for real-time travel mode detection in urban centers,"Context-aware applications in intelligent transportation systems have a growing need for travel mode detection systems. However, few applications allow real-time travel mode detection through the use of smartphones. In this paper, we propose a real-time travel mode detection application based on GPS traces using a data mining technique through which these traces are preprocessed, grouped in motion segments and classified by supervised machine learning algorithms. An application prototype was implemented on the Android platform, used by smartphones, for movement data collection and user travel mode detection using the WEKA API in Java. Finally, to evaluate the performance of the application in a real environment, field tests were carried out with dozens of volunteers in the metropolitan area of Rio de Janeiro. Therefore, 1338 travel mode inferences were obtained by four machine learning techniques and the results were evaluated and compared by the indicators of the confusion matrix. Thus, through the performance evaluation carried out, it was possible to verify that the proposed application is useful for real-time travel mode detection in urban centers."
122,unknown,10.1109/access.2020.3015655,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9163328/,2020-01-01 00:00:00,a two-layer water demand prediction system in urban areas based on micro-services and lstm neural networks,"In recent years, scarce water resources became one of the main problems that endanger human species existence and the advancement of any nation. In this research, smart water meters were implemented, distributed, and installed in a regional area in Cairo while data were collected at uniform intervals then sent to the cloud instantly. The solution paradigm uses an Internet of Things (IoT) based on micro-services and containers. The design incorporates real-time streaming and infrastructure performance optimization to store data. A second layer to analyze the acquired data was used to model water consumption using Long Short-Term Memory (LSTM). The designed LSTM is validated and tested to be utilized in the forecast of future water demand. Moreover, two alternative machine learning methods, namely Support Vector Regression and Random Forest commonly utilized in time series forecasting applications, were used for a comparative analysis of which LSTM has proven to be superior. The proper integration of the system elements is the key to the proposed system success. Based on the success of the designed system, it can be applicable on a national scale. That can enable the optimal management of consumers' demand and improve water infrastructure utilization. The proposed paradigm presents a testbed for various scenarios that can be used in water resources management."
123,unknown,10.3929/ethz-b-000347534,,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/478928b128cc8cd15515d348b644746bb0197b98,2018-01-01 00:00:00,"first analyses of rainfall patterns retrieved by a newly installed x-band radar over the metropolitan area of cagliari (sardinia, italy)","The growing urbanization and aggregation of metropolitan territorial communities, sustainable development, citizen engagement, economic and cultural attractiveness and governance are among the most important issues for modern cities. The increasing complexity of these problems and technological development are leading to an urgent need and the opportunity to radically rethink the way we build and manage our cities. The recent institution of the Metropolitan area of Cagliari, that counts more than 500 thousands inhabitants, stimulated the government of the Sardinian region to fund an innovative project (Tessuto Digitale Metropolitano), that will be developed jointly between the Center for advanced studies, research and development in Sardinia and the University of Cagliari. Specifically, the project aims at studying and developing innovative methods and technologies to offer new smart solutions to improve the attractiveness of the city, the management of resources and the safety and quality of life of citizens. These objectives can be pursued through the synergic use and experimentation of advanced communication infrastructures and widespread sensors, and the development of innovative vertical solutions. Among the others, the improvement of citizens’ safety against environmental risks is a priority objective, with a special regard to the development of monitoring and prediction systems of extreme precipitation events. In general, common characteristic of these phenomena is that their occurrence cannot be predicted with sufficient accuracy using traditional weather forecasting methods nor monitored by punctual traditional tipping buckets raingauges. This introduces the need for rainfall monitoring continuously in time and space, both to check their evolution in real time, and to dispose the necessary measures of civil protection. At the same time, the analysis of past observations, in terms of patterns and principal directions, allow to forecast (nowcasting) the occurrence of similar phenomena 30 minutes1hour ahead the rain hits the ground. Following these premises, the Department of Civil, Environmental Engineering and Architecture (DICAAR) of the University of Cagliari installed a weather radar (figure 1, left panel) over the towershaft elevator of a building of the Faculty of Engineering and Architecture, University of Cagliari (Lon 9.108720°, Lat 39.228991°). The radar is the SuperGauge model, produced by Envisens Technologies: it is an X-band radar characterized by a single elevation and single polarization, with 1 minute resolution in time and 60 m resolution in range. The radar can monitor an area within a radius of 30 km, with an azimuth resolution linearly increasing with distance up to 1500 m at the maximum distance (30 km). Hence, from the current position the instrument can monitor the whole Cagliari metropolitan area. Each scan is then processed to return the retrieved rainfall field in a regular grid with 60x60 meter grid-cells every minute. UrbanRain18 11 International Workshop on Precipitation in Urban Areas Fig 1: Left: Radar installation site. Right: Rainfall field observed during the 02-05-2018 event. The radar position was decided in order to limit electromagnetic interferences and minimize the ground clutter effects, which in turns are due to morphology and surrounding buildings. Initially, the radar was set with 0° elevation for the antenna. The first instrument run was during the rain events occurred throughout Sardinia at the beginning of May 2018, which showed high rainfall rates and precipitation volumes. Meteorological models correctly forecasted the storm occurrences and the civil protection issued several warnings of severe weather conditions; as a consequence, several damages were registered. A snapshot of the rainfall field as recorded by the radar during the event of 02-05-2018 is reported in figure 1 (right panel), showing some areas where the rainfall rate exceeds 40 mm/h. The comparison between the above observations and those collected by the National Radar Network supports the correct functioning of the instrument, at least in terms of registered rainfall patterns. Some adjustments and calibration are still needed: first, in order to minimize the ground clutter, hence improving the quality of the measurements, the elevation angle will be increased up to 3°. Second, rainfall observations inferred by the radar will be accurately adjusted taking the advantage of the Sardinia’s rain gauges network. When retrievals of other events will be collected and available, some nowcasting procedures will be implemented in order to use radar observations also to issue real time warnings. Traditional methods, based on cell tracking, area tracking, and stochastic algorithms will be compared to innovative methods, based on machine learning. Finally, radar and rain gauge data will be integrated with the sensor network envisaged by the abovementioned project, aimed at monitoring multiple environmental parameters (temperature, water level, wind speed, relative humidity). This complete data set will improve the forecast reliability, not only in terms of precipitation fields but also for many other quantities related to environmental security. Acknowledgments: This research was supported under the ROP Sardegna ERDF Action 1.2.2 (project “Tessuto Digitale Metropolitano”) and by Sardinian Regional Authorities."
124,unknown,10.1109/csci.2017.81,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8560838/,2017-12-16 00:00:00,object movement detection by real-time deep learning for security surveillance camera,Developing a smart Web Video Player application connected to a security surveillance camera to keep track of the object of interest is an ongoing research. This paper presents a methodology to real time data mining of the sequence of frames from a live stream collected by security camera by processing trajectories of an object of interest. Two classifiers and a clustering method are implemented all working in real-time. Real-time Deep Learning and Support Vector Machines (SVM) machine learning algorithms are implemented on a local server without the use of cloud computing. This is a popular architecture for many buildings and industries who want to have an in-house smart security camera application.
125,unknown,10.1016/j.chemolab.2021.104329,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105292476,2021-07-15,a novel approach for water quality classification based on the integration of deep learning and feature extraction techniques,"Water quality monitoring plays a vital role in the protection of water resources, environmental management, and decision-making. Artificial intelligence (AI) based on machine learning techniques has been widely used to evaluate and classify water quality for the last two decades. However, traditional machine learning techniques face many limitations, the most important of which is the inability to apply these techniques with big data generated by smart water quality monitoring stations to improve the prediction. Real-time water quality monitoring with high accuracy and efficiency for intelligent water quality monitoring stations requires new and sophisticated techniques based on machine and deep learning techniques. For this purpose, we propose a novel approach based on the integration of deep learning and feature extraction techniques to improve water quality classification. In this paper, was chosen the Tilesdit dam in Bouira (Algeria) as a case study. Moreover, we implemented the advanced deep learning method - Long Short Term Memory Recurrent Neural Networks (LSTM RNNs) to construct an intelligent model for drinking water quality classification. Furthermore, principal component analysis (PCA), linear discriminant analysis (LDA) and independent component analysis (ICA) techniques were used for features extraction and data reduction from original features. Additionally, we used three methods of cross-validation and two methods of the out-of-sample test to estimate the performance of LSTM RNNs model. From the results we found that the integration of LSTM RNNs with LDA, and LSTM RNNs with ICA yields an accuracy of 99.72%, using Random-Holdout technique."
126,included,10.1109/isc2.2016.7580798,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7580798/,2016-09-15 00:00:00,smartseal: a ros based home automation framework for heterogeneous devices interconnection in smart buildings,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it."
127,unknown,10.1109/spices.2017.8091310,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8091310/,2017-08-10 00:00:00,implementation of a self-adaptive real time recommendation system using spark machine learning libraries,"Real time recommendation systems have become an essential component of e-commerce web applications. With increasing volume and velocity of data handled by these applications, known as the bigdata problem, traditional recommendation systems that analyze data and update models at regular time intervals would not be able to satisfy this requirement. With the evolution of technologies for processing bigdata in real time, it has become fairly easy to implement real time recommendation systems. Stream-computing is a new computing paradigm for handling the velocity attribute of bigdata which makes it possible to develop real time bigdata applications. This paper gives the details of implementation of a real time recommendation system using Apache Spark, a widely used platform for stream computing. This system is implemented for recommending TV channels to viewers in real time. This becomes a challenging task due to continuous changes in the set of available channels and the context dependent preference of viewers. In channel recommendation scenario, characterized by its dynamic nature, volume of data, and tight time constraints, traditional approaches cannot be used. We have implemented a highly scalable TV channel recommendation system optimized for the processing of real-time data streams originating from set-top boxes. The proposed system implements a self-adaptive approach for model building. The system effectively uses distributed processing power of Apache Spark to make recommendations in real time with scalability to meet the real time constraints with increasing load. The Spark Machine Learning Libraries (Spark MLLib) provide several algorithms which were used for developing the proposed recommendation system. The large amount of data in the system is efficiently managed by the data processing method of Lambda Architecture."
128,unknown,10.1109/conielecomp.2014.6808580,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6808580/,2014-02-28 00:00:00,implementation of an embedded system on a ts7800 board for robot control,"Growing Functional Modules (GFM) learning based controllers need to be experimented on real robots. In 2009, looking to develop a flexible and generic embedded interface for such robots, we decided to use a TS-7800 single board computer (SBC) with a Debian Linux operating system. Despite the many advantages of this board, implementing the embedded system has been a complex task. This paper describes the implementation of protocols through the TS-7800 different ports (RS232, TCP/IP, USB, analog and digital pins) as well as the connection of external boards (TS-ADC24, TS-DIO64, SSC-32 and LCD display). This implementation was required to connect a large range of actuators, sensors and other peripherals. Furthermore, the architecture of the embedded system is exposed in detail, including topics such as the XML configuration file that specifies the peripherals connected to the SBC, the concept of virtual sensors, the implementation of parallelism and the embedded system interface launcher. Technical aspects such as the optimization of video capture and processing are detailed because their execution required specific compilers versions, EABI emulation and extra libraries (openCV libjpg and libpngand libv4l). The final embedded system was implemented in a humanoid robot and connected to the GFM controller in charge of developing its equilibrium subsystem."
129,unknown,10.1109/tsp.2019.8768883,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8768883/,2019-07-03 00:00:00,edge-ai in lora-based health monitoring: fall detection system with fog computing and lstm recurrent neural networks,"Remote healthcare monitoring has exponentially grown over the past decade together with the increasing penetration of Internet of Things (IoT) platforms. IoT-based health systems help to improve the quality of healthcare services through real-time data acquisition and processing. However, traditional IoT architectures have some limitations. For instance, they cannot properly function in areas with poor or unstable Internet. Low power wide area network (LPWAN) technologies, including long-range communication protocols such as LoRa, are a potential candidate to overcome the lacking network infrastructure. Nevertheless, LPWANs have limited transmission bandwidth not suitable for high data rate applications such as fall detection systems or electrocardiography monitoring. Therefore, data processing and compression are required at the edge of the network. We propose a system architecture with integrated artificial intelligence that combines Edge and Fog computing, LPWAN technology, IoT and deep learning algorithms to perform health monitoring tasks. In particular, we demonstrate the feasibility and effectiveness of this architecture via a use case of fall detection using recurrent neural networks. We have implemented a fall detection system from the sensor node and Edge gateway to cloud services and end-user applications. The system uses inertial data as input and achieves an average precision of over 90% and an average recall over 95% in fall detection."
130,unknown,10.1109/iwcmc48107.2020.9148180,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9148180/,2020-06-19 00:00:00,real-time health monitoring system based on wearable devices,"This paper proposed a novel electrocardiogram (ECG) automatic diagnose system for health assistance and rescue related with cardiovascular diseases. This system consists of three parts: 1) Data acquisition subsystem, this subsystem acquires ECG data from wearable devices on users' body and transmit them to the cloud server. 2) Deep learning analysis subsystem, with the help of convolutional neural network, the important feature lied inside ECG signal can be extract for abnormal heart condition detection. Hierarchical residual modules provide the network the ability to see seconds of signal and make a decision through the combination of features. Meanwhile, the global max pooling layer on top of the network enables it to capture the most important feature across the whole ECG signal with periodicity. This subsystem is a crucial part for cardiac status based health caring. 3) Back-stage management subsystem, methodical data storage and management were conducted in this subsystem, which also provides the users an interface to access their healthy data and body status. Assembling these three parts of system, real-time ECG diagnose for people in need and timely medical rescue can be implemented."
131,unknown,10.1109/bigdata47090.2019.9005638,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9005638/,2019-12-12 00:00:00,speech emotion detection using iot based deep learning for health care,"Human emotions are essential to recognize the behavior and state of mind of a person. Emotion detection through speech signals has started to receive more attention lately. This paper proposes the method for detecting human emotions using speech signals and its implementation in real-time using the Internet of Things (IoT) based deep learning for the care of older adults in nursing homes. The research has two main contributions. First, we have implemented a real-time system based on audio IoT, where we have recorded human voice and predicted emotions via deep learning. Secondly, for advance classification, we have designed a model using data normalization and data augmentation techniques. Finally, we have created an integrated deep learning model, called Speech Emotion Detection (SED), using a 2D convolutional neural networks (CNN). The best accuracy that was reported by our method was approximately 95%, which outperformed all state-of-the-art approaches. We have further extended to apply the SED model to a live audio sentiment analysis system with IoT technologies for the care of older adults in nursing homes."
132,unknown,10.1016/j.ipm.2020.102340,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85087504158,2020-11-01,a topic modeling framework for spatio-temporal information management,"Real-time processing and learning of conflicting data, especially messages coming from different ideas, locations, and time, in a dynamic environment such as Twitter is a challenging task that recently gained lots of attention. This paper introduces a framework for managing, processing, analyzing, detecting, and tracking topics in streaming data. We propose a model selector procedure with a hybrid indicator to tackle the challenge of online topic detection. In this framework, we built an automatic data processing pipeline with two levels of cleaning. Regular and deep cleaning are applied using multiple sources of meta knowledge to enhance data quality. Deep learning and transfer learning techniques are used to classify health-related tweets, with high accuracy and improved F1-Score. In this system, we used visualization to have a better understanding of trending topics. To demonstrate the validity of this framework, we implemented and applied it to health-related twitter data from users originating in the USA over nine months. The results of this implementation show that this framework was able to detect and track the topics at a level comparable to manual annotation. To better explain the emerging and changing topics in various locations over time the result is graphically displayed on top of the United States map."
133,unknown,10.1109/icac51239.2020.9357161,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9357161/,2020-12-11 00:00:00,deepfake audio detection: a deep learning based solution for group conversations,"The recent advancements in deep learning and other related technologies have led to improvements in various areas such as computer vision, bio-informatics, and speech recognition etc. This research mainly focuses on a problem with synthetic speech and speaker diarization. The developments in audio have resulted in deep learning models capable of replicating natural-sounding voice also known as text-to-speech (TTS) systems. This technology could be manipulated for malicious purposes such as deepfakes, impersonation, or spoofing attacks. We propose a system that has the capability of distinguishing between real and synthetic speech in group conversations.We built Deep Neural Network models and integrated them into a single solution using different datasets, including but not limited to Urban-Sound8K (5.6GB), Conversational (12.2GB), AMI-Corpus (5GB), and FakeOrReal (4GB). Our proposed approach consists of four main components. The speech-denoising component cleans and preprocesses the audio using Multilayer- Perceptron and Convolutional Neural Network architectures, with 93% and 94% accuracies accordingly. The speaker diarization was implemented using two different approaches, Natural Language Processing for text conversion with 93% accuracy and Recurrent Neural Network model for speaker labeling with 80% accuracy and 0.52 Diarization-Error-Rate. The final component distinguishes between real and fake audio using a CNN architecture with 94 % accuracy. With these findings, this research will contribute immensely to the domain of speech analysis."
134,unknown,10.1109/iccci50826.2021.9402589,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9402589/,2021-01-29 00:00:00,smart staff attendance system using convolutional neural network,"In this paper we have implemented Deep Learning model Convolutional Neural Network architecture for face detection to build a smart attendance system that will detect the faces of all the staff members and the attendance is marked automatically. This is a real time application which comes with day-to-day activities of handling attendance system in an institution. The process involves recognizing the face of staff members from the video taken through surveillance camera kept at different locations in the institution and other information technologies associated with the system. The proposed system will be able to find and recognize staff member faces fast and precisely with an accuracy of 90%. In addition, various data augmentation techniques are employed in the proposed system that improves the system accuracy further from 90% to 96%"
135,unknown,10.1109/tensymp52854.2021.9550904,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9550904/,2021-08-25 00:00:00,deep learning based smart parking for a metropolitan area,"In this study, we have introduced a method for utilizing the maximum parking space available for a metropolitan city. This will result in much lesser traffic congestion due to street-side parking. Furthermore, it will also decrease the hassle drivers face when they have to leave their vehicles on the side of the road to do other activities. The method introduces a Deep Learning based system where parking spaces are detected using Data Capturing Units (DCU). These DCUs feed data into our database which can be accessed by the users from our mobile application. The users can book parking spaces accordingly. All these data are saved in real-time and can be accessed through the mobile application. A vehicle classification system has also been designed that achieves an accuracy of 77% from multiple vehicle classes. Furthermore, a number plate recognition system has been used for the identification and safety protocols of the vehicles in parking sites. The number plate identification system is very precise and achieves an accuracy of over 90% for each digit. To the best of our knowledge, no other system of this kind has been implemented for the city of Dhaka before this. On top of that, successful implementation in a hectic city like Dhaka implies that it can be applied anywhere in the world. We believe this system can have a huge impact in reducing traffic congestions and can save an endless measure of time and money for citizens in a metropolitan area."
136,unknown,10.1109/coginf.2011.6016132,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6016132/,2011-08-20 00:00:00,an intelligent fault recognizer for rotating machinery via remote characteristic vibration signal detection,"Monitoring industrial machine health in real-time is not only highly demanded but also significantly complicated and difficult. Possible reasons for this include: (a) Access to the machines on site is sometimes impracticable; and (b) The environment in which they operate is usually not human-friendly due to pollution, noise, hazardous wastes, etc. Despite the theoretically sound findings on developing intelligent solutions for machine condition based monitoring, there are few commercial tools in the market that can readily be used. This paper reports on the development of an intelligent fault recognition and monitoring system (Melvin I), which detects and diagnoses rotating machine conditions according to changes in fault frequency indicators. The signals and data are remotely collected from designated sections of machines via data acquisition cards. They are processed by a signal processor in order to extract characteristic vibration signals of ten key performance indicators (KPIs). A 3-layer neural network is designed to recognize and classify faults based on the set of KPIs. The system implemented in our laboratory and applied in the field can also incorporate new experiences into the knowledge base without overwriting previous training. Preliminary results have demonstrated that Melvin I is a smart tool for both system vibration analysts and industrial machine operators."
137,unknown,10.1007/s11042-022-13514-7,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11042-022-13514-7,2022-07-26 00:00:00,bamcloud: a cloud based mobile biometric authentication framework,"There has been an exponential increase in the number of users switching to mobile banking. Therefore, various countries are adopting biometric solutions as security measures. Biometric technologies provide the potential security framework to make banking more convenient and secure than it has ever been. These technologies are gaining much popularity because of the ease in capturing biometric data in real-time using one’s mobile phone. At the same time, the exponential growth of enrollment in the biometric system produces a massive amount of high-dimensional data. To overcome performance-related issues arising due to the resulting data deluge, this paper aims to propose a distributed mobile biometric system based on a high-performance cluster Cloud. In this paper, a Cloud-based mobile biometric authentication framework (BAMCloud) is proposed that uses dynamic signatures for authentication. The process flow of the BAMCloud system involves capturing data using any handheld mobile device, followed by its storage, preprocessing, and training of the system in a distributed manner over the Cloud. MapReduce has been implemented on the Hadoop platform to reduce the processing time. For model training, The Levenberg-Marquardt backpropagation neural network has been used. It achieves a speed of 8.5 times the original speed and performance of 96.23%. Furthermore, the cost-benefit analysis of the implemented system shows that the cost of implementation and execution of the system is less than the existing ones. The experiments demonstrate that better performance is achieved by implementing the proposed framework as compared to other methods used in recent literature."
138,unknown,10.1109/ijcnn.2015.7280718,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7280718/,2015-07-17 00:00:00,real-time video object recognition using convolutional neural network,"A convolutional neural network (CNN) is implemented on a field-programmable gate array (FPGA) and used for recognizing objects in real-time video streams. In this system, an image pyramid is constructed by successively down-scaling the input video stream. Image blocks are extracted from the image pyramid and classified by the CNN core. The detected parts are then marked on the output video frames. The CNN core is composed of six hardware neurons and two receptor units. The hardware neurons are designed as fully-pipelined digital circuits synchronized with the system clock, and are used to compute the model neurons in a time-sharing manner. The receptor units scan the input image for local receptive fields and continuously supply data to the hardware neurons as inputs. The CNN core module is controlled according to the contents of a table describing the sequence of computational stages and containing the system parameters required to control each stage. The use of this table makes the hardware system more flexible, and various CNN configurations can be accommodated without re-designing the system. The system implemented on a mid-range FPGA achieves a computational speed greater than 170,000 classifications per second, and performs scale-invariant object recognition from a 720×480 video stream at a speed of 60 fps. This work is a part of a commercial project, and the system is targeted for recognizing any pre-trained objects with a small physical volume and low power consumption."
139,unknown,10.1109/saci.2007.375494,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/4262496/,2007-05-18 00:00:00,fpga parallel implementation of cmac type neural network with on chip learning,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor."
140,unknown,10.1016/j.aei.2020.101044,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85078852726,2020-01-01,iot edge computing-enabled collaborative tracking system for manufacturing resources in industrial park,"In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability."
141,unknown,10.1109/asonam.2012.66,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6425738/,2012-08-29 00:00:00,semi-supervised policy recommendation for online social networks,"Fine grain policy settings in social network sites is becoming a very important requirement for managing user's privacy. Incorrect privacy policy settings can easily lead to leaks in private and personal information. At the same time, being too restrictive would reduce the benefits of online social networks. This is further complicated with the growing adoption of social networks and with the rapid growth in information uploading and sharing. The problem of facilitating policy settings has attracted numerous access control, and human computer interaction researchers. The solutions proposed range from usable interfaces for policy settings to automated policy settings. We propose a fine grained policy recommendation system that is based on an iterative semi-supervised learning approach that uses the social graph propagation properties. Active learning and social graph properties were used to detect the most informative instances to be labeled as training sets. We implemented and tested our approach using real Facebook dataset. We compared our proposed approach to supervised learning and random walk approaches. Our proposed approaches provided high accuracy and precision when compared to the other approaches."
142,unknown,10.1016/j.procs.2020.03.044,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85085566175,2020-01-01,an artificial intelligence based crowdsensing solution for on-demand accident scene monitoring,"Road traffic crashes have a devastating impact on societies by claiming more than 1.35 million lives each year and causing up to 50 million injuries. Improving the efficiency of emergency management systems constitutes a key measure to reduce road traffic deaths and injuries. In this work, we propose a comprehensive crowdsensing-based solution for the real-time collection and the analysis of accident scene intelligence as a means to improve the efficiency of the emergency response process and help reduce road fatalities. The solution leverages sensory, mobile, and web technologies for the real-time monitoring of accident scenes, and employs Artificial Intelligence for the automatic analysis of the accident scene data, to allow the automatic generation of accident intelligence reports. Police officers and rescue teams can use those reports for fast and accurate situational assessment and effective response to emergencies. The proposed system was fully implemented and its operation was successfully tested using a variety of scenarios. This work gives interesting insights into the possibility of leveraging crowdsensing and artificial intelligence for offering emergency situational awareness and improving the efficiency of emergency response operations."
143,unknown,10.1016/j.promfg.2018.12.026,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85072561400,2019-01-01,hybrid artificial intelligence system for the design of highly-automated production systems,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype."
144,unknown,10.1016/j.ifacol.2021.04.197,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85107879835,2020-01-01,cognitive artificial population system: framework and application,"Agent-based social simulation has been comprehensively applied in the research of social and ecological systems. At its core is an artificial population, which endogenously drives the system evolution for particular applications, such as urban transportation, reginal economics, analysis of infectious disease transmission, and military simulation. In contrast with the previous population simulations where simple mathematical models are used to ‘reproduce’ actual demographic features, this paper proposes a self-evolutionary digital population system, named as Cognitive Artificial Population System (CAPS). At a more fine-grained level, CAPS focuses on the agent cognitive, reasoning and learning process in their surrounding environment, thus can exploit most advantages from cognitive computing and Artificial Intelligence. As a case study, Chinese population evolution is implemented using the proposed framework. Computational experiments indicate that CAPS is able to achieve good predicted population structures for real social systems."
145,unknown,10.1016/j.glt.2020.09.004,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85102077498,2020-01-01,development of an iot based real-time traffic monitoring system for city governance,"A significant amount of research work carried out on traffic management systems, but intelligent traffic monitoring is still an active research topic due to the emerging technologies such as the Internet of Things (IoT) and Artificial Intelligence (AI). The integration of these technologies will facilitate the techniques for better decision making and achieve urban growth. However, the existing traffic prediction methods mostly dedicated to highway and urban traffic management, and limited studies focused on collector roads and closed campuses. Besides, reaching out to the public, and establishing active connections to assist them in decision-making is challenging when the users are not equipped with any smart devices. This research proposes an IoT based system model to collect, process, and store real-time traffic data for such a scenario. The objective is to provide real-time traffic updates on traffic congestion and unusual traffic incidents through roadside message units and thereby improve mobility. These early-warning messages will help citizens to save their time, especially during peak hours. Also, the system broadcasts the traffic updates from the administrative authorities. A prototype is implemented to evaluate the feasibility of the model, and the results of the experiments show good accuracy in vehicle detection and a low relative error in road occupancy estimation. The study is part of the Omani-funded research project, investigating Real-Time Feedback for Adaptive Traffic Signals."
146,unknown,10.1109/uic-atc-scalcom-cbdcom-iop.2015.177,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7518356/,2015-08-14 00:00:00,fast fine-grained air quality index level prediction using random forest algorithm on cluster computing of spark,"As particulate materials in the air can cause several kinds of respiratory and cardiovascular diseases, the air quality information predicting attracts more and more attention. Knowing these information in advance is very important to protect human from health problems. With the development of computer technology, the data we can collect is increasingly becoming fine-grained. Most important of all, they need to be analyzed in real-time. However, existing methods could not meet the demand of real-time analysis. In this paper, we predict air quality based on a Spark implementation of random forest algorithm. First, a distributed random forest algorithm is implemented using Spark on the basis of resilient distributed dataset and shared variable. Then, we build an air quality prediction model using the parallelized random forest algorithm. The proposed method is evaluated with real meteorology data obtained from Beijing. The experiment results show that the proposed method is fast in predicting concentration level of PM2.5. And the results also prove the effectiveness and scalability of our method when deal with big data."
147,unknown,10.1109/ecai.2018.8679045,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8679045/,2018-06-30 00:00:00,intelligent conversational agent for online sales,"A conversational agent is one of emerging technologies that will take place in several of life domains, including e-commerce. The common features of any conversational agent in any domain is its ability to have a dialog with a human user and 24/7 available. Thus, including a conversational agent in an online business will give benefits not only to online retailers, but also e-customers. However, a conversational agent for online sales may have a different feature than a conversational agent in other domain such as health care. The main task of a conversational agent in an online sales domain is to engage and persuade e-commerce customers to buy a product. In a real life, persuasion is considered as an abstract process, which only can be achieved through a series of dialogs. In a virtual world, a conversational agent for online sales is considered intelligent if it is able to demonstrate this feature. The aim of this paper is to propose a method to demonstrate persuasion feature through recommendation and negotiation dialogs to meet e-customers' needs. A prototype of the proposed system has been implemented and preliminary results indicate the proposed method is successful."
148,unknown,10.1109/andescon50619.2020.9272196,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9272196/,2020-10-16 00:00:00,multipurpose unmanned system: an efficient solution to increase the capabilities of the uavs,"The results of this research propose the implementation of a system that significantly increases the capacity of unmanned vehicles, turning them into multifunctional vehicles. The system has a logistics dispatch module and a video analytics module. The first module allows the delivery of medical, food, smoke, disinfectant, etc. The module is practical, safe and economical, features that denote the feasibility of immediate implementation in unmanned vehicles of any rank and / or classification. Note that the implementation of the dispatch module does not require additional radio frequency systems. The second module includes a video analysis process in real time, an aspect that constitutes a significant contribution to the proposed solution, since it allows obtaining important information during the flight; it also reduces the risk in air operations and simultaneously increases the efficiency of themselves. Note that video analytics optimizes resources and avoids jeopardizing the lives of aircraft pilots and crews who traditionally should carry out these activities. In times of pandemic, this innovation avoids direct contact with an infected population and can guarantee the sanitary conditions required in certain circumstances. The solution increases the capabilities of unmanned vehicles and makes them useful tools in various scenarios, whether caused by natural or man-made disasters. Our proposal is very flexible, reliable, and scalable and can be adapted to various models and makes of unmanned vehicles. The system has been implemented on fixed-wing and rotary-wing unmanned vehicles, showing satisfactory results."
149,unknown,10.1016/j.jbi.2019.103138,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85062392033,2019-04-01,distributed learning from multiple ehr databases: contextual embedding models for medical events,"Electronic health record (EHR) data provide promising opportunities to explore personalized treatment regimes and to make clinical predictions. Compared with regular clinical data, EHR data are known for their irregularity and complexity. In addition, analyzing EHR data involves privacy issues and sharing such data is often infeasible among multiple research sites due to regulatory and other hurdles. A recently published work uses contextual embedding models and successfully builds one predictive model for more than seventy common diagnoses. Despite of the high predictive power, the model cannot be generalized to other institutions without sharing data. In this work, a novel method is proposed to learn from multiple databases and build predictive models based on Distributed Noise Contrastive Estimation (Distributed NCE). We use differential privacy to safeguard the intermediary information sharing. The numerical study with a real dataset demonstrates that the proposed method not only can build predictive models in a distributed manner with privacy protection, but also preserve model structure well and achieve comparable prediction accuracy. The proposed methods have been implemented as a stand-alone Python library and the implementation is available on Github (https://github.com/ziyili20/DistributedLearningPredictor) with installation instructions and use-cases."
150,unknown,10.1016/j.jbi.2016.09.015,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84989166008,2016-12-01,smart environment architecture for emotion detection and regulation,"This paper introduces an architecture as a proof-of-concept for emotion detection and regulation in smart health environments. The aim of the proposal is to detect the patient’s emotional state by analysing his/her physiological signals, facial expression and behaviour. Then, the system provides the best-tailored actions in the environment to regulate these emotions towards a positive mood when possible. The current state-of-the-art in emotion regulation through music and colour/light is implemented with the final goal of enhancing the quality of life and care of the subject. The paper describes the three main parts of the architecture, namely “Emotion Detection”, “Emotion Regulation” and “Emotion Feedback Control”. “Emotion Detection” works with the data captured from the patient, whereas “Emotion Regulation” offers him/her different musical pieces and colour/light settings. “Emotion Feedback Control” performs as a feedback control loop to assess the effect of emotion regulation over emotion detection. We are currently testing the overall architecture and the intervention in real environments to achieve our final goal."
151,unknown,10.1016/j.artmed.2015.12.001,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84994662260,2016-02-01,smartfaber: recognizing fine-grained abnormal behaviors for early detection of mild cognitive impairment,"Objective
                  In an ageing world population more citizens are at risk of cognitive impairment, with negative consequences on their ability of independent living, quality of life and sustainability of healthcare systems. Cognitive neuroscience researchers have identified behavioral anomalies that are significant indicators of cognitive decline. A general goal is the design of innovative methods and tools for continuously monitoring the functional abilities of the seniors at risk and reporting the behavioral anomalies to the clinicians. SmartFABER is a pervasive system targeting this objective.
               
                  Methods
                  A non-intrusive sensor network continuously acquires data about the interaction of the senior with the home environment during daily activities. A novel hybrid statistical and knowledge-based technique is used to analyses this data and detect the behavioral anomalies, whose history is presented through a dashboard to the clinicians. Differently from related works, SmartFABER can detect abnormal behaviors at a fine-grained level.
               
                  Results
                  We have fully implemented the system and evaluated it using real datasets, partly generated by performing activities in a smart home laboratory, and partly acquired during several months of monitoring of the instrumented home of a senior diagnosed with MCI. Experimental results, including comparisons with other activity recognition techniques, show the effectiveness of SmartFABER in terms of recognition rates."
152,unknown,10.1016/j.ifacol.2017.08.902,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85031797675,2017-07-01,a networked production system to implement virtual enterprise and product lifecycle information loops,"This paper is aimed at considering supply chain and related data management within an integrated vision of the product lifecycle management (PLM) implemented through the unified approach which is proper to the Industry 4.0 initiative. In particular, with the proposed manufacturing system architecture, decision support tools can use a unified repository fed by a factory replication application, powered by data from the field, even from remote production units. Such data allow to monitor the behaviour of the digital twin of the real machine and produces a digital twin of the real product, incorporating its actual characteristics measured by means of suitable acquiring systems (in the treated example: a 3D laser scanner). Moreover, it is provided a description of the plant technological subsystems that allow to share designing and manufacturing activities across multiple similar units located in remote areas. In this context of virtual enterprise, the supply chain management results as a key factor in enabling a cooperative approach."
153,unknown,10.1109/icais50930.2021.9395970,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9395970/,2021-03-27 00:00:00,garbage zero (garb0): an iot framework for effective garbage management in smart cities,"Today, there is a significant trend of blending optimized waste management strategies with low power, low-cost Internet of Things (IoT) solutions. This paper discusses and investigates Garbage Zero (Garb0), an indigenous product developed for outdoor garbage bins. Garb0 aims at developing a power efficient IoT based real-time solid waste monitoring solution that helps cities to optimize their waste collection and maintain clean and green environment. The Garb0 sensor module can be installed within public garbage bins to provide real time fill level data. The related information regarding the current fill level of the garbage bin will be transmitted to a cloud using the Low Power Wide Area Networks (LPWAN) based Long Range (LoRa) technology [1][2], which will be further processed and forwarded to garbage collector's mobile application and to the municipal dashboard for updating the status. Optimized schedule and route for collecting the garbage from the garbage bins will be generated on the garbage collector's app and accordingly directs the driver to follow the most efficient route. The main focus of this paper is on the hardware and software architecture implemented to achieve low cost, low power consumption and long battery life."
154,included,10.1016/j.engappai.2018.03.016,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85045475454,2018-06-01,proa: an intelligent multi-criteria personalized route assistant,"Personalization of pedestrian routes becomes a necessity due to the wide variety of user profiles that may differ on preferences or requirements to choose a route. Several software applications offer routes usually based on single criterion like distance or time; however, these criteria do not often fit the pedestrian needs.
                  Here, we will first focus on the Personalized Routes Problem and then we will approach the specific case of designing accessible and green pedestrian routes.
                  The proposal is implemented as a freely available Android application (named as PRoA, by intelligent multi-criteria Personalized Route Assistant), which automatically obtains geographical data and information for the decision criteria from open datasets.
                  The proposal is evaluated using real cases at the city of Granada, Spain."
155,unknown,10.1016/0736-5845(95)00008-9,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/0028719992,1994-01-01,cooperative multi-agent system for an assembly robotics cell,"A multi-agent system architecture is described and justified for the sake of its application to an assembly robotics testbed. A blackboard-based agent using a GoalBlackboard/DataBlackboard facility for intra (not inter) agent communication and including knowledge about all the agent's community, is presented as well as its main functionality. Coordination of different agents dynamically playing the roles either of organizers or respondents may lead to the use of either negotiation or client/server protocols for cooperation. Also come cooperative strategies and involved knowledge have been studied, classified and implemented in the robotics testbed enabling a sophisticated agent behavior both in terms of cooperation and local control. A real testbed, whose agents are briefly presented here, working with real-time constraints, has already been implemented and tested in our laboratory."
156,unknown,10.1109/ispa.2012.145,IEEE,ieeexplore,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6280306/,2012-07-13 00:00:00,anomaly detection algorithms on ibm infosphere streams: anomaly detection for data in motion,"This paper presents and shares excerpts from our implementation of near real-time anomaly detection algorithms on the IBM InfoSphere Streams platform. The purpose of this article is to: 1) Describe how to design and implement known anomaly detection algorithms on IBM InfoSphere Streams. 2) Present some performance optimization capabilities of IBM InfoSphere Streams platform and propose a method to use them in anomaly detection applications. 3) Present some IBM InfoSphere Streams best practices and describe how their adoption in the context of anomaly detection application. The document describes the architecture and design of anomaly detection algorithms developed on IBM InfoSphere Streams. Although the solution was designed to be used for cyber security, the implemented algorithms are agnostic regarding the data type that they monitor and therefore can detect anomalies in data from various industries such as healthcare, finance and retail. The document describes the implementation of two anomaly detection algorithms: KOAD and PCA. The KOAD algorithm performs online anomaly detection with incremental learning and the PCA algorithm in performs offline anomaly detection. The solution was designed to provide near real-time insight into low latency on large data volume observation."
157,unknown,10.1109/icmcce.2018.00050,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8537548/,2018-09-16 00:00:00,a smart manufacturing compliance architecture of electronic batch recording system (ebrs) for life sciences industry,"The paradigm shift brought about by smart manufacturing or Industrie 4.0 has posed threefold challenges to electronic batch recording system (eBRS) in Life Sciences Industry: 1) the structure of the data should be informative and standard for interoperate using information models, 2) administration of synchronization between physical world and cyber world for smart decision making and optimization using cyber physical system (CPS) and 3) so-called digital manufacturing operations management (digital MOM) characterized by decentralization, comprehensive collaboration and servitization shall be implemented. Under the new situations of smart manufacturing or Industrie 4.0, the requirements from information models, CPS and digital MOM will become the most principal criteria to be considered for future eBRS/MES and other operations management information system in shop floor. To fulfill these demands, an approach combining ISA95/88 hybrid model with activities ontology and variant domain-driven design for SOA-based eBRS development has been presented. An eBRS software platform has been developed on the theoretical basis and applied to a specific application scenario of Lyophilized Injection Production for verifying its feasibility purpose."
158,unknown,http://arxiv.org/abs/2107.07502v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2107.07502v2,2021-07-15 00:00:00,multibench: multiscale benchmarks for multimodal representation learning,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community."
159,unknown,10.1186/s40537-020-00340-7,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1186/s40537-020-00340-7,2020-08-12 00:00:00,big data architecture for intelligent maintenance: a focus on query processing and machine learning algorithms,"Exploiting available condition monitoring data of industrial machines for intelligent maintenance purposes has been attracting attention in various application fields. Machine learning algorithms for fault detection, diagnosis and prognosis are popular and easily accessible. However, our experience in working at the intersection of academia and industry showed that the major challenges of building an end-to-end system in a real-world industrial setting go beyond the design of machine learning algorithms. One of the major challenges is the design of an end-to-end data management solution that is able to efficiently store and process large amounts of heterogeneous data streams resulting from a variety of physical machines. In this paper we present the design of an end-to-end Big Data architecture that enables intelligent maintenance in a real-world industrial setting. In particular, we will discuss various physical design choices for optimizing high-dimensional queries, such as partitioning and Z-ordering, that serve as the basis for health analytics. Finally, we describe a concrete fault detection use case with two different health monitoring algorithms based on machine learning and classical statistics and discuss their advantages and disadvantages. The paper covers some of the most important aspects of the practical implementation of such an end-to-end solution and demonstrates the challenges and their mitigation for the specific application of laser cutting machines."
160,unknown,10.1016/j.cie.2019.106031,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85071975175,2019-11-01,machine learning based concept drift detection for predictive maintenance,"In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope."
161,unknown,10.1109/fccm.2017.58,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7966655/,2017-05-02 00:00:00,accelerating large-scale graph analytics with fpga and hmc,"Graph analytics that explores the relationship among interconnected entities is becoming increasingly important due to its broad applicability from machine learning to social science. However, one major challenge for graph processing systems is the irregular data access pattern of graph computation which can significantly degrade the performance. The algorithms, software, and hardware that have been tailored for mainstream parallel applications are, as a result, generally not effective for massive-scale sparse graphs from the real world due to their complexity and irregularity. To address the performance issues in large-scale graph analytics, we combine the emerging Hybrid Memory Cube (HMC) with a modern FPGA in order to achieve exceptional random access performance without any loss of flexibility or efficiency in computation. In particular, we develop collaborative software/hardware techniques to perform a level-synchronized breadth first search (BFS) on the FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that fully exploits the platform's capability to improve data locality and memory access efficiency. For each input graph, this algorithm provides an efficient data layout that allows the FPGA to coalesce memory requests into the largest possible HMC payload requests so that the number of memory requests, which is the primary factor in runtime, can be minimized. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by adding a merging unit. The merging unit takes the best advantage of the increased data locality resulting from graph clustering. We evaluated the performance of our BFS implementation using the AC-510 development kit from Micron over a set of benchmarks from a wide range of applications. We observed that the combination of the clustering algorithm and the merging hardware achieved 2.8 × average performance improvement compared to the latest FPGA-HMC based graph processing system."
162,unknown,http://arxiv.org/abs/2102.02638v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.02638v1,2021-02-02 00:00:00,"autodidactic neurosurgeon: collaborative deep inference for mobile edge
  intelligence via online learning","Recent breakthroughs in deep learning (DL) have led to the emergence of many
intelligent mobile applications and services, but in the meanwhile also pose
unprecedented computing challenges on resource-constrained mobile devices. This
paper builds a collaborative deep inference system between a
resource-constrained mobile device and a powerful edge server, aiming at
joining the power of both on-device processing and computation offloading. The
basic idea of this system is to partition a deep neural network (DNN) into a
front-end part running on the mobile device and a back-end part running on the
edge server, with the key challenge being how to locate the optimal partition
point to minimize the end-to-end inference delay. Unlike existing efforts on
DNN partitioning that rely heavily on a dedicated offline profiling stage to
search for the optimal partition point, our system has a built-in online
learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn
the optimal partition point on-the-fly. Therefore, ANS is able to closely
follow the changes of the system environment by generating new knowledge for
adaptive decision making. The core of ANS is a novel contextual bandit learning
algorithm, called $\mu$LinUCB, which not only has provable theoretical learning
performance guarantee but also is ultra-lightweight for easy real-world
implementation. We implement our system on a video stream object detection
testbed to validate the design of ANS and evaluate its performance. The
experiments show that ANS significantly outperforms state-of-the-art benchmarks
in terms of tracking system changes and reducing the end-to-end inference
delay."
163,unknown,10.1016/j.procir.2019.02.101,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065424368,2019-01-01,autonomous order dispatching in the semiconductor industry using reinforcement learning,"Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system."
164,unknown,10.1007/s00170-021-07248-3,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00170-021-07248-3,2021-08-01 00:00:00,cognitive capabilities for the caai in cyber-physical production systems,"This paper presents the cognitive module of the Cognitive Architecture for Artificial Intelligence (CAAI) in cyber-physical production systems (CPPS). The goal of this architecture is to reduce the implementation effort of artificial intelligence (AI) algorithms in CPPS. Declarative user goals and the provided algorithm-knowledge base allow the dynamic pipeline orchestration and configuration. A big data platform (BDP) instantiates the pipelines and monitors the CPPS performance for further evaluation through the cognitive module. Thus, the cognitive module is able to select feasible and robust configurations for process pipelines in varying use cases. Furthermore, it automatically adapts the models and algorithms based on model quality and resource consumption. The cognitive module also instantiates additional pipelines to evaluate algorithms from different classes on test functions. CAAI relies on well-defined interfaces to enable the integration of additional modules and reduce implementation effort. Finally, an implementation based on Docker, Kubernetes, and Kafka for the virtualization and orchestration of the individual modules and as messaging technology for module communication is used to evaluate a real-world use case."
165,unknown,10.1016/j.elerap.2021.101098,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85119699964,2021-11-01,an intelligent knowledge-based chatbot for customer service,"This study proposes an intelligent knowledge-based conversational agent system architecture to support customer services in e-commerce sales and marketing. A pilot implementation of a chatbot for customer services is reported in a leading women’s intimate apparel manufacturing firm. The proposed system incorporates various emerging technologies, including web crawling, natural language processing, knowledge bases, and artificial intelligence. In this study, a prototype system is built in a real-world setting. The results of the system prototype evaluation are satisfactory and support the contention that the system is effective. The study also discusses the challenges and lessons learned during system implementation and the theoretical and managerial implications of this study."
166,unknown,10.1007/s10796-020-09997-0,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s10796-020-09997-0,2021-02-01 00:00:00,garnlp: a natural language processing pipeline for garnishment documents,"Basic elements of the law, such as statuses and regulations, are embodied in natural language, and strictly depend on linguistic expressions. Hence, analyzing legal contents is a challenging task, and the legal domain is increasingly looking for automatic-processing support. This paper focuses on a specific context in the legal domain, which has so far remained unexplored: automatic processing of garnishment documents. A garnishment is a legal procedure by which a creditor can collect what a debtor owes by requiring to confiscate a debtor’s property (e.g., a checking account) that is hold by a third party, dubbed garnishee. Our proposal, motivated by a real-world use case, is a versatile natural-language-processing pipeline to support a garnishee in the processing of a large-scale flow of garnishment documents. In particular, we mainly focus on two tasks: (i) categorize received garnishment notices onto a predefined taxonomy of categories; (ii) perform an information-extraction phase, which consists in automatically identifying from the text various information, such as identity of involved actors, amounts, and dates. The main contribution of this work is to describe challenges, design, implementation, and performance of the core modules and methods behind our solution. Our proposal is a noteworthy example of how data-science techniques can be successfully applied to a novel yet challenging real-world context."
167,unknown,10.1109/pacificvis.2018.00026,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8365986/,2018-04-13 00:00:00,a visual analytics approach for equipment condition monitoring in smart factories of process industry,"Monitoring equipment conditions is of great value in manufacturing, which can not only reduce unplanned downtime by early detecting anomalies of equipment but also avoid unnecessary routine maintenance. With the coming era of Industry 4.0 (or industrial internet), more and more assets and machines in plants are equipped with various sensors and information systems, which brings an unprecedented opportunity to capture large-scale and fine-grained data for effective on-line equipment condition monitoring. However, due to the lack of systematic methods, analysts still find it challenging to carry out efficient analyses and extract valuable information from the mass volume of data collected, especially for process industry (e.g., a petrochemical plant) with complex manufacturing procedures. In this paper, we report the design and implementation of an interactive visual analytics system, which helps managers and operators at manufacturing sites leverage their domain knowledge and apply substantial human judgements to guide the automated analytical approaches, thus generating understandable and trustable results for real-world applications. Our system integrates advanced analytical algorithms (e.g., Gaussian mixture model with a Bayesian framework) and intuitive visualization designs to provide a comprehensive and adaptive semi-supervised solution to equipment condition monitoring. The example use cases based on a real-world manufacturing dataset and interviews with domain experts demonstrate the effectiveness of our system."
168,unknown,10.1109/sbesc49506.2019.9046078,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9046078/,2019-11-22 00:00:00,a linked data-based semantic information model for smart cities,"Smart cities typically involve a myriad of inter-connected systems intended to promote better management of urban and natural resources of cities, thereby contributing to the improve the quality of life of citizens. The heterogeneity of domains, systems, data, and relationships among them requires defining a data model able to express information in a flexible, extensible way while promoting interoperability between systems and applications. Furthermore, smart city systems can benefit from georeferenced information to allow for more effective actions over the real-world urban space. Aiming at tackling challenges related to data heterogeneity while considering georeferenced information, this work introduces LGeoSIM, a semantic-based information model for smart cities as means of fostering interoperability and powerful automated reasoning upon unambiguous information. LGeoSIM relies on the recent NGSI-LD Specification, thereby encompassing the principles of Linked Data to allow semantically defining information through ontologies and their interconnection. This paper also presents an implementation of LGeoSIM within Smart Geo Layers, a geographic-layered data middleware platform conceived to integrate data provided by heterogeneous sources in a smart city environment."
169,unknown,10.1109/iwcmc55113.2022.9825089,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9825089/,2022-06-03 00:00:00,real-time application for recognition and visualization of arabic words with vowels based dl and ar,"Text is difficult to read in some cases due to text orientation, writing style, very light colors, etc. Visually impaired or visually impaired people have difficulty reading text in all of these situations. The architecture proposed in this work is intended to detect and identify Arabic characters with vowels in natural environments. This architecture can help visually impaired or blind people to read the text correctly. It allows users to read the text in a better and more immersive way by combining augmented reality with digital material. The approach uses both deep learning and more specifically the VGG 19 model and augmented reality to improve the efficiency, clarity, and accuracy of text reading. For text detection and identification we use the VGG 19 model, and for text visualization, we use augmented reality. The implementation technique presented in this research for an augmented reality interactive virtual assistant system is for users to use their smartphone&#x0027;s camera to receive enhanced text information via a text image and a three-dimensional image to understand the displayed text. It offers an interesting way to understand their environment. The use of augmented reality to better display recognized text in 3D is a fantastic feature. User research studies are conducted to assess usability and user satisfaction."
170,unknown,10.1109/csci49370.2019.00077,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9070974/,2019-12-07 00:00:00,"embedded or integrated, autonomous intelligent monitoring architecture and framework for training assistance and automation","A key element to readiness lies in the ability to provide qualified trainers and role player personnel, a challenge in a budget constrained environment. The goal of this effort is to employ practical artificial intelligence (AI) and automation techniques to minimize staffing requirements for training, and to improve the quality and pace of training. This paper outlines implementation experiments and results of these techniques to interoperate multiple AI algorithm types in an architecture and framework to automate certain aspects of training systems. A core capability the ability to integrate into a variety of training systems, from embedded and virtual reality simulations to constructive wargames, via a user-friendly system of APIs."
171,unknown,10.1016/j.isatra.2020.02.023,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85080060395,2020-07-01,strengthening the perception of the virtual worlds in a virtual reality environment,"Virtual reality is becoming more and more improved primarily due to numerous applications and the powers of mobile devices. Using various sensors, precise displays and high computing powers smartphone are becoming devices that make the boost in technology. Now it is necessary to efficiently use various sensors without affecting system operation and improve control abilities for various purposes. Especially in practical applications received by mass users such as games and any kind of experience. In this article, we propose a system that allows to extend the perception of the virtual world by conveying information about the user’s movements in reality into the supervised model. The system retrieves data from several sources, quickly analyzes them using artificial intelligence techniques, and returns information to the mobile phone about the activity that is being processed. The concept extends the understanding of today’s virtual reality by allowing the user to move and perform simple gestures in a specially designed room. Moreover, we propose multiplayer mode in virtual reality, where players are in different places. The proposed architecture of the system has been tested on simple applications, and the results show high potential for implementations in various apps by achieving almost 90% efficiency in changing player direction in real time and only 7.5% of collision cases."
172,unknown,10.1109/robot.2000.844768,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/844768/,2000-04-28 00:00:00,application of automatic action planning for several work cells to the german ets-vii space robotics experiments,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces."
173,unknown,http://arxiv.org/abs/2101.10869v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.10869v2,2021-01-23 00:00:00,"a raspberry pi-based traumatic brain injury detection system for
  single-channel electroencephalogram","Traumatic Brain Injury (TBI) is a common cause of death and disability.
However, existing tools for TBI diagnosis are either subjective or require
extensive clinical setup and expertise. The increasing affordability and
reduction in size of relatively high-performance computing systems combined
with promising results from TBI related machine learning research make it
possible to create compact and portable systems for early detection of TBI.
This work describes a Raspberry Pi based portable, real-time data acquisition,
and automated processing system that uses machine learning to efficiently
identify TBI and automatically score sleep stages from a single-channel
Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and
verification of the system that can digitize EEG signal using an Analog to
Digital Converter (ADC) and perform real-time signal classification to detect
the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN)
and XGBoost based predictive models to evaluate the performance and demonstrate
the versatility of the system to operate with multiple types of predictive
models. We achieve a peak classification accuracy of more than 90% with a
classification time of less than 1 s across 16 s - 64 s epochs for TBI vs
control conditions. This work can enable development of systems suitable for
field use without requiring specialized medical equipment for early TBI
detection applications and TBI research. Further, this work opens avenues to
implement connected, real-time TBI related health and wellness monitoring
systems."
174,included,10.1007/978-3-030-77070-9_10,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-77070-9_10,2021-01-01 00:00:00,smart and intelligent chatbot assistance for future industry 4.0,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers."
175,included,10.1109/access.2020.2970178,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8974224/,2020-01-01 00:00:00,a novel software engineering approach toward using machine learning for improving the efficiency of health systems,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications."
176,unknown,10.1016/j.scs.2021.103215,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85111854275,2021-11-01,ioht-enabled gliomas disease management using fog computing computing for sustainable societies,"The proliferation of sensor-based applications in healthcare has given rise to Internet of Health Things (IoHT) that improves patient safety, staff morale, and operational efficiency. Edge-fog computing has seen significant development in recent years and supports the association of various intelligent things with sensors for establishing smooth data transfer. However, it becomes challenging for edge-fog computing to tackle diverse IoHT settings such as efficient disease management, emergency response management, etc. The key limitation of existing architectures is the restricted scalability and inability to meet the demands of hierarchical computing environments for IoHT. This is because latency-sensitive applications often require large quantities of data to be measured and transferred to the data centers, which causes delay and reduced output. This research proposes a novel edge-fog computing framework for the convergence of machine learning ensemble with edge-fog computing. The proposed architecture delivers healthcare as a fog system that handles data from different sources to manage the diseases effectively. The proposed framework is used for the real-life implementation and automatic detection of gliomas diseases. Glioma is a kind of tumor, which ensues in the spinal cord and a portion of the brain. Glioma instigates in the glial cells that surround the nerve cells. The proposed edge-fog framework efficiently manages the real-time data related to gliomas. This framework is configured for specific operating modes including diverse edge-fog scenarios, different user requirements, quality of service, precision, and predictive accuracy. The proposed framework is evaluated using real-time datasets from various sources and experimentally tested with reliable datasets that disclose the effectiveness of the proposed architecture. The performance of the proposed model is evaluated in terms of power consumption, latency, accuracy, and execution time, respectively."
177,unknown,10.1016/j.eswa.2020.113251,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85079340111,2020-07-01,integrating complex event processing and machine learning: an intelligent architecture for detecting iot security attacks,"The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives."
178,unknown,10.1016/j.micpro.2019.102960,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85077060597,2020-03-01,a novel hybrid optimized and adaptive reconfigurable framework for the implementation of hybrid bio-inspired classifiers for diagnosis,"Due to recent advances in IoT (Internet of Things) technologies, availability of reliable data and emergence of machine learning, bio-inspired learning and artificial intelligence, has demonstrated its ability to solve the large complex problems which is not possible before. In particular, machine learning and bio-inspired learning algorithms provides the effective solutions in image processing techniques. However, the implementation of the above-mentioned algorithms in the general CPU requires the intensive usage of bandwidth, area and power which makes the CPU unhealthy of usage and implementation. To overcome this problem, ASIC (application specific integrated circuits), GPU (Graphics Processing Unit) &FPGA (Field Programmable gate arrays) have been employed to improve the performance of the hybrid machine learning (ML) classifiers and deep learning algorithms. FPGA has been recently employed for an effective implementation and to achieve the high performance of the learning algorithms. But integrating the complex learning algorithms in FPGA still remains to be real challenge among the researchers. The paper proposes new reconfigurable architectures for bio- inspired classifiers to diagnosis the medical casualties which can be suitable for the tele health care applications. This paper aim is as follows (i) Design and implementation of Parallel Fusion of FSM and Reconfigurable shared Distributed Arithmetic for Bio-Inspired Classifiers (ii) Development of Accelerator Environment to test the performance of proposed architecture (iii) Performance evaluation of proposed architecture in terms of accuracy of detection in compared with MATLAB simulation iv) Implementation of proposed architectures in different ARtix-7 architectures and determination of power, throughput and area . Moreover, the proposed architecture has been tested with the and compared with the other existing architectures."
179,unknown,10.1016/j.gie.2019.03.019,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065917454,2019-07-01,quality assurance of computer-aided detection and diagnosis in colonoscopy,"Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field “deep learning,” have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice—polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing."
180,included,10.1109/syscon.2018.8369547,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8369547/,2018-04-26 00:00:00,an interactive architecture for industrial scale prediction: industry 4.0 adaptation of machine learning,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics."
181,unknown,10.1109/ehb50910.2020.9280165,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9280165/,2020-10-30 00:00:00,drivers’ drowsiness detection and warning systems for critical infrastructures,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events."
182,unknown,10.1109/wf-iot.2019.8767291,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8767291/,2019-04-18 00:00:00,mountain pine beetle monitoring with iot,"Outbreaks of forest pests cause large-scale damages, which lead to significant impact on the ecosystem as well as the forestry industry. Current methods of monitoring pest outbreaks involve field, aerial and remote sensing surveys. These methods only provide partial spatial coverage and can detect outbreaks only after they have substantially progressed across wide geographic areas. This paper presents an IoT system for real-time insect infestation detection using bioacoustic recognition via machine learning techniques. Specifically, we focus on detecting the Mountain Pine Beetle (MPB), which is the most destructive insect of mature pines in western North American forests. We present the design of the system and describe its various hardware and software components. Experimental results collected from a prototype implementation of the system are presented, which show that the system can detect MPB with 82% accuracy. We also demonstrate the applicability of our system in other noise monitoring applications, and report our experimental results on urban noise detection and classification."
183,unknown,10.1016/j.procs.2021.08.095,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85116946450,2021-01-01,soda: a real-time simulation framework for object detection and analysis in smart manufacturing,"For modern manufacturing firms, automation has already become a norm but constantly needs to be improved as firms still face strong demand to increase their productivity. This can be achieved by reducing dependability on manpower, reaching lean and even unmanned production and this is where some of the standards of Industry 4.0 come in useful, not to mention: Machine Vision, Image Recognition or Machine Learning. In our paper, we present SODA – our approach to build a flexible ML and AI enabled framework for object detection, analysis, and simulation. The framework is designed to support a development process of solutions requiring real-time analysis of images of different types of moving objects on a conveyor belt. In our work we discuss architectural challenges of the developed framework as well as the basic components of the system. We do also provide information on how to use the framework and present a sample implementation of an actual system employing some of the machine learning methods."
184,unknown,http://arxiv.org/abs/1806.07761v3,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1806.07761v3,2018-06-20 00:00:00,"quick and plenty: achieving low delay and high rate in 802.11ac edge
  networks","We consider transport layer approaches for achieving high rate, low delay
communication over edge paths where the bottleneck is an 802.11ac WLAN. We
first show that by regulating send rate so as to maintain a target aggregation
level it is possible to realise high rate, low delay communication over
802.11ac WLANs. We then address two important practical issues arising in
production networks, namely that (i) many client devices are non-rooted mobile
handsets/tablets and (ii) the bottleneck may lie in the backhaul rather than
the WLAN, or indeed vary between the two over time. We show that both these
issues can be resolved by use of simple and robust machine learning techniques.
We present a prototype transport layer implementation of our low delay rate
allocation approach and use this to evaluate performance under real radio
conditions."
185,unknown,10.1016/j.procs.2020.09.009,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85093365315,2020-01-01,passenger bibo detection with iot support and machine learning techniques for intelligent transport systems,"The present article discusses the issue of automation of the CICO (Check-In/Check-Out) process for public transport fare collection systems, using modern tools forming part of the Internet of Things, such as Beacon and Smartphone. It describes the concept of an integrated passenger identification model applying machine learning technology in order to reduce or eliminate the risks associated with the incorrect classification of a smartphone user as a vehicle passenger. This will allow for the construction of an intelligent fare collection system, operating in the BIBO (Be-In/Be-Out) model, implementing the ""hands-free"" and ""pay-as-you-go"" approach. The article describes the architecture of the research environment, and the implementation of the elaborated model in the Bad.App4 proprietary solution. We also presented the complete process of concept verification under real-life conditions. Research results were described and supplemented with commentary."
186,unknown,10.1109/wimob.2019.8923286,"2019 International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)",semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e16e96b90d80a8c52eba904b6f99d72d40d09437,2019-01-01 00:00:00,design and deployment of a wireless ban at the edge for reliable healthcare monitoring,"Body Area Networks (BANs) have attracted a lot of research interest in the last decades as also witnessed by standardization activities and European Commission fundings. Today, commercial devices, which implement simplified BAN monitoring have appeared, although they usually imply expensive subscription costs or need for a supporting device carried by the user (e.g. a smart phone) which, in case of elderly people, cannot be easily held. However, the integration between commercial devices and external sensors located on/in the body is still an open issue due to the limited processing capabilities of low cost commercial devices. Also, no tools for anomaly detection aimed at reliable healthcare monitoring are currently commercially available. In this paper, we focus on Cloud-Assisted BANs and evolve this vision according to the emerging paradigm of edge computing. We present design, implementation and experimentation of a wireless BAN system which performs data transmission using a commercial, cheap, off-the-shelf gateway smart watch. A mechanism for prompt anomaly detection at the edge node is also supported for the purpose of reliable healthcare monitoring as well as pre-filtering of the data at the smart device itself. Also, in order to reduce the overhead caused by propagation of useless and time-correlated data, and to guarantee a prompt action in case of emergency, edge network nodes located closer to the patient BAN are exploited since they can execute machine learning algorithms to process large amounts of data and activate potential alerts in a shorter time and without overloading the cloud. In this work we describe a real system and evaluate the effectiveness of the approach in terms of false alarm probability."
187,unknown,10.1109/iccci50826.2021.9402701,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9402701/,2021-01-29 00:00:00,iot based two way safety enabled intelligent stove with age verification using machine learning,"Smart embedded systems have become a core component in the latest technologies, and IoT based smart embedded system is the trendiest field in the research area. In our research, we are proposing an IoT based smart stove. Any accident might occur at any time from a stove. So we are designing a two-way safety enabled stove with a child lock system and gas leakage detection feature. The intelligent stove will try to ensure safety and will detect age from real-time video streaming. Our main focus is a child would not be able to turn the stove on. As well as, the stove can entitle safety via gas detection alarm. We are using a Raspberry Pi and Gas Detection Module with a buzzer for the hardware implementation. Also, we are applying a Machine Learning object detection algorithm (Haar Cascade) and a deep learning architecture (CNN) for the system execution. Since our stove is IoT-based, the stove is ensuring safety remotely as well as manually which will try to prevent accidental occurrences."
188,unknown,10.1109/ic4me247184.2019.9036531,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9036531/,2019-07-12 00:00:00,object detection based security system using machine learning algorthim and raspberry pi,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally."
189,unknown,10.11591/ijece.v12i1.pp331-338,'Institute of Advanced Engineering and Science',core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/478033462.pdf,2022-02-01 00:00:00,real-time traffic sign detection and recognition using raspberry pi,"Nowadays, the number of road accident in Malaysia is increasing expeditiously. One of the ways to reduce the number of road accident is through the development of the advanced driving assistance system (ADAS) by professional engineers. Several ADAS system has been proposed by taking into consideration the delay tolerance and the accuracy of the system itself. In this work, a traffic sign recognition system has been developed to increase the safety of the road users by installing the system inside the car for driver’s awareness. TensorFlow algorithm has been considered in this work for object recognition through machine learning due to its high accuracy. The algorithm is embedded in the Raspberry Pi 3 for processing and analysis to detect the traffic sign from the real-time video recording from Raspberry Pi camera NoIR. This work aims to study the accuracy, delay and reliability of the developed system using a Raspberry Pi 3 processor considering several scenarios related to the state of the environment and the condition of the traffic signs. A real-time testbed implementation has been conducted considering twenty different traffic signs and the results show that the system has more than 90% accuracy and is reliable with an acceptable delay"
190,unknown,http://arxiv.org/abs/1907.12817v2,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1907.12817v2,2019-07-30 00:00:00,"increasing scalability of process mining using event dataframes: how
  data structure matters","Process Mining is a branch of Data Science that aims to extract
process-related information from event data contained in information systems,
that is steadily increasing in amount. Many algorithms, and a general-purpose
open source framework (ProM 6), have been developed in the last years for
process discovery, conformance checking, machine learning on event data.
However, in very few cases scalability has been a target, prioritizing the
quality of the output over the execution speed and the optimization of
resources. This is making progressively more difficult to apply process mining
with mainstream workstations on real-life event data with any open source
process mining framework. Hence, exploring more scalable storage techniques,
in-memory data structures, more performant algorithms is a strictly incumbent
need. In this paper, we propose the usage of mainstream columnar storages and
dataframes to increase the scalability of process mining. These can replace the
classic event log structures in most tasks, but require completely different
implementations with regards to mainstream process mining algorithms.
Dataframes will be defined, some algorithms on such structures will be
presented and their complexity will be calculated."
191,unknown,10.1109/isrcs.2013.6623773,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6623773/,2013-08-15 00:00:00,scalable machine learning framework for behavior-based access control,"Today's activities in cyber space are more connected than ever before, driven by the ability to dynamically interact and share information with a changing set of partners over a wide variety of networks. The success of approaches aimed at securing the infrastructure has changed the threat profile to point where the biggest threat to the US cyber infrastructure is posed by targeted cyber attacks. The Behavior-Based Access Control (BBAC) effort has been investigating means to increase resilience against these attacks. Using statistical machine learning, BBAC (a) analyzes behaviors of insiders pursuing targeted attacks and (b) assesses trustworthiness of information to support real-time decision making about information sharing. The scope of this paper is to describe the challenge of processing disparate cyber security information at scale, together with an architecture and work-in-progress prototype implementation for a cloud framework supporting a strategic combination of stream and batch processing."
192,unknown,10.1109/access.2021.3100865,'Institute of Electrical and Electronics Engineers (IEEE)',core,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-01-01 00:00:00,development and implementation of a <underline>f</underline>ramework for <underline>a</underline>erospace <underline>ve</underline>hicle <underline>r</underline>easoning (faver),"This paper discusses the development and implementation of the architecture of a Framework for Aerospace Vehicle Reasoning, &#x2018;FAVER&#x2019;. Integrated Vehicle Health Management systems require a holistic view of the aircraft to isolate faults cascading between aircraft systems. FAVER is a system-agnostic framework developed to isolate such propagating faults by incorporating Digital Twins (DTs) and reasoning techniques. The flexibility of FAVER to work with different types and scales of DTs and diagnostics, and its ability to adapt and expand for previously unknown faults and new systems are demonstrated in this paper. The paper also shows the novel combination of relationship matrix and fault attributes database used to structure the knowledge of FAVER&#x2019;s expert system. The paper provides the working mechanism of FAVER&#x2019;s reasoning and its ability to isolate faults at the system level, identify their root causes, and predict the cascading effects at the vehicle level. Four aircraft systems are used for demonstration purposes: i) the Electrical Power System, ii) the Fuel System, iii) the Engine, and iv) the Environmental Control System, and the use case scenarios are adapted from real aircraft incidents. The paper also discusses the pros and cons of FAVER&#x2019;s reasoning via demonstrations and evaluates the performance of FAVER&#x2019;s reasoning through a comparative study with a supervised neural network model"
193,unknown,10.1109/icmlc.2007.4370883,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/4370883/,2007-08-22 00:00:00,design and application of multimedia mobile learning framework,"Due to the thrive of mobile network and portable device, distance learning is evolved from desktop computer to mobile device. This paper combines mobile computing with streaming media technology in the application of distance education to design an open multimedia mobile learning framework for university students. This framework consists of mobile learning software and custom learning device. System architecture and several implementation issues are discussed in this paper. Moreover, practical applications show that our framework can significantly increase student's interest and effect in learning as compare to the traditional mode. The system is of preferable feature of real-time, interactive and expansion."
194,unknown,10.1109/sysose.2017.7994953,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7994953/,2017-06-21 00:00:00,autonomous decision making for a driver-less car,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome."
195,unknown,http://arxiv.org/abs/2103.11052v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.11052v1,2021-03-19 00:00:00,"a first step towards automated species recognition from camera trap
  images of mammals using ai in a european temperate forest","Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices."
196,unknown,http://arxiv.org/abs/1909.06526v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.06526v1,2019-09-14 00:00:00,ffdl : a flexible multi-tenant deep learning platform,"Deep learning (DL) is becoming increasingly popular in several application
domains and has made several new application features involving computer
vision, speech recognition and synthesis, self-driving automobiles, drug
design, etc. feasible and accurate. As a result, large scale on-premise and
cloud-hosted deep learning platforms have become essential infrastructure in
many organizations. These systems accept, schedule, manage and execute DL
training jobs at scale.
  This paper describes the design, implementation and our experiences with
FfDL, a DL platform used at IBM. We describe how our design balances
dependability with scalability, elasticity, flexibility and efficiency. We
examine FfDL qualitatively through a retrospective look at the lessons learned
from building, operating, and supporting FfDL; and quantitatively through a
detailed empirical evaluation of FfDL, including the overheads introduced by
the platform for various deep learning models, the load and performance
observed in a real case study using FfDL within our organization, the frequency
of various faults observed including unanticipated faults, and experiments
demonstrating the benefits of various scheduling policies. FfDL has been
open-sourced."
197,unknown,http://arxiv.org/abs/2107.13473v3,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2107.13473v3,2021-07-28 00:00:00,"the portiloop: a deep learning-based open science tool for closed-loop
  brain stimulation","Closed-loop brain stimulation refers to capturing neurophysiological measures
such as electroencephalography (EEG), quickly identifying neural events of
interest, and producing auditory, magnetic or electrical stimulation so as to
interact with brain processes precisely. It is a promising new method for
fundamental neuroscience and perhaps for clinical applications such as
restoring degraded memory function; however, existing tools are expensive,
cumbersome, and offer limited experimental flexibility. In this article, we
propose the Portiloop, a deep learning-based, portable and low-cost closed-loop
stimulation system able to target specific brain oscillations. We first
document open-hardware implementations that can be constructed from
commercially available components. We also provide a fast, lightweight neural
network model and an exploration algorithm that automatically optimizes the
model hyperparameters to the desired brain oscillation. Finally, we validate
the technology on a challenging test case of real-time sleep spindle detection,
with results comparable to off-line expert performance on the Massive Online
Data Annotation spindle dataset (MODA; group consensus). Software and plans are
available to the community as an open science initiative to encourage further
development and advance closed-loop neuroscience research."
198,unknown,10.1016/j.epsr.2017.09.003,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85029511531,2018-01-01,new online load forecasting system for the spanish transport system operator,"This paper presents the implementation of a new online real-time hybrid load-forecasting model based on an autoregressive model and neural networks. This new system is currently running at the Spanish Transport System Operator (REE) and provides an hourly forecast for the current day and the next nine days timely every hour for the national system as well as 18 regions of Spain. These requirements impose a heavy computational burden that needs to be considered during the design phase. The system is developed to improve forecasting accuracy specifically on difficult days like hot, cold and special days. In order to achieve this goal, a deep analysis of the temperature series from 59 stations is made for each region and the relevant series are included individually in the model. Special days are also analyzed and a thorough classification of days is proposed for the Spanish national and regional system. The model is designed and tested with data from 2005 to 2015. The results provided for the period from December 2014 to October 2015 show how the addition of the proposed model to the TSO’s ensemble causes a 5% RMSE overall error reduction and a 15% reduction on the 59 difficult days considered in the testing period."
199,included,10.1109/rweek.2018.8473535,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8473535/,2018-08-23 00:00:00,framework for data driven health monitoring of cyber-physical systems,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features."
200,unknown,10.1109/ieem.2018.8607399,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8607399/,2018-12-19 00:00:00,a chatbot-supported smart wireless interactive healthcare system for weight control and health promotion,"People who are overweight and obese have a greater risk of developing serious diseases and health conditions. A steadily increasing trend of obesity is not only limited to developed countries, but to developing nations as well. As smartphones have rapidly gained mainstream popularity, mobile applications (apps) are used in public health as intervention to keep track of diets, activity as well as weight, which is deemed more accurate than relying on user's self-report measure, for the sake of weight management. A solution called “Smart Wireless Interactive Healthcare System” (SWITCHes) is developed to facilitate objective data reception and transmission in a real-time manner. Based on the user data acquired from SWITCHes app and the auxiliary data from medical instruments, not only SWITCHes app can engage user with tailored feedback in an interactive way, in terms of artificial intelligence-powered health chatbot, but the healthcare professional can provide the more accurate medical advice to user also. This paper presents an overview of development and implementation of SWITCHes."
201,included,10.1109/itc-egypt52936.2021.9513888,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9513888/,2021-07-15 00:00:00,a proposed end to end telemedicine system based on embedded system and mobile application using cmos wearable sensors,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver."
202,unknown,10.1016/j.compind.2020.103329,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85092922057,2020-12-01,a middleware platform for intelligent automation: an industrial prototype implementation,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms."
203,unknown,10.1109/access.2020.3010609,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9144582/,2020-01-01 00:00:00,a privacy-aware crowd management system for smart cities and smart buildings,"Cities are growing at a dizzying pace and they require improved methods to manage crowded areas. Crowd management stands for the decisions and actions taken to supervise and control densely populated spaces and it involves multiple challenges, from recognition and assessment to application of actions tailored to the current situation. To that end, Wi-Fi-based monitoring systems have emerged as a cost-effective solution for the former one. The key challenge that they impose is the requirement to handle large datasets and provide results in near real-time basis. However, traditional big data and event processing approaches have important shortcomings while dealing with crowd management information. In this paper, we describe a novel system architecture for real-time crowd recognition for smart cities and smart buildings that can be easily replicated. The described system proposes a privacy-aware platform that enables the application of artificial intelligence mechanisms to assess crowds' behavior in buildings employing sensed Wi-Fi traces. Furthermore, the present paper shows the implementation of the system in two buildings, an airport and a market, as well as the results of applying a set of classification algorithms to provide crowd management information."
204,unknown,10.1109/rtss46320.2019.00041,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9052123/,2019-12-06 00:00:00,cost-aware edge resource probing for infrastructure-free edge computing: from optimal stopping to layered learning,"To meet the stringent requirement of artificial intelligence applications, such as face recognition and video streaming analytics, a resource-constrained device can offload its task to nearby resource-rich devices in edge computing. Resource awareness, as a prime prerequisite for offloading decision-making, is critical for achieving efficient collaborative computation performance. In this paper, we consider cost-aware edge resource probing (CERP) framework design for infrastructure-free edge computing wherein a task device self-organizes its resource probing for informed computation offloading. We first propose a multi-stage optimal stopping formulation for the problem, and derive the optimal probing strategy which reveals a nice multi-threshold structure. Accordingly, we then devise a data-driven layered learning mechanism for more practical and complicated application environments. Layered learning enables the task device to adaptively learn the optimal probing sequence and decision thresholds at runtime, aiming at deriving a good balance between the gain of choosing the best edge device and the accumulated cost of deep resource probing. We further conduct thorough performance evaluation of the proposed CERP schemes using both extensive numerical simulations and realistic system prototype implementation, which demonstrate the superior performance of CERP in the diverse application scenarios."
205,unknown,10.1016/j.neucom.2020.11.066,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85098065075,2021-03-07,bayesuites: an open web framework for massive bayesian networks focused on neuroscience,"BayeSuites is the first web framework for learning, visualizing, and interpreting Bayesian networks (BNs) that can scale to tens of thousands of nodes while providing fast and friendly user experience. All the necessary features that enable this are reviewed in this paper; these features include scalability, extensibility, interoperability, ease of use, and interpretability. Scalability is the key factor in learning and processing massive networks within reasonable time; for a maintainable software open to new functionalities, extensibility and interoperability are necessary. Ease of use and interpretability are fundamental aspects of model interpretation, fairly similar to the case of the recent explainable artificial intelligence trend. We present the capabilities of our proposed framework by highlighting a real example of a BN learned from genomic data obtained from Allen Institute for Brain Science. The extensibility properties of the software are also demonstrated with the help of our BN-based probabilistic clustering implementation, together with another genomic-data example."
206,unknown,10.1016/j.enggeo.2020.105817,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85090411791,2020-12-05,"successful implementations of a real-time and intelligent early warning system for loess landslides on the heifangtai terrace, china","Real-time monitoring and intelligent early warning system are crucial and significant to take mitigation measures and reduce casualties and property losses related to landslides. It is difficult to obtain entire monitoring data in the accelerated deformation phase in a landslide event, and hard to issue early warning information using a traditional monitoring approach with fixed and low sampling frequency. Displacement increments of loess landslides induced by agriculture irrigation on the Heifangtai terrace could be sudden and extremely rapid. Typical landslide types include loess flowslides and loess falls. It is of practical significance to develop a self-adaptive data acquisition monitoring technique and establish a real-time landslide early warning system (LEWS) to meet the needs for risk mitigation of rapid sliding slopes on the Heifangtai terrace. The monitoring technique can wirelessly transmit displacement data and the LEWS was devised using the new artificial intelligence. The LEWS could automatically release the warning information in advance of the event once the early warning parameters exceed default thresholds. In this study, the early warning procedures, real-time monitoring approach, intelligent LEWS, a multiple criteria warning model, warning release and emergency mitigation measures, and performance are introduced in detail. Six loess landslides at Heifangtai and eight landslides in other regions of China have been successfully warned since its implementation in 2012. This study proposed an effective and practical solution for the early warning of loess landslides at Heifangtai. Two typical loess landslides that had successful early warnings at Heifangtai were presented. The successful implementation could serve as a reference for global rapid slope failure cases, considering the complex nature of landslide behaviors and failure mechanisms."
207,unknown,10.1109/access.2019.2910641,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8691403/,2019-01-01 00:00:00,integration of recommendation systems into connected health for effective management of chronic diseases,"An important trend in meeting the needs of modern caregiving is providing a well-rounded care delivery. In doing this, access to data of care receiver is important. This, however, has been shown to be possible with connected health with the aid of modern technology. In this paper, we are presenting a design that will expand on the opportunities for better data accessibility and use, by integrating the recommendation system into connected health. In order to ensure a design that meets the needs of care receivers, we conducted two independent surveys with a view to gathering requirements for the design. The result has shown the relevance of timeliness in caregiving along with what the care receivers are interested in such as what their data say about their conditions, the immediate and future expectation of their conditions, and collaborative efforts in managing their conditions. The requirements gathered were used in the design and implementation of the mobile app called Recommendations Sharing Community for Aged and Chronically Ill People (ReSCAP). The benefits of this solution have been real-time recommendations' availability, cost saving, adequate care, and access to health data and information for better care decision-making."
208,unknown,10.1109/itaic.2019.8785620,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8785620/,2019-05-26 00:00:00,architecture design of distributed medical big data platform based on spark,"An Apache Spark based distributed computing and storage system designed for large-scale health data. The system provides the solution for health data digitalization and analysis, while enabling high-throughput data processing, real-time processing and messaging capabilities. The design of the system has the potential to provide many health-related services to medical professionals, such as data retrieving/processing, real-time alerts, data mining. The article described key considerations throughout the system designing process, including comparison of different component, finding the optimal data flow mechanics for specific tasks, etc. During the design and implementation, these core technologies are involved: (1) Java, Scala, programming languages, with IntelliJ IDEA IDE. (2)Apache Spark, general purpose distributed data processing engine. (3)Apache Hadoop: HDFS and YARN,distributed storage system and computational resource manager, respectively. (4)MySQL, relational database engine.(5)Apache Hive, distributed relational database engine.(6)Apache Kafka, distributed messaging system."
209,unknown,10.1109/icecce52056.2021.9514169,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9514169/,2021-06-13 00:00:00,real-time fraud prediction on streaming customer-behaviour data,"The field of detection and analysis of fraud situations continues to be an essential topic in the telecom industry. This study proposes a methodology that can predict abnormal behavior situations on real-time streaming customer behavior data in the telecommunication domain. The prototype implementation of the proposed method is designed, developed, and applied to the telecommunications dataset. We perform performance tests for the method's prediction success and scalability metrics on the designed prototype application. The results indicate that the proposed method proves to be a promising approach in the telecommunication sector."
210,unknown,10.1016/j.resconrec.2016.03.012,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85028239611,2016-11-01,implementation of optimass to optimise municipal wastewater sludge processing chains: proof of concept,"In sludge management, sludge is increasingly perceived as a marketable product rather than as a waste material. This awareness in combination with the variety of factors influencing the optimal management strategy and disposal route, introduces the need to optimise the sludge treatment throughout the whole chain instead of only minimising its production. In this paper, OPTIMASS, a mixed integer linear programming model to optimise strategic and tactical decisions in biomass-based supply chains, is proposed in order to meet this need. The applicability of OPTIMASS is illustrated through its implementation with a view to minimise the overall global warming impact of a real municipal wastewater sludge processing chain in “region X”. A first scenario addresses the optimisation of the allocation and treatment of municipal wastewater sludge within the current network. Second, OPTIMASS is used to identify the optimal location(s) for new drying facilities in this chain. Finally, the effect on the optimal chain of changes in municipal wastewater sludge production and of changes in global warming impact of the cement industry as a disposal route is evaluated.
                  The analysis reveals that municipal wastewater sludge processing chains can be considered to be instances of the generic biomass-based supply chain and that the OPTIMASS tool can be applied to support strategic and tactical decisions for optimising sludge management in case new technologies, new treatment facility locations, new disposal options, etc. are at stake. The validity of the OPTIMASS approach is confirmed by the close correspondence between its outcome and the results of a decision support system, specifically developed for the municipal wastewater sludge processing chain."
211,included,10.1016/j.compag.2018.09.037,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85054181612,2018-11-01,a decision support tool to enhance agricultural growth in the mékrou river basin (west africa),"We describe in this paper the implementation of E-Water, an open software Decision Support System (DSS), designed to help local managers assess the Water Energy Food Environment (WEFE) nexus. E-Water aims at providing optimal management solutions to enhance food crop production at river basin level. The DSS was applied in the transboundary Mékrou river basin, shared among Benin, Burkina Faso and Niger. The primary sector for local economy in the region is agriculture, contributing significantly to income generation and job creation. Fostering the productivity of regional agricultural requires the intensification of farming practices, promoting additional inputs (mainly nutrient fertilizers and water irrigation) but, also, a more efficient allocation of cropland.
                  In order to cope with the heterogeneity of data, and the analyses and issues required by the WEFE nexus approach, our DSS integrates the following modules: (1) the EPIC biophysical agricultural model; (2) a simplified regression metamodel, linking crop production with external inputs; (3) a linear programming and a multiobjective genetic algorithm optimization routines for finding efficient agricultural strategies; and (4) a user-friendly interface for input/output analysis and visualization.
                  To test the main features of the DSS, we apply it to various real and hypothetical scenarios in the Mékrou river basin. The results obtained show how food unavailability due to insufficient local production could be reduced by, approximately, one third by enhancing the application and optimal distribution of fertilizers and irrigation. That would also affect the total income of the farming sector, eventually doubling it in the best case scenario. Furthermore, the combination of optimal agricultural strategies and modified optimal cropland allocation across the basin would bring additional moderate increases in food self-sufficiency, and more substantial gains in the total agricultural income.
                  The proposed software framework proves to be effective, enabling decision makers to identify efficient and site-specific agronomic management strategies for nutrients and water. Such practices would augment crop productivity, which, in turn, would allow to cope with increasing future food demands, and find a balanced use of natural resources, also taking other economic sectors—like livestock, urban or energy—into account."
212,unknown,10.1109/bigdata47090.2019.9006554,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9006554/,2019-12-12 00:00:00,kryptooracle: a real-time cryptocurrency price prediction platform using twitter sentiments,"Cryptocurrencies, such as Bitcoin, are becoming increasingly popular, having been widely used as an exchange medium in areas such as financial transaction and asset transfer verification. However, there has been a lack of solutions that can support real-time price prediction to cope with high currency volatility, handle massive heterogeneous data volumes, including social media sentiments, while supporting fault tolerance and persistence in real time, and provide real-time adaptation of learning algorithms to cope with new price and sentiment data. In this paper we introduce KryptoOracle, a novel real-time and adaptive cryptocurrency price prediction platform based on Twitter sentiments. The integrative and modular platform is based on (i) a Spark-based architecture which handles the large volume of incoming data in a persistent and fault tolerant way; (ii) an approach that supports sentiment analysis which can respond to large amounts of natural language processing queries in real time; and (iii) a predictive method grounded on online learning in which a model adapts its weights to cope with new prices and sentiments. Besides providing an architectural design, the paper also describes the KryptoOracle platform implementation and experimental evaluation. Overall, the proposed platform can help accelerate decision-making, uncover new opportunities and provide more timely insights based on the available and ever-larger financial data volume and variety."
213,included,10.1109/smartworld.2018.00106,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8560084/,2018-10-12 00:00:00,real-time data processing architecture for multi-robots based on differential federated learning,"The emergency of ubiquitous intelligence in various things has become the ultimate cornerstone in building a smart interconnection of the physical world and the human world, which also caters to the idea of Internet of Things (IoT). Nowadays, robots as a new type of ubiquitous IoT devices have gained much attention. With the increasing number of distributed multi-robots, such smart environment generates unprecedented amounts of data. Robotic applications are faced with challenges of such big data: the serious real-time assurance and data privacy. Therefore, in order to obtain the big data values via knowledge sharing under the premise of ensuring the real-time data processing and data privacy, we propose a real-time data processing architecture for multi-robots based on the differential federated learning, called RT-robots architecture. A global shared model with differential privacy protection is trained on the cloud iteratively and distributed to multiple edge robots in each round, and the robotic tasks are processed locally in real time. Our implementation and experiments demonstrate that our architecture can be applied on multiple robotic recognition tasks, balance the trade-off between the performance and privacy."
214,unknown,10.1109/bigcomp.2019.8679121,IEEE,ieeexplore,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8679121/,2019-03-02 00:00:00,question understanding based on sentence embedding on dialog systems for banking service,"This paper introduce a question understanding system to respond appropriate answers in a dialog system for banking services. The question understanding system provides an automated response service in a specific domain (e.g. banking). This can increase response rate of a customer counseling service, and improve business efficiency and expertise. The question understanding system classify domains, specific categories, and speech acts of questions. Finally, the system analyze meanings and intents of the questions, and searching correct answers even various input sentences. In this paper, we describe methods of keyword tokenizing, pattern recognition, sentence embedding, analyzing dialogue intention, and searching similar FAQs. Through these methods, we have developed the question understanding unit in a real interactive system for financial services for real insurance companies and banks, and analyze the usefulness of the system through practical system implementation examples."
215,unknown,http://arxiv.org/abs/1808.07921v3,arxiv,arxiv,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1808.07921v3,2018-08-23 00:00:00,"soter: a runtime assurance framework for programming safe robotics
  systems","The recent drive towards achieving greater autonomy and intelligence in
robotics has led to high levels of complexity. Autonomous robots increasingly
depend on third party off-the-shelf components and complex machine-learning
techniques. This trend makes it challenging to provide strong design-time
certification of correct operation.
  To address these challenges, we present SOTER, a robotics programming
framework with two key components: (1) a programming language for implementing
and testing high-level reactive robotics software and (2) an integrated runtime
assurance (RTA) system that helps enable the use of uncertified components,
while still providing safety guarantees. SOTER provides language primitives to
declaratively construct a RTA module consisting of an advanced,
high-performance controller (uncertified), a safe, lower-performance controller
(certified), and the desired safety specification. The framework provides a
formal guarantee that a well-formed RTA module always satisfies the safety
specification, without completely sacrificing performance by using higher
performance uncertified components whenever safe. SOTER allows the complex
robotics software stack to be constructed as a composition of RTA modules,
where each uncertified component is protected using a RTA module.
  To demonstrate the efficacy of our framework, we consider a real-world
case-study of building a safe drone surveillance system. Our experiments both
in simulation and on actual drones show that the SOTER-enabled RTA ensures the
safety of the system, including when untrusted third-party components have bugs
or deviate from the desired behavior."
216,unknown,10.1109/ct.1997.617707,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/617707/,1997-08-28 00:00:00,the intelligent room project,"At the MIT Artificial Intelligence Laboratory, we have been working on technologies for an Intelligent Room. Rather than pull people into the virtual world of the computer, we are trying to pull the computer out into the real world of people. To do this, we are combining robotics and vision technology with speech understanding systems and agent-based architectures to provide ready-at-hand computation and information services for people engaged in day-to-day activities, both on their own and in conjunction with others. We have built a layered architecture where, at the bottom level, vision systems track people and identify their activities and gestures, and, through word spotting, decide whether people in the room are talking to each other or to the room itself. At the next level, an agent architecture provides a uniform interface to such specially-built systems, and to other off-the-shelf software, such as Web browsers, etc. At the highest level, we are able to build application systems that provide occupants of the room with specialized services; examples we have built include systems for command-and-control situations rooms and as a room for giving presentations."
217,unknown,http://arxiv.org/abs/1909.08703v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.08703v1,2019-09-18 00:00:00,"deep complex networks for protocol-agnostic radio frequency device
  fingerprinting in the wild","Researchers have demonstrated various techniques for fingerprinting and
identifying devices. Previous approaches have identified devices from their
network traffic or transmitted signals while relying on software or operating
system specific artifacts (e.g., predictability of protocol header fields) or
characteristics of the underlying protocol (e.g.,frequency offset). As these
constraints can be a hindrance in real-world settings, we introduce a
practical, generalizable approach that offers significant operational value for
a variety of scenarios, including as an additional factor of authentication for
preventing impersonation attacks. Our goal is to identify artifacts in
transmitted signals that are caused by a device's unique hardware
""imperfections"" without any knowledge about the nature of the signal. We
develop RF-DCN, a novel Deep Complex-valued Neural Network (DCN) that operates
on raw RF signals and is completely agnostic of the underlying applications and
protocols. We present two DCN variations: (i) Convolutional DCN (CDCN) for
modeling full signals, and (ii) Recurrent DCN (RDCN) for modeling time series.
Our system handles raw I/Q data from open air captures within a given spectrum
window, without knowledge of the modulation scheme or even the carrier
frequencies. While our experiments demonstrate the effectiveness of our system,
especially under challenging conditions where other neural network
architectures break down, we identify additional challenges in signal-based
fingerprinting and provide guidelines for future explorations. Our work lays
the foundation for more research within this vast and challenging space by
establishing fundamental directions for using raw RF I/Q data in novel
complex-valued networks."
218,included,10.1109/iceccme52200.2021.9591113,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9591113/,2021-10-08 00:00:00,cobots for fintech,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space."
219,unknown,10.1109/ains47559.2019.8968698,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8968698/,2019-11-21 00:00:00,cyber security risk assessment on industry 4.0 using ics testbed with ai and cloud,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures."
220,unknown,10.1109/icse43902.2021.00120,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9401972/,2021-05-30 00:00:00,enhancing genetic improvement of software with regression test selection,"Genetic improvement uses artificial intelligence to automatically improve software with respect to non-functional properties (AI for SE). In this paper, we propose the use of existing software engineering best practice to enhance Genetic Improvement (SE for AI). We conjecture that existing Regression Test Selection (RTS) techniques (which have been proven to be efficient and effective) can and should be used as a core component of the GI search process for maximising its effectiveness. To assess our idea, we have carried out a thorough empirical study assessing the use of both dynamic and static RTS techniques with GI to improve seven real-world software programs. The results of our empirical evaluation show that incorporation of RTS within GI significantly speeds up the whole GI process, making it up to 78% faster on our benchmark set, being still able to produce valid software improvements. Our findings are significant in that they can save hours to days of computational time, and can facilitate the uptake of GI in an industrial setting, by significantly reducing the time for the developer to receive feedback from such an automated technique. Therefore, we recommend the use of RTS in future test-based automated software improvement work. Finally, we hope this successful application of SE for AI will encourage other researchers to investigate further applications in this area."
221,unknown,10.1016/j.robot.2008.10.014,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/58549110024,2009-02-28,hrp-2w: a humanoid platform for research on support behavior in daily life environments,"We introduce a concept of a real-world-oriented humanoid robot that can support humans’ activities in daily life. In such environments, robots have to watch humans, understand their behavior, and support their daily life tasks. In particular, these robots must be capable of such real-world behavior as handling tableware and delivering daily commodities by hand. We developed a humanoid robot, HRP-2W, which has an upper body of HRP-2 [K. Kaneko, F. Kanehiro, S. Kajita, H. Hirukawa, T. Kawasaki, M. Hirata, K. Akachi, T. Isozumi, Humanoid Robot HRP-2, in: Proceedings of the 2004 IEEE International Conference on Robotics & Automation, 2004, pp. 1083–1090] and a wheel module instead of legs, as a research platform to fulfill this aim. We also developed basic software configuration in order to integrate our platform with other research groups. Through experiments, we demonstrated the feasibility of the humanoid robot platform and the potential of the software architecture."
222,unknown,10.1016/j.procs.2020.09.269,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85093364102,2020-01-01,"alexa, what classes do i have today? the use of artificial intelligence via smart speakers in education","Looking back to the rumours from the early 2000’s, when the world of technology bloomed together with the curiosity towards what was next to come, by 2020, robots should have assisted and supported almost every task from our daily life. While this may seem as a Sci-Fi movie scenario, it is partially a tangible reality, that we quickly got used to, thanks to the introduction of smart speakers.
                  As the world changes, so does the future of our students. In this respects, the evolution of the technology comes up with specific environments for educational purpose. Building smart learning environments supported by e-learning platforms is an important area of research in education domain within our days. The evolution of these smart learning environments is justified by some events (Covid19) that force students to learn remotely.
                  The paper proposes a software application component using Alexa smart speaker, that integrates different services (Amazon Web Services, Microsoft Services) for a proper virtual environment platform, for both students and teachers. It addresses the main concerns of the current educational system, and provides a smart solution through the use of Artificial Intelligence based tools. The proposed approach not only achieves unifying data and knowledge-share mechanisms in a remotely mode, but it brings also a good learning experience, increasing the effectiveness and the efficiency of the learning process."
223,unknown,10.23919/cycon49761.2020.9131724,2020 12th International Conference on Cyber Conflict (CyCon),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/abbd4715971471fb025bd063e8f97a8568a5b80c,2020-01-01 00:00:00,hacking the ai - the next generation of hijacked systems,"Within the next decade, the need for automation, intelligent data handling and pre-processing is expected to increase in order to cope with the vast amount of information generated by a heavily connected and digitalised world. Over the past decades, modern computer networks, infrastructures and digital devices have grown in both complexity and interconnectivity. Cyber security personnel protecting these assets have been confronted with increasing attack surfaces and advancing attack patterns. In order to manage this, cyber defence methods began to rely on automation and (artificial) intelligence supporting the work of humans. However, machine learning (ML) and artificial intelligence (AI) supported methods have not only been integrated in network monitoring and endpoint security products but are almost omnipresent in any application involving constant monitoring, complex or large volumes of data. Intelligent IDS, automated cyber defence, network monitoring and surveillance as well as secure software development and orchestration are all examples of assets that are reliant on ML and automation. These applications are of considerable interest to malicious actors due to their importance to society. Furthermore, ML and AI methods are also used in audio-visual systems utilised by digital assistants, autonomous vehicles, face-recognition applications and many others. Successful attack vectors targeting the AI of audio-visual systems have already been reported. These attacks range from requiring little technical knowledge to complex attacks hijacking the underlying AI.With the increasing dependence of society on ML and AI, we must prepare for the next generation of cyber attacks being directed against these areas. Attacking a system through its learning and automation methods allows attackers to severely damage the system, while at the same time allowing them to operate covertly. The combination of being inherently hidden through the manipulation made, its devastating impact and the wide unawareness of AI and ML vulnerabilities make attack vectors against AI and ML highly favourable for malicious operators. Furthermore, AI systems tend to be difficult to analyse post-incident as well as to monitor during operations. Discriminating a compromised from an uncompromised AI in real-time is still considered difficult.In this paper, we report on the state of the art of attack patterns directed against AI and ML methods. We derive and discuss the attack surface of prominent learning mechanisms utilised in AI systems. We conclude with an analysis of the implications of AI and ML attacks for the next decade of cyber conflicts as well as mitigations strategies and their limitations."
224,included,10.1007/s42979-021-00726-1,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s42979-021-00726-1,2021-06-19 00:00:00,towards regulatory-compliant mlops: oravizio’s journey from a machine learning experiment to a deployed certified medical product,"Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context."
225,unknown,10.1038/s42256-021-00337-8,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s42256-021-00337-8,2021-06-01 00:00:00,end-to-end privacy preserving deep learning on multi-institutional medical imaging,"Using large, multi-national datasets for high-performance medical imaging AI systems requires innovation in privacy-preserving machine learning so models can train on sensitive data without requiring data transfer. Here we present PriMIA (Privacy-preserving Medical Image Analysis), a free, open-source software framework for differentially private, securely aggregated federated learning and encrypted inference on medical imaging data. We test PriMIA using a real-life case study in which an expert-level deep convolutional neural network classifies paediatric chest X-rays; the resulting model’s classification performance is on par with locally, non-securely trained models. We theoretically and empirically evaluate our framework’s performance and privacy guarantees, and demonstrate that the protections provided prevent the reconstruction of usable data by a gradient-based model inversion attack. Finally, we successfully employ the trained model in an end-to-end encrypted remote inference scenario using secure multi-party computation to prevent the disclosure of the data and the model. Gaining access to medical data to train AI applications can present problems due to patient privacy or proprietary interests. A way forward can be privacy-preserving federated learning schemes. Kaissis, Ziller and colleagues demonstrate here their open source framework for privacy-preserving medical image analysis in a remote inference scenario."
226,unknown,10.1109/fads.2017.8253198,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8253198/,2017-10-25 00:00:00,the potential of conversational agents to provide a rapid hiv counseling and testing services,"In low-and middle-income countries where demand for health services outstrips the available supply of skilled labor, advances in information and communication technologies have already been shown to hold promise. While much of the mHealth literature continues to explore mature technologies such as text message and web portals, continual advancement in machine learning opens innovative new areas of exploration for public health practitioners. This paper explores one such possibility, a conversational agent, able to guide users through an HIV counseling and testing session. Using commercially available software (http://api.ai), an agent was designed and built according to the Center for Disease Control's guidelines for the provision of HIV counseling and testing in a non-clinical setting. The agent was linked to the Telegram chat client (http://telegram.org) and 10 testers were invited to participate in a simulated HIV counseling interaction. Six testers found that talking to the agent felt natural, and equivalent to chatting to a human. Seven said they would feel comfortable taking a real HIV test with the agent. Key concerns with the current agent were the use of overly formal language, the speed at which the agent responded (too fast) and the agent either misunderstanding or not understanding the tester. Positive sentiment towards the agent included the fact that testers felt like the session was more private and anonymous, and avoided the need for them to visit a public health facility and stand in a long queue to get tested."
227,unknown,10.1016/j.cmpb.2019.105277,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076945151,2020-02-01,an extensible software platform for interdisciplinary cardiovascular imaging research,"Background and objective
                  Cardiovascular imaging is an exponentially growing field with aspects ranging from image acquisition and analysis to disease characterization, and evaluation of therapy approaches.The transfer of innovative new technological and algorithmic solutions into clinical practice is still slow. In addition to the verification of solutions, their integration in the clinical processing workflow must be enabled for the assessment of clinical impact and risks. The goal of our software platform for cardiac image processing – CAIPI – is to support researchers from different specialties such as imaging physics, computer science, and medicine by a common extensible platform to address typical challenges and hurdles in interdisciplinary cardiovascular imaging research. It provides an integrated solution for method comparison, integrated analysis, and validation in the clinical context. The interface concept enables a combination with existing frameworks that address specific aspects of the pipeline, such as modeling (e.g., OpenCMISS, CARP) or image reconstruction (Gadgetron).
               
                  Methods
                  In our platform, we developed a concept for import, integration, and management of cardiac image data. The integration approach considers the spatiotemporal properties of the beating heart through a specific data model. The solution is based on MeVisLab and provides functionalities for data retrieval and storage. Two types of plugins can be added. While ToolPlugins usually provide processing algorithms such as image correction and segmentation, AnalysisPlugins enable interactive data exploration and reporting. GUI integration concepts are presented for both plugin types. We developed domain-specific reporting and visualization tools (e.g., AHA segment model) to enable validation studies by clinical experts. The platform offers plugins for calculating and reporting quantitative parameters such as cardiac function, which can be used to, e.g., evaluate the effect of processing algorithms on clinical parameters. Export functionalities include quantitative measurements to Excel, image data to PACS, and STL models to modeling and simulation tools.
               
                  Results
                  To demonstrate the applicability of this concept both for method development and clinical application, we present use cases representing different problems along the innovation chain in cardiac MR imaging.
                  Validation of an image reconstruction method (MRI T1 mapping)
                  Validation of an image correction method for real-time 2D-PC MRI
                  Comparison of quantification methods for blood flow analysis
                  Training and integration of machine learning solutions with expert annotations
                  Clinical studies with new imaging techniques (flow measurements in the carotid arteries and peripheral veins as well as cerebral spinal fluid).
               
                  Conclusion
                  The presented platform can be used in interdisciplinary teams, in which engineers or data scientists perform the method validation, followed by clinical research studies in patient collectives. The demonstrated use cases show how it enables the transfer of innovations through validation in the cardiovascular application context."
228,unknown,10.1016/j.procs.2016.09.052,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84992317444,2016-01-01,designing and testing healthtracker for activity recognition and energy expenditure estimation within the daphne platform,"This paper describes the design and evaluation of a mobile software library, HealthTracker, which aims to produce activity and energy expenditure estimations in real-time from accelerometer and gyroscope data provided by wearable sensors. Using feature extraction together with a classifier trained using machine learning, the system will automatically and periodically send all the produced estimations to a cloud-based platform that will allow later evaluation by both the user and a physician or caretaker. The system is presented within the DAPHNE platform, an ICT ecosystem designed to provide a means for remote health and lifestyle monitoring and guidance between physicians and their patients."
229,unknown,http://arxiv.org/abs/2003.06700v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.06700v3,2020-03-14 00:00:00,"cocopie: making mobile ai sweet as pie --compression-compilation
  co-design goes a long way","Assuming hardware is the major constraint for enabling real-time mobile
intelligence, the industry has mainly dedicated their efforts to developing
specialized hardware accelerators for machine learning and inference. This
article challenges the assumption. By drawing on a recent real-time AI
optimization framework CoCoPIE, it maintains that with effective
compression-compiler co-design, it is possible to enable real-time artificial
intelligence on mainstream end devices without special hardware. CoCoPIE is a
software framework that holds numerous records on mobile AI: the first
framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,
language models, and so on; the fastest DNN pruning and acceleration framework,
up to 180X faster compared with current DNN pruning on other frameworks such as
TensorFlow-Lite; making many representative AI applications able to run in
real-time on off-the-shelf mobile devices that have been previously regarded
possible only with special hardware support; making off-the-shelf mobile
devices outperform a number of representative ASIC and FPGA solutions in terms
of energy efficiency and/or performance."
230,unknown,10.1109/bigdata.2016.7840859,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7840859/,2016-12-08 00:00:00,building a research data science platform from industrial machines,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions."
231,unknown,10.1109/icecs.2010.5724589,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5724589/,2010-12-15 00:00:00,estimating design quality of digital systems via machine learning,"Although the term design quality of digital systems can be assessed from many aspects, the distribution and density of bugs are two decisive factors. This paper presents the application of machine learning techniques to model the relationship between specified metrics of high-level design and its associated bug information. By employing the project repository (i.e., high level design and bug repository), the resultant models can be used to estimate the quality of associated designs, which is very beneficial for design, verification and even maintenance processes of digital systems. A real industrial microprocessor is employed to validate our approach. We hope that our work can shed some light on the application of software techniques to help improve the reliability of various digital designs."
232,unknown,10.1016/j.procs.2021.01.245,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105671943,2021-01-01,water wise - a digital water solution for smart cities and water management entities,"Efficient water management of the urban water cycle is one of the current concerns with the increase of (peri) urban areas due to the population growth, economic development, and possibility of water scarcity due to climate change. To face the increase of the water demand it is imperative the creation of digital water solutions to provide a real-time monitoring, decision support system, to manage the water supply network efficiently and optimize the water-energy nexus. This paper presents a Water Wise System – W2S, results from a R&D project supported by an EU and Portuguese Government Grant. The paper provides a preliminary study of an architecture solution to Water Wise System software, focuses on the water challenges, present technology, digital water, IoT and the future of smart cities. The solution aims to support a paradigm shift in the management of water distribution networks, with predictive and analytical convergence supported in Machine Learning, Deep Learning and an integration with SCADA, GIS and EPANET."
233,unknown,10.1109/fie44824.2020.9273981,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9273981/,2020-10-24 00:00:00,machine learning for middle-schoolers: children as designers of machine-learning apps,"This Research to Innovative Practice Full Paper presents a multidisciplinary, design-based research study that aims to develop and study pedagogical models and tools for integrating machine-learning (ML) topics into education. Although children grow up with ML systems, few theoretical or empirical studies have focused on investigating ML and data-driven design in K-12 education to date. This paper presents the theoretical grounds for a design-oriented pedagogy and the results from exploring and implementing those theoretical ideas in practice through a case study conducted in Finland. We describe the overall process in which middle-schoolers (N = 34) co-designed and made ML applications for solving meaningful, everyday problems. The qualitative content analysis of the pre-and post-tests, student interviews, and the students' own ML design ideas indicated that co-designing real-life applications lowered the barriers for participating in some of the core practices of computer science. It also supported children in exploring abstract ML concepts and workflows in a highly personalized and embodied way. The article concludes with a discussion on pedagogical insights for supporting middle-schoolers in becoming innovators and software designers in the age of ML."
234,unknown,10.1109/wrc-sara.2019.8931920,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8931920/,2019-08-22 00:00:00,software-defined cloud manufacturing in the context of industry 4.0,"In the practice of &#x201C;Cloud Manufacturing (CMfg)&#x201D; or &#x201C;Industrial Internet&#x201D;, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &#x0026; upgrading."
235,unknown,10.1016/j.robot.2021.103830,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85109177424,2021-09-01,visual recognition of gymnastic exercise sequences. application to supervision and robot learning by demonstration,"This work presents a novel software architecture to autonomously identify and evaluate the gymnastic activity that people are carrying out. It is composed of three different interconnected layers. The first corresponds to a Multilayer Perceptron (MLP) trained from a set of angular magnitudes derived from the information provided by the OpenPose library. This library works frame by frame, so some postures may be incorrectly detected due to eventual occlusions. The MLP layer makes it possible to accurately identify the posture a person is performing. A second layer, based on a Hidden Markov Model (HMM) and the Viterbi algorithm, filters the incorrect spurious postures. Thus, the accuracy of the algorithm is improved, leading to a precise sequence of postures. A third layer identifies the current exercise and evaluates whether the person is doing it at a correct speed. This layer uses an innovative Modified Levenshtein Distance (MLD), which considers not only the number of operations to transform a given sequence, but also the nature of the elements participating in the comparison. The system works in real time with little delay, thus recognizing sequences of arbitrary length and providing continuous feedback on the exercises being performed. An experiment carried out consisted in reproducing the output of the second layer on an autonomous Pepper robot that can be used in environments where physical exercise is performed, such as a residence for the elderly or others. It has reproduced different exercises previously executed by an instructor so that people can copy the robot. The article analyzes the current situation of the automated gymnastic activities recognition, presents the architecture, the different experiments carried out and the results obtained. The integration of the three components (MLP, HMM and MLD) results in a robust system that has allowed us to improve the results of previous works."
236,unknown,10.1109/compsac48688.2020.0-168,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9202848/,2020-07-17 00:00:00,an early warning system for hemodialysis complications utilizing transfer learning from hd iot dataset,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed."
237,unknown,10.1016/j.jnca.2017.09.001,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85032219805,2018-02-01,an intelligent power distribution service architecture using cloud computing and deep learning techniques,"Smart management of power consumption for green living is important for sustainable development. Existing approaches could not provide a complete solution for both smart monitoring of electricity consumption, and also intelligent processing of the collected data effectively. This paper presents a cloud-based intelligent power distribution service architecture, where an intelligent electricity box (IEB) is designed using Zigbee and Raspberry Pi, and a standard MQTT (Message Queuing Telemetry Transport) protocol is used to transfer monitored data to the backend Cloud computing infrastructure using open source software packages. The IEB provides cloud services of real-time electricity information checking, power consumption monitoring, and remote control of switches. The current and historical data are stored in HBase and analyzed using Long Short Term Memory (LSTM). Evaluations and practical usage show that our proposed solution is very efficient in terms of availability, performance, and the deep learning based approach has better prediction accuracy than that of both classical SVR based approach and the latest XGBoost approach."
238,unknown,10.1109/phm-paris.2019.00052,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8756426/,2019-05-05 00:00:00,a common service middleware for intelligent complex software system,"With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technology, various intelligent complex software systems (i-CSS) are increasingly popular, becoming one of the most important software system development paradigms. Its inherent growth construction and adaptive evolution properties pose new challenges to existing software design and development methods. Especially, how to achieve growth construction by quickly reusing existing excellent software resources, and how to establish data flow across system boundaries around the business flow to achieve adaptive evolution based on data intelligence. Facing the above challenges, this paper proposes novel data-oriented analysis and design method (DOAD), microservice and container-based mashup development method (SCMD). On this basis, the paper implements i-CSS common service middleware to support the above methods in engineering. In a real cloud-based PHM system and the other three industry projects, the proposed methods and middleware are used for application verification, the results show that they can greatly reduce the complexity of i-CSS design and development, reduce the ability threshold of the i-CSS development team, improve the development efficiency of the development team, reduce the team development workload by 31.5% on average, and help the i-CSS team effectively cope with the challenges of growth construction and adaptive evolution."
239,unknown,http://arxiv.org/abs/2104.04076v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2104.04076v1,2021-04-01 00:00:00,"an artificial intelligence and internet of things based automated
  irrigation system","It is not hard to see that the need for clean water is growing by considering
the decrease of the water sources day by day in the world. Potable fresh water
is also used for irrigation, so it should be planned to decrease freshwater
wastage. With the development of technology and the availability of cheaper and
more effective solutions, the efficiency of irrigation increased and the water
loss can be reduced. In particular, Internet of things (IoT) devices has begun
to be used in all areas. We can easily and precisely collect temperature,
humidity and mineral values from the irrigation field with the IoT devices and
sensors. Most of the operations and decisions about irrigation are carried out
by people. For people, it is hard to have all the real-time data such as
temperature, moisture and mineral levels in the decision-making process and
make decisions by considering them. People usually make decisions with their
experience. In this study, a wide range of information from the irrigation
field was obtained by using IoT devices and sensors. Data collected from IoT
devices and sensors sent via communication channels and stored on MongoDB. With
the help of Weka software, the data was normalized and the normalized data was
used as a learning set. As a result of the examinations, a decision tree (J48)
algorithm with the highest accuracy was chosen and an artificial intelligence
model was created. Decisions are used to manage operations such as starting,
maintaining and stopping the irrigation. The accuracy of the decisions was
evaluated and the irrigation system was tested with the results. There are
options to manage, view the system remotely and manually and also see the
system s decisions with the created mobile application."
240,unknown,10.1109/3ict.2019.8910276,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8910276/,2019-09-23 00:00:00,ground operations management using a data governance dashboard,"An incident involving the use of chemical, biological, radiological, and nuclear (CBRN) materials might represent a significant challenge for crime scene investigators. The paper presents a full system architecture to assess the hazardous situations resulting from CBRN materials. This issue is crucial since there were a number of incidents that occurred in which forensics people could not reach the location either due to being unreachable or due to harmful emissions. The proposed solution integrates various inputs including data from sensors, video streaming, geo-data along with using Artificial Intelligence (AI) for good decision-making and data analysis. A geo-dashboard was also designed to demonstrate, in real-time, the collected data from several angles and according to various queries. It also monitors the performance in real-time. The topic is not new however the novelty of the proposed solution is the integration of multiple sources of data, applying deep neural nets and projecting the data and data analytics in real-time on a dashboard that displays the analysis and data from different perspectives considering the viewpoint of the individuals who will use that system. The paper also presents how the ROCSAFE multidisciplinary research project addresses the identified scenario. The project combines topics from robotics, sensor technology, analytical and situation awareness software, transforming data into knowledgeable insights to support the decision-making process."
241,unknown,10.1016/0950-7051(94)90024-8,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/38149147114,1994-01-01,"multi-paradigm software environment for the real-time processing of sound, music and multimedia","The paper introduces a system and a software architecture for the representation and real-time processing of sound, music, and multimedia based on artificial intelligence techniques. This system, called WinProcne/HARP, is able to represent objects in a two-fold formalism—symbolic and analogical—at different levels of abstraction, and to carry out plans according to the user's goals. It also provides both formal and informal analysis capabilities for extracting information. In WinProcne/HARP the user can build, update, browse, and merge various knowledge bases of sound, music, and multimedia material, as well as enter queries, start and manage real time performance, using a high-level graphical user interface. The system is currently used by researchers and composers in various experiments, including (a) advanced robotics projects, in which the system is used as a tool for interacting, cotrolling and simulating robot movements, and (b) theatrical automation, where the system is delegated to manage and integrate sound, music, and three-dimensional computer animations of humanoid figures. The paper explicitly refers to some applications in the music field."
242,unknown,10.1109/smartnets48225.2019.9069763,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9069763/,2019-12-19 00:00:00,management of smart water treatment plant using iot cloud services,"Water Treatment Plant (WTP) is an important infrastructure to ensure human health and the environment. In its development, aspects of environmental safety and health are of great importance. Smart WTP is a water station that is managed using software-based tools such as data analytics, visualization, and predictive analytics. WTP smart management system is developed to manage Big Data information flows from many sensors and smart devices that allow for real-time responses and connectivity to Internet of Things (IoT) Cloud platforms services. The performance of the Smart WTP operations should be consistently evaluated to ensure that the plant is operating efficiently, thus minimizing energy costs and improving water purity and quality conservation parameters. Our proposed solution is based on sensors monitoring and Big Data analysis of Smart Water Treatment Plant (SWTP) using IoT hardware devices that have an internet connection to an IoT Cloud platform. The Cloud platform such as Thing Speak has the capability to analyze, visualize and react based on the Big Data analytics to send risk alarms and operate risk management plans to overcome the failure scenarios and minimize the downtime operation of the Smart WTP."
243,unknown,10.1109/icoten52080.2021.9493478,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9493478/,2021-07-05 00:00:00,iot-based cardiac healthcare system for ubiquitous healthcare service,"The working of smart cities is enhanced by the application of IoT devices, software devices, User Interface (UI) and communication systems. Traditional Healthcare System is criticized for some inbuilt flaws like, long term wait time, being expensive, manual payment process, covering the long distance to reach the medical facility and visiting doctors regularly. Smart healthcare systems have all that it requires to replace the traditional healthcare system, and the Internet of Things (IoT) is considered as the ultimate solution for inexpensive round the clock monitoring of patients. In this paper, we have proposed an IoT-based Cardiac Healthcare System for ubiquitous access. The proposed Smart Cardiac Care System promises to give an affordable and accurate solution with real-time observations, with complete privacy of patients and minimum physical examinations by the professionals to Cardiac Units. Multiple physical signs required for a cardiac patient are designed to be sampled at different rates continuously. The hybrid combination of multiple parameters along with Electrocardiographic (ECG) analysis adds to the uniqueness of this model. This combination has never been used before. The System is also capable of generating alerts and warnings for abnormal values. Patient&#x2019;s record on cloud server helps to ensure ubiquitous access."
244,unknown,10.1016/j.smhl.2018.07.008,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85059789843,2018-12-01,are you smoking? automatic alert system helping people keep away from cigarettes,"Tobacco smoking is responsible for one out of every five deaths in the US, according to the Centers for Disease Control and Prevention (CDC). Recent advances in treatment delivery include technology-based mobile health approaches, which seek to deliver real-time feedback to smokers to aid quit attempts and mitigate lapses. With regard to the measurement of smoking, clinical trials rely on participant self-report and/or biochemical verification of smoking status to evaluate outcomes. Wearable sensors have the potential to improve current approaches by providing personalized feedback and objective verification of smoking status (Burns, 2000). In this paper, we describe the development of a novel smoking cessation system that combines motion detection and an Android software application to monitor smoking in real-time. In this system, a personalized smoking cessation plan will be created based on the goal of complete cessation or smoking reduction. Once the plan is created, the mobile system will monitor the users׳ smoking activity and provide feedback. An LSTM algorithm has been computed to train and test the motion data, which was collected from two armbands, to detect smoking and non-smoking motions. The internet message service will be used to remind users to stick to their plan when the sensor detects current smoking. Related video links are pushed and pulled to the users via Short Message Service (SMS) to support smoking cessation. Findings have implications for tobacco cessation treatment delivery and assessment of smoking status."
245,unknown,10.1109/iciss.2010.5656975,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5656975/,2010-10-24 00:00:00,research and implementation of the temperature control system of heat treatment based on .net and rs-485 bus,"RS-485 is a widely used industrial field bus. The successful application of AIBUS protocol in AI series display control instrument, make the AIDCS system's cost significantly lower than traditional DCS system. The paper successfully developed a prototype system based on RS-485 bus for high precision temperature control system of heat treatment, and based on the .net and AIBUS protocol, developed it' s system management software. This system have the characteristics of good real-time, high control-precision, high degree of automation, and friendly human-machine interface."
