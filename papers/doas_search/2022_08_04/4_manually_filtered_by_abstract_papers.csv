id,status,doi,publisher,database,query_name,query_value,url,publication_date,title,abstract
1,included,10.1109/sysose.2017.7994953,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7994953/,6/21/2017 0:00,autonomous decision making for a driver-less car,"Autonomous driving has been a hot topic with companies like Google, Uber, and Tesla because of the complexity of the problem, seemingly endless applications, and capital gain. The technology's brain child is DARPA's autonomous urban challenge from over a decade ago. Few companies have had some success in applying algorithms to commercial cars. These algorithms range from classical control approaches to Deep Learning. In this paper, we will use Deep Learning techniques and the Tensor flow framework with the goal of navigating a driverless car through an urban environment. The novelty in this system is the use of Deep Learning vs. traditional methods of real-time autonomous operation as well as the application of the Tensorflow framework itself. This paper provides an implementation of AlexNet's Deep Learning model for identifying driving indicators, how to implement them in a real system, and any unforeseen drawbacks to these techniques and how these are minimized and overcome."
2,excluded,10.1109/ictc.2017.8190968,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8190968/,10/20/2017 0:00,unmanned aerial vehicle surveillance system (uavss) for forest surveillance and data acquisition,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture."
3,included,10.1109/isaect50560.2020.9523700,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9523700/,11/27/2020 0:00,edge-cloud architectures using uavs dedicated to industrial iot monitoring and control applications,"The deployment of new technologies to ease the control and management of a massive data volume and its uncertainty is a very significant challenge in the industry. Under the name ""Smart Factory"", the Industrial Internet of Things (IoT) aims to send data from systems that monitor and control the physical world to data processing systems for which cloud computing has proven to be an important tool to meet processing needs. unmanned aerial vehicles (UAVs) are now being introduced as part of IIoT and can perform important tasks. UAVs are now considered one of the best remote sensing techniques for collecting data over large areas. In the field of fog and edge computing, the IoT gateway connects various objects and sensors to the Internet. It function as a common interface for different networks and support different communication protocols. Edge intelligence is expected to replace Deep Learning (DL) computing in the cloud, providing a variety of distributed, low-latency and reliable intelligent services. In this paper, An unmanned aerial vehicle is automatically integrated into an industrial control system through an IoT gateway platform. Rather than sending photos from the UAV to the cloud for processing, an AI cloud trained model is deployed in the IoT gateway and used to process the taken photos. This model is designed to overcome the latency channels of the cloud computing architecture. The results show that the monitoring and tracking process using advanced computing in the IoT gateway is significantly faster than in the cloud."
4,unknown,10.1109/isc246665.2019.9071705,2019 IEEE International Smart Cities Conference (ISC2),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1c2a98e580f7c222e052fd623cb6f8ceed49e110,1/1/2019 0:00,on the use of machine learning for state-of- charge forecasting in electric vehicles,"nowadays, it is well known that a main solution for pollution reduction in cities will be the introduction of electric and hybrid vehicles on transportation roads. Many research efforts have been dedicated to develop new technologies to further promote the use of this type of vehicles. However, their penetration on transportation roads faces some obstacles that have not yet been fully tackled. For instance, the development of intelligent battery management systems needs to be further investigated taking into consideration the uncertainty linked to how vehicles will perform in different scenarios, such as traffic situation, driver behavior, and road profile. The work presented in this study is towards developing a battery management system by investigating new approaches for accurate estimation and prediction of remaining charge, the expected lifetime of the batteries and the remaining driving rang. We focus mainly on the integration of predictive analytics techniques for forecasting the state-of-charge. We first deployed statistical- and machine learning based techniques in real-sitting scenarios (LSTM, ARIMA and XGBoost). Experiments have been conducted using an electric vehicle platform and results are reported to shed more light on their accuracy for multiple-horizon forecasts of battery’s state of charge."
5,excluded,10.1109/ispass48437.2020.00019,2020 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d7cc8603b7bdb2f8cb2c86778da3c803e308d7f9,1/1/2020 0:00,clan: continuous learning using asynchronous neuroevolution on commodity edge devices,"Recent advancements in machine learning algorithms, especially the development of Deep Neural Networks (DNNs) have transformed the landscape of Artificial Intelligence (AI). With every passing day, deep learning based methods are applied to solve new problems with exceptional results. The portal to the real world is the edge. The true impact of AI can only be fully realized if we can have AI agents continuously interacting with the real world and solving everyday problems. Unfortunately, high compute and memory requirements of DNNs acts a huge barrier towards this vision. Today we circumvent this problem by deploying special purpose inference hardware on the edge while procuring trained models from the cloud. This approach, however, relies on constant interaction with the cloud for transmitting all the data, training on massive GPU clusters, and downloading updated models. This is challenging for bandwidth, privacy, and constant connectivity concerns that autonomous agents may exhibit. In this paper we evaluate techniques for enabling adaptive intelligence on edge devices with zero interaction with any high-end cloud/server. We build a prototype distributed system of Raspberry Pis communicating via WiFi running NeuroEvolutionary (NE) learning and inference. We evaluate the performance of such a collaborative system and detail the compute/communication characteristics of different arrangements of the system that trade-off parallelism versus communication. Using insights from our analysis, we also propose algorithmic modifications to reduce communication by up to 3.6x during the learning phase to enhance scalability even further and match performance of higher end computing devices at scale. We believe that these insights will enable algorithm-hardware co-design efforts for enabling continuous learning on the edge."
6,included,10.1109/icccn52240.2021.9522281,2021 International Conference on Computer Communications and Networks (ICCCN),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e5db631b9237de584eadab60dd6529470438ad5d,1/1/2021 0:00,realization of an intrusion detection use-case in onap with acumos,"With Software-Defined Networking and Machine Learning/Artificial Intelligence (ML/AI) reaching new paradigms in their corresponding fields, both academia and industry have exhibited interests in discovering unique aspects of intelligent and autonomous communication networks. Transforming such intentions and interests to reality involves software development and deployment, which has its own story of significant evolution. There has been a notable shift in the strategies and approaches to software development. Today, the divergence of tools and technologies as per demand is so substantial that adapting a software application from one environment to another could involve tedious redesign and redevelopment. This implies enormous effort in migrating existing applications and research works to a modern industrial setup. Additionally, the struggles with sustainability maintenance of such applications could be painful. Concerning ML/AI, the capabilities to train, deploy, retrain, and re-deploy AI models as quickly as possible will be crucial for AI-driven network systems. An end-to-end workflow using unified open-source frameworks is the need of the hour to facilitate the integration of ML/AI models into the modern software-driven virtualized communication networks. Hence, in our paper, we present such a prototype by demonstrating the journey of a sample SVM classifier from being a python script to be deployed as a micro-service using ONAP and Acumos. While illustrating various features of Acumos and ONAP, this paper intends to make readers familiar with an end-to-end workflow taking advantage of the integration of both open-source platforms."
7,excluded,10.23919/cycon49761.2020.9131724,2020 12th International Conference on Cyber Conflict (CyCon),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/abbd4715971471fb025bd063e8f97a8568a5b80c,1/1/2020 0:00,hacking the ai - the next generation of hijacked systems,"Within the next decade, the need for automation, intelligent data handling and pre-processing is expected to increase in order to cope with the vast amount of information generated by a heavily connected and digitalised world. Over the past decades, modern computer networks, infrastructures and digital devices have grown in both complexity and interconnectivity. Cyber security personnel protecting these assets have been confronted with increasing attack surfaces and advancing attack patterns. In order to manage this, cyber defence methods began to rely on automation and (artificial) intelligence supporting the work of humans. However, machine learning (ML) and artificial intelligence (AI) supported methods have not only been integrated in network monitoring and endpoint security products but are almost omnipresent in any application involving constant monitoring, complex or large volumes of data. Intelligent IDS, automated cyber defence, network monitoring and surveillance as well as secure software development and orchestration are all examples of assets that are reliant on ML and automation. These applications are of considerable interest to malicious actors due to their importance to society. Furthermore, ML and AI methods are also used in audio-visual systems utilised by digital assistants, autonomous vehicles, face-recognition applications and many others. Successful attack vectors targeting the AI of audio-visual systems have already been reported. These attacks range from requiring little technical knowledge to complex attacks hijacking the underlying AI.With the increasing dependence of society on ML and AI, we must prepare for the next generation of cyber attacks being directed against these areas. Attacking a system through its learning and automation methods allows attackers to severely damage the system, while at the same time allowing them to operate covertly. The combination of being inherently hidden through the manipulation made, its devastating impact and the wide unawareness of AI and ML vulnerabilities make attack vectors against AI and ML highly favourable for malicious operators. Furthermore, AI systems tend to be difficult to analyse post-incident as well as to monitor during operations. Discriminating a compromised from an uncompromised AI in real-time is still considered difficult.In this paper, we report on the state of the art of attack patterns directed against AI and ML methods. We derive and discuss the attack surface of prominent learning mechanisms utilised in AI systems. We conclude with an analysis of the implications of AI and ML attacks for the next decade of cyber conflicts as well as mitigations strategies and their limitations."
8,excluded,10.4043/29335-ms,"Day 2 Tue, May 07, 2019",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/095aeceba84ba188a2b3bb6e086d3755f11e824a,1/1/2019 0:00,"using augmented intelligence to automate subsea inspection data acquisition, processing, analysis, reporting and access","
 Augmented Intelligence (AI2) involves fusing Analyst Intuition with Artificial Intelligence to deliver an optimised combination of human-machine decision support.
 AI2 is being incorporated by i-Tech Services / Leidos into the physical inspection of offshore Oil, Gas, and Renewables assets, delivering valuable data driven insights that contribute to greater efficiency, enhanced condition monitoring, improved asset integrity and asset life extension.
 The deployment of vehicular and diver assets to obtain such inspection data, with associated support vessels, remains a major cost challenge for Operators.
 We believe the industry needs to approach this challenge from two key directions. Firstly, through the application of autonomous systems for data acquisition and delivery, reducing vessel reliance, and secondly through automating the acquisition and processing of data and maximising the insight provided by the data.
 This paper will examine the use of Augmented Intelligence to optimise the Subsea Inspection data workflow as a key use case, to demonstrate the principles.
 The historic paradigm consists of a fragmented evolving approach, with insufficient consideration and design across all the sensors, processing analytical engines and data visualisation. The approach being adopted is to closely link all aspects of the data workflow, within the context of delivering the data and beyond in terms of harvesting additional insight and value.
 To achieve the optimum workflow a number of developmental initiatives are being knitted into a modular platform, each element providing standalone value but the sum of the parts generates the most significant value and cost reduction.
 The elements being combined are automatic data quality control at acquisition source and through the full workflow, automated processing, machine vision for object recognition and reporting and machine learning to optimise the system intelligence. All of these are designed to augment the expertise of the analyst / user, detecting change to learnt parameters, by using real time data and critically by referencing large historical data sets and as-built data.
 The outputs from a system holistic approach will be improved data acquisition with more efficient high quality right first time data reporting. In addition layers of analytics, with smart, intuitive data access and retrieval will optimise delivery of key information within large data sets, together with maximising value and insight."
9,unknown,10.1109/wispnet.2017.8300034,"2017 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/5d449d17d0babccd02f8557b09622616b9b7af9a,1/1/2017 0:00,a reliable and secure approach for efficient car-to-car communication in intelligent transportation systems,"Intelligent Transportation System (ITS) is considered to be an integral part of the Smart City concept. As per the idea of ITS, intelligence incorporated in existing vehicles, both private and public, is expected to make vehicle management inexpensive and less complex. Some of the barriers in successful deployment of ITS have been its reliability, scalability, interoperability and security. Communication plays a big role in ITS and Wi-Fi/ cellular technologies have been used to facilitate Car-to-Server (C2S) and Server-to-Server (S2S) type communication. Car-to-Car (C2C) communication emerges as a more resource constrained and challenging model. Due to strict real-time, low delay and high security requirements, C2C presents several avenues for research. In this work, we investigate the network and security concerns of C2C architecture of ITS. Based on our study, we propose an effective approach for C2C communication using event-triggered broadcast of information. We use Public Key Infrastructure (PKI) based sender authentication for information verification. Furthermore, we provide Machine Learning based solutions to some common problems encountered during C2C communication."
10,excluded,10.1109/icodt255437.2022.9787424,2022 2nd International Conference on Digital Futures and Transformative Technologies (ICoDT2),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/0cd4639954db4c8bde3d28f8fa99f8804516fc36,1/1/2022 0:00,"machine learning based theoretical framework for failure prediction, detection, and correction of mission-critical flight software","Mission-critical flight software acts as the control mechanism for autonomous flights and lies at the heart of next-generation developments in the aviation industry. Most state-of-the-art technological evolution is realized through the use of contemporary software which implements the essentially required, novel, innovative, and featuring value additions. Real-time physical exposure and the data-driven flying nature of aerial vehicles make them vulnerable to an ever-evolving new threat spectrum of cyber security. Nation or state-sponsored cyber attacks through sensors’ data corruption, hardware Trojans, or counterfeit wireless signals may exploit dormant and residual software vulnerabilities. It may lead to severe and catastrophic consequences including but not limited to serious injury or death of the crew, extreme damage or loss to equipment and environment. We have proposed a machine learning based theoretical framework for real-time monitoring and failure analysis of autonomous flight software. It has been introduced to protect the mission-critical flight software from run-time data-driven semantic bugs and exploitation that may be caused by missing, jammed, or spoofed data values, due to malicious online cyber activities. The effectiveness of the proposed framework has been demonstrated by the evaluation of a real-world incident of grounding an aerial vehicle by the actors in their vicinity without the intent of the original equipment manufacturer (OEM). The results show that the reported undesired but successful cyber attack may has been avoided by the effective utilization of our proposed cyber defense approach, which is targeted at software failure prediction, detection, and correction for autonomous aerial vehicles."
11,excluded,10.2147/rmhp.s338186,Risk management and healthcare policy,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/86041f9ba7e725862af6c84126d6d0122be799e5,1/1/2022 0:00,a sustainable model for emergency medical services in developing countries: a novel approach using partial outsourcing and machine learning,"Introduction Unlike Western countries, many low- and middle-income countries (LMIC), like India, have a de-centralized emergency medical services (EMS) involving both semi-government and non-government organizations. It is alarming that due to the absence of a common ecosystem, the utilization of resources is inefficient, which leads to shortage of available vehicles and larger response time. Fragmentation of emergency supply chain resources motivates us to propose a new vehicle routing and scheduling model equipped with novel features to ensure minimal response time using existing resources. Materials and Methods The data set of medical and fire-related emergencies from January 2018 to May 2018 of Uttarakhand State in India was provided by GVK Emergency Management and Research Institute (GVK EMRI) also known as 108 EMSs was used in the study. The proposed model integrates all the available EMS vehicles including partial outsourcing to non-ambulatory vehicles like police vans, taxis, etc., using a novel two-echelon heuristic approach. In the first stage, an offline learning model is developed to yield the deployment strategy for EMS vehicles. Seven well researched machine learning (ML) algorithms were analyzed for parameter prediction namely random forest (RF), convolutional neural network (CNN), k-nearest neighbor (KNN), classification and regression tree (CART), support vector machine (SVM), logistic regression (LR), and linear discriminant analysis (LDA). In the second stage, a real-time routing model is proposed for EMS vehicle routing at the time of emergency, considering partial outsourcing. Results and Discussion The results indicate that the RF classifier outperforms the LR, LDA, SVM, CNN, CART and NB classifier in terms of both accuracy as well as F-1 score. The proposed vehicle routing and scheduling model for automated decision-making shows an improvement of 42.1%, 54%, 27.9% and 62% in vehicle assignment time, vehicle travel time from base to scene, travel time from scene to hospital, and total response time, respectively, in urban areas."
12,excluded,http://arxiv.org/abs/2110.15127v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2110.15127v1,10/28/2021 0:00,"lightweight mobile automated assistant-to-physician for global
  lower-resource areas","Importance: Lower-resource areas in Africa and Asia face a unique set of
healthcare challenges: the dual high burden of communicable and
non-communicable diseases; a paucity of highly trained primary healthcare
providers in both rural and densely populated urban areas; and a lack of
reliable, inexpensive internet connections. Objective: To address these
challenges, we designed an artificial intelligence assistant to help primary
healthcare providers in lower-resource areas document demographic and medical
sign/symptom data and to record and share diagnostic data in real-time with a
centralized database. Design: We trained our system using multiple data sets,
including US-based electronic medical records (EMRs) and open-source medical
literature and developed an adaptive, general medical assistant system based on
machine learning algorithms. Main outcomes and Measure: The application
collects basic information from patients and provides primary care providers
with diagnoses and prescriptions suggestions. The application is unique from
existing systems in that it covers a wide range of common diseases, signs, and
medication typical in lower-resource countries; the application works with or
without an active internet connection. Results: We have built and implemented
an adaptive learning system that assists trained primary care professionals by
means of an Android smartphone application, which interacts with a central
database and collects real-time data. The application has been tested by dozens
of primary care providers. Conclusions and Relevance: Our application would
provide primary healthcare providers in lower-resource areas with a tool that
enables faster and more accurate documentation of medical encounters. This
application could be leveraged to automatically populate local or national EMR
systems."
13,excluded,http://arxiv.org/abs/2107.07502v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2107.07502v2,7/15/2021 0:00,multibench: multiscale benchmarks for multimodal representation learning,"Learning multimodal representations involves integrating information from
multiple heterogeneous sources of data. It is a challenging yet crucial area
with numerous real-world applications in multimedia, affective computing,
robotics, finance, human-computer interaction, and healthcare. Unfortunately,
multimodal research has seen limited resources to study (1) generalization
across domains and modalities, (2) complexity during training and inference,
and (3) robustness to noisy and missing modalities. In order to accelerate
progress towards understudied modalities and tasks while ensuring real-world
robustness, we release MultiBench, a systematic and unified large-scale
benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6
research areas. MultiBench provides an automated end-to-end machine learning
pipeline that simplifies and standardizes data loading, experimental setup, and
model evaluation. To enable holistic evaluation, MultiBench offers a
comprehensive methodology to assess (1) generalization, (2) time and space
complexity, and (3) modality robustness. MultiBench introduces impactful
challenges for future research, including scalability to large-scale multimodal
datasets and robustness to realistic imperfections. To accompany this
benchmark, we also provide a standardized implementation of 20 core approaches
in multimodal learning. Simply applying methods proposed in different research
areas can improve the state-of-the-art performance on 9/15 datasets. Therefore,
MultiBench presents a milestone in unifying disjoint efforts in multimodal
research and paves the way towards a better understanding of the capabilities
and limitations of multimodal models, all the while ensuring ease of use,
accessibility, and reproducibility. MultiBench, our standardized code, and
leaderboards are publicly available, will be regularly updated, and welcomes
inputs from the community."
14,excluded,http://arxiv.org/abs/2104.09876v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2104.09876v1,4/20/2021 0:00,"iiot-enabled health monitoring for integrated heat pump system using
  mixture slow feature analysis","The sustaining evolution of sensing and advancement in communications
technologies have revolutionized prognostics and health management for various
electrical equipment towards data-driven ways. This revolution delivers a
promising solution for the health monitoring problem of heat pump (HP) system,
a vital device widely deployed in modern buildings for heating use, to timely
evaluate its operation status to avoid unexpected downtime. Many HPs were
practically manufactured and installed many years ago, resulting in fewer
sensors available due to technology limitations and cost control at that time.
It raises a dilemma to safeguard HPs at an affordable cost. We propose a hybrid
scheme by integrating industrial Internet-of-Things (IIoT) and intelligent
health monitoring algorithms to handle this challenge. To start with, an IIoT
network is constructed to sense and store measurements. Specifically,
temperature sensors are properly chosen and deployed at the inlet and outlet of
the water tank to measure water temperature. Second, with temperature
information, we propose an unsupervised learning algorithm named mixture slow
feature analysis (MSFA) to timely evaluate the health status of the integrated
HP. Characterized by frequent operation switches of different HPs due to the
variable demand for hot water, various heating patterns with different heating
speeds are observed. Slowness, a kind of dynamics to measure the varying speed
of steady distribution, is properly considered in MSFA for both heating pattern
division and health evaluation. Finally, the efficacy of the proposed method is
verified through a real integrated HP with five connected HPs installed ten
years ago. The experimental results show that MSFA is capable of accurately
identifying health status of the system, especially failure at a preliminary
stage compared to its competing algorithms."
15,excluded,http://arxiv.org/abs/2101.10869v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.10869v2,1/23/2021 0:00,"a raspberry pi-based traumatic brain injury detection system for
  single-channel electroencephalogram","Traumatic Brain Injury (TBI) is a common cause of death and disability.
However, existing tools for TBI diagnosis are either subjective or require
extensive clinical setup and expertise. The increasing affordability and
reduction in size of relatively high-performance computing systems combined
with promising results from TBI related machine learning research make it
possible to create compact and portable systems for early detection of TBI.
This work describes a Raspberry Pi based portable, real-time data acquisition,
and automated processing system that uses machine learning to efficiently
identify TBI and automatically score sleep stages from a single-channel
Electroen-cephalogram (EEG) signal. We discuss the design, implementation, and
verification of the system that can digitize EEG signal using an Analog to
Digital Converter (ADC) and perform real-time signal classification to detect
the presence of mild TBI (mTBI). We utilize Convolutional Neural Networks (CNN)
and XGBoost based predictive models to evaluate the performance and demonstrate
the versatility of the system to operate with multiple types of predictive
models. We achieve a peak classification accuracy of more than 90% with a
classification time of less than 1 s across 16 s - 64 s epochs for TBI vs
control conditions. This work can enable development of systems suitable for
field use without requiring specialized medical equipment for early TBI
detection applications and TBI research. Further, this work opens avenues to
implement connected, real-time TBI related health and wellness monitoring
systems."
16,unknown,http://arxiv.org/abs/2101.03989v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.03989v2,1/11/2021 0:00,technology readiness levels for machine learning systems,"The development and deployment of machine learning (ML) systems can be
executed easily with modern tools, but the process is typically rushed and
means-to-an-end. The lack of diligence can lead to technical debt, scope creep
and misaligned objectives, model misuse and failures, and expensive
consequences. Engineering systems, on the other hand, follow well-defined
processes and testing standards to streamline development for high-quality,
reliable results. The extreme is spacecraft systems, where mission critical
measures and robustness are ingrained in the development process. Drawing on
experience in both spacecraft engineering and ML (from research through product
across domain areas), we have developed a proven systems engineering approach
for machine learning development and deployment. Our ""Machine Learning
Technology Readiness Levels"" (MLTRL) framework defines a principled process to
ensure robust, reliable, and responsible systems while being streamlined for ML
workflows, including key distinctions from traditional software engineering.
Even more, MLTRL defines a lingua franca for people across teams and
organizations to work collaboratively on artificial intelligence and machine
learning technologies. Here we describe the framework and elucidate it with
several real world use-cases of developing ML methods from basic research
through productization and deployment, in areas such as medical diagnostics,
consumer computer vision, satellite imagery, and particle physics."
17,excluded,http://arxiv.org/abs/2010.02715v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2010.02715v1,10/3/2020 0:00,"assessing automated machine learning service to detect covid-19 from
  x-ray and ct images: a real-time smartphone application case study","The recent outbreak of SARS COV-2 gave us a unique opportunity to study for a
non interventional and sustainable AI solution. Lung disease remains a major
healthcare challenge with high morbidity and mortality worldwide. The
predominant lung disease was lung cancer. Until recently, the world has
witnessed the global pandemic of COVID19, the Novel coronavirus outbreak. We
have experienced how viral infection of lung and heart claimed thousands of
lives worldwide. With the unprecedented advancement of Artificial Intelligence
in recent years, Machine learning can be used to easily detect and classify
medical imagery. It is much faster and most of the time more accurate than
human radiologists. Once implemented, it is more cost-effective and
time-saving. In our study, we evaluated the efficacy of Microsoft Cognitive
Service to detect and classify COVID19 induced pneumonia from other
Viral/Bacterial pneumonia based on X-Ray and CT images. We wanted to assess the
implication and accuracy of the Automated ML-based Rapid Application
Development (RAD) environment in the field of Medical Image diagnosis. This
study will better equip us to respond with an ML-based diagnostic Decision
Support System(DSS) for a Pandemic situation like COVID19. After optimization,
the trained network achieved 96.8% Average Precision which was implemented as a
Web Application for consumption. However, the same trained network did not
perform the same like Web Application when ported to Smartphone for Real-time
inference. Which was our main interest of study. The authors believe, there is
scope for further study on this issue. One of the main goal of this study was
to develop and evaluate the performance of AI-powered Smartphone-based
Real-time Application. Facilitating primary diagnostic services in less
equipped and understaffed rural healthcare centers of the world with unreliable
internet service."
18,excluded,10.1007/s00170-021-07248-3,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00170-021-07248-3,8/1/2021 0:00,cognitive capabilities for the caai in cyber-physical production systems,"This paper presents the cognitive module of the Cognitive Architecture for Artificial Intelligence (CAAI) in cyber-physical production systems (CPPS). The goal of this architecture is to reduce the implementation effort of artificial intelligence (AI) algorithms in CPPS. Declarative user goals and the provided algorithm-knowledge base allow the dynamic pipeline orchestration and configuration. A big data platform (BDP) instantiates the pipelines and monitors the CPPS performance for further evaluation through the cognitive module. Thus, the cognitive module is able to select feasible and robust configurations for process pipelines in varying use cases. Furthermore, it automatically adapts the models and algorithms based on model quality and resource consumption. The cognitive module also instantiates additional pipelines to evaluate algorithms from different classes on test functions. CAAI relies on well-defined interfaces to enable the integration of additional modules and reduce implementation effort. Finally, an implementation based on Docker, Kubernetes, and Kafka for the virtualization and orchestration of the individual modules and as messaging technology for module communication is used to evaluate a real-world use case."
19,included,10.1007/s42979-021-00726-1,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s42979-021-00726-1,6/19/2021 0:00,towards regulatory-compliant mlops: oravizio’s journey from a machine learning experiment to a deployed certified medical product,"Agile software development embraces change and manifests working software over comprehensive documentation and responding to change over following a plan. The ability to continuously release software has enabled a development approach where experimental features are put to use, and, if they stand the test of real use, they remain in production. Examples of such features include machine learning (ML) models, which are usually pre-trained, but can still evolve in production. However, many domains require more plan-driven approach to avoid hazard to environment and humans, and to mitigate risks in the process. In this paper, we start by presenting continuous software engineering practices in a regulated context, and then apply the results to the emerging practice of MLOps, or continuous delivery of ML features. Furthermore, as a practical contribution, we present a case study regarding Oravizio, first CE-certified medical software for assessing the risks of joint replacement surgeries. Towards the end of the paper, we also reflect the Oravizio experiences to MLOps in regulatory context."
20,unknown,10.1007/s00521-021-05726-z,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00521-021-05726-z,1/25/2021 0:00,towards design and implementation of industry 4.0 for food manufacturing,"Today’s factories are considered as smart ecosystems with humans, machines and devices interacting with each other for efficient manufacturing of products. Industry 4.0 is a suite of enabler technologies for such smart ecosystems that allow transformation of industrial processes. When implemented, Industry 4.0 technologies have a huge impact on efficiency, productivity and profitability of businesses. The adoption and implementation of Industry 4.0, however, require to overcome a number of practical challenges, in most cases, due to the lack of modernisation and automation in place with traditional manufacturers. This paper presents a first of its kind case study for moving a traditional food manufacturer, still using the machinery more than one hundred years old, a common occurrence for small- and medium-sized businesses, to adopt the Industry 4.0 technologies. The paper reports the challenges we have encountered during the transformation process and in the development stage. The paper also presents a smart production control system that we have developed by utilising AI, machine learning, Internet of things, big data analytics, cyber-physical systems and cloud computing technologies. The system provides novel data collection, information extraction and intelligent monitoring services, enabling improved efficiency and consistency as well as reduced operational cost. The platform has been developed in real-world settings offered by an Innovate UK-funded project and has been integrated into the company’s existing production facilities. In this way, the company has not been required to replace old machinery outright, but rather adapted the existing machinery to an entirely new way of operating. The proposed approach and the lessons outlined can benefit similar food manufacturing industries and other SME industries."
21,included,10.1007/978-3-030-77070-9_10,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-77070-9_10,1/1/2021 0:00,smart and intelligent chatbot assistance for future industry 4.0,"Chatbot is an implementation of artificial intelligence (AI) technology that is used to interact with human beings and make them feel like they are talking to the real person, and the chatbot helps them to solve their queries. A chatbot can provide 24 × 7 customer support so that the customer may have a good service experience by any organization. Chatbot helps to resolve the queries and respond to the questions of users. The user is providing the input to the chatbot first, and then, the same input will be processed further; this input can be in the form of text or voice. Therefore, on the basis of the given input and after processing it, the chatbot application will generate the response to the user, and the same response will be the best answer found by the chat application. This response can be in any format like text or a voice output. In this chapter, various approaches of chatbots and how they interact with users are discussed. The proposed approach is also defined using Dialogflow, and it can be accessible through mobile phones, laptops, and portable devices. Chatbots such as Facebook chatbot, WeChat chatbot, Hike chatbot called Natasha, etc. are available in the marker and will respond on the basis of their local databases (DBs). In the proposed method, the focus will be on the scalability, user interactivity, and flexibility of the system, which can be provided by adding both local and Web databases due to which our system will be more fast and accurate. Chatbot uses unification of emerging technologies like machine learning and artificial intelligence. The motive of this chapter is to improve the chatbot system to support and scale businesses and industry domain and maintain relations with customers."
22,unknown,10.1186/s40537-020-00340-7,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1186/s40537-020-00340-7,8/12/2020 0:00,big data architecture for intelligent maintenance: a focus on query processing and machine learning algorithms,"Exploiting available condition monitoring data of industrial machines for intelligent maintenance purposes has been attracting attention in various application fields. Machine learning algorithms for fault detection, diagnosis and prognosis are popular and easily accessible. However, our experience in working at the intersection of academia and industry showed that the major challenges of building an end-to-end system in a real-world industrial setting go beyond the design of machine learning algorithms. One of the major challenges is the design of an end-to-end data management solution that is able to efficiently store and process large amounts of heterogeneous data streams resulting from a variety of physical machines. In this paper we present the design of an end-to-end Big Data architecture that enables intelligent maintenance in a real-world industrial setting. In particular, we will discuss various physical design choices for optimizing high-dimensional queries, such as partitioning and Z-ordering, that serve as the basis for health analytics. Finally, we describe a concrete fault detection use case with two different health monitoring algorithms based on machine learning and classical statistics and discuss their advantages and disadvantages. The paper covers some of the most important aspects of the practical implementation of such an end-to-end solution and demonstrates the challenges and their mitigation for the specific application of laser cutting machines."
23,excluded,10.1109/ieem.2018.8607399,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8607399/,12/19/2018 0:00,a chatbot-supported smart wireless interactive healthcare system for weight control and health promotion,"People who are overweight and obese have a greater risk of developing serious diseases and health conditions. A steadily increasing trend of obesity is not only limited to developed countries, but to developing nations as well. As smartphones have rapidly gained mainstream popularity, mobile applications (apps) are used in public health as intervention to keep track of diets, activity as well as weight, which is deemed more accurate than relying on user's self-report measure, for the sake of weight management. A solution called “Smart Wireless Interactive Healthcare System” (SWITCHes) is developed to facilitate objective data reception and transmission in a real-time manner. Based on the user data acquired from SWITCHes app and the auxiliary data from medical instruments, not only SWITCHes app can engage user with tailored feedback in an interactive way, in terms of artificial intelligence-powered health chatbot, but the healthcare professional can provide the more accurate medical advice to user also. This paper presents an overview of development and implementation of SWITCHes."
24,excluded,10.1109/compsac48688.2020.00-35,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9202847/,7/17/2020 0:00,an intelligent health analysis approach to detecting potential threats with health data reuse,"Health care is becoming imperative in normal life because threats from several aspects could have negative influences on people's health. Along with machine learning development, some application software can be used for evaluating health level and provide health care reports for health supervising and management. However, it is rarely being used in common life for health managing and supervising. Meanwhile, most of the health data that are collected or processed can rarely be reused in any of applied systems. This paper is used for applying machine learning in health care management and supervision. The threats from circumstance could be detected by processing static data and dynamic data. Entire system is equipped with wearable equipment, such as cloth and shoes, which can be used to achieve real time supervising threats for the users. Machine learning can be used to achieve accurate calculation for the health data. Health management and threats supervision performance can be improved effectively. Distinguished subjects are combined to achieve new methods and integrated knowledge is used for achieving novel theory to support health care system establishment. Creative computing methods, knowledge combination is used for generating such integrated methods."
25,excluded,10.1109/tsp.2019.8768883,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8768883/,7/3/2019 0:00,edge-ai in lora-based health monitoring: fall detection system with fog computing and lstm recurrent neural networks,"Remote healthcare monitoring has exponentially grown over the past decade together with the increasing penetration of Internet of Things (IoT) platforms. IoT-based health systems help to improve the quality of healthcare services through real-time data acquisition and processing. However, traditional IoT architectures have some limitations. For instance, they cannot properly function in areas with poor or unstable Internet. Low power wide area network (LPWAN) technologies, including long-range communication protocols such as LoRa, are a potential candidate to overcome the lacking network infrastructure. Nevertheless, LPWANs have limited transmission bandwidth not suitable for high data rate applications such as fall detection systems or electrocardiography monitoring. Therefore, data processing and compression are required at the edge of the network. We propose a system architecture with integrated artificial intelligence that combines Edge and Fog computing, LPWAN technology, IoT and deep learning algorithms to perform health monitoring tasks. In particular, we demonstrate the feasibility and effectiveness of this architecture via a use case of fall detection using recurrent neural networks. We have implemented a fall detection system from the sensor node and Edge gateway to cloud services and end-user applications. The system uses inertial data as input and achieves an average precision of over 90% and an average recall over 95% in fall detection."
26,included,10.1109/rweek.2018.8473535,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8473535/,8/23/2018 0:00,framework for data driven health monitoring of cyber-physical systems,"Modern infrastructure is heavily reliant on systems with interconnected computational and physical resources, named Cyber-Physical Systems (CPSs). Hence, building resilient CPSs is a prime need and continuous monitoring of the CPS operational health is essential for improving resilience. This paper presents a framework for calculating and monitoring of health in CPSs using data driven techniques. The main advantages of this data driven methodology is that the ability of leveraging heterogeneous data streams that are available from the CPSs and the ability of performing the monitoring with minimal a priori domain knowledge. The main objective of the framework is to warn the operators of any degradation in cyber, physical or overall health of the CPS. The framework consists of four components: 1) Data acquisition and feature extraction, 2) state identification and real time state estimation, 3) cyber-physical health calculation and 4) operator warning generation. Further, this paper presents an initial implementation of the first three phases of the framework on a CPS testbed involving a Microgrid simulation and a cyber-network which connects the grid with its controller. The feature extraction method and the use of unsupervised learning algorithms are discussed. Experimental results are presented for the first two phases and the results showed that the data reflected different operating states and visualization techniques can be used to extract the relationships in data features."
27,unknown,10.1109/fmec.2019.8795327,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8795327/,6/13/2019 0:00,health monitoring with low power iot devices using anomaly detection algorithm,"The healthcare industry is rapidly adopting new technologies such as the Internet of Things (IoT), which are dropping costs and improving healthcare outcomes. Such IoT systems typically include edge devices (glucose monitors, ventilators, pacemakers), gateway devices that aggregate the data from the edge devices and transmit it to the cloud, and cloud-based systems which analyze the device data to draw conclusions, display information, or direct the connected devices to take action. This process can lead to communication lags and delayed responses to patient conditions/treatment. The aim of this proposal is to overcome these delays with IoT technology and allow for prompt urgent treatment to patients. The solution proposed includes a model to monitor and process the data disseminated by wearable devices related to the patients' health issues and connect the data to IoT cloud platforms. Analysis of the patients' health data to identify anomalies will be performed at the device level by developing an offline machine learning model using specific algorithms for anomaly detection and deploying them on the IoT devices or IoT gateway. Processing of the real-time health data will be performed at the device level and the prediction of anomalous data will be sent to the third-party cloud for implementing any necessary actions."
28,excluded,10.1109/icoei.2019.8862754,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8862754/,4/25/2019 0:00,machine learning based health prediction system using ibm cloud as paas,"Adaptable Critical Patient Caring system is a key concern for hospitals in developing countries like Bangladesh. Most of the hospital in Bangladesh lack serving proper health service due to unavailability of appropriate, easy and scalable smart systems. The aim of this project is to build an adequate system for hospitals to serve critical patients with a real-time feedback method. In this paper, we propose a generic architecture, associated terminology and a classificatory model for observing critical patient's health condition with machine learning and IBM cloud computing as Platform as a service (PaaS). Machine Learning (ML) based health prediction of the patients is the key concept of this research. IBM Cloud, IBM Watson studio is the platform for this research to store and maintain our data and ml models. For our ml models, we have chosen the following Base Predictors: Naïve Bayes, Logistic Regression, KNeighbors Classifier, Decision Tree Classifier, Random Forest Classifier, Gradient Boosting Classifier, and MLP Classifier. For improving the accuracy of the model, the bagging method of ensemble learning has been used. The following algorithms are used for ensemble learning: Bagging Random Forest, Bagging Extra Trees, Bagging KNeighbors, Bagging SVC, and Bagging Ridge. We have developed a mobile application named “Critical Patient Management System - CPMS” for real-time data and information view. The system architecture is designed in such a way that the ml models can train and deploy in a real-time interval by retrieving the data from IBM Cloud and the cloud information can also be accessed through CPMS in a requested time interval. To help the doctors, the ml models will predict the condition of a patient. If the prediction based on the condition gets worse, the CPMS will send an SMS to the duty doctor and nurse for getting immediate attention to the patient. Combining with the ml models and mobile application, the project may serve as a smart healthcare solution for the hospitals."
29,excluded,10.1109/icesc51422.2021.9532913,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9532913/,8/6/2021 0:00,natural language processing based human assistive health conversational agent for multi-users,"Background: Most of the people are not medically qualified for studying or understanding the extremity of their diseases or symptoms. This is the place where natural language processing plays a vital role in healthcare. These chatbots collect patients' health data and depending on the data, these chatbot give more relevant data to patients regarding their body conditions and recommending further steps also. Purposes: In the medical field, AI powered healthcare chatbots are beneficial for assisting patients and guiding them in getting the most relevant assistance. Chatbots are more useful for online search that users or patients go through when patients want to know for their health symptoms. Methods: In this study, the health assistant system was developed using Dialogflow application programming interface (API) which is a Google's Natural language processing powered algorithm and the same is deployed on google assistant, telegram, slack, Facebook messenger, and website and mobile app. With this web application, a user can make health requests/queries via text message and might also get relevant health suggestions/recommendations through it. Results: This chatbot acts like an informative and conversational chatbot. This chatbot provides medical knowledge such as disease symptoms and treatments. Storing patients personal and medical information in a database for further analysis of the patients and patients get real time suggestions from doctors. Conclusion: In the healthcare sector AI-powered applications have seen a remarkable spike in recent days. This covid crisis changed the whole healthcare system upside down. So this NLP powered chatbot system reduced office waiting, saving money, time and energy. Patients might be getting medical knowledge and assisting ourselves within their own time and place."
30,unknown,10.1109/asmc49169.2020.9185239,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9185239/,8/26/2020 0:00,real-time tool health monitoring and defect inspection during epoxy dispense process,"We demonstrate a new real-time inspection system developed to monitor tool health and detect defects during the epoxy dispense process. The system includes both hardware and software components. The hardware was designed to be low-cost and fit into a small footprint within the existing tools. Our software contains a tool setup/calibration utility and a user interface for recipe creation and real-time inspection. The software also provides extensive logging of key results including tabulated data, annotated images, and live inspection results on the user interface. The algorithm uses a mixture of advanced machine learning and computer vision algorithms to identify unwanted process variation. The new system has provided excellent results, an order of magnitude below the qualification targets, while ensuring the throughput time targets are not impacted."
31,excluded,10.1109/iwcmc48107.2020.9148180,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9148180/,6/19/2020 0:00,real-time health monitoring system based on wearable devices,"This paper proposed a novel electrocardiogram (ECG) automatic diagnose system for health assistance and rescue related with cardiovascular diseases. This system consists of three parts: 1) Data acquisition subsystem, this subsystem acquires ECG data from wearable devices on users' body and transmit them to the cloud server. 2) Deep learning analysis subsystem, with the help of convolutional neural network, the important feature lied inside ECG signal can be extract for abnormal heart condition detection. Hierarchical residual modules provide the network the ability to see seconds of signal and make a decision through the combination of features. Meanwhile, the global max pooling layer on top of the network enables it to capture the most important feature across the whole ECG signal with periodicity. This subsystem is a crucial part for cardiac status based health caring. 3) Back-stage management subsystem, methodical data storage and management were conducted in this subsystem, which also provides the users an interface to access their healthy data and body status. Assembling these three parts of system, real-time ECG diagnose for people in need and timely medical rescue can be implemented."
32,unknown,10.1109/bigdata47090.2019.9005638,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9005638/,12/12/2019 0:00,speech emotion detection using iot based deep learning for health care,"Human emotions are essential to recognize the behavior and state of mind of a person. Emotion detection through speech signals has started to receive more attention lately. This paper proposes the method for detecting human emotions using speech signals and its implementation in real-time using the Internet of Things (IoT) based deep learning for the care of older adults in nursing homes. The research has two main contributions. First, we have implemented a real-time system based on audio IoT, where we have recorded human voice and predicted emotions via deep learning. Secondly, for advance classification, we have designed a model using data normalization and data augmentation techniques. Finally, we have created an integrated deep learning model, called Speech Emotion Detection (SED), using a 2D convolutional neural networks (CNN). The best accuracy that was reported by our method was approximately 95%, which outperformed all state-of-the-art approaches. We have further extended to apply the SED model to a live audio sentiment analysis system with IoT technologies for the care of older adults in nursing homes."
33,included,10.1109/access.2020.2970178,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8974224/,1/1/2020 0:00,a novel software engineering approach toward using machine learning for improving the efficiency of health systems,"Recently, machine learning has become a hot research topic. Therefore, this study investigates the interaction between software engineering and machine learning within the context of health systems. We proposed a novel framework for health informatics: the framework and methodology of software engineering for machine learning in health informatics (SEMLHI). The SEMLHI framework includes four modules (software, machine learning, machine learning algorithms, and health informatics data) that organize the tasks in the framework using a SEMLHI methodology, thereby enabling researchers and developers to analyze health informatics software from an engineering perspective and providing developers with a new road map for designing health applications with system functions and software implementations. Our novel approach sheds light on its features and allows users to study and analyze the user requirements and determine both the function of objects related to the system and the machine learning algorithms that must be applied to the dataset. Our dataset used in this research consists of real data and was originally collected from a hospital run by the Palestine government covering the last three years. The SEMLHI methodology includes seven phases: designing, implementing, maintaining and defining workflows; structuring information; ensuring security and privacy; performance testing and evaluation; and releasing the software applications."
34,included,10.1109/compsac48688.2020.0-168,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9202848/,7/17/2020 0:00,an early warning system for hemodialysis complications utilizing transfer learning from hd iot dataset,"According to the 2018 annual report of US Department of Kidney Data System (USRDS), Taiwan's dialysis rate and prevalence rate are the highest in the world due to population aging, diabetes and progresses in cardiovascular care. With the rise of artificial intelligence deep learning in recent years, various analytical software resources have gradually become easier to obtain. At the same time, wearable cyber physical sensors are becoming more and more popular. Measurements on vital signs such as heartbeat, electrocardiogram, and blood oxygenation blood pressure values are ubiquitous. We propose an integrated system that combines dialysis big data deep learning analysis with cross platform physiological sensing. We specifically tackle the early warning of dialysis discomfort such as hypotension, hypertension, cramps, etc., this requires a large amount of data collection, related training, data sources including dialysis treatment process and home physiological data. Although the Dialysis machine is able to produce huge amount of IoT data, the usable data for early warning system training is not as huge due to the limited physician labors devoted for labeling questionable samples. This generally leads to low accuracy for regular CNN training methods. We enhance the AI training performance via a transfer learning technique. The AI training accuracy reaches the value of 99% with the help of transfer learning, while that of an original CNN process on the HD data bears a low 60% accuracy. Given the high prediction accuracy of our AI engine, we are able to integrate the real time measurements from Dialysis machine with wearable devices such as ECG sensors and wrist health watches, and make precision prediction of incoming discomfort during the HD treatments. The ECG signal of the same group patients are also analyzed with the same technique. The same accuracy enhancement are also observed."
35,excluded,10.1109/coginf.2011.6016132,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6016132/,8/20/2011 0:00,an intelligent fault recognizer for rotating machinery via remote characteristic vibration signal detection,"Monitoring industrial machine health in real-time is not only highly demanded but also significantly complicated and difficult. Possible reasons for this include: (a) Access to the machines on site is sometimes impracticable; and (b) The environment in which they operate is usually not human-friendly due to pollution, noise, hazardous wastes, etc. Despite the theoretically sound findings on developing intelligent solutions for machine condition based monitoring, there are few commercial tools in the market that can readily be used. This paper reports on the development of an intelligent fault recognition and monitoring system (Melvin I), which detects and diagnoses rotating machine conditions according to changes in fault frequency indicators. The signals and data are remotely collected from designated sections of machines via data acquisition cards. They are processed by a signal processor in order to extract characteristic vibration signals of ten key performance indicators (KPIs). A 3-layer neural network is designed to recognize and classify faults based on the set of KPIs. The system implemented in our laboratory and applied in the field can also incorporate new experiences into the knowledge base without overwriting previous training. Preliminary results have demonstrated that Melvin I is a smart tool for both system vibration analysts and industrial machine operators."
36,included,10.1109/iceccme52200.2021.9591113,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9591113/,10/8/2021 0:00,cobots for fintech,"Embedded devices enabling payments transaction processing in Financial Services industry cannot have any margin for error. These devices need to be tested & validated by replicating production like environment to the extent possible. This means literally handling payments related events like swiping a credit card, tapping a mobile phone or pressing buttons amongst many other things like in real world. Embedded Software development is time consuming as it involves multiple man-machine interactions and dependencies such as managing and handling embedded devices, operating devices (Push buttons, interpret display panels, read receipt printouts etc.) and sharing devices for collaboration within team. During the current pandemic, it was impossible for software teams to travel to office, share devices or even procure necessary devices on time for project related tasks. This caused delay to project delivery and increased Time to market. The paper describes how the team used Capgemini's flexible Robotics as a Service (RaaS) platform that helped during pandemic to automate feasible man-machine interactions using Robotic arms. The paper provides details of the work done by the team that involves internet of things (IoT), Artificial Intelligence (AI) to remotely handle and operate hardware and devices thereby completing embedded software development life cycles faster and well within budget while ensuring superior product quality and importantly ensuring team's health and safety. This is novel in Financial Services space."
37,unknown,10.1109/etfa45728.2021.9613448,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9613448/,9/10/2021 0:00,computer vision based privacy protected fall detection and behavior monitoring system for the care of the elderly,"The elderly population constitutes a large percentage of the society hence making elderly care a top priority. Falls have been identified as a leading issue among major problems faced by them. Concerning this, many monitoring devices have been developed, most of them focusing solely on one specific health care aspect or related to fall detection, and are based on sensors and wearable devices which are usually uncomfortable for daily use. Considering these aspects, the solution proposed in this research is a real time computer vision-based system that monitors behavior and detects anomalies through deep learning. The monitoring is mainly focused on detecting unusual behavior including falls, and monitoring routine activities to detect deviations. A device approach is used to deploy the deep learning models and consists of IP camera-based monitoring which uses a special privacy protected procedure that ensures the detection is done based on meta data and therefore no camera image or footage is stored. The research is mainly focused on four major components which are user identification, fall detection, routine variance detection and device configuration."
38,unknown,10.1109/cns48642.2020.9162311,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9162311/,7/1/2020 0:00,heka: a novel intrusion detection system for attacks to personal medical devices,"Modern Smart Health Systems (SHS) involve the concept of connected personal medical devices. These devices significantly improve the patient's lifestyle as they permit remote monitoring and transmission of health data (i.e., telemedicine), lowering the treatment costs for both the patient and the healthcare providers. Although specific SHS communication standards (i.e., ISO/IEEE 11073) enable real-time plug-and-play interoperability and communication between different personal medical devices, they do not specify any features for secure communications. In this paper, we demonstrate how personal medical device communication is indeed vulnerable to different cyber attacks. Specifically, we show how an external attacker can hook into the personal medical device's communication and eavesdrop the sensitive health data traffic, and implement manin-the-middle, replay, false data injection, and denial-of-service attacks. Furthermore, we also propose an Intrusion Detection System (IDS), HEKA, to monitor personal medical device traffic and detect attacks on them. HEKA passively hooks into the personal medical traffic generated by medical devices to learn the contiguous sequence of packets information from the captured traffic and detects irregular traffic-flow patterns using an n-grambased approach and different machine learning techniques. We implemented HEKA in a testbed consisting of eight off-the-shelf personal medical devices and evaluated its performance against four different attacks. Our extensive evaluation shows that HEKA can effectively detect different attacks on personal medical devices with an accuracy of 98.4% and Fl-score of 98%."
39,included,10.1109/icdmw.2019.00123,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8955523/,11/11/2019 0:00,implementation of mobile-based real-time heart rate variability detection for personalized healthcare,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework."
40,excluded,10.1109/phm-besancon49106.2020.00009,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9115516/,5/7/2020 0:00,machine performance monitoring and fault classification using vibration frequency analysis,"Machine anomalies in manufacturing directly affect the production yield and factory operation efficiency if such anomalies cannot be detected in time. Real-time monitoring of machine health condition not only improves machine throughput by reducing unplanned downtime caused by machine failure but also saves cost for unnecessary routine maintenance. This paper presents a systematic approach for real-time or near real-time machine performance monitoring solution development from data collection, feature extraction, data analytics to real-time machine fault and machine status classification. Three data-driven machine-learning approaches using one vibration sensor data are proposed to detect two common machine failure modes during machine turning process. To evaluate the the performance of each approach, three machine-learning algorithms (Random Forest, K Nearest Neighborhood, and Support Vector Machine) are implemented and tested. Evaluation results on the actual machine data shows that a two-layered classification structure with random forest algorithm as the base has high classification accuracy on the machine status including machine fault detection. The developed data-driven machine health monitoring solution is deployed in the IoT device for real-time data collection and processing and results are sent data server for data visualization."
41,excluded,10.1109/snpdwinter52325.2021.00019,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9403457/,1/30/2021 0:00,machine learning based network intrusion detection for data streaming iot applications,"In recent years, Internet of Things (IoT) technologies have been widely used in many fields such as surveillance, health-care, smart metering and environment monitoring. This extensive usage leads to massive data management and a complexity in data analysis. A huge number of IoT sensors are deployed for monitoring task and send continuously their collected data to gateways. IoT applications are analyzing these data flows and making real time decisions about specific monitored events (fire, flood, terrorist attacks, etc.). Anomalies that may be related to sensor failures or network intrusions are affecting such decisions. Therefore, they should be detected and eliminated as soon as they arrive. This task requires real time data processing detectors for making accurate and fast predictions. In this paper, we design an architecture for a real time network intrusion detection system for IoT streaming data. The system was developed, deployed and tested with the two leading stream processing frameworks (Apache Flink and Apache Spark Streaming). We used two different public datasets and different machine learning algorithms. Results show considerable throughputs and high detection accuracy especially for Apache Flink."
42,included,10.1109/ijcnn.2013.6706957,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6706957/,8/9/2013 0:00,optimized neuro genetic fast estimator (ongfe) for efficient distributed intelligence instantiation within embedded systems,"The Optimized Neuro Genetic Fast Estimator (ONGFE) is a software tool that allows for embedding system, subsystem, and component failure detection, identification, and prognostics (FDI&P) capability by using Intelligent Software Elements (ISE) based upon Artificial Neural Networks (ANN). With an Application Programming Interface (API), highly innovative algorithms are compiled for efficient distributed intelligence instantiation within embedded systems. The original design had the purpose of providing a real time kernel to deploy health monitoring functions for Condition Based Maintenance (CBM) and Real Time Monitoring (RTM) systems in a broad variety of applications (such as aerospace, structural, and widely distributed support systems). The ONGFE contains embedded fast and on-line training for designing ANNs to perform several high performance FDI&P functions. A key advantage of this technology is an optimization block based upon pseudogenetic algorithms which compensate for effects due to initial weight values and local minimums without the computational burden of genetic algorithms. The ONGFE also provides a synchronization block for communication with secondary diagnostic modules. The algorithms are designed for a distributed, scalar, and modular deployment. Based on this technology, a scheme for conducting sensor data validation has been embedded in Smart Sensors."
43,included,10.1109/qrs51102.2020.00018,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9282796/,12/14/2020 0:00,phm technology for memory anomalies in cloud computing for iaas,"The IaaS (Infrastructure as a Service) is one of the most popular services from todays cloud service providers, where the virtual machines (VM) are rented by users who can deploy any program they want in the VMs to make their own websites or use as their remote desktops. However, this poses a major challenge for cloud IaaS providers who cannot control the software programs that users develop, install or download on their rented VMs. Those programs may not be well developed with various bugs or even downloaded/installed together with virus, which often make damages to the VMs or infect the cloud platform. To keep the health of a cloud IaaS platform, it is very important to implement the PHM (Prognostics and Health Management) technology for detecting those software problems and self-healing them in an intelligent and timely way. This paper realized a novel PHM technology inspired by biological autonomic nervous system to deal with the memory anomalies of those programs running on the cloud IaaS platform. We first present an innovative autonomic computing technology called Bionic Autonomic Nervous System (BANS) to endow the cloud system with distinctive capabilities of perception, detection, reflection, and learning. Then, we propose a BANS-based Prognostics and Health Management (BPHM) technology to enable the cloud system self-dealing with various memory anomalies. AI-based failure prognostics, immediate self-healing, self-learning ability and self-improvement functions are implemented. Experimental results illustrate that the designed BPHM can automatically and intelligently deal with complex memory anomalies in a real cloud system for IaaS, to keep the system much more reliable and healthier."
44,unknown,10.1109/bds/hpsc/ids18.2018.00045,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8552303/,5/5/2018 0:00,real-time intelligent air quality evaluation on a resource-constrained embedded platform,"Indoor air quality has a major impact on health and comfort of building occupants. Poor air quality may reduce productivity in offices and impair students learning in classes. In order to provide localized air pollution data and tailor it for individual, wearable air quality sensor is a promising solution. Furthermore, crowd sensing has emerged as an Internet-of-Things (IoT) solution, which is economical, scalable and easy to deploy and re-deploy as it uses the power of crowd data collection. The goal of our proposed system is to monitor indoor air quality through a crowd sensing system that will use a set of sensors to measure air quality, monitor the concentration of pollutants continuously, and make recommendations in real time for improved air quality. In this paper artificial network is developed to perform real-time indoor air quality control. Utilizing created neural network embedded into a smart controller comfort level of air quality parameters such as temperature, CO_2 air concentration and humidity could be estimated after every measurement and used for adapting air conditioning systems to adjust air quality. Neural network data preparation and training process are discussed along with deployment of trained network on a smart controller."
45,excluded,10.1109/fads.2017.8253198,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8253198/,10/25/2017 0:00,the potential of conversational agents to provide a rapid hiv counseling and testing services,"In low-and middle-income countries where demand for health services outstrips the available supply of skilled labor, advances in information and communication technologies have already been shown to hold promise. While much of the mHealth literature continues to explore mature technologies such as text message and web portals, continual advancement in machine learning opens innovative new areas of exploration for public health practitioners. This paper explores one such possibility, a conversational agent, able to guide users through an HIV counseling and testing session. Using commercially available software (http://api.ai), an agent was designed and built according to the Center for Disease Control's guidelines for the provision of HIV counseling and testing in a non-clinical setting. The agent was linked to the Telegram chat client (http://telegram.org) and 10 testers were invited to participate in a simulated HIV counseling interaction. Six testers found that talking to the agent felt natural, and equivalent to chatting to a human. Seven said they would feel comfortable taking a real HIV test with the agent. Key concerns with the current agent were the use of overly formal language, the speed at which the agent responded (too fast) and the agent either misunderstanding or not understanding the tester. Positive sentiment towards the agent included the fact that testers felt like the session was more private and anonymous, and avoided the need for them to visit a public health facility and stand in a long queue to get tested."
46,unknown,10.1109/iceee2.2017.7935834,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7935834/,4/10/2017 0:00,using lstm networks to predict engine condition on large scale data processing framework,"As the Internet of Things technology is developing rapidly, companies have an ability to observe the health of engine components and constructed systems through collecting signals from sensors. According to output of IoT sensors, companies can build systems to predict the conditions of components. Practically the components are required to be maintained or replaced before the end of life in performing their assigned task. Predicting the life condition of a component is so crucial for industries that have intent to grow in a fast paced technological environment. Recent studies on predictive maintenance help industries to create an alert before the components are corrupted. Thanks to prediction of component failures, companies have a chance to sustain their operations efficiently while reducing their maintenance cost by repairing components in advance. Since maintenance affects production capacity and the service quality directly, optimized maintenance is the key factor for organizations to have more revenue and stay competitive in developing industrialized world. With the aid of well-designed prediction system for understanding current situation of an engine, components could be taken out of active service before malfunction occurs. With the help of inspection, effective maintenance extends component life, improves equipment availability and keeps components in a proper condition while reducing costs. Real time data collected from sensors is a great source to model component deteriorations. Markov Chain models, Survival Analysis, Optimization algorithms and several machine learning approaches have been implemented in order to model predictive maintenance. In this paper Long Short Term Memory (LSTM) networks has been performed to predict the current situation of an engine. LSTM model deals with a sequential input data. Training process of LSTM networks has been performed on large-scale data processing engine with high performance. Since huge amount of data is flowing into the predictive model, Apache Spark which is offering a distributed clustering environment has been used. The output of the LSTM network is deciding the current life condition of components and offering the alerts for components before the end of their life. The proposed model also trained and tested on an open source data that is about an engine degradation simulation provided by the Prognostics CoE at NASA Ames."
47,unknown,10.1109/access.2021.3051583,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9324826/,1/1/2021 0:00,data-driven condition monitoring of mining mobile machinery in non-stationary operations using wireless accelerometer sensor modules,"This paper presents the development of an easy-to-deploy and smart monitoring IoT system that utilizes vibration measurement devices to assess real-time condition of bulldozers, power shovels and backhoes, in non-stationary operations in the mining industry. According to operating experience data and the type of mining machine, total loss failure rates per machine fleet can reach up to 30%. Vibration analysis techniques are commonly used for condition monitoring and early detection of unforeseen failures to generate predictive maintenance plans for heavy machinery. However, this maintenance strategy is intensively used only for stationary machines and/or mobile machinery in stationary operations. Today, there is a lack of proper solutions to detect and prevent critical failures for non-stationary machinery. This paper shows a cost-effective solution proposal for implementing a vibration sensor network with wireless communication and machine learning data-driven capabilities for condition monitoring of non-stationary heavy machinery in mining operations. During the machine operation, 3-axis accelerations were measured using two sensors deployed across the machine. The machine accelerations (amplitudes and frequencies) are measured in two different frequency spectrums to improve each sensing location's time resolution. Multiple machine learning algorithms use this machine data to assess conditions according to manufacturer recommendations and operational benchmarks Proposed data-driven machine learning models classify the machine condition in states according to the ISO 2372 standards for vibration severity: Good, Acceptable, Unsatisfactory, or Unacceptable. After performing field tests with bulldozers and backhoes from different manufacturers, the machine learning algorithms are able to classify machine health status with an accuracy between 85% - 95%. Moreover, the system allows early detection of “Unacceptable” states between 120 to 170 hours prior to critical failure. These results demonstrate that the proposed system will collect relevant data to generate predictive maintenance plans and avoid unplanned downtimes."
48,unknown,10.1109/access.2018.2883106,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8543563/,1/1/2018 0:00,intelligent and real-time data acquisition for medical monitoring in smart campus,"With the continuous development of information technology, people are gradually moving from the digital age to the intelligent era. As one of the most representative emerging technologies in this era, the artificial intelligence is quietly changing our lives at an unprecedented rate. At present, the Internet of Things has been formally included the five emerging strategic industries in the country. Its development route is from safe city, digital city, to the perception of China. As an important part of a safe city, the medical and health field is also moving toward to the intelligent era. Therefore, it is imperative to construct a smart campus hospital environment monitoring system based on Internet of Things technology. The most core part of home environment intelligent monitoring system is the design of data acquisition and display methods. This paper mainly designs and implements the system client, system data format conversion, and system data transmission. The main technical points involved are HL7 protocol, AMQP protocol, and RabbitMQ framework, besides, and the cache technology is applied to the server. Information is cached to provide data assembly for different clients. Finally, real-time monitoring and alarming of the environment are implemented in the PC client and the Android client. The paper monitors the smart campus hospital environment, then carries out real-time transmission, storage, display, and finally analyzes the data to intelligently identify the smart campus hospital environment."
49,unknown,10.1109/wimob.2019.8923286,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8923286/,10/23/2019 0:00,design and deployment of a wireless ban at the edge for reliable healthcare monitoring,"Body Area Networks (BANs) have attracted a lot of research interest in the last decades as also witnessed by standardization activities and European Commission fundings. Today, commercial devices, which implement simplified BAN monitoring have appeared, although they usually imply expensive subscription costs or need for a supporting device carried by the user (e.g. a smart phone) which, in case of elderly people, cannot be easily held. However, the integration between commercial devices and external sensors located on/in the body is still an open issue due to the limited processing capabilities of low cost commercial devices. Also, no tools for anomaly detection aimed at reliable healthcare monitoring are currently commercially available. In this paper, we focus on Cloud-Assisted BANs and evolve this vision according to the emerging paradigm of edge computing. We present design, implementation and experimentation of a wireless BAN system which performs data transmission using a commercial, cheap, off-the-shelf gateway smart watch. A mechanism for prompt anomaly detection at the edge node is also supported for the purpose of reliable healthcare monitoring as well as pre-filtering of the data at the smart device itself. Also, in order to reduce the overhead caused by propagation of useless and time-correlated data, and to guarantee a prompt action in case of emergency, edge network nodes located closer to the patient BAN are exploited since they can execute machine learning algorithms to process large amounts of data and activate potential alerts in a shorter time and without overloading the cloud. In this work we describe a real system and evaluate the effectiveness of the approach in terms of false alarm probability."
50,included,10.1109/itc-egypt52936.2021.9513888,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9513888/,7/15/2021 0:00,a proposed end to end telemedicine system based on embedded system and mobile application using cmos wearable sensors,"Internet of things (IoT) and Embedded systems have extensive applications in healthcare markets. Integration of IoT with healthcare started with wearable smartwatches monitoring some signals and storing this data in the cloud. With 4G/5G and WiFi 6 networks. Healthcare data can be analyzed with Artificial Intelligence providing new era Internet of Medical Things (IoMT) that encompass an array of internet-capable medical devices that are in constant communication with each other or with the cloud; Internet of Healthcare Things (IoHT) that is the digital transformation of the healthcare industry. This article presents an end-to-end architecture with realization of three modules for key IoT aspects for healthcare and telemedicine. Results from a real implementation of application Platform for Data Processing including patient and doctor data base-based web site, MySQL data base, Android based mobile App, and PHP webserver."
51,included,10.1109/wf-iot.2019.8767231,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8767231/,4/18/2019 0:00,efficient deployment of predictive analytics in edge gateways: fall detection scenario,"Ambient Assisted Living (AAL) represents the most promising Internet of Things (IoT) application due to its relevance in the elders healthcare and improvement of their quality of life. Recently, the AAL IoT ecosystem has been enriched with promising technologies such as edge computing, which has demonstrated to be the best approach to overcome the demanding requirements of AAL and healthcare services by providing a reduction of the amount of data to transfer to the cloud, an improvement of the response time, and quality of experience. Also, the deployment of Artificial Intelligence (AI) technologies at the edge provides intelligence to improve the decision making timely. However, this approach has been scarcely studied in AAL scenarios and the few proposals based on deploying machine learning models at the edge lack efficiency, security, mechanisms of resource management, service management, and deployment, as well as a real and experimental AAL scenario. For these reasons, this paper proposes an innovative edge gateway architecture to support the deployment of deep learning (DL) models in AAL and healthcare scenarios efficiently. To do so, we have added a predictive analytics module to deploy the models. Since AI technologies demand more resources, a container-based virtualization technology is employed on the edge gateway to manage the limited resources, and provide security and lifecycle services management. The edge gateway performance was evaluated deploying a DL-based fall detection application on it. As a result, our approach improves the inference time compared to that based on the cloud in 34 seconds and to similar approaches in 8 seconds."
52,unknown,10.1109/tencon.2019.8929612,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8929612/,10/20/2019 0:00,lung nodule detection from low dose ct scan using optimization on intel xeon and core processors with intel distribution of openvino toolkit,"With the advancement of AI in the field of medical imaging, medical diagnosis is getting faster and viable for medical practitioners especially for cancer diagnosis. Earlier Deep Learning solutions had to be deployed on High Performance Computing devices like GPU for achieving real time performance. But with Optimization on Intel Core and Xeon processors with Intel Distribution of OpenVINO Toolkit (Open Visual Inference and Neural Network Optimization), it is possible to deploy Deep Learning models with accelerated performance, than running Tensorflow / Caffe models on CPU machines. In this paper we describe the proposed work wherein we ported our DetectNet Deep Learning Model with NVIDIA specific custom layer for lung nodule detection trained on LIDC dataset, using Intel Distribution of OpenVINO, and deployed the same in Intel Core/Xeon processors with accelerated performance."
53,unknown,10.1109/bibm47256.2019.8983322,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8983322/,11/21/2019 0:00,openhi2 — open source histopathological image platform,"Transition from conventional to digital pathology requires a new category of biomedical informatic infrastructure which could facilitate delicate pathological routine. Pathological diagnoses are sensitive to many external factors and is known to be subjective. Only systems that can meet strict requirements in pathology would be able to run along pathological routines and eventually digitized the area, and the developed platform should comply with existing pathological routines and international standards. Currently, there are a number of available software tools which can perform histopathological tasks including virtual slide viewing, annotating, and basic image analysis, however, none of them can serve as a digital platform for pathology. Here we describe OpenHI2, an enhanced version Open Histopathological Image platform which is capable of supporting all basic pathological tasks and file formats; ready to be deployed in medical institutions on a standard server environment or cloud computing infrastructure. In this paper, we also describe the development decisions for the platform and propose solutions to overcome technical challenges including responsive region retrieval and viewing, virtual slide magnification, recording of diagnostic areas. These factors would promote OpenHI2 be used as a platform for histopathological images in real-world clinical settings. Furthermore, in research, OpenHI2 inherited the annotation functionality from the previous version, thus acquired annotations can be directly utilized by the newly added machine learning module which include popular machine learning models to perform tasks such as histology image classification and segmentation in the same environment. Addition can be made to the platform since each component is modularized and fully documented. OpenHI2 is free, open-source, and available at https://gitlab.com/BioAI/OpenHI."
54,unknown,10.1109/iccca49541.2020.9250902,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9250902/,10/31/2020 0:00,smart accident recognition and alerting system for edge devices,"The rate of road accidents has been rising over the years and the high fatalities in these accidents is a matter of great concern. In accidents like these, every second matters. Many times the medical services are unable to make it in time resulting in an unfortunate loss of life, making road accident deaths an integral issue. Also, chances of accidents are equally high during late hours. In an emergency at odd hours, one cannot guarantee if anyone will be present around to inform hospitals, or the police. This paper proposes an End-to-End Deep Learning solution to automate accident recognition and send real-time alerts to emergency services, that is the nearest hospitals and police stations. The proposed system is aimed to be deployed on edge devices attached to roadside CCTV cameras. For this purpose, an optimization step is performed on the deep learning models using the Intel's OpenVINO toolkit which improves performance on edge devices."
55,unknown,10.1109/access.2019.2919736,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8728285/,1/1/2019 0:00,federated learning-based computation offloading optimization in edge computing-supported internet of things,"Recently, smart cities, smart homes, and smart medical systems have challenged the functionality and connectivity of the large-scale Internet of Things (IoT) devices. Thus, with the idea of offloading intensive computing tasks from them to edge nodes (ENs), edge computing emerged to supplement these limited devices. Benefit from this advantage, IoT devices can save more energy and still maintain the quality of the services they should provide. However, computational offload decisions involve federation and complex resource management and should be determined in the real-time face to dynamic workloads and radio environments. Therefore, in this work, we use multiple deep reinforcement learning (DRL) agents deployed on multiple edge nodes to indicate the decisions of the IoT devices. On the other hand, with the aim of making DRL-based decisions feasible and further reducing the transmission costs between the IoT devices and edge nodes, federated learning (FL) is used to train DRL agents in a distributed fashion. The experimental results demonstrate the effectiveness of the decision scheme and federated learning in the dynamic IoT system."
56,included,10.1109/icac.2017.21,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8005354/,7/21/2017 0:00,ananke: a q-learning-based portfolio scheduler for complex industrial workflows,"Complex workflows that process sensor data are useful for industrial infrastructure management and diagnosis. Although running such workflows in clouds promises reduced operational costs, there are still numerous scheduling challenges to overcome. Such complex workflows are dynamic, exhibit periodic patterns, and combine diverse task groupings and requirements. In this work, we propose ANANKE, a scheduling system addressing these challenges. Our approach extends the state-of-the-art in portfolio scheduling for data centers with a reinforcement-learning technique, and proposes various scheduling policies for managing complex workflows. Portfolio scheduling addresses the dynamic aspect of the workload. Q-learning, allows our approach to adapt to the periodic patterns of the workload, and to tune the other configuration parameters. The proposed policies are heuristics that guide the provisioning process, and map workflow tasks to the provisioned cloud resources. Through real-world experiments based on real and synthetic industrial workloads, we analyze and compare our prototype implementation of ANANKE with a system without portfolio scheduling (baseline) and with a system equipped with a standard portfolio scheduler. Overall, our experimental results give evidence that a learning-based portfolio scheduler can perform better and consume fewer resources than state-of-the-art alternatives, in particular for workloads with uniform arrival patterns."
57,unknown,10.1016/j.scs.2021.103215,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85111854275,11/1/2021,ioht-enabled gliomas disease management using fog computing computing for sustainable societies,"The proliferation of sensor-based applications in healthcare has given rise to Internet of Health Things (IoHT) that improves patient safety, staff morale, and operational efficiency. Edge-fog computing has seen significant development in recent years and supports the association of various intelligent things with sensors for establishing smooth data transfer. However, it becomes challenging for edge-fog computing to tackle diverse IoHT settings such as efficient disease management, emergency response management, etc. The key limitation of existing architectures is the restricted scalability and inability to meet the demands of hierarchical computing environments for IoHT. This is because latency-sensitive applications often require large quantities of data to be measured and transferred to the data centers, which causes delay and reduced output. This research proposes a novel edge-fog computing framework for the convergence of machine learning ensemble with edge-fog computing. The proposed architecture delivers healthcare as a fog system that handles data from different sources to manage the diseases effectively. The proposed framework is used for the real-life implementation and automatic detection of gliomas diseases. Glioma is a kind of tumor, which ensues in the spinal cord and a portion of the brain. Glioma instigates in the glial cells that surround the nerve cells. The proposed edge-fog framework efficiently manages the real-time data related to gliomas. This framework is configured for specific operating modes including diverse edge-fog scenarios, different user requirements, quality of service, precision, and predictive accuracy. The proposed framework is evaluated using real-time datasets from various sources and experimentally tested with reliable datasets that disclose the effectiveness of the proposed architecture. The performance of the proposed model is evaluated in terms of power consumption, latency, accuracy, and execution time, respectively."
58,excluded,10.1016/j.eswa.2021.114951,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85104365062,9/1/2021,providing music service in ambient intelligence: experiments with gym users,"Ambient Intelligence (AmI) is an interdisciplinary research area of ICT which has evolved since the 90s, taking great advantage from the advent of the Internet of Things (IoT). AmI creates, by using Artificial Intelligence (AI), an intelligent ecosystem in which computers, sensors, lighting, music, personal devices, and distributed services, work together to improve the user experience through the support of natural and intuitive user interfaces. Nowadays, AmI is used in various contexts, e.g., for building smart homes and smart cities, providing healthcare, and creating an adequate atmosphere in retail and public environments.
                  In this paper, we propose a novel AmI system for gym environments, named Gym Intelligence, able to provide adequate music atmosphere, according to the users’ physical effort during the training. The music is taken from Spotify and is classified according to some music features, as provided by Spotify itself. The system is based on a multi-agent computational intelligence model built on two main components: 
                        
                           (
                           i
                           )
                        
                      machine learning methods that forecast appropriate values for the Spotify music features, and 
                        
                           (
                           ii
                           )
                        
                      a multi-objective dynamic genetic algorithm that selects a specific Spotify music track, according to such values. Gym Intelligence is built by sensing the ambient with a minimal, low-cost, and non-intrusive set of sensors, and it has been designed considering the outcome of a preliminary analysis in real gyms, involving real users. We have considered well-known regression methods and we have validated them using a collected data 
                        
                           (
                           i
                           )
                        
                      about the users’ physical effort, through the sensors, and 
                        
                           (
                           ii
                           )
                        
                      about the users’ music preferences, through an Android app that the users have used during the training. Among the regression methods considered, the one that provided the best results is the Random Forest, which predicted Spotify music features with a mean absolute error of 0.02 and a root mean squared error of 0.05. We have implemented Gym Intelligence and deployed it in five real gyms. We have evaluated it conducting several experiments. The experiments show how, with the help of Gym Intelligence, the users’ satisfaction about the provided background music, rose from 3.05 to 4.91 (on a scale from 1 to 5, where 5 is the maximum score)."
59,unknown,10.1016/j.micpro.2020.103301,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85094168107,2/1/2021,iot enabled cancer prediction system to enhance the authentication and security using cloud computing,"In recent days, Internet of Things, Cloud Computing, Deep learning, Machine learning and Artificial Intelligence are considered to be an emerging technologies to solve variety of real world problems. These techniques are importantly applied in various fields such as healthcare systems, transportation systems, agriculture and smart cities to produce fruitful results for number of issues in today's environment. This research work focuses on one such application in the field of IoT together with cloud computing. More number of sensors that are deployed in human body is used to collect patient related data such as deviation in body temperature and others which leads to variation in blood cells that turned to be cancerous cells. Main intention of this work is design a cancer prediction system using Internet of Things upon extracting the details of blood results to test whether it is normal or abnormal. In addition to this, encryption is done on the blood results of cancer affected patient and store it in cloud for quick reference through Internet for the doctor or healthcare nurse to handle the patient data secretly. This research work concentrates on enhancing the health care computations and processing. It provides a framework to enhance the performance of the existing health care industry across the globe. As the entire medical data has to be saved in cloud, the traditional medical treatment limitations can be overcome. Encryption and decryption is done using AES algorithm in order to provide authentication and security in handling cancer patients. The main focus is to handle healthcare data effectively for the patient when they are away from the home town since the needed cancer treatment details are stored in cloud. The task completion time is greatly reduce from 400 to 160  by using VMs. CloudSim gives an adaptable simulation structure that empowers displaying and reproduced results."
60,unknown,10.1016/j.procs.2021.03.025,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85106733954,1/1/2021,lightweight photoplethysmography quality assessment for real-time iot-based health monitoring using unsupervised anomaly detection,"Real-time remote health monitoring is dramatically growing, revolutionizing healthcare delivery and outcome in everyday settings. Such remote services enable monitoring individuals anywhere and anytime, allowing diseases early detection and prevention. Photoplethysmography (PPG) is a non-invasive and convenient technique that enables tracking vital signs such as heart rate, heart rate variability, respiration rate, and blood oxygen saturation. PPG is broadly used in various clinical and commercial wearable devices, as it is easy-to-implement and low-cost. However, the technique is highly susceptible to motion artifacts and environmental noises, which distort the collected signals. Therefore, the signal quality needs to be investigated, and unreliable signals should be discarded. In the literature, rule-based and machine learning-based PPG quality assessment methods have been investigated in several studies. However, the rule-based methods are mostly inaccurate in remote health monitoring, where users engage in different physical activities. The supervised machine learning-based methods –including deep learning–are also infeasible for real-time monitoring applications since they are slow and are dependent on a massive pool of annotated data to train the model. In this paper, we introduce a PPG quality assessment method, enabled by an elliptical envelope, which requires low computational resources. The method clusters the PPG signals into two groups as “reliable” and “unreliable.” We also investigate various features extracted from the PPG signals. Five features with the highest scoring values are selected to be fed to the elliptical envelope model. Moreover, we assess the performance of the proposed method in terms of accuracy and execution time, using data collected in free-living conditions via an Internet-of-Things-based health monitoring system enabled by smart wristbands. The method is evaluated in comparison to a state-of-the-art PPG quality assessment method. We also provide the model implemented in Python for the community to be used in their solutions."
61,excluded,10.1016/j.ipm.2020.102340,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85087504158,11/1/2020,a topic modeling framework for spatio-temporal information management,"Real-time processing and learning of conflicting data, especially messages coming from different ideas, locations, and time, in a dynamic environment such as Twitter is a challenging task that recently gained lots of attention. This paper introduces a framework for managing, processing, analyzing, detecting, and tracking topics in streaming data. We propose a model selector procedure with a hybrid indicator to tackle the challenge of online topic detection. In this framework, we built an automatic data processing pipeline with two levels of cleaning. Regular and deep cleaning are applied using multiple sources of meta knowledge to enhance data quality. Deep learning and transfer learning techniques are used to classify health-related tweets, with high accuracy and improved F1-Score. In this system, we used visualization to have a better understanding of trending topics. To demonstrate the validity of this framework, we implemented and applied it to health-related twitter data from users originating in the USA over nine months. The results of this implementation show that this framework was able to detect and track the topics at a level comparable to manual annotation. To better explain the emerging and changing topics in various locations over time the result is graphically displayed on top of the United States map."
62,included,10.1016/j.iot.2020.100185,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85086362688,9/1/2020,highly-efficient fog-based deep learning aal fall detection system,"Falls is one of most concerning accidents in aged population due to its high frequency and serious repercussion; thus, quick assistance is critical to avoid serious health consequences. There are several Ambient Assisted Living (AAL) solutions that rely on the technologies of the Internet of Things (IoT), Cloud Computing and Machine Learning (ML). Recently, Deep Learning (DL) have been included for its high potential to improve accuracy on fall detection. Also, the use of fog devices for the ML inference (detecting falls) spares cloud drawback of high network latency, non-appropriate for delay-sensitive applications such as fall detectors. Though, current fall detection systems lack DL inference on the fog, and there is no evidence of it in real environments, nor documentation regarding the complex challenge of the deployment. Since DL requires considerable resources and fog nodes are resource-limited, a very efficient deployment and resource usage is critical. We present an innovative highly-efficient intelligent system based on a fog-cloud computing architecture to timely detect falls using DL technics deployed on resource-constrained devices (fog nodes). We employ a wearable tri-axial accelerometer to collect patient monitoring data. In the fog, we propose a smart-IoT-Gateway architecture to support the remote deployment and management of DL models. We deploy two DL models (LSTM/GRU) employing virtualization to optimize resources and evaluate their performance and inference time. The results prove the effectiveness of our fall system, that provides a more timely and accurate response than traditional fall detector systems, higher efficiency, 98.75% accuracy, lower delay, and service improvement."
63,excluded,10.1016/j.eswa.2020.113251,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85079340111,7/1/2020,integrating complex event processing and machine learning: an intelligent architecture for detecting iot security attacks,"The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives."
64,excluded,10.1016/j.iot.2019.100130,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85077314197,3/1/2020,an iot based device-type invariant fall detection system,"As the world elderly population is increasing rapidly, the use of technology for the development of accurate and fast automatic fall detection systems has become a necessity. Most of the fall detection systems are developed for specific devices which reduces the versatility of the fall detection system. This paper proposes a centralized unobtrusive IoT based device-type invariant fall detection and rescue system for monitoring of a large population in real-time. Any type of devices such as Smartphones, Raspberry Pi, Arduino, NodeMcu, and Custom Embedded Systems can be used to monitor a large population in the proposed system. The devices are placed into the users’ left or right pant pocket. The accelerometer data from the devices are continuously sent to a multithreaded server which hosts a pre-trained machine learning model that analyzes the data to determine whether a fall has occurred or not. The server sends the classification results back to the corresponding devices. If a fall is detected, the server notifies the mediator of the user's location via an SMS. As a failsafe, the corresponding device alerts nearby individuals by sounding the buzzer and contacts emergency medical services and mediators via SMS for immediate medical assistance, thus saving the user's life. The proposed system achieved 99.7% accuracy, 96.3% sensitivity, and 99.6% specificity. Finally, the proposed system can be implemented on a variety of devices and used to reliably monitor a large population with low false alarm rate, without obstructing the users’ daily living, as no external connections are required."
65,included,10.1016/j.micpro.2019.102960,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85077060597,3/1/2020,a novel hybrid optimized and adaptive reconfigurable framework for the implementation of hybrid bio-inspired classifiers for diagnosis,"Due to recent advances in IoT (Internet of Things) technologies, availability of reliable data and emergence of machine learning, bio-inspired learning and artificial intelligence, has demonstrated its ability to solve the large complex problems which is not possible before. In particular, machine learning and bio-inspired learning algorithms provides the effective solutions in image processing techniques. However, the implementation of the above-mentioned algorithms in the general CPU requires the intensive usage of bandwidth, area and power which makes the CPU unhealthy of usage and implementation. To overcome this problem, ASIC (application specific integrated circuits), GPU (Graphics Processing Unit) &FPGA (Field Programmable gate arrays) have been employed to improve the performance of the hybrid machine learning (ML) classifiers and deep learning algorithms. FPGA has been recently employed for an effective implementation and to achieve the high performance of the learning algorithms. But integrating the complex learning algorithms in FPGA still remains to be real challenge among the researchers. The paper proposes new reconfigurable architectures for bio- inspired classifiers to diagnosis the medical casualties which can be suitable for the tele health care applications. This paper aim is as follows (i) Design and implementation of Parallel Fusion of FSM and Reconfigurable shared Distributed Arithmetic for Bio-Inspired Classifiers (ii) Development of Accelerator Environment to test the performance of proposed architecture (iii) Performance evaluation of proposed architecture in terms of accuracy of detection in compared with MATLAB simulation iv) Implementation of proposed architectures in different ARtix-7 architectures and determination of power, throughput and area . Moreover, the proposed architecture has been tested with the and compared with the other existing architectures."
66,unknown,10.1016/j.cmpb.2019.105277,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076945151,2/1/2020,an extensible software platform for interdisciplinary cardiovascular imaging research,"Background and objective
                  Cardiovascular imaging is an exponentially growing field with aspects ranging from image acquisition and analysis to disease characterization, and evaluation of therapy approaches.The transfer of innovative new technological and algorithmic solutions into clinical practice is still slow. In addition to the verification of solutions, their integration in the clinical processing workflow must be enabled for the assessment of clinical impact and risks. The goal of our software platform for cardiac image processing – CAIPI – is to support researchers from different specialties such as imaging physics, computer science, and medicine by a common extensible platform to address typical challenges and hurdles in interdisciplinary cardiovascular imaging research. It provides an integrated solution for method comparison, integrated analysis, and validation in the clinical context. The interface concept enables a combination with existing frameworks that address specific aspects of the pipeline, such as modeling (e.g., OpenCMISS, CARP) or image reconstruction (Gadgetron).
               
                  Methods
                  In our platform, we developed a concept for import, integration, and management of cardiac image data. The integration approach considers the spatiotemporal properties of the beating heart through a specific data model. The solution is based on MeVisLab and provides functionalities for data retrieval and storage. Two types of plugins can be added. While ToolPlugins usually provide processing algorithms such as image correction and segmentation, AnalysisPlugins enable interactive data exploration and reporting. GUI integration concepts are presented for both plugin types. We developed domain-specific reporting and visualization tools (e.g., AHA segment model) to enable validation studies by clinical experts. The platform offers plugins for calculating and reporting quantitative parameters such as cardiac function, which can be used to, e.g., evaluate the effect of processing algorithms on clinical parameters. Export functionalities include quantitative measurements to Excel, image data to PACS, and STL models to modeling and simulation tools.
               
                  Results
                  To demonstrate the applicability of this concept both for method development and clinical application, we present use cases representing different problems along the innovation chain in cardiac MR imaging.
                  Validation of an image reconstruction method (MRI T1 mapping)
                  Validation of an image correction method for real-time 2D-PC MRI
                  Comparison of quantification methods for blood flow analysis
                  Training and integration of machine learning solutions with expert annotations
                  Clinical studies with new imaging techniques (flow measurements in the carotid arteries and peripheral veins as well as cerebral spinal fluid).
               
                  Conclusion
                  The presented platform can be used in interdisciplinary teams, in which engineers or data scientists perform the method validation, followed by clinical research studies in patient collectives. The demonstrated use cases show how it enables the transfer of innovations through validation in the cardiovascular application context."
67,excluded,10.1016/j.gie.2019.03.019,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065917454,7/1/2019,quality assurance of computer-aided detection and diagnosis in colonoscopy,"Recent breakthroughs in artificial intelligence (AI), specifically via its emerging sub-field “deep learning,” have direct implications for computer-aided detection and diagnosis (CADe and/or CADx) for colonoscopy. AI is expected to have at least 2 major roles in colonoscopy practice—polyp detection (CADe) and polyp characterization (CADx). CADe has the potential to decrease the polyp miss rate, contributing to improving adenoma detection, whereas CADx can improve the accuracy of colorectal polyp optical diagnosis, leading to reduction of unnecessary polypectomy of non-neoplastic lesions, potential implementation of a resect-and-discard paradigm, and proper application of advanced resection techniques. A growing number of medical-engineering researchers are developing both CADe and CADx systems, some of which allow real-time recognition of polyps or in vivo identification of adenomas, with over 90% accuracy. However, the quality of the developed AI systems as well as that of the study designs vary significantly, hence raising some concerns regarding the generalization of the proposed AI systems. Initial studies were conducted in an exploratory or retrospective fashion by using stored images and likely overestimating the results. These drawbacks potentially hinder smooth implementation of this novel technology into colonoscopy practice. The aim of this article is to review both contributions and limitations in recent machine-learning-based CADe and/or CADx colonoscopy studies and propose some principles that should underlie system development and clinical testing."
68,unknown,10.1016/j.compeleceng.2017.03.009,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85015331423,1/1/2018,applying spark based machine learning model on streaming big data for health status prediction,"Machine learning is one of the driving forces of science and commerce, but the proliferation of Big Data demands paradigm shifts from traditional methods in the application of machine learning techniques on this voluminous data having varying velocity. With the availability of large health care datasets and progressions in machine learning techniques, computers are now well equipped in diagnosing many health issues. This work aims at developing a real time remote health status prediction system built around open source Big Data processing engine, the Apache Spark, deployed in the cloud which focus on applying machine learning model on streaming Big Data. In this scalable system, the user tweets his health attributes and the application receives the same in real time, extracts the attributes and applies machine learning model to predict user's health status which is then directly messaged to him/her instantly for taking appropriate action."
69,excluded,10.1016/j.ajo.2017.03.026,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85017447877,6/1/2017,using electronic health records to build an ophthalmologic data warehouse and visualize patients' data,"Purpose
                  To develop a near-real-time data warehouse (DW) in an academic ophthalmologic center to gain scientific use of increasing digital data from electronic medical records (EMR) and diagnostic devices.
               
                  Design
                  Database development.
               
                  Methods
                  Specific macular clinic user interfaces within the institutional hospital information system were created. Orders for imaging modalities were sent by an EMR-linked picture-archiving and communications system to the respective devices. All data of 325 767 patients since 2002 were gathered in a DW running on an SQL database. A data discovery tool was developed. An exemplary search for patients with age-related macular degeneration, performed cataract surgery, and at least 10 intravitreal (excluding bevacizumab) injections was conducted.
               
                  Results
                  Data related to those patients (3 142 204 diagnoses [including diagnoses from other fields of medicine], 720 721 procedures [eg, surgery], and 45 416 intravitreal injections) were stored, including 81 274 optical coherence tomography measurements. A web-based browsing tool was successfully developed for data visualization and filtering data by several linked criteria, for example, minimum number of intravitreal injections of a specific drug and visual acuity interval. The exemplary search identified 450 patients with 516 eyes meeting all criteria.
               
                  Conclusions
                  A DW was successfully implemented in an ophthalmologic academic environment to support and facilitate research by using increasing EMR and measurement data. The identification of eligible patients for studies was simplified. In future, software for decision support can be developed based on the DW and its structured data. The improved classification of diseases and semiautomatic validation of data via machine learning are warranted."
70,excluded,10.1016/j.procs.2016.09.052,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84992317444,1/1/2016,designing and testing healthtracker for activity recognition and energy expenditure estimation within the daphne platform,"This paper describes the design and evaluation of a mobile software library, HealthTracker, which aims to produce activity and energy expenditure estimations in real-time from accelerometer and gyroscope data provided by wearable sensors. Using feature extraction together with a classifier trained using machine learning, the system will automatically and periodically send all the produced estimations to a cloud-based platform that will allow later evaluation by both the user and a physician or caretaker. The system is presented within the DAPHNE platform, an ICT ecosystem designed to provide a means for remote health and lifestyle monitoring and guidance between physicians and their patients."
71,included,10.1184/r1/6710654.v1,,core,health,health' AND 'machine learning' AND ('real-world' AND 'deploy'),10.1184/r1/6710654.v1,6/30/2018 0:00,software and system health management for autonomous robotics missions,"Advanced autonomous robotics space missions rely heavily on the flawless interaction of complex hardware, multiple sensors, and a mission-critical software system.  This software system consists of an operating system, device drivers, controllers, and executives; recently highly complex AI-based autonomy software have also been introduced. Prior to launch, this software has to undergo rigorous verification and validation (V&V).  Nevertheless, dormant software bugs, failing sensors, unexpected hardware-software interactions, and unanticipated environmental conditions—likely on a space exploration mission—can cause major software faults that can endanger the entire mission.

Our Integrated Software Health Management (ISWHM) system continuously monitors the hardware sensors and the software in real-time. The ISWHM uses Bayesian networks, compiled to arithmetic circuits, to model software and hardware interactions. Advanced reasoning algorithms using arithmetic circuits not only enable the ISWHM to handle large, hierarchical models that are necessary in the realm of complex autonomous systems, but also enable efficient execution on small embedded processors. The latter capability is of extreme importance for small (mobile) autonomous units with limited computational power and low telemetry bandwidth.  In this paper, we discuss the requirements of ISWHM.  As our initial demonstration platform, we use a primitive Lego rover. A Lego 
Mindstorms microcontroller is used to implement a highly simplified autonomous rover driving system, running on the OSEK real-time operating system. We demonstrate that our ISWHM, running on this small embedded microcontroller, can perform fault detection as well as on-board reasoning for advanced diagnosis and root-cause detection in real time"
72,unknown,10.2118/204794-ms,"Day 4 Wed, December 01, 2021",semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d11c7341d3bcc4b0c73076048dcd10b35748f355,1/1/2021 0:00,satellite fields digitalization & als optimization with edge & advance analytics application,"
 Data monitoring in remote satellite field without any DOF platform is a challenging task but critical for ALS monitoring and optimization. In SRP wells the VFD data collection is important for analysis of downhole pump behavior and system health. SRP maintenance crew collects data from VFDs daily, but it is time consuming and can target only few wells in a day. The steps from requirement of dyna to final decision taken for ALS optimization are mobilizing team, permits approvals, download data, e-mail dynacards, dyna visualization, final decision.
 The problems with above process were: -
 Insufficient and discrete data for any post-failure analysis or ALS-optimization Minimal data to investigate the pre failure events
 The lack of real time monitoring was resulting in well downtime and associated production loss. The combination of IOT, Cloud Computing and Machine learning was implemented to shift from the reactive to proactive approach which helped in ALS Optimization and reduced production loss.
 The data was transmitted to a Cloud server and further it was transmitted to web-based app. Since thousands of Dynacards are generated in a day, hence it requires automated classification using computer driven pattern recognition techniques. The real time data is used for analysis involving basic statistic and Machine learning algorithms. The critical pump signatures were identified using machine learning libraries and email is generated for immediate action. Several informative dashboards were developed which provide quick analysis of ALS performance. The types of dashboard are as below
 Well Operational Status Dynacards Interpretation module SRP parameters visualization Machine Learning model calibration module Pump Performance Statistics
 After collection of enough data and creation of analytical dashboards on the three wells using domain knowledge the gained insights were used for ALS optimization. To keep the model in an evergreen high-confidence prediction state, inputs from domain experts are often required. After regular fine-tuning the prediction accuracy of the ML model increased to 80-85 %. In addition, system was made flexible so that a new algorithm can be deployed when required. Smart Alarms were generated involving statistic and Machine Learning by the system which gives alerts by e-mail if an abnormal behavior or erratic dynacards were identified. This helped in reduction of well downtime in some events which were treated instinctively before.
 The integration of domain knowledge and digitalization enables an engineer to take informed and effective decisions. The techniques discussed above can be implemented in marginal fields where DOF implementation is logistically and economically challenged. EDGE along with advanced analytics will gain more technological advances and can be used in other potential domains as well in near future."
73,unknown,10.3390/ijerph18137087,International journal of environmental research and public health,semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/176d31f73bc65e07dc6e650b75404c31666dcdbb,1/1/2021 0:00,design of a spark big data framework for pm2.5 air pollution forecasting,"In recent years, with rapid economic development, air pollution has become extremely serious, causing many negative effects on health, environment and medical costs. PM2.5 is one of the main components of air pollution. Therefore, it is necessary to know the PM2.5 air quality in advance for health. Many studies on air quality are based on the government’s official air quality monitoring stations, which cannot be widely deployed due to high cost constraints. Furthermore, the update frequency of government monitoring stations is once an hour, and it is hard to capture short-term PM2.5 concentration peaks with little warning. Nevertheless, dealing with short-term data with many stations, the volume of data is huge and is calculated, analyzed and predicted in a complex way. This alleviates the high computational requirements of the original predictor, thus making Spark suitable for the considered problem. This study proposes a PM2.5 instant prediction architecture based on the Spark big data framework to handle the huge data from the LASS community. The Spark big data framework proposed in this study is divided into three modules. It collects real time PM2.5 data and performs ensemble learning through three machine learning algorithms (Linear Regression, Random Forest, Gradient Boosting Decision Tree) to predict the PM2.5 concentration value in the next 30 to 180 min with accompanying visualization graph. The experimental results show that our proposed Spark big data ensemble prediction model in next 30-min prediction has the best performance (R2 up to 0.96), and the ensemble model has better performance than any single machine learning model. Taiwan has been suffering from a situation of relatively poor air pollution quality for a long time. Air pollutant monitoring data from LASS community can provide a wide broader monitoring, however the data is large and difficult to integrate or analyze. The proposed Spark big data framework system can provide short-term PM2.5 forecasts and help the decision-maker to take proper action immediately."
74,excluded,http://arxiv.org/abs/2207.03066v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2207.03066v1,7/7/2022 0:00,device-cloud collaborative recommendation via meta controller,"On-device machine learning enables the lightweight deployment of
recommendation models in local clients, which reduces the burden of the
cloud-based recommenders and simultaneously incorporates more real-time user
features. Nevertheless, the cloud-based recommendation in the industry is still
very important considering its powerful model capacity and the efficient
candidate generation from the billion-scale item pool. Previous attempts to
integrate the merits of both paradigms mainly resort to a sequential mechanism,
which builds the on-device recommender on top of the cloud-based
recommendation. However, such a design is inflexible when user interests
dramatically change:
  the on-device model is stuck by the limited item cache while the cloud-based
recommendation based on the large item pool do not respond without the new
re-fresh feedback.
  To overcome this issue, we propose a meta controller to dynamically manage
the collaboration between the on-device recommender and the cloud-based
recommender, and introduce a novel efficient sample construction from the
causal perspective to solve the dataset absence issue of meta controller. On
the basis of the counterfactual samples and the extended training, extensive
experiments in the industrial recommendation scenarios show the promise of meta
controller in the device-cloud collaboration."
75,excluded,http://arxiv.org/abs/2101.07831v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.07831v1,1/19/2021 0:00,"multi-task network pruning and embedded optimization for real-time
  deployment in adas","Camera-based Deep Learning algorithms are increasingly needed for perception
in Automated Driving systems. However, constraints from the automotive industry
challenge the deployment of CNNs by imposing embedded systems with limited
computational resources. In this paper, we propose an approach to embed a
multi-task CNN network under such conditions on a commercial prototype
platform, i.e. a low power System on Chip (SoC) processing four surround-view
fisheye cameras at 10 FPS.
  The first focus is on designing an efficient and compact multi-task network
architecture. Secondly, a pruning method is applied to compress the CNN,
helping to reduce the runtime and memory usage by a factor of 2 without
lowering the performances significantly. Finally, several embedded optimization
techniques such as mixed-quantization format usage and efficient data transfers
between different memory areas are proposed to ensure real-time execution and
avoid bandwidth bottlenecks. The approach is evaluated on the hardware
platform, considering embedded detection performances, runtime and memory
bandwidth. Unlike most works from the literature that focus on classification
task, we aim here to study the effect of pruning and quantization on a compact
multi-task network with object detection, semantic segmentation and soiling
detection tasks."
76,unknown,http://arxiv.org/abs/2011.09463v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.09463v3,11/18/2020 0:00,"easytransfer -- a simple and scalable deep transfer learning platform
  for nlp applications","The literature has witnessed the success of leveraging Pre-trained Language
Models (PLMs) and Transfer Learning (TL) algorithms to a wide range of Natural
Language Processing (NLP) applications, yet it is not easy to build an
easy-to-use and scalable TL toolkit for this purpose. To bridge this gap, the
EasyTransfer platform is designed to develop deep TL algorithms for NLP
applications. EasyTransfer is backended with a high-performance and scalable
engine for efficient training and inference, and also integrates comprehensive
deep TL algorithms, to make the development of industrial-scale TL applications
easier. In EasyTransfer, the built-in data and model parallelism strategies,
combined with AI compiler optimization, show to be 4.0x faster than the
community version of distributed training. EasyTransfer supports various NLP
models in the ModelZoo, including mainstream PLMs and multi-modality models. It
also features various in-house developed TL algorithms, together with the
AppZoo for NLP applications. The toolkit is convenient for users to quickly
start model training, evaluation, and online deployment. EasyTransfer is
currently deployed at Alibaba to support a variety of business scenarios,
including item recommendation, personalized search, conversational question
answering, etc. Extensive experiments on real-world datasets and online
applications show that EasyTransfer is suitable for online production with
cutting-edge performance for various applications. The source code of
EasyTransfer is released at Github (https://github.com/alibaba/EasyTransfer)."
77,unknown,http://arxiv.org/abs/2003.06700v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.06700v3,3/14/2020 0:00,"cocopie: making mobile ai sweet as pie --compression-compilation
  co-design goes a long way","Assuming hardware is the major constraint for enabling real-time mobile
intelligence, the industry has mainly dedicated their efforts to developing
specialized hardware accelerators for machine learning and inference. This
article challenges the assumption. By drawing on a recent real-time AI
optimization framework CoCoPIE, it maintains that with effective
compression-compiler co-design, it is possible to enable real-time artificial
intelligence on mainstream end devices without special hardware. CoCoPIE is a
software framework that holds numerous records on mobile AI: the first
framework that supports all main kinds of DNNs, from CNNs to RNNs, transformer,
language models, and so on; the fastest DNN pruning and acceleration framework,
up to 180X faster compared with current DNN pruning on other frameworks such as
TensorFlow-Lite; making many representative AI applications able to run in
real-time on off-the-shelf mobile devices that have been previously regarded
possible only with special hardware support; making off-the-shelf mobile
devices outperform a number of representative ASIC and FPGA solutions in terms
of energy efficiency and/or performance."
78,unknown,http://arxiv.org/abs/2003.02454v4,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.02454v4,3/5/2020 0:00,agl: a scalable system for industrial-purpose graph machine learning,"Machine learning over graphs have been emerging as powerful learning tools
for graph data. However, it is challenging for industrial communities to
leverage the techniques, such as graph neural networks (GNNs), and solve
real-world problems at scale because of inherent data dependency in the graphs.
As such, we cannot simply train a GNN with classic learning systems, for
instance parameter server that assumes data parallel. Existing systems store
the graph data in-memory for fast accesses either in a single machine or graph
stores from remote. The major drawbacks are in three-fold. First, they cannot
scale because of the limitations on the volume of the memory, or the bandwidth
between graph stores and workers. Second, they require extra development of
graph stores without well exploiting mature infrastructures such as MapReduce
that guarantee good system properties. Third, they focus on training but ignore
the optimization of inference over graphs, thus makes them an unintegrated
system.
  In this paper, we design AGL, a scalable, fault-tolerance and integrated
system, with fully-functional training and inference for GNNs. Our system
design follows the message passing scheme underlying the computations of GNNs.
We design to generate the $k$-hop neighborhood, an information-complete
subgraph for each node, as well as do the inference simply by merging values
from in-edge neighbors and propagating values to out-edge neighbors via
MapReduce. In addition, the $k$-hop neighborhood contains information-complete
subgraphs for each node, thus we simply do the training on parameter servers
due to data independency. Our system AGL, implemented on mature
infrastructures, can finish the training of a 2-layer graph attention network
on a graph with billions of nodes and hundred billions of edges in 14 hours,
and complete the inference in 1.2 hour."
79,unknown,http://arxiv.org/abs/1911.05771v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1911.05771v1,11/13/2019 0:00,"machine learning based network vulnerability analysis of industrial
  internet of things","It is critical to secure the Industrial Internet of Things (IIoT) devices
because of potentially devastating consequences in case of an attack. Machine
learning and big data analytics are the two powerful leverages for analyzing
and securing the Internet of Things (IoT) technology. By extension, these
techniques can help improve the security of the IIoT systems as well. In this
paper, we first present common IIoT protocols and their associated
vulnerabilities. Then, we run a cyber-vulnerability assessment and discuss the
utilization of machine learning in countering these susceptibilities. Following
that, a literature review of the available intrusion detection solutions using
machine learning models is presented. Finally, we discuss our case study, which
includes details of a real-world testbed that we have built to conduct
cyber-attacks and to design an intrusion detection system (IDS). We deploy
backdoor, command injection, and Structured Query Language (SQL) injection
attacks against the system and demonstrate how a machine learning based anomaly
detection system can perform well in detecting these attacks. We have evaluated
the performance through representative metrics to have a fair point of view on
the effectiveness of the methods."
80,excluded,http://arxiv.org/abs/1908.08998v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1908.08998v2,8/13/2019 0:00,aibench: an industry standard internet service ai benchmark suite,"Today's Internet Services are undergoing fundamental changes and shifting to
an intelligent computing era where AI is widely employed to augment services.
In this context, many innovative AI algorithms, systems, and architectures are
proposed, and thus the importance of benchmarking and evaluating them rises.
However, modern Internet services adopt a microservice-based architecture and
consist of various modules. The diversity of these modules and complexity of
execution paths, the massive scale and complex hierarchy of datacenter
infrastructure, the confidential issues of data sets and workloads pose great
challenges to benchmarking. In this paper, we present the first
industry-standard Internet service AI benchmark suite---AIBench with seventeen
industry partners, including several top Internet service providers. AIBench
provides a highly extensible, configurable, and flexible benchmark framework
that contains loosely coupled modules. We identify sixteen prominent AI problem
domains like learning to rank, each of which forms an AI component benchmark,
from three most important Internet service domains: search engine, social
network, and e-commerce, which is by far the most comprehensive AI benchmarking
effort. On the basis of the AIBench framework, abstracting the real-world data
sets and workloads from one of the top e-commerce providers, we design and
implement the first end-to-end Internet service AI benchmark, which contains
the primary modules in the critical paths of an industry scale application and
is scalable to deploy on different cluster scales. The specifications, source
code, and performance numbers are publicly available from the benchmark council
web site http://www.benchcouncil.org/AIBench/index.html."
81,excluded,http://arxiv.org/abs/1807.00139v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1807.00139v1,6/30/2018 0:00,harnessing constrained resources in service industry via video analytics,"Service industries contribute significantly to many developed and developing
- economies. As their business activities expand rapidly, many service
companies struggle to maintain customer's satisfaction due to sluggish service
response caused by resource shortages. Anticipating resource shortages and
proffering solutions before they happen is an effective way of reducing the
adverse effect on operations. However, this proactive approach is very
expensive in terms of capacity and labor costs. Many companies fall into
productivity conundrum as they fail to find sufficient strong arguments to
justify the cost of a new technology yet cannot afford not to invest in new
technologies to match up with competitors. The question is whether there is an
innovative solution to maximally utilize available resources and drastically
reduce the effect that the shortages of resources may cause yet achieving high
level of service quality at a low cost. This work demonstrates with a practical
analysis of a trolley tracking system we designed and deployed at Hong Kong
International Airport (HKIA) on how video analytics helps achieve management's
goal of satisfying customer's needs via real-time detection and prevention of
problems they may encounter during the service consumption process using
existing video technology rather than adopting new technologies. This paper
presents the integration of commercial video surveillance system with deep
learning algorithms for video analytics. We show that our system can provide
accurate decision when faced with total or partial occlusion with high accuracy
and it significantly improves daily operation. It is envisioned that this work
will heighten the appreciation of integrative technologies for resource
management within the service industries and as a measure for real-time
customer assistance."
82,excluded,10.1007/s11042-022-13514-7,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11042-022-13514-7,7/26/2022 0:00,bamcloud: a cloud based mobile biometric authentication framework,"There has been an exponential increase in the number of users switching to mobile banking. Therefore, various countries are adopting biometric solutions as security measures. Biometric technologies provide the potential security framework to make banking more convenient and secure than it has ever been. These technologies are gaining much popularity because of the ease in capturing biometric data in real-time using one’s mobile phone. At the same time, the exponential growth of enrollment in the biometric system produces a massive amount of high-dimensional data. To overcome performance-related issues arising due to the resulting data deluge, this paper aims to propose a distributed mobile biometric system based on a high-performance cluster Cloud. In this paper, a Cloud-based mobile biometric authentication framework (BAMCloud) is proposed that uses dynamic signatures for authentication. The process flow of the BAMCloud system involves capturing data using any handheld mobile device, followed by its storage, preprocessing, and training of the system in a distributed manner over the Cloud. MapReduce has been implemented on the Hadoop platform to reduce the processing time. For model training, The Levenberg-Marquardt backpropagation neural network has been used. It achieves a speed of 8.5 times the original speed and performance of 96.23%. Furthermore, the cost-benefit analysis of the implemented system shows that the cost of implementation and execution of the system is less than the existing ones. The experiments demonstrate that better performance is achieved by implementing the proposed framework as compared to other methods used in recent literature."
83,excluded,10.1007/978-981-16-5847-1_6,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-981-16-5847-1_6,1/1/2022 0:00,an architecture for quality centric crop production system,"Artificial Intelligence (AI) can be applied to find the patterns in historical data and help finding real-time predictions for making data-driven decisions. Digital transformation and automation are required for industries for better growth. AI-predictive analytics software solution that at its core, delivers the ability to recommend or select farm fields, practices, timing and inputs that have a high probability of delivering crops that meet an optimized set of quality attributes. These attributes allow the client to produce food with specific characteristics for competitive advantage and/or financial benefit. The system will allow growers to benefit by allowing “what-if” modeling of farming practices to anticipate desirable crop attributes. In this chapter, we proposed an architecture for developing a recommendation system for helping farmers produce crops with specific quality attributes which are required by the specific consumer. The recommendations can be generated by using the endogenous and exogenous data captured through IoT sensors and AI modeling. The architecture is generic and solutions can be designed to work for a range of foods/crops. The architecture is designed to develop solutions that deliver food producer’s a focused value proposition. It uses an Agcentric methodology in creating the capability to recommend the fields for crop production."
84,included,10.1109/syscon.2018.8369547,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8369547/,4/26/2018 0:00,an interactive architecture for industrial scale prediction: industry 4.0 adaptation of machine learning,"According to wiki definition, there are four design principles in Industry 4.0. These principles support companies in identifying and implementing Industry 4.0 scenarios, namely, Interoperability, Information transparency, Technical assistance, Decentralized decisions. In this paper we have discussed our work on an implementation of a machine learning based interactive architecture for industrial scale prediction for dynamic distribution of water resources across the continent, keeping the four corners of Industry 4.0 in place. We report the possibility of producing most probable high resolution estimation regarding the water balance in any region within Australia by implementation of an intelligent system that can integrate spatial-temporal data from various independent sensors and models, with the ground truth data produced by 250 practitioners from the irrigation industry across Australia. This architectural implementation on a cloud computing platform linked with a freely distributed mobile application, allowing interactive ground truthing of a machine learning model on a continental scale, shows accuracy of 90% with 85% sensitivity of correct surface soil moisture estimation with end users at its complete control. Along with high level of information transparency and interoperability, providing on-demand technical supports and motivating users by allowing them to customize and control their own local predictive models, show the successfulness of principles in Industry 4.0 in real environmental issues in the future adaptation in various industries starting from resource management to modern generation soft robotics."
85,included,10.1109/med.2017.7984310,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7984310/,7/6/2017 0:00,cloud computing for big data analytics in the process control industry,"The aim of this article is to present an example of a novel cloud computing infrastructure for big data analytics in the Process Control Industry. Latest innovations in the field of Process Analyzer Techniques (PAT), big data and wireless technologies have created a new environment in which almost all stages of the industrial process can be recorded and utilized, not only for safety, but also for real time optimization. Based on analysis of historical sensor data, machine learning based optimization models can be developed and deployed in real time closed control loops. However, still the local implementation of those systems requires a huge investment in hardware and software, as a direct result of the big data nature of sensors data being recorded continuously. The current technological advancements in cloud computing for big data processing, open new opportunities for the industry, while acting as an enabler for a significant reduction in costs, making the technology available to plants of all sizes. The main contribution of this article stems from the presentation for a fist time ever of a pilot cloud based architecture for the application of a data driven modeling and optimal control configuration for the field of Process Control. As it will be presented, these developments have been carried in close relationship with the process industry and pave a way for a generalized application of the cloud based approaches, towards the future of Industry 4.0."
86,excluded,10.1109/ains47559.2019.8968698,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8968698/,11/21/2019 0:00,cyber security risk assessment on industry 4.0 using ics testbed with ai and cloud,"Industry 4.0 is a new concept, thus risk assessment is necessary. Several risk assessment methods for Industrial Control System (ICS) and Industry 4.0 have been proposed, however, it is difficult to identify impacts on the physical world caused by cyber attacks against ICS since many of these are based on tabletop analysis or software simulations. Therefore, we focus on the risk assessment using actual machines (ICS testbed) which can help to solve the above problems. In Industry 4.0, autonomous judgment and execution are required for the cyber-physical system, it is based on information exchange using Artificial Intelligence (AI) and cloud technologies. In this research, we evaluate cyber risks through attacks against ICS with AI and cloud using ICS testbed. The proposed method can clarify cyber risks and impacts on the real world, and corresponding countermeasures."
87,excluded,10.1109/icsima47653.2019.9057343,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9057343/,8/29/2019 0:00,real-time wireless monitoring for three phase motors in industry: a cost-effective solution using iot,"In recent days modern environment industries are facing rapid flourishing for performance capabilities and their requirements for corporate clients and industrial sector. Internet of Things (IoT) is an innovative and rapidly growing field for automation and evaluation in networks, Artificial Intelligence, data sensing, data mining, and big data. These systems have a great tendency to monitor and control different process used in industries. IoT systems have been implemented and have applications in different industries due to their cost-effectiveness and flexibility In this paper we have developed a system which includes real-time monitoring of current reading of three-phase motor through a wireless network. With the help of this system, data can be saved and monitored and then transmitted to cloud storage. This system contains Arduino-UNO board, ACS-712 current sensor, ESP-8266 Wi-Fi module which sends information to an IoT API service THING-SPEAK that behave like a cloud for various sensors to monitor data. The proposed system was successfully deployed in Aisha Steel Mills, Karachi, Pakistan."
88,unknown,10.1109/wrc-sara.2019.8931920,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8931920/,8/22/2019 0:00,software-defined cloud manufacturing in the context of industry 4.0,"In the practice of &#x201C;Cloud Manufacturing (CMfg)&#x201D; or &#x201C;Industrial Internet&#x201D;, there still exist key problems, including: 1) big data analytics and decision-making in the cloud could not meet the requirements of time-sensitive manufacturing applications, moreover uploading ZettaBytes of future device data to the cloud may cause serious network congestion, 2) the manufacturing system lacks openness and evolvability, thus restricting the rapid optimization and transformation of the system, 3) big data from the shop-floor IoT devices and the internet has not been effectively utilized to guide the optimization and upgrade of the manufacturing system. In view of these key practical problems, we propose an open evolutionary architecture of intelligent CMfg system with collaborative edge and cloud processing capability. Hierarchical gateways near shop-floor things are introduced to enable fast processing for time-sensitive applications. Big data in another dimension from the software defined perspective will be used to decide the efficient operations and highly dynamic upgrade of the system. From the software system view, we also propose a new mode - AI-Mfg-Ops (AI-enabled Cloud Manufacturing Operations) with a supporting framework, which can promote the fast operation and upgrading of CMfg systems with AI enabled monitoring-analysis-planning-execution close loop. This work can improve the universality of CMfg for real-time fast response and operation &#x0026; upgrading."
89,unknown,10.1109/phm-paris.2019.00052,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8756426/,5/5/2019 0:00,a common service middleware for intelligent complex software system,"With the rapid development of the Internet of Things (IoT) and artificial intelligence (AI) technology, various intelligent complex software systems (i-CSS) are increasingly popular, becoming one of the most important software system development paradigms. Its inherent growth construction and adaptive evolution properties pose new challenges to existing software design and development methods. Especially, how to achieve growth construction by quickly reusing existing excellent software resources, and how to establish data flow across system boundaries around the business flow to achieve adaptive evolution based on data intelligence. Facing the above challenges, this paper proposes novel data-oriented analysis and design method (DOAD), microservice and container-based mashup development method (SCMD). On this basis, the paper implements i-CSS common service middleware to support the above methods in engineering. In a real cloud-based PHM system and the other three industry projects, the proposed methods and middleware are used for application verification, the results show that they can greatly reduce the complexity of i-CSS design and development, reduce the ability threshold of the i-CSS development team, improve the development efficiency of the development team, reduce the team development workload by 31.5% on average, and help the i-CSS team effectively cope with the challenges of growth construction and adaptive evolution."
90,unknown,10.1109/csci49370.2019.00084,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9071016/,12/7/2019 0:00,a real-time based intelligent system for predicting equipment status,"In manufacturing industry, significant productivity losses arise due to equipment failures. Therefore, it is an important task to prevent the equipment from failure by monitoring each machine's sensor data in advance. However, most of the current developed systems have been only focused on monitoring the sensor data and have a difficulty in applying advanced algorithms to the real-time stream data. To address issues, we implemented an intelligent system that employs real-time streaming engine loaded with the machine learning libraries for predictive maintenance analysis. By applying a deep-learning based model to the real-time streaming data, we can provide not only trends of raw sensor data but also give an indicator representing an equipment's status in real-time. We anticipate that our system contributes to recognize the equipment's status by monitoring the indicator for productivity improvement in manufacturing industry in real-time."
91,excluded,10.1109/icmla.2019.00115,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8999203/,12/19/2019 0:00,an edge computing visual system for vegetable categorization,"In self-service supermarket and retail industry, efforts to reduce customer wait time using automatic grocery item identification are challenged by low recognition accuracy, long response time and substantial requirement for equipment. In this paper, we propose a novel edge computing system named EdgeVegfru for vegetable and fruit image classification. While existing work on Vegfru dataset shows excellent performance, few of them have been deployed in real-world applications. We adopt an edge computing paradigm, design, implement and evaluate the whole system on the Android devices. The proposed deep learning model and quantization algorithm reduce the model size and inference time significantly. Our system has shown out-standing accuracy within limited time and computation resources, compared with other machine learning methods(such as Support Vector Machine(SVM), Random Forest(RF)), thus providing the potential path for automatic recognition and pricing in self-service retail stores."
92,excluded,10.1109/bigdata.2016.7840859,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7840859/,12/8/2016 0:00,building a research data science platform from industrial machines,"Data Science research has a long history in academia which spans from large-scale data management, to data mining and data analysis using technologies from database management systems (DBMS's). While traditional HPC offers tools on leveraging existing technologies with data processing needs, the large volume of data and the speed of data generation pose significant challenges. Using the Hadoop platform and tools built on top of it drew immense interest from academia after it gained success in industry. Georgia Institute of Technology received a donation of 200 compute nodes from Yahoo. Turning these industrial machines into a research Data Science Platform (DSP) poses unique challenges, such as: nontrivial hardware design decisions, configuration tool choices, node integration into existing HPC infrastructure, partitioning resource to meet different application needs, software stack choices, etc. We have 40 nodes up and running, 24 running as a Hadoop and Spark cluster, 12 running as a HBase and OpenTSDB cluster, the others running as service nodes. We successfully tested it against Spark Machine Learning algorithms using a 88GB image dataset, Spark DataFrame and GraphFrame with a Wikipedia dataset, and Hadoop MapReduce wordcount on a 300GB dataset. The OpenTSDB cluster is for real-time time series data ingestion and storage for sensor data. We are working on bringing up more nodes. We share our first-hand experience gained in our journey, which we believe will benefit and inspire other academic institutions."
93,unknown,10.1109/ehb50910.2020.9280165,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9280165/,10/30/2020 0:00,drivers’ drowsiness detection and warning systems for critical infrastructures,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events."
94,unknown,10.1109/csit49958.2020.9321954,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9321954/,9/26/2020 0:00,eco-friendly home automation system implemented using machine learning algorithms,"This paper presents the exemplary system of house automation implemented with the use of Industry 4.0 inventions. The proposed system tries to benefit from weather conditions to heat or cool the house without any electrical heaters or air conditioners. It is implemented with the aid of Machine Learning algorithms, the Internet of Things, and Cloud technology. The paper contains a technical and practical description of the system, the results of the real use, and proposed extensions that can improve the presented solution."
95,excluded,10.1109/bigdata.2018.8622583,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8622583/,12/13/2018 0:00,ensemble machine learning systems for the estimation of steel quality control,"Recent advances in the steel industry have encountered challenges in soliciting decision making solutions for quality control of products based on data mining techniques. In this paper, we present a steel quality control prediction system encompassing with real-world data as well as comprehensive data analysis results. The core process is cautiously designed as a regression problem, which is then best handled by grouping various learning algorithms with their massive resource of historical production datasets. The characteristics of the currently most popular learning models used in regression problem analysis are as well investigated and compared. The performance indicates our steel quality control prediction system based on ensemble machine learning model can offer promising result whilst delivering high usability for local manufacturers to address the production problem by aid of development of machine learning techniques. Furthermore, real-world deployment of this system is demonstrated and discussed. Finally, future directions and the performance expectation are pointed out."
96,excluded,10.1109/dcoss.2019.00079,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8804483/,5/31/2019 0:00,middleware for real-time event detection andpredictive analytics in smart manufacturing,"Industry 4.0 is a recent trend of automation for manufacturing technologies and represents the fourth industrial revolution which transforms current industrial processes with the use of technologies such as automation, data analytics, cyber-physical systems, IoT, artificial intelligence, etc. The vision of Industry 4.0 is to build an end-to-end industrial transformation with the support of digitization. Data analytics plays a key role to get a better understanding of business processes and to design intelligent decision support systems. However, a key challenge faced by industry is to integrate multiple autonomous processes, machines and businesses to get an integrated view for data analytics activities. Another challenge is to develop methods and mechanisms for real-time data acquisition and analytics on-the-fly. In this paper, we propose a semantically interoperable framework for historical data analysis combined with real-time data acquisition, event detection, and real-time data analytics for very precise production forecasting within a manufacturing unit. Besides historical data analysis techniques, our middleware is capable of collecting data from diverse autonomous applications and operations in real time using various IoT devices, analyzing the collected data on the fly, and evaluating the impact of any detected unexpected events. Using semantic technologies we integrate multiple autonomous systems (e.g. production system, supply chain management and open data). The outcome of real-time data analytics is used in combination with machine learning models trained over historical data in order to precisely forecast production in a manufacturing unit in real time. We also present our key findings and challenges faced while deploying our solution in real industrial settings for a large manufacturing unit."
97,included,10.23919/iconac.2019.8895095,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8895095/,9/7/2019 0:00,ant colony optimization algorithm for industrial robot programming in a digital twin,"Advanced manufacturing that is adaptable to constantly changing product designs often requires dynamic changes on the factory floor to enable manufacture. The integration of robotic manufacture with machine learning approaches offers the possibility to enable such dynamic changes on the factory floor. While ensuring safety and the possibility of losses of components and waste of material are against their usage. Furthermore, developments in design of virtual environments makes it possible to perform simulations in a virtual environment, to enable human-in-the-loop production of parts correctly the first time like never before. Such powerful simulation and control software provides the means to design a digital twin of manufacturing environment in which trials are completed at almost at no cost. In this paper, ant colony optimization is used to program an industrial robot to avoid obstacles and find its way to pick and place objects during an assembly task in an environment containing obstacles that must be avoided. The optimization is completed in a digital twin environment first and movements transferred to the real robot after human inspection. It is shown that the proposed methodology can find the optimal solution, in addition to avoiding collisions, for an assembly task with minimum human intervention."
98,included,10.1109/isie45063.2020.9152441,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9152441/,6/19/2020 0:00,deployment of a smart and predictive maintenance system in an industrial case study,"Industrial manufacturing environments are often characterized as being stochastic, dynamic and chaotic, being crucial the implementation of proper maintenance strategies to ensure the production efficiency, since the machines' breakdown leads to a degradation of the system performance, causing the loss of productivity and business opportunities. In this context, the use of emergent ICT technologies, such as Internet of Things (IoT), machine learning and augmented reality, allows to develop smart and predictive maintenance systems, contributing for the reduction of unplanned machines' downtime by predicting possible failures and recovering faster when they occur. This paper describes the deployment of a smart and predictive maintenance system in an industrial case study, that considers IoT and machine learning technologies to support the online and real-time data collection and analysis for the earlier detection of machine failures, allowing the visualization, monitoring and schedule of maintenance interventions to mitigate the occurrence of such failures. The deployed system also integrates machine learning and augmented reality technologies to support the technicians during the execution of maintenance interventions."
99,excluded,10.1109/ro-man50785.2021.9515431,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9515431/,8/12/2021 0:00,simplifying the a.i. planning modeling for human-robot collaboration,"For an effective deployment in manufacturing, Collaborative Robots should be capable of adapting their behavior to the state of the environment and to keep the user safe and engaged during the interaction. Artificial Intelligence (AI) enables robots to autonomously operate understanding the environment, planning their tasks and acting to achieve some given goals. However, the effective deployment of AI technologies in real industrial environments is not straightforward. There is a need for engineering tools facilitating communication and interaction between AI engineers and Domain experts. This paper proposes a novel software tool, called TENANT (Tool fostEriNg Ai plaNning in roboTics) whose aim is to facilitate the use of AI planning technologies by providing domain experts like e.g., production engineers, with a graphical software framework to synthesize AI planning models abstracting from syntactic features of the underlying planning formalism."
100,excluded,10.1109/his.2011.6122180,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6122180/,12/8/2011 0:00,neural network prognostics model for industrial equipment maintenance,"This paper presents a new prognostics model based on neural network technique for supporting industrial maintenance decision. In this study, the probabilities of failure based on the real condition equipment are initially calculated by using logistic regression method. The failure probabilities are subsequently utilized as input for prognostics model to predict the future value of failure condition and then used to estimate remaining useful lifetime of equipment. By having a time series of predicted failure probability, the failure distribution can be generated and used in the maintenance cost model to decide the optimal time to do maintenance. The proposed prognostic model is implemented in the industrial equipment known as autoclave burner. The result from the model reveals that it can give prior warnings and indication to the maintenance department to take an appropriate decision instead of dealing with the failures while the autoclave burner is still operating. This significant contribution provides new insights into the maintenance strategy which enables the use of existing condition data from industrial equipment and prognostics approach."
101,unknown,10.1109/rtsi50628.2021.9597339,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9597339/,9/9/2021 0:00,towards graph machine learning for smart grid knowledge graphs in industrial scenarios,"Knowledge Graphs (KGs) demonstrated promising application perspective in different scenarios, especially when combined with Graph Machine Learning (GML) techniques able to interpret and infer over facts. Given the natural network structures of Smart Grid equipment and the exponential growth of electric power data, Smart Grid Knowledge Graphs (SGKGs) provides unprecedented opportunities to manage massive power resources and provide intelligent applications. However, a single representation of the SGKGs is never sufficient to properly exploit GML techniques that leverage different aspects of the KG for various objectives. In this work, we provide a methodology to extract various significant views of the SGKG by iteratively applying a series of transformation to the description of the power network in the IEC CIM standard. Our implementation is based on a declarative approach to guarantee easier portability, and we deploy the transformations as a stateless microservice, facilitating modular integration with the rest of the Smart Grid Semantic Platform. Experimental evaluation on two real power distribution networks demonstrates the efficacy of our approach in highlighting important topological information, without discarding precious additional knowledge present in the SGKG."
102,included,10.1109/fdl53530.2021.9568376,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9568376/,9/10/2021 0:00,a container-based design methodology for robotic applications on kubernetes edge-cloud architectures,"Programming modern Robots&#x0027; missions and behavior has become a very challenging task. The always increasing level of autonomy of such platforms requires the integration of multi-domain software applications to implement artificial intelligence, cognition, and human-robot/robot-robot interaction applications. In addition, to satisfy both functional and nonfunctional requirements such as reliability and energy efficiency, robotic SW applications have to be properly developed to take advantage of heterogeneous (Edge-Fog-Cloud) architectures. In this context, containerization and orchestration are becoming a standard practice as they allow for better information flow among different network levels as well as increased modularity in the use of software components. Nevertheless, the adoption of such a practice along the design flow, from simulation to the deployment of complex robotic applications by addressing the de-facto development standards (i.e., robotic operating system - ROS - compliancy for robotic applications) is still an open problem. We present a design methodology based on Docker and Kubernetes that enables containerization and orchestration of ROS-based robotic SW applications for heterogeneous and hierarchical HW architectures. The design methodology allows for (i) integration and verification of multi-domain components since early in the design flow, (ii) task-to-container mapping techniques to guarantee minimum overhead in terms of performance and memory footprint, and (iii) multi-domain verification of functional and non-functional constraints before deployment. We present the results obtained in a real case of study, in which the design methodology has been applied to program the mission of a Robotnik RB-Kairos mobile robot in an industrial agile production chain. The source code of the mobile robot is publicly available on GitHub."
103,excluded,10.1109/icct46805.2019.8947193,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8947193/,10/19/2019 0:00,edge ai for heterogeneous and massive iot networks,"By combining multiple sensing and wireless access technologies, the Internet of Things (IoT) shall exhibit features with large-scale, massive, and heterogeneous sensors and data. To integrate diverse radio access technologies, we present the architecture of heterogeneous IoT system for smart industrial parks and build an IoT experimental platform. Various sensors are installed on the IoT devices deployed on the experimental platform. To efficiently process the raw sensor data and realize edge artificial intelligence (AI), we describe four statistical features of the raw sensor data that can be effectively extracted and processed at the network edge in real time. The statistical features are calculated and fed into a back-propagation neural network (BPNN) for sensor data classification. By comparing to the k-nearest neighbor classification algorithm, we examine the BPNN-based classification method with a great amount of raw data gathered from various sensors. We evaluate the system performance according to the classification accuracy of BPNN and the performance indicators of the cloud server, which shows that the proposed approach can effectively enable the edge-AI-based heterogeneous IoT system to process the sensor data at the network edge in real time while reducing the demand for computing and network resources of the cloud."
104,excluded,10.1109/ijcnn52387.2021.9533808,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9533808/,7/22/2021 0:00,end-to-end federated learning for autonomous driving vehicles,"In recent years, with the development of computation capability in devices, companies are eager to investigate and utilize suitable ML/DL methods to improve their service quality. However, with the traditional learning strategy, companies need to first build up a powerful data center to collect and analyze data from the edge and then perform centralized model training, which turns out to be inefficient. Federated Learning has been introduced to solve this challenge. Because of its characteristics such as model-only exchange and parallel training, the technique can not only preserve user data privacy but also accelerate model training speed. The method can easily handle real-time data generated from the edge without taking up a lot of valuable network transmission resources. In this paper, we introduce an approach to end-to-end on-device Machine Learning by utilizing Federated Learning. We validate our approach with an important industrial use case in the field of autonomous driving vehicles, the wheel steering angle prediction. Our results show that Federated Learning can significantly improve the quality of local edge models and also reach the same accuracy level as compared to the traditional centralized Machine Learning approach without its negative effects. Furthermore, Federated Learning can accelerate model training speed and reduce the communication overhead, which proves that this approach has great strength when deploying ML/DL components to various real-world embedded systems."
105,excluded,10.1109/iciss.2010.5656975,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5656975/,10/24/2010 0:00,research and implementation of the temperature control system of heat treatment based on .net and rs-485 bus,"RS-485 is a widely used industrial field bus. The successful application of AIBUS protocol in AI series display control instrument, make the AIDCS system's cost significantly lower than traditional DCS system. The paper successfully developed a prototype system based on RS-485 bus for high precision temperature control system of heat treatment, and based on the .net and AIBUS protocol, developed it' s system management software. This system have the characteristics of good real-time, high control-precision, high degree of automation, and friendly human-machine interface."
106,excluded,10.23919/aeit.2018.8577226,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8577226/,10/5/2018 0:00,smart farms for a sustainable and optimized model of agriculture,"Nowadays, public and private companies, are in a constant race to increase profitability, chasing the costs reduction while facing the market competition. Also in the agriculture an analysis of cost-effectiveness, measuring technological innovation and profitability becomes necessary. The `smart farm' model exploits information coming from technologies like sensors, intelligent systems and the Internet of Things (IoT) paradigm to understand the influential and non-influential factors while considering environmental, productive and structural data coming from a large number of sources. The goal of this work is to design and deploy practical tasks that exploit heterogeneous real datasets with the aim to forecast and reconstruct values using and comparing innovative machine learning techniques with more standard ones. The application of these methodologies, in fields that are only apparently refractory to the technology such as the agricultural one, shows that there are ample margins for innovation and investment while supporting requests and needs coming from companies that wish to employ a sustainable and optimized agricultural industrial business."
107,excluded,10.1016/j.elerap.2021.101098,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85119699964,11/1/2021,an intelligent knowledge-based chatbot for customer service,"This study proposes an intelligent knowledge-based conversational agent system architecture to support customer services in e-commerce sales and marketing. A pilot implementation of a chatbot for customer services is reported in a leading women’s intimate apparel manufacturing firm. The proposed system incorporates various emerging technologies, including web crawling, natural language processing, knowledge bases, and artificial intelligence. In this study, a prototype system is built in a real-world setting. The results of the system prototype evaluation are satisfactory and support the contention that the system is effective. The study also discusses the challenges and lessons learned during system implementation and the theoretical and managerial implications of this study."
108,excluded,10.1016/j.asoc.2021.107465,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105315919,9/1/2021,click-event sound detection in automotive industry using machine/deep learning,"In the automotive industry, despite the robotic systems on the production lines, factories continue employing workers in several custom tasks getting for semi-automatic assembly operations. Specifically, the assembly of electrical harnesses of engines comprises a set of connections between electrical components. Despite the task is easy to perform, employees tend not to notice that a few components are not being connected properly due to physical fatigue provoked by repetitive tasks. This yields a low quality of the assembly production line and possible hazards. In this work, we propose a sound detection system based on machine/deep learning (ML/DL) approaches to identify click sounds produced when electrical harnesses are connected. The purpose of this system is to count the number of connections properly made and to feedback to the employees. We collect and release a public dataset of 25,000 click sounds of 25 ms length at 22 kHz during three months of assembly operations in an automotive production line located in Mexico. Then, we design an ML/DL-based methodology for click sound detection of assembled harnesses under real conditions of a noisy environment (noise level ranging from 
                        
                           −
                           16
                           .
                           67
                        
                      dB to 
                        
                           −
                           12
                           .
                           87
                        
                      dB) including other machinery sounds. Our best ML/DL model (i.e., a combination between five acoustic features and an optimized convolutional neural network) is able to detect click sounds in a real assembly production line with an accuracy of 
                        
                           94
                           .
                           55
                           ±
                           0
                           .
                           83
                        
                      %. To the best of our knowledge, this is the first time a click sounds detection system in assembling electrical harnesses of engines for giving feedback to the workers is proposed and implemented in a real-world automotive production line. We consider this work valuable for the automotive industry on how to apply ML/DL approaches for improving the quality of semi-automatic assembly operations."
109,included,10.1016/j.jmsy.2021.04.005,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85106283308,7/1/2021,learningadd: machine learning based acoustic defect detection in factory automation,"Defect inspection of glass bottles in the beverage industrial is of significance to prevent unexpected losses caused by the damage of bottles during manufacturing and transporting. The commonly used manual methods suffer from inefficiency, excessive space consumption, and beverage wastes after filling. To replace the manual operations in the pre-filling detection with improved efficiency and reduced costs, this paper proposes a machine learning based Acoustic Defect Detection (LearningADD) system. Moreover, to realize scalable deployment on edge and cloud computing platforms, deployment strategies especially partitioning and allocation of functionalities need to be compared and optimized under realistic constraints such as latency, complexity, and capacity of the platforms. In particular, to distinguish the defects in glass bottles efficiently, the improved Hilbert-Huang transform (HHT) is employed to extend the extracted feature sets, and then Shuffled Frog Leaping Algorithm (SFLA) based feature selection is applied to optimize the feature sets. Five deployment strategies are quantitatively compared to optimize real-time performances based on the constraints measured from a real edge and cloud environment. The LearningADD algorithms are validated by the datasets from a real-life beverage factory, and the F-measure of the system reaches 98.48 %. The proposed deployment strategies are verified by experiments on private cloud platforms, which shows that the Distributed Heavy Edge deployment outperforms other strategies, benefited from the parallel computing and edge computing, where the Defect Detection Time for one bottle is less than 2.061 s in 99 % probability."
110,included,10.1016/j.enconman.2021.113856,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85101129959,4/1/2021,the mutual benefits of renewables and carbon capture: achieved by an artificial intelligent scheduling strategy,"Renewable power and carbon capture are key technologies to transfer the power industry into low carbon generation. Renewables have been developed fast, however, the intermittent nature has imposed higher requirement for the flexibility of the power grid. Retrofitting carbon capture technologies to existing fossil-fuel fired power plants is an important solution to avoid the “lock-in” of emissions, but the high operating costs hinders their large scale application. The coexistence of renewable power and carbon capture opens up a new avenue that the deployment of carbon capture can provide additional flexibility for better accommodation of renewable power while excess renewables can be used to reduce the operating costs of carbon capture. To this end, this paper proposes an artificial intelligence based optimal scheduling strategy for the power plant-carbon capture system in the context of renewable power penetration to show that the mutual benefits between carbon capture and renewable power can be achieved when the carbon capture process is made fully adjustable. An artificial intelligent deep belief neural network is used to reflect the complex interactions between carbon, heat and electricity within the power plant carbon capture system. Multiple operating goals are considered in the scheduling such as minimizing the operating costs, renewable power curtailment and carbon emission, and the particle swarm heuristic optimization is employed to find the optimal solution. The impacts of carbon capture constraint mode, carbon emission penalty coefficient, carbon dioxide production constraints and renewable power installed capacity are investigated to provide broader insight on the potential benefit of carbon capture in future low-carbon energy system. A case study using real world data of weather condition and load demand shows that renewable power curtailment can be reduced by 51% with the integration of post-combustion capture systems and 35% of total carbon emission are captured by the use of excess renewable power through optimal scheduling. This paper points out a new way of using artificial intelligent technologies to coordinate the couplings between carbon and electricity for efficient and environmentally friendly operation of future low-carbon energy system."
111,unknown,10.1016/j.procs.2021.08.095,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85116946450,1/1/2021,soda: a real-time simulation framework for object detection and analysis in smart manufacturing,"For modern manufacturing firms, automation has already become a norm but constantly needs to be improved as firms still face strong demand to increase their productivity. This can be achieved by reducing dependability on manpower, reaching lean and even unmanned production and this is where some of the standards of Industry 4.0 come in useful, not to mention: Machine Vision, Image Recognition or Machine Learning. In our paper, we present SODA – our approach to build a flexible ML and AI enabled framework for object detection, analysis, and simulation. The framework is designed to support a development process of solutions requiring real-time analysis of images of different types of moving objects on a conveyor belt. In our work we discuss architectural challenges of the developed framework as well as the basic components of the system. We do also provide information on how to use the framework and present a sample implementation of an actual system employing some of the machine learning methods."
112,excluded,10.1016/j.compind.2020.103329,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85092922057,12/1/2020,a middleware platform for intelligent automation: an industrial prototype implementation,"The development of dynamic data-based Decision Support Systems (DSSs) along with the increasing availability of data in the industry, makes real-time data acquisition and management a challenge. Intelligent automation appears as a holistic combination of automation with analytics and decisions made by artificial intelligence, delivering smart manufacturing and mass customization while improving resource efficiency. However, challenges towards the development of intelligent automation architectures include the lack of interoperability between systems, complex data preparation steps, and the inability to deal with both high-frequency and high-volume data in a timely fashion. This paper contributes to industrial frameworks focused on the development of standardized system architectures for Industry 4.0, closing the gap between generic architectures and physical realizations. It proposes a platform for intelligent automation relying on a gateway or middleware between field devices, enterprise databases, and DSSs in real-time scenarios. This is achieved by providing the middleware interoperability, determinism, and automatic data structuring over an industrial communication infrastructure such as the OPC UA Standard over Time Sensitive Networks (TSN). Cloud services and database warehousing used to address some of the challenges are handled using fog computing and a multi-workload database. This paper presents an implementation of the platform in the pharmaceutical industry, providing interoperability and real-time reaction capability to changes to an industrial prototype using dynamic scheduling algorithms."
113,excluded,10.1016/j.compind.2020.103244,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85084401966,9/1/2020,machine learning for predictive scheduling and resource allocation in large scale manufacturing systems,"The digitalization processes in manufacturing enterprises and the integration of increasingly smart shop floor devices and software control systems caused an explosion in the data points available in Manufacturing Execution Systems. The degree in which enterprises can capture value from big data processing and extract useful insights represents a differentiating factor in developing controls that optimize production and protect resources. Machine learning and Big Data technologies have gained increased traction being adopted in some critical areas of planning and control. Cloud manufacturing allows using these technologies in real time, lowering the cost of implementing and deployment. In this context, the paper offers a machine learning approach for reality awareness and optimization in cloud.
                  Specifically, the paper focuses on predictive production planning (operation scheduling, resource allocation) and predictive maintenance. The main contribution of this research consists in developing a hybrid control solution that uses Big Data techniques and machine learning algorithms to process in real time information streams in large scale manufacturing systems, focusing on energy consumptions that are aggregated at various layers. The control architecture is distributed at the edge of the shop floor for data collecting and format transformation, and then centralized at the cloud computing platform for data aggregation, machine learning and intelligent decisions. The information is aggregated in logical streams and consolidated based on relevant metadata; a neural network is trained and used to determine possible anomalies or variations relative to the normal patterns of energy consumption at each layer. This novel approach allows for accurate forecasting of energy consumption patterns during production by using Long Short-term Memory neural networks and deep learning in real time to re-assign resources (for batch cost optimization) and detect anomalies (for robustness) based on predicted energy data."
114,included,10.1016/j.adhoc.2019.102047,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076174369,3/1/2020,an intelligent edge-iot platform for monitoring livestock and crops in a dairy farming scenario,"Today’s globalized and highly competitive world market has broadened the spectrum of requirements in all the sectors of the agri-food industry. This paper focuses on the dairy industry, on its need to adapt to the current market by becoming more resource efficient, environment-friendly, transparent and secure. The Internet of Things (IoT), Edge Computing (EC) and Distributed Ledger Technologies (DLT) are all crucial to the achievement of those improvements because they allow to digitize all parts of the value chain, providing detailed information to the consumer on the final product and ensuring its safety and quality. In Smart Farming environments, IoT and DLT enable resource monitoring and traceability in the value chain, allowing producers to optimize processes, provide the origin of the produce and guarantee its quality to consumers. In comparison to a centralized cloud, EC manages the Big Data generated by IoT devices by processing them at the network edge, allowing for the implementation of services with shorter response times, and a higher Quality of Service (QoS) and security. This work presents a platform oriented to the application of IoT, Edge Computing, Artificial Intelligence and Blockchain techniques in Smart Farming environments, by means of the novel Global Edge Computing Architecture, and designed to monitor the state of dairy cattle and feed grain in real time, as well as ensure the traceability and sustainability of the different processes involved in production. The platform is deployed and tested in a real scenario on a dairy farm, demonstrating that the implementation of EC contributes to a reduction in data traffic and an improvement in the reliability in communications between the IoT-Edge layers and the Cloud."
115,included,10.1016/j.procs.2020.03.044,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85085566175,1/1/2020,an artificial intelligence based crowdsensing solution for on-demand accident scene monitoring,"Road traffic crashes have a devastating impact on societies by claiming more than 1.35 million lives each year and causing up to 50 million injuries. Improving the efficiency of emergency management systems constitutes a key measure to reduce road traffic deaths and injuries. In this work, we propose a comprehensive crowdsensing-based solution for the real-time collection and the analysis of accident scene intelligence as a means to improve the efficiency of the emergency response process and help reduce road fatalities. The solution leverages sensory, mobile, and web technologies for the real-time monitoring of accident scenes, and employs Artificial Intelligence for the automatic analysis of the accident scene data, to allow the automatic generation of accident intelligence reports. Police officers and rescue teams can use those reports for fast and accurate situational assessment and effective response to emergencies. The proposed system was fully implemented and its operation was successfully tested using a variety of scenarios. This work gives interesting insights into the possibility of leveraging crowdsensing and artificial intelligence for offering emergency situational awareness and improving the efficiency of emergency response operations."
116,unknown,10.1016/j.aei.2020.101044,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85078852726,1/1/2020,iot edge computing-enabled collaborative tracking system for manufacturing resources in industrial park,"In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability."
117,unknown,10.1016/j.aei.2019.101013,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85075778987,1/1/2020,guidelines for applied machine learning in construction industry—a case of profit margins estimation,"The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects."
118,excluded,10.1016/j.cie.2019.106031,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85071975175,11/1/2019,machine learning based concept drift detection for predictive maintenance,"In this work we present a machine learning based approach for detecting drifting behavior – so-called concept drifts – in continuous data streams. The motivation for this contribution originates from the currently intensively investigated topic Predictive Maintenance (PdM), which refers to a proactive way of triggering servicing actions for industrial machinery. The aim of this maintenance strategy is to identify wear and tear, and consequent malfunctioning by analyzing condition monitoring data, recorded by sensor equipped machinery, in real-time. Recent developments in this area have shown potential to save time and material by preventing breakdowns and improving the overall predictability of industrial processes. However, due to the lack of high quality monitoring data and only little experience concerning the applicability of analysis methods, real-world implementations of Predictive Maintenance are still rare. Within this contribution, we present a method, to detect concept drift in data streams as potential indication for defective system behavior and depict initial tests on synthetic data sets. Further on, we present a real-world case study with industrial radial fans and discuss promising results gained from applying the detailed approach in this scope."
119,excluded,10.1016/j.compind.2019.04.010,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065732680,8/1/2019,managing workflow of customer requirements using machine learning,"Customer requirements – product specifications issued by the customer – organize the dialog between suppliers and customers and, hence, affect the dynamics of supply networks. These large and complex documents are frequently updated over time, while changes are seldom marked by the customers who issue the requirements. The lack of structure and defined responsibilities, thus, demands an expert to manually process the requirements. Here, the possibility to improve the usual workflow with machine learning algorithms is explored.
                  The whole requirements management process has two major bottlenecks, which can be automatized. The first one, detecting changes, can be accomplished via a document comparison tool. The second one, recognizing the responsibilities and assigning them to the right department, can be solved with standard machine learning algorithms. Here, such algorithms are applied to a dataset obtained from a global automotive industry supplier.
                  The proposed method improves the requirements management process by reducing an expert’s workload and thus decreasing the time for processing one document was reduced from 2 weeks to 1 h. Moreover, the method gives a high accuracy of department assignment and can self-improve once implemented into a requirements management system.
                  Although the machine learning methods are very popular nowadays, they are seldom used to improve business processes in real companies, especially in the case of processes that did not require digitalization in the past. Here we show, how such methods can solve some of the management problems and improve their workflow."
120,unknown,10.1016/j.promfg.2018.12.026,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85072561400,1/1/2019,hybrid artificial intelligence system for the design of highly-automated production systems,"The automated design of production systems is a young field of research which has not been widely explored by industry nor research in recent decades. Currently, the effort spent in production system design is increasing significantly in automotive industry due to the number of product variants and product complexity. Intelligent methods can support engineers in repetitive tasks and give them more opportunity to focus on work which requires their core competencies. This paper presents a novel artificial intelligence methodology that automatically generates initial production system configurations based on real industrial scenarios in the automotive field of body-in-white production. The hybrid methodology reacts flexibly against data sets of different content and has been implemented in a software prototype."
121,excluded,10.1016/j.procir.2019.02.101,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85065424368,1/1/2019,autonomous order dispatching in the semiconductor industry using reinforcement learning,"Cyber Physical Production Systems (CPPS) provide a huge amount of data. Simultaneously, operational decisions are getting ever more complex due to smaller batch sizes, a larger product variety and complex processes in production systems. Production engineers struggle to utilize the recorded data to optimize production processes effectively because of a rising level of complexity. This paper shows the successful implementation of an autonomous order dispatching system that is based on a Reinforcement Learning (RL) algorithm. The real-world use case in the semiconductor industry is a highly suitable example of a cyber physical and digitized production system."
122,excluded,10.1016/j.procir.2018.03.022,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85049587790,1/1/2018,fostering robust human-robot collaboration through ai task planning,"Recent advances in Artificial Intelligence (AI) are facilitating the deployment of intelligent systems in manufacturing. In Human-Robot Collaboration (HRC), industrial robots offer accuracy and efficiency while humans guarantee both experience and specialized and not replaceable skills. The seamless coordination of such different abilities constitutes one of the current challenges. This paper presents a dynamic task sequencing system for robust HRC developed within a EU-funded project. The proposed solution uses AI techniques to deal with the temporal variance entailed by the active presence of humans as well as to dynamically adapt task plans according to actual behavior of the pair human-worker/robot. The tool has been deployed in a real pilot plant."
123,excluded,http://arxiv.org/abs/2104.04076v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2104.04076v1,4/1/2021 0:00,"an artificial intelligence and internet of things based automated
  irrigation system","It is not hard to see that the need for clean water is growing by considering
the decrease of the water sources day by day in the world. Potable fresh water
is also used for irrigation, so it should be planned to decrease freshwater
wastage. With the development of technology and the availability of cheaper and
more effective solutions, the efficiency of irrigation increased and the water
loss can be reduced. In particular, Internet of things (IoT) devices has begun
to be used in all areas. We can easily and precisely collect temperature,
humidity and mineral values from the irrigation field with the IoT devices and
sensors. Most of the operations and decisions about irrigation are carried out
by people. For people, it is hard to have all the real-time data such as
temperature, moisture and mineral levels in the decision-making process and
make decisions by considering them. People usually make decisions with their
experience. In this study, a wide range of information from the irrigation
field was obtained by using IoT devices and sensors. Data collected from IoT
devices and sensors sent via communication channels and stored on MongoDB. With
the help of Weka software, the data was normalized and the normalized data was
used as a learning set. As a result of the examinations, a decision tree (J48)
algorithm with the highest accuracy was chosen and an artificial intelligence
model was created. Decisions are used to manage operations such as starting,
maintaining and stopping the irrigation. The accuracy of the decisions was
evaluated and the irrigation system was tested with the results. There are
options to manage, view the system remotely and manually and also see the
system s decisions with the created mobile application."
124,unknown,http://arxiv.org/abs/2103.13997v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.13997v1,3/25/2021 0:00,real-time low-resource phoneme recognition on edge devices,"While speech recognition has seen a surge in interest and research over the
last decade, most machine learning models for speech recognition either require
large training datasets or lots of storage and memory. Combined with the
prominence of English as the number one language in which audio data is
available, this means most other languages currently lack good speech
recognition models.
  The method presented in this paper shows how to create and train models for
speech recognition in any language which are not only highly accurate, but also
require very little storage, memory and training data when compared with
traditional models. This allows training models to recognize any language and
deploying them on edge devices such as mobile phones or car displays for fast
real-time speech recognition."
125,unknown,http://arxiv.org/abs/2102.12165v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.12165v1,2/24/2021 0:00,"efficient low-latency dynamic licensing for deep neural network
  deployment on edge devices","Along with the rapid development in the field of artificial intelligence,
especially deep learning, deep neural network applications are becoming more
and more popular in reality. To be able to withstand the heavy load from
mainstream users, deployment techniques are essential in bringing neural
network models from research to production. Among the two popular computing
topologies for deploying neural network models in production are
cloud-computing and edge-computing. Recent advances in communication
technologies, along with the great increase in the number of mobile devices,
has made edge-computing gradually become an inevitable trend. In this paper, we
propose an architecture to solve deploying and processing deep neural networks
on edge-devices by leveraging their synergy with the cloud and the
access-control mechanisms of the database. Adopting this architecture allows
low-latency DNN model updates on devices. At the same time, with only one model
deployed, we can easily make different versions of it by setting access
permissions on the model weights. This method allows for dynamic model
licensing, which benefits commercial applications."
126,excluded,http://arxiv.org/abs/2102.02638v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.02638v1,2/2/2021 0:00,"autodidactic neurosurgeon: collaborative deep inference for mobile edge
  intelligence via online learning","Recent breakthroughs in deep learning (DL) have led to the emergence of many
intelligent mobile applications and services, but in the meanwhile also pose
unprecedented computing challenges on resource-constrained mobile devices. This
paper builds a collaborative deep inference system between a
resource-constrained mobile device and a powerful edge server, aiming at
joining the power of both on-device processing and computation offloading. The
basic idea of this system is to partition a deep neural network (DNN) into a
front-end part running on the mobile device and a back-end part running on the
edge server, with the key challenge being how to locate the optimal partition
point to minimize the end-to-end inference delay. Unlike existing efforts on
DNN partitioning that rely heavily on a dedicated offline profiling stage to
search for the optimal partition point, our system has a built-in online
learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn
the optimal partition point on-the-fly. Therefore, ANS is able to closely
follow the changes of the system environment by generating new knowledge for
adaptive decision making. The core of ANS is a novel contextual bandit learning
algorithm, called $\mu$LinUCB, which not only has provable theoretical learning
performance guarantee but also is ultra-lightweight for easy real-world
implementation. We implement our system on a video stream object detection
testbed to validate the design of ANS and evaluate its performance. The
experiments show that ANS significantly outperforms state-of-the-art benchmarks
in terms of tracking system changes and reducing the end-to-end inference
delay."
127,included,http://arxiv.org/abs/2101.04930v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.04930v2,1/13/2021 0:00,"an empirical study on deployment faults of deep learning based mobile
  applications","Deep Learning (DL) is finding its way into a growing number of mobile
software applications. These software applications, named as DL based mobile
applications (abbreviated as mobile DL apps) integrate DL models trained using
large-scale data with DL programs. A DL program encodes the structure of a
desirable DL model and the process by which the model is trained using training
data. Due to the increasing dependency of current mobile apps on DL, software
engineering (SE) for mobile DL apps has become important. However, existing
efforts in SE research community mainly focus on the development of DL models
and extensively analyze faults in DL programs. In contrast, faults related to
the deployment of DL models on mobile devices (named as deployment faults of
mobile DL apps) have not been well studied. Since mobile DL apps have been used
by billions of end users daily for various purposes including for
safety-critical scenarios, characterizing their deployment faults is of
enormous importance. To fill the knowledge gap, this paper presents the first
comprehensive study on the deployment faults of mobile DL apps. We identify 304
real deployment faults from Stack Overflow and GitHub, two commonly used data
sources for studying software faults. Based on the identified faults, we
construct a fine-granularity taxonomy consisting of 23 categories regarding to
fault symptoms and distill common fix strategies for different fault types.
Furthermore, we suggest actionable implications and research avenues that could
further facilitate the deployment of DL models on mobile devices."
128,included,http://arxiv.org/abs/2008.05255v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2008.05255v1,8/12/2020 0:00,"identity-aware attribute recognition via real-time distributed inference
  in mobile edge clouds","With the development of deep learning technologies, attribute recognition and
person re-identification (re-ID) have attracted extensive attention and
achieved continuous improvement via executing computing-intensive deep neural
networks in cloud datacenters. However, the datacenter deployment cannot meet
the real-time requirement of attribute recognition and person re-ID, due to the
prohibitive delay of backhaul networks and large data transmissions from
cameras to datacenters. A feasible solution thus is to employ mobile edge
clouds (MEC) within the proximity of cameras and enable distributed inference.
In this paper, we design novel models for pedestrian attribute recognition with
re-ID in an MEC-enabled camera monitoring system. We also investigate the
problem of distributed inference in the MEC-enabled camera network. To this
end, we first propose a novel inference framework with a set of distributed
modules, by jointly considering the attribute recognition and person re-ID. We
then devise a learning-based algorithm for the distributions of the modules of
the proposed distributed inference framework, considering the dynamic
MEC-enabled camera network with uncertainties. We finally evaluate the
performance of the proposed algorithm by both simulations with real datasets
and system implementation in a real testbed. Evaluation results show that the
performance of the proposed algorithm with distributed inference framework is
promising, by reaching the accuracies of attribute recognition and person
identification up to 92.9% and 96.6% respectively, and significantly reducing
the inference delay by at least 40.6% compared with existing methods."
129,unknown,http://arxiv.org/abs/2002.11045v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2002.11045v1,2/22/2020 0:00,"deep learning for ultra-reliable and low-latency communications in 6g
  networks","In the future 6th generation networks, ultra-reliable and low-latency
communications (URLLC) will lay the foundation for emerging mission-critical
applications that have stringent requirements on end-to-end delay and
reliability. Existing works on URLLC are mainly based on theoretical models and
assumptions. The model-based solutions provide useful insights, but cannot be
directly implemented in practice. In this article, we first summarize how to
apply data-driven supervised deep learning and deep reinforcement learning in
URLLC, and discuss some open problems of these methods. To address these open
problems, we develop a multi-level architecture that enables device
intelligence, edge intelligence, and cloud intelligence for URLLC. The basic
idea is to merge theoretical models and real-world data in analyzing the
latency and reliability and training deep neural networks (DNNs). Deep transfer
learning is adopted in the architecture to fine-tune the pre-trained DNNs in
non-stationary networks. Further considering that the computing capacity at
each user and each mobile edge computing server is limited, federated learning
is applied to improve the learning efficiency. Finally, we provide some
experimental and simulation results and discuss some future directions."
130,excluded,http://arxiv.org/abs/1901.04985v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1901.04985v1,1/12/2019 0:00,"nnstreamer: stream processing paradigm for neural networks, toward
  efficient development and execution of on-device ai applications","We propose nnstreamer, a software system that handles neural networks as
filters of stream pipelines, applying the stream processing paradigm to neural
network applications. A new trend with the wide-spread of deep neural network
applications is on-device AI; i.e., processing neural networks directly on
mobile devices or edge/IoT devices instead of cloud servers. Emerging privacy
issues, data transmission costs, and operational costs signifies the need for
on-device AI especially when a huge number of devices with real-time data
processing are deployed. Nnstreamer efficiently handles neural networks with
complex data stream pipelines on devices, improving the overall performance
significantly with minimal efforts. Besides, nnstreamer simplifies the neural
network pipeline implementations and allows reusing off-shelf multimedia stream
filters directly; thus it reduces the developmental costs significantly.
Nnstreamer is already being deployed with a product releasing soon and is open
source software applicable to a wide range of hardware architectures and
software platforms."
131,unknown,http://arxiv.org/abs/1808.07647v4,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1808.07647v4,8/23/2018 0:00,"machine learning at the edge: a data-driven architecture with
  applications to 5g cellular networks","The fifth generation of cellular networks (5G) will rely on edge cloud
deployments to satisfy the ultra-low latency demand of future applications. In
this paper, we argue that such deployments can also be used to enable advanced
data-driven and Machine Learning (ML) applications in mobile networks. We
propose an edge-controller-based architecture for cellular networks and
evaluate its performance with real data from hundreds of base stations of a
major U.S. operator. In this regard, we will provide insights on how to
dynamically cluster and associate base stations and controllers, according to
the global mobility patterns of the users. Then, we will describe how the
controllers can be used to run ML algorithms to predict the number of users in
each base station, and a use case in which these predictions are exploited by a
higher-layer application to route vehicular traffic according to network Key
Performance Indicators (KPIs). We show that the prediction accuracy improves
when based on machine learning algorithms that rely on the controllers' view
and, consequently, on the spatial correlation introduced by the user mobility,
with respect to when the prediction is based only on the local data of each
single base station."
132,excluded,http://arxiv.org/abs/1806.07761v3,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1806.07761v3,6/20/2018 0:00,"quick and plenty: achieving low delay and high rate in 802.11ac edge
  networks","We consider transport layer approaches for achieving high rate, low delay
communication over edge paths where the bottleneck is an 802.11ac WLAN. We
first show that by regulating send rate so as to maintain a target aggregation
level it is possible to realise high rate, low delay communication over
802.11ac WLANs. We then address two important practical issues arising in
production networks, namely that (i) many client devices are non-rooted mobile
handsets/tablets and (ii) the bottleneck may lie in the backhaul rather than
the WLAN, or indeed vary between the two over time. We show that both these
issues can be resolved by use of simple and robust machine learning techniques.
We present a prototype transport layer implementation of our low delay rate
allocation approach and use this to evaluate performance under real radio
conditions."
133,included,http://arxiv.org/abs/1604.04384v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1604.04384v2,4/15/2016 0:00,the strands project: long-term autonomy in everyday environments,"Thanks to the efforts of the robotics and autonomous systems community,
robots are becoming ever more capable. There is also an increasing demand from
end-users for autonomous service robots that can operate in real environments
for extended periods. In the STRANDS project we are tackling this demand
head-on by integrating state-of-the-art artificial intelligence and robotics
research into mobile service robots, and deploying these systems for long-term
installations in security and care environments. Over four deployments, our
robots have been operational for a combined duration of 104 days autonomously
performing end-user defined tasks, covering 116km in the process. In this
article we describe the approach we have used to enable long-term autonomous
operation in everyday environments, and how our robots are able to use their
long run times to improve their own performance."
134,included,10.1007/978-3-030-28925-6_1,Springer,springer,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-28925-6_1,1/1/2020 0:00,cityflow: supporting spatial-temporal edge computing for urban machine learning applications,"A growing trend in smart cities is the use of machine learning techniques to gather city data, formulate learning tasks and models, and use these to develop solutions to city problems. However, although these processes are sufficient for theoretical experiments, they often fail when they meet the reality of city data and processes, which by their very nature are highly distributed, heterogeneous, and exhibit high degrees of spatial and temporal variance. In order to address those problems, we have designed and implemented an integrated development environment called CityFlow that supports developing machine learning applications. With CityFlow, we can develop, deploy, and maintain machine learning applications easily by using an intuitive data flow model. To verify our approach, we conducted two case studies: deploying a road damage detection application to help monitor transport infrastructure and an automatic labeling application in support of a participatory sensing application. These applications show both the generic applicability of our approach, and its ease of use; both critical if we wish to deploy sophisticated ML based applications to smart cities."
135,included,10.1109/access.2020.3010609,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9144582/,1/1/2020 0:00,a privacy-aware crowd management system for smart cities and smart buildings,"Cities are growing at a dizzying pace and they require improved methods to manage crowded areas. Crowd management stands for the decisions and actions taken to supervise and control densely populated spaces and it involves multiple challenges, from recognition and assessment to application of actions tailored to the current situation. To that end, Wi-Fi-based monitoring systems have emerged as a cost-effective solution for the former one. The key challenge that they impose is the requirement to handle large datasets and provide results in near real-time basis. However, traditional big data and event processing approaches have important shortcomings while dealing with crowd management information. In this paper, we describe a novel system architecture for real-time crowd recognition for smart cities and smart buildings that can be easily replicated. The described system proposes a privacy-aware platform that enables the application of artificial intelligence mechanisms to assess crowds' behavior in buildings employing sensed Wi-Fi traces. Furthermore, the present paper shows the implementation of the system in two buildings, an airport and a market, as well as the results of applying a set of classification algorithms to provide crowd management information."
136,excluded,10.1109/atc52653.2021.9598291,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9598291/,10/16/2021 0:00,an edge-ai heterogeneous solution for real-time parking occupancy detection,"In the digital era, building smart cities is a highly desired goal that every country strives to achieve. With the advancement of technology, many smart city systems have been developed at a rapid rate of which Smart Parking is emerging as one of the core components. Smart Parking promises to automate the parking process, thereby saving time, resources and effort for searching an optimal parking space as well as reducing traffic congestion and population. As one of the newly emerging and disrupting technology, Artificial Intelligence, Machine Learning and Deep Learning (AI/ML/DL) are being utilized in many aspects of developing a Smart Parking system. In this paper, we propose an solution for accelerating AI/ML/DL algorithms deployed on low-cost System-on-Chip platforms (SoCs), which are often used as edge devices in Smart Parking system. In particular, we leverage Binary Neural Network (BNN), one of the most advanced deep learning models, to build a heterogeneous algorithm for real-time identifying parking occupancy based on the integration of SoCs and existing surveillance systems. The proposed solution is implemented and evaluated in Zynq UltraScale+ MPSoC with high accuracy (approx. 87%), low latency (avg. 16ms) and high frame per second (FPS) rate."
137,unknown,10.1109/iotais.2018.8600904,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8600904/,11/3/2018 0:00,smart air quality monitoring system with lorawan,"Nowadays, cities all over the globe are transforming into smart cities. Smart cities initiatives need to address environmental concerns such as air pollution to provide clean air. A scalable and cost-effective air monitoring system is imperative to monitor and control air pollution for smart city development. Air pollution has notable effects on the well-being of the population a whole, global atmosphere, and worldwide economy. This paper presents a scalable smart air quality monitoring system with low-cost sensors and long-range communication protocol. The sensors collect four parameters, temperature, humidity, dust and carbon dioxide in the air. The proposed end-to-end system has been implemented and deployed in Yangon, the business capital of Myanmar, as a case study since Jun 2018. The system allows the users to log in to an online dashboard to monitor the real-time status. In addition, based the collected air quality parameters for the past two months, a machine learning model has been trained to make predictions of parameters such that proactive actions can be taken to alleviate the impacts from air pollution."
138,unknown,10.1109/isc2.2016.7580869,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7580869/,9/15/2016 0:00,urbansense: an urban-scale sensing platform for the internet of things,"A critical step towards smarter and safer cities is to endow them with the abilities to massively gather a wide variety of data sets and to automatically feed those data to decision support tools and applications that leverage artificial intelligence. We present UrbanSense, a platform deployed on the streets of a mid-size European city (Porto, Portugal) to collect key environmental data. The main innovations of UrbanSense are (1) design for affordability and extensibility, (2) its ability to leverage heterogeneous networks to send the data to the cloud (using both real-time and delay-tolerant communications), and (3) its Internet of Things integration to expose the data streams to smart city tools and applications. Beyond discussing the design choices, we present operational results for 6 months of operation and give a detailed account of the challenges faced by the successful deployment of urban sensing technologies in the wild."
139,excluded,10.1109/tenconspring.2017.8070078,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8070078/,7/16/2017 0:00,identifying uncollected garbage in urban areas using crowdsourcing and machine learning,"Waste management in Urban Cities of India is a serious concern with growing amounts of uncollected garbage on the streets. In this paper, we present a mobile application based solution to empower the citizens to report instances of uncollected garbage and draw the attention of authorities. Our application has been successfully deployed and has seen more than a million complaints registered across many Indian cities. One of the challenges we have had to address on deployment of the application was the presence of a number of spurious complaints, such complaints result in unnecessary work by the municipal authorities. We tackled the challenge by using machine learning techniques to identify spurious complaints. We have been able to achieve an accuracy of over 85% in segregating the spurious complaints from actual ones by analyzing the image uploaded along with the complaint. The application can be used in real time once the model is trained."
140,excluded,10.1109/vtcfall.2017.8288311,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8288311/,9/27/2017 0:00,towards an application for real-time travel mode detection in urban centers,"Context-aware applications in intelligent transportation systems have a growing need for travel mode detection systems. However, few applications allow real-time travel mode detection through the use of smartphones. In this paper, we propose a real-time travel mode detection application based on GPS traces using a data mining technique through which these traces are preprocessed, grouped in motion segments and classified by supervised machine learning algorithms. An application prototype was implemented on the Android platform, used by smartphones, for movement data collection and user travel mode detection using the WEKA API in Java. Finally, to evaluate the performance of the application in a real environment, field tests were carried out with dozens of volunteers in the metropolitan area of Rio de Janeiro. Therefore, 1338 travel mode inferences were obtained by four machine learning techniques and the results were evaluated and compared by the indicators of the confusion matrix. Thus, through the performance evaluation carried out, it was possible to verify that the proposed application is useful for real-time travel mode detection in urban centers."
141,excluded,10.1109/access.2020.3015655,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9163328/,1/1/2020 0:00,a two-layer water demand prediction system in urban areas based on micro-services and lstm neural networks,"In recent years, scarce water resources became one of the main problems that endanger human species existence and the advancement of any nation. In this research, smart water meters were implemented, distributed, and installed in a regional area in Cairo while data were collected at uniform intervals then sent to the cloud instantly. The solution paradigm uses an Internet of Things (IoT) based on micro-services and containers. The design incorporates real-time streaming and infrastructure performance optimization to store data. A second layer to analyze the acquired data was used to model water consumption using Long Short-Term Memory (LSTM). The designed LSTM is validated and tested to be utilized in the forecast of future water demand. Moreover, two alternative machine learning methods, namely Support Vector Regression and Random Forest commonly utilized in time series forecasting applications, were used for a comparative analysis of which LSTM has proven to be superior. The proper integration of the system elements is the key to the proposed system success. Based on the success of the designed system, it can be applicable on a national scale. That can enable the optimal management of consumers' demand and improve water infrastructure utilization. The proposed paradigm presents a testbed for various scenarios that can be used in water resources management."
142,included,10.1109/icecce49384.2020.9179349,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9179349/,6/13/2020 0:00,a cloud based smart recycling bin for in-house waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. Most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification for personal in-house usage. A centralized Information System (IS) collects measurements from smart bins that can be deployed virtually anywhere and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low compared to other implementations."
143,included,10.1109/mocast49295.2020.9200283,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9200283/,9/9/2020 0:00,a cloud based smart recycling bin for waste classification,"Due to the Earth's population rapid growth along with the modern lifestyle the urban waste constantly increases. People consume more and the products are designed to have shorter lifespans. Recycling is the only way to make a sustainable environment. The process of recycling requires the separation of waste materials, which is a time consuming procedure. However, most of the proposed research works found in literature are neither budget-friendly nor effective to be practical in real world applications. In this paper, we propose a solution: a low-cost and effective Smart Recycling Bin that utilizes the power of cloud to assist with waste classification. A centralized Information System (IS) collects measurements from smart bins that are deployed all around the city and classifies the waste of each bin using Artificial Intelligence and neural networks. Our implementation is capable of classifying different types of waste with an accuracy of 93.4% while keeping deployment cost and power consumption very low."
144,excluded,10.1109/icac51239.2020.9357161,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9357161/,12/11/2020 0:00,deepfake audio detection: a deep learning based solution for group conversations,"The recent advancements in deep learning and other related technologies have led to improvements in various areas such as computer vision, bio-informatics, and speech recognition etc. This research mainly focuses on a problem with synthetic speech and speaker diarization. The developments in audio have resulted in deep learning models capable of replicating natural-sounding voice also known as text-to-speech (TTS) systems. This technology could be manipulated for malicious purposes such as deepfakes, impersonation, or spoofing attacks. We propose a system that has the capability of distinguishing between real and synthetic speech in group conversations.We built Deep Neural Network models and integrated them into a single solution using different datasets, including but not limited to Urban-Sound8K (5.6GB), Conversational (12.2GB), AMI-Corpus (5GB), and FakeOrReal (4GB). Our proposed approach consists of four main components. The speech-denoising component cleans and preprocesses the audio using Multilayer- Perceptron and Convolutional Neural Network architectures, with 93% and 94% accuracies accordingly. The speaker diarization was implemented using two different approaches, Natural Language Processing for text conversion with 93% accuracy and Recurrent Neural Network model for speaker labeling with 80% accuracy and 0.52 Diarization-Error-Rate. The final component distinguishes between real and fake audio using a CNN architecture with 94 % accuracy. With these findings, this research will contribute immensely to the domain of speech analysis."
145,unknown,10.1109/tensymp52854.2021.9550904,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9550904/,8/25/2021 0:00,deep learning based smart parking for a metropolitan area,"In this study, we have introduced a method for utilizing the maximum parking space available for a metropolitan city. This will result in much lesser traffic congestion due to street-side parking. Furthermore, it will also decrease the hassle drivers face when they have to leave their vehicles on the side of the road to do other activities. The method introduces a Deep Learning based system where parking spaces are detected using Data Capturing Units (DCU). These DCUs feed data into our database which can be accessed by the users from our mobile application. The users can book parking spaces accordingly. All these data are saved in real-time and can be accessed through the mobile application. A vehicle classification system has also been designed that achieves an accuracy of 77% from multiple vehicle classes. Furthermore, a number plate recognition system has been used for the identification and safety protocols of the vehicles in parking sites. The number plate identification system is very precise and achieves an accuracy of over 90% for each digit. To the best of our knowledge, no other system of this kind has been implemented for the city of Dhaka before this. On top of that, successful implementation in a hectic city like Dhaka implies that it can be applied anywhere in the world. We believe this system can have a huge impact in reducing traffic congestions and can save an endless measure of time and money for citizens in a metropolitan area."
146,excluded,10.1109/icws.2017.76,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8029817/,6/30/2017 0:00,early air pollution forecasting as a service: an ensemble learning approach,"Air quality has become a major global concern for human beings involving all social stratums, for both developing and developed countries. Web service of precise and early air pollution forecasting is of great importance as it allows people to pro-actively take preventative and protective measurements. As an endeavor on the course of machine learning based air quality forecasting, this paper presents an initiative and its technological details in solving this challenging problem. Specifically, this work involves three major highlights regarding with both algorithmic innovation and deployment with its impact: 1) We propose a multi-channel ensemble learning framework, 2) We propose a new supervised feature learning and extraction method, i.e. sufficient statistics feature mapping based on Deep Boltzman Machine, which serves as a building block for our learning system, 3) We target our air pollution prediction method to the city of Beijing, China as it is at the forefront for battling against air pollution, which is embodied as a web service for prediction. Extensive experiments of real time air pollution forecasting on the real-world data demonstrates the effectiveness of the proposed method and value of the deployed web service system."
147,included,10.1109/vlsid51830.2021.00035,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9407373/,2/24/2021 0:00,binary neural network based real time emotion detection on an edge computing device to detect passenger anomaly,"Passenger safety in public transportation especially while riding in the form of shared cabs, and taxis are often ignored, and not much preventive protocols are devised. In the connected mobility world, emotion recognition from facial expressions is a possibility, however a faster processing and edge computing device to derive anomaly state inferences will be apt for further notifying about the safety of the passenger. FPGA implementation is a viable approach to not only implement in the embedded system automotive electronics, but also accelerate the inference results, hence making it as an ideal real time candidate for passenger anomaly state identification. For the same, a real time emotion detection system using facial features was implemented on FPGA. A Binary Neural Network (BNN) feeded by Local Binary Pattern (LBP) output was designed towards the development of an improved and faster emotion recognition system. LBP is configured as a preprocessing step to extract facial features that is passed on to the BNN layer for successful inference. The preprocessing method utilizes Viola-Jones (VJ) algorithm to extract facial data while removing other background information from the image. The LBP-BNN network is modelled using Facial Expression 2013 (FER-2013) data set for training. The custom hardware accelerator or the overlay is synthesized and the designed IP is implemented on FPGA for the inference. Inference is done using the trained model on FPGA to enable faster classified results. Emotion detection using facial expressions is classified to six states namely: angry, disgust, fear, happy, sad, and surprise. The LBP-BNN network is implemented in FPGA, to realize a real time facial emotion recognition by capturing the image of a person from a web camera interfaced to the FPGA acting as edge computing inference device, with acceptable accuracy. The image processing based emotion detection design is highly suitable for other applications including tracking of emotions for movement disorder patients in hospitals."
148,included,10.1109/camad.2018.8515001,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8515001/,9/19/2018 0:00,uncertainty management for wearable iot wristband sensors using laplacian-based matrix completion,"Contemporary sensing devices provide reliable mechanisms for continuous process monitoring, accommodating use cases related to mHealth and smart mobility, by generating real-time data streams of numerous physiological and vital parameters. Such data streams can be later utilized by machine learning algorithms and decision support systems to predict critical clinical states and motivate users to adopt behaviours that improve the quality of their life and the society as a whole. However, in many cases, even when deployed over highly sophisticated, cutting-edge network infrastructure and deployment paradigms, data may exhibit missing values and non-uniformities due to various reasons, including device malfunction, deliberate data reduction for efficient processing, or data loss due to sensing and communication failures. This work proposes a novel approach to deal with missing entries in heart rate measurements. Benefiting from the low-rank property of the generated data matrices and the proximity of neighbouring measurements, we provide a novel method that combines classical matrix completion approaches with weighted Laplacian interpolation offering high reconstruction accuracy at fast execution times. Extensive evaluation studies carried out with real measurements show that the proposed methods could be effectively deployed by modern wristband-cloud computing systems increasing the robustness, the reliability and the energy efficiency of these systems."
149,excluded,10.1016/j.procs.2021.01.245,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105671943,1/1/2021,water wise - a digital water solution for smart cities and water management entities,"Efficient water management of the urban water cycle is one of the current concerns with the increase of (peri) urban areas due to the population growth, economic development, and possibility of water scarcity due to climate change. To face the increase of the water demand it is imperative the creation of digital water solutions to provide a real-time monitoring, decision support system, to manage the water supply network efficiently and optimize the water-energy nexus. This paper presents a Water Wise System – W2S, results from a R&D project supported by an EU and Portuguese Government Grant. The paper provides a preliminary study of an architecture solution to Water Wise System software, focuses on the water challenges, present technology, digital water, IoT and the future of smart cities. The solution aims to support a paradigm shift in the management of water distribution networks, with predictive and analytical convergence supported in Machine Learning, Deep Learning and an integration with SCADA, GIS and EPANET."
150,excluded,10.1016/j.glt.2020.09.004,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85102077498,1/1/2020,development of an iot based real-time traffic monitoring system for city governance,"A significant amount of research work carried out on traffic management systems, but intelligent traffic monitoring is still an active research topic due to the emerging technologies such as the Internet of Things (IoT) and Artificial Intelligence (AI). The integration of these technologies will facilitate the techniques for better decision making and achieve urban growth. However, the existing traffic prediction methods mostly dedicated to highway and urban traffic management, and limited studies focused on collector roads and closed campuses. Besides, reaching out to the public, and establishing active connections to assist them in decision-making is challenging when the users are not equipped with any smart devices. This research proposes an IoT based system model to collect, process, and store real-time traffic data for such a scenario. The objective is to provide real-time traffic updates on traffic congestion and unusual traffic incidents through roadside message units and thereby improve mobility. These early-warning messages will help citizens to save their time, especially during peak hours. Also, the system broadcasts the traffic updates from the administrative authorities. A prototype is implemented to evaluate the feasibility of the model, and the results of the experiments show good accuracy in vehicle detection and a low relative error in road occupancy estimation. The study is part of the Omani-funded research project, investigating Real-Time Feedback for Adaptive Traffic Signals."
151,excluded,10.1016/j.procs.2020.09.009,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85093365315,1/1/2020,passenger bibo detection with iot support and machine learning techniques for intelligent transport systems,"The present article discusses the issue of automation of the CICO (Check-In/Check-Out) process for public transport fare collection systems, using modern tools forming part of the Internet of Things, such as Beacon and Smartphone. It describes the concept of an integrated passenger identification model applying machine learning technology in order to reduce or eliminate the risks associated with the incorrect classification of a smartphone user as a vehicle passenger. This will allow for the construction of an intelligent fare collection system, operating in the BIBO (Be-In/Be-Out) model, implementing the ""hands-free"" and ""pay-as-you-go"" approach. The article describes the architecture of the research environment, and the implementation of the elaborated model in the Bad.App4 proprietary solution. We also presented the complete process of concept verification under real-life conditions. Research results were described and supplemented with commentary."
152,excluded,10.1016/b978-0-12-814601-9.00020-1,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85072183737,11/16/2018,wearable systems for improving tourist experience,"In this chapter we present original approaches for the development of a smart audio-guide that adapts to the actions and interests of visitors of cultural heritage sites and exhibitions either in indoor or outdoor scenarios. The guide is capable of perceiving the context. It understands what the user is looking at, if he is moving or is inattentive (e.g. talking with someone), in order to provide relevant information at the appropriate timing. Automatic recognition of artworks is performed with different approaches depending on the scenario, i.e. indoor and outdoor. These approaches are, respectively, based on Convolutional Neural Network (CNN) and SIFT descriptors, performing, when appropriate, object localization and classification. The computer-vision system works in real time on the mobile device, exploiting also a fusion of audio and motion sensors. Configurable interfaces to ease interaction and fruition of multimedia insights are provided for both scenarios. The audio-guide has been deployed on a NVIDIA Jetson TX1 and a NVIDIA Shield Tablet K1, tested in a real world environment (Bargello Museum of Florence and the historical city center of Florence), and evaluated with regard to system usability."
153,excluded,10.1016/j.jnca.2017.09.001,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85032219805,2/1/2018,an intelligent power distribution service architecture using cloud computing and deep learning techniques,"Smart management of power consumption for green living is important for sustainable development. Existing approaches could not provide a complete solution for both smart monitoring of electricity consumption, and also intelligent processing of the collected data effectively. This paper presents a cloud-based intelligent power distribution service architecture, where an intelligent electricity box (IEB) is designed using Zigbee and Raspberry Pi, and a standard MQTT (Message Queuing Telemetry Transport) protocol is used to transfer monitored data to the backend Cloud computing infrastructure using open source software packages. The IEB provides cloud services of real-time electricity information checking, power consumption monitoring, and remote control of switches. The current and historical data are stored in HBase and analyzed using Long Short Term Memory (LSTM). Evaluations and practical usage show that our proposed solution is very efficient in terms of availability, performance, and the deep learning based approach has better prediction accuracy than that of both classical SVR based approach and the latest XGBoost approach."
154,excluded,10.1109/percomworkshops51409.2021.9431061,2021 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e9201bea3bd785e962697f5c3ddd3a30348cbc48,1/1/2021 0:00,ultra-fast machine learning classifier execution on iot devices without sram consumption,"With the introduction of edge analytics, IoT devices are becoming smart and ready for AI applications. A few modern ML frameworks are focusing on the generation of small-size ML models (often in kBs) that can directly be flashed and executed on tiny IoT devices, particularly the embedded systems. Edge analytics eliminates expensive device-to-cloud communications, thereby producing intelligent devices that can perform energy-efficient real-time offline analytics. Any increase in the training data results in a linear increase in the size and space complexity of the trained ML models, making them unable to be deployed on IoT devices with limited memory. To alleviate the memory issue, a few studies have focused on optimizing and fine-tuning existing ML algorithms to reduce their complexity and size. However, such optimization is usually dependent on the nature of IoT data being trained. In this paper, we presented an approach that protects model quality without requiring any alteration to the existing ML algorithms. We propose SRAM-optimized implementation and efficient deployment of widely used standard/stable ML-frameworks classifier versions (e.g., from Python scikit-learn). Our initial evaluation results have demonstrated that ours is the most resource-friendly approach, having a very limited memory footprint while executing large and complex ML models on MCU-based IoT devices, and can perform ultra-fast classifications while consuming 0 bytes of SRAM. When we tested our approach by executing it on a variety of MCU-based devices, the majority of models ported and executed produced 1-4x times faster inference results in comparison with the models ported by the sklearn-porter, m2cgen, and emlearn libraries."
155,excluded,10.1109/isce.2018.8408911,2018 International Symposium on Consumer Technologies (ISCT),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/f6ff5cb091a3e27e67893744cd2995ad6fef3407,1/1/2018 0:00,low-cost real-time non-intrusive appliance identification and controlling through machine learning algorithm,"The existing power generation sources are unable to meet the hyper escalating electricity demand. Common solution is to install new generation plants to fulfill the electricity demand which it is not cost-effective. A cheap and effective solution is to monitor all the appliances running inside a building and to use them efficiently. Non-intrusive load monitoring (NILM) is one of the economical techniques to identify the appliances on the basis of their unique load signatures. In this paper, a machine learning based technique is presented to identify the devices for monitoring purposes. A low-cost hardware setup, called Appliance Identification and Management System (AIMS) is developed to identify and control the appliances remotely. The appliance identification algorithm is developed in Python and deployed on Raspberry Pi, coupled with Arduino. The hardware setup furnishes consumers with the real-time status of all home appliances on their smartphone and web server. Controlling module is also integrated with the identification hardware to provide smart access to consumer for the remote control of home appliances."
156,unknown,10.1109/jiot.2018.2828144,IEEE Internet of Things Journal,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/cc19ef2b057bd6a4c3137dc348d9e5adf866c1a7,1/1/2018 0:00,implemented iot-based self-learning home management system (shms) for singapore,"Internet of Things makes deployment of smart home concept easy and real. Smart home concept ensures residents to control, monitor, and manage their energy consumption without any wastage. This paper presents a self-learning home management system. In the proposed system, a home energy management system, demand side management system, and supply side management system were developed and integrated for real time operation of a smart home. This integrated system has some capabilities such as price forecasting, price clustering, and power alert system to enhance its functions. These enhancing capabilities were developed and implemented using computational and machine learning technologies. In order to validate the proposed system, real-time power consumption data was collected from a Singapore smart home and a realistic experimental case study was carried out. The case study has shown that the developed system has performed well and created energy awareness to the residents. This proposed system also displays its ability to customize the model for different types of environments compared to traditional smart home models."
157,unknown,10.3390/s21020405,Sensors,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/644b6322c111a291d46041366ec3436eebc329bc,1/1/2021 0:00,"dolars, a distributed on-line activity recognition system by means of heterogeneous sensors in real-life deployments—a case study in the smart lab of the university of almería","Activity Recognition (AR) is an active research topic focused on detecting human actions and behaviours in smart environments. In this work, we present the on-line activity recognition platform DOLARS (Distributed On-line Activity Recognition System) where data from heterogeneous sensors are evaluated in real time, including binary, wearable and location sensors. Different descriptors and metrics from the heterogeneous sensor data are integrated in a common feature vector whose extraction is developed by a sliding window approach under real-time conditions. DOLARS provides a distributed architecture where: (i) stages for processing data in AR are deployed in distributed nodes, (ii) temporal cache modules compute metrics which aggregate sensor data for computing feature vectors in an efficient way; (iii) publish-subscribe models are integrated both to spread data from sensors and orchestrate the nodes (communication and replication) for computing AR and (iv) machine learning algorithms are used to classify and recognize the activities. A successful case study of daily activities recognition developed in the Smart Lab of The University of Almería (UAL) is presented in this paper. Results present an encouraging performance in recognition of sequences of activities and show the need for distributed architectures to achieve real time recognition."
158,excluded,10.1109/ic3i44769.2018.9007294,2018 3rd International Conference on Contemporary Computing and Informatics (IC3I),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/db2783813c1071e47f55385454f13cc55dc8fb16,1/1/2018 0:00,iot based precision horticulture in north india,"Horticulture incorporates a major impact on economy of the country. It is a subdivision of agriculture which deals with plant gardening under controlled environment. Heap of analysis has been dole out in automating the irrigation system by using wireless device and mobile computing. Conjointly analysis has been done in applying machine learning in Horticultural system too. Machine to Machine (M2M) communication is a growing technology that permits devices, objects to speak among one another and send knowledge to Server or Cloud through the Core Network. Therefore, consequently we tend to have developed a Brainy IOT primarily based machine-controlled Irrigation system. Wherever device knowledge is touching wet soil, temperature is captured and consequently machine learning algorithmic is deployed for analysing the device knowledge for prediction towards irrigating the soil with water or switching ON/OFF fan to control temperature. This can be a totally machine-controlled wherever devices communicate among themselves and apply the intelligence in real time monitoring & analysis. Making data available online through Cloud to Scientists to remotely make smart decisions on Precision Horticulture. System has been developed using low value embedded devices like Raspberry Pi3, Arduino and successfully implemented in Delhi."
159,excluded,10.7717/peerj-cs.787,PeerJ Comput. Sci.,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/7c095c8b997e9419465656e34a656e800235d478,1/1/2021 0:00,detecting security attacks in cyber-physical systems: a comparison of mule and wso2 intelligent iot architectures,"The Internet of Things (IoT) paradigm keeps growing, and many different IoT devices, such as smartphones and smart appliances, are extensively used in smart industries and smart cities. The benefits of this paradigm are obvious, but these IoT environments have brought with them new challenges, such as detecting and combating cybersecurity attacks against cyber-physical systems. This paper addresses the real-time detection of security attacks in these IoT systems through the combined used of Machine Learning (ML) techniques and Complex Event Processing (CEP). In this regard, in the past we proposed an intelligent architecture that integrates ML with CEP, and which permits the definition of event patterns for the real-time detection of not only specific IoT security attacks, but also novel attacks that have not previously been defined. Our current concern, and the main objective of this paper, is to ensure that the architecture is not necessarily linked to specific vendor technologies and that it can be implemented with other vendor technologies while maintaining its correct functionality. We also set out to evaluate and compare the performance and benefits of alternative implementations. This is why the proposed architecture has been implemented by using technologies from different vendors: firstly, the Mule Enterprise Service Bus (ESB) together with the Esper CEP engine; and secondly, the WSO2 ESB with the Siddhi CEP engine. Both implementations have been tested in terms of performance and stress, and they are compared and discussed in this paper. The results obtained demonstrate that both implementations are suitable and effective, but also that there are notable differences between them: the Mule-based architecture is faster when the architecture makes use of two message broker topics and compares different types of events, while the WSO2-based one is faster when there is a single topic and one event type, and the system has a heavy workload."
160,excluded,10.3929/ethz-b-000347534,,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/478928b128cc8cd15515d348b644746bb0197b98,1/1/2018 0:00,"first analyses of rainfall patterns retrieved by a newly installed x-band radar over the metropolitan area of cagliari (sardinia, italy)","The growing urbanization and aggregation of metropolitan territorial communities, sustainable development, citizen engagement, economic and cultural attractiveness and governance are among the most important issues for modern cities. The increasing complexity of these problems and technological development are leading to an urgent need and the opportunity to radically rethink the way we build and manage our cities. The recent institution of the Metropolitan area of Cagliari, that counts more than 500 thousands inhabitants, stimulated the government of the Sardinian region to fund an innovative project (Tessuto Digitale Metropolitano), that will be developed jointly between the Center for advanced studies, research and development in Sardinia and the University of Cagliari. Specifically, the project aims at studying and developing innovative methods and technologies to offer new smart solutions to improve the attractiveness of the city, the management of resources and the safety and quality of life of citizens. These objectives can be pursued through the synergic use and experimentation of advanced communication infrastructures and widespread sensors, and the development of innovative vertical solutions. Among the others, the improvement of citizens’ safety against environmental risks is a priority objective, with a special regard to the development of monitoring and prediction systems of extreme precipitation events. In general, common characteristic of these phenomena is that their occurrence cannot be predicted with sufficient accuracy using traditional weather forecasting methods nor monitored by punctual traditional tipping buckets raingauges. This introduces the need for rainfall monitoring continuously in time and space, both to check their evolution in real time, and to dispose the necessary measures of civil protection. At the same time, the analysis of past observations, in terms of patterns and principal directions, allow to forecast (nowcasting) the occurrence of similar phenomena 30 minutes1hour ahead the rain hits the ground. Following these premises, the Department of Civil, Environmental Engineering and Architecture (DICAAR) of the University of Cagliari installed a weather radar (figure 1, left panel) over the towershaft elevator of a building of the Faculty of Engineering and Architecture, University of Cagliari (Lon 9.108720°, Lat 39.228991°). The radar is the SuperGauge model, produced by Envisens Technologies: it is an X-band radar characterized by a single elevation and single polarization, with 1 minute resolution in time and 60 m resolution in range. The radar can monitor an area within a radius of 30 km, with an azimuth resolution linearly increasing with distance up to 1500 m at the maximum distance (30 km). Hence, from the current position the instrument can monitor the whole Cagliari metropolitan area. Each scan is then processed to return the retrieved rainfall field in a regular grid with 60x60 meter grid-cells every minute. UrbanRain18 11 International Workshop on Precipitation in Urban Areas Fig 1: Left: Radar installation site. Right: Rainfall field observed during the 02-05-2018 event. The radar position was decided in order to limit electromagnetic interferences and minimize the ground clutter effects, which in turns are due to morphology and surrounding buildings. Initially, the radar was set with 0° elevation for the antenna. The first instrument run was during the rain events occurred throughout Sardinia at the beginning of May 2018, which showed high rainfall rates and precipitation volumes. Meteorological models correctly forecasted the storm occurrences and the civil protection issued several warnings of severe weather conditions; as a consequence, several damages were registered. A snapshot of the rainfall field as recorded by the radar during the event of 02-05-2018 is reported in figure 1 (right panel), showing some areas where the rainfall rate exceeds 40 mm/h. The comparison between the above observations and those collected by the National Radar Network supports the correct functioning of the instrument, at least in terms of registered rainfall patterns. Some adjustments and calibration are still needed: first, in order to minimize the ground clutter, hence improving the quality of the measurements, the elevation angle will be increased up to 3°. Second, rainfall observations inferred by the radar will be accurately adjusted taking the advantage of the Sardinia’s rain gauges network. When retrievals of other events will be collected and available, some nowcasting procedures will be implemented in order to use radar observations also to issue real time warnings. Traditional methods, based on cell tracking, area tracking, and stochastic algorithms will be compared to innovative methods, based on machine learning. Finally, radar and rain gauge data will be integrated with the sensor network envisaged by the abovementioned project, aimed at monitoring multiple environmental parameters (temperature, water level, wind speed, relative humidity). This complete data set will improve the forecast reliability, not only in terms of precipitation fields but also for many other quantities related to environmental security. Acknowledgments: This research was supported under the ROP Sardegna ERDF Action 1.2.2 (project “Tessuto Digitale Metropolitano”) and by Sardinian Regional Authorities."
161,included,http://arxiv.org/abs/2201.09550v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2201.09550v1,1/24/2022 0:00,crowd tracking and monitoring middleware via map-reduce,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques."
162,unknown,http://arxiv.org/abs/2103.11052v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.11052v1,3/19/2021 0:00,"a first step towards automated species recognition from camera trap
  images of mammals using ai in a european temperate forest","Camera traps are used worldwide to monitor wildlife. Despite the increasing
availability of Deep Learning (DL) models, the effective usage of this
technology to support wildlife monitoring is limited. This is mainly due to the
complexity of DL technology and high computing requirements. This paper
presents the implementation of the light-weight and state-of-the-art YOLOv5
architecture for automated labeling of camera trap images of mammals in the
Bialowieza Forest (BF), Poland. The camera trapping data were organized and
harmonized using TRAPPER software, an open source application for managing
large-scale wildlife monitoring projects. The proposed image recognition
pipeline achieved an average accuracy of 85% F1-score in the identification of
the 12 most commonly occurring medium-size and large mammal species in BF using
a limited set of training and testing data (a total 2659 images with animals).
  Based on the preliminary results, we concluded that the YOLOv5 object
detection and classification model is a promising light-weight DL solution
after the adoption of transfer learning technique. It can be efficiently
plugged in via an API into existing web-based camera trapping data processing
platforms such as e.g. TRAPPER system. Since TRAPPER is already used to manage
and classify (manually) camera trapping datasets by many research groups in
Europe, the implementation of AI-based automated species classification may
significantly speed up the data processing workflow and thus better support
data-driven wildlife monitoring and conservation. Moreover, YOLOv5 developers
perform better performance on edge devices which may open a new chapter in
animal population monitoring in real time directly from camera trap devices."
163,included,http://arxiv.org/abs/2011.03630v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.03630v1,11/6/2020 0:00,"unmasking communication partners: a low-cost ai solution for digitally
  removing head-mounted displays in vr-based telepresence","Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions."
164,unknown,http://arxiv.org/abs/2010.00432v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2010.00432v1,10/1/2020 0:00,"the rfml ecosystem: a look at the unique challenges of applying deep
  learning to radio frequency applications","While deep machine learning technologies are now pervasive in
state-of-the-art image recognition and natural language processing
applications, only in recent years have these technologies started to
sufficiently mature in applications related to wireless communications. In
particular, recent research has shown deep machine learning to be an enabling
technology for cognitive radio applications as well as a useful tool for
supplementing expertly defined algorithms for spectrum sensing applications
such as signal detection, estimation, and classification (termed here as Radio
Frequency Machine Learning, or RFML). A major driver for the usage of deep
machine learning in the context of wireless communications is that little, to
no, a priori knowledge of the intended spectral environment is required, given
that there is an abundance of representative data to facilitate training and
evaluation. However, in addition to this fundamental need for sufficient data,
there are other key considerations, such as trust, security, and
hardware/software issues, that must be taken into account before deploying deep
machine learning systems in real-world wireless communication applications.
This paper provides an overview and survey of prior work related to these major
research considerations. In particular, we present their unique considerations
in the RFML application space, which are not generally present in the image,
audio, and/or text application spaces."
165,included,http://arxiv.org/abs/2009.10679v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2009.10679v1,9/22/2020 0:00,"an embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames."
166,unknown,http://arxiv.org/abs/2004.05740v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2004.05740v2,4/13/2020 0:00,"deep-edge: an efficient framework for deep learning model update on
  heterogeneous edge","Deep Learning (DL) model-based AI services are increasingly offered in a
variety of predictive analytics services such as computer vision, natural
language processing, speech recognition. However, the quality of the DL models
can degrade over time due to changes in the input data distribution, thereby
requiring periodic model updates. Although cloud data-centers can meet the
computational requirements of the resource-intensive and time-consuming model
update task, transferring data from the edge devices to the cloud incurs a
significant cost in terms of network bandwidth and are prone to data privacy
issues. With the advent of GPU-enabled edge devices, the DL model update can be
performed at the edge in a distributed manner using multiple connected edge
devices. However, efficiently utilizing the edge resources for the model update
is a hard problem due to the heterogeneity among the edge devices and the
resource interference caused by the co-location of the DL model update task
with latency-critical tasks running in the background. To overcome these
challenges, we present Deep-Edge, a load- and interference-aware,
fault-tolerant resource management framework for performing model update at the
edge that uses distributed training. This paper makes the following
contributions. First, it provides a unified framework for monitoring,
profiling, and deploying the DL model update tasks on heterogeneous edge
devices. Second, it presents a scheduler that reduces the total re-training
time by appropriately selecting the edge devices and distributing data among
them such that no latency-critical applications experience deadline violations.
Finally, we present empirical results to validate the efficacy of the framework
using a real-world DL model update case-study based on the Caltech dataset and
an edge AI cluster testbed."
167,unknown,http://arxiv.org/abs/1909.06526v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.06526v1,9/14/2019 0:00,ffdl : a flexible multi-tenant deep learning platform,"Deep learning (DL) is becoming increasingly popular in several application
domains and has made several new application features involving computer
vision, speech recognition and synthesis, self-driving automobiles, drug
design, etc. feasible and accurate. As a result, large scale on-premise and
cloud-hosted deep learning platforms have become essential infrastructure in
many organizations. These systems accept, schedule, manage and execute DL
training jobs at scale.
  This paper describes the design, implementation and our experiences with
FfDL, a DL platform used at IBM. We describe how our design balances
dependability with scalability, elasticity, flexibility and efficiency. We
examine FfDL qualitatively through a retrospective look at the lessons learned
from building, operating, and supporting FfDL; and quantitatively through a
detailed empirical evaluation of FfDL, including the overheads introduced by
the platform for various deep learning models, the load and performance
observed in a real case study using FfDL within our organization, the frequency
of various faults observed including unanticipated faults, and experiments
demonstrating the benefits of various scheduling policies. FfDL has been
open-sourced."
168,unknown,http://arxiv.org/abs/1905.07082v6,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1905.07082v6,5/17/2019 0:00,"the audio auditor: user-level membership inference in internet of things
  voice services","With the rapid development of deep learning techniques, the popularity of
voice services implemented on various Internet of Things (IoT) devices is ever
increasing. In this paper, we examine user-level membership inference in the
problem space of voice services, by designing an audio auditor to verify
whether a specific user had unwillingly contributed audio used to train an
automatic speech recognition (ASR) model under strict black-box access. With
user representation of the input audio data and their corresponding translated
text, our trained auditor is effective in user-level audit. We also observe
that the auditor trained on specific data can be generalized well regardless of
the ASR model architecture. We validate the auditor on ASR models trained with
LSTM, RNNs, and GRU algorithms on two state-of-the-art pipelines, the hybrid
ASR system and the end-to-end ASR system. Finally, we conduct a real-world
trial of our auditor on iPhone Siri, achieving an overall accuracy exceeding
80\%. We hope the methodology developed in this paper and findings can inform
privacy advocates to overhaul IoT privacy."
169,included,http://arxiv.org/abs/1804.09914v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1804.09914v1,4/26/2018 0:00,"itelescope: intelligent video telemetry and classification in real-time
  using software defined networking","Video continues to dominate network traffic, yet operators today have poor
visibility into the number, duration, and resolutions of the video streams
traversing their domain. Current approaches are inaccurate, expensive, or
unscalable, as they rely on statistical sampling, middle-box hardware, or
packet inspection software. We present {\em iTelescope}, the first intelligent,
inexpensive, and scalable SDN-based solution for identifying and classifying
video flows in real-time. Our solution is novel in combining dynamic flow rules
with telemetry and machine learning, and is built on commodity OpenFlow
switches and open-source software. We develop a fully functional system, train
it in the lab using multiple machine learning algorithms, and validate its
performance to show over 95\% accuracy in identifying and classifying video
streams from many providers including Youtube and Netflix. Lastly, we conduct
tests to demonstrate its scalability to tens of thousands of concurrent
streams, and deploy it live on a campus network serving several hundred real
users. Our system gives unprecedented fine-grained real-time visibility of
video streaming performance to operators of enterprise and carrier networks at
very low cost."
170,included,http://arxiv.org/abs/1802.08960v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1802.08960v2,2/25/2018 0:00,"bonnet: an open-source training and deployment framework for semantic
  segmentation in robotics using cnns","The ability to interpret a scene is an important capability for a robot that
is supposed to interact with its environment. The knowledge of what is in front
of the robot is, for example, relevant for navigation, manipulation, or
planning. Semantic segmentation labels each pixel of an image with a class
label and thus provides a detailed semantic annotation of the surroundings to
the robot. Convolutional neural networks (CNNs) are popular methods for
addressing this type of problem. The available software for training and the
integration of CNNs for real robots, however, is quite fragmented and often
difficult to use for non-experts, despite the availability of several
high-quality open-source frameworks for neural network implementation and
training. In this paper, we propose a tool called Bonnet, which addresses this
fragmentation problem by building a higher abstraction that is specific for the
semantic segmentation task. It provides a modular approach to simplify the
training of a semantic segmentation CNN independently of the used dataset and
the intended task. Furthermore, we also address the deployment on a real
robotic platform. Thus, we do not propose a new CNN approach in this paper.
Instead, we provide a stable and easy-to-use tool to make this technology more
approachable in the context of autonomous systems. In this sense, we aim at
closing a gap between computer vision research and its use in robotics
research. We provide an open-source codebase for training and deployment. The
training interface is implemented in Python using TensorFlow and the deployment
interface provides a C++ library that can be easily integrated in an existing
robotics codebase, a ROS node, and two standalone applications for label
prediction in images and videos."
171,excluded,10.1109/access.2019.2926206,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8752358/,1/1/2019 0:00,a fusion-based framework for wireless multimedia sensor networks in surveillance applications,"Multimedia sensors enable monitoring applications to obtain more accurate and detailed information. However, the development of efficient and lightweight solutions for managing data traffic over wireless multimedia sensor networks (WMSNs) has become vital because of the excessive volume of data produced by multimedia sensors. As part of this motivation, this paper proposes a fusion-based WMSN framework that reduces the amount of data to be transmitted over the network by intra-node processing. This framework explores three main issues: (1) the design of a wireless multimedia sensor (WMS) node to detect objects using machine learning techniques; (2) a method for increasing the accuracy while reducing the amount of information transmitted by the WMS nodes to the base station, and; (3) a new cluster-based routing algorithm for the WMSNs that consumes less power than the currently used algorithms. In this context, a WMS node is designed and implemented using commercially available components. In order to reduce the amount of information to be transmitted to the base station and thereby extend the lifetime of a WMSN, a method for detecting and classifying objects on three different layers has been developed. A new energy-efficient cluster-based routing algorithm is developed to transfer the collected information/data to the sink. The proposed framework and the cluster-based routing algorithm are applied to our WMS nodes and tested experimentally. The results of the experiments clearly demonstrate the feasibility of the proposed WMSN architecture in the real-world surveillance applications."
172,excluded,10.1109/csci49370.2019.00077,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9070974/,12/7/2019 0:00,"embedded or integrated, autonomous intelligent monitoring architecture and framework for training assistance and automation","A key element to readiness lies in the ability to provide qualified trainers and role player personnel, a challenge in a budget constrained environment. The goal of this effort is to employ practical artificial intelligence (AI) and automation techniques to minimize staffing requirements for training, and to improve the quality and pace of training. This paper outlines implementation experiments and results of these techniques to interoperate multiple AI algorithm types in an architecture and framework to automate certain aspects of training systems. A core capability the ability to integrate into a variety of training systems, from embedded and virtual reality simulations to constructive wargames, via a user-friendly system of APIs."
173,included,10.1109/aivr.2018.00018,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8613637/,12/12/2018 0:00,a compensation method of two-stage image generation for human-ai collaborated in-situ fashion design in augmented reality environment,"In this paper, we consider a human-AI collaboration task, fashion design, in augmented reality environment. In particular, we propose a compensation method of two-stage image generation neural network for generating fashion design with progressive users' inputs. Our work is based on a recent proposed deep learning model, pix2pix, that can successfully transform an image from one domain into another domain, such as from line drawings to color images. However, the pix2pix model relies on the condition that input images should come from the same distribution, which is usually hard for applying it to real human computer interaction tasks, where the input from users differs from individual to individual. To address the problem, we propose a compensation method of two-stage image generation. In the first stage, we ask users to indicate their design preference with an easy task, such as tuning clothing landmarks, and use the input to generate a compensation input. With the compensation input, in the second stage, we then concatenate it with the real sketch from users to generate a perceptual better result. In addition, to deploy the two-stage image generation neural network in augmented reality environment, we designed and implemented a mobile application where users can create fashion design referring to real world human models. With the augmented 2D screen and instant feedback from our system, users can design clothing by seamlessly mixing the real and virtual environment. Through an online experiment with 46 participants and an offline use case study, we showcase the capability and usability of our system. Finally, we discuss the limitations of our system and further works on human-AI collaborated design."
174,excluded,10.1109/iwcmc55113.2022.9825089,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9825089/,6/3/2022 0:00,real-time application for recognition and visualization of arabic words with vowels based dl and ar,"Text is difficult to read in some cases due to text orientation, writing style, very light colors, etc. Visually impaired or visually impaired people have difficulty reading text in all of these situations. The architecture proposed in this work is intended to detect and identify Arabic characters with vowels in natural environments. This architecture can help visually impaired or blind people to read the text correctly. It allows users to read the text in a better and more immersive way by combining augmented reality with digital material. The approach uses both deep learning and more specifically the VGG 19 model and augmented reality to improve the efficiency, clarity, and accuracy of text reading. For text detection and identification we use the VGG 19 model, and for text visualization, we use augmented reality. The implementation technique presented in this research for an augmented reality interactive virtual assistant system is for users to use their smartphone&#x0027;s camera to receive enhanced text information via a text image and a three-dimensional image to understand the displayed text. It offers an interesting way to understand their environment. The use of augmented reality to better display recognized text in 3D is a fantastic feature. User research studies are conducted to assess usability and user satisfaction."
175,included,10.1109/aero50100.2021.9438232,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9438232/,3/13/2021 0:00,a pipeline for vision-based on-orbit proximity operations using deep learning and synthetic imagery,"Deep learning has become the gold standard for image processing over the past decade. Simultaneously, we have seen growing interest in orbital activities such as satellite servicing and debris removal that depend on proximity operations between spacecraft. However, two key challenges currently pose a major barrier to the use of deep learning for vision-based on-orbit proximity operations. Firstly, efficient implementation of these techniques relies on an effective system for model development that streamlines data curation, training, and evaluation. Secondly, a scarcity of labeled training data (images of a target spacecraft) hinders creation of robust deep learning models. This paper presents an open-source deep learning pipeline, developed specifically for on-orbit visual navigation applications, that addresses these challenges. The core of our work consists of two custom software tools built on top of a cloud architecture that interconnects all stages of the model development process. The first tool leverages Blender, an open-source 3D graphics toolset, to generate labeled synthetic training data with configurable model poses (positions and orientations), lighting conditions, backgrounds, and commonly observed in-space image aberrations. The second tool is a plugin-based framework for effective dataset curation and model training; it provides common functionality like metadata generation and remote storage access to all projects while giving complete independence to project-specific code. Time-consuming, graphics-intensive processes such as synthetic image generation and model training run on cloud-based computational resources which scale to any scope and budget and allow development of even the largest datasets and models from any machine. The presented system has been used in the Texas Spacecraft Laboratory with marked benefits in development speed and quality. Remote development, scalable compute, and automatic organization of data and artifacts have dramatically decreased iteration time while increasing reproducibility and system comprehension. Diverse, high-fidelity synthetic images that more closely replicate the real environment have improved model performance against real-world data. These results demonstrate that the presented pipeline offers tangible benefits to the application of deep learning for vision-based on-orbit proximity operations."
176,included,10.1109/icfec51620.2021.00018,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9458893/,5/13/2021 0:00,a privacy preserving system for ai-assisted video analytics,"The emerging Edge computing paradigm facilitates the deployment of distributed AI-applications and hardware, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits for parties in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, the widespread deployment of such mechanism in public areas are a growing cause of privacy and security concerns. Data protection strategies need to be appropriately designed and correctly implemented in order to mitigate the associated risks. Most existing approaches focus on privacy and security related operations of the video stream itself or protecting its transmission. In this paper, we propose a privacy preserving system for AI-assisted video analytics, that extracts relevant information from video data and governs the secure access to that information. The system ensures that applications leveraging extracted data have no access to the video stream. An attribute-based authorization scheme allows applications to only query a predefined subset of extracted data. We demonstrate the feasibility of our approach by evaluating an application motivated by the recent COVID-19 pandemic, deployed on typical edge computing infrastructure."
177,included,10.1109/cnna.2010.5430245,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5430245/,2/5/2010 0:00,a multi-fpga distributed embedded system for the emulation of multi-layer cnns in real time video applications,"This paper describes the design and the implementation of an embedded system based on multiple FPGAs that can be used to process real time video streams in standalone mode for applications that require the use of large Multi-Layer CNNs (ML-CNNs). The system processes video in progressive mode and provides a standard VGA output format. The main features of the system are determined by using a distributed computing architecture, based on Independent Hardware Modules (IHM), which facilitate system expansion and adaptation to new applications. Each IHM is composed by an FPGA board that can hold one or more CNN layers. The total computing capacity of the system is determined by the number of IHM used and the amount of resources available in the FPGAs. Our architecture supports traditional cloned templates, but also the (simultaneous) use of time-variant and space-variant templates."
178,included,10.1109/cbms.2019.00041,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8787393/,6/7/2019 0:00,action recognition in real homes using low resolution depth video data,"We report work in progress from interdisciplinary research on Assisted Living Technology in smart homes for older adults with mild cognitive impairments or dementia. We present our field trial, the set-up for collecting and storing data from real homes, and preliminary results on action recognition using low resolution depth video cameras. The data have been collected from seven apartments with one resident each over a period of two weeks. We propose a pre-processing of the depth videos by applying an Infinite Response Filter (IIR) for extracting the movements in the frames prior to classification. In this work we classify four actions: TV interaction (turn it on/ off and switch over), standing up, sitting down, and no movement. Our first results indicate that using the IIR filter for movement information extraction improves accuracy and can be an efficient method for recognizing actions. Our current implementation uses a convolutional long short-term memory (ConvLSTM) neural network, and achieved an average peak accuracy of 86%."
179,included,10.1145/3326285.3329051,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9068647/,6/25/2019 0:00,leap: learning-based smart edge with caching and prefetching for adaptive video streaming,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4&#x0025; and reduces video rebuffering by 42.7&#x0025;, which leads to at least 15.9&#x0025; improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP."
180,unknown,10.1109/ijcnn.2015.7280718,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7280718/,7/17/2015 0:00,real-time video object recognition using convolutional neural network,"A convolutional neural network (CNN) is implemented on a field-programmable gate array (FPGA) and used for recognizing objects in real-time video streams. In this system, an image pyramid is constructed by successively down-scaling the input video stream. Image blocks are extracted from the image pyramid and classified by the CNN core. The detected parts are then marked on the output video frames. The CNN core is composed of six hardware neurons and two receptor units. The hardware neurons are designed as fully-pipelined digital circuits synchronized with the system clock, and are used to compute the model neurons in a time-sharing manner. The receptor units scan the input image for local receptive fields and continuously supply data to the hardware neurons as inputs. The CNN core module is controlled according to the contents of a table describing the sequence of computational stages and containing the system parameters required to control each stage. The use of this table makes the hardware system more flexible, and various CNN configurations can be accommodated without re-designing the system. The system implemented on a mid-range FPGA achieves a computational speed greater than 170,000 classifications per second, and performs scale-invariant object recognition from a 720×480 video stream at a speed of 60 fps. This work is a part of a commercial project, and the system is targeted for recognizing any pre-trained objects with a small physical volume and low power consumption."
181,excluded,10.1109/icdcsw53096.2021.00009,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9545916/,7/10/2021 0:00,towards understanding the adaptation space of ai-assisted data protection for video analytics at the edge,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy."
182,included,10.1109/tnsm.2019.2929511,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8765778/,9/1/2019 0:00,itelescope: softwarized network middle-box for real-time video telemetry and classification,"Video continues to dominate network traffic, yet operators today have poor visibility into the number, duration, and resolutions of the video streams traversing their domain. Current monitoring approaches are inaccurate, expensive, or unscalable, as they rely on statistical sampling, middle-box hardware, or packet inspection software. We present iTelescope, the first intelligent, inexpensive, and scalable softwarized network middle-box solution for identifying and classifying video flows in realtime. Our solution is novel in combining dynamic flow rules with telemetry and machine learning, and is built on commodity OpenFlow switches and open-source software. We develop a fully functional system, train it in the lab using multiple machine learning algorithms, and validate its performance to show over 95% accuracy in identifying and classifying video streams from many providers, including YouTube and Netflix. Lastly, we conduct tests to demonstrate its scalability to tens of thousands of concurrent streams, and deploy it live on a campus network serving several hundred real users. Our traffic monitoring system gives unprecedented fine-grained real-time visibility of video streaming performance to operators of enterprise and carrier networks at very low cost."
183,included,10.1109/iria53009.2021.9588707,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9588707/,9/22/2021 0:00,automatic license plate recognition system using ssd,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95&#x0025;."
184,excluded,10.1109/3ict.2019.8910276,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8910276/,9/23/2019 0:00,ground operations management using a data governance dashboard,"An incident involving the use of chemical, biological, radiological, and nuclear (CBRN) materials might represent a significant challenge for crime scene investigators. The paper presents a full system architecture to assess the hazardous situations resulting from CBRN materials. This issue is crucial since there were a number of incidents that occurred in which forensics people could not reach the location either due to being unreachable or due to harmful emissions. The proposed solution integrates various inputs including data from sensors, video streaming, geo-data along with using Artificial Intelligence (AI) for good decision-making and data analysis. A geo-dashboard was also designed to demonstrate, in real-time, the collected data from several angles and according to various queries. It also monitors the performance in real-time. The topic is not new however the novelty of the proposed solution is the integration of multiple sources of data, applying deep neural nets and projecting the data and data analytics in real-time on a dashboard that displays the analysis and data from different perspectives considering the viewpoint of the individuals who will use that system. The paper also presents how the ROCSAFE multidisciplinary research project addresses the identified scenario. The project combines topics from robotics, sensor technology, analytical and situation awareness software, transforming data into knowledgeable insights to support the decision-making process."
185,unknown,10.1109/iccci50826.2021.9402701,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9402701/,1/29/2021 0:00,iot based two way safety enabled intelligent stove with age verification using machine learning,"Smart embedded systems have become a core component in the latest technologies, and IoT based smart embedded system is the trendiest field in the research area. In our research, we are proposing an IoT based smart stove. Any accident might occur at any time from a stove. So we are designing a two-way safety enabled stove with a child lock system and gas leakage detection feature. The intelligent stove will try to ensure safety and will detect age from real-time video streaming. Our main focus is a child would not be able to turn the stove on. As well as, the stove can entitle safety via gas detection alarm. We are using a Raspberry Pi and Gas Detection Module with a buzzer for the hardware implementation. Also, we are applying a Machine Learning object detection algorithm (Haar Cascade) and a deep learning architecture (CNN) for the system execution. Since our stove is IoT-based, the stove is ensuring safety remotely as well as manually which will try to prevent accidental occurrences."
186,included,10.1109/ic4me247184.2019.9036531,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9036531/,7/12/2019 0:00,object detection based security system using machine learning algorthim and raspberry pi,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally."
187,excluded,10.1109/csci.2017.81,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8560838/,12/16/2017 0:00,object movement detection by real-time deep learning for security surveillance camera,Developing a smart Web Video Player application connected to a security surveillance camera to keep track of the object of interest is an ongoing research. This paper presents a methodology to real time data mining of the sequence of frames from a live stream collected by security camera by processing trajectories of an object of interest. Two classifiers and a clustering method are implemented all working in real-time. Real-time Deep Learning and Support Vector Machines (SVM) machine learning algorithms are implemented on a local server without the use of cloud computing. This is a popular architecture for many buildings and industries who want to have an in-house smart security camera application.
188,excluded,10.1109/infocom41043.2020.9155467,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9155467/,7/9/2020 0:00,rldish: edge-assisted qoe optimization of http live streaming with reinforcement learning,"Recent years have seen a rapidly increasing traffic demand for HTTP-based high-quality live video streaming. The surging traffic demand, as well as the real-time property of live videos, make it challenging for content delivery networks (CDNs) to guarantee the Quality-of-Experiences (QoE) of viewers. The initial video segment (IVS) of live streaming plays an important role in the QoE of live viewers, particularly when users require fast join time and smooth view experience. State-of-the-art research on this regard estimates network throughput for each viewer and thus may incur a large overhead that offsets the benefit. To tackle the problem, we propose Rldish, a scheme deployed at the edge CDN server, to dynamically select a suitable IVS for new live viewers based on Reinforcement Learning (RL). Rldish is transparent to both the client and the streaming server. It collects the real-time QoE observations from the edge without any client-side assistance, then uses these QoE observations as real-time rewards in RL. We deploy Rldish as a virtualized network function (VNF) in a real HTTP cache server, and evaluate its performance using streaming servers distributed over the world. Our experiments show that Rldish improves the state- of-the-art IVS selection scheme w.r.t. the average QoE of live viewers by up to 22%."
189,excluded,10.1109/icnnb.2005.1614810,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1614810/,10/15/2005 0:00,security assurance using face recognition & detection system based on neural networks,"In this paper, we have proposed a new method of implementing an assurance system using the facial information of the people, this is a different approach to the conventional security system which uses biometric information or cryptography for assurance, here we use an efficient self-scaling face recognition system supported with a face detection system, the system is capable enough to extract the human faces from a real time video and to recognize the people using a face recognition system, we are designing the framework for face recognition system with a hybrid RBF neural network, the real advantage of the system lies in its capability to inculcate some basic features of the self organizing map (SOM) so that the system can scale on its own and it doesn't get outdated with time, for the face detection system we use a content based face detection algorithm, the facial feature so detected is inputted into the face recognition system, if the person's information is already present in the system, authentication can be accomplished, this system can be used in public places like airports and supermarkets, the information of criminals can be stored in the system and in case any of the criminals are detected by the system, the security personnel can be signaled, this system can also be implemented in robotics, the system can help the computer to identify individual users distinctly, our facial recognition system has been tested and found to be persistent in recognizing the individual even if the input facial image is of different gesture or holds some extra lineament like beard, moustache or spectacles so we can definitely state that the system is reliable and efficient"
190,unknown,10.1109/iccci50826.2021.9402589,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9402589/,1/29/2021 0:00,smart staff attendance system using convolutional neural network,"In this paper we have implemented Deep Learning model Convolutional Neural Network architecture for face detection to build a smart attendance system that will detect the faces of all the staff members and the attendance is marked automatically. This is a real time application which comes with day-to-day activities of handling attendance system in an institution. The process involves recognizing the face of staff members from the video taken through surveillance camera kept at different locations in the institution and other information technologies associated with the system. The proposed system will be able to find and recognize staff member faces fast and precisely with an accuracy of 90%. In addition, various data augmentation techniques are employed in the proposed system that improves the system accuracy further from 90% to 96%"
191,included,10.1109/tnsm.2021.3085097,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9444314/,9/1/2021 0:00,seta++: real-time scalable encrypted traffic analytics in multi-gbps networks,"The security and privacy of the end-users are a few of the most important components of a communication network. Though end-to-end encryption (e.g., TLS/SSL) fulfils this requirement, it makes inspecting network traffic with legacy solutions such as Deep Packet Inspection difficult. Recent Machine Learning techniques have shown outstanding performance in encrypted traffic classification. Nevertheless, such approaches require efficient flow sampling at real enterprise-scale networks due to the sheer volume of transferred data. Through this paper, we propose a holistic architecture to extract flow information of encrypted data at multi Gbps line rate using sampling and sketching mechanisms, enabling network operators to estimate flow size distribution accurately and understand the behavior of VPN-obfuscated traffic. Using over 6000 video traffic traces, under three main evaluation scenarios based on trace duration and starting time point, we show that it is possible to achieve 99% accuracy for service provider classification and over 90% accuracy for content classification for a given service provider in the best case. We also deploy our solution at an operational enterprise-scale network leveraging kernel bypassing to demonstrate its capability to efficiently sample live traffic for analytics."
192,included,10.1109/iceeccot46775.2019.9114716,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9114716/,12/14/2019 0:00,facial recognition using machine learning algorithms on raspberry pi,"Facial recognition is a non-invasive method of biometric authentication and useful for numerous applications. The real time implementation of the algorithm with adequate accuracy is required, with hardware timing into consideration. This paper deals with the implementation of machine learning algorithm for real time facial image recognition. Two dominant methods out of many facial recognition methods are discussed, simulated and implemented using Raspberry Pi. A rigorous comparative analysis is presented considering various limitations which may be the case required for innumerable application which utilize facial recognition. The drawbacks and different use cases of each method is highlighted. The facial recognition software uses algorithms to compare a digital image captured through a camera, to the stored face print so as to authenticate a person's identity. The Haar-Cascade method was one of the first methods developed for facial recognition. The HOG (Histogram of Oriented Gradients) method has worked very effectively for object recognition and thus suitable for facial recognition also. Both the methods are compared with Eigen feature-based face recognition algorithm. Various important features are experimented like speed of operation, lighting condition, frontal face profile, side profiles, distance of image, size of image etc. The facial recognition model is implemented to detect and recognize faces in real-time by means of Raspberry Pi and Pi camera for the user defined database in addition to the available databases."
193,unknown,10.1109/cds49703.2020.00012,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9275963/,8/2/2020 0:00,welding seam recognition robots based on edge computing,"In order to meet the requirements of the accuracy and real-time performance during the working process of underwater welding robots, a scheme of welding seam recognition robots system based on the edge computing is proposed in this paper. A number of pre-processing methods for capturing welding seam image were designed, including Thresholding, Filtering and Edge Detect. A Convolutional Neural Network(CNN) model for welding seam recognition was also created. In the experiments, the image pre-processing and CNN algorithms were integrated in and deployed to the robots, and the learning and training algorithms of the CNN were deployed to the cloud servers. The image pre-processing methods filtered the interference in underwater operations and achieved the image compression and feature extraction. The cloud servers fulfilled the training and parameter optimization of the CNN, which improved the accuracy of welding seam image recognition."
194,excluded,10.1109/ct.1997.617707,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/617707/,8/28/1997 0:00,the intelligent room project,"At the MIT Artificial Intelligence Laboratory, we have been working on technologies for an Intelligent Room. Rather than pull people into the virtual world of the computer, we are trying to pull the computer out into the real world of people. To do this, we are combining robotics and vision technology with speech understanding systems and agent-based architectures to provide ready-at-hand computation and information services for people engaged in day-to-day activities, both on their own and in conjunction with others. We have built a layered architecture where, at the bottom level, vision systems track people and identify their activities and gestures, and, through word spotting, decide whether people in the room are talking to each other or to the room itself. At the next level, an agent architecture provides a uniform interface to such specially-built systems, and to other off-the-shelf software, such as Web browsers, etc. At the highest level, we are able to build application systems that provide occupants of the room with specialized services; examples we have built include systems for command-and-control situations rooms and as a room for giving presentations."
195,excluded,10.1016/j.future.2020.06.017,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85087196994,11/1/2020,ppcensor: architecture for real-time pornography detection in video streaming,"Convolutional neural network (CNN) models are typically composed of several gigabytes of data, requiring dedicated hardware and significant processing capabilities for proper handling. In addition, video-detection tasks are typically performed offline, and each video frame is analyzed individually, meaning that the video’s categorization (class assignment) as normal or pornographic is only complete after all the video frames have been evaluated. This paper proposes the Private Parts Censor (PPCensor), a CNN-based architecture for transparent and near real-time detection and obfuscation of pornographic video frame regions. Our contribution is two-fold. First, the proposed architecture is the first that addresses the detection of pornographic content as an object detection problem. The objective is to apply user-friendly content filtering such that an inevitable false positive will obfuscate only regions (objects) within the video frames instead of blocking the entire video. Second, the PPCensor architecture is deployed on dedicated hardware, and real-time detection is deployed using a video-oriented streaming proxy. If a pornographic video frame is identified in the video, the system can hide pornographic content (private parts) in real time without user interaction or additional processing on the user’s device. Based on more than 50,000 objects labeled manually, the evaluation results show that the PPCensor is capable of detecting private parts in near real time for video streaming. Compared to cutting-edge CNN architectures for image classification, PPCensor achieved similar results, but operated in real time. In addition, when deployed on a desktop computer, PPCensor handled up to 35 simultaneous connections without the need for additional processing on the end-user device."
196,excluded,10.1016/j.isatra.2020.02.023,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85080060395,7/1/2020,strengthening the perception of the virtual worlds in a virtual reality environment,"Virtual reality is becoming more and more improved primarily due to numerous applications and the powers of mobile devices. Using various sensors, precise displays and high computing powers smartphone are becoming devices that make the boost in technology. Now it is necessary to efficiently use various sensors without affecting system operation and improve control abilities for various purposes. Especially in practical applications received by mass users such as games and any kind of experience. In this article, we propose a system that allows to extend the perception of the virtual world by conveying information about the user’s movements in reality into the supervised model. The system retrieves data from several sources, quickly analyzes them using artificial intelligence techniques, and returns information to the mobile phone about the activity that is being processed. The concept extends the understanding of today’s virtual reality by allowing the user to move and perform simple gestures in a specially designed room. Moreover, we propose multiplayer mode in virtual reality, where players are in different places. The proposed architecture of the system has been tested on simple applications, and the results show high potential for implementations in various apps by achieving almost 90% efficiency in changing player direction in real time and only 7.5% of collision cases."
197,excluded,10.1016/j.procs.2020.09.269,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85093364102,1/1/2020,"alexa, what classes do i have today? the use of artificial intelligence via smart speakers in education","Looking back to the rumours from the early 2000’s, when the world of technology bloomed together with the curiosity towards what was next to come, by 2020, robots should have assisted and supported almost every task from our daily life. While this may seem as a Sci-Fi movie scenario, it is partially a tangible reality, that we quickly got used to, thanks to the introduction of smart speakers.
                  As the world changes, so does the future of our students. In this respects, the evolution of the technology comes up with specific environments for educational purpose. Building smart learning environments supported by e-learning platforms is an important area of research in education domain within our days. The evolution of these smart learning environments is justified by some events (Covid19) that force students to learn remotely.
                  The paper proposes a software application component using Alexa smart speaker, that integrates different services (Amazon Web Services, Microsoft Services) for a proper virtual environment platform, for both students and teachers. It addresses the main concerns of the current educational system, and provides a smart solution through the use of Artificial Intelligence based tools. The proposed approach not only achieves unifying data and knowledge-share mechanisms in a remotely mode, but it brings also a good learning experience, increasing the effectiveness and the efficiency of the learning process."
198,excluded,10.11591/ijece.v12i1.pp331-338,'Institute of Advanced Engineering and Science',core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/478033462.pdf,2/1/2022 0:00,real-time traffic sign detection and recognition using raspberry pi,"Nowadays, the number of road accident in Malaysia is increasing expeditiously. One of the ways to reduce the number of road accident is through the development of the advanced driving assistance system (ADAS) by professional engineers. Several ADAS system has been proposed by taking into consideration the delay tolerance and the accuracy of the system itself. In this work, a traffic sign recognition system has been developed to increase the safety of the road users by installing the system inside the car for driver’s awareness. TensorFlow algorithm has been considered in this work for object recognition through machine learning due to its high accuracy. The algorithm is embedded in the Raspberry Pi 3 for processing and analysis to detect the traffic sign from the real-time video recording from Raspberry Pi camera NoIR. This work aims to study the accuracy, delay and reliability of the developed system using a Raspberry Pi 3 processor considering several scenarios related to the state of the environment and the condition of the traffic signs. A real-time testbed implementation has been conducted considering twenty different traffic signs and the results show that the system has more than 90% accuracy and is reliable with an acceptable delay"
199,excluded,http://arxiv.org/abs/2110.13041v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2110.13041v1,10/25/2021 0:00,applications and techniques for fast machine learning in science,"In this community review report, we discuss applications and techniques for
fast machine learning (ML) in science -- the concept of integrating power ML
methods into the real-time experimental data processing loop to accelerate
scientific discovery. The material for the report builds on two workshops held
by the Fast ML for Science community and covers three main areas: applications
for fast ML across a number of scientific domains; techniques for training and
implementing performant and resource-efficient ML algorithms; and computing
architectures, platforms, and technologies for deploying these algorithms. We
also present overlapping challenges across the multiple scientific domains
where common solutions can be found. This community report is intended to give
plenty of examples and inspiration for scientific discovery through integrated
and accelerated ML solutions. This is followed by a high-level overview and
organization of technical advances, including an abundance of pointers to
source material, which can enable these breakthroughs."
200,included,http://arxiv.org/abs/2004.05953v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2004.05953v1,4/13/2020 0:00,"software-defined network for end-to-end networked science at the
  exascale","Domain science applications and workflow processes are currently forced to
view the network as an opaque infrastructure into which they inject data and
hope that it emerges at the destination with an acceptable Quality of
Experience. There is little ability for applications to interact with the
network to exchange information, negotiate performance parameters, discover
expected performance metrics, or receive status/troubleshooting information in
real time. The work presented here is motivated by a vision for a new smart
network and smart application ecosystem that will provide a more deterministic
and interactive environment for domain science workflows. The Software-Defined
Network for End-to-end Networked Science at Exascale (SENSE) system includes a
model-based architecture, implementation, and deployment which enables
automated end-to-end network service instantiation across administrative
domains. An intent based interface allows applications to express their
high-level service requirements, an intelligent orchestrator and resource
control systems allow for custom tailoring of scalability and real-time
responsiveness based on individual application and infrastructure operator
requirements. This allows the science applications to manage the network as a
first-class schedulable resource as is the current practice for instruments,
compute, and storage systems. Deployment and experiments on production networks
and testbeds have validated SENSE functions and performance. Emulation based
testing verified the scalability needed to support research and education
infrastructures. Key contributions of this work include an architecture
definition, reference implementation, and deployment. This provides the basis
for further innovation of smart network services to accelerate scientific
discovery in the era of big data, cloud computing, machine learning and
artificial intelligence."
201,unknown,http://arxiv.org/abs/1909.08703v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.08703v1,9/18/2019 0:00,"deep complex networks for protocol-agnostic radio frequency device
  fingerprinting in the wild","Researchers have demonstrated various techniques for fingerprinting and
identifying devices. Previous approaches have identified devices from their
network traffic or transmitted signals while relying on software or operating
system specific artifacts (e.g., predictability of protocol header fields) or
characteristics of the underlying protocol (e.g.,frequency offset). As these
constraints can be a hindrance in real-world settings, we introduce a
practical, generalizable approach that offers significant operational value for
a variety of scenarios, including as an additional factor of authentication for
preventing impersonation attacks. Our goal is to identify artifacts in
transmitted signals that are caused by a device's unique hardware
""imperfections"" without any knowledge about the nature of the signal. We
develop RF-DCN, a novel Deep Complex-valued Neural Network (DCN) that operates
on raw RF signals and is completely agnostic of the underlying applications and
protocols. We present two DCN variations: (i) Convolutional DCN (CDCN) for
modeling full signals, and (ii) Recurrent DCN (RDCN) for modeling time series.
Our system handles raw I/Q data from open air captures within a given spectrum
window, without knowledge of the modulation scheme or even the carrier
frequencies. While our experiments demonstrate the effectiveness of our system,
especially under challenging conditions where other neural network
architectures break down, we identify additional challenges in signal-based
fingerprinting and provide guidelines for future explorations. Our work lays
the foundation for more research within this vast and challenging space by
establishing fundamental directions for using raw RF I/Q data in novel
complex-valued networks."
202,unknown,http://arxiv.org/abs/1907.12817v2,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1907.12817v2,7/30/2019 0:00,"increasing scalability of process mining using event dataframes: how
  data structure matters","Process Mining is a branch of Data Science that aims to extract
process-related information from event data contained in information systems,
that is steadily increasing in amount. Many algorithms, and a general-purpose
open source framework (ProM 6), have been developed in the last years for
process discovery, conformance checking, machine learning on event data.
However, in very few cases scalability has been a target, prioritizing the
quality of the output over the execution speed and the optimization of
resources. This is making progressively more difficult to apply process mining
with mainstream workstations on real-life event data with any open source
process mining framework. Hence, exploring more scalable storage techniques,
in-memory data structures, more performant algorithms is a strictly incumbent
need. In this paper, we propose the usage of mainstream columnar storages and
dataframes to increase the scalability of process mining. These can replace the
classic event log structures in most tasks, but require completely different
implementations with regards to mainstream process mining algorithms.
Dataframes will be defined, some algorithms on such structures will be
presented and their complexity will be calculated."
203,excluded,10.1109/fccm.2017.58,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7966655/,5/2/2017 0:00,accelerating large-scale graph analytics with fpga and hmc,"Graph analytics that explores the relationship among interconnected entities is becoming increasingly important due to its broad applicability from machine learning to social science. However, one major challenge for graph processing systems is the irregular data access pattern of graph computation which can significantly degrade the performance. The algorithms, software, and hardware that have been tailored for mainstream parallel applications are, as a result, generally not effective for massive-scale sparse graphs from the real world due to their complexity and irregularity. To address the performance issues in large-scale graph analytics, we combine the emerging Hybrid Memory Cube (HMC) with a modern FPGA in order to achieve exceptional random access performance without any loss of flexibility or efficiency in computation. In particular, we develop collaborative software/hardware techniques to perform a level-synchronized breadth first search (BFS) on the FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that fully exploits the platform's capability to improve data locality and memory access efficiency. For each input graph, this algorithm provides an efficient data layout that allows the FPGA to coalesce memory requests into the largest possible HMC payload requests so that the number of memory requests, which is the primary factor in runtime, can be minimized. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by adding a merging unit. The merging unit takes the best advantage of the increased data locality resulting from graph clustering. We evaluated the performance of our BFS implementation using the AC-510 development kit from Micron over a set of benchmarks from a wide range of applications. We observed that the combination of the clustering algorithm and the merging hardware achieved 2.8 × average performance improvement compared to the latest FPGA-HMC based graph processing system."
204,unknown,10.1109/fie44824.2020.9273981,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9273981/,10/24/2020 0:00,machine learning for middle-schoolers: children as designers of machine-learning apps,"This Research to Innovative Practice Full Paper presents a multidisciplinary, design-based research study that aims to develop and study pedagogical models and tools for integrating machine-learning (ML) topics into education. Although children grow up with ML systems, few theoretical or empirical studies have focused on investigating ML and data-driven design in K-12 education to date. This paper presents the theoretical grounds for a design-oriented pedagogy and the results from exploring and implementing those theoretical ideas in practice through a case study conducted in Finland. We describe the overall process in which middle-schoolers (N = 34) co-designed and made ML applications for solving meaningful, everyday problems. The qualitative content analysis of the pre-and post-tests, student interviews, and the students' own ML design ideas indicated that co-designing real-life applications lowered the barriers for participating in some of the core practices of computer science. It also supported children in exploring abstract ML concepts and workflows in a highly personalized and embodied way. The article concludes with a discussion on pedagogical insights for supporting middle-schoolers in becoming innovators and software designers in the age of ML."
205,included,10.1109/bigdata.2018.8621926,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8621926/,12/13/2018 0:00,harnessing the nature of spam in scalable online social spam detection,"Disinformation in social networks has been a worldwide problem. Social users are surrounded by a huge volume of malicious links, biased comments, fake reviews, or fraudulent advertisements, etc. Traditional spam detection approaches propose a variety of statistical feature-based models to filter out social spam from a historical dataset. However, they omit the real word situation of social data, that is, social spam is fast changing with new topics or events. Therefore, traditional approaches cannot effectively achieve online detection of the ""drifting"" social spam with a fixed statistic feature set. In this paper, we present Sifter, a system which can detect online social spam in a scalable manner without the labor-intensive feature engineering. The Sifter system is two-fold: (1) a decentralized DHT-based overlay deployment for harnessing the group characteristics of social spam activities within a specific topic/event; (2) a social spam processing with the support of Recurrent Neural Network (RNN) to get rid of the traditional manual feature engineering. Results show that Sifter achieves graceful spam detection performances with the minimal size of data and good balance in group management."
206,unknown,10.1109/trustcom50675.2020.00243,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9343128/,1/1/2021 0:00,monitoring social media for vulnerability-threat prediction and topic analysis,"Publicly available software vulnerabilities and exploit code are often abused by malicious actors to launch cyberattacks to vulnerable targets. Organizations not only have to update their software to the latest versions, but do effective patch management and prioritize security-related patching as well. In addition to intelligence sources such as Computer Emergency Response Team (CERT) alerts, cybersecurity news, national vulnerability database (NBD), and commercial cybersecurity vendors, social media is another valuable source that facilitates early stage intelligence gathering. To early detect future cyber threats based on publicly available resources on the Internet, we propose a dynamic vulnerability-threat assessment model to predict the tendency to be exploited for vulnerability entries listed in Common Vulnerability Exposures, and also to analyze social media contents such as Twitter to extract meaningful information. The model takes multiple aspects of vulnerabilities gathered from different sources into consideration. Features range from profile information to contextual information about these vulnerabilities. For the social media data, this study leverages machine learning techniques specially for Twitter which helps to filter out non-cybersecurity-related tweets and also label the topic categories of each tweet. When applied to predict the vulnerabilities exploitation and analyzed the real-world social media discussion data, it showed promising prediction accuracy with purified social media intelligence. Moreover, the AI-enabling modules have been deployed into a threat intelligence platform for further applications."
207,excluded,10.1109/asonam.2012.66,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6425738/,8/29/2012 0:00,semi-supervised policy recommendation for online social networks,"Fine grain policy settings in social network sites is becoming a very important requirement for managing user's privacy. Incorrect privacy policy settings can easily lead to leaks in private and personal information. At the same time, being too restrictive would reduce the benefits of online social networks. This is further complicated with the growing adoption of social networks and with the rapid growth in information uploading and sharing. The problem of facilitating policy settings has attracted numerous access control, and human computer interaction researchers. The solutions proposed range from usable interfaces for policy settings to automated policy settings. We propose a fine grained policy recommendation system that is based on an iterative semi-supervised learning approach that uses the social graph propagation properties. Active learning and social graph properties were used to detect the most informative instances to be labeled as training sets. We implemented and tested our approach using real Facebook dataset. We compared our proposed approach to supervised learning and random walk approaches. Our proposed approaches provided high accuracy and precision when compared to the other approaches."
208,included,10.1109/icitr51448.2020.9310890,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9310890/,12/4/2020 0:00,hybrid approach and architecture to detect fake news on twitter in real-time using neural networks,"Fake news has been a key issue since the dawn of social media. Currently, we are at a stage where it is merely impossible to differentiate between real and fake news. This directly and indirectly affects people's decision patterns and makes us question the credibility of the news shared via social media platforms. Twitter is one of the leading social networks in the world by active users. There has been an exponential spread of fake news on Twitter in the recent past. In this paper, we will discuss the implementation of a browser extension which will identify fake news on Twitter using deep learning models with a focus on real-world applicability, architectural stability and scalability of such a solution. Experimental results show that the proposed browser extension has an accuracy of 86% accuracy in fake news detection. To the best of our knowledge, our work is the first of its kind to detect fake news on Twitter real-time using a hybrid approach and evaluate using real users."
209,unknown,10.1109/icmla.2015.152,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7424435/,12/11/2015 0:00,mlaas: machine learning as a service,"The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time."
210,included,10.1109/tifs.2021.3131026,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9627681/,1/1/2022 0:00,poligraph: intrusion-tolerant and distributed fake news detection system,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally ( ${4\%}$ – ${7\%}$ ) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach."
211,unknown,10.1109/isrcs.2013.6623773,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6623773/,8/15/2013 0:00,scalable machine learning framework for behavior-based access control,"Today's activities in cyber space are more connected than ever before, driven by the ability to dynamically interact and share information with a changing set of partners over a wide variety of networks. The success of approaches aimed at securing the infrastructure has changed the threat profile to point where the biggest threat to the US cyber infrastructure is posed by targeted cyber attacks. The Behavior-Based Access Control (BBAC) effort has been investigating means to increase resilience against these attacks. Using statistical machine learning, BBAC (a) analyzes behaviors of insiders pursuing targeted attacks and (b) assesses trustworthiness of information to support real-time decision making about information sharing. The scope of this paper is to describe the challenge of processing disparate cyber security information at scale, together with an architecture and work-in-progress prototype implementation for a cloud framework supporting a strategic combination of stream and batch processing."
212,excluded,10.1016/j.chemolab.2021.104329,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85105292476,7/15/2021,a novel approach for water quality classification based on the integration of deep learning and feature extraction techniques,"Water quality monitoring plays a vital role in the protection of water resources, environmental management, and decision-making. Artificial intelligence (AI) based on machine learning techniques has been widely used to evaluate and classify water quality for the last two decades. However, traditional machine learning techniques face many limitations, the most important of which is the inability to apply these techniques with big data generated by smart water quality monitoring stations to improve the prediction. Real-time water quality monitoring with high accuracy and efficiency for intelligent water quality monitoring stations requires new and sophisticated techniques based on machine and deep learning techniques. For this purpose, we propose a novel approach based on the integration of deep learning and feature extraction techniques to improve water quality classification. In this paper, was chosen the Tilesdit dam in Bouira (Algeria) as a case study. Moreover, we implemented the advanced deep learning method - Long Short Term Memory Recurrent Neural Networks (LSTM RNNs) to construct an intelligent model for drinking water quality classification. Furthermore, principal component analysis (PCA), linear discriminant analysis (LDA) and independent component analysis (ICA) techniques were used for features extraction and data reduction from original features. Additionally, we used three methods of cross-validation and two methods of the out-of-sample test to estimate the performance of LSTM RNNs model. From the results we found that the integration of LSTM RNNs with LDA, and LSTM RNNs with ICA yields an accuracy of 99.72%, using Random-Holdout technique."
213,included,10.1016/j.neucom.2020.11.066,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85098065075,3/7/2021,bayesuites: an open web framework for massive bayesian networks focused on neuroscience,"BayeSuites is the first web framework for learning, visualizing, and interpreting Bayesian networks (BNs) that can scale to tens of thousands of nodes while providing fast and friendly user experience. All the necessary features that enable this are reviewed in this paper; these features include scalability, extensibility, interoperability, ease of use, and interpretability. Scalability is the key factor in learning and processing massive networks within reasonable time; for a maintainable software open to new functionalities, extensibility and interoperability are necessary. Ease of use and interpretability are fundamental aspects of model interpretation, fairly similar to the case of the recent explainable artificial intelligence trend. We present the capabilities of our proposed framework by highlighting a real example of a BN learned from genomic data obtained from Allen Institute for Brain Science. The extensibility properties of the software are also demonstrated with the help of our BN-based probabilistic clustering implementation, together with another genomic-data example."
214,unknown,10.1021/acs.accounts.0c00736,Accounts of chemical research,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/2efecdfa2c6d7459b2b941d714e24bd3e64e6bc5,1/1/2021 0:00,automated experimentation powers data science in chemistry.,"ConspectusData science has revolutionized chemical research and continues to break down barriers with new interdisciplinary studies. The introduction of computational models and machine learning (ML) algorithms in combination with automation and traditional experimental techniques has enabled scientific advancement across nearly every discipline of chemistry, from materials discovery, to process optimization, to synthesis planning. However, predictive tools powered by data science are only as good as their data sets and, currently, many of the data sets used to train models suffer from several limitations, including being sparse, limited in scope and requiring human curation. Likewise, computational data faces limitations in terms of accurate modeling of nonideal systems and can suffer from low translation fidelity from simulation to real conditions. The lack of diverse data and the need to be able to test it experimentally reduces both the accuracy and scope of the predictive models derived from data science. This Account contextualizes the need for more complex and diverse experimental data and highlights how the seamless integration of robotics, machine learning, and data-rich monitoring techniques can be used to access it with minimal human labor.We propose three broad categories of data in chemistry: data on fundamental properties, data on reaction outcomes, and data on reaction mechanics. We highlight flexible, automated platforms that can be deployed to acquire and leverage these data. The first platform combines solid- and liquid-dosing modules with computer vision to automate solubility screening, thereby gathering fundamental data that are necessary for almost every experimental design. Using computer vision offers the additional benefit of creating a visual record, which can be referenced and used to further interrogate and gain insight on the data collected. The second platform iteratively tests reaction variables proposed by a ML algorithm in a closed-loop fashion. Experimental data related to reaction outcomes are fed back into the algorithm to drive the discovery and optimization of new materials and chemical processes. The third platform uses automated process analytical technology to gather real-time data related to reaction kinetics. This system allows the researcher to directly interrogate the reaction mechanisms in granular detail to determine exactly how and why a reaction proceeds, thereby enabling reaction optimization and deployment."
215,excluded,10.1109/saci.2007.375494,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/4262496/,5/18/2007 0:00,fpga parallel implementation of cmac type neural network with on chip learning,"The hardware implementation of neural networks is a new step in the evolution and use of neural networks in practical applications. The CMAC cerebellar model articulation controller is intended especially for hardware implementation, and this type of network is used successfully in the areas of robotics and control, where the real time capabilities of the network are of particular importance. The implementation of neural networks on FPGA's has several benefits, with emphasis on parallelism and the real time capabilities. This paper discusses the hardware implementation of the CMAC type neural network, the architecture and parameters and the functional modules of the hardware implemented neuro-processor."
216,included,10.1109/isc2.2016.7580798,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7580798/,9/15/2016 0:00,smartseal: a ros based home automation framework for heterogeneous devices interconnection in smart buildings,"With this paper we present the SmartSEAL inter-connection system developed for the nationally founded SEAL project. SEAL is a research project aimed at developing Home Automation (HA) solutions for building energy management, user customization and improved safety of its inhabitants. One of the main problems of HA systems is the wide range of communication standards that commercial devices use. Usually this forces the designer to choose devices from a few brands, limiting the scope of the system and its capabilities. In this context, SmartSEAL is a framework that aims to integrate heterogeneous devices, such as sensors and actuators from different vendors, providing networking features, protocols and interfaces that are easy to implement and dynamically configurable. The core of our system is a Robotics middleware called Robot Operating System (ROS). We adapted the ROS features to the HA problem, designing the network and protocol architectures for this particular needs. These software infrastructure allows for complex HA functions that could be realized only levering the services provided by different devices. The system has been tested in our laboratory and installed in two real environments, Palazzo Fogazzaro in Schio and “Le Case” childhood school in Malo. Since one of the aim of the SEAL project is the personalization of the building environment according to the user needs, and the learning of their patterns of behaviour, in the final part of this work we also describe the ongoing design and experiments to provide a Machine Learning based re-identification module implemented with Convolutional Neural Networks (CNNs). The description of the adaptation module complements the description of the SmartSEAL system and helps in understanding how to develop complex HA services through it."
217,excluded,10.1109/icsys47076.2019.8982469,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8982469/,9/19/2019 0:00,fpga-enabled binarized convolutional neural networks toward real-time embedded object recognition system for service robots,"In this presentation, we report the results of applying a binarized Convolutional Neural Network (CNN) and a Field Programmable Gate Array (FPGA) for image-based object recognition. While the demand rises for robots with robust object recognition implemented with Neural Networks, a tradeoff between data processing rate and power consumption persists. Some applications utilise Graphics Processing Units (GPU), which results in high power consumption, thus undesirable for embedded systems, while the others communicate with cloud computers to minimise computational resources at the clients' side, i.e. robots, raising another concern that the robots are unable to perform object recognition without the servers and network connections. To overcome these difficulties, we propose an embedded object recognition system implemented with a binarized CNN and an FPGA. FPGAs consist of a matrix of reconfigurable logic gates allowing parallel computing which befit most image processing algorithms such as the CNN. We train the binarized CNN on one of our datasets that contain images of several kinds of food and beverages. The results of the experiments show that the binarized CNN with an FPGA maintains high accuracy as well as real-time computation, suggesting that the proposed system is suitable for robots to perform their tasks in a real-world environment without needing to communicate with a server."
218,excluded,10.1109/fuzz48607.2020.9177654,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9177654/,7/24/2020 0:00,ai-fml agent for robotic game of go and aiot real-world co-learning applications,"In this paper, we propose an AI-FML agent for robotic game of Go and AIoT real-world co-learning applications. The fuzzy machine learning mechanisms are adopted in the proposed model, including fuzzy markup language (FML)-based genetic learning (GFML), eXtreme Gradient Boost (XGBoost), and a seven-layered deep fuzzy neural network (DFNN) with backpropagation learning, to predict the win rate of the game of Go as Black or White. This paper uses Google AlphaGo Master sixty games as the dataset to evaluate the performance of the fuzzy machine learning, and the desired output dataset were predicted by Facebook AI Research (FAIR) ELF Open Go AI bot. In addition, we use IEEE 1855 standard for FML to describe the knowledge base and rule base of the Open Go Darkforest (OGD) prediction platform in order to infer the win rate of the game. Next, the proposed AI-FML agent publishes the inferred result to communicate with the robot Kebbi Air based on MQTT protocol to achieve the goal of human and smart machine co-learning. From Sept. 2019 to Jan. 2020, we introduced the AI-FML agent into the teaching and learning fields in Taiwan. The experimental results show the robots and students can co-learn AI tools and FML applications effectively. In addition, XGBoost outperforms the other machine learning methods but DFNN has the most obvious progress after learning. In the future, we hope to deploy the AI-FML agent to more available robot and human co-learning platforms through the established AI-FML International Academy in the world."
219,included,10.1109/ams.2017.22,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8424312/,12/6/2017 0:00,autonomous rover navigation using gps based path planning,"Nowadays, with the constant evolution of Artificial Intelligence and Machine Learning, robots are getting more perceptive than ever. For this quality they are being used in varying circumstances which humans cannot control. Rovers are special robots, capable of traversing through areas that are too difficult for humans. Even though it is a robust bot, lack of proper intelligence and automation are its basic shortcomings. As the main purpose of a rover is to traverse through areas of extreme difficulties, therefore an intelligent path generation and following system is highly required. Our research work aimed at developing an algorithm for autonomous path generation using GPS (Global Positioning System) based coordinate system and implementation of this algorithm in real life terrain, which in our case is MDRS, Utah, USA. Our prime focus was the development of a robust but easy to implement system. After developing such system, we have been able to successfully traverse our rover through that difficult terrain. It uses GPS coordinates of target points that will be fed into the rover from a control station. The rover capturing its own GPS signal generates a path between the current location and the destination location on its own. It then finds the deviation in its current course of direction and position. And eventually it uses Proportional Integral Derivative control loop feedback mechanism (PID control algorithm) for compensating the error or deviation and thus following that path and reach destination. A low cost on board computer (Raspberry Pi in our case) handles all the calculations during the process and drives the rover fulfilling its task using an microcontroller (Arduino)."
220,excluded,10.1109/ieeeconf49454.2021.9382607,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9382607/,1/14/2021 0:00,teaching system for multimodal object categorization by human-robot interaction in mixed reality,"As service robots are becoming essential to support aging societies, teaching them how to perform general service tasks is still a major challenge preventing their deployment in daily-life environments. In addition, developing an artificial intelligence for general service tasks requires bottom-up, unsupervised approaches to let the robots learn from their own observations and interactions with the users. However, compared to the top-down, supervised approaches such as deep learning where the extent of the learning is directly related to the amount and variety of the pre-existing data provided to the robots, and thus relatively easy to understand from a human perspective, the learning status in bottom-up approaches is by their nature much harder to appreciate and visualize. To address these issues, we propose a teaching system for multimodal object categorization by human-robot interaction through Mixed Reality (MR) visualization. In particular, our proposed system enables a user to monitor and intervene in the robot&#x2019;s object categorization process based on Multimodal Latent Dirichlet Allocation (MLDA) to solve unexpected results and accelerate the learning. Our contribution is twofold by 1) describing the integration of a service robot, MR interactions, and MLDA object categorization in a unified system, and 2) proposing an MR user interface to teach robots through intuitive visualization and interactions."
221,included,10.1109/robot.2004.1308781,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1308781/,5/1/2004 0:00,software approach for the autonomous inspection robot makro,"The sewer inspection robot MAKRO is an autonomous multi-segment robot with worm-like shape driven by wheels. It is currently under development in the project MAKRO-PLUS. The robot has to navigate autonomously within sewer systems. Its first tasks is to take water probes, analyze them onboard, and measure positions of manholes and pipes to detect pollution loaded sewage and to improve current maps of sewer systems. One of the challenging problems is the control software, which should enable the robot to navigate in the sewer system and perform the inspection tasks autonomously, while always taking care of its own safety. Tests in our test environment and in a real sewer system show promising results. This paper focuses on the software approach. To manage the complexity a layered architecture has been chosen, each layer defining a different level of abstraction. After determining the abstraction levels, we use different methods for implementation. For the highest abstraction level a standard AI-planning algorithm is used. For the next level, finite state automata has been chosen. For ""simple"" task implementation we use a modular C++ based method (MCA2), which is also used on the lowest software level."
222,included,10.1109/lars-sbr.2016.49,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7783535/,10/12/2016 0:00,integration of people detection and simultaneous localization and mapping systems for an autonomous robotic platform,"This paper presents the implementation of a people detection system for a robotic platform able to perform Simultaneous Localization and Mapping (SLAM), allowing the exploration and navigation of the robot considering people detection interaction. The robotic platform consists of a Pioneer 3DX robot equipped with an RGB-D camera, a Sick Lms200 sensor laser and a computer using the robot operating system ROS. The idea is to integrate the people detection system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. Furthermore, this paper presents an evaluation of two different approaches for the people detection system. The first one uses a manual feature extraction technique, and the other one is based on deep learning methods. The manual feature extraction method in the first approach is based on HOG (Histogram of Oriented Gradients) detectors. The accuracy of the techniques was evaluated using two different libraries. The PCL library (Point Cloud Library) implemented in C ++ and the VLFeat MatLab library with two HOG variants, the original one, and the DPM (Deformable Part Model) variant. The second approaches are based on a Deep Convolutional Neural Network (CNN), and it was implemented using the MatLab MatConvNet library. Tests were made objecting the evaluation of losses and false positives in the people's detection process in both approaches. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment."
223,excluded,10.1016/j.enggeo.2020.105817,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85090411791,12/5/2020,"successful implementations of a real-time and intelligent early warning system for loess landslides on the heifangtai terrace, china","Real-time monitoring and intelligent early warning system are crucial and significant to take mitigation measures and reduce casualties and property losses related to landslides. It is difficult to obtain entire monitoring data in the accelerated deformation phase in a landslide event, and hard to issue early warning information using a traditional monitoring approach with fixed and low sampling frequency. Displacement increments of loess landslides induced by agriculture irrigation on the Heifangtai terrace could be sudden and extremely rapid. Typical landslide types include loess flowslides and loess falls. It is of practical significance to develop a self-adaptive data acquisition monitoring technique and establish a real-time landslide early warning system (LEWS) to meet the needs for risk mitigation of rapid sliding slopes on the Heifangtai terrace. The monitoring technique can wirelessly transmit displacement data and the LEWS was devised using the new artificial intelligence. The LEWS could automatically release the warning information in advance of the event once the early warning parameters exceed default thresholds. In this study, the early warning procedures, real-time monitoring approach, intelligent LEWS, a multiple criteria warning model, warning release and emergency mitigation measures, and performance are introduced in detail. Six loess landslides at Heifangtai and eight landslides in other regions of China have been successfully warned since its implementation in 2012. This study proposed an effective and practical solution for the early warning of loess landslides at Heifangtai. Two typical loess landslides that had successful early warnings at Heifangtai were presented. The successful implementation could serve as a reference for global rapid slope failure cases, considering the complex nature of landslide behaviors and failure mechanisms."
224,excluded,10.1016/j.simpat.2019.102015,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85074521783,5/1/2020,a fog computing model for implementing motion guide to visually impaired,"A guide dog robot system for visually impaired often needs to process many kinds of information, such as image, voice and other sensor information. Information processing methods based on deep neural network can achieve better results. However, it requires expensive computing and communication resources to meet the real-time requirement. Fog computing has emerged as a promising solution for applications that are data-intensive and delay-sensitive. We propose a fog computing framework named PEN (Phone + Embedded board + Neural compute stick) for the guide dog robot system. The robot’s functions in PEN are wrapped as services and deployed on the appropriate devices. Services are combined as an application in a visual programming language environment. Neural compute stick accelerates image processing speed at low power consumption. A simulation environment and a prototype are built on the framework. The simulated guide dog system is developed for operating in a miniature environment, including a small robot dog, a small wheelchair, model cars, traffic lights, and traffic blockage. The prototype is a full-sized portable guide system that can be used by a visually impaired person in a real environment. Simulation and experiments show that the framework can meet the functional and performance requirements for implementing the guide systems for visually impaired."
225,included,10.1016/j.robot.2018.02.010,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85044145526,6/1/2018,visual attention and object naming in humanoid robots using a bio-inspired spiking neural network,"Recent advances in behavioural and computational neuroscience, cognitive robotics, and in the hardware implementation of large-scale neural networks, provide the opportunity for an accelerated understanding of brain functions and for the design of interactive robotic systems based on brain-inspired control systems. This is especially the case in the domain of action and language learning, given the significant scientific and technological developments in this field. In this work we describe how a neuroanatomically grounded spiking neural network for visual attention has been extended with a word learning capability and integrated with the iCub humanoid robot to demonstrate attention-led object naming. Experiments were carried out with both a simulated and a real iCub robot platform with successful results. The iCub robot is capable of associating a label to an object with a ‘preferred’ orientation when visual and word stimuli are presented concurrently in the scene, as well as attending to said object, thus naming it. After learning is complete, the name of the object can be recalled successfully when only the visual input is present, even when the object has been moved from its original position or when other objects are present as distractors."
226,excluded,10.1016/0950-7051(94)90024-8,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/38149147114,1/1/1994,"multi-paradigm software environment for the real-time processing of sound, music and multimedia","The paper introduces a system and a software architecture for the representation and real-time processing of sound, music, and multimedia based on artificial intelligence techniques. This system, called WinProcne/HARP, is able to represent objects in a two-fold formalism—symbolic and analogical—at different levels of abstraction, and to carry out plans according to the user's goals. It also provides both formal and informal analysis capabilities for extracting information. In WinProcne/HARP the user can build, update, browse, and merge various knowledge bases of sound, music, and multimedia material, as well as enter queries, start and manage real time performance, using a high-level graphical user interface. The system is currently used by researchers and composers in various experiments, including (a) advanced robotics projects, in which the system is used as a tool for interacting, cotrolling and simulating robot movements, and (b) theatrical automation, where the system is delegated to manage and integrate sound, music, and three-dimensional computer animations of humanoid figures. The paper explicitly refers to some applications in the music field."
227,excluded,10.1109/spices.2017.8091310,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8091310/,8/10/2017 0:00,implementation of a self-adaptive real time recommendation system using spark machine learning libraries,"Real time recommendation systems have become an essential component of e-commerce web applications. With increasing volume and velocity of data handled by these applications, known as the bigdata problem, traditional recommendation systems that analyze data and update models at regular time intervals would not be able to satisfy this requirement. With the evolution of technologies for processing bigdata in real time, it has become fairly easy to implement real time recommendation systems. Stream-computing is a new computing paradigm for handling the velocity attribute of bigdata which makes it possible to develop real time bigdata applications. This paper gives the details of implementation of a real time recommendation system using Apache Spark, a widely used platform for stream computing. This system is implemented for recommending TV channels to viewers in real time. This becomes a challenging task due to continuous changes in the set of available channels and the context dependent preference of viewers. In channel recommendation scenario, characterized by its dynamic nature, volume of data, and tight time constraints, traditional approaches cannot be used. We have implemented a highly scalable TV channel recommendation system optimized for the processing of real-time data streams originating from set-top boxes. The proposed system implements a self-adaptive approach for model building. The system effectively uses distributed processing power of Apache Spark to make recommendations in real time with scalability to meet the real time constraints with increasing load. The Spark Machine Learning Libraries (Spark MLLib) provide several algorithms which were used for developing the proposed recommendation system. The large amount of data in the system is efficiently managed by the data processing method of Lambda Architecture."
228,unknown,10.1007/978-3-030-60884-2_6,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-60884-2_6,2020-01-01 00:00:00,implementation of a svm on an embedded system: a case study on fall detection,"Edge Computing seeks to bring Machine Learning as close as possible to the source events of interest, providing an almost instant interpretation to data acquired by sensors giving sense to raw data while addressing concerns of particular applications such as latency, privacy and server stress relieve. Due to a lack of research on this particular type of application, we are faced with difficulties both in software and hardware as embedded systems are known to possess serious limitations on its available processing resources. To address this, we make use of the concepts of edge computing and offline programming to accomplish a reliable machine learning model deployment on the microprocessor. By studying real case problem, we can get measurements on the resources required by such an application as well as its performance. In this study, we address the implementation of such an application in an embedded system focusing on the detection of human falls."
229,unknown,10.1145/3004056,ACM Trans. Cyber Phys. Syst.,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/39e0091d8e83f72346eeff751089a8f2507697d1,2017-01-01 00:00:00,a cloud-based black-box solar predictor for smart homes,"The popularity of rooftop solar for homes is rapidly growing. However, accurately forecasting solar generation is critical to fully exploiting the benefits of locally generated solar energy. In this article, we present two machine-learning techniques to predict solar power from publicly available weather forecasts. We use these techniques to develop SolarCast, a cloud-based web service that automatically generates models that provide customized site-specific predictions of solar generation. SolarCast utilizes a “black box” approach that requires only (1) a site’s geographic location and (2) a minimal amount of historical generation data. Since we intend SolarCast for small rooftop deployments, it does not require detailed site- and panel-specific information, which owners may not know, but instead automatically learns these parameters for each site. We evaluate the accuracy of SolarCast’s different algorithms on two publicly available datasets, each containing over 100 rooftop deployments with a variety of attributes (e.g., climate, tilt, orientation, etc.). We show that SolarCast learns a more accurate model using much less data (∼1 month) than prior SVM-based approaches, which require ∼3 months of data. SolarCast also provides a programmatic API, enabling developers to integrate its predictions into energy efficiency applications. Finally, we present two case studies of using SolarCast to demonstrate how real-world applications can leverage its predictions. We first evaluate a “sunny” load scheduler, which schedules a dryer’s energy usage to maximally align with a home’s solar generation. We then evaluate a smart solar-powered charging station, which can optimally charge the maximum number of electric vehicles (EVs) on a given day. Our results indicate that a representative home is capable of reducing its grid demand up to 40% by providing a modest amount of flexibility (of ∼5 hours) in the dryer’s start time with opportunistic load scheduling. Further, our charging station uses SolarCast to provide EV owners the amount of energy they can expect to receive from solar energy sources."
230,unknown,http://arxiv.org/abs/1801.03002v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1801.03002v2,2018-01-08 00:00:00,deepstyle: multimodal search engine for fashion and interior design,"In this paper, we propose a multimodal search engine that combines visual and
textual cues to retrieve items from a multimedia database aesthetically similar
to the query. The goal of our engine is to enable intuitive retrieval of
fashion merchandise such as clothes or furniture. Existing search engines treat
textual input only as an additional source of information about the query image
and do not correspond to the real-life scenario where the user looks for 'the
same shirt but of denim'. Our novel method, dubbed DeepStyle, mitigates those
shortcomings by using a joint neural network architecture to model contextual
dependencies between features of different modalities. We prove the robustness
of this approach on two different challenging datasets of fashion items and
furniture where our DeepStyle engine outperforms baseline methods by 18-21% on
the tested datasets. Our search engine is commercially deployed and available
through a Web-based application."
231,unknown,10.1016/j.procs.2022.01.318,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85127782163,2022-01-01,"predictive maintenance on sensorized stamping presses by time series segmentation, anomaly detection, and classification algorithms","Sheet metal forming tools, like stamping presses, play an ubiquitous role in the manufacture of several products. With increasing requirements of quality and efficiency, ensuring maximum uptime of these tools is fundamental to marketplace competitiveness. Using anomaly detection and predictive maintenance techniques, it is possible to develop lower risk and more intelligent approaches to maintenance scheduling, however, industrial implementations of these methods remain scarce due to the difficulties of obtaining acceptable results in real-world scenarios, making applications of such techniques in stamping processes seldom found. In this work, we propose a combination of two distinct approaches: (a) time segmentation together with feature dimension reduction and anomaly detection; and (b) machine learning classification algorithms, for effective downtime prediction. The approach (a)+(b) allows for an improvement rate up to 22.971% of the macro F1-score, when compared to sole approach (b). A ROC AUC index of 96% is attained by using Randomized Decision Trees, being the best classifier of twelve tested. An use case with a decentralized predictive maintenance architecture for the downtime forecasting of a stamping press, which is a critical machine in the manufacturing facilities of Bosch Thermo Technology, is discussed."
232,excluded,10.15835/buasvmcn-hort:2021.0030,'AcademicPres (EAP) Publishing House',core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/490608362.pdf,2021-11-29 00:00:00,bayesian ridge algorithm for brix prediction in  industrial tomato,"Tomato is one of the most significant vegetables in the world. Specifically, for the industrial tomato cultivation, the product is harvested when °Brix are at their peak. Technological advancements nowadays have made Decision Support Systems, based on Machine Learning Algorithms more applicable in a daily basis. Sustainable agriculture is evolving since farmers could be advised by this technology in order to take the best decision for their crops. Farmers who adopt this kind of technology will be able to know the quality of tomatoes. The implementation of a Decision Support System capable to predict the °Brix was conducted, based on various data from previous years, such as quality characteristics, the tomato hybrid used, weather conditions and soil data from the selected fields. Data came from fields from 6 different regions in Peloponnese, Greece over 3 cultivation periods. 12 different algorithms were tested in order to find which is the best one in terms of efficiency. Results of this research showed that the predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. The use of this DSS using real time weather data as an input will be a valuable tool for the farmers"
233,excluded,10.1007/978-3-030-86230-5_53,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-86230-5_53,2021-01-01 00:00:00,cloud based decision making for multi-agent production systems,"The use of multi-agent systems (MAS) as a distributed control method for shop-floor manufacturing control applications has been extensively researched. MAS provides new implementation solutions for smart manufacturing requirements such as the high dynamism and flexibility required in modern manufacturing applications. MAS in smart manufacturing is becoming increasingly important to achieve increased automation of machines and other components. Emerging technologies like artificial intelligence, cloud-based infrastructures, and cloud computing can also provide systems with intelligent, autonomous, and more scalable solutions. In the current work, a decision-making framework is proposed based on the combination of MAS cloud computing, agent technology, and machine learning. The framework is demonstrated in a quality control use case with vision inspection and agent-based control. The experiment utilizes a cloud-based machine learning pipeline for part classification and agent technology for routing. The results show the applicability of the framework in real-world scenarios bridging cloud service-oriented architecture with agent technology for production systems."
234,excluded,10.1109/icphm.2018.8448431,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8448431/,2018-06-13 00:00:00,industrial ai enabled prognostics for high-speed railway systems,"The vision of pervasive applications of artificial intelligence (AI) and the fact that hardware is becoming more portable and computationally powerful has encouraged the development of industrial AI. High-speed railway (HSR) transportation is an area of focus for industrial AI, since it demands maximized safety, reliability, availability, and minimized cost. Given the system complexity, the predictive maintenance for high-speed railway could hardly sustain with a raw experience-based system. This paper presents a framework for industrial AI enabled PHM system for HSR, which creates cyber twins of physical key subsystems and components to improve the condition transparency and decision efficiency. Enabled by advanced signal processing and machine learning with domain insights on historical data, cyber twins monitor real-time performance and predict potential faults to prevent unexpected downtime and support optimized decisions. Instead of performing analytics with large amount of raw data on the cloud, the cyber railway transportation system leverages the advantage of edge computing for real-time feature extraction and anomaly detection. The proposed framework introduces the general approach of implementing industrial AI. The paper also discusses the key methods of data-driven solutions for selected critical subsystems."
235,unknown,10.2118/203037-ms,,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/2a0dc87e45ebc4647cae0a3845d2dd8e44887bcc,2020-01-01 00:00:00,artificial intelligence aims to save lives in offshore marine vessels,"
 In ADNOC Oil and Gas 4.0 mission, we are committed to empower people with the needed capabilities and Artificial Intelligence (AI) technologies to fuel innovation, efficiency and more importantly achieve and sustain a 100% HSE, by transforming the way of handling HSE events by moving from reactive to proactive approach. The ultimate objective is to save lives, empower the vessel Captains to immediately identify and respond to violators, improve the HSE culture of the crew, and automatically generate live data analytics and statistics with the aim of improving safety in operations. The implemented AI use cases are; deviation for not wearing Protective Safety equipment in designated areas, violation of not utilizing safety passages, alert when no watchman in muster station, alarm when man overboard incident, alarm when man fell on stairs, and live Personnel on board each weather-deck. When introduced the Artificial Intelligence cameras, our marine vessels will adopt a smarter automated response and reporting culture, which will in turn, lead to increased safety oversight of our critical offshore operations. Therefore, with the advent of the AI technology, many common business processes have been automated thus enabling personnel to increase their focus on more important tasks while technologies like the AI System can handle many of the time consuming tasks.
 The solution components consists of Artificial Intelligence platform, high definition cameras, local server, wide-range WiFi access point, network infrastructure and a tablet. On the tablet device, the captain have full coverage of the vessel weather decks, working areas and restricted zones with a feature to generate alerts when detecting an emergency situation. This was provided to empower the vessel Captain to acknowledge and respond to violations as well as take a proactive action to prevent incidents from happening. The Machine Learning algorithm has been trained on actual scenarios and will be continuously improved by adding more recorded event to retrain the initial model. Currently, the prediction model is performing on the vessel operation mode and recording events with high rate of accuracy. In case of automatically detecting an alerting or non-compliance event, the captain would be notified, beacon lights and sound, and log recorded in the local and central system with a photo and a short video clip of the incident. The process of identifying HSE deviations are becoming digitally transformed by deploying AI capabilities on real-time video streams. The AI-based camera system leverages Computer Vision features that enables machines to get and analyze visual information and take action. The whole process of identifying HSE violation events has been digitally transformed by deploying an artificial intelligence solution to perform real time video analytics."
236,unknown,10.1109/access.2018.2879117,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8590712/,2019-01-01 00:00:00,a framework to estimate the nutritional value of food in real time using deep learning techniques,"There has been a rapid increase in dietary ailments during the last few decades, caused by unhealthy food routine. Mobile-based dietary assessment systems that can record real-time images of the meal and analyze it for nutritional content can be very handy and improve the dietary habits and, therefore, result in a healthy life. This paper proposes a novel system to automatically estimate food attributes such as ingredients and nutritional value by classifying the input image of food. Our method employs different deep learning models for accurate food identification. In addition to image analysis, attributes and ingredients are estimated by extracting semantically related words from a huge corpus of text, collected over the Internet. We performed experiments with a dataset comprising 100 classes, averaging 1000 images for each class to acquire top 1 classification rate of up to 85%. An extension of a benchmark dataset Food-101 is also created to include sub-continental foods. Results show that our proposed system is equally efficient on the basic Food-101 dataset and its extension for sub-continental foods. The proposed system is implemented as a mobile app that has its application in the healthcare sector."
237,excluded,10.1016/j.ipm.2018.04.011,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85048575075,2019-05-01,real-time processing of social media with sentinel: a syndromic surveillance system incorporating deep learning for health classification,"Interest in real-time syndromic surveillance based on social media data has greatly increased in recent years. The ability to detect disease outbreaks earlier than traditional methods would be highly useful for public health officials. This paper describes a software system which is built upon recent developments in machine learning and data processing to achieve this goal. The system is built from reusable modules integrated into data processing pipelines that are easily deployable and configurable. It applies deep learning to the problem of classifying health-related tweets and is able to do so with high accuracy. It has the capability to detect illness outbreaks from Twitter data and then to build up and display information about these outbreaks, including relevant news articles, to provide situational awareness. It also provides nowcasting functionality of current disease levels from previous clinical data combined with Twitter data.
                  The preliminary results are promising, with the system being able to detect outbreaks of influenza-like illness symptoms which could then be confirmed by existing official sources. The Nowcasting module shows that using social media data can improve prediction for multiple diseases over simply using traditional data sources."
238,unknown,10.1109/electr.1991.718282,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/718282/,1991-04-18 00:00:00,imaging and controls for mars robots with neural networks,"Two aspects of the design of space robots is covered implemented by neural networks and by hybrid approach with artificial intelligence. One is a neurocontroller for a real-time autonomous system. An optical control system developed saves the time for the image processing that analyzes an image sensor through the environment and induces a transformation over the sensor array. A prototype of the neurocontroller is able to learn and control by itself. The second aspect deals with the design of a Servo Control System for a Robot with the capability of ""learning in Unanticipated Situations"" incorporated in the system. The robot is assumed to be employed to perform useful tasks in an alien evironment. The model developed is shown to provide the robot with the capability to recover from unanticipated situations that can lead to the disruption of its normal operation, and to learn to avoid such situations in the future. These two aspects will be integrated for a design of a very intelligent autonomous space robot."
239,excluded,10.1007/s00607-020-00897-4,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00607-020-00897-4,2021-01-06 00:00:00,a novel indoor localization system using machine learning based on bluetooth low energy with cloud computing,"In this paper, we propose a novel indoor localization system in a multi-indoor environment using cloud computing. Prior studies show that there are always concerns about how to avoid signal occlusion and interference in the single indoor environment. However, we find some general rules to support our system being immune to interference generated by occlusion in the multi-indoor environment. A convenient way is measured to deploy Bluetooth low energy devices, which mainly collect large information to assist localization. A neural network-based classification is proposed to improve localization accuracy, compared with several algorithms and their performance comparison is discussed. We also design a distributed data storage structure and establish a platform considering the storage load with Redis. Our real experimental validation shows that our system will meet the four aspects of performance requirements, which are higher accuracy, less power consumption, and increased levels of system magnitude and deployment efficiency."
240,included,10.1109/itsc45102.2020.9294435,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1fcacf7fc81ff9366229c02440e1dc70c3ae28c1,2020-01-01 00:00:00,architecture design and development of an on-board stereo vision system for cooperative automated vehicles,"In a cooperative automated driving scenario like platooning, the ego vehicle needs reliable and accurate perception capabilities to autonomously follow the lead vehicle. This paper presents the architecture design and development of an on-board stereo vision system for cooperative automated vehicles. The input to the proposed system is stereo image pairs. It uses three deep neural networks to detect and classify objects, lane markings, and free space boundary simultaneously in front of the ego vehicle. The rectified left and right image frames of the stereo camera are used to compute a disparity map to estimate the detected object’s depth and radial distance. It also estimates the object’s relative velocity, azimuth, and elevation angle with respect to the ego vehicle. It sends the perceived information to the vehicle control system and displays the perceived information in a meaningful way on the human-machine interface. The system runs on both PC (x86_64 architecture) with Nvidia GPU, and the Nvidia Drive PX 2 (aarch64 architecture) automotive-grade compute platform. It is deployed and evaluated on Renault Twizy cooperative automated driving research platform. The presented results show that the stereo vision system works in real-time and is useful for cooperative automated vehicles."
241,excluded,10.1109/tie.2021.3068681,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9390342/,2022-03-01 00:00:00,degradation estimation and prediction of electronic packages using data-driven approach,"Recent trends in automotive electronics such as automated driving will increase the number and complexity of electronics used in safety-relevant applications. Applications in logistics or ridesharing will require a specific year of service rather than the conventional mileage usage. Reliable operations of the electronic systems must be assured at all times, regardless of the usage condition. A more dynamic and on-demand way of assuring the system availability will have to be developed. This article proposes a thermomechanical stress-based prognostics method as a potential solution. The goal is achieved by several novel advancements. On the experimental front, a key microelectronics package is developed to directly apply the prognostics and health management concept using a piezoresistive silicon-based stress sensor. Additional hardware for safe and secure data transmission and data processing is also developed, which is critically required for recording in situ and real-time data. On the data management front, proper data-driven approaches have to be identified to handle the unique dataset from the stress sensor employed in this study. The approaches effectively handle the massive amount of data that reveals the important information and automation of the prognostic process and thus to be able to detect, classify, locate, and predict the failure. The statistical techniques for diagnostics and the machine learning algorithms for health assessment and prognostics are also determined to implement the approaches in a simple, fast, but accurate way within the capacity of limited computing power. The proposed prognostics approach is implemented with actual microelectronics packages subjected to harsh accelerated testing conditions. The results corroborate the validity of the proposed prognostics approach."
242,excluded,10.1109/tps.2013.2276537,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6582663/,2013-09-01 00:00:00,space-varying templates for real-time applications of cellular nonlinear networks to pattern recognition in nuclear fusion,"In this paper, a new methodology consisting of space-varying templates in cellular nonlinear networks (CNNs) for real-time visual pattern recognition in nuclear fusion devices is presented. The development of space-varying templates is a new upgrade, driven by the need to process different parts of the images in different ways. The new approach has been applied to the identification in real time of various objects present in the Joint European Torus videos of both infrared (IR) and visible cameras. IR videos are here used to detect hot spots and the regions of the walls in which dangerously high temperatures are reached, whereas visible cameras provide information about multifaceted asymmetric radiations from the edge, which are dangerous instabilities that can lead to disruptions. Their identification is particularly difficult because of their movement and their shape which is similar to other objects present in the frames. Therefore, in addition to space-varying template CNNs, quite sophisticated morphological operators have to be deployed and their outputs processed by machine learning tools, such as support vector machines. The implementation of the whole methodology was performed in a field-programmable gate array board, obtaining, in both applications, a final success rate close to 100% and a frame rate higher than 200 frames/s."
243,unknown,http://arxiv.org/abs/2005.05287v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2005.05287v2,2020-05-11 00:00:00,"using computer vision to enhance safety of workforce in manufacturing in
  a post covid world","The COVID-19 pandemic forced governments across the world to impose lockdowns
to prevent virus transmissions. This resulted in the shutdown of all economic
activity and accordingly the production at manufacturing plants across most
sectors was halted. While there is an urgency to resume production, there is an
even greater need to ensure the safety of the workforce at the plant site.
Reports indicate that maintaining social distancing and wearing face masks
while at work clearly reduces the risk of transmission. We decided to use
computer vision on CCTV feeds to monitor worker activity and detect violations
which trigger real time voice alerts on the shop floor. This paper describes an
efficient and economic approach of using AI to create a safe environment in a
manufacturing setup. We demonstrate our approach to build a robust social
distancing measurement algorithm using a mix of modern-day deep learning and
classic projective geometry techniques. We have deployed our solution at
manufacturing plants across the Aditya Birla Group (ABG). We have also
described our face mask detection approach which provides a high accuracy
across a range of customized masks."
244,unknown,10.1109/ccnc49033.2022.9700676,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9700676/,2022-01-11 00:00:00,qos-aware priority-based task offloading for deep learning services at the edge,"Emerging Edge Computing (EC) technology has shown promise for many delay-sensitive Deep Learning (DL) based applications of smart cities in terms of improved Quality-of-Service (QoS). EC requires judicious decisions which jointly consider the limited capacity of the edge servers and provided QoS of DL-dependent services. In a smart city environment, tasks may have varying priorities in terms of when and how to serve them; thus, priorities of the tasks have to be considered when making resource management decisions. In this paper, we focus on finding optimal offloading decisions in a three-tier user-edge-cloud architecture while considering different priority classes for the DL-based services and making a trade-off between a task&#x2019;s completion time and the provided accuracy by the DL-based service. We cast the optimization problem as an Integer Linear Program (ILP) where the objective is to maximize a function called gain of system (GoS) defined based on provided QoS and priority of the tasks. We prove the problem is NP-hard. We then propose an efficient offloading algorithm, called PGUS, that is shown to achieve near-optimal results in terms of the provided GoS. Finally, we compare our proposed algorithm, PGUS, with heuristics and a state-of-the-art algorithm, called GUS, using both numerical analysis and real-world implementation. Our results show that PGUS outperforms GUS by a factor of 45% in average in terms of serving the top 25% higher priority classes of the tasks while still keeping the overall percentage of the dropped tasks minimal and the overall gain of system maximized."
245,unknown,10.1007/s11704-015-4404-7,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11704-015-4404-7,2015-12-01 00:00:00,non-intrusive sleep pattern recognition with ubiquitous sensing in elderly assistive environment,"The quality of sleep may be a reflection of an elderly individual’s health state, and sleep pattern is an important measurement. Recognition of sleep pattern by itself is a challenge issue, especially for elderly-care community, due to both privacy concerns and technical limitations. We propose a novelmulti-parametric sensing system called sleep pattern recognition system (SPRS). This system, equipped with a combination of various non-invasive sensors, can monitor an elderly user’s sleep behavior. It accumulates the detecting data from a pressure sensor matrix and ultra wide band (UWB) tags. Based on these two types of complementary sensing data, SPRS can assess the user’s sleep pattern automatically via machine learning algorithms. Compared to existing systems, SPRS operateswithout disrupting the users’ sleep. It can be used in normal households with minimal deployment. Results of tests in our real assistive apartment at the Smart Elder-care Lab are also presented in this paper."
246,unknown,10.23919/eecsi53397.2021.9624311,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9624311/,2021-10-21 00:00:00,strawberry fruit quality assessment for harvesting robot using ssd convolutional neural network,"Strawberry has a tremendous economic value as well as being visually appealing. Therefore, strawberry farmers need to ensure that they only harvest good quality strawberries. However, assessing the quality of strawberries is not an easy problem, especially for local plantations which do not have enough human resources. As robotics becomes accessible and widely used for agriculture work such as harvesting fruit, the real-time embedded system computation power becomes much more powerful nowadays. This paper discusses the harvesting robot&#x0027;s ability to distinguish the quality of strawberries in realtime detection using computer vision technology in the form of object detection by utilizing a deep neural network in a single board computer (SBC). The robot software is built on Robot Operating System (ROS) framework. The proposed method is tested on a robot equipped with a monocular camera. The learning process shows that the robot can detect and differentiate between good and bad quality strawberries with 90&#x0025; accuracy and maintain a high frame rate."
247,unknown,10.1109/icarm.2017.8273193,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8273193/,2017-08-31 00:00:00,development of a mobile app for the operation monitoring and health management system of a steam turbine,"In order to realize the real-time monitoring of the operation state and predict the possible faults of a steam turbine, a mobile APP based on the steam turbine operation monitoring and health management system is proposed in this paper. Firstly, some common faults which will be encountered in the operation of steam turbine are analyzed and the mechanism and characteristics of the faults are studied. Secondly, the hardware and software platform on the PC are built to diagnose the fault and analyze the trend by combining the methods of wavelet packet decomposition and BP neural network. Finally, the APP on the mobile terminal for the real-time operation monitoring of the steam turbine is developed. Furthermore, the test results of the mobile APP demonstrate that the developed mobile APP can effectively diagnose the fault and predict the trend. It also proved that the developed mobile APP can make the operation monitoring and health management system of the steam turbine run stably and meet the practical application requirements."
248,excluded,10.1016/b978-0-323-85117-6.00007-8,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85129907981,2021-01-01,livestock health monitoring using a smart iot-enabled neural network recognition system,"A poultry farm monitoring system along with a livestock monitoring system is a principal system for investigating the status of bird health by collecting biological traits such as their uttered sounds. This theme combines the Internet of Things (IoT) with nonintrusive and reliable wearable sensing technology. In recent developments, machine learning (ML) or artificial neural networks (ANNs) have been well applied and recognized as an effective tool for a range of complicated scenario analyses in real time, including healthcare sector applications. This will help to alleviate problems typically suffered and faced by medical researchers in these fields by saving time for practitioners by providing unbiased results. In this chapter we discuss the utilization of analytics learning and neural network usage toward clinical concerns in health care using the IoT. ANN is a prime research domain with recent deployment of computational sophistication in hardware and software in several application domains with highly complicated computing scenarios. The healthcare sector is one area which is capable of automation to save time and that is subjective by nature. Therefore, ML- and ANN-based simulations generate unbiased outcomes. This chapter describes an IoT-structured wearable sensing platform with the inclusion of an audio feature and temperature of the livestock. In particular, the secure audio-wellbeing features are incorporated into the platform to spontaneously examine and conclude using voice information from the livestock for recognition of diseased birds. One month of long-term recognition experimentation analysis was performed where the recognition accuracy of the onset of disease bird was about 91% using a spiking neural network (SNN).The recognition accuracy of SNN in this regard is better than the performance of an ANN. A sequence of steps was taken in connection with a specific event that occurred and involved in examining the interrelationship across the central monitoring unit and the local monitoring unit using the IoT by utilizing the bird voice features, bird temperature, and room temperature and humidity."
249,excluded,http://arxiv.org/abs/1907.00594v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1907.00594v1,2019-07-01 00:00:00,"fingerprint-based localization using commercial lte signals: a
  field-trial study","Wireless localization for mobile device has attracted more and more interests
by increasing the demand for location based services. Fingerprint-based
localization is promising, especially in non-Line-of-Sight (NLoS) or rich
scattering environments, such as urban areas and indoor scenarios. In this
paper, we propose a novel fingerprint-based localization technique based on
deep learning framework under commercial long term evolution (LTE) systems.
Specifically, we develop a software defined user equipment to collect the real
time channel state information (CSI) knowledge from LTE base stations and
extract the intrinsic features among CSI observations. On top of that, we
propose a time domain fusion approach to assemble multiple positioning
estimations. Experimental results demonstrated that the proposed localization
technique can significantly improve the localization accuracy and robustness,
e.g. achieves Mean Distance Error (MDE) of 0.47 meters for indoor and of 19.9
meters for outdoor scenarios, respectively."
250,unknown,10.1007/978-3-319-95273-4_5,Springer,springer,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-319-95273-4_5,2019-01-01 00:00:00,applying sound-based analysis at porsche production: towards predictive maintenance of production machines using deep learning and internet-of-things technology,"(a) Situation faced : All mechanical and mechatronic devices are subject to wear, tear and breakdown. Failure of such devices can cause significant costs, e.g., in automotive factories. Established predictive maintenance approaches usually require deep integration with the specific machine. Such approaches are not practically feasible because of technical, legal and financial restrictions. A non-intrusive, lightweight and generic solution approach is desired. (b) Action taken : A solution concept was developed which, at its heart, is based on deep learning algorithms that monitor sound sequences captured from a microphone, analyze them and return classification results for use in further steps of a control loop, such as planning actions and execution steps. We named this approach the ‘Sound Detective’ and it was evaluated by retrofitting a coffee machine using simple microphones to capture production sounds. The sound sequences are subsequently analyzed using neural networks developed in Keras and TensorFlow. During prototyping, multiple kinds of neural networks and architectures were tested and the experiment was realized with two different kinds of coffee machines to validate the generalizability of the solution to different platforms. (c) Results achieved : The prototype can analyze sounds produced by a mechanical machine and classify different states. The technical realization relies on cheap commodity hardware and open-source software, demonstrating the applicability of existing technologies and the feasibility of the implementation. Especially, it was described that the proposed approach can be applied to solve predictive maintenance tasks. (d) Lessons learned : The present work demonstrates the feasibility of the Sound Detective’s reference architecture and discusses challenges and learnings during implementation. Specifically, key learnings include the importance of data quality, preprocessing and consistency, influences of the experimental setup on real-world prediction performance and the relevance of microcomputers, the target hardware and type of the programming language for complex analyses."
251,excluded,10.1109/hpsr54439.2022.9831399,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9831399/,2022-06-08 00:00:00,a novel personnel counting method based on wifi perception,"In the Internet of Things (IoT) era, WiFi is now commonly implemented worldwide as a convenient wireless data transmission technology and brings much convenience to people&#x2019;s life. Personnel counting, which plays an indispensable role in many areas, such as indoor crowd control, public safety, and marketing analytics, is also important in some special events. In emergencies, such as bank robberies, where police need to capture the number of robbers and hostages in a robbed bank, counting the number of people within the region using WiFi perception technology is more reliable and safer than traditional counting methods based on video streaming. In the early work of others, researchers used received signal strength indicator (RSSI) from the MAC layer for personnel counting studies. In recent years, it has been found that the channel state information (CSI) from the physical layer is more stable and the personnel counting method based on CSI has a higher accuracy rate. In their excellent works, however, researchers do not take into consideration the unpredictability of crowd behavior in real life. Meanwhile, the experimental accuracy needs to be greatly improved. This paper, therefore, focuses on proposing a novel CSI-based WiFi perception personnel counting method using the Long Short Term Memory (LSTM) algorithm that is useful for hidden counting the number of people in different situations. We conducted people counting experiments under three real experimental scenarios. In addition, we have compared three other algorithms for machine learning and optimized some parameters to achieve an overall accuracy of over 97% for our method."
252,excluded,10.1109/metroind4.0iot51437.2021.9488447,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9488447/,2021-06-09 00:00:00,iot data-driven experimental process optimisation for kevlar fiberglass components for aeronautic,"This paper describes the work carried out during the PROOF experiment (IOT data-driven experimental PROcess Optimization for kevlar Fiberglass components for aeronautic), winner of the second open call of the MIDIH EU project. The main objectives of the experiment are the integration of smart sensing devices with the Energy@Work IoT gateways and the development of cloudified innovative data-driven methodologies and data analytics tools to support process optimization in the production of hybrid composite material parts for the aeronautical sector. Collection of real-time production-data from multiple sensors with several industrial protocols and data transfer to the MIDIH project platform has been performed adopting the IoT gateway developed by Energy@Work, following MIDIH reference architecture for advanced data processing and visualization (e.g., Fiware Orion Context Broker, Apache Flink and Fiware Knowage) by using MQTT protocol. Then, historical and new acquired data has been analysed using advanced clustering techniques and trends, with the purpose to allow a novel CPS-based predictive system on the production process. Machine-Learning algorithms and visualisations (GUI based on Fiware Knowage) in real operating conditions have been used to validate the performance and assess the outcome. Finally, thanks to the implementation of specific optimization rules, able to process data gathered from the sensor network, a framework for distributed processing engine has been exploited by (i) generating tips for energy efficiency and process optimization and (ii) providing different type of alarms based on expected consumptions, resulting in concrete support to production managers for the improvement of the whole production value chain."
253,excluded,10.1109/jiot.2020.3020911,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9184072/,2015-02-15 20:21:00,achieving democracy in edge intelligence: a fog-based collaborative learning scheme,"The emergence of fog computing has brought unprecedented opportunities to the Internet-of-Things (IoT) field, and it is now feasible to incorporate deep learning at the edge of the IoT network to provide a wide range of highly tailored services. In this article, we present a fog-based democratically collaborative learning scheme in which fog nodes collaborate on the model training process even without the support of the cloud, contributing to the advances of IoT in terms of realizing a more intelligent edge. To achieve that, we design a voting strategy so that a fog node could be elected as the coordinator node based on both distance and computational power metrics to coordinate the training process. Also, a collaborative learning algorithm is proposed to generalize the training of different deep learning models in the fog-enabled IoT environment. We then implement two popular use cases, including a user trajectory prediction and a distributed image recognition, to demonstrate the feasibility, practicality, and effectiveness of the scheme. More importantly, the experiments on both use cases are conducted through a real world, in-door fog deployment. The result shows that the scheme can utilize fog to obtain a well-performing deep learning model in the cloudless IoT environment while mitigating the data locality issue for each fog node."
254,unknown,10.1109/icscds53736.2022.9760986,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9760986/,2022-04-09 00:00:00,enterprise email server data protection system using geo-fence technology and machine learning,"Data sharing and protection is becoming an increasingly important element of end users&#x0027; daily life as they access various systems, services, and apps. Data disclosure often occurs in real-world email services. Verification and copyright protection of multimedia content has always been a huge problem for secure data transfer media. The problem is exacerbated by the increasing use of the Internet and digital technology. But, implementing copyright protection is very complex and difficult. Digital watermarking emerged as a best solution to the problem of copyright protection. In the proposed method using Geofence technology both the Watermarking and Encryption method is used to share active content. A geofence is a virtual boundary around a physical location. A Geofence could be built dynamically in the form of a radius around a spot. Watermarking is used to hide information such as hiding private information on digital media such as photos. Encryption techniques are used to provide data safety and security. By encryption, encrypted information prevents unauthorized access and unauthorized persons from reading it. Finally, the authorized user can extract the encryption key with the help of the entered data entry process. Unauthorized access can be detected, where user information does not match the embedded information. This proposed application helps to track unauthorized access and prevent the distribution of content in the email environment Also provide group data sharing based on legal process using machine learning algorithm and provide approval for the post delivery system. suppose any unauthorized access to the information is identified, a notification to the admin regarding the forgery will be sent."
255,excluded,10.1109/isqed.2018.8357325,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8357325/,2018-03-14 00:00:00,low cost and power cnn/deep learning solution for automated driving,"Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is “finding drivable free space”, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving."
256,excluded,https://core.ac.uk/download/pdf/144848064.pdf,A Low-cost Approach for Detecting Activities of Daily Living using Audio Information: A Use Case on Bathroom Activity Monitoring,core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),,2016-04-21 00:00:00,10.5220/0005803700260032,"In this paper, we present an architecture for recognizing events related to activities of daily living in the context of a health monitoring environment. The proposed approach explores the integration of a Raspberry PI single-board PC both as an audio acquisition and analysis unit. A set of real-time feature extraction and classification procedures has been implemented and integrated on the Raspberry PI device, in order to provide continuous and online audio event recognition. In addition, a tuning and calibration workflow is presented, according to which the technicians installing the device in a fast ans user-friendly manner, without any requirements for machine learning expertise. The proposed approach has been evaluated against a particular scenario that is rather important in the context of any healthcare monitoring system for the elder, namely the ""bathroom scenario"" according to which a single microphone installed on a Raspberry PI device is used to monitor bathroom activity in a 24/7 basis. Experimental results indicate a satisfactory performance rate on the classification process (around 70% for five bathroom-related audio classes) even when less than two minutes of annotated data are used for training in the installation procedure. This makes the whole procedure non demanding in terms of time and effort needed to be calibrated by the technician"
257,excluded,10.1016/j.enbuild.2022.111988,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85125646504,2022-04-15,a digital twin predictive maintenance framework of air handling units based on automatic fault detection and diagnostics,"The building industry consumes the most energy globally, making it a priority in energy efficiency initiatives. Heating, ventilation, and air conditioning (HVAC) systems create the heart of buildings. Stable air handling unit (AHU) functioning is vital to ensuring high efficiency and extending the life of HVAC systems. This research proposes a Digital Twin predictive maintenance framework of AHU to overcome the limitations of facility maintenance management (FMM) systems now in use in buildings. Digital Twin technology, which is still at an initial stage in the facility management industry, use Building Information Modeling (BIM), Internet of things (IoT) and semantic technologies to create a better maintenance strategy for building facilities. Three modules are implemented to perform a predictive maintenance framework: operating fault detection in AHU based on the APAR (Air Handling Unit Performance Assessment Rules) method, condition prediction using machine learning techniques, and maintenance planning. Furthermore, the proposed framework was tested in a real-world case study with data between August 2019 and October 2021 for an educational building in Norway to validate that the method was feasible. Inspection information and previous maintenance records are also obtained through the FM system. The results demonstrate that the continually updated data combined with APAR and machine learning algorithms can detect faults and predict the future state of Air Handling Unit (AHU) components, which may assist in maintenance scheduling. Removing the detected operating faults resulted in annual energy savings of several thousand dollars due to eliminating the identified operating faults."
258,excluded,http://arxiv.org/abs/2109.07846v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2109.07846v1,2021-09-16 00:00:00,"telehealthcare and covid-19: a noninvasive & low cost invasive, scalable
  and multimodal real-time smartphone application for early diagnosis of
  sars-cov-2 infection","The global coronavirus pandemic overwhelmed many health care systems,
enforcing lockdown and encouraged work from home to control the spread of the
virus and prevent overrunning of hospitalized patients. This prompted a sharp
widespread use of telehealth to provide low-risk care for patients.
Nevertheless, a continuous mutation into new variants and widespread
unavailability of test kits, especially in developing countries, possess the
challenge to control future potential waves of infection. In this paper, we
propose a novel Smartphone application-based platform for early diagnosis of
possible Covid-19 infected patients. The application provides three modes of
diagnosis from possible symptoms, cough sound, and specific blood biomarkers.
When a user chooses a particular setting and provides the necessary
information, it sends the data to a trained machine learning (ML) model
deployed in a remote server using the internet. The ML algorithm then predicts
the possibility of contracting Covid-19 and sends the feedback to the user. The
entire procedure takes place in real-time. Our machine learning models can
identify Covid-19 patients with an accuracy of 100%, 95.65%, and 77.59% from
blood parameters, cough sound, and symptoms respectively. Moreover, the ML
sensitivity for blood and sound is 100%, which indicates correct identification
of Covid positive patients. This is significant in limiting the spread of the
virus. The multimodality offers multiplex diagnostic methods to better classify
possible infectees and together with the instantaneous nature of our technique,
demonstrates the power of telehealthcare as an easy and widespread low-cost
scalable diagnostic solution for future pandemics."
259,excluded,10.1364/jocn.403205,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9275288/,2021-01-01 00:00:00,open whitebox architecture for smart integration of optical networking and data center technology [invited],"In this paper, we identify challenges in developing future optical network infrastructure for new services based on technologies such as 5G, virtual reality, and artificial intelligence, and we suggest approaches to handling these challenges that include a business model, architecture, and diversity. Through activities in multiservice agreement and de facto standard organizations, we have shown how the hardware abstraction layer interfaces of optical transceivers are implemented for multivendor and heterogeneous environments, coherent digital signal processor interoperability, and optical transport whiteboxes. We have driven the effort to define the transponder abstraction interface with partners. The feasibility of such implementation was verified through demonstrations and trials. In addition, we are constructing an open-transport platform by combining existing open-source software and implementing software components that automate and enhance operations. An open architecture maintains a healthy ecosystem for industry and allows for a flexible, operator-driven network."
260,unknown,10.1016/j.procs.2020.04.199,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85086630682,2020-01-01,perspective vehicle license plate transformation using deep neural network on genesis of cpnet,"Recent development in vehicular industries and increased number of cars in modern society leads the people to pay more attention on Vehicle License Plate Recognition (V-LPR). V-LPR plays a major role in traffic related application such as road traffic monitoring, vehicle parking lots access control etc. Existing state of the art V-LPR systems in real world deployment works under restricted conditions, such as static illumination, fixed background etc. Most of them fails to work when any of the above given conditions are violated. Hence to address this issue, a novel V-LPR system is designed using modern deep learning framework called ""Capsule Network"". The proposed system is robust and works fine in any condition. Further, the proposed method aims to improve the processing time by integrating the segmentation process within the CN framework which involves the training and recognizing of entire license plate cropped region. Moreover, the feature extraction is performed by CN framework over a segmented alphanumeric character. Finally, Data augmentation technique is also used as a supplement to the CN framework to strengthen the process of training with various orientations like rotation, shift and flip for improving the recognition task."
261,excluded,10.1109/gucon48875.2020.9231114,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9231114/,2020-10-04 00:00:00,a novel machine learning based wearable belt for fall detection,"Falls are a major cause of hip fractures in elderly people. Such fractures take a lot of time to heal even after a surgery is performed. In addition to this, the majority of such injuries prove to be fatal due to the lack of quick communication and immediate medical response. This situation is pretty common in today's world where the elderly are most of the time unattended at home. Owing to this need, we have designed a system to detect such falls leveraging machine learning and signal processing algorithms deployed over a simple 32-bit micro-controller. To achieve higher accuracy, we prepared our custom dataset of various types of fall as well as other daily routine activities. Our device informs the close relatives/family via a GSM Module when a fall is detected. The main purpose of our system is to detect a fall and trigger the alert system and take immediate action to minimize the impact of the fall. Our system has been able to detect a fall within 0.25 seconds with high accuracy. This can further be used to develop a real-time safety mechanism gear to minimize the injury in case of a fall."
262,unknown,10.1109/icit.2017.7915520,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7915520/,2017-03-25 00:00:00,an intelligent maintenance planning framework prototype for production systems,"The Intelligent Maintenance Planner (IMP) is designed to automate and improve maintenance processes in industrial applications. The system tracks the entire process cycle beginning with data acquisition and management, it then detects and classifies failure states, initializes maintenance cases, and selects and assigns the required resources. IMP guides maintenance work processes, by automatically providing instructions and augmented reality information. Subsequent feedback of the maintenance process and new or updated information is added to the system and used to train selection algorithms. A prototype of IMP was implemented based on an industrial SCADA system and cloud solutions for storage and machine learning capabilities. This report explains the stages of the maintenance process and provides an outline of the implementation and project results."
263,excluded,10.1109/tase.2020.3048056,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9326384/,2022-04-01 00:00:00,intelligent fault diagnosis for large-scale rotating machines using binarized deep neural networks and random forests,"Recently, deep neural network (DNN) models work incredibly well, and edge computing has achieved great success in real-world scenarios, such as fault diagnosis for large-scale rotational machinery. However, DNN training takes a long time due to its complex calculation, which makes it difficult to optimize and retrain models. To address such an issue, this work proposes a novel fault diagnosis model by combining binarized DNNs (BDNNs) with improved random forests (RFs). First, a BDNN-based feature extraction method with binary weights and activations in a training process is designed to reduce the model runtime without losing the accuracy of feature extraction. Its generated features are used to train an RF-based fault classifier to relieve the information loss caused by binarization. Second, considering the possible classification accuracy reduction resulting from those very similar binarized features of two instances with different classes, we replace a Gini index with ReliefF as the attribute evaluation measure in training RFs to further enhance the separability of fault features extracted by BDNN and accordingly improve the fault identification accuracy. Third, an edge computing-based fault diagnosis mode is proposed to increase diagnostic efficiency, where our diagnosis model is deployed distributedly on a number of edge nodes close to the end rotational machines in distinct locations. Extensive experiments are conducted to validate the proposed method on the data sets from rolling element bearings, and the results demonstrate that, in almost all cases, its diagnostic accuracy is competitive to the state-of-the-art DNNs and even higher due to a form of regularization in some cases. Benefited from the relatively lower computing and storage requirements of BDNNs, it is easy to be deployed on edge nodes to realize real-time fault diagnosis concurrently. <i>Note to Practitioners</i>&#x2014;Rotating machines, such as engines and motors, are the cornerstones of the modern industry. Edge computing is an emerging computing paradigm where computation is performed on the edges of networks rather than on the central cloud, thereby reducing system response time, transmission overhead, storage space, and computation resources of the cloud. Motivated by the high demand on computation for deploying DNN models and lower computation complexity for running BDNN models and easiness for large-scale deployment of BDNNs, an edge computing-based method for real-time fault diagnosis of rotating machines is proposed. First, we design a BDNN-based feature extractor to decrease the amount of computation and speed up a diagnosis processes. Then, the resulting binary features are fed to train an RF-based classifier, where we use ReliefF instead of Gini index when training a random forest model to further improve the proposed method&#x2019;s diagnostic accuracy. Finally, a novel cloud-edge collaborative computing-based fault diagnostic mode is presented, where the model trained from the central cloud is deployed on the edge computing devices distributed in large-scale scenarios to realize real-time fault diagnosis. Experiment results show that the proposed method can maintain the desired accuracy but greatly enhance the diagnosis speed when deployed on the edge nodes near end physical machines. It is easily extended and used for fault detection in many industrial sectors."
264,unknown,10.1109/iemtronics55184.2022.9795771,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9795771/,2022-06-04 00:00:00,a real-time parking space occupancy detection using deep learning model,"A camera is a tool to record visual footage in the form of photographs, film or in video format. However, a smart camera can be recognized as a device to retrieve application-specific information from the recorded footage. In this paper, we have proposed a solution to detect parking lot occupancy status using deep learning model and commercially used CCTV cameras in real time. Our implemented solution is decentralized and efficient in terms of light-weight deployment to low powered devices like Raspberry Pi. Our proposed solution is compared with the existing approaches. Our deep learning model is also tested on other datasets having images taking from multiple CCTV camera implemented in different height. Along with this, we have tested our model on indoor both outdoor parking garages in low light conditions during day and evening. Result of the performed experiments shows that our model is operable in low-powered embedded devices with effective accuracy."
265,excluded,http://arxiv.org/abs/2205.10635v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2205.10635v1,2022-05-21 00:00:00,"splitplace: ai augmented splitting and placement of large-scale neural
  networks in mobile edge environments","In recent years, deep learning models have become ubiquitous in industry and
academia alike. Deep neural networks can solve some of the most complex
pattern-recognition problems today, but come with the price of massive compute
and memory requirements. This makes the problem of deploying such large-scale
neural networks challenging in resource-constrained mobile edge computing
platforms, specifically in mission-critical domains like surveillance and
healthcare. To solve this, a promising solution is to split resource-hungry
neural networks into lightweight disjoint smaller components for pipelined
distributed processing. At present, there are two main approaches to do this:
semantic and layer-wise splitting. The former partitions a neural network into
parallel disjoint models that produce a part of the result, whereas the latter
partitions into sequential models that produce intermediate results. However,
there is no intelligent algorithm that decides which splitting strategy to use
and places such modular splits to edge nodes for optimal performance. To combat
this, this work proposes a novel AI-driven online policy, SplitPlace, that uses
Multi-Armed-Bandits to intelligently decide between layer and semantic
splitting strategies based on the input task's service deadline demands.
SplitPlace places such neural network split fragments on mobile edge devices
using decision-aware reinforcement learning for efficient and scalable
computing. Moreover, SplitPlace fine-tunes its placement engine to adapt to
volatile environments. Our experiments on physical mobile-edge environments
with real-world workloads show that SplitPlace can significantly improve the
state-of-the-art in terms of average response time, deadline violation rate,
inference accuracy, and total reward by up to 46, 69, 3 and 12 percent
respectively."
266,excluded,10.1364/jocn.449009,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9721710/,2022-04-01 00:00:00,scalability analysis of machine learning qot estimators for a cloud-native sdn controller on a wdm over sdm network,"Maintaining a good quality of transmission (QoT) in optical transport networks is key to maintaining the service level agreement between the user and the service provider. QoT prediction techniques have been used to assure the quality of new lightpaths as well as that of the previously provisioned ones. Traditionally, two different approaches have been used: analytical methods, which take into account most physical impairments that are accurate but complex, and high margin formulas, which require much less computational resources at the cost of high margins. With the recent progress of machine learning (ML) together with software defined networking (SDN), ML has been considered as another option that could be both accurate and that does not consume as many resources as analytical methods. SDN architectures are difficult to scale because they are usually centralized; this is even worse with QoT predictors using ML. In this paper, a solution to this issue is presented using a cloud-native architecture, and its scalability is evaluated using three different ML QoT predictors and experimentally validated in a real wavelength-division multiplexing (WDM) over spatial-division multiplexing (SDM) testbed."
267,excluded,10.1109/oceanse.2019.8867398,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8867398/,2019-06-20 00:00:00,ocean of things : affordable maritime sensors with scalable analysis,"DARPA's Ocean of Things (OoT) program enables persistent maritime situational awareness over large ocean areas by deploying thousands of low-cost, intelligent floats that drift as a distributed sensor network. Each float manages a suite of commercially available sensors to collect environmental data such as sea surface temperature, sea state, and location as well as activity data about vessels and marine mammals moving across the ocean. The floats periodically transmit processed data, or immediately report events based on internal prioritization schemes. Messages travel via commercial satellite to a government cloud for storage and real-time analysis. Cloud-based data analytics feature machine learning aimed at discovering emergent features and behaviors from sparse data. The multiple performers manufacturing floats and developing software are being led by a government management team to employ commercial design methodology and agile best practices. At-sea float deployments are planned in two phases over 2019 (1-month) and 2020 (3-month). Program benefits include ocean environmental products derived from high-density, in-situ measurements and analytical applications, which can simultaneously provide users a range of outputs to include ocean circulation prediction, vessel and marine mammal tracking, and dynamic ocean resource management."
268,unknown,10.1109/bibm52615.2021.9669704,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9669704/,2021-12-12 00:00:00,web platform for medical deep learning services,"In the last decade, deep learning has been transforming the healthcare scenario with the provision of new computer-assisted diagnosis tools in an unprecedented way. The spread of specialized hardware and software has been supporting this reality. The development of such predictive models, however, requires expertise and time. In this context, this article proposes and describes the implementation of a no-coding software framework for the automation of deep learning training processes, aiming to support the development of medical image diagnostic classifiers by healthcare professionals with limited expertise in deep learning. It is a web solution that allows the easy management of data, models, and training processes associated with user requests, where the benefits extend to non-experts in the machine learning field. It is an intuitive web solution that contains a set of additional modern tools like Automatic Machine Learning (AutoML) to aid design models and a ranking system to help improve and share them."
269,unknown,10.1109/iccworkshops49005.2020.9145434,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9145434/,2020-06-11 00:00:00,an inter-disciplinary modelling approach in industrial 5g/6g and machine learning era,"Unlike conventional cellular systems, the fifth generation (5G) and beyond includes intrinsic support for vertical industries with diverse service requirements. Industrial process automation with autonomous fault detection and prediction, optimised operations and proactive control can be considered as one of the key verticals of 5G and beyond. Such applications enable equipping industrial plants with a reasoning sixth sense for optimised operations and fault avoidance. In this direction, we introduce an inter-disciplinary approach integrating wireless sensor networks with machine learning-enabled industrial plants to build a step towards developing this sixth sense technology, i.e., the reasoning ability. We develop a modular-based system that can be adapted to the vertical-specific elements. Without loss of generalisation, exemplary use cases are developed and presented including a fault detection/prediction scheme in a wireless communication network with sensors and actuators to enable the sixth sense technology with guaranteed service load requirements. The proposed schemes and modelling approach are implemented in a real chemical plant for testing purposes, and a high fault detection and prediction accuracy is achieved coupled with optimised sensor density analysis."
270,excluded,10.1109/uemcon51285.2020.9298178,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9298178/,2020-10-31 00:00:00,anomaly detection and identification using visual techniques in streaming video,"There are many intelligent systems and tools which uses highly efficient processing models to identify different anomalies with high accuracy. The anomaly detection is of high importance and mostly will come as an absolute requirement at high risk environments and situations. The amount of processing involved in quick decision taking systems bare high deployment costs which restricts the anomaly detection only to a selected few who are capable of building such resource centered systems. Modern world uses drones and other video feeds in order to find and keep track of any anomalous events around a specific area. But most such detection requires absolute manual attention as well as processing power to keep up with real time detection and recognition. The proposed research solution aims to automate this process and includes a two-step anomaly detection system which gives a quicker anomaly detection in an average processing unit time with an advanced recognition model with up to 90% accuracy. The deep learning model (VGG 16) together with alert system and comparison techniques on videos leads into unsupervised anomaly detection of a landscape. The system generates alerts and recognizes anomalies on the alerted video frames. The proposed solution can also be used by any source and does not require high capacity of capability system to get the optimal output. Moreover, the solution brings a simple yet sophisticated technique to address modern anomaly detection and quick alerting system."
271,unknown,10.1109/ism52913.2021.00039,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9666077/,2021-12-01 00:00:00,combining linked open data and multimedia knowledge base for digital cultural heritage robotic applications,"The current trends in society evolution are showing rapid changes in our habitual environments and consequently affecting human interactions with them. Among the key factors of this process we can certainly cite the growth of BigData favored by the knowledge digitalization, the dissemination of sensors in environments and the advancements of connectivity capabilities. At the same time, the progress in artificial intelligence and cognitive robotics has lead to the production of sophisticated humanoid robots, which are progressively spreading to a wide public. In this way, research efforts are needed for a proper knowledge management and knowledge acquisition by machines, in order to have more natural and friendly human-robot interactions in daily tasks, usually performed by human beings. In this paper we show an approach related to a human-robot interaction in cultural heritage context, simulating a digital ecosystem where a robot plays the role of a guide for tourists and it is able to proactively interact with its interlocutors by combining both semantic and visual information. The proposed approach, its implementation and experimental results on a real robotic platform are shown and discussed."
272,unknown,10.1109/icstcee54422.2021.9708587,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9708587/,2021-12-17 00:00:00,logistic regression versus xgboost: machine learning for counterfeit news detection,"In this age of globalization, the unstoppable spreading of fake news via the internet is unstoppable. The spread of false news cannot be supported due to the negative consequences. Society is extremely concerning. In addition, itleads to more serious problems and possible threats, like confusion, misunderstandings, defamation and falsehoods that induce users to share inflammatory content. With the convenience and tremendous increase in information gathering on social networks, it is becoming difficult to differentiate between what is false and what is real. Information can be easily disseminated through sharing, which has contributed to the exponential growth of their forgeries. Machine learning played an important role, in classifying information, although there are some limitations. This article explores various machine learning techniques used to detect fake and fabricated messages. The limitations are discussed using deep learning implementation. In this project, the methodology used is model development and Logistic Regression classifier is considered to detect false news. Based on previous research, this classifier performed well in classification tasks. In this approach, TF-IDF feature is used for the construction of this fake news model to get higher accuracy. The goal of this project is to detect false news using NLP and Machine Learning based on the news content of the article. Following the development of the appropriate Machine Learning model to detect fake/true news, it is deployed into a web interface using Python Flask."
273,excluded,10.1109/icrito48877.2020.9198018,IEEE,ieeexplore,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9198018/,2020-06-05 00:00:00,augmenting banking and fintech with intelligent internet of things technology,"Banks and Financial institutions play a crucial role in economic development of country albeit slow to embrace IoT technology. IoT has the potential to revolutionize financial services by enabling a deeper understanding of economic trends, customer preferences and capturing their needs and preferences in real time. Integrating Artificial Intelligence in IoT enhances IoT performance by analysing the information and extracting knowledge therefore making them more intelligent. The FinTech(Financial Technology) needs to be amalgamated with Intelligent IoT to improve customer services, gain customer's perspicacity and to improve efficiency. This paper proffers a cloud centric Intelligent Internet of Things conceptual architecture for banking and financial sector. Furthermore a case study of implementing Intelligent IoT for vehicle loans has also been presented."
274,unknown,10.1016/j.compchemeng.2012.06.021,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84869501412,2012-12-20,smartgantt - an interactive system for generating and updating rescheduling knowledge using relational abstractions,"Generating and updating rescheduling knowledge that can be used in real time has become a key issue in reactive scheduling due to the dynamic and uncertain nature of industrial environments and the emergent trend towards cognitive systems in production planning and execution control. Disruptive events have a significant impact on the feasibility of plans and schedules. In this work, the automatic generation and update through learning of rescheduling knowledge using simulated transitions of abstract schedule states is proposed. An industrial example where a current schedule must be repaired in response to unplanned events such as the arrival of a rush order, raw material delay, or an equipment failure which gives rise to the need for rescheduling is discussed. A software prototype (SmartGantt) for interactive schedule repair in real-time is presented. Results demonstrate that responsiveness is dramatically improved by using relational reinforcement learning and relational abstractions to develop a repair policy."
275,unknown,10.1007/s11761-020-00292-z,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11761-020-00292-z,2020-09-01 00:00:00,could or could not of grid-loc: grid ble structure for indoor localisation system using machine learning,"Indoor localisation and its various applications have received significant attention in recent years. The state-of-the-art systems include a large number of complex hardware structures and algorithms making the system not suitable for practical applications. In this paper, we integrate a localisation system that consists of device development, model deployment, data collection and localisation algorithm to explore the localisation accuracy in a special static indoor environment (i.e. a meeting room or a parking lot). Compared with previous studies, the significance of our work is to find out a more convenient and practical way to deploy devices with a simple algorithm (e.g. machine learning algorithm) in such a scenario. Besides, it is meaningful to explore the technology of indoor localisation based on the application scenario. We propose a Grid-Loc system that presents a grid structure of Bluetooth low-energy devices to collect data assisting localisation. The system is easy to deploy for reducing the signal attenuation caused by the objects’ occlusion. Meanwhile, the system applies an algorithm that combines adaptive boosting with a support vector machine algorithm to support the system. In our deployed localisation scenario, we also compare localisation performances for several algorithms; the result shows the Grid-Loc system achieves the accuracy of 91.2%, computing time within 3 s in real time and a low cost. The system is also robust and scalable under the same indoor environments."
276,included,10.1038/s41598-022-07764-6,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s41598-022-07764-6,2022-03-08 00:00:00,real-time infection prediction with wearable physiological monitoring and ai to aid military workforce readiness during covid-19,"Infectious threats, like the COVID-19 pandemic, hinder maintenance of a productive and healthy workforce. If subtle physiological changes precede overt illness, then proactive isolation and testing can reduce labor force impacts. This study hypothesized that an early infection warning service based on wearable physiological monitoring and predictive models created with machine learning could be developed and deployed. We developed a prototype tool, first deployed June 23, 2020, that delivered continuously updated scores of infection risk for SARS-CoV-2 through April 8, 2021. Data were acquired from 9381 United States Department of Defense (US DoD) personnel wearing Garmin and Oura devices, totaling 599,174 user-days of service and 201 million hours of data. There were 491 COVID-19 positive cases. A predictive algorithm identified infection before diagnostic testing with an AUC of 0.82. Barriers to implementation included adequate data capture (at least 48% data was needed) and delays in data transmission. We observe increased risk scores as early as 6 days prior to diagnostic testing (2.3 days average). This study showed feasibility of a real-time risk prediction score to minimize workforce impacts of infection."
277,excluded,10.1109/iccmst54943.2021.00014,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9784593/,2021-12-18 00:00:00,internet of things-based middleware against cyber-attacks on smart homes using software-defined networking and deep learning,"Internet of Things (IoT) devices are expected to number about 3.5 billion by 2023.; a tremendous amount of Internet of Things (IoT) data that is generating (IoT) devices is estimated to exceed 79.4 zettabytes by 2025. Security challenges will become an increasingly significant issue, especially in smart homes that depend entirely on IoT devices. Due to the weak infrastructure of the IoT, it is vulnerable to various types of cyber-attacks. The most common IoT attacks are distributed denial of service (DDoS). Most traditional security solutions, like intrusion detection systems (IDS), cannot detect most attacks. Complexity is being hidden by a new paradigm that recently arose, called the software-defined system that is brings a significant change to the networking industry, a great solution for mitigation of attacks that can adopt deep learning technique to encounter cyber-attacks based on the attack behavior and by filtering normal and attack traffic by using well-defined rules. This paper proposed a system by suggesting middleware that can help mitigate or prevent various attacks on IoT on smart home environment. Machine learning has included in the middleware to provide automatic protection against cyber-attacks on IoT networks. A promising approach to protecting real-time, highly accurate attacks on SDN-managed IoT networks has been proposed. This middleware allows IoT devices to efficiently handle evolving security threats dynamically and adaptively without impacting the IoT devices."
278,unknown,10.1109/i2ct54291.2022.9825287,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9825287/,2022-04-09 00:00:00,real time protein crystal monitoring system,"High-resolution crystal structures of the biological macromolecules (protein, DNA, RNA or their complexes) are essential for understanding their biological functions and causes of diseases at a molecular level. X-ray crystallographic technique gives the atomic positions of these biological macromolecules and decodes the intermolecular interactions between protein-drug, protein-protein, protein-nucleic acids which are crucial for rational drug design. To aid the macromolecular crystallization experiments, we have indigenously developed automated system, called Real Time Protein Crystal Monitoring System (RT-PCMS) for crystal imaging and to monitor the progress of protein crystal growth during crystallization. RT-PCMS images crystallization drops at frequent time intervals in 24- or 96-well crystallization plate formats. The system consists of precise robotics motion and a custom designed motorized microscopic imaging system, capturing multi-focus composite images of protein crystals in droplets in multiple wells. In a typical successful trial, the crystallization drop comprises protein crystals of size 20 to 300 microns located at the different regions of an elliptical droplet. We have implemented powerful image processing and deep learning algorithms in this system, fusing multi-depth crystal images within a crystallization drop and classification into different crystallization stages. The system is controlled through a personal computer that provides a custom developed graphical user interface software for visualization of all captured images at different crystallization stages and record the results of crystal formation."
279,included,10.1016/j.procs.2019.09.169,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85076257910,2019-01-01,iassistme - adaptable assistant for persons with eye disabilities,"Visually challenged people may experience certain difficulties in their daily interaction with technology. That is essentially because the main way to exchange and process information is by written text, images or videos. Since the basic purpose of innovation is to improve people’s lifestyle, in this paper we propose a system that can make technology accessible to a broader group. Our prototype is presented as a mobile application based on vocal interaction, which can help people facing visual disorders consult their personal agenda, create an event, invite other friends to attend it, check the weather in certain areas and many other day-to-day tasks. Regarding the implementation, the project consists of a mobile application that interacts with a cloud based system, which makes it reliable and low in latency due to the resource availability in multiple global regions, provided by the newly emerging platform used in building the infrastructure. The novelty of the system lays in the highly flexible serverless architecture [1] that is open to extension and closed to modification through the set of autonomous cloud processing methods that sustain the base of the functionality. This distributed processing approach guarantees that the user always receives a response from his personal assistant, either by using artificial intelligence context generated phrases, by real-time cloud function processing or by fallback to the training answers."
280,unknown,http://arxiv.org/abs/2102.08936v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.08936v2,2021-02-17 00:00:00,"deep learning anomaly detection for cellular iot with applications in
  smart logistics","The number of connected Internet of Things (IoT) devices within
cyber-physical infrastructure systems grows at an increasing rate. This poses
significant device management and security challenges to current IoT networks.
Among several approaches to cope with these challenges, data-based methods
rooted in deep learning (DL) are receiving an increased interest. In this
paper, motivated by the upcoming surge of 5G IoT connectivity in industrial
environments, we propose to integrate a DL-based anomaly detection (AD) as a
service into the 3GPP mobile cellular IoT architecture. The proposed
architecture embeds autoencoder based anomaly detection modules both at the IoT
devices (ADM-EDGE) and in the mobile core network (ADM-FOG), thereby balancing
between the system responsiveness and accuracy. We design, integrate,
demonstrate and evaluate a testbed that implements the above service in a
real-world deployment integrated within the 3GPP Narrow-Band IoT (NB-IoT)
mobile operator network."
281,unknown,10.1016/j.matpr.2021.01.072,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85116453644,2021-01-01,flash flood risk management modeling in indian cities using iot based reinforcement learning,"Each year, flash floods in India have affected life, infrastructure and the economy of the country and there is a need for a systemic model of real-time flash flood management. So, to mitigate these losses, we proposed a flash flood management model focused on reinforcement learning. Based on their severity, the flash flood data is collected and rewards are distributed and this data is compared to the data collected from smart IoT devices deployed in the region impacted by the flood. We allocate the state-based reward values once the comparison is completed. The evacuation of flash flood water through the contour of the flash flood is carried out. This bypass has gates at different points that mean that, depending on the incentives, the gates are opened to remove flash flood water. The proposed solution was evaluated to be a faster, more effective and more accurate real-time flash flood management method."
282,unknown,10.1109/platcon53246.2021.9680756,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9680756/,2021-08-25 00:00:00,design and implementation of real-time bio signals management system based on hl7 fhir for healthcare services,"Recently, attention has been focused on services that combine medical technology with ICT technologies such as artificial intelligence, big data, Internet of Things, and block chains. In addition, Research on healthcare services that can collect bio signal data through wearable sensors using IoT technology and monitor and manage health based on the collected data is increasing significantly. In particular, in a situation where the world is entering a rapidly aging society, health care services are being researched and developed in the direction of preventing diseases in advance and maintaining a healthy life. Healthcare services are bringing important changes in the pandemic era caused by covid-19. There is a need for a system capable of efficiently sharing and exchanging information of heterogeneous services to prevent emergencies and support optimal medical services. In this paper, we designed and developed a system that can collect, convert, and store bio-signals from various wearable sensors into international standard data to develop such healthcare services. HL7 (health level seven) FHIR (fast healthcare interoperability resources) applied mutandis in this paper is a standard protocol for data exchange between medical information systems of real-time collected bio signals. In this paper, we implement an interface module that converts bio signals such as EEG (electroencephalography), ECG (electrocardiogram), EMG (electromyography), and PPG (photoplethysmography) collected in real time from a wearable sensor into a message structure defined by HL7 FHIR. The interface module consists of a client part and a server part. The client part generates a variety of signal data from the healthcare service user and delivers the message to the server part. The server part is designed and implemented to parse the received message by segment field unit and transmit whether the message is abnormal or not to the client part. The system designed and implemented in this paper will be utilized as a technology that can mutually share and exchange medical information in a customized healthcare service that reflects the needs of various customers and a telemedicine system."
283,excluded,10.1109/besc.2014.7059529,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7059529/,2014-11-01 00:00:00,comuptional reasoning and learning for smart manufacturing under realistic conditions,"Smart manufacturing has increasingly become a prominent research topic across the academia and industry during recent years. However, in the real world, most factories' conditions are normally insufficient for implementing full scale smart manufacturing. In order to increase smartness for manufacturing systems under different situations, one may observe that computational reasoning and learning, including latest machine learning methodology and traditional rule based systems, are able to offer potential powerful theoretical foundations as well as technical tools for enabling such smarter systems. Unfortunately, few studies exist to propose a practical yet systematic procedure that implements computational reason and learning to implement smartness for typical manufacturing systems. This paper proposes a general computational reasoning and learning framework to describe the key functions in smart manufacturing under the ""three Fs in one"" system framework, namely a system of interconnected data, integrated automation, and intelligent information. Among three Fs, the intelligent information plays the most important role in working towards smartness by connecting the other two Fs. Furthermore, to achieve it, a learning enabled comprehensive multi-agent decision model is developed. In particular, we first design a computational learning based architecture for analyzing support information for manufacturing processes. Then we provide an optimization architecture that enables realtime learning for a manufacturing process. At last, we employ a rule based learning system to integrate these two architectures to facilitate self-evolution of the manufacturing system. The advantages of our procedure include adaptive responses to dynamic environment, efficient computations, and abilities to fulfill complex manufacturing processes, which are demonstrated by a specific modeling procedure."
284,unknown,10.2118/205465-ms,"Day 4 Fri, September 10, 2021",semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/25ee6b6cffcc3ff756bc53c30c3400d44d0fff1a,2021-01-01 00:00:00,industry first ai-powered fully automated safety observation system deployed to global offshore fleet,"
 As the oil and gas industry is facing tumultuous challenges, adoption of cutting-edge digital technologies has been accelerated to deliver safer, more efficient operations with less impact on the environment.
 While advanced AI and other digital technologies have been rapidly evolving in many fields in the industry, the HSE sector is playing catch-up. With the increasing complexity of risks and safety management processes, the effective application of data-driven technologies has become significantly harder, particularly for international organizations with varying levels of digital readiness across diverse global operations. Leaders are more cautious to implement solutions that are not fit-for purpose, due to concerns over inconsistencies in rolling out the program across international markets and the impact this may have on ongoing operations.
 This paper describes how the effective application of Artificial intelligence (AI) and Machine Learning (ML) technologies have been used to engineer a solution that fully digitizes and automates the end-to-end offshore behavior-based safety program across a global offshore fleet; optimizing a critical safety process used by many leading oil & gas organization to drive positive workplace safety culture. The complex safety program has been transformed into clear, efficient and automated workflow, with real-time analytics and live transparent dashboards which detail critical safety indicators in real time, aiding decision-making and improving operational performance.
 The novel behavior-based safety digital solution, referred to as 3C observation tool within Noble drilling, has been built to be fully aligned with the organization's safety management system requirements and procedures, using modern and agile tools and applications for fully scalability and easy deployment. It has been critical in sharpening the offshore safety observation program across global operations, resulting in a boost of the workforce engagement by 30%, and subsequently increasing safety awareness skill set attainment; improving overall offshore safety culture, all while reducing operating costs by up to 70% and cutting carbon footprint through the elimination of 15,000 manhours and half a million paper cards each year, when compared to previously used methods and workflows"
285,included,10.1109/access.2022.3141913,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9676574/,2022-01-01 00:00:00,decentralized federated learning for healthcare networks: a case study on tumor segmentation,"Smart healthcare relies on artificial intelligence (AI) functions for learning and analysis of patient data. Since large and diverse datasets for training of Machine Learning (ML) models can rarely be found in individual medical centers, classical centralized AI requires moving privacy-sensitive data from medical institutions to data centers that process the fused information. Training on data centers thus requires higher communication resource/energy demands while violating privacy. This is considered today as a significant bottleneck in pursuing scientific collaboration across trans-national clinical medical research centers. Recently, federated learning (FL) has emerged as a distributed AI approach that enables the cooperative training of ML models, without the need of sharing patient data. This paper dives into the analysis of different FL methods and proposes a real-time distributed networking framework based on the Message Queuing Telemetry Transport (MQTT) protocol. In particular, we design a number of solutions for ML over networks, based on FL tools relying on a parameter server (PS) and fully decentralized paradigms driven by consensus methods. The proposed approach is validated in the context of brain tumor segmentation, using a modified version of the popular U-NET model with representative clinical datasets obtained from the daily clinical workflow. The FL process is implemented on multiple physically separated machines located in different countries and communicating over the Internet. The real-time test-bed is used to obtain measurements of training accuracy vs. latency trade-offs, and to highlight key operational conditions that affect the performance in real deployments."
286,unknown,10.1109/icmla.2013.142,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6786127/,2013-12-07 00:00:00,epg content recommendation in large scale: a case study on interactive tv platform,"Recommender systems in TV applications mostly focusing on the recommendation of video-on-demand (VOD) content, though the major part of users' content consumption is realized on linear channel programs, termed EPG content. In this case study we present how we tackled the EPG recommendation task, which exhibits several differences compared to the VOD scenario, including the lack of explicit user feedbacks, the magnitude of cold start problem, as well as data cleaning and feature selection necessary to be applied on raw consumption data. We provide both offline and online model validation. First we showcase the typical approach in machine learning by evaluating models against recall in an offline setting. Then, we investigate in depth the real-world results of the recommendation app using the pre-trained models, and analyze how personalized recommendation influence users watching behavior. The experimentation results are based on our recommender system deployed at a Canadian IPTV service provider using Microsoft Media room middleware."
287,included,http://arxiv.org/abs/1909.13343v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1909.13343v2,2019-09-29 00:00:00,"isthmus: secure, scalable, real-time and robust machine learning
  platform for healthcare","In recent times, machine learning (ML) and artificial intelligence (AI) based
systems have evolved and scaled across different industries such as finance,
retail, insurance, energy utilities, etc. Among other things, they have been
used to predict patterns of customer behavior, to generate pricing models, and
to predict the return on investments. But the successes in deploying machine
learning models at scale in those industries have not translated into the
healthcare setting. There are multiple reasons why integrating ML models into
healthcare has not been widely successful, but from a technical perspective,
general-purpose commercial machine learning platforms are not a good fit for
healthcare due to complexities in handling data quality issues, mandates to
demonstrate clinical relevance, and a lack of ability to monitor performance in
a highly regulated environment with stringent security and privacy needs. In
this paper, we describe Isthmus, a turnkey, cloud-based platform which
addresses the challenges above and reduces time to market for operationalizing
ML/AI in healthcare. Towards the end, we describe three case studies which shed
light on Isthmus capabilities. These include (1) supporting an end-to-end
lifecycle of a model which predicts trauma survivability at hospital trauma
centers, (2) bringing in and harmonizing data from disparate sources to create
a community data platform for inferring population as well as patient level
insights for Social Determinants of Health (SDoH), and (3) ingesting
live-streaming data from various IoT sensors to build models, which can
leverage real-time and longitudinal information to make advanced time-sensitive
predictions."
288,unknown,10.1109/icasi55125.2022.9774490,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9774490/,2022-04-23 00:00:00,application of augmented reality for aviation equipment inspection and maintenance training,"In the procedure of aviation equipment maintenance, a healthy status and prior knowledge are essential to every person. Making the newcomers get familiar with their tasks rapidly and reducing human error are important issues. Augmented reality (AR) integrated with the popular artificial intelligence (AI) technology can provide smart system inspections and train maintenance professionals on how to perform important maintenance procedures effectively and accurately. Utilization of the AR not only replaces classic manual training but also provides high mobility pre-training opportunities. In other words, AR is capable of taking equipment maintenance to the next level. In this paper, an application for aviation equipment inspection is developed. By using AR and AI, traditional standard operation procedures (SOP) can be visualized and standardized. Therefore, people who are not familiar with the maintenance procedure can still finish the standard inspections. The developed software gives a great contribution to human resources for maintenance training. Demonstrations including turbofan and landing gear are provided by using HoloLens 2 to illustrate the application novelty. Finally, pros and cons regarding maintenance using AR are also discussed."
289,included,10.1109/snpd.2017.8022765,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8022765/,2017-06-28 00:00:00,recognizing adls of one person household based on non-intrusive environmental sensing,"Pervasive sensing technologies are promising for increasing one-person households (OPH), where the sensors monitor and assist the resident to maintain healthy life rhythm. Towards the practical use, the recognition of activities of daily living (ADL) is an important step. Many studies of the ADL recognition have been conducted so far, for real-life and human-centric applications such as eldercare and healthcare. However, most existing methods have limitations in deployment cost, privacy exposure, and inconvenience for residents. To cope with the limitations, this paper presents a new indoor ADL recognition system especially for OPH. To minimize the deployment cost as well as the intrusions to user and house, we exploit an IoT-based environment-sensing device, called Autonomous Sensor Box (SensorBox) which can autonomously measure 7 kinds of environment attributes. We apply machine-learning techniques to the collected data, and predicts 7 kinds of ADLs. We conduct an experiment within an actual apartment of a single user. The result shows that the proposed system achieves the average accuracy of ADL recognition with more than 88%, by carefully developing the features of environment attributes."
290,unknown,10.1109/iecbes48179.2021.9398735,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9398735/,2021-03-03 00:00:00,high-intensity interval training exercise recognition using smartwatch,"The use of a smartwatches to enable human activity recognition has brought forth immersive applications. This paper presents an end-to-end approach using deep learning to recognise physical exercises from a commercially available smartwatch. The exercises are recognised based on two different settings namely; constrained and unconstrained workouts in the form of High-Intensity Interval Training. The model reported a 97.35% accuracy for constrained exercise recognition, and a 82.29% accuracy for unconstrained exercise recognition. This method is capable of recognising 18 High-Intensity Interval Training exercises. The model was deployed to Google Cloud Platform to recognise exercises in real-time settings. The method will be further expanded to operate as a real-time “Fitness Coach”, which could automatically suggest optimal workout plans for users and monitor their health conditions during workout sessions."
291,unknown,10.3390/s21020434,Sensors,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/4fe978ebae006e1203d7254aa9d70ada7ff00758,2021-01-01 00:00:00,image-based automatic watermeter reading under challenging environments,"With the rapid development of artificial intelligence and fifth-generation mobile network technologies, automatic instrument reading has become an increasingly important topic for intelligent sensors in smart cities. We propose a full pipeline to automatically read watermeters based on a single image, using deep learning methods to provide new technical support for an intelligent water meter reading. To handle the various challenging environments where watermeters reside, our pipeline disentangled the task into individual subtasks based on the structures of typical watermeters. These subtasks include component localization, orientation alignment, spatial layout guidance reading, and regression-based pointer reading. The devised algorithms for orientation alignment and spatial layout guidance are tailored to improve the robustness of our neural network. We also collect images of watermeters in real scenes and build a dataset for training and evaluation. Experimental results demonstrate the effectiveness of the proposed method even under challenging environments with varying lighting, occlusions, and different orientations. Thanks to the lightweight algorithms adopted in our pipeline, the system can be easily deployed and fully automated."
292,excluded,10.1109/isscs.2013.6651227,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6651227/,2013-07-12 00:00:00,a face recognition system based on a kinect sensor and windows azure cloud technology,"The aim of this paper is to build a system for human detection based on facial recognition. The state-of-the-art face recognition algorithms obtain high recognition rates base on demanding costs - computational, energy and memory. The use of these classical algorithms on an embedded system cannot achieve such performances due to the existing constrains: computational power and memory. Our objective is to develop a cheap, real time embedded system able to recognize faces without any compromise on system's accuracy. The system is designed for automotive industry, smart house application and security systems. To achieve superior performance (higher recognition rates) in real time, an optimum combination of new technologies was used for detection and classification of faces. The face detection system uses skeletal-tracking feature of Microsoft Kinect sensor. The face recognition, more precisely - the training of neural network, the most computing-intensive part of the software, is achieved based on the Windows Azures cloud technology."
293,unknown,10.1016/j.heliyon.2022.e09634,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85131417211,2022-06-01,smart deployment of iot-telosb service care streamrobot using software-defined reliability optimisation design,"Intelligent service care robots have increasingly been developed in mission-critical sectors such as healthcare systems, transportation, manufacturing, and environmental applications. The major drawbacks include the open-source Internet of Things (IoT) platform vulnerabilities, node failures, computational latency, and small memory capacity in IoT sensing nodes. This article provides reliable predictive analytics with the optimisation of data transmission characteristics in StreamRobot. Software-defined reliable optimisation design is applied in the system architecture. For the IoT implementation, the edge system model formulation is presented with a focus on edge cluster log-normality distribution, reliability, and equilibrium stability considerations. A real-world scenario for accurate data streams generation from in-built TelosB sensing nodes is converged at a sink-analytic dashboard. Two-phase configurations, namely off-taker and on-demand, link-state protocols are mapped for deterministic data stream offloading. An orphan reconnection trigger mechanism is used for reliable node-to-sink resilient data transmissions. Data collection is achieved, using component-based programming in the experimental testbed. Measurement parameters are derived with TelosB IoT nodes. Reliability validations on remote monitoring and prediction processes are studied considering neural constrained software-defined networking (SDN) intelligence. An OpenFlow-SDN construct is deployed to offload traffic from the edge to the fog layer. At the core, fog detection-to-cloud predictive machine learning (FD-CPML) is used to predict real-time data streams. Prediction accuracy is validated with decision tree, logistic regression, and the proposed FD-CPML. The data streams latency gave 40.00%, 33.33%, and 26.67%, respectively. Similarly, linear predictive scalability behaviour on the network plane gave 30.12%, 33.73%, and 36.15% respectively. The results show satisfactory responses in terms of reliable communication and intelligent monitoring of node failures."
294,excluded,10.1109/wf-iot.2019.8767291,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8767291/,2019-04-18 00:00:00,mountain pine beetle monitoring with iot,"Outbreaks of forest pests cause large-scale damages, which lead to significant impact on the ecosystem as well as the forestry industry. Current methods of monitoring pest outbreaks involve field, aerial and remote sensing surveys. These methods only provide partial spatial coverage and can detect outbreaks only after they have substantially progressed across wide geographic areas. This paper presents an IoT system for real-time insect infestation detection using bioacoustic recognition via machine learning techniques. Specifically, we focus on detecting the Mountain Pine Beetle (MPB), which is the most destructive insect of mature pines in western North American forests. We present the design of the system and describe its various hardware and software components. Experimental results collected from a prototype implementation of the system are presented, which show that the system can detect MPB with 82% accuracy. We also demonstrate the applicability of our system in other noise monitoring applications, and report our experimental results on urban noise detection and classification."
295,excluded,10.1109/comcas52219.2021.9629097,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9629097/,2021-11-03 00:00:00,using machine learning and virtual reality for orthopedic treatment and abnormality detection based on multivariate time series data,"In this work we present a virtual reality machine-learning system for telehealth orthopedic treatment. Our system can recognize orthopedic abnormalities and the presence of pain. It is based on a widely used virtual reality system, combined with its sensors. We implemented an algorithm that can identify very accurately wrist and neck pain and can serve as a real-time remote system for rehabilitation doctors or physical therapists, as part of a virtual reality telehealth treatment program. Our algorithms synchronize the patient’s movement data with a dedicated data server. The system has an easy-to-use interface for analysis of the collected data. We achieved more than 90% success rates evaluating the presence of neck pain and wrist pain across given exercises for each of our volunteers. Our system can serve as the basis for a real-world telehealth, clinically operative machine."
296,excluded,http://arxiv.org/abs/2205.11267v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2205.11267v1,2022-05-23 00:00:00,"fed-dart and fact: a solution for federated learning in a production
  environment","Federated Learning as a decentralized artificial intelligence (AI) solution
solves a variety of problems in industrial applications. It enables a
continuously self-improving AI, which can be deployed everywhere at the edge.
However, bringing AI to production for generating a real business impact is a
challenging task. Especially in the case of Federated Learning, expertise and
resources from multiple domains are required to realize its full potential.
Having this in mind we have developed an innovative Federated Learning
framework FACT based on Fed-DART, enabling an easy and scalable deployment,
helping the user to fully leverage the potential of their private and
decentralized data."
297,unknown,http://arxiv.org/abs/1606.03966v2,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1606.03966v2,2016-06-13 00:00:00,making contextual decisions with low technical debt,"Applications and systems are constantly faced with decisions that require
picking from a set of actions based on contextual information.
Reinforcement-based learning algorithms such as contextual bandits can be very
effective in these settings, but applying them in practice is fraught with
technical debt, and no general system exists that supports them completely. We
address this and create the first general system for contextual learning,
called the Decision Service.
  Existing systems often suffer from technical debt that arises from issues
like incorrect data collection and weak debuggability, issues we systematically
address through our ML methodology and system abstractions. The Decision
Service enables all aspects of contextual bandit learning using four system
abstractions which connect together in a loop: explore (the decision space),
log, learn, and deploy. Notably, our new explore and log abstractions ensure
the system produces correct, unbiased data, which our learner uses for online
learning and to enable real-time safeguards, all in a fully reproducible
manner.
  The Decision Service has a simple user interface and works with a variety of
applications: we present two live production deployments for content
recommendation that achieved click-through improvements of 25-30%, another with
18% revenue lift in the landing page, and ongoing applications in tech support
and machine failure handling. The service makes real-time decisions and learns
continuously and scalably, while significantly lowering technical debt."
298,excluded,http://arxiv.org/abs/1901.01632v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1901.01632v2,2019-01-07 00:00:00,"exploiting network loss for distributed approximate computing with
  netapprox","Many data center applications such as machine learning and big data analytics
can complete their analysis without processing the complete set of data. While
extensive approximate-aware optimizations have been proposed at hardware,
programming language, and application levels. However, to date, the approximate
computing optimizations have ignored the network layer.
  We propose NetApprox, which to the best of our knowledge, is the first
approximate-aware network layer comprising transport-layer protocol, network
resource allocation schemes, and scheduling/priority-assignment policies.
Building on the observation that approximate applications can tolerate loss,
NetApprox's main insights are to aggressively send approximate traffic (which
improves the performance of approximate applications) and to minimize the
network resources allocated to approximate traffic (which simultaneously limits
the impact of aggressive approximate traffic while freeing up resources that,
in turn, improve non-approximate applications' performance). We ported Flink,
Kafka, Spark, and PyTorch to NetApprox and evaluated NetApprox with both
large-scale simulation and real implementation. Our evaluation results show
that NetApprox improves job completion times by up to 80% compared to
network-oblivious approximation solutions, and improves the performance of
co-running non-approximate workloads by 79%."
299,excluded,10.1109/icmac54080.2021.9678242,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9678242/,2021-12-22 00:00:00,ai-based real-time classification of human activity using software defined radios,"Real-time monitoring is an essential part in the development of healthcare monitoring systems. Research has shown that human movement affects the propagation of radio frequencies, as signals will reflect off the human body. Machine Learning techniques have been used in research to classify patterns observed in the signal propagation. This paper makes use of universal software radio peripheral devices to create a wireless communication link where the signal propagation data, known as channel state information, is collected while a user moves or remains still. A machine learning model which achieved an accuracy result of 93.25 % is used to classify between movement and no activity. Inference is then used to decide if the human position is sitting or standing and detected movements are used to differentiate between the two positions. The testbed implements cloud storage and a web-interface to present a visualisation of the human position."
300,excluded,http://arxiv.org/abs/2105.01058v2,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2105.01058v2,2021-05-03 00:00:00,"a dataset and system for real-time gun detection in surveillance video
  using deep learning","Gun violence is a severe problem in the world, particularly in the United
States. Deep learning methods have been studied to detect guns in surveillance
video cameras or smart IP cameras and to send a real-time alert to security
personals. One problem for the development of gun detection algorithms is the
lack of large public datasets. In this work, we first publish a dataset with
51K annotated gun images for gun detection and other 51K cropped gun chip
images for gun classification we collect from a few different sources. To our
knowledge, this is the largest dataset for the study of gun detection. This
dataset can be downloaded at www.linksprite.com/gun-detection-datasets. We
present a gun detection system using a smart IP camera as an embedded edge
device, and a cloud server as a manager for device, data, alert, and to further
reduce the false positive rate. We study to find solutions for gun detection in
an embedded device, and for gun classification on the edge device and the cloud
server. This edge/cloud framework makes the deployment of gun detection in the
real world possible."
301,excluded,10.1109/access.2021.3121254,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9582812/,2021-01-01 00:00:00,deepdespy: a deep learning-based wireless spy camera detection system,"Spy cameras planted in various private places, such as motels, hotels, homestays (i.e., Airbnb), and restrooms, have raised immense privacy concerns. Wi-Fi spy cameras are used extensively by various adversaries because of easy installability, followed by size reduction. To prevent invasions of privacy, most studies have detected wireless cameras based on video traffic analysis and require additional synchronous data from external sensors or stimulus hardware to confirm the user’s motion. Such supplements make the users uncomfortable, requiring extra effort and time for setting. This paper proposes an effective spy camera detection system called DeepDeSpy to detect the recording of a spy camera with no effort from the user. The core idea is using the channel state information (CSI) and the network traffic from the camera to detect whether the wireless camera records the movements of the user. The CSI signal is prone to motion, and detecting motion from an enormous amount of CSI data in real-time is challenging. This was handled by leveraging the convolutional neural network (CNN) and bidirectional long short-term memory (BiLSTM) deep learning methods. Such synergistic CNN and BiLSTM deep learning models enable instant and accurate detection by automatically extracting meaningful features from the sequential raw CSI data. The feasibility of DeepDeSpy was verified by implementing it on both a PC and a smartphone and evaluating it in real-life scenarios (e.g., various room sizes and user physical activities). The average accuracy achieved in different real-life settings was approximately 96%, reaching 98.9% with intensive physical activity in the large-size room. Moreover, the ability to achieve instant detection on a smartphone within only a one-second response time makes it workable for real-time applications."
302,unknown,10.1109/icirca51532.2021.9544776,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9544776/,2021-09-04 00:00:00,software implementation architecture design for online display of animation works based on vr technology,"The design of the online display system of virtual reality animation is at the intersection of art, design, and VR technology. VR technology is a fusion of computer artificial intelligence, simulation technology, display technology, sensor technology and other technologies. In order to solve the shortcomings of the traditional animation display system, this paper introduces VR technology and designs a software architecture that can display animation works online. First, it introduces the research status and application prospects of VR technology. Then I learned about the software architecture of the animation display system. Finally, an animation display system is designed."
303,excluded,10.1109/roman.2009.5326235,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5326235/,2009-10-02 00:00:00,gestural teleoperation of a mobile robot based on visual recognition of sign language static handshapes,"This paper presents results achieved in the frames of a national research project (titled ldquoDIANOEMArdquo), where visual analysis and sign recognition techniques have been explored on Greek Sign Language (GSL) data. Besides GSL modelling, the aim was to develop a pilot application for teleoperating a mobile robot using natural hand signs. A small vocabulary of hand signs has been designed to enable desktopbased teleoperation at a high-level of supervisory telerobotic control. Real-time visual recognition of the hand images is performed by training a multi-layer perceptron (MLP) neural network. Various shape descriptors of the segmented hand posture images have been explored as inputs to the MLP network. These include Fourier shape descriptors on the contour of the segmented hand sign images, moments, compactness, eccentricity, and histogram of the curvature. We have examined which of these shape descriptors are best suited for real-time recognition of hand signs, in relation to the number and choice of hand postures, in order to achieve maximum recognition performance. The hand-sign recognizer has been integrated in a graphical user interface, and has been implemented with success on a pilot application for real-time desktop-based gestural teleoperation of a mobile robot vehicle."
304,unknown,10.1109/access.2017.2756069,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8049520/,2017-01-01 00:00:00,digital twin shop-floor: a new shop-floor paradigm towards smart manufacturing,"With the developments and applications of the new information technologies, such as cloud computing, Internet of Things, big data, and artificial intelligence, a smart manufacturing era is coming. At the same time, various national manufacturing development strategies have been put forward, such as Industry 4.0, Industrial Internet, manufacturing based on Cyber-Physical System, and Made in China 2025. However, one of specific challenges to achieve smart manufacturing with these strategies is how to converge the manufacturing physical world and the virtual world, so as to realize a series of smart operations in the manufacturing process, including smart interconnection, smart interaction, smart control and management, etc. In this context, as a basic unit of manufacturing, shop-floor is required to reach the interaction and convergence between physical and virtual spaces, which is not only the imperative demand of smart manufacturing, but also the evolving trend of itself. Accordingly, a novel concept of digital twin shopfloor (DTS) based on digital twin is explored and its four key components are discussed, including physical shop-floor, virtual shop-floor, shop-floor service system, and shop-floor digital twin data. What is more, the operation mechanisms and implementing methods for DTS are studied and key technologies as well as challenges ahead are investigated, respectively."
305,unknown,10.1109/mwscas.2018.8624056,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8624056/,2018-08-08 00:00:00,emg-based hand gesture control system for robotics,"In this paper, a Electromyogram (EMG) based hand gesture control system is developed. A wearable human machine interface (HMI) device is designed for an in-home assistance service robot. An EMG-based control system utilizes MyoWave muscle sensor to acquire and amplify EMG signal. A microcontroller system is used to an artificial neural network (ANN) to classify the EMG signal. Based on different hand movements, commands are sent through WiFi to control the motor in a service robot. The on-board Camera system mounted the robot can capture video real-time. In addition, a web server is implemented to provide live video feedback for robot navigation and user instructions."
306,excluded,http://arxiv.org/abs/2003.04987v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.04987v1,2020-02-17 00:00:00,a financial service chatbot based on deep bidirectional transformers,"We develop a chatbot using Deep Bidirectional Transformer models (BERT) to
handle client questions in financial investment customer service. The bot can
recognize 381 intents, and decides when to say ""I don't know"" and escalates
irrelevant/uncertain questions to human operators. Our main novel contribution
is the discussion about uncertainty measure for BERT, where three different
approaches are systematically compared on real problems. We investigated two
uncertainty metrics, information entropy and variance of dropout sampling in
BERT, followed by mixed-integer programming to optimize decision thresholds.
Another novel contribution is the usage of BERT as a language model in
automatic spelling correction. Inputs with accidental spelling errors can
significantly decrease intent classification performance. The proposed approach
combines probabilities from masked language model and word edit distances to
find the best corrections for misspelled words. The chatbot and the entire
conversational AI system are developed using open-source tools, and deployed
within our company's intranet. The proposed approach can be useful for
industries seeking similar in-house solutions in their specific business
domains. We share all our code and a sample chatbot built on a public dataset
on Github."
307,unknown,10.1016/j.asoc.2014.10.018,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84912132496,2015-01-01,performance assessment of heat exchanger using intelligent decision making tools,"Process and manufacturing industries today are under pressure to deliver high quality outputs at lowest cost. The need for industry is therefore to implement cost savings measures immediately, in order to remain competitive. Organizations are making strenuous efforts to conserve energy and explore alternatives. This paper explores the development of an intelligent system to identify the degradation of heat exchanger system and to improve the energy performance through online monitoring system. The various stages adopted to achieve energy performance assessment are through experimentation, design of experiments and online monitoring system. Experiments are conducted as per full factorial design of experiments and the results are used to develop artificial neural network models. The predictive models are used to predict the overall heat transfer coefficient of clean/design heat exchanger. Fouled/real system value is computed with online measured data. Overall heat transfer coefficient of clean/design system is compared with the fouled/real system and reported. It is found that neural net work model trained with particle swarm optimization technique performs better comparable to other developed neural network models. The developed model is used to assess the performance of heat exchanger with the real/fouled system. The performance degradation is expressed using fouling factor, which is derived from the overall heat transfer coefficient of design system and real system. It supports the system to improve the performance by asset utilization, energy efficient and cost reduction in terms of production loss. This proposed online energy performance system is implemented into the real system and the adoptability is validated."
308,unknown,10.23919/splitech.2019.8783034,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8783034/,2019-06-21 00:00:00,episense: towards a smart solution for epileptic patients’ care,"Epilepsy is a chronic neurological brain disorder that affects 50 million people globally. There are several challenges associated with the care of epileptic patients, including: 1) the timely and accurate diagnosis of the condition; 2) the long-term non-intrusive monitoring and detection of epileptic seizures in real time for suitable interventions; 3) alleviating the mental health issues associated with epilepsy, such as anxiety and depression; and 4) the lack of availability of large scale datasets related to epileptic patients with different profiles, needed to advance research in epilepsy. In this work, we propose EpiSense - a smart healthcare solution for epileptic patients' care. EpiSense leverages sensory, mobile, and web technologies, as well as machine learning techniques for the real-time detection of epileptic seizures. As part of the system, a patient's mobile app. is provided to allow the detection of seizures' occurrence in real time and the sending of alarm notifications to care takers, for appropriate actions. Moreover, a web portal enables doctors to view the progress of their patients and get notified about seizures' occurrence and statistics. The EpiSense system was designed and implemented, and three machine learning models were tested for real-time epileptic seizure detection. This work gives interesting insights about the possibility of using sensory technologies and data analytics for the improvement of epileptic patients' care, and offers the possibility of personalized healthcare management."
309,excluded,http://arxiv.org/abs/1812.01813v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1812.01813v1,2018-12-05 00:00:00,"machine-learned epidemiology: real-time detection of foodborne illness
  at scale","Machine learning has become an increasingly powerful tool for solving complex
problems, and its application in public health has been underutilized. The
objective of this study is to test the efficacy of a machine-learned model of
foodborne illness detection in a real-world setting. To this end, we built
FINDER, a machine-learned model for real-time detection of foodborne illness
using anonymous and aggregated web search and location data. We computed the
fraction of people who visited a particular restaurant and later searched for
terms indicative of food poisoning to identify potentially unsafe restaurants.
We used this information to focus restaurant inspections in two cities and
demonstrated that FINDER improves the accuracy of health inspections;
restaurants identified by FINDER are 3.1 times as likely to be deemed unsafe
during the inspection as restaurants identified by existing methods.
Additionally, FINDER enables us to ascertain previously intractable
epidemiological information, for example, in 38% of cases the restaurant
potentially causing food poisoning was not the last one visited, which may
explain the lower precision of complaint-based inspections. We found that
FINDER is able to reliably identify restaurants that have an active lapse in
food safety, allowing for implementation of corrective actions that would
prevent the potential spread of foodborne illness."
310,excluded,10.1016/j.trpro.2020.03.108,scopus,sciencedirect,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85084662425,2020-01-01,estimating time of arrival of trains at level crossings for the provision of multimodal cooperative services,"While cooperative services have been almost fully deployed in the road sector and are already being implemented in various cities in Europe as a pre-requisite for the introduction of autonomous vehicles, few attempts have been made in the same direction for the rail sector. This study proposes a system that aims to improve safety and minimize risk in the meeting point between road and rail, known as level crossings, by monitoring the location of floating road vehicles via a mobile device application. A neural network predictive model for estimating time of arrival of trains is also utilized. The safety system has been implemented and tested under real life conditions in the city of Thessaloniki, Greece."
311,unknown,10.1109/iscsic54682.2021.00059,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9644395/,2021-11-14 00:00:00,near-real time quality prediction in a plastic injection molding process using apache spark,"The automotive industry is undergoing wide scope transformation. Industry 4.0 has both expanded the possibilities of digital transformation in automotive, increased its importance to all mobility ecosystem and being driven by continued digitization of the entire value chain. Manufacturing data which is unceasingly flow during serial production is one of the great sources towards Industry 4.0 goal to fully automatizing complex human dependent processes. However, there are few challenges to consider such as collecting and filtering various data from shop floor in given production cycle time range and make them ready for real time analytics as well as constructing efficient data pipeline to reach useful outcomes which is reliable enough to meet customer expectations. In this study, we will extract meaningful relation between injection machine parameters from Farplas Automotive Company's shop floor and describe their effects on the product quality. We will train and test machine learning models with different hyperparameters and test model performance to identify defected products. Finally, we will show implementation of streaming data pipeline using Kafka and Spark to be able to analyze injection machine data and effectively predict plastic injection product's OK-NOK condition real time even before human operator reaches the product itself. Consequently, detecting defected products will be independent from human attention which makes production areas one step closer to dark factory."
312,excluded,10.3390/electronics10020182,'MDPI AG',core,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/523305217.pdf,2021-01-01 00:00:00,design and implementation of deep learning based contactless authentication system using hand gestures,"Hand gestures based sign language digits have several contactless applications. Applications include communication for impaired people, such as elderly and disabled people, health-care
applications, automotive user interfaces, and security and surveillance. This work presents the design
and implementation of a complete end-to-end deep learning based edge computing system that
can verify a user contactlessly using ‘authentication code’. The ‘authentication code’ is an ‘n’ digit
numeric code and the digits are hand gestures of sign language digits. We propose a memory-efficient
deep learning model to classify the hand gestures of the sign language digits. The proposed deep
learning model is based on the bottleneck module which is inspired by the deep residual networks.
The model achieves classification accuracy of 99.1% on the publicly available sign language digits
dataset. The model is deployed on a Raspberry pi 4 Model B edge computing system to serve as an
edge device for user verification. The edge computing system consists of two steps, it first takes input
from the camera attached to it in real-time and stores it in the buffer. In the second step, the model
classifies the digit with the inference rate of 280 ms, by taking the first image in the buffer as input.publishedVersio"
313,excluded,10.1109/isie45063.2020.9152407,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9152407/,2020-06-19 00:00:00,modeling and predicting an industrial process using a neural network and automation data,"Production optimization and prevention of faults and unplanned production halts are areas of particular interest in industry. Predictive analysis is commonly implemented with data analytics and machine learning techniques. Usually, the usage of such tools requires knowledge of the machine learning theory and the subject to be studied, e.g. a pumping process. This paper presents a case study on modeling of a pumping process using stored automation data. The model is trained to predict the performance percentage of the process with minimal background knowledge of the process and data analytics. The proposed model is built with IBM SPSS Modeler, a data analysis tool not usually used in real-time industrial predictive analysis as it is not often considered the best tool when working with time series data. The model is deployed in a cloud service to implement a real-time, visualized predictive analysis system. The case study shows that Modeler can be used for data analysis, modeling, and production purposes. Depending on the case, Modeler can provide an alternative tool compared with typical machine learning tools, as models built with Modeler can be deployed into a cloud service for production use. The findings indicate that industrial automation data are a valuable resource, and data analysis can be conducted on various platforms and tools."
314,excluded,10.1109/icitr54349.2021.9657405,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9657405/,2021-12-03 00:00:00,vision based intelligent shelf-management system,"Currently supermarkets are more popular, and the local stores are leaving the competition. when people go to supermarkets, they find various items stocked on seemingly unlimited shelves. Supermarket shelves needed to be filled with the items accordingly. The most common problems in the supermarkets are identifying the empty shelves, on-shelf availability, and future sales. The labors cannot always track the empty shelves and on shelf availability levels due to their workloads. Moreover, it is a time-consuming method for the labors which can affect the customer satisfaction and business profit. Every month, supermarkets buy the required number of products from related manufacturing companies by analyzing the previously purchased products and their sales. This is usually done manually by managing excel sheets which is also time consuming and not reliable. Especially during the seasonal times or pandemic situations they cannot use the manual method which must also be done as fast as possible. Therefore, this system can be used to assist in empty shelf detection, percentage of on-shelf availability and in the prediction of future sales. The implementation of on-shelves percentage detection service is done using machine learning. Machine learning processes are carried out for implementing the necessary functionalities and algorithms. Initially, the camera captures clear and real time images regularly. Then the system processes and detects the image similar to the threshold percentage or detect the empty shelves. When the system detects the threshold percentage or empty shelves, the system will provide an alert to the labors. The Implementation of the predicting the future supply and demands is done using time series analysis using several existing machine learning algorithms by utilizing historical data. In this research the prediction of future sales and demand in the supermarkets is done by considering the customers&#x0027; behavior, the variety of product groups they buy and seasonal changes. These predictions are made on the assumption of a constant per capital supply of products and demand in our system."
315,unknown,10.1049/cp.2018.1540,IET,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8726965/,2018-11-14 00:00:00,convolutional neural networks for facial emotion recognition towards the development of automatic pain quantifier,"Facial Expression Recognition (FER) has been one of the mainstream topics in the areas of computer vision. Numerous databases of facial expressions are available to the research community and are used as fundamental tools for the evaluation of a wide range of algorithms for FER. With the continuous evolution of deep learning in computer vision, FER has enticed many researchers within the area of Artificial Intelligence, applying powerful machine learnings to enhance the overall user experience in solving many real-world problems. In this study, we review the state of the art of Convolutional Neural Networks (CNNs) as a computational model for image classification, accentuating the algorithmic structure and its functional impact. We implemented CNN building framework to validate a real-time system that is applied to automatically classify key human emotions: Angry, Disgust, Fear, Happy, Sad, Surprise, and Neutrality using FER2013 emotion datasets. The study showed that CNN model yields a high-efficiency result in achieving a high-level performance of automatically recognizing emotions from dynamic facial expressions. Our paradigm extends to the development of an artificially intelligent system capable of recognizing pain and its intensity and can diagnose a convincing and reliable outcome that speaks the same language from a medical perspective."
316,excluded,10.1109/w-ficloud.2018.00026,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8488186/,2018-08-08 00:00:00,embedded fatigue detection using convolutional neural networks with mobile integration,"Fatigued or drowsy drivers pose a significant risk of causing life-threatening accidents. Yet, many sleep-deprived drivers are behind the wheels exposing lives to danger. In this paper, we propose a low-cost and real-time embedded system for fatigue detection using convolutional neural networks (CNN). Our system starts by spatially processing the video signal using a real-time face detection algorithm to establish a region of interest and reduce computations. The video signal comes from a camera module mounted on the car dashboard connected to an embedded Linux board set to monitor the driver's eyes. Detected faces are then passed to an optimized fatigue recognition CNN binary classifier to detect the event of fatigued or normal driving. When temporally persistent fatigue is detected, alerts are sent to the driver's smart phone, and to possibly others, for prevention measures to be taken before accidents happen. Our testing shows that the system can robustly detect fatigue and can effectively be deployed to address the problem."
317,excluded,10.1109/iciea49774.2020.9102083,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9102083/,2020-04-21 00:00:00,a rapid deployment indoor positioning architecture based on image recognition,"With the rapid development of information, the immediacy, interoperability, and portability of information long been essential elements in present society. Technology has also continued to develop from these elements and creating more and more information equipment. In recent years, as new technologies continue to be developed, they have been mainly divided into two major technologies, namely “deep learning” and “augmented reality. These are including face recognition, image recognition, and augmented reality (AR), virtual reality (VR) and so on which are the key development trends now whether on mobile devices or web pages. In this study, the emerging mainstream “deep learning” and “augmented reality” are used to solve the problem of Google Map that cannot perform indoor navigation. The image recognition technology in deep learning is used to achieve the positioning function and the algorithm is used to calculate the best route to your destination, then indoor navigation presented. Finally, augmented reality (AR) technology is used to set up an exclusive AR floating electronic bulletin board at a specific location to display the relevant information and content of the specific location point for users. This research has the characteristics of rapid deployment which can be launched quickly in an indoor environment."
318,excluded,10.1016/b978-0-12-817356-5.00014-0,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85123886383,2019-01-01,combining predictive analytics and artificial intelligence with human intelligence in iot-based image-guided surgery,"When analyzing the history of medical surgery, we can see that humans have developed and refined instruments for surgical procedures. Evolution in medical advancements is on a par with that observed in the disease-causing agents and viruses. Starting from a medical era during which invasive surgeries were performed without anesthesia, the need for painless, safe, hygienic, and successful surgeries has led to the modern era of surgery, which has a relatively small mortality rate. The practical use of minimally invasive approaches that result in fewer wound-related complications, quick organ function return, and shorter hospitalizations has led to higher acceptance of image-guided surgical procedures. In our proposed system, we present an IoT-based surgical model that involves a virtual reality-based (VR-based) user interface, predictive analytics that predict the “what-ifs” for an activity to be performed during the surgery based on the data collected during similar previous surgeries, and artificial intelligence that learns from the same dataset used by predictive analytics to assist surgeons during the surgery. Virtual reality refers to an environment artificially created by the support of computer software. When humans access such an environment, they believe that they are actually there in the artificially created environment. Headsets and other navigators can be used along with the VR set-up in order to provide a more immersive environment. The experience obtained through the power of VR is no less than the actual reality. For a more immersive experience, the Oculus Rift VR system is to be used as part of the proposed model. The VR system is to be connected to the SAP IoT interface of the SAP Cloud system. Predictive analytics is one of the latest software paradigms that enables analysis of large datasets and prediction of future outcomes and behavior. It involves building a predictive analytics model by using big data and IoT sensors to uncover hidden risks, explore unforeseen opportunities, and reach a better understanding. In our proposed model, SAP predictive analytics is to be deployed to build a surgical predictive model that could provide real-time predictions during surgery based on the data collected during previous surgeries. Artificial intelligence is a computing paradigm used to create systems that automate intelligent processes. AI can be used for learning, problem solving, and decision-making processes and can work with a speed and precision that humans are able to achieve only with great effort. AI is to be deployed in our proposed surgical model to minimize the surgeons’ efforts in navigating the noninvasive surgical equipment and the camera. Often there is a difference between the intention with which a surgeon navigates and the actual navigation which happens internally during the surgery. The surgeon has to put in more effort to overcome this mismatch. In order to address this issue, formulating AI in the surgical internal device navigation could result in reduced manual effort by the surgeon, which in turn would help him in concentrating on comparatively more important surgical thoughts and activities. The SAP Leonardo Machine Learning feature is to be deployed in the proposed model."
319,unknown,10.1109/icpst.2002.1047521,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1047521/,2002-10-17 00:00:00,a study and implementation of an intelligent load forecast support system,"Computer decision support systems have been more widely applied in the planning of urban power systems, especially the distribution systems. A load forecast is a basic stage in the process of the distribution system planning. Hence, a load forecast support system (LFSS) is a basic sub-system in the package of a decision support system for urban power systems planning. The knowledge and experience of planners play a vital part in the practical forecasting process. Thus, it is urgent for researchers to find an approach to accumulate planners' knowledge and experience during load forecasting and reuse them in the further forecasting. The combination of artificial intelligence (AI) technology and LFSS provides an effective approach to solve this problem. This paper has proposes a model and an implementation frame of an intelligent load-forecast support system (ILFSS), which provides a total solution to store and reusing load forecasting knowledge. This approach has applied the AI technology to the whole process of load forecasting, including model definition, model selecting, result adjusting and decision making. Some AI technologies like rule-based reasoning and case-based reasoning are involved in the implementation of ILFSS, which cooperate with conventional load forecasting models. Up to now, some of the proposed architecture has been implemented in a real decision support system for urban power system planning called ""CNP"", which is widely used in China."
320,excluded,10.1109/ies53407.2021.9594038,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9594038/,2021-09-30 00:00:00,design and implementation of real-time pothole detection using convolutional neural network for iot smart environment,"Road is a transportation infrastructure that has a very important role in supporting the economic, social and culture as well as various other aspects of community life. Various community activities are affected by road conditions. Roads that are in good condition will provide comfort and facilitate economic activity in an area. However, in reality there are still many roads that are not in good condition, such as potholes. Various attempts have been made to detect potholes automatically, especially with the two-dimensional imaging method. In this paper, we propose real-time potholes detection using the Convolutional Neural Network (CNN) method based on the Edge Tensor Processing Unit (TPU) with the MobileNet SSD v2. Our system was implemented on Jetson Nano with several additional sensors such as a camera and GPS. The accuracy of the system is verified through experiments using the testbed."
321,excluded,10.1109/miltechs.2017.7988861,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7988861/,2017-06-02 00:00:00,multiple people detection and identification system integrated with a dynamic simultaneous localization and mapping system for an autonomous mobile robotic platform,"This paper presents the integration of a multiple people detection and identification system with a dynamic simultaneous localization and mapping system for an autonomous robotic platform. This integration allows the exploration and navigation of the robot considering people identification. The robotic platform consists of a Pioneer 3DX robot equipped with an RGBD camera, a Sick Lms200 sensor laser and a computer using the robot operating system (ROS). The idea is to integrate the people detection and identification system to the simultaneous localization and mapping (SLAM) system of the robot using ROS. The people detection and identification system is performed in two steps. The first one is for detecting multiple people on scene and the other one is for an individual person identification. Both steps are implemented as ROS nodes that works integrated with the SLAM ROS node. The multiple people detection's node uses a manual feature extraction technique based on HOG (Histogram of Oriented Gradients) detectors, implemented using the PCL library (Point Cloud Library) in C ++. The person's identification node is based on a Deep Convolutional Neural Network (CNN) that are implemented using the MatLab MatConvNet library. This step receives the detected people centroid from the previous step and performs the classification of a specific person. After that, the desired person centroid is send to the SLAM node, that consider it during the mapping process. Tests were made objecting the evaluation of accurateness in the people's detection and identification process. It allowed us to evaluate the people detection system during the navigation and exploration of the robot, considering the real time interaction of people recognition in a semi-structured environment."
322,excluded,10.1109/icde51399.2021.00313,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9458701/,2021-04-22 00:00:00,floravision: a spatial crowd-based learning system for california native plants,"With the availability of massive amounts of visual data covering wide geographical regions, various image learning applications have emerged, including classifying the street cleanliness level, detecting forest fires or road hazards. Such applications share similar characteristics as they need to 1) detect specific objects or events (what), 2) associate the detected object with a location (where), and 3) know the time that the event happened (when). Advancements in image-based machine learning (ML) benefit these applications as they can automate the detection of objects of interest. Along with the edge computing (EC) paradigm, the processing cost is offloaded to the devices, hence reducing latency and communication cost. Moreover, sensors on the edge devices (e.g., GPS) enrich the collected data with metadata. However, a shortcoming of existing approaches is that they rely on pre-trained ""static"" models. Nonetheless, crowdsourced data at diverse locations can be leveraged to iteratively improve the robustness of a model. We refer to the aforementioned strategy as ""spatial crowd-based learning"".To showcase this class of applications, we present FloraVision, an end-to-end system that integrates ML, crowdsourcing, and EC to automate the detection, mapping, and exploration of California Native Plants. FloraVision implements a pipeline to collect and clean publicly available image data, train a lightweight MobileNet-based classification model, and then deploy the model on mobile devices. It leverages spatial crowd-based learning to iteratively evolve the initial model from crowdsourced data. Its mobile application facilitates detecting plants and mapping their geolocations. Finally, it allows end-users to submit ad hoc spatio-temporal nearest neighbor queries and visualizes the results in an augmented reality user interface. Although our application focuses on plants, several other applications follow similar architectural patterns."
323,excluded,10.1109/jsyst.2019.2921867,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8746591/,2020-06-01 00:00:00,a smart optimization of fault diagnosis in electrical grid using distributed software-defined iot system,"Electrical power demands have increased significantly over the last years due to rapid increase in air conditioning units and home appliances per domestic unit, particularly in Iraq. Having an uninterrupted power supply is essential for the continuity of power-generated home services and industrial platforms. Currently, in Iraq, electrical power interruption has become a big concern to the utility suppliers. Despite successive attempts to put an end to this dilemma, the issue still prevails. One of the main factors in power outages in local zones is persistent faults in distribution transformers (DTs). DT is considered one of the main elements in the electrical network that is essential for the reliability of the grid supply. Due to the internal lack of monitoring system and periodic maintenance, DT is relentlessly subject to faults due to high overhead utilization. Therefore, in order to enhance the grid reliability, transformer health check, and maintenance practices, we propose a remote condition Internet of Things monitoring and fault prediction system that is based on a customized software-defined networking (SDN) technology. This approach is a transition to smart grid implementation by fusing the power grid with efficient and real-time wireless communication architecture. The SDN implementation is considered in two phases: one is a controller installed per local zone and the other is the main controller that is installed between zones and connected to the core network. The core network consists of redundant links to recover from any future fails. Furthermore, we propose a prediction system based on an artificial neural network algorithm, called distribution transformer fault prediction, that is installed in the management plane for periodic prediction based on real-time sensor traffic to our proposed cloud. Moreover, we propose a communication protocol in the local zone called local SDN-sense. The SDN-sense ensures a reliable communication and local node selection to relay DT sensor data to the main controller. Our proposed architecture showcases an efficient approach to handle future interruption and faults in power grid using cost-effective and reliable infrastructure that can predict and provide real-time health monitoring indices for the Iraqi grid network with minimal power interruptions. After extensive testing, the prediction accuracy was about 96.1%."
324,excluded,http://arxiv.org/abs/1902.02236v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1902.02236v1,2019-02-06 00:00:00,dynamic pricing for airline ancillaries with customer context,"Ancillaries have become a major source of revenue and profitability in the
travel industry. Yet, conventional pricing strategies are based on business
rules that are poorly optimized and do not respond to changing market
conditions. This paper describes the dynamic pricing model developed by Deepair
solutions, an AI technology provider for travel suppliers. We present a pricing
model that provides dynamic pricing recommendations specific to each customer
interaction and optimizes expected revenue per customer. The unique nature of
personalized pricing provides the opportunity to search over the market space
to find the optimal price-point of each ancillary for each customer, without
violating customer privacy. In this paper, we present and compare three
approaches for dynamic pricing of ancillaries, with increasing levels of
sophistication: (1) a two-stage forecasting and optimization model using a
logistic mapping function; (2) a two-stage model that uses a deep neural
network for forecasting, coupled with a revenue maximization technique using
discrete exhaustive search; (3) a single-stage end-to-end deep neural network
that recommends the optimal price. We describe the performance of these models
based on both offline and online evaluations. We also measure the real-world
business impact of these approaches by deploying them in an A/B test on an
airline's internet booking website. We show that traditional machine learning
techniques outperform human rule-based approaches in an online setting by
improving conversion by 36% and revenue per offer by 10%. We also provide
results for our offline experiments which show that deep learning algorithms
outperform traditional machine learning techniques for this problem. Our
end-to-end deep learning model is currently being deployed by the airline in
their booking system."
325,excluded,10.1109/fabs52071.2021.9702559,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9702559/,2021-12-22 00:00:00,real time alert system to prevent car accident,"The use of artificial intelligence to solve a complex problem and to solve a complex issue such as foreseeing and avoiding accidents on roads implies a robust and proven procedure. An effective and proven method is needed to predict and prevent accidents on the road. Artificial vision is an area of artificial intelligence to explain and understand the visual world. With machine vision and Intelligent Transportation Systems (ITS), we can provide drivers with a safety net. These technologies help reduce human error in the automotive industry and provide riders with tools and features to help them avoid serious mistakes and accidents. In this paper we present a Drowsiness Detection System which indicates the drowsiness of the car driver by detecting the drivers face in real time video and considering the various eye landmarks indicates the ratio of each eye telling how much the driver&#x2019;s eyes are open or close. This system is developed using the state-of-art Computer Vision technology making it easy to implement highly effective. This system is implemented as an Android Application which uses android mobile back camera for recording car driver&#x2019;s live video and implementing the above developed model to calculate the eye aspect ratio and indicate it along with the eye in the video."
326,excluded,10.1145/3448016.3457550,SIGMOD Conference,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/28b5df48dd23ffc7e7d64fc43e2a420e05ab88f8,2021-01-01 00:00:00,milvus: a purpose-built vector data management system,"Recently, there has been a pressing need to manage high-dimensional vector data in data science and AI applications. This trend is fueled by the proliferation of unstructured data and machine learning (ML), where ML models usually transform unstructured data into feature vectors for data analytics, e.g., product recommendation. Existing systems and algorithms for managing vector data have two limitations: (1) They incur serious performance issue when handling large-scale and dynamic vector data; and (2) They provide limited functionalities that cannot meet the requirements of versatile applications. This paper presents Milvus, a purpose-built data management system to efficiently manage large-scale vector data. Milvus supports easy-to-use application interfaces (including SDKs and RESTful APIs); optimizes for the heterogeneous computing platform with modern CPUs and GPUs; enables advanced query processing beyond simple vector similarity search; handles dynamic data for fast updates while ensuring efficient query processing; and distributes data across multiple nodes to achieve scalability and availability. We first describe the design and implementation of Milvus. Then we demonstrate the real-world use cases supported by Milvus. In particular, we build a series of 10 applications (e.g., image/video search, chemical structure analysis, COVID-19 dataset search, personalized recommendation, biological multi-factor authentication, intelligent question answering) on top of Milvus. Finally, we experimentally evaluate Milvus with a wide range of systems including two open source systems (Vearch and Microsoft SPTAG) and three commercial systems. Experiments show that Milvus is up to two orders of magnitude faster than the competitors while providing more functionalities. Now Milvus is deployed by hundreds of organizations worldwide and it is also recognized as an incubation-stage project of the LF AI & Data Foundation. Milvus is open-sourced at https://github.com/milvus-io/milvus."
327,unknown,10.1016/j.cie.2021.107824,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85122422552,2022-01-01,new perspectives and results for smart operators in industry 4.0: a human-centered approach,"Digital solutions (including Extended Reality as well as web, mobile and AI technologies), among others, are entering a broad variety of industries and modifying the capabilities of operators through (close to) real-time display of context-dependent information. However, little is known with respect to two major issues: (i) how these technologies can be seamlessly integrated for product/process and overall manufacturing system management providing of syntactic, semantic and functional interoperability and reusability among the different manufacturing systems departments; (ii) how they can be conveyed to operators also taking into account the cognitive load that is incurred by the operators.
                  To this end, starting from a previous article (Longo et al., 2017), this article designs and proposes the KNOW4I approach and its practical implementation in an ICT platform (the KNOW4I platform) to further empower the Smart Operators concept.
                  Two major objectives are pursued. The former is to set a standard referred to as the KNOW4I methodological approach for knowledge representation, knowledge management and digital contents management within the Smart Operator domain. The latter is the implementation of the aforementioned approach as part of the KNOW4I platform that includes a suite of Smart Utilities and Objects intelligently and interactively linked with a newly released version of the Sophos-MS digital and intelligent Assistant. Experimentations and results based on multiple KPIs are carried out to account for the effectiveness of the proposed framework."
328,unknown,10.1109/icate49685.2021.9465023,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9465023/,2021-05-29 00:00:00,pv monitoring system using industrial internet of things technologies based on graphical programming,"Photovoltaic Panel Systems are already a commodity, the widespread use of such systems being no longer a pioneering topic. Monitoring these power generation systems is not a trivial task since the hunger of real-time data is growing continuously. The advent of IoT technologies, the ubiquitous presence of communication technologies, including remote areas, are making these solutions easier to develop, deploy and use. This will constitute the nurturing bed for optimization processes, which would eventually rely on Artificial Intelligence. This paper is describing how to design and deploy such data collecting system, emphasizing on its Software Architecture based on graphical programming technologies. Ready-to-run virtual instruments are facilitating real-time measurements and analysis, process data being securely stored and visualized using cloud technologies."
329,excluded,10.1109/iraset52964.2022.9737786,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9737786/,2022-03-04 00:00:00,smart energy management system: oil immersed power transformer failure prediction and classification techniques based on dga data,"The power transformer is the key element in the electrical grid. The failures of the power transformer impact critically the grid, can cause energy loss and blackouts. In energy production, transmission, distribution, and industrial applications, the oil immersed power transformer is the most used. The maintenance of this key equipment is highly important which can be done with different techniques such as thermal and vibration analysis, frequency analysis using wavelet transform and dissolved gas analysis. The application of predictive maintenance of the power transformer represents an important feature of the Smart Energy Management System in micro grids, that can reduce the percentage of failure occurrence while increasing the availability of the power transformer and prevents blackouts. This paper represents different failure classification techniques based on dissolved gas analysis data mainly logistic regression, multiclass jungle, multiclass decision tree and artificial neural network. The application can diagnosis the power transformer failures based on the parts-per-million of the different gas generated in the oil. The results of applying different types of classification algorithms shows the best technique to be part of a bigger system of monitoring and diagnostic of different installed equipment in a micro grid. The implementation of such application in real time energy management system requires different type of sensors and the interaction of offline database, the paper also shows the steps to integrate the algorithm in the Smart Energy Management System."
330,excluded,10.1155/2021/7906047,'Hindawi Limited',core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-01-01 00:00:00,optimization of the rapid design system for arts and crafts based on big data and 3d technology,"In this paper, to solve the problem of slow design of arts and crafts and to improve design efficiency and aesthetics, the existing big data and 3D technology are used to conduct an in-depth analysis of the optimization of the rapid design system of arts and crafts machine salt baking. In the system requirement analysis, the functional modules of this system are identified as nine functional modules such as design terminology management system and external information import function according to the actual usage requirements. In the system design, the overall structure design, database design, and functional module design of the system are comprehensively elaborated, and the key issues such as 3D display and home layout generation algorithm based on reinforcement learning are analyzed and designed. In the implementation part of the system, the overall construction of the system and the composition of functional modules are introduced in detail and the main functional modules of the system are presented with interface diagrams. In the system implementation part, the overall system construction and functional module composition are introduced in detail, the main functional modules of the system are shown with interface diagrams, codes, and algorithms, and the specific implementation process of 3D display and soft layout algorithms are also explained in detail. The process of Surface Mount Technology (SMT) big data processing and analysis is designed, and the design of SMT production line data collection scheme and real-time data processing architecture is completed. Based on the characteristics of SMT production line data, the K-means algorithm is used to detect data outliers and verify the accuracy of the method; also, the Spark-based association rule printing parameter recommendation model is designed, and the efficiency of the Apriori algorithm is significantly improved by parallelization"
331,unknown,10.1109/access.2020.2988735,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9072143/,2020-01-01 00:00:00,a framework for in-network qoe monitoring of encrypted video streaming,"With the amount of global network traffic steadily increasing, mainly due to video streaming services, network operators are faced with the challenge of efficiently managing their resources while meeting customer demands and expectations. A prerequisite for such Quality-of-Experience-driven (QoE) network traffic management is the monitoring and inference of application-level performance in terms of video Key Performance Indicators (KPIs) that directly influence end-user QoE. Given the persistent adoption of end-to-end encryption, operators lack direct insights into video quality metrics such as start-up delays, resolutions, or stalling events, which are needed to adequately estimate QoE and drive resource management decisions. Numerous solutions have been proposed to tackle this challenge on individual use-cases, most of them relying on machine learning (ML) for inferring KPIs from observable traffic patterns and statistics. In this paper, we summarize the key findings in state-of-the-art research on the topic. Going beyond previous work, we devise the concept of a generic framework for ML-based QoE/KPI monitoring of HTTP adaptive streaming (HAS) services, including model training, deployment, and re- evaluation. Components of the framework are designed in a generic way, independent of a particular streaming service and platform. The methodology for applying different framework components is discussed across various use-cases. In particular, we demonstrate framework applicability in a concrete use-case involving the YouTube service delivered to smartphones via the mobile YouTube app, as this presents one of the most prominent examples of accessing YouTube. We tackle both QoE/KPI estimation on a per-video-session level (utilizing the validated ITU-T P.1203 QoE model), as well as “real-time” KPI estimation over short time intervals. Obtained results provide important insights and challenges related to the deployment of a generic in-network QoE monitoring framework for encrypted video streams."
332,excluded,10.1016/j.cogsys.2020.11.002,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85097539429,2021-03-01,earthquake disaster avoidance learning system using deep learning,"The popularity of deep learning has influenced the field of surveillance and human safety. We adopt the advantages of deep learning techniques to recognize potentially harmful objects inside living rooms, offices, and dining rooms during earthquakes. In this study, we propose an educational system to teach earthquake risks using indoor object recognition based on deep learning algorithms. The system is based on the You Look Only Once (YOLO) deployed on our cloud-based server named Earthquake Situation Learning System (ESLS) for the detection of harmful objects associated with risk tags. ESLS is trained on our own indoor images dataset. The user interacts with the ESLS server through video or image files, and the object detection algorithm using YOLO recognizes the indoor objects with associated risk tags. Results show that the service time of ESLS is low enough to serve it to users in 0.8 s on average, including processing and communication times. Furthermore, the accuracy of the harmful object detection is 96% in the general indoor lighting situation. The results show that the proposed ESLS is applicable to real service for teaching the earthquake disaster avoidance."
333,unknown,10.32628/ijsrset229240,"International Journal of Scientific Research in Science, Engineering and Technology",semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/9656acc489a092e7d4c93c86ac45ccd411214da7,2022-01-01 00:00:00,machine learning based data analytics for iot enabled industry automation,"The main aims of this projects to the replacement of old communication that uses wired links with new communication that is wireless communication.The main reason to move to wireless communication is to improve the mobility, reduce the deployment cost, reduce cable damage and to improve the scalability.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence. The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The current industrial revolution is the 4.0 industrial revolution which combines different technologies such as Internet of Things (IOT), robotics, virtual reality and artificial intelligence.The second aim of this project is to connect devices to IOT so as to improve theaccessibility of the industry from anywhere in the world. These services are known as Best Effort services."
334,excluded,http://arxiv.org/abs/2011.10823v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.10823v2,2020-11-21 00:00:00,"a system for automatic rice disease detection from rice paddy images
  serviced via a chatbot","A LINE Bot System to diagnose rice diseases from actual paddy field images
was developed and presented in this paper. It was easy-to-use and automatic
system designed to help rice farmers improve the rice yield and quality. The
targeted images were taken from the actual paddy environment without special
sample preparation. We used a deep learning neural networks technique to detect
rice diseases from the images. We developed an object detection model training
and refinement process to improve the performance of our previous research on
rice leave diseases detection. The process was based on analyzing the model's
predictive results and could be repeatedly used to improve the quality of the
database in the next training of the model. The deployment model for our LINE
Bot system was created from the selected best performance technique in our
previous paper, YOLOv3, trained by refined training data set. The performance
of the deployment model was measured on 5 target classes and found that the
Average True Positive Point improved from 91.1% in the previous paper to 95.6%
in this study. Therefore, we used this deployment model for Rice Disease LINE
Bot system. Our system worked automatically real-time to suggest primary
diagnosis results to the users in the LINE group, which included rice farmers
and rice disease specialists. They could communicate freely via chat. In the
real LINE Bot deployment, the model's performance was measured by our own
defined measurement Average True Positive Point and was found to be an average
of 78.86%. The system was fast and took only 2-3 s for detection process in our
system server."
335,unknown,10.1016/j.ssci.2019.06.025,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85067872085,2019-12-01,securing instant messaging based on blockchain with machine learning,"Instant Messaging (IM) offers real-time communications between two or more participants on Internet. Nowadays, most IMs take place on mobile applications, such as WhatsApp, WeChat, Viber and Facebook Messenger, which have more users than social networks, such as Twitter and Facebook. Among the applications of IMs, online shopping has become a part of our everyday life, primarily those who are busiest. However, transaction disputes are often occurred online shopping. Since most IMs are centralized and message history is not stored in the center, the messaging between users and owners of online shops are not reliable and traceable. In China, online shopping sales have soared from practically zero in 2003 to nearly 600 hundred million dollars last year, and now top those in the United States. It is very crucial to secure the instant messaging in online shopping in China. We present techniques to exploit blockchain and machine learning algorithms to secure instant messaging. Since the cryptography of Chinese national standard is encouraged to adopt in security applications of China, we propose a blockchain-based IM scheme with the Chinese cryptographic bases. First, we design a message authentication model based on SM2 to avoid the counterfeit attack and replay attack. Second, we design a cryptographic hash mode based on SM3 to verify the integrity of message. Third, we design a message encryption model based on SM4 to protect the privacy of users. Besides, we propose a method based on machine learning algorithms to monitor the activity on blockchain to detect anomaly. To prove and verify the blockchain-based IM scheme, a blockchain-based IM system has been designed on Linux platforms. The implementation result shows that it is a practical and secure IM system, which can be applied to a variety of instant messaging applications directly."
336,excluded,10.1016/j.imu.2020.100335,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85084287220,2020-01-01,spark architecture for deep learning-based dose optimization in medical imaging,"Background and objectives
                  Deep Learning (DL) and Machine Learning (ML) have brought several breakthroughs to biomedical image analysis by making available more consistent and robust tools for the identification, classification, reconstruction, denoising, quantification, and segmentation of patterns in biomedical images. Recently, some applications of DL and ML in Computed Tomography (CT) scans for low dose optimization were developed. Nowadays, DL algorithms are used in CT to perform replacement of missing data (processing technique) such as low dose to high dose, sparse view to full view, low resolution to high resolution, and limited angle to full angle. Thus, DL comes with a new vision to process biomedical data imagery from CT scan. It becomes important to develop architectures and/or methods based on DL algorithms for minimizing radiation during a CT scan exam thanks to reconstruction and processing techniques.
               
                  Methods
                  This paper describes DL for CT scan low dose optimization, shows examples described in the literature, briefly discusses new methods used in CT scan image processing, and offers conclusions. We based our study on the literature and proposed a pipeline for low dose CT scan image reconstruction. Our proposed pipeline relies on DL and the Spark Framework using MapReduce programming. We discuss our proposed pipeline with those proposed in the literature to conclude the efficiency and importance.
               
                  Results
                  An architecture for low dose optimization using CT imagery is suggested. We used the Spark Framework to design the architecture. The proposed architecture relies on DL, and permits us to develop efficient and appropriate methods to process dose optimization with CT scan imagery. The real implementation of our pipeline for image denoising shows that we can reduce the radiation dose, and use our proposed pipeline to improve the quality of the captured image.
               
                  Conclusion
                  The proposed architecture based on DL is complete and enables faster processing of biomedical CT imagery as compared with prior methods described in the literature."
337,excluded,10.1109/iciba52610.2021.9688086,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9688086/,2021-12-19 00:00:00,research on recommendation algorithm based on e-commerce user behavior sequence,"With the rapid rise and development of the Internet, e-commerce has become one of the most promising and fastest-growing industries. we propose an architecture of a personalized recommendation system based on offline mining, real-time mining, and deep learning technology. First, through the Flume + Kafka + Spark Streaming system, user behavior data is collected and stored in the business database after data preprocessing First, through the Flume + Kafka + Spark Streaming system, user behavior data is collected and stored in the business database after data preprocessing, which is the work preparing for the next step of data mining. Data mining includes offline mining and real-time mining. Offline mining mainly runs MapReduce on the Hadoop platform, there are also some calculations on the Spark platform, and the calculation results are stored in the business database; The implementation of real-time mining mainly is based on the message subscription of the Kafka cluster, and get real-time consumption statistics through Spark clusters; Deeplearning4j, a deep learning framework, runs on a multi-GPU Spark distributed cluster, which can be optimized the algorithm model online and generate recommendation results to push to the real-time business database and give feedback to the users.; It can also extract features from these data for training automatically, which is conducive to improving the quality and efficiency of users' shopping decisions, and improve the platform's cross-selling capabilities, shorten the user's shopping path, increase traffic conversion rates."
338,excluded,10.1007/s12652-017-0571-8,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s12652-017-0571-8,2018-10-01 00:00:00,fuzzy association rule mining for recognising daily activities using kinect sensors and a single power meter,"The recognition of activities of daily living (ADLs) by home monitoring systems can be helpful in order to objectively assess the health-related living behaviour and functional ability of older adults. Many ADLs involve human interactions with household electrical appliances (HEAs) such as toasters and hair dryers. Advances in sensor technology have prompted the development of intelligent algorithms to recognise ADLs via inferential information provided from the use of HEAs. The use of robust unsupervised machine learning techniques with inexpensive and retrofittable sensors is an ongoing focus in the ADL recognition research. This paper presents a novel unsupervised activity recognition method for elderly people living alone. This approach exploits a fuzzy-based association rule-mining algorithm to identify the home occupant’s interactions with HEAs using a power sensor, retrofitted at the house electricity panel, and a few Kinect sensors deployed at various locations within the home. A set of fuzzy rules is learned automatically from unlabelled sensor data to map the occupant’s locations during ADLs to the power signatures of HEAs. The fuzzy rules are then used to classify ADLs in new sensor data. Evaluations in real-world settings in this study demonstrated the potential of using Kinect sensors in conjunction with a power meter for the recognition of ADLs. This method was found to be significantly more accurate than just using power consumption data. In addition, the evaluation results confirmed that, owing to the use of fuzzy logic, the proposed method tolerates real-life variations in ADLs where the feature values in new sensor data differ slightly from those in the learning patterns."
339,excluded,10.1109/access.2020.2996214,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9097876/,2020-01-01 00:00:00,a machine learning security framework for iot systems,"Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges."
340,excluded,10.1109/jiot.2021.3096637,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9481251/,2001-02-01 20:22:00,an integrated framework for health state monitoring in a smart factory employing iot and big data techniques,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &#x0026; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues."
341,excluded,10.1109/idt52577.2021.9497540,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9497540/,2021-06-24 00:00:00,phishing detection for secure operations of uavs,"Unmanned Aerial Vehicles (UAV) or drones are used in various domains more and more, including military operations, monitoring, rescue of victims, and transport. Often, UAV resources are developed as web services so that they can be accessed anywhere on the Internet through the World Wide Web. However, this makes them vulnerable to phishing activities of criminals who may try to access these resources and other sensitive information. Therefore, the development of a phishing detection tool based on data mining is presented in this paper. It consists of a browser extension monitoring visited webpages and a backend communicating with the browser extension for the purposes of executing some specific tasks. The browser extension is implemented in JavaScript and the ReactJS framework, and it contains an implementation of classifications with a Bayesian network, decision tree, nearest neighbor classifier and neural network. The backend uses PHP, Python scripts and the Apache HTTP Server. In addition, a browser extension is implemented so that data about webpages can be collected and this data is used for the creation of data mining models. Experimental validation with 10-fold cross-validation and through the browsing of real-world websites show promising results in phishing detection."
342,excluded,10.1109/igcc.2016.7892604,2016 Seventh International Green and Sustainable Computing Conference (IGSC),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/8ad5ecbd6b5003bc8ecd8ee14add64030eaf3aa0,2016-01-01 00:00:00,autoplug: an automated metadata service for smart outlets,"Low-cost network-connected smart outlets are now available for monitoring, controlling, and scheduling the energy usage of electrical devices. As a result, such smart outlets are being integrated into automated home management systems, which remotely control them by analyzing and interpreting their data. However, to effectively interpret data and control devices, the system must know the type of device that is plugged into each smart outlet. Existing systems require users to manually input and maintain the outlet metadata that associates a device type with a smart outlet. Such manual operation is time-consuming and error-prone: users must initially inventory all outlet-to-device mappings, enter them into the management system, and then update this metadata every time a new device is plugged in or moves to a new outlet. Inaccurate metadata may cause systems to misinterpret data or issue incorrect control actions. To address the problem, we propose AutoPlug, a system that automatically identifies and tracks the devices plugged into smart outlets in real time without user intervention. AutoPlug combines machine learning techniques with time-series analysis of device energy data in real time to accurately identify and track devices on startup, and as they move from outlet-to-outlet. We show that AutoPlug achieves ∼90% identification accuracy on real data collected from 13 distinct device types, while also detecting when a device changes outlets with an accuracy >90%. We implement an AutoPlug prototype on a Raspberry Pi and deploy it live in a real home for a period of 20 days. We show that its performance enables it to monitor up to 25 outlets, while detecting new devices or changes in devices with <50s latency."
343,unknown,10.1109/i-smac49090.2020.9243544,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9243544/,2020-10-09 00:00:00,scalable iot solution using cloud services – an automobile industry use case,"The role of IoT and related internet-based applications in otherwise mechanical devices to monitor, manage and enhance the performance of the same is quite widespread now. Almost all public cloud service providers provide scalable, fully managed and elastic IoT related services. The data flows from these services are essentially streaming and can be consumed for further use in various predictive, descriptive and visualization modules. The cloud platforms enable ingestion, transformation and usage of the data by providing streaming, machine learning and sharable visualization services. This ecosystem greatly reduces the time to create IoT based minimum viable product creation which in turn enhances the business value realization cycle. The effect of cycle time reduction to design, architect and develop IoT solutions leads to a rapid improvement of business lead time and makes it easier for businesses to gain from the data insights and plan the next course of action. In this paper, one such enterprise graded use case is explored, in which the Azure IoT platform in terms of the offerings and associated ecosystem of Azure Stream Analytics and Azure Machine learning services are explained. This paper covers design, architecture, development and deployment of the solution prepared and how the same is monitored once in production. Security is a very important aspect of the same and here the security architecture is being explored. A conclusion is presented with the scope of future enhancements using auto ML services in serverless platforms to enable real-time automated decision making augmented with human expertise and intelligence."
344,unknown,http://arxiv.org/abs/2001.06202v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2001.06202v1,2020-01-17 00:00:00,"fedvision: an online visual object detection platform powered by
  federated learning","Visual object detection is a computer vision-based artificial intelligence
(AI) technique which has many practical applications (e.g., fire hazard
monitoring). However, due to privacy concerns and the high cost of transmitting
video data, it is highly challenging to build object detection models on
centrally stored large training datasets following the current approach.
Federated learning (FL) is a promising approach to resolve this challenge.
Nevertheless, there currently lacks an easy to use tool to enable computer
vision application developers who are not experts in federated learning to
conveniently leverage this technology and apply it in their systems. In this
paper, we report FedVision - a machine learning engineering platform to support
the development of federated learning powered computer vision applications. The
platform has been deployed through a collaboration between WeBank and Extreme
Vision to help customers develop computer vision-based safety monitoring
solutions in smart city applications. Over four months of usage, it has
achieved significant efficiency improvement and cost reduction while removing
the need to transmit sensitive data for three major corporate customers. To the
best of our knowledge, this is the first real application of FL in computer
vision-based tasks."
345,excluded,10.1109/bigdatacongress.2019.00032,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8818217/,2019-07-13 00:00:00,"premises, a scalable data-driven service to predict alarms in slowly-degrading multi-cycle industrial processes","In recent years, the number of industry-4.0-enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, Machine Learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack."
346,excluded,10.1109/access.2020.2996576,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9098886/,2020-01-01 00:00:00,an embedded system for collection and real-time classification of a tactile dataset,"Tactile perception of the material properties in real-time using tiny embedded systems is a challenging task and of grave importance for dexterous object manipulation such as robotics, prosthetics and augmented reality. As the psychophysical dimensions of the material properties cover a wide range of percepts, embedded tactile perception systems require efficient signal feature extraction and classification techniques to process signals collected by tactile sensors in real-time. For this purpose, we developed two embedded systems, one that served as a vibrotactile stimulator system and one that recorded and classified the vibrotactile signals collected by its sensors. The quality of the collected data was first verified offline using Fourier transform for feature extraction and then applying powerful machine learning classifiers such as support vector machines and neural networks. We implemented the proposed memory-less signal feature extraction method in order to achieve real-time processing as the data is being collected. The experimental results have shown that the proposed method significantly reduces the computational complexity of feature extraction and still has led to high classification accuracy even when fed to the less complex classifiers such as random forests that can be easily implemented on embedded systems. Finally, we have also shown that low-cost, highly accurate, and real-time tactile texture classification can be achieved using the proposed approach with an ensemble of sensors."
347,unknown,10.1145/3292500.3330723,KDD,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/7436d86cd6b572a3f52caa7820c07e7bfcf16f86,2019-01-01 00:00:00,gmail smart compose: real-time assisted writing,"In this paper, we present Smart Compose, a novel system for generating interactive, real-time suggestions in Gmail that assists users in writing mails by reducing repetitive typing. In the design and deployment of such a large-scale and complicated system, we faced several challenges including model selection, performance evaluation, serving and other practical issues. At the core of Smart Compose is a large-scale neural language model. We leveraged state-of-the-art machine learning techniques for language model training which enabled high-quality suggestion prediction, and constructed novel serving infrastructure for high-throughput and real-time inference. Experimental results show the effectiveness of our proposed system design and deployment approach. This system is currently being served in Gmail."
348,unknown,10.1016/j.promfg.2020.04.017,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85085527469,2020-01-01,ppe compliance detection using artificial intelligence in learning factories,"This project demonstrates the application of Artificial Intelligence (AI) and machine vision for the identification of Personal Protective Equipment (PPE), particularly safety glasses in zones of the Learning Factory, where safety risks exist. The objective is to design and implement an automated system for ensuring the safety of personnel when they are in the vicinity of machinery that presents potential risks to the eyes. Microsoft Azure Custom Vision AI and Intelligent AI Services, in conjunction with low-cost vision devices with lightweight onboard AI capability, provide a platform for a deep learning neural network model using publicly available images under the Creative Commons License. A combination of cloud-based and on-premises AI is used in this proof of concept system to provide a real-time vision-based safety system capable of detecting and recording potential safety breaches, promoting compliance, and ultimately preventing accidents before they happen. This system can be used to initiate different control actions in the event of safety violations and can detect multiple forms of protective wear. The flexibility of the system offers multiple benefits to learning factories and manufacturing organizations such as improved user safety, reduced insurance costs, and better detection and recording of safety violations. The hybrid AI architecture approach allows for flexibility in training and deployment based on the capability of local computing resources."
349,excluded,10.1016/j.eswa.2017.02.022,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85013790845,2017-07-15,improvement of newborn screening using a fuzzy inference system,"This paper presents a decision support system (DSS) called DSScreening to rapidly detect inborn errors of metabolism (IEMs) in newborn screening (NS). The system has been created using the Aide-DS framework, which uses techniques imported from model-driven software engineering (MDSE) and soft computing, and it is available through eGuider, a web portal for the enactment of computerised clinical practice guidelines and protocols.
                  MDSE provides the context and techniques to build new software artefacts based on models which conform to a specific metamodel. It also offers separation of concern, to disassociate medical from technological knowledge, thus allowing changes in one domain without affecting the other. The changes might include, for instance, the addition of new disorders to the DSS or new measures to the computation related to a disorder. Artificial intelligence and soft computing provide fuzzy logic to manage uncertainty and ambiguous situations. Fuzzy logic is embedded in an inference system to build a fuzzy inference system (FIS); specifically, a single-input rule modules connected zero-order Takagi-Sugeno FIS. The automatic creation of FISs is performed by the Aide-DS framework, which is capable of embedding the generated FISs in computerized clinical guidelines. It can also create a desktop application to execute the FIS. Technologically, it supports the addition of new target languages for the desktop applications and the inclusion of new ways of acquiring data.
                  DSScreening has been tested by comparing its predictions with the results of 152 real analyses from two groups: (1) NS samples and (2) clinical samples belonging to individuals of all ages with symptoms that do not necessarily correspond to an IEM. The system has reduced the time needed by 98.7% when compared to the interpretation time spent by laboratory professionals. Besides, it has correctly classified 100% of the NS samples and obtained an accuracy of 70% for samples belonging to individuals with clinical symptoms."
350,excluded,10.1049/ic:19960180,IET,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/578417/,1996-02-19 00:00:00,intelligent agents for distributed patient care,"The provision of medical care typically involves a number of individuals, located in a number of different institutions, whose decisions and actions need to be coordinated if the care is to be effective and efficient. To facilitate this decision making and to ensure the coordination process runs smoothly, the use of software support is becoming increasingly widespread. To this end, this paper describes an agent-based system which was developed to help manage the care process in real-world settings. The agents themselves are implemented using a layered architecture, called AADCare (Agent Architecture for Distributed Care), which combines a number of AI and agent techniques. The utility of this approach is demonstrated through the development of an application prototype for the clinical process of cancer treatment."
351,included,10.1109/bigdata.2017.8258089,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8258089/,2017-12-14 00:00:00,"flux: groupon's automated, scalable, extensible machine learning platform","As machine learning becomes the driving force of the daily operation of companies within the information technology sector, infrastructure that enables automated, scalable machine learning is a core component of the systems of many large companies. Various systems and products are being built, offered, and open sourced. As an e-commerce company, numerous aspects of Groupon's business is driven by machine learning. To solve the scalability issue and provide a seamless collaboration between data scientists and engineers, we built Flux, a system that expedites the deployment, execution, and monitoring of machine learning models. Flux focuses on enabling data scientists to build model prototypes with languages and tools they are most proficient in, and integrating the models into the enterprise production system. It manages the life cycle of deployed models, and executes them in distributed batch mode, or exposes them as micro-services for real-time use cases. Its design focuses on automation and easy management, scalability, and extensibility. Flux is the central system for supervised machine learning tasks at Groupon and has been supporting multiple teams across the company."
352,included,10.3390/drones4020018,Drones,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/4f6624b0fe7735dd4f7b703a1b27020d6f6df839,2020-01-01 00:00:00,sharkeye: real-time autonomous personal shark alerting via aerial surveillance,"While aerial shark spotting has been a standard practice for beach safety for decades, new technologies offer enhanced opportunities, ranging from drones/unmanned aerial vehicles (UAVs) that provide new viewing capabilities, to new apps that provide beachgoers with up-to-date risk analysis before entering the water. This report describes the Sharkeye platform, a first-of-its-kind project to demonstrate personal shark alerting for beachgoers in the water and on land, leveraging innovative UAV image collection, cloud-hosted machine learning detection algorithms, and reporting via smart wearables. To execute, our team developed a novel detection algorithm trained via machine learning based on aerial footage of real sharks and rays collected at local beaches, hosted and deployed the algorithm in the cloud, and integrated push alerts to beachgoers in the water via a shark app to run on smartwatches. The project was successfully trialed in the field in Kiama, Australia, with over 350 detection events recorded, followed by the alerting of multiple smartwatches simultaneously both on land and in the water, and with analysis capable of detecting shark analogues, rays, and surfers in average beach conditions, and all based on ~1 h of training data in total. Additional demonstrations showed potential of the system to enable lifeguard-swimmer communication, and the ability to create a network on demand to enable the platform. Our system was developed to provide swimmers and surfers with immediate information via smart apps, empowering lifeguards/lifesavers and beachgoers to prevent unwanted encounters with wildlife before it happens."
353,unknown,10.1109/ocit53463.2021.00036,2021 19th OITS International Conference on Information Technology (OCIT),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1c81ce769ea84598a5a94dd540cff152b21e5e09,2021-01-01 00:00:00,intelligent edge detection of attacks on ip-based iot deployments,"Edge-based computing and intelligence is a promising approach to reduce backhaul traffic load. This technology when applied to Intrusion Detection Systems (IDS), has the potential of detecting the edge-network intrusions in real-time, with minimal latency as compared to centralized cloud-based approaches. IP-based deployment of IoT has become common choice because of the wide availability of IP-based smart devices for building and home automation. In this paper, a novel intelligent edge-based architecture using machine learning, for classification, in order to detect intrusions in IP based IoT deployments is explored. The focus of this paper is to practically realize the proposed architecture as a prototype, rather than the application of machine learning techniques on available datasets. The paper presents the design details of this architecture. The results encompassing the functional and performance tests of this system prototype, as applied for the SYN-flood attack are shown. The engineering details namely, i) Feature extraction ii) Machine learning techniques used iii) Azure edge porting and usage on Raspberry Pi3 as edge device, which generates alerts to a cloud-based client, are detailed. The classification techniques XGBoost, Support Vector Method, and Logistic Regression are used and their performance is compared."
354,excluded,10.1109/access.2021.3074319,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9408578/,2021-01-01 00:00:00,hawk-eye: an ai-powered threat detector for intelligent surveillance cameras,"With recent advances in both AI and IoT capabilities, it is possible than ever to implement surveillance systems that can automatically identify people who might represent a potential security threat to the public in real-time. Imagine a surveillance camera system that can detect various on-body weapons, masked faces, suspicious objects and traffic. This system could transform surveillance cameras from passive sentries into active observers which would help in preventing a possible mass shooting in a school, stadium or mall. In this paper, we present a prototype implementation of such systems, Hawk-Eye, an AI-powered threat detector for smart surveillance cameras. Hawk-Eye can be deployed on centralized servers hosted in the cloud, as well as locally on the surveillance cameras at the network edge. Deploying AI-enabled surveillance applications at the edge enables the initial analysis of the captured images to take place on-site, which reduces the communication overheads and enables swift security actions. At the cloud side, we built a Mask R-CNN model that can detect suspicious objects in an image captured by a camera at the edge. The model can generate a high-quality segmentation mask for each object instance in the image, along with the confidence percentage and classification time. The camera side used a Raspberry Pi 3 device, Intel Neural Compute Stick 2 (NCS 2), and Logitech C920 webcam. At the camera side, we built a CNN model that can consume a stream of images directly from an on-site webcam, classify them, and displays the results to the user via a GUI-friendly interface. A motion detection module is developed to capture images automatically from the video when a new motion is detected. Finally, we evaluated our system using various performance metrics such as classification time and accuracy. Our experimental results showed an average overall prediction accuracy of 94% on our dataset."
355,excluded,10.1109/iciafs52090.2021.9605823,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9605823/,2021-08-13 00:00:00,deep learning & computer vision for iot based intelligent driver assistant system,"With the exponential increment of vehicles, roadside accidents also have been increasing rapidly where approximately 80% of these accidents were caused by human error. Therefore, the automobile industry and the government authorities are more focused on accident prevention by introducing improved road safety systems for the general public. Driver assistance system (DAS) is an intelligent road safety development where it senses the surrounding of a moving vehicle which will assist the driver to avoid the dangers and also warn the drivers of immediate dangers. With the current technological advancements, the automobile industry is equipped with the internet of things (IoT) based data transfer mechanisms, wherewith the concept of ‘connected car’ the passengers and the other interconnected vehicles with the internet can share data with back end applications. The data is consisting of the current location, the distance travelled by the vehicle, whether the vehicle requires urgent service. This study is mainly focused on the development of an intelligent driver assistance system based on computer vision and deep learning, which can prevent accidents with early detection of drowsiness, harmful objects and also by alerting the driver with the signboards and the road lines. The system is capable of passing the emergency messages to the driver as well as the other interconnected vehicles through the website by communicating the real-time road map generated within the system. The proposed system has been implemented and tested with multiple detection scenarios where machine learning has been employed to improve the accuracy of the results."
356,unknown,10.1016/j.envsci.2021.06.011,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85111282536,2021-10-01,a big data and artificial intelligence framework for smart and personalized air pollution monitoring and health management in hong kong,"All people in the world are entitled to enjoy a clean environment and a good quality of life. With big data and artificial intelligence technologies, it is possible to estimate personalized air pollution exposure and synchronize it with activity, health, quality of life and behavioural data, and provide real-time, personalized and interactive alert and advice to improve the health and well-being of individual citizens. In this paper, we propose an overarching framework outlining five major challenges to personalized air pollution monitoring and health management, and respective methodologies in an integrated interdisciplinary manner. First, urban air quality data is sparse, rendering it difficult to provide timely personalized alert and advice. Second, collected data, especially those involving human inputs such as health perception, are often missing and erroneous. Third, the data collected are heterogeneous, and highly complex, not easily comprehensible to facilitate individual and collective decision-making. Fourth, the causal relationships between personal air pollutants exposure (specifically, PM2.5 and PM1.0 and NO2) and personal health conditions, and health-related quality of life perception, of young asthmatics and young healthy citizens in Hong Kong (HK), are yet to be established. Fifth, whether personalized and smart information and advice provided can induce behavioural change and improve health and quality of life are yet to be determined. To overcome these challenges, our first novelty is to develop an AI and big data framework to estimate and forecast air quality in high temporal-spatial resolution and real-time. Our second novelty includes the deployment of mobile pollution sensor platforms to substantially improve the accuracy of estimated and forecasted air quality data, and the collection of activity, health condition and perception data. Our third novelty is the development of visualization tools and comprehensible indexes, by correlating personal exposure with four types of personal data, to provide timely, personalized pollution, health and travel alerts and advice. Our fourth novelty is determining causal relationship, if any, between personal pollutants, PM1.0 and PM2.5, NO2 exposure and personal health condition, and personal health perception, based on a clinical experiment of 150 young asthmatics and 150 young healthy citizens in HK. Our fifth novelty is an intervention study to determine if smart information, presented via our proposed visualized platform, will induce personal behavioural change. Our novel big data AI-driven approach, when integrated with other analytical approaches, provides an integrated interdisciplinary framework for personalized air pollution monitoring and health management, easily transferrable to and applicable in other domains and countries."
357,unknown,10.1016/j.procs.2021.09.145,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85116935586,2021-01-01,"architecture and organization of a platform for diagnostics, therapy and post-covid complications using ai and mobile monitoring","Infectious diseases accompanied mankind throughout its existence. However, in the 20th century, with the implementation od mass vaccination, this problem was partially forgotten. It reappeared at the end of the 2019 with the COVID-19 pandemic. The diseases are associated with high mortality, the main causes of which are: respiratory failure, acute respiratory distress syndrome, thrombotic complications, etc. As many centuries ago, the key to fighting a pandemic is to diagnose patients with infections as quickly as possible, isolate them, and implement treatment procedures. In this paper we propose a Platform supporting medics in the fight against epidemic. Unlike alternative systems, the proposed IT Platform will ultimately cover all areas of fighting against COVID-19, from the diagnosis of infection, through treatment, to rehabilitation of post-disease complications. Like most clinical information systems, the Platform is based on Artificial Intelligence, in particular Federated Learning. Also, unlike known solutions, it uses all available historical data of the patient’s health and information from real-time mobile diagnostics, using cellular communication and Internet of Things solutions. Such solutions could be helpful in fighting against any future mass infections."
358,unknown,10.1109/access.2018.2800728,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8278160/,2018-01-01 00:00:00,a convolutional neural network smartphone app for real-time voice activity detection,"This paper presents a smartphone app that performs real-time voice activity detection based on convolutional neural network. Real-time implementation issues are discussed showing how the slow inference time associated with convolutional neural networks is addressed. The developed smartphone app is meant to act as a switch for noise reduction in the signal processing pipelines of hearing devices, enabling noise estimation or classification to be conducted in noise-only parts of noisy speech signals. The developed smartphone app is compared with a previously developed voice activity detection app as well as with two highly cited voice activity detection algorithms. The experimental results indicate that the developed app using convolutional neural network outperforms the previously developed smartphone app."
359,unknown,http://arxiv.org/abs/2206.07490v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2206.07490v1,2022-05-21 00:00:00,demo: low-power communications based on ris and ai for 6g,"Ultra-massive multiple-input-multiple-output (UM-MIMO) is promising to meet
the high rate requirements for future 6G. However, due to the large number of
antennas and high path loss, the hardware power consumption and computing power
consumption of UM-MIMO will be unaffordable. To address this problem, we
implement a low-power communication system based on reconfigurable intelligent
surface (RIS) and artificial intelligence (AI) for 6G. For hardware design, we
employ a 256-element RIS at the base station to replace the traditional phased
array. Moreover, a 2304-element RIS is developed as a relay to assist
communication with much reduced transmit power. For software implementation, we
develop an AI-based transmission design to reduce computing power consumption.
By jointly designing the hardware and software, this prototype can realize
real-time 4K video transmission with much reduced power consumption."
360,excluded,10.1016/0926-5805(92)90037-k,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/44049119146,1992-01-01,expert robots for process environments,"The Robotics Technology Group at the Savannah River Laboratory has developed expert mobile robots and control strategies for process applications. An expert robot is an intelligent machine that senses its environment, responds to its environment, and can perform independent tasks as outlined by a knowledge-based control system that incorporates artificial intelligence techniques. A real-time expert system is employed, whereby robots are treated as dynamic process elements in the overall process environment. The system has been implemented using a functional, layered control architecture. Decision, analysis, and control tasks are shared between the mobile robot actually performing the task and supervisory computers that interact with process data. Investigated are the control architecture and the performance of the laboratory development system."
361,unknown,10.1109/csnt54456.2022.9787591,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9787591/,2022-04-24 00:00:00,q-learning based intelligent and interactive healthcare system,"With the advent of the COVID-19 pandemic, the health care system has suffered a tremendous setback, especially in developing countries. Q Learning is a leading and widely used Reinforcement Learning scheme. Q-Learning can be applied to a variety of real-time applications. This paper proposes a health care system based on the Q learning algorithm and its implementation for a set of targeted nodes. The proposed system consists of 4 phases, front-end system, RRH, BBU pool, and computing. Each phase consists of different network components that are considered nodes. We have different rewards for nodes and pathways. The AI agent will continue to change its approach for future actions."
362,excluded,10.1109/icsens.2016.7808626,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7808626/,2016-11-03 00:00:00,floodeye: real-time flash flood prediction system for urban complex water flow,"This study presents a water-level sensor system that has multiple functions: real-time river-level monitoring and high accurate flood prediction for urban complex water flow (CWF) flooding caused by Localized Heavy Rain. Traditional water-level sensors for large-scale river can detect water-level rising, though they have limitations in detecting urban river dyke height to alert about flooding. This is a result of various riverside environmental limitations. Additionally, previous prediction methods are difficult to detect CWF flooding with high accuracy because water rising differ depends on a location's environment. Therefore, we propose a detection scheme for CWF by developing a water-level sensor system that works in various installation environments using infrared image processing with both low installation and operation cost. We implemented a CWF prediction system that produces accurate and early predictions using a Linear Regression (LR) of deep learning approach with data assimilation. Evaluating based on actual installations (over 1,000 days, 13 locations), our system proved to accurately monitor and predict water-level rising after 5 minutes."
363,excluded,10.1109/icitr.2018.8736158,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8736158/,2018-12-07 00:00:00,google map and camera based fuzzified adaptive networked traffic light handling model,"Rising traffic congestion has turned into a certain issue as the number of vehicles on roads are increasing. This research study was conducted to develop `Google Map and Camera Based Fuzzified Adaptive Networked Traffic Light Handling Model'. The main road with six major junctions was selected as the target route for the project. During this study, we were able to plan a limit and control traffic congestion utilizing two neural networks which process together to provide an efficient, productive and optimized solution based on real-time situations. Real-time video streams and Google Map traffic layer were used as primary input sources to the system. The Main algorithm was used to reduce traffic at a specific point whereas secondary algorithm was used to produce optimum decisions for the overall network. As a further advancement, REST endpoint was implemented to get the best route considering all the accessible data. With the aid of the previously mentioned techniques, an optimal traffic management model was developed."
364,excluded,10.1016/j.ifacol.2016.11.160,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85006454620,2016-01-01,neural networks as a diagnosing tool for industrial level measurement through non-contacting radar type and support to the decision for its better application,"The aim of this study was to develop an analysis tool based on artificial neural networks (ANN) to detect level measurement problems with free wave propagation radars. The trend of using this type of radar has been growing in the last ten years mainly because of its easy installation on the top of tanks and reservoirs, and for its low rate maintenance comparing to other level measurement technologies. For the experiments, a Rosemount radar was used and the training of the neural network was based on the data from the software Radar Master. Therefore, some network topologies in different scenarios were tested and it was possible to demonstrate the efficiency of the ANN with accuracy rate between 94.44 to 100% for the first experiment with networks using 10, 20 or 50 neurons in the hidden layer. This technique was applied in a real industrial application, a sugar and ethanol mill, and accuracy rate was about 87,0 to 96,1%. This methodology can be applied to asset management software for diagnosis report or troubleshooting which would increase the level measurement reliability and plant safety."
365,unknown,10.1007/978-3-030-65310-1_22,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-65310-1_22,2020-01-01 00:00:00,real-time automatic configuration tuning for smart manufacturing with federated deep learning,"Manufacturing systems contain a large number of parameters, and a proper configuration of parameters is very important to ensure the stability of product quality. Traditional configuration methods rely heavily on manual tuning, which is labor-intensive, time-consuming, and poor performance. In this paper, we propose to build deep learning models on the vast amount of industrial data collected by IIoT devices for automatic configuration tuning. In order to address key challenges such as high data redundancy, limited device capacity, latency-sensitivity, and system heterogeneity, we propose a two-level federated deep learning framework. We first extract representative features from redundant data, and reduce network traffic and latency through joint training on plants and the cloud. Timely configuration tuning is made through local models of plants, and the tuning accuracy is improved through the global model in the cloud. We have deployed and evaluated the performance of the proposed model in real-world smart manufacturing systems, and the experimental results confirm its effectiveness."
366,excluded,10.1121/10.0010575,The Journal of the Acoustical Society of America,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/ee92b1f060f4e772161a1eac90df9da40d23925d,2022-01-01 00:00:00,combining remote sensing and artificial intelligence to locate sounds,"Smart cities use sensors to collect and analyze data to manage their resources more efficiently and thereby enhance the quality of life for residents, especially in densely populated cities. They monitor a wide range of information, including pollution, traffic, and parking. Urban environments feature large numbers of sounds, such as noise pollution and other specific sounds, which can be harmful to citizens. Therefore, this unrestrained growing issue of urban sounds should be addressed by smart cities to improve noise mitigation. To sidestep that problem, the “Listening system Using a Crow’s nest arraY” (LUCY) is currently in development at Fraunhofer FKIE. The acoustic system aims are to automatically detect meaningful audio events contained in noisy data, such as impulsive sounds, and to accurately estimate their geographic locations. To accomplish these tasks in near real-time, LUCY has to combine advanced array processing techniques, including beamforming, with artificial intelligence methods, such as deep learning using spectro-temporal features. The proposed acoustic system is a low-cost, small and lightweight system. It consists of a peculiar volumetric array of tiny MEMS microphones, called the “Crow’s Nest Array” (CNA), which has a crucial influence on the accuracy of the sound localization estimation, and a miniature computer to process methods including sound localization calculation. Due to its small size, LUCY can easily be deployed on numerous types of platforms, including Unmanned Aerial Vehicles (UAVs)."
367,unknown,10.1145/2886107,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/802fdb1252dddefa7db7c3ceba49d7274e11426c,2016-01-01 00:00:00,an architecture for and fast and general data processing on large clusters,"Today, a myriad data sources, from the Internet to business operations to scientific instruments, produce large and valuable data streams. However, the processing capabilities of single machines have not kept up with the size of data. As a result, organizations increasingly need to scale out these computations to clusters of hundreds of machines. 
 
At the same time, the speed and sophistication required of data processing have grown. In addition to simple queries, complex algorithms like machine learning and graph analysis are becoming common. And in addition to batch processing, streaming analysis of real-time data is required to let organizations take timely action. Future computing platforms will need to not only scale out traditional workloads, but support these new applications too. 
 
This book, a revised version of the 2014 ACM Dissertation Award winning dissertation, proposes an architecture for cluster computing systems that can tackle emerging data processing workloads at scale. Whereas early cluster computing systems, like MapReduce, handled batch processing, our architecture also enables streaming and interactive queries, while keeping MapReduce's scalability and fault tolerance. And whereas most deployed systems only support simple one-pass computations (e.g., SQL queries), ours also extends to the multi-pass algorithms required for complex analytics like machine learning. Finally, unlike the specialized systems proposed for some of these workloads, our architecture allows these computations to be combined, enabling rich new applications that intermix, for example, streaming and batch processing. 
 
We achieve these results through a simple extension to MapReduce that adds primitives for data sharing, called Resilient Distributed Datasets (RDDs). We show that this is enough to capture a wide range of workloads. We implement RDDs in the open source Spark system, which we evaluate using synthetic and real workloads. Spark matches or exceeds the performance of specialized systems in many domains, while offering stronger fault tolerance properties and allowing these workloads to be combined. Finally, we examine the generality of RDDs from both a theoretical modeling perspective and a systems perspective. 
 
This version of the dissertation makes corrections throughout the text and adds a new section on the evolution of Apache Spark in industry since 2014. In addition, editing, formatting, drawing of illustrations, and links for the references have been added."
368,unknown,10.3390/s21093061,Sensors,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d43f0ad90234d1e52e429251216febdf32319939,2021-01-01 00:00:00,a navigation and augmented reality system for visually impaired people †,"In recent years, we have assisted with an impressive advance in augmented reality systems and computer vision algorithms, based on image processing and artificial intelligence. Thanks to these technologies, mainstream smartphones are able to estimate their own motion in 3D space with high accuracy. In this paper, we exploit such technologies to support the autonomous mobility of people with visual disabilities, identifying pre-defined virtual paths and providing context information, reducing the distance between the digital and real worlds. In particular, we present ARIANNA+, an extension of ARIANNA, a system explicitly designed for visually impaired people for indoor and outdoor localization and navigation. While ARIANNA is based on the assumption that landmarks, such as QR codes, and physical paths (composed of colored tapes, painted lines, or tactile pavings) are deployed in the environment and recognized by the camera of a common smartphone, ARIANNA+ eliminates the need for any physical support thanks to the ARKit library, which we exploit to build a completely virtual path. Moreover, ARIANNA+ adds the possibility for the users to have enhanced interactions with the surrounding environment, through convolutional neural networks (CNNs) trained to recognize objects or buildings and enabling the possibility of accessing contents associated with them. By using a common smartphone as a mediation instrument with the environment, ARIANNA+ leverages augmented reality and machine learning for enhancing physical accessibility. The proposed system allows visually impaired people to easily navigate in indoor and outdoor scenarios simply by loading a previously recorded virtual path and providing automatic guidance along the route, through haptic, speech, and sound feedback."
369,excluded,10.1109/icosec51865.2021.9591717,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9591717/,2021-10-09 00:00:00,record.ai - an ai based solution to classify calls based on conversation,"Communication over the phone has always been one of the most prone communication channels as it just requires a phone number to get connected and audio to communicate. Other than the already ""spam"" categorized calls of telecommunication companies providing offers and services, there are many other kinds of suspicious and unauthorized phone calls where the caller just stays behind the screen pretending to be a legit and an authorized person while being a non-legit one with malicious intent. People often come across the real-life incidents of such fraudulent activities including fund transfer on a call through illegal affairs or using the phone to make a threat or blackmail call to somebody. The solution for those fraudulent activity is to have software that can automatically categorize the call as non-legit or legit call and also store important points of conversations in the form of text-based notes for future reference, along with the audio recordings. The solution, therefore, is an ML based mobile application that can run in the background, record the call and analyse the conversations by using the ML algorithms to build the classification model. The classification obtained would then lead to the call being categorized as legit/spam/threat/blackmail and all the conversations would also be converted and stored as notes along with the audio recording for future reference."
370,excluded,10.1109/icnsc.2017.8000185,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8000185/,2017-05-18 00:00:00,healthmate: smart wearable system for health monitoring (swshm),"For the recent few years, Smart Wearable Systems (SWS) have been one of the most productive areas that can be considered for clinical utilization. They are an end-to-end connected source and sink (aggregator) nodes implemented to be in communication together either locally or remotely. However, a number of these sensors can be integrated on the Wireless Body Area Networks (WBAN), which is a new method for health monitoring. In particular, technology has become a trend that affect the patients' needs for requesting an innovation, low cost and flexible system in order to be aware in the change of their physiological and non-physiological body parameters. To provide this demand, HealthMate system is proposed as a solution for real-time health monitoring system that is based on Smart Wearable System for Health Monitoring (SWSHM). This paper is aimed at providing a health monitoring system for patients to measure their body parameters through e-Health sensors via smart phone and enabling doctors to check the health status of their patients remotely. Our solution allows patients to measure their body physiological and non-physiological parameters using body temperature, electrocardiogram, pulse and oxygen in blood, airflow and body position sensors. In addition, by using internet technologies, XBee wireless (802.15.4), and SMS to transmit measured patients body parameters to either mobile application or database on a cloud server, where artificial intelligence is accomplished to examine the data in order to notify health professions in emergency case using SMS services or e-mail."
371,unknown,10.1109/cib.2009.4925689,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/4925689/,2009-04-02 00:00:00,a facial presence monitoring system for information security,"Information security requires a method to establish digital credentials that can reliably identify individual users. Since biometrics is concerned with the measurements of unique human physiological or behavioural characteristics, the technology has been used to verify the identity of computer or network users. Given today's heightened security requirements of military as well as other applications such as banking, health care, etc., it is becoming critical to be able to monitor the presence of the authenticated user throughout a session. This paper presents a prototype system that uses facial recognition technology to monitor the authenticated user. The objective is to ensure that the user who is using the computer is the same person that logged onto the system. A neural network-based algorithm is implemented to carry out face detection, and an eigenface method is employed to perform facial recognition. A graphical user interface (GUI) has been developed which allows the performance of face detection and facial recognition to be monitored at run time. The experimental results demonstrate the feasibility of near-real-time continuous user verification for high-level security information systems."
372,excluded,10.1109/aero47225.2020.9172804,2020 IEEE Aerospace Conference,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/f92d3e1083474924db40e42f2ca4422fa5e4c212,2020-01-01 00:00:00,autonomous search for underground mine rescue using aerial robots,"In this paper we present a comprehensive solution for autonomous underground mine rescue using aerial robots. In particular, a new class of Micro Aerial Vehicles are equipped with the ability to localize and map in subterranean settings, explore unknown mine environments on their own, and perform detection and localization of objects of interest for the purposes of mine rescue (i.e., “human survivors” and associated objects such as “backpacks”, “smartphones” or “tools”). For the purposes of GPS-denied localization and mapping in the visually-degraded underground environments (e.g., a smoke-filled mine during an accident) the solution relies on the fusion of LiDAR data with thermal vision frames and inertial cues. Autonomous exploration is enabled through a graph-based search algorithm and an online volumetric representation of the environment. Object search is then enabled through a deep learning-based classifier, while the associated location is queried using the online reconstructed map. The complete software framework runs onboard the aerial robots utilizing the integrated embedded processing resources. The overall system is extensively evaluated in real-life deployments in underground mines."
373,excluded,10.1109/ojits.2021.3132725,IEEE Open Journal of Intelligent Transportation Systems,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/ba1f7d6b7ba0a3aa7ed1f14cd8012c3359c4e408,2021-01-01 00:00:00,conceptualisation of human-on-the-loop haptic teleoperation with fully autonomous self-driving vehicles in the urban environment,"The automotive industry aims to deploy commercial level-5 fully autonomous self-driving vehicles (FA-SDVs) in a diverse range of benefit-driven concepts on city roads in the years to come. In all future visions of operating networks of FA-SDVs, humans are expected to intervene with some kind of remote supervisory role. Recent advances in cyber-physical systems (CPS) within the concept of Internet of Everything (IoE) using tactile Internet (TI) teleport us to teleoperate remote objects within the cyber-world. Human-on-the-loop (HOTL) haptic teleoperation with an extension of human control and sensing capability by coupling with artificial sensors and actuators with an increased sense of real-time driving in the remote vehicle can help overcome the challenging tasks when the new driver — artificial intelligence (AI) agent — encounters an unorthodox situation that can’t be addressed by the autonomous capabilities. This paper analyses HOTL real-time haptic delay-sensitive teleoperation with FA-SDVs, in the aspects of human-vehicle teamwork by establishing two similar remote parallel worlds — real-world vehicle time-varying environment and cyber-world emulation of this environment, i.e., digital twins (DTs) — in which a human telesupervisor (HTS), as a biological agent, can be immersed with no cybersickness enabling omnipresence through a timely bidirectional flow of energy and information. The experiments conducted as a proof of concept of HOTL haptic teleoperation regarding learning with human-vehicle collaboration show promising results and the potential of benefiting from the proposed framework."
374,included,10.1038/s41591-022-01895-z,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s41591-022-01895-z,2022-07-01 00:00:00,factors driving provider adoption of the trews machine learning-based early warning system and its effects on sepsis treatment timing,"Machine learning-based clinical decision support tools for sepsis create opportunities to identify at-risk patients and initiate treatments at early time points, which is critical for improving sepsis outcomes. In view of the increasing use of such systems, better understanding of how they are adopted and used by healthcare providers is needed. Here, we analyzed provider interactions with a sepsis early detection tool (Targeted Real-time Early Warning System), which was deployed at five hospitals over a 2-year period. Among 9,805 retrospectively identified sepsis cases, the early detection tool achieved high sensitivity (82% of sepsis cases were identified) and a high rate of adoption: 89% of all alerts by the system were evaluated by a physician or advanced practice provider and 38% of evaluated alerts were confirmed by a provider. Adjusting for patient presentation and severity, patients with sepsis whose alert was confirmed by a provider within 3 h had a 1.85-h (95% CI 1.66–2.00) reduction in median time to first antibiotic order compared to patients with sepsis whose alert was either dismissed, confirmed more than 3 h after the alert or never addressed in the system. Finally, we found that emergency department providers and providers who had previous interactions with an alert were more likely to interact with alerts, as well as to confirm alerts on retrospectively identified patients with sepsis. Beyond efforts to improve the performance of early warning systems, efforts to improve adoption are essential to their clinical impact and should focus on understanding providers’ knowledge of, experience with and attitudes toward such systems. Prospective evaluation of a machine learning-based early warning system for sepsis, deployed at five hospitals, showed that healthcare providers interacted with the system at a high rate and that this interaction was associated with faster antibiotic ordering."
375,unknown,10.1007/978-3-319-23461-8_7,Springer,springer,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-319-23461-8_7,2015-01-01 00:00:00,online analysis of high-volume data streams in astroparticle physics,"Experiments in high-energy astroparticle physics produce large amounts of data as continuous high-volume streams. Gaining insights from the observed data poses a number of challenges to data analysis at various steps in the analysis chain of the experiments. Machine learning methods have already cleaved their way selectively at some particular stages of the overall data mangling process. In this paper we investigate the deployment of machine learning methods at various stages of the data analysis chain in a gamma-ray astronomy experiment. Aiming at online and real-time performance, we build up on prominent software libraries and discuss the complete cycle of data processing from raw-data capturing to high-level classification using a data-flow based rapid-prototyping environment. In the context of a gamma-ray experiment, we review user requirements in this interdisciplinary setting and demonstrate the applicability of our approach in a real-world setting to provide results from high-volume data streams in real-time performance."
376,excluded,10.1016/j.future.2018.02.011,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85042391186,2019-03-01,collaborative prognostics in social asset networks,"With the spread of Internet of Things (IoT) technologies, assets have acquired communication, processing and sensing capabilities. In response, the field of Asset Management has moved from fleet-wide failure models to individualised asset prognostics. Individualised models are seldom truly distributed, and often fail to capitalise the processing power of the asset fleet. This leads to hardly scalable machine learning centralised models that often must find a compromise between accuracy and computational power. In order to overcome this, we present a novel theoretical approach to collaborative prognostics within the Social Internet of Things. We introduce the concept of Social Asset Networks, defined as networks of cooperating assets with sensing, communicating and computing capabilities. In the proposed approach, the information obtained from the medium by means of sensors is synthesised into a Health Indicator, which determines the state of the asset. The Health Indicator of each asset evolves according to an equation determined by a triplet of parameters. Assets are given the form of the equation but they ignore their parametric values. To obtain these values, assets use the equation in order to perform a non-linear least squares fit of their Health Indicator data. Using these estimated parameters, they are interconnected to a subset of collaborating assets by means of a similarity metric. We show how by simply interchanging their estimates, networked assets are able to precisely determine their Health Indicator dynamics and reduce maintenance costs. This is done in real time, with no centralised library, and without the need for extensive historical data. We compare Social Asset Networks with the typical self-learning and fleet-wide approaches, and show that Social Asset Networks have a faster convergence and lower cost. This study serves as a conceptual proof for the potential of collaborative prognostics for solving maintenance problems, and can be used to justify the implementation of such a system in a real industrial fleet."
377,excluded,10.1016/j.rcim.2019.101887,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85074770255,2020-06-01,deep learning-based smart task assistance in wearable augmented reality,"Wearable augmented reality (AR) smart glasses have been utilized in various applications such as training, maintenance, and collaboration. However, most previous research on wearable AR technology did not effectively supported situation-aware task assistance because of AR marker-based static visualization and registration. In this study, a smart and user-centric task assistance method is proposed, which combines deep learning-based object detection and instance segmentation with wearable AR technology to provide more effective visual guidance with less cognitive load. In particular, instance segmentation using the Mask R-CNN and markerless AR are combined to overlay the 3D spatial mapping of an actual object onto its surrounding real environment. In addition, 3D spatial information with instance segmentation is used to provide 3D task guidance and navigation, which helps the user to more easily identify and understand physical objects while moving around in the physical environment. Furthermore, 2.5D or 3D replicas support the 3D annotation and collaboration between different workers without predefined 3D models. Therefore, the user can perform more realistic manufacturing tasks in dynamic environments. To verify the usability and usefulness of the proposed method, we performed quantitative and qualitative analyses by conducting two user studies: 1) matching a virtual object to a real object in a real environment, and 2) performing a realistic task, that is, the maintenance and inspection of a 3D printer. We also implemented several viable applications supporting task assistance using the proposed deep learning-based task assistance in wearable AR."
378,excluded,10.1109/bigdata52589.2021.9671660,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9671660/,2021-12-18 00:00:00,a processing architecture for real-time predictive smart city digital twins,"Smart Cities will create many benefits for society. One of the most significant will be real-time, active management of the systems making up a Smart City. Active management will be enabled by processing large amounts of real-time data, by machine learning to deliver timely insights and by large-scale simulation to create effective interventions. To realise active management of Smart Cities requires processing systems that can take in data at large volume and variety, and perform complex processing on these data, all in real-time. Digital twins are used to create a continuous, consistent view of the current state of the city, and simulation of the evolution of digitals twins is used to detect problems arising in the near future and to optimise the impact of interventions. In this paper we propose and validate a Smart City processing architecture that can deliver these benefits by building a digital twin of a city, combining real-time streamed data ingestion, simulation and analytics. Streamed data systems can deliver a real time response with large amount of incoming data but are not suited to time advancing simulations where the interaction between modelled objects is important. We propose a processing architecture that mitigates this limitation by separating object interactions from streamed data processing. Finally, we introduce DEDUCE-PT, a demonstration implementation of this Smart City processing architecture that is aimed at proactive management of public transport systems."
379,unknown,10.1109/coase.2018.8560553,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8560553/,2018-08-24 00:00:00,a cloud-based robust semaphore mirroring system for social robots,"We present a cloud-based human-robot interaction system that automatically controls a humanoid robot to mirror a human demonstrator performing flag semaphores. We use a cloud-based framework called Human Augmented Robotic Intelligence (HARI) to perform gesture recognition of the human demonstrator and gesture control of a local humanoid robot, named Pepper. To ensure that the system is real-time, we design a system to maximize cloud computation contribution to the deep-neural-network-based gesture recognition system, OpenPose, and to minimize communication costs between the cloud and the robot. A hybrid control system is used to hide latency caused by either routing or physical distances. We conducted real-time semaphore mirroring experiments in which both the robots and the demonstrator were located in Tokyo, Japan, whereas the cloud server was deployed in the United States. The total latency was 400ms for the video streaming to the cloud and 108ms for the robot commanding from the cloud. Further, we measured the reliability of our gesture-based semaphore recognition system with two human subjects, and were able to achieve 90% and 76.7% recognition accuracy, respectively, for the two subjects with open-loop when the subjects were not allowed to see the recognition results. We could achieve 100% recognition accuracy when both subjects were allowed to adapt to the recognition system under a closed-loop setting. Lastly, we showed that we can support two humanoid robots with a single server at the same time. With this real-time cloud-based HRI system, we illustrate that we can deploy gesture-based human-robot globally and at scale."
380,excluded,10.1109/aims52415.2021.9466068,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9466068/,2021-04-30 00:00:00,implementation of cloud based action recognition backend platform,"The Internet of Things (IoT) growth are rapidly in various fields such as industry 4.0, smart cities, and smart homes. Implementation of IoT for electronic assistance had been researched to increase the longevity of human life. However, not all IoT implementation as human life assistance provides action recognition monitoring on multiple elderly people, provide information such as real-time action monitoring, and real-time streaming in a mobile application. Therefore, this research intends to create a system that can receive and provide information on each elderly people who registered. The Action Recognition Backend Platform will be working as cloud computing to receive and manage input data from Edge Computing Action Recognition. This platform integrated Deep Learning, Data Analytics, Big Data Warehouse that implemented Extract, Transform, and Load (ETL) methods, communication services with MQTT, and Kafka Streaming Processor. The test result showed that the edge computing action recognition got better model accuracy performance from our last model [1], which can predict with 50,7% accuracy in 0.5 confidence threshold. Moreover, the backend platform had been successfully implemented a simple IoT paradigm and got an average delivery time of MQTT communication at 204ms, for streaming data process took an average delay of 680ms."
381,unknown,10.1109/ice2t.2017.8215979,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8215979/,2017-09-20 00:00:00,application framework for forest surveillance and data acquisition using unmanned aerial vehicle system,"An application framework is proposed in this paper that considers low cost surveillance mechanism and data acquisition in the forest. An application is developed as proof of concept with detailed design that can take advantage of unmanned urban vehicle to be directly configured and controlled in real-time. The advantages are numerous; it can be used for many purposes. For example, it can be used for observing critical and important area for intruder activities or to know the current state of any object of interest. We considered using machine learning and image processing and can be used for species of trees in the forest by color and size detection. A separate service running on separate remote server will be responsible for this. We have proposed a application framework particularly to be cheap and targeted to be easy to handle by non-technical persons and that it does not require large software system knowledge like Pix4D or DroneDeploy. This system will be useful for operations and research specially the forestry and palm oil plantation surveillance, and sustainable timber industry that specially needs carefully collected imageries and data from objects. Collection of raw data from sensor networks is also proposed in the system architecture."
382,included,10.1186/s40537-020-00303-y,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1186/s40537-020-00303-y,2020-04-06 00:00:00,extending reference architecture of big data systems towards machine learning in edge computing environments,"Background Augmented reality, computer vision and other (e.g. network functions, Internet-of-Things (IoT)) use cases can be realised in edge computing environments with machine learning (ML) techniques. For realisation of the use cases, it has to be understood how data is collected, stored, processed, analysed, and visualised in big data systems. In order to provide services with low latency for end users, often utilisation of ML techniques has to be optimized. Also, software/service developers have to understand, how to develop and deploy ML models in edge computing environments. Therefore, architecture design of big data systems to edge computing environments may be challenging. Findings The contribution of this paper is reference architecture (RA) design of a big data system utilising ML techniques in edge computing environments. An earlier version of the RA has been extended based on 16 realised implementation architectures, which have been developed to edge/distributed computing environments. Also, deployment of architectural elements in different environments is described. Finally, a system view is provided of the software engineering aspects of ML model development and deployment. Conclusions The presented RA may facilitate concrete architecture design of use cases in edge computing environments. The value of RAs is reduction of development and maintenance costs of systems, reduction of risks, and facilitation of communication between different stakeholders."
383,unknown,10.1109/urtc49097.2019.9660498,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9660498/,2019-10-13 00:00:00,a vision-based hygiene monitoring system: using deep learning to assess handwashing procedure in real time,"In a healthcare environment, workers must follow proper handwashing protocol to prevent the transmission of pathogens. The World Health Organization (WHO) recommends six handwashing motions to ensure that all parts of the hands are cleaned properly. This project aimed to apply deep learning to identify whether a subject is correctly performing handwashing motions in compliance with the WHO protocol. Video footage from ten subjects was used to train a ResNet-18 neural network capable of identifying the six different hand poses. All training was done using the Amazon Web Service (AWS) cloud platform. The resulting model was deployed to an AWS DeepLens camera to produce real time evaluations of live handwashing footage. The system consistently identified the correct handwashing motions with an 88.5&#x0025; average accuracy and displayed the live inferences on a connected monitor instantaneously. The results demonstrated the practicality of using deep learning to develop a low-cost vision-based system for monitoring handwashing technique in real time."
384,unknown,10.1016/j.smhl.2021.100249,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85120868737,2022-03-01,a pilot study towards a smart-health framework to collect and analyze biomarkers with low-cost and flexible wearables,"Artificial intelligence-enabled applications on edge devices have the potential to revolutionize disease detection and monitoring with smart health (sHealth) applications. The major challenges are user-friendly and flexible sensors for seamless collection of physiological data for long hours, online and on-device processing of sensitive medical data to facilitate privacy protection, reliable extraction of disease-related biomarkers, and implementation of lightweight artificially intelligent algorithms for inference at the edge without degrading the system performance. In this pilot project, we conducted a yearlong field study with 9 participants conducting 480 data collection sessions in the “living lab” environment. We used smartphones as the edge computing device and implemented pre-trained machine learning algorithms in the smartphone app for computing disease-related Events-of-Interest (EoI). We considered real-time data processing on the smartphone itself without sharing raw data with the cloud or any other computing facility to minimize privacy concerns, and network bandwidth requirements. We used a commercial smart band and a custom-designed zero-power inkjet-printed sensor for physiological sensing and capturing health biomarkers such as heart rate variability (HRV) and core body temperature. The extracted HRV feature values are within the 95% confidence interval of normative values. On top of that, the extracted HRV shows some informative trends i.e. hammock pattern for healthy subjects which may be helpful in subsequent research studies. Moreover, we used core body temperature with user-reported outcomes for estimating flu-related symptoms severity and visualizing the spatiotemporal trend in a cloud-server to facilitate personalized as well as community-wide health monitoring. Inference at the edge provided a data reduction of 3 order while the runtime latency, power consumption, memory requirement, and storage size of the smartphone app were 500 ms, 51.90 mAH, 9.4 MB, and 2.4 MB, respectively. Our developed framework of sHealth enables automated community-wide monitoring of symptoms severity in addition to personalized monitoring which paves the way for early monitoring of a disease outbreak for a smart and connected community."
385,excluded,http://arxiv.org/abs/2011.09359v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2011.09359v1,2020-11-18 00:00:00,flaas: federated learning as a service,"Federated Learning (FL) is emerging as a promising technology to build
machine learning models in a decentralized, privacy-preserving fashion. Indeed,
FL enables local training on user devices, avoiding user data to be transferred
to centralized servers, and can be enhanced with differential privacy
mechanisms. Although FL has been recently deployed in real systems, the
possibility of collaborative modeling across different 3rd-party applications
has not yet been explored. In this paper, we tackle this problem and present
Federated Learning as a Service (FLaaS), a system enabling different scenarios
of 3rd-party application collaborative model building and addressing the
consequent challenges of permission and privacy management, usability, and
hierarchical model training. FLaaS can be deployed in different operational
environments. As a proof of concept, we implement it on a mobile phone setting
and discuss practical implications of results on simulated and real devices
with respect to on-device training CPU cost, memory footprint and power
consumed per FL model round. Therefore, we demonstrate FLaaS's feasibility in
building unique or joint FL models across applications for image object
detection in a few hours, across 100 devices."
386,unknown,10.1089/big.2014.0064,Big Data,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/f8cd9e3da23d01ee7487be29ca251b6ad20889af,2016-01-01 00:00:00,combining human computing and machine learning to make sense of big (aerial) data for disaster response,"Aerial imagery captured via unmanned aerial vehicles (UAVs) is playing an increasingly important role in disaster response. Unlike satellite imagery, aerial imagery can be captured and processed within hours rather than days. In addition, the spatial resolution of aerial imagery is an order of magnitude higher than the imagery produced by the most sophisticated commercial satellites today. Both the United States Federal Emergency Management Agency (FEMA) and the European Commission's Joint Research Center (JRC) have noted that aerial imagery will inevitably present a big data challenge. The purpose of this article is to get ahead of this future challenge by proposing a hybrid crowdsourcing and real-time machine learning solution to rapidly process large volumes of aerial data for disaster response in a time-sensitive manner. Crowdsourcing can be used to annotate features of interest in aerial images (such as damaged shelters and roads blocked by debris). These human-annotated features can then be used to train a supervised machine learning system to learn to recognize such features in new unseen images. In this article, we describe how this hybrid solution for image analysis can be implemented as a module (i.e., Aerial Clicker) to extend an existing platform called Artificial Intelligence for Disaster Response (AIDR), which has already been deployed to classify microblog messages during disasters using its Text Clicker module and in response to Cyclone Pam, a category 5 cyclone that devastated Vanuatu in March 2015. The hybrid solution we present can be applied to both aerial and satellite imagery and has applications beyond disaster response such as wildlife protection, human rights, and archeological exploration. As a proof of concept, we recently piloted this solution using very high-resolution aerial photographs of a wildlife reserve in Namibia to support rangers with their wildlife conservation efforts (SAVMAP project, http://lasig.epfl.ch/savmap ). The results suggest that the platform we have developed to combine crowdsourcing and machine learning to make sense of large volumes of aerial images can be used for disaster response."
387,unknown,10.1109/sii.2019.8700415,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8700415/,2019-01-16 00:00:00,development of wearable gait assistive device using recurrent neural network,"In elderly population, gait disorders are common where majority of these disorders are associated as symptoms of neurodegenerative diseases including Parkinson's Disease (PD), Huntingtons Disease (HD), and Amyotrophic Lateral Sclerosis (ALS). In addition to affected mobility, the patients are also susceptible to greater risk of falls, hence increasing the demand for caretakers. With the trend of aging population, personal assistive device could be deployed to assist patients to regain independence and improve their quality of life. This paper proposes an end-to-end solution architecture for real-time standalone wearable gait assistive device to automate the rehabilitation activity. A key aspect of this study is to incorporate recurrent neural network (RNN) model that provides accurate pattern recognition and output actuation cue to the patients. Prototype and simulation data was used to show the feasibility of the proposed architecture and machine learning model. Preliminary results indicate favorable accuracy gait cycle detection for implementation. However, further optimizations are required to lower the computational costs and shorten the time lag between cycles to ensure low cost feasibility of the device."
388,unknown,10.1016/j.compag.2021.106644,scopus,sciencedirect,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85121931762,2022-02-01,ric-net: a plant disease classification model based on the fusion of inception and residual structure and embedded attention mechanism,"In this paper, we proposed a convolutional neural network based on Inception and residual structure with an embedded modified convolutional block attention module (CBAM), aiming to improve the classification of plant leaf diseases. Corn, potatoes and tomatoes are the most cultivated grains in southern China. The leaves of the three crops are very fragile and sensitive and are susceptible to leaf diseases, such as leaf blight of corn, late blight of potato and mosaic virus of tomato. These diseases cannot be identified at early stages. Therefore, an efficient solution is proposed by deep learning techniques to detect the disease categories of crops, which can effectively prevent the spread of diseases and ensure the normal growth of plants. In this experiment, our model achieved an overall accuracy of 99.55% for the identification of the three diseases of corn, potato and tomato. In addition, we tested the three plants individually. The classification accuracy of our model on corn, potato and tomato was 98.44%, 99.43% and 95.20%, respectively. We have also developed a web-based real-time plant disease classification system and deployed our model. The system had good performance in time and accuracy evaluation metrics. The results of this study showed that our model had fewer parameters, shorter training time, and higher recognition accuracy compared to existing image classification models."
389,unknown,10.1109/ivs.2014.6856600,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6856600/,2014-06-11 00:00:00,looking-in and looking-out vision for urban intelligent assistance: estimation of driver attentive state and dynamic surround for safe merging and braking,"This paper details the research, development, and demonstrations of real-world systems intended to assist the driver in urban environments, as part of the Urban Intelligent Assist (UIA) research initiative. A 3-year collaboration between Audi AG, Volkswagen Group of America Electronics Research Laboratory, and UC San Diego, the driver assistance portion of the UIA project focuses on two main use cases of vital importance in urban driving. The first, Driver Attention Guard, applies novel computer vision and machine learning research for accurately tracking the driver's head position and rotation using an array of cameras. The system then infers the driver's focus of attention, alerting the driver and engaging safety systems in case of extended driver inattention. The second application, Merge and Lane Change Assist, applies a novel probabilistic compact representation of the on-road environment, fusing data from a variety of sensor modalities. The system then computes safe and low-cost merge and lane-change maneuver recommendations. It communicates desired speeds to the driver via Head-up Display, when the driver touches the blinker, indicating his desired lane. The fully-implemented systems, complete with HMI, were demonstrated to the public and press in San Francisco in January of 2014."
390,unknown,10.1109/percomworkshops51409.2021.9431009,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9431009/,2021-03-26 00:00:00,architecture and pervasive platform for machine learning services in industry 4.0,"Pervasive computing promotes the integration of smart electronic devices in our living and working spaces in order to provide new, advanced services. Recently many prototype services based on machine learning techniques have been proposed in a number of domains like smart homes, smart buildings or smart plants. However, the number of applications effectively deployed in the real world is still limited. We believe that architectural principles and integrated frameworks are still missing today to successfully and repetitively support application developers and operators. In this paper, we present a novel architecture and a pervasive platform allowing the development of machine learning based applications in smart buildings."
391,unknown,10.1016/j.tra.2022.05.024,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85131413326,2022-08-01,a new curb lane monitoring and illegal parking impact estimation approach based on queueing theory and computer vision for cameras with low resolution and low frame rate,"The rapid development of the internet of things (IoT), sensing technologies, and machine learning and deep learning techniques, along with the growing variety and volume of data, have yielded new perspectives on how novel technologies can be applied to obtain new sources of curb data to achieve cost-effective curb management. This study presents a new computer vision–based data acquisition and analytics approach for curb lane monitoring and illegal parking impact assessment. The proposed “rank, detect, and quantify impacts” system consists of three main modules: 1) hotspot identification based on rankings generated by local spatial autocorrelation analysis, 2) curb lane occupancy estimation leveraging traffic cameras and computer vision techniques, and 3) illegal parking traffic impact quantification using an M/M/∞ queueing model. To demonstrate the feasibility and validity of the proposed approach, it was tested and empirically validated using field data collected from three case study sites in Midtown Manhattan, New York City (NYC)—one of the most complex urban transportation networks in the world. Different types of curb lane occupancy, including parking and bus lanes, and different frequencies of illegal parking (high, moderate, low) were investigated. The proposed method was proven to be effective even for low resolution and discontinuous video feeds obtained from publicly available traffic cameras. All three case study sites achieved good detection accuracy (86% to 96%) for parking and bus lane occupancy, and acceptable precision and recall in detecting illegal parking events. The queueing model was also proven to effectively quantify link travel time with the appearance of illegal parking events of different frequencies. The proposed “rank, detect, and quantify impacts” system is friendly for large-scale real-time implementation and is highly scalable to help evaluate the impact of other modes such as bike or mobility-on-demand (MOD) services. It can also be easily adopted by other cities to provide transportation agencies with effective data collection and innovative curb space management strategies."
392,excluded,10.2523/iptc-20111-ms,,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/3d0e8c86b49bb4674e248ace106b4e943019edba,2020-01-01 00:00:00,smart autopilot drone system for surface surveillance and anomaly detection via customizable deep neural network,"
 Copter-based unmanned aerial vehicle (drone) systems are being utilized for surveillance, inspection and security purposes for well sites, gathering centers, pipelines, refineries, and other surface facilities. However, most of the practices largely rely on humans, including drone operation, data transfer, image analysis, etc. In this paper, we present a comprehensive, cloud-enabled, human-free autopilot drone system and its application in field surveillance and anomaly detection powered by customizable deep neural network and computer vision models.
 The proposed system consists of customized quadcopter drones equipped with high-definition cameras, thermal imaging and gas sensing devices, autopiloted by cloud-connected onboard computers. A series of advanced algorithms are developed and deployed onboard and over the Cloud for processing and diagnosing the image/thermal/gas sensing data collected by the drones in real-time or near real-time, including accurate 2D geospatial aerial mapping, anomaly detection and classification for events like oil leak, gas leak, facility failure, human activities, etc. Object detection deep learning models are customized and parallelized for low-profile multi-core single board computers.
 In our case study, a pre-configured drone flew along the same path twice at a 6-month gap. A robust, iterative image registering algorithm is developed to precisely align and overlay images taken at different days at the same or similar GPS locations, even with significant changes to the environment due to season shift, human activities, camera angles or height variations. Local changes are filtered and selected based on their sizes and magnitudes in the residual images by subtracting pairs of perfectly overlaid scenes. Pre-trained Residual Convolutional Neural network (He et al. 2015) is rapidly re-trained to further classify the type of changes using the techniques of transfer learning and data augmentation. An ROC of 99% was achieved in the multi-task binary classification, wherein the detected changes are divided into positive anomalies (such as oil/gas leak, facility failures, unauthored human activities) and negative (natural/insignificant) signals. Comparing against a support vector machine baseline with a ROC=92%, the ResNet model demonstrates significant, more promising detection accuracy at a faster training time.
 This innovative integrated platform is presented that combines physical drone, onboard imaging/sensing devices, cloud connectivity, onboard and back-end control system, deep learning and computer vision architecture for situational awareness of oil & gas fields and the mining industry. It achieves full automation of mass surveillance, data acquisition and storage, diagnostics and asset situational understanding. The system architecture, especially the onboard and cloud computation engines, can be readily transferred and applied to other common drone platforms."
393,excluded,10.1109/isncc49221.2020.9297341,"2020 International Symposium on Networks, Computers and Communications (ISNCC)",semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e72af567423d6b8e1ba5e9d905f9986106492080,2020-01-01 00:00:00,an iot based smart farming system using machine learning,"Smart farming allows to analyze the growth of plants and to influence the parameters of our system in real time in order to optimize plant growth and support the farmer in his activity. Internet of Things (IoT) arrangements, based on the application particular sensors data measurements and intelligent processing, are bridging the holes between the cyber and physical worlds. In this paper, we propose the design and the experiment of a smart farming system based on an intelligent platform which enables prediction capabilities using artificial intelligence (AI) techniques. This system is based on the technology of wireless sensor networks and its implementation requires three main phases, i) data collection phase using sensors deployed in an agricultural field, ii) data cleaning and storage phase, and iii) predictive processing using some AI methods."
394,excluded,10.1109/honet.2019.8907997,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8907997/,2019-10-09 00:00:00,ar-iomt mental health rehabilitation applications for smart cities,"Augmented Reality (AR) applications provide major opportunities for healthcare. Mental health rehabilitation in particular is an area that stands to gain from innovations in this field. Debilitating medical conditions related to traumatic brain injuries, aging, and drug abuse, can take advantage from the benefits that these applications provide. Coupled with IoMT (Internet of Medical Things) sensors and machine learning, effective system implementations that predict mental rehabilitation outcomes can be built. This can help in administering better treatment protocols. The collective use of these technologies for therapeutic approaches to alter patient behaviour at a physiological level falls in the emerging field of Digital Therapeutics (DT). In this paper, the current state of AR for mental health rehabilitation is described and used to motivate the implementation of a proof of concept mental DT rehabilitation system developed by us. Similar AR-IoMT systems can reduce inpatient counts in smart cities where an increase in aging populations is already a concern, and existing healthcare resources may be clogged up due to minor inpatient health checkups. In addition, the paper discusses the deployment challenges of the implemented system, which include the lack and need thereof of a standardized protocol for IoMT data collection and analysis for cross interoperability across different healthcare providers and vendors."
395,excluded,10.1186/s13677-021-00250-w,Springer,springer,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1186/s13677-021-00250-w,2021-07-02 00:00:00,an edge-cloud collaborative computing platform for building aiot applications efficiently,"The convergence of Artificial Intelligence (AI) and the Internet of Things (IoT), or AIoT, has breathed a new life into IoT operations and human-machine interactions. Currently, resource-constrained IoT devices usually cannot provide sufficient capability for data storage and processing so as to support building modern AI models. An intuitive solution is to integrate cloud computing technology into AIoT and exploit the powerful and elastic computing as well as the storage capacity of the servers on the cloud end. Nevertheless, the network bandwidth and communication latency increasingly become serious bottlenecks. The emerging edge computing can complement the cloud-based AIoT in terms of communication latency, and hence attracts more and more attention from the AIoT area. In this paper, we present an industrial edge-cloud collaborative computing platform, namely Sophon Edge , that helps to build and deploy AIoT applications efficiently. As an enterprise-level solution for the AIoT computing paradigm, Sophon Edge adopts a pipeline-based computing model for streaming data from IoT devices. Besides, this platform supports an iterative way for model evolution and updating so as to enable the AIoT applications agile and data-driven. Through a real-world example, we demonstrate the effectiveness and efficiency of building an AIoT application based on the Sophon Edge platform."
396,excluded,10.1016/s0921-8890(98)00005-0,scopus,sciencedirect,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/0032180335,1998-10-31,a robust landmark-based system for vehicle location using low-bandwidth vision,"This paper presents novel computer algorithms, a system architecture, and the prototype implementation of a vision-based automatic vehicle location system. The objective of the vehicle location system is to keep track of the vehicle location for a human driver, and perhaps to provide the driver with real-time audio directions to his destination. The techniques developed here are equally applicable to autonomous robot navigation. The prototype system uses odometer readings and a skeleton map to perform dead reckoning, and uses low-bandwidth visual information and neural networks to recognize places for correcting cumulative dead reckoning errors. The visual information is also used to detect turns, for dead reckoning at intersections. The system is self-contained in the sense that it requires no infrastructure outside the vehicle, such as external beacons installed on roadways or satellites used by Global Positioning Systems (GPS). The system maintains a large number of location hypotheses and searches for a large number of landmarks stored in a database in real time. Hence the system is robustly able to recover the vehicle location after being lost for various reasons. The system has been tested, with success, in both day and night time, in all four seasons, and on roads in New York City, a regional highway, and on suburban streets."
397,unknown,10.1016/j.cmpb.2022.106655,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85124421101,2022-04-01,a real-time integrated framework to support clinical decision making for covid-19 patients,"Background
                  The COVID-19 pandemic affected healthcare systems worldwide. Predictive models developed by Artificial Intelligence (AI) and based on timely, centralized and standardized real world patient data could improve management of COVID-19 to achieve better clinical outcomes. The objectives of this manuscript are to describe the structure and technologies used to construct a COVID-19 Data Mart architecture and to present how a large hospital has tackled the challenge of supporting daily management of COVID-19 pandemic emergency, by creating a strong retrospective knowledge base, a real time environment and integrated information dashboard for daily practice and early identification of critical condition at patient level. This framework is also used as an informative, continuously enriched data lake, which is a base for several on-going predictive studies.
               
                  Methods
                  The information technology framework for clinical practice and research was described. It was developed using SAS Institute software analytics tool and SAS® Vyia® environment and Open-Source environment R ® and Python ® for fast prototyping and modeling. The included variables and the source extraction procedures were presented.
               
                  Results
                  The Data Mart covers a retrospective cohort of 5528 patients with SARS-CoV-2 infection. People who died were older, had more comorbidities, reported more frequently dyspnea at onset, had higher d-dimer, C-reactive protein and urea nitrogen. The dashboard was developed to support the management of COVID-19 patients at three levels: hospital, single ward and individual care level.
               
                  Interpretation
                  The COVID-19 Data Mart based on integration of a large collection of clinical data and an AI-based integrated framework has been developed, based on a set of automated procedures for data mining and retrieval, transformation and integration, and has been embedded in the clinical practice to help managing daily care. Benefits from the availability of a Data Mart include the opportunity to build predictive models with a machine learning approach to identify undescribed clinical phenotypes and to foster hospital networks. A real-time updated dashboard built from the Data Mart may represent a valid tool for a better knowledge of epidemiological and clinical features of COVID-19, especially when multiple waves are observed, as well as for epidemic and pandemic events of the same nature (e. g. with critical clinical conditions leading to severe pulmonary inflammation). Therefore, we believe the approach presented in this paper may find several applications in comparable situations even at region or state levels. Finally, models predicting the course of future waves or new pandemics could largely benefit from network of DataMarts."
398,unknown,10.1109/tc.2020.3033627,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9239888/,2021-11-01 00:00:00,revealing dram operating guardbands through workload-aware error predictive modeling,"Improving the energy efficiency of DRAMs becomes very challenging due to the growing demand for storage capacity and failures induced by the manufacturing process. To protect against failures, vendors adopt conservative margins in the refresh period and supply voltage. Previously, it was shown that these margins are too pessimistic and will become impractical due to high-power costs, especially in future DRAM technologies. In this article, we present a new technique for automatic scaling the DRAM refresh period under reduced supply voltage that minimizes the probability of failures. The main idea behind the proposed approach is that DRAM error behavior is workload-dependent and can be predicted based on particular program inherent features. We use a Machine Learning (ML) method to build a workload-aware DRAM error behavior model based on the program features which we extract from real workloads during our DRAM error characterization campaign. With such a model, we identify the marginal value of the DRAM refresh period under relaxed voltage for each DRAM module of a server that enable us to reduce the DRAM power. We implement a temperature-driven OS governor which automatically sets the module-specific marginal DRAM parameters discovered by the ML model. Our governor reduces the DRAM power by 24 percent on average while minimizing the probability of failures. Unlike previous studies, our technique: i) does not require intrusive changes to hardware; ii) is implemented on a real server; iii) uses a mechanism that prevents any abnormal DRAM error behavior; and iv) can be easily deployed in data centers."
399,excluded,10.1007/s13278-021-00853-w,'Springer Science and Business Media LLC',core,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),,2022-01-01 00:00:00,social media analytics system for action inspection on social networks,"Social networks are increasingly used for discussing all kinds of topics, including those related to politics, serving as a virtual arena. Consequently, analysing online conversations, for example, to predict election outcomes, is becoming a popular and challenging research area. On social networking sites, citizens express themselves spontaneously regarding political topics, often driven by specific events in social life. Real-time analysis of social media can provide valuable feedback and insights to both politicians and news agencies. In this paper, we discuss the design and implementation of a system for tracking and analysing social media. The SocMINT system provides an easy-to-use, visual dashboard to monitor the discussion on specific topics, to capture trends in communities and, by iteratively applying multidimensional data analysis and filtering, to deeply analyse posts and influencers. SocMINT aggregates data from multiple social sources and performs sentiment analysis on textual, visual and mixed content via a specifically designed neural network architecture. The system was applied in a real context by administrative staff of a political party to effectively analyse candidates\u2019 political communication on Facebook, Instagram and Twitter and the related online community reactions and discussion. In the paper, we report on this real-world case study, showing how the system meaningfully captures trends in public opinion, comparing the main KPIs provided by SocMINT with the outcomes of traditional polls"
400,unknown,http://arxiv.org/abs/1911.04469v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1911.04469v1,2019-11-09 00:00:00,"a proposed artificial intelligence model for real-time human action
  localization and tracking","In recent years, artificial intelligence (AI) based on deep learning (DL) has
sparked tremendous global interest. DL is widely used today and has expanded
into various interesting areas. It is becoming more popular in cross-subject
research, such as studies of smart city systems, which combine computer science
with engineering applications. Human action detection is one of these areas.
Human action detection is an interesting challenge due to its stringent
requirements in terms of computing speed and accuracy. High-accuracy real-time
object tracking is also considered a significant challenge. This paper
integrates the YOLO detection network, which is considered a state-of-the-art
tool for real-time object detection, with motion vectors and the Coyote
Optimization Algorithm (COA) to construct a real-time human action localization
and tracking system. The proposed system starts with the extraction of motion
information from a compressed video stream and the extraction of appearance
information from RGB frames using an object detector. Then, a fusion step
between the two streams is performed, and the results are fed into the proposed
action tracking model. The COA is used in object tracking due to its accuracy
and fast convergence. The basic foundation of the proposed model is the
utilization of motion vectors, which already exist in a compressed video bit
stream and provide sufficient information to improve the localization of the
target action without requiring high consumption of computational resources
compared with other popular methods of extracting motion information, such as
optical flows. This advantage allows the proposed approach to be implemented in
challenging environments where the computational resources are limited, such as
Internet of Things (IoT) systems."
401,unknown,10.1109/access.2022.3194264,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9841554/,2022-01-01 00:00:00,"an architecture combining blockchain, docker and cloud storage for improving digital processes in cloud manufacturing","The Blockchain has been given great attention in recent literature among emerging technologies in software architectures. More specifically, when verifiable transactions between untrusted parties are concerned in a safe and reliable environment, its peculiar decentralized and tamper-proof structure makes it suitable for a vast class of business domains, such as Cloud Manufacturing, which is a new paradigm in the industry based on cloud technologies. However, the stiffness of existing solutions, that are unable to provide and implement heterogeneous services in a Cloud environment, emphasizes the need of a standard framework to overcome this limit and improve collaboration. Firstly, this paper introduces a Blockchain based platform designed with Smart Contracts for improving digital processes in a manufacturing environment. The primary contribution is the integration of two popular cloud technologies within the Blockchain: Docker, a scalable platform to run applications in lightweight environments, and Cloud Storage. Each process available in the platform requires input files and produces output files by using cloud storage as a repository and it is delivered by the owner as a self-contained Docker image, whose digest is safely stored in the chain. Secondly, with the purpose of selecting the fastest node for each new process instance required by consumers, we introduce a task assignment problem based on a deep learning approach and past metrics. The proposed platform is applied to a real-world industrial case study regarding ophthalmic lenses manufacturing and the optimization of lens surface calculation."
402,excluded,10.18127/j19998465-202104-05,Science Intensive Technologies,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/978d5a43d2d85bfc41776593147588193f3472ce,2021-01-01 00:00:00,cyberphysical system for controlling the movement of unmanned vehicles,"The active development of unmanned vehicles, in control systems of which most of the currently created artificial intelligence technologies are used, from machine vision systems to decision-making in multi-criteria control tasks, has led to the emergence of such vehicles on public roads and, in fact, has become an objective reality of ours. life. In such a situation, the person driving needs additional information to drive safely. Currently, the road infrastructure is dynamically developing, forming into a developed distributed telecommunication system with accompanying services. And now it is increasingly associated with the concept of ""intelligent transport system"" (ITS), to which are connected automotive equipment subsystems, wireless communication subsystems, roadside equipment subsystems and a global navigation satellite system module. Unmanned vehicles, which are integrated into the road environment, form a single telecommunication system for controlling the movement of vehicles with it. Wireless technologies and digital models of road infrastructure are important components of such a system. The information technology concept, which implies the integration of computing resources into physical entities, in particular autonomous robots and unmanned vehicles, is called the concept of cyber-physical systems. The computational component in it is distributed throughout the system. The study of the possibility of constructing digital models of roads and road infrastructure with their subsequent intrasystem transfer between interacting objects is undoubtedly of considerable interest. At the research stage of the implementation of such a technology, it is important to analyze the possibility of building and deploying modern ITS, highlight the main problems associated with the visualization of digital ITS models, and propose ways to solve the tasks. As part of the study, an overview of modern wireless technologies and communication standards with the prospect of their application in the infrastructure of the road environment is given, foreign experience of deploying such systems, their features and limitations, is considered. The functional structure of the intelligent transport system is proposed. The results of modeling the road network (creating a digital model of roads) and the practical implementation of software for compiling local maps are presented. The developed software took into account the shortcomings of specialized products on the market, and also implemented the possibility of converting a digital model of roads into the MAPEM format. After preprocessing MAPEM files, they are placed on the server of the intelligent transport system, from where they are then sent via RSU. RSUs start broadcasting this information, and it goes to the unmanned vehicle, which, in turn, processes the received file and waits for a request to move. Upon receipt of such a request, it builds the trajectory of the path and starts moving. The techniques obtained as a result of the research were implemented and applied at the test site."
403,excluded,http://arxiv.org/abs/2106.15021v1,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2106.15021v1,2021-06-21 00:00:00,"how to reach real-time ai on consumer devices? solutions for
  programmable and custom architectures","The unprecedented performance of deep neural networks (DNNs) has led to large
strides in various Artificial Intelligence (AI) inference tasks, such as object
and speech recognition. Nevertheless, deploying such AI models across commodity
devices faces significant challenges: large computational cost, multiple
performance objectives, hardware heterogeneity and a common need for high
accuracy, together pose critical problems to the deployment of DNNs across the
various embedded and mobile devices in the wild. As such, we have yet to
witness the mainstream usage of state-of-the-art deep learning algorithms
across consumer devices. In this paper, we provide preliminary answers to this
potentially game-changing question by presenting an array of design techniques
for efficient AI systems. We start by examining the major roadblocks when
targeting both programmable processors and custom accelerators. Then, we
present diverse methods for achieving real-time performance following a
cross-stack approach. These span model-, system- and hardware-level techniques,
and their combination. Our findings provide illustrative examples of AI systems
that do not overburden mobile hardware, while also indicating how they can
improve inference accuracy. Moreover, we showcase how custom ASIC- and
FPGA-based accelerators can be an enabling factor for next-generation AI
applications, such as multi-DNN systems. Collectively, these results highlight
the critical need for further exploration as to how the various cross-stack
solutions can be best combined in order to bring the latest advances in deep
learning close to users, in a robust and efficient manner."
404,unknown,10.1109/i2ct54291.2022.9825335,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9825335/,2022-04-09 00:00:00,realty scout &#x2013; smart system for real estate analysis &#x0026; forecasting with interactive user interface,"The real estate industry is one of the highest income generating sources in the country. As the country moves toward a highly diversified economy, the role of real estate has become a more important part of the country&#x2019;s economy. However, the state of the local real estate industry is yet to improve and is currently lagging the technology curve. As a result of this issue, useful information is not made available to the end-users. Therefore, the real estate industry needs to improve its adoption of ongoing technologies to move from traditional to smart real estate industry. Therefore, we developed a Smart Real estate system called ""Realty Scout"" which can analyze and forecast real estate information accurately. The ""Realty Scout"" is implemented with a highly interactive view of the properties with a given virtual tour for the users to enhance the user experience. This smart real estate system also collects data on property values, in addition to a trained data set, to forecast future property values. Certain machine learning algorithms are used in the backend to generate future values. An accurate and fast prediction of the real estate value is important to buyers, sellers, and other stakeholders. Furthermore, by gathering users&#x2019; personal information and tracking their search history through the system, the system recommends properties to users based on collected data. As potential users of the system, they can gain an advantage from this feature by finding their desired property without spending more time. In addition, this system aimed to give advanced property filtrations options to the user. Building up a smart system for the real estate industry would be an advantage for all stakeholders who are actively engaged with the real estate industry."
405,unknown,10.1109/iceta48886.2019.9040157,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9040157/,2019-11-22 00:00:00,students’ attendance monitoring through the face recognition,"This paper describes experimental software solution for monitoring of students' attendance through the face detection and recognition in the video. The analysis of existing face detection and recognition algorithms was carried out, and the design of the system solution is described, implementing the previously analyzed algorithms. The Viola-Jones algorithm and HOG are used to detect the face in the video. To recognize an identity of a student, the convolutional neural network (NN) is utilized. The system was tested in the real university environment on selected courses. Once the face recognition is finished, the attendance list of present people may be generated. The system then provides additional visual verification of recognized faces to the lecturer (administrator)."
406,unknown,http://arxiv.org/abs/2001.00269v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2001.00269v2,2020-01-01 00:00:00,"a smart, efficient, and reliable parking surveillance system with edge
  artificial intelligence on iot devices","Cloud computing has been a main-stream computing service for years. Recently,
with the rapid development in urbanization, massive video surveillance data are
produced at an unprecedented speed. A traditional solution to deal with the big
data would require a large amount of computing and storage resources. With the
advances in Internet of things (IoT), artificial intelligence, and
communication technologies, edge computing offers a new solution to the problem
by processing the data partially or wholly on the edge of a surveillance
system. In this study, we investigate the feasibility of using edge computing
for smart parking surveillance tasks, which is a key component of Smart City.
The system processing pipeline is carefully designed with the consideration of
flexibility, online surveillance, data transmission, detection accuracy, and
system reliability. It enables artificial intelligence at the edge by
implementing an enhanced single shot multibox detector (SSD). A few more
algorithms are developed on both the edge and the server targeting optimal
system efficiency and accuracy. Thorough field tests were conducted in the
Angle Lake parking garage for three months. The experimental results are
promising that the final detection method achieves over 95% accuracy in
real-world scenarios with high efficiency and reliability. The proposed smart
parking surveillance system can be a solid foundation for future applications
of intelligent transportation systems."
407,unknown,10.1109/giots49054.2020.9119497,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9119497/,2020-06-03 00:00:00,industrial iot and digital twins for a smart factory : an open source toolkit for application design and benchmarking,"The rapid evolution of digital technology and designed intelligence, such as the Internet of Things (IoT), Big data analytics, Artificial Intelligence (AI), Cyber Physical Systems (CPS), has been a catalyst for the 4th industrial revolution (known as industry 4.0). Among other, the two key state-of-the-art concepts in Industry 4.0, are Industrial IoT (IIoT) and digital twins. IIoT facilitates real-time data acquisition, processing and analytics over large amount of sensor data streams produced by sensors installed within a smart factory, while the ‘digital twin’ concept aims to enable smart factories via the digital replication or representation of physical machines, processes, people in cyber-space. This paper explores the capability of present-state open-source platforms to collectively achieve digital twin capabilities, including IoT real-time data acquisition, virtual representation, analytics, and visualisation. The aim of this work is to ‘close the gap’ between research and implementation, through a collective open source IoT and Digital Twin architecture. The performance of the open-source architecture in this work, is demonstrated in a use-case utilising industry ‘open data’, and is bench-marked with universal testing tools."
408,unknown,10.1109/access.2019.2948160,IEEE Access,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/6d9cd2d863d33b57b9bf435605a4e149d13e6d39,2019-01-01 00:00:00,ml4iot: a framework to orchestrate machine learning workflows on internet of things data,"Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT framework enables the implementation of several types of ML models, each one with a different workflow. These models can be easily configured and used through a simple pipeline. ML4IoT has been designed to use container-based components to enable training and deployment of various ML models in parallel. The results obtained suggest that the proposed framework can manage real-world IoT heterogeneous data by providing elasticity, robustness, and performance."
409,unknown,10.1109/icssit46314.2019.8987926,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8987926/,2019-11-29 00:00:00,3d holographic and interactive artificial intelligence system,"In modern world, Artificial Intelligence (AI) system plays an important role. The usage of AI system has increased tremendously over the past few decades. As a result, many AI assistants are available in the market such as Software and Hardware based (Google Assistant, Amazon Alexa, and Apple Siri) and Augmented Reality based (Microsoft HoloLens). However, all the current AI system lacks interactive and scope to visualize the output is limited because of certain constraints. In this paper, we propose a new 3D Holographic and Interactive Artificial Intelligence System. The system combines field of 3D Holography and AI to leverage a new type of AI System. Our system presents 3D Holographic output using pepper ghost pyramid projection technique. The 3D holographic output projected using this technique appears to be floating in the air. The 3D holographic output can be interacted by performing various hand gesture in the air which are supported using gesture detection module integrated in the system. To perform basic AI operations, the system uses Natural Language Processing to understand and provide answer to the user queries with the accuracy of 80%. Also, the system uses Motion Machine Learning (MotionML) to make man-machine interaction natural by adding various non-verbal communication cues. 3D Holographic and Interactive AI system finds its application in the field of education, medical, home automation and many others."
410,unknown,10.1109/worlds4.2019.8904020,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8904020/,2019-07-31 00:00:00,"an integrated framework for autonomous driving: object detection, lane detection, and free space detection","In this paper, we present a deep neural network based real-time integrated framework to detect objects, lane markings, and drivable space using a monocular camera for advanced driver assistance systems. The object detection framework detects and tracks objects on the road such as cars, trucks, pedestrians, bicycles, motorcycles, and traffic signs. The lane detection framework identifies the different lane markings on the road and also distinguishes between the ego lane and adjacent lane boundaries. The free space detection framework estimates the drivable space in front of the vehicle. In our integrated framework, we propose a pipeline combining the three deep neural networks into a single framework, for object detection, lane detection, and free space detection simultaneously. The integrated framework is implemented in C++ and runs real-time on the Nvidia's Drive PX 2 platform."
411,unknown,10.3390/computers10080099,'MDPI AG',core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-08-01 00:00:00,an integrated mobile augmented reality digital twin monitoring system,"The increasing digitalization and advancement in information communication technologies has greatly changed how humans interact with digital information. Nowadays, it is not sufficient to only display relevant data in production activities, as the enormous amount of data generated from smart devices can overwhelm operators without being fully utilized. Operators often require extensive knowledge of the machines in use to make informed decisions during processes such as maintenance and production. To enable novice operators to access such knowledge, it is important to reinvent the way of interacting with digitally enhanced smart devices. In this research, a mobile augmented reality remote monitoring system is proposed to help operators with low knowledge and experience level comprehend digital twin data of a device and interact with the device. It analyses both historic logs as well as real-time data through a cloud server and enriches 2D data with 3D models and animations in the 3D physical space. A cloud-based machine learning algorithm is applied to transform learned knowledge into live presentations on a mobile device for users to interact with. A scaled-down case study is conducted using a tower crane model to demonstrate the potential benefits as well as implications when the system is deployed in industrial environments. This user study verifies that the proposed solution yields consistent measurable improvements for novice users in human-device interaction that is statistically significant"
412,unknown,10.1109/tsc.2017.2777478,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8119814/,2021-02-01 00:00:00,solar: services-oriented deep learning architectures-deep learning as a service,"Deep learning has been an emerging field of machine learning during past decades. However, the diversity and large scale data size have posed significant challenge to construct a flexible and high performance implementations of deep learning neural networks. In order to improve the performance as well to maintain the scalability, in this paper we present SOLAR, a services-oriented deep learning architecture using various accelerators like GPU and FPGA. SOLAR provides a uniform programming model to users so that the hardware implementation and the scheduling is invisible to the programmers. At runtime, the services can be executed either on the software processors or the hardware accelerators. To leverage the trade-offs between the metrics among performance, power, energy, and efficiency, we present a multitarget design space exploration. Experimental results on the real state-of-the-art FPGA board demonstrate that the SOLAR is able to provide a ubiquitous framework for diverse applications without increasing the burden of the programmers. Moreover, the speedup of the GPU and FPGA hardware accelerator in SOLAR can achieve significant speedup comparing to the conventional Intel i5 processors with great scalability."
413,unknown,10.1109/i2ct54291.2022.9823974,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9823974/,2022-04-09 00:00:00,covid-tracker: surveillance of potential clusters using a wristband and location-based data,"COVID-19 is a global pandemic that has threatened the survival of humans and other living beings. COVID-19 causes illnesses varying from the very mild cold to serious health complications resulting in death. Most Information Technology based solutions have been implemented to prevent the COVID-19 pandemic while raising awareness in the public. However, there is a limited number of reliable and real-time applications of self-awareness on COVID-19. Currently, the globe is dealing with the COVID-19 epidemic, particularly in pursuit of economic growth in each country. Therefore, an accurate, efficient automatic method to raise self-awareness by avoiding risky contacts is useful for human survival. This paper describes the automatic detection of temperature using a wearable device and an automatic alerting mechanism to inform the users of potentially risky contacts with higher temperatures nearby within a considerable time frame. COVID-Tracker produces results with high accuracy and efficiency, this is beneficial to improve self-awareness among users, to visualize potential covid clusters, and also to improve the mental health of self-isolated people. The developed application consists of four main components namely: temperature measuring band, mobile application, prediction model-based visualization dashboard and an AI bot. Based on the results reported here, developed methods can help people to achieve self-awareness of COVID-19 by avoiding risk factors early and accurately."
414,unknown,http://arxiv.org/abs/1712.09347v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1712.09347v1,2017-12-25 00:00:00,"smart fog: fog computing framework for unsupervised clustering analytics
  in wearable internet of things","The increasing use of wearables in smart telehealth generates heterogeneous
medical big data. Cloud and fog services process these data for assisting
clinical procedures. IoT based ehealthcare have greatly benefited from
efficient data processing. This paper proposed and evaluated use of low
resource machine learning on Fog devices kept close to the wearables for smart
healthcare. In state of the art telecare systems, the signal processing and
machine learning modules are deployed in the cloud for processing physiological
data. We developed a prototype of Fog-based unsupervised machine learning big
data analysis for discovering patterns in physiological data. We employed Intel
Edison and Raspberry Pi as Fog computer in proposed architecture. We performed
validation studies on real-world pathological speech data from in home
monitoring of patients with Parkinson's disease (PD). Proposed architecture
employed machine learning for analysis of pathological speech data obtained
from smartwatches worn by the patients with PD. Results showed that proposed
architecture is promising for low-resource clinical machine learning. It could
be useful for other applications within wearable IoT for smart telehealth
scenarios by translating machine learning approaches from the cloud backend to
edge computing devices such as Fog."
415,included,10.1109/tpds.2019.2922205,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8735806/,2019-12-01 00:00:00,t-gaming: a cost-efficient cloud gaming system at scale,"Cloud gaming (CG) system could pursue both high-quality gaming experience via intensive computing, and ultimate convenience anywhere at anytime through any energy-constrained mobile devices. Despite the abundance of efforts devoted, state-of-the-art CG systems still suffer from multiple key limitations: expensive deployment cost, high bandwidth consumption and unsatisfied quality of experience (QoE). As a result, existing works are not widely adopted in reality. This paper proposes a Transparent Gaming framework called T-Gaming that allows users to play any popular high-end desktop/console games on-the-fly over the Internet. T-Gaming utilizes the off-the-shelf consumer GPUs without resorting to the expensive proprietary GPU virtualization (vGPU) technology to reduce the deployment cost. Moreover, it enables prioritized video encoding based on the human visual feature to reduce the bandwidth consumption without noticeable visual quality degradation. Last but not least, T-Gaming adopts adaptive real-time streaming based on deep reinforcement learning (RL) to improve user's QoE. To evaluate the performance of T-Gaming, we implement and test a prototype system in the real world. Compared with the existing cloud gaming systems, T-Gaming not only reduces the expense per user by 75 percent hardware cost reduction and 14.3 percent network cost reduction, but also improves the normalized average QoE by 3.6-27.9 percent."
416,unknown,http://arxiv.org/abs/2107.13212v1,arxiv,arxiv,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2107.13212v1,2021-07-28 00:00:00,"the trip to the enterprise gourmet data product marketplace through a
  self-service data platform","Data Analytics provides core business reporting needs in many software
companies, acts as a source of truth for key information, and enables building
advanced solutions, e.g., predictive models, machine learning, real-time
recommendations, to grow the business.
  A self-service, multi-tenant, API-first, and scalable data platform is the
foundational requirement in creating an enterprise data marketplace, which
enables the creation, publishing, and exchange of data products. Such a
marketplace enables the exploration and discovery of data products, further
providing high-level data governance and oversight on marketplace contents. In
this paper, we describe our way to the gourmet data product marketplace. We
cover the design principles, the implementation details, technology choices,
and the journey to build an enterprise data platform that meets the above
characteristics. The platform consists of ingestion, streaming, storage,
transformation, schema generation, fail-safe, data sharing, access management,
PII data automatic identification, self-service storage optimization
recommendations, and CI/CD integration.
  We then show how the platform enables and operates the data marketplace,
facilitating the exchange of stable data products across users and tenants. We
motivate and show how we run scalable decentralized data governance. All of
this is built and run for Cimpress Technology (CT), which operates the Mass
Customization Platform for Cimpress and its businesses. The CT data platform
serves 1000s of users from different platform participants, with data sourced
from heterogeneous sources. Data is ingested at a rate of well over 1000
individual messages per second and serves more than 100k analytical queries
daily."
417,excluded,10.1109/access.2019.2948949,IEEE,ieeexplore,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8879579/,2019-01-01 00:00:00,a big data mining approach of pso-based bp neural network for financial risk management with iot,"In recent years, the technology about IoT (Internet of Things) has been applied into finance domain, and the generated data, such as the real-time data of chattel mortgage supervision with GPS, sensors, network cameras, mobile devices, etc., has been used to improve the capability of financial credit risk management of bank loans. Financial credit risk is by far one of the most significant risks that commercial banks have to face, however, when confronting to the massively growing financial data from multiple sources including Internet, mobile networks or IoT, traditional statistical models and neural network models might not operate fairly or accurately enough for credit risk assessment with those diverse data. Hence, there is a practical need to establish more powerful risk prediction models with artificial intelligence based on big data analytics to predict default behaviors with better accuracy and capacity. In this article, a big data mining approach of Particle Swarm Optimization (PSO) based Backpropagation (BP) neural network is proposed for financial risk management in commercial banks with IoT deployment, which constructs a nonlinear parallel optimization model with Apache Spark and Hadoop HDFS techniques on the dataset of on-balance sheet item and off-balance sheet item. The experiment results indicate that this parallel risk management model has fast convergence rate and powerful predictive capacity, and performs efficiently in screening default behaviors. In the meanwhile, the distributed implementation on big data clusters largely reduces the processing time of model training and testing."
418,excluded,10.1109/iccike51210.2021.9410745,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9410745/,2021-03-18 00:00:00,a social distance monitoring system to ensure social distancing in public areas,"Social distancing measures are important to reduce Covid spread. In order to break the chain of spread, social distancing is strictly followed as a norm. This paper demonstrates a system which is useful in monitoring public places like ATMs, malls and hospitals for any social distancing violations. With the help of this proposed system, it would be conveniently possible to monitor individuals whether they are maintaining the social distancing in the area under surveillance and also to alert the individuals as and when there is any violations from the predefined limits. The proposed deep learning technology based system can be installed for coverage within a certain limited distance. The algorithm could be implemented on the live images of CCTV cameras to perform the task. The simulated model uses deep learning algorithms with OpenCV library to estimate distance between the people in the frame, and a YOLO model trained on COCO dataset to identify people in the frame. The system has to be configured according to the location it is being installed at. By implementing the algorithm, the number of violations are reported based on the distance and set threshold. Number of violations reported are one and two for two real time images respectively. The red box highlighting the violations are displayed along with distance. Reporting efficiency and correctness were validated for more number of samples."
419,unknown,10.1109/tits.2021.3091542,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9471013/,2022-07-01 00:00:00,jp-dap: an intelligent data analytics platform for metro rail transport systems,"This paper deals with an intelligent data analytics platform - Jaison-Paul Data Analytics Platform (JP-DAP) - for metro rail transport systems. JP-DAP is intended to ensure smooth functioning, improved customer experience, ridership forecasting, and efficient administration of metro rail transportation systems by integrating and analysing its many data sources. It consists of a middleware which is built on the top of a Hadoop Distributed File System (HDFS) and Spark framework, along with a set of open-source software tools like Apache Hive, Pandas, Google TensorFlow and Spark ML-lib for real-time and legacy data processing. The benchmarking of JP-DAP was conducted using TestDFSIO and have found that it performs well according to industry standards. The specific use case for this project is Kochi Metro Rail Limited (KMRL). The analysis of Automated Fare Collection data from KMRL on JP-DAP framework have produced descriptive statistics visualisation of inflow and outflow analysis, travel patterns during weekdays and weekends, origin-destination matrix, etc.. Moreover JP-DAP framework is capable of producing short term passenger flow predictions using SVR machine learning algorithm with linear, radial basis function and polynomial kernels. Our experiments have shown that SVR linear kernel gives the most accurate results with the least errors in predicting the next day&#x2019;s passenger count using the previous five weekdays data. The station usage (one-to-all) prediction using Long Short-Term Memory (LSTM) is also integrated to this framework. The visualisation as well as analytical outcomes of JP-DAP framework have also been made available to the external world using a rich set of REST APIs and are projected on to a web-dashboard."
420,excluded,http://arxiv.org/abs/1602.02339v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1602.02339v1,2016-02-07 00:00:00,"dynamic selection of virtual machines for application servers in cloud
  environments","Autoscaling is a hallmark of cloud computing as it allows flexible
just-in-time allocation and release of computational resources in response to
dynamic and often unpredictable workloads. This is especially important for web
applications whose workload is time dependent and prone to flash crowds. Most
of them follow the 3-tier architectural pattern, and are divided into
presentation, application/domain and data layers. In this work we focus on the
application layer. Reactive autoscaling policies of the type ""Instantiate a new
Virtual Machine (VM) when the average server CPU utilisation reaches X%"" have
been used successfully since the dawn of cloud computing. But which VM type is
the most suitable for the specific application at the moment remains an open
question. In this work, we propose an approach for dynamic VM type selection.
It uses a combination of online machine learning techniques, works in real time
and adapts to changes in the users' workload patterns, application changes as
well as middleware upgrades and reconfigurations. We have developed a
prototype, which we tested with the CloudStone benchmark deployed on AWS EC2.
Results show that our method quickly adapts to workload changes and reduces the
total cost compared to the industry standard approach."
421,excluded,10.1007/978-3-319-19602-2_27,Springer,springer,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-319-19602-2_27,2016-01-01 00:00:00,landmark-based pedestrian navigation using augmented reality and machine learning,"The prevalence of smartphones and tablets featuring various kinds of sensors and the improvements in computation capabilities of those devices, have led to an acceleration of using geospatial data in many domains. The large number of sensors deployed on the devices has made it possible to detect user’s location, heading and orientation as well as getting contextual information from various sources of online data. Combining the stream of data from positioning and orientation sensors with camera, has also made it more feasible to deploy practical Augmented Reality (AR) applications on mobile devices. This paper, explains a system and its related study that provides a view of the navigation experience which composed of the AR view as well as continuous personal feedbacks about the relative location of the user in relation to the closest landmarks. In the system, navigation and path finding are based on landmarks. Relative position of the user with regard to landmark is determined using GPS sensors as well as image processing algorithm for finding distance from a landmark. In addition, feedbacks for navigation instructions are customized for each user based on his/her movement profile and use of continuous tuning of a machine learning algorithm. Experiment of using the system showed a significant improvement in acquiring of spatial knowledge for the users in comparison with turn-by-turn systems."
422,included,10.1109/dsaa.2019.00070,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8964147/,2019-10-08 00:00:00,"bighead: a framework-agnostic, end-to-end machine learning platform","With the increasing need to build systems and products powered by machine learning inside organizations, it is critical to have a platform that provides machine learning practitioners with a unified environment to easily prototype, deploy, and maintain their models at scale. However, due to the diversity of machine learning libraries, the inconsistency between environments, and various scalability requirement, there is no existing work to date that addresses all of these challenges. Here, we introduce Bighead, a framework-agnostic, end-to-end platform for machine learning. It offers a seamless user experience requiring only minimal efforts that span feature set management, prototyping, training, batch (offline) inference, real-time (online) inference, evaluation, and model lifecycle management. In contrast to existing platforms, it is designed to be highly versatile and extensible, and supports all major machine learning frameworks, rather than focusing on one particular framework. It ensures consistency across different environments and stages of the model lifecycle, as well as across data sources and transformations. It scales horizontally and elastically in response to the workload such as dataset size and throughput. Its components include a feature management framework, a model development toolkit, a lifecycle management service with UI, an offline training and inference engine, an online inference service, an interactive prototyping environment, and a Docker image customization tool. It is the first platform to offer a feature management component that is a general-purpose aggregation framework with lambda architecture and temporal joins. Bighead is deployed and widely adopted at Airbnb, and has enabled the data science and engineering teams to develop and deploy machine learning models in a timely and reliable manner. Bighead has shortened the time to deploy a new model from months to days, ensured the stability of the models in production, facilitated adoption of cutting-edge models, and enabled advanced machine learning based product features of the Airbnb platform. We present two use cases of productionizing models of computer vision and natural language processing."
423,excluded,10.1109/icece.2010.643,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5629792/,2010-06-27 00:00:00,a self-adaptive monitoring and analysis system for students' behaviors in laboratory course,"Recently, there are many problems in software laboratory course management. It is, particularly, widespread that student play video games or do other things which are unrelated to the course. However, most of present monitoring software are all based on the operation of teachers, rather than a real-time system, which is very inconvenient. In order to carry out the laboratory course management better and help teachers to monitor and analyze students' behaviors of operating computer, we design and implement a model of Self-adaptive Monitoring and Analysis System (SMAS) for students' behaviors in laboratory course. For monitoring, we implement a module which uses the technology neural-network-based expert system, with which we can monitor the students' behavior real-time and self-adaptively. For analysis, with the methodologies such as Factor Analysis, Cluster Analysis, and Discriminant Analysis, we propose and implement an evaluation algorithm, which can evaluate the students' performance in a course quantificationally and obviously."
424,unknown,10.1016/j.micpro.2022.104554,scopus,sciencedirect,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85130597393,2022-07-01,real-time edge computing on multi-processes and multi-threading architectures for deep learning applications,"As the computing power of embedded system hardware devices continues to grow, more and more deep learning models have been gradually transplanted into edge devices. Accordingly, a variety of application scenarios have been developed with more complex inference models or multiple models that work together. Moreover, with consideration given to cost, a single edge computing device can integrate multiple input sources and simultaneously complete the application of multiple scenarios. To achieve good performance on edge computing devices, this paper probes into the software architecture design of multi-processes and multi-threading architectures on edge computing device, in order to realize real-time edge computing. In the experiment, multiple face-related deep learning models, namely, face detection, face recognition, age estimation, gender estimation, and emotion estimation, are used to demonstrate the differences between multi-processes and multi-threading on edge computing devices. According to the experimental results, it is known that if the number of central processing unit (CPU) cores and memory space are small, the multi-threading architecture can better improve efficiency; conversely, the multi-processes architecture can be used. The architecture proposed in this paper has reference value, and can improve the execution efficiency of deep learning technology on edge computing, and reduce the cost of deployment."
425,excluded,10.1038/s41598-021-94178-5,'Springer Science and Business Media LLC',core,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://core.ac.uk/download/479436074.pdf,2021-08-04 00:00:00,real-world? artificial intelligence-based opportunistic screening for diabetic retinopathy in endocrinology and indigenous healthcare settings in australia,"This study investigated the diagnostic performance, feasibility, and end-user experiences of an artificial intelligence (AI)-assisted diabetic retinopathy (DR) screening model in real-world Australian healthcare settings. The study consisted of two components: (1) DR screening of patients using an AI-assisted system and (2) in-depth interviews with health professionals involved in implementing screening. Participants with type 1 or type 2 diabetes mellitus attending two endocrinology outpatient and three Aboriginal Medical Services clinics between March 2018 and May 2019 were invited to a prospective observational study. A single 45-degree (macula centred), non-stereoscopic, colour retinal image was taken of each eye from participants and were instantly screened for referable DR using a custom offline automated AI system. A total of 236 participants, including 174 from endocrinology and 62 from Aboriginal Medical Services clinics, provided informed consent and 203 (86.0%) were included in the analysis. A total of 33 consenting participants (14%) were excluded from the primary analysis due to ungradable or missing images from small pupils (n = 21, 63.6%), cataract (n = 7, 21.2%), poor fixation (n = 2, 6.1%), technical issues (n = 2, 6.1%), and corneal scarring (n = 1, 3%). The area under the curve, sensitivity, and specificity of the AI system for referable DR were 0.92, 96.9% and 87.7%, respectively. There were 51 disagreements between the reference standard and index test diagnoses, including 29 which were manually graded as ungradable, 21 false positives, and one false negative. A total of 28 participants (11.9%) were referred for follow-up based on new ocular findings, among whom, 15 (53.6%) were able to be contacted and 9 (60%) adhered to referral. Of 207 participants who completed a satisfaction questionnaire, 93.7% specified they were either satisfied or extremely satisfied, and 93.2% specified they would be likely or extremely likely to use this service again. Clinical staff involved in screening most frequently noted that the AI system was easy to use, and the real-time diagnostic report was useful. Our study indicates that AI-assisted DR screening model is accurate and well-accepted by patients and clinicians in endocrinology and indigenous healthcare settings. Future deployments of AI-assisted screening models would require consideration of downstream referral pathways"
426,excluded,10.1109/uemcon.2018.8796576,"2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/f75d822b32ef7cf97ab175a11b2fd4dfadbaa820,2018-01-01 00:00:00,non-intrusive activity detection and prediction in smart residential spaces,"Non-intrusive human activity detection and prediction is an important and challenging problem in smart and pervasive spaces. The advantages of such a design are the reduced dependency on the users and fewer security/privacy concerns. However, these also make it difficult to effectively and accurately understand the activities in real-time. In residential spaces, this can be even a bigger challenge due to nonuniform space boundaries and multiple people sharing the space. In this paper, we present a system, that consists of both hardware and software components, capable of detecting and predicting human activities in a smart residential environment. Our system deploys a finite state machine-based activity detection with 96% accuracy in real-time. Afterwards, we use several machine learning methods to create an effective activity prediction framework. We demonstrate that we can achieve up to 98.5% activity prediction accuracy with 4ms delay, making it a perfect real-time system example. Since our smart and pervasive space implementation does not use any intrusive sensor or data acquisition unit (such as wearables, camera, or audio sources), we reduce the dependency to the user and potential security/privacy issues."
427,excluded,10.3390/s21051640,Sensors,semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/5856e2a77cdcde9346a6b1a02e67588166904727,2021-01-01 00:00:00,towards secure fitness framework based on iot-enabled blockchain network integrated with machine learning algorithms,"Blockchain technology has recently inspired remarkable attention due to its unique features, such as privacy, accountability, immutability, and anonymity, to name of the few. In contrast, core functionalities of most Internet of Things (IoT) resources make them vulnerable to security threats. The IoT devices, such as smartphones and tablets, have limited capacity in terms of network, computing, and storage, which make them easier for vulnerable threats. Furthermore, a massive amount of data produced by the IoT devices, which is still an open challenge for the existing platforms to process, analyze, and unearth underlying patterns to provide convenience environment. Therefore, a new solution is required to ensure data accountability, improve data privacy and accessibility, and extract hidden patterns and useful knowledge to provide adequate services. In this paper, we present a secure fitness framework that is based on an IoT-enabled blockchain network integrated with machine learning approaches. The proposed framework consists of two modules: a blockchain-based IoT network to provide security and integrity to sensing data as well as an enhanced smart contract enabled relationship and inference engine to discover hidden insights and useful knowledge from IoT and user device network data. The enhanced smart contract aims to support users with a practical application that provides real-time monitoring, control, easy access, and immutable logs of multiple devices that are deployed in several domains. The inference engine module aims to unearth underlying patterns and useful knowledge from IoT environment data, which helps in effective decision making to provide convenient services. For experimental analysis, we implement an intelligent fitness service that is based on an enhanced smart contract enabled relationship and inference engine as a case study where several IoT fitness devices are used to securely acquire user personalized fitness data. Furthermore, a real-time inference engine investigates user personalized data to discover useful knowledge and hidden insights. Based on inference engine knowledge, a recommendation model is developed to recommend a daily and monthly diet, as well as a workout plan for better and improved body shape. The recommendation model aims to facilitate a trainer formulating effective future decisions of trainee’s health in terms of a diet and workout plan. Lastly, for performance analysis, we have used Hyperledger Caliper to access the system performance in terms of latency, throughput, resource utilization, and varying orderer and peers nodes. The analysis results imply that the design architecture is applicable for resource-constrained IoT blockchain platform and it is extensible for different IoT scenarios."
428,excluded,10.1109/comsnets53615.2022.9668420,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9668420/,2022-01-08 00:00:00,ml-based device-agnostic human activity detection with wifi sniffer traffic,"Human Activity Detection plays a pivotal role in smoothly managing the health care for the elderly and those with chronic health conditions in smart home environments. Even though there are several technological advancements made in this area, solutions like smartwatches are costly to afford and the solutions that rely on sensors and cameras suffer from privacy concerns. While wireless channel state information-based approaches can address some of these limitations, these approaches either require special hardware to be deployed or modifications at the WiFi access point. In this paper, we propose to detect human activities in a device-agnostic manner by leveraging passively captured passively captured WiFi MAC-layer traffic with the help of a sniffer. In that way, elderly people and those who suffer from chronic health conditions do not need to put any sensors on their body. This approach is not only cost-effective, but it is also easy to deploy without requiring any changes at the WiFi access point or installing special sensors in the environment. We train and test six off-the-shelf machine learning models on 15+ hours worth of WiFi MAC-layer traffic collected in a home environment. We present a proof-of-concept system prototype that employs this approach. We are able to detect six activities - (a) Walking vs Sitting, (b) Sleeping vs Awake, and (c) Using Phone vs Not Using Phone in three different real-world scenarios. Our evaluation reveals that WiFi MAC-layer traffic has special signatures to detect human activities and we are able to achieve 92.49 &#x0025; detection accuracy in the best case."
429,unknown,10.1109/dasa53625.2021.9682299,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9682299/,2021-12-08 00:00:00,development of a novel decision-making tool for vessel efficiency optimization using iot and dl,"The era of the Internet of Things and big data has completely changed the landscape of many scientific disciplines. Embracing the big data revolution, machine learning techniques seem to have substituted traditional physics-based models due to their effectiveness and flexibility. According to Rolls-Royce [1], the smart ship will revolutionize the landscape of ship design and operations, the same way smartphones did our daily life. These enhanced data-driven analytics yield substantial benefits in terms of cost savings by limiting the degree to which human factor intervenes to perform certain operations, and also by providing reliable and accurate information in real-time information regarding the operational health of the machinery on board, ensuring the safety of the crew as well as the reliability of the equipment. [2]. Despite its importance, the majority of efforts towards this direction are limited to academic efforts or black-box approaches. In the current work, a distributed software platform that augments the decision-making process within the shipping industry is critically presented. Key technological pillars are Big Data technologies, data fusion, and large-scale deep learning-based processing techniques. Evaluation of the platform, based on actual case data, provides evidence of its capacity for diagnosis and anomaly detection on the operation of critical subsystems."
430,included,10.1016/j.procs.2022.01.300,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85127760892,2022-01-01,an i4.0 data intensive platform suitable for the deployment of machine learning models: a predictive maintenance service case study,"The Artificial Intelligence is one of the key enablers of the Industry 4.0. The building of learning models as well as their deployment in environments where the rate of data generation is high and their analysis must meet real time requirements lead to the need of selecting a big data platform suitable for this purpose. The heterogeneous and distributed nature of I4.0 environments where data becomes highly relevant requires the use of a data centric, distributed and scalable platform where the different applications are deployed as services. In this paper we present an I4.0 digital platform based on RAI4.0 reference architecture on which a predictive maintenance service has been built and deployed in Amazon Web Service cloud. Different strategies to build the predictor are described as well as the stages carried out for its construction. Finally, the predictor built with k-nearest algorithm is chosen because it is the fastest in producing an answer and its accuracy of 99.87% is quite close to the best model for our case study."
431,unknown,10.1007/978-3-642-36365-8_11,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-642-36365-8_11,2013-01-01 00:00:00,adaptive classification of special events in agroecological monitoring systems for pest management,"With powerful feasible integration of distributed sensing capability, real-time data analysis, and remote surveillance, wireless sensor networks (WSNs) provide a new insight into agroecological observation in ways of extending spatial and temporal scales. Through WSNs, some unexpected phenomena can be found, and new paradigms can be developed. Although employing the WSN technology can facilitate agroecological observation, one of the major challenges that need to be overcome is the abnormal readings caused by sensor failure, energy depletion of sensor nodes, low durability of protective cases in a wild environment, unreliable wireless communication, etc. In this study, a WSN-based ecological monitoring system is presented and practically deployed in a field to monitor the number of the oriental fruit fly ( Bactrocera dorsalis (Hendel)) and capture long-term and up-to-minute natural environmental fluctuations. Moreover, an adaptive classification approach, built upon self-organizing maps and support vector machines, is incorporated into the monitoring system to automatically identify special events of pest outbreaks and sensor faults. Once the events are detected, farmers and government officials can take precautionary action in time before pest outbreaks cause an extensive loss or schedule maintenance tasks to repair monitoring devices. The proposed classification approach is easily adopted in different monitored farms, and it can automatically identify special events based on machine learning techniques without requiring additional manpower."
432,excluded,10.1109/aero53065.2022.9843515,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9843515/,2022-03-12 00:00:00,real-time segmentation of desiccation cracks onboard uavs for planetary exploration,"Planetary surfaces are a primary focus of space exploration. Some of the most challenging current efforts in planetary exploration relate to the search for life, or biosignatures, in these environments. Detecting water-related textures, and thus evidence for potentially habitable environments, has the potential to focus and accelerate the search for biosignatures on other planets. Desiccation cracks are sedimentary features providing evidence of sediment-water interaction. They are known from both Earth and Mars, and are likely to be found via aerial exploration approaches of ancient lakes, rivers, or shallow marine environments where biosignatures may be found. Current approaches using image processing to detect desiccation cracks focus on segmenting just the cracks and prove somewhat successful. However, the use of Unmanned Aerial Vehicles (UAVs) to detect and highlight areas with desiccation cracks for closer inspection over much larger surface areas has not yet been explored. This paper describes the development and deployment of a desiccation crack detection system using UAVs and AI. We describe data collection at varying heights above ground level and data-augmentation with a range of pixel-level and spatial transforms. Three state-of-the-art CNN segmentation networks are trained and evaluated using PyTorch. The networks are deployed on an edge-AI device integrated with a companion computer onboard a sub-2kg quadrotor UAV. Results indicate that the models can segment desiccation cracks on airborne-collected images at various locations at heights ranging from 5 to 20m. Deployment of the models for real-time inference onboard small UAVs shows potential for application in the field. This research shows the feasibility of a low-volume data training to UAV deployment pipeline while highlighting potential hurdles in the processing pipeline for future efforts. We present a system and architecture for onboard UAV detectors of sedimentary features, which can speed up the search for life on other planets."
433,unknown,10.1109/dese51703.2020.9450783,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9450783/,2020-12-17 00:00:00,overcoming speech anxiety using virtual reality with voice and heart rate analysis,"Social phobias are afflictions that millions of people suffer from. One common social phobia is speech anxiety (Glossophobia), which makes it difficult for people to talk in public or with others. To address this problem, this system was created to help people with Glossophobia practice making presentations or having personal interviews with less fear. The objective is to train them before their presentations or interviews by simulating 360° video environments with virtual reality (VR) technology. During the practice, the system analyzes the voice and heart rate of the person to discover any emotional and physical symptoms of speech anxiety using Arduino heart rate sensors, machine learning, and speech recognition techniques. The system will generate advice based on the symptoms to help make the user more confident. Additionally, after several training sessions, the system will present a report showing the progress in the user's performance. The system has been fully implemented and has demonstrated its operational effectiveness."
434,unknown,10.1109/wf-iot51360.2021.9595639,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9595639/,2021-07-31 00:00:00,ecg real-time monitoring and heart anomaly detection reimagined,"The electrocardiogram (ECG) test is developed to monitor the functionality of the cardiovascular system. Nowadays, numerous attentions have been given to the accurate and early detection of heartbeat anomalies in real-time to prevent complications and take necessary measures. This paper proposes a robust real-time binary classification for ECG signals to detect possible anomalies. We implement an initial detection phase right where ECG data is collected through lightweight deep learning analysis. We evaluate the system on two widely used datasets, PTB and MIT-BIH datasets from PhysioNet. Our experiments suggest using artificial neural network (ANN) algorithms for their superior performance over other machine learning algorithms with an accuracy up to 99.3%. Furthermore, we implemented our system on a Raspberry Pi B+ representing an ECG patch to collect and process ECG signals and detect any abnormalities using the proposed ANN model. To create a scalable system, we stream the data in real-time using Apache Kafka and MQTT to keep records of patients’ ECG data and use it for further analysis to identify causes and support medical diagnosis. The system notifies healthcare providers when abnormalities are detected."
435,unknown,10.1016/b978-0-12-822884-5.00020-9,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85128012085,2021-01-01,a scalable medication intake monitoring system,"Poor medication adherence is a global issue, causing adverse health-care problems and economic consequences. Many recent studies have designed and developed medication intake monitoring systems for both directly and indirectly monitoring patients using various sensors and advanced signal processing and machine learning algorithms. However, many studies have failed to deliver a system architecture that can be easily adapted to real-life scenarios with respect to cost, size, wearability, and social acceptance. A modern system architecture for medication intake monitoring must overcome these concerns, providing a practical design that combines hardware and software to accurately identify medication intake. Furthermore, for storing and processing high-frequency sensor data streams from multiple users simultaneously, it is essential to utilize scalable data storage and computing frameworks. In this chapter, we introduce a recently developed smartwatch application and a cloud-based data pipeline. The smartwatch application collects activity sensor data and sound data using embedded inertial sensors and microphones. The cloud-based data pipeline includes distributed data storage, Apache Spark-based distributed computing, and H2O-based distributed machine learning frameworks in order to build a machine learning model that identifies instances of medication intake."
436,excluded,http://arxiv.org/abs/2102.09548v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2102.09548v2,2021-02-18 00:00:00,"therapeutics data commons: machine learning datasets and tasks for drug
  discovery and development","Therapeutics machine learning is an emerging field with incredible
opportunities for innovatiaon and impact. However, advancement in this field
requires formulation of meaningful learning tasks and careful curation of
datasets. Here, we introduce Therapeutics Data Commons (TDC), the first
unifying platform to systematically access and evaluate machine learning across
the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets
spread across 22 learning tasks and spanning the discovery and development of
safe and effective medicines. TDC also provides an ecosystem of tools and
community resources, including 33 data functions and types of meaningful data
splits, 23 strategies for systematic model evaluation, 17 molecule generation
oracles, and 29 public leaderboards. All resources are integrated and
accessible via an open Python library. We carry out extensive experiments on
selected datasets, demonstrating that even the strongest algorithms fall short
of solving key therapeutics challenges, including real dataset distributional
shifts, multi-scale modeling of heterogeneous data, and robust generalization
to novel data points. We envision that TDC can facilitate algorithmic and
scientific advances and considerably accelerate machine-learning model
development, validation and transition into biomedical and clinical
implementation. TDC is an open-science initiative available at
https://tdcommons.ai."
437,excluded,10.1109/ectc32696.2021.00346,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9501916/,2021-07-04 00:00:00,an automated optical inspection system for pip solder joint classification using convolutional neural networks,"In the fields of electronics manufacturing, the application of through-hole devices is still required, as heat dissipation and high current carrying capacity plays an important role. To ensure the highest quality standards, these electronics production processes take a multitude of inspection processes into account. For the detection of error patterns regarding the quality of the solder connections, usually, high-end inspection machines are utilized in the industrial application. The Automated Optical Inspection is a commonly conducted process, using visible light and rule-based inspection routines, setup by process experts for the evaluation of the Region of Interest. The high overhead of creating and maintaining product-specific checking routines and machine acquisition leads to increased costs and severe dependency on expert know-how. A flexible inspection algorithm, implemented into low-cost equipment for image generation is expected to reduce acquisition and optimization costs, and lower dependency on expert knowledge and high-end machinery. In this contribution, we present a novel framework for the automatic, near real-time solder joint classification based on Convolutional Neural Networks, flexibly detecting, and classifying solder connections. We utilize existing Deep Learning architectures for detection and classification. The localization model utilizes a YOLO-architecture (you-only-look-once), learning feature inputs based on a supervised learning approach. Pseudo-labeling is carried out automatically by an anomaly detection model. The image generation is executed by an industrial low-cost camera and an industrial rack-PC. The developed prototype is integrated into the existing production infrastructure. The results indicate a satisfactory detection and classification of the investigated solder connections with the proposed system. Hence, this system represent an alternative to commercially available high-end inspection systems being used for an inline control of Pin-in-Paste and through-hole device solder connections."
438,unknown,10.1109/iemcon51383.2020.9284818,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9284818/,2020-11-07 00:00:00,an adapter for ibm streams and apache spark to facilitate multi-level data analytics,"Data analytics with unsupervised clustering of data streams has provided revolutionary breakthroughs in fields like healthcare, and E-commerce. IBM Streams and Apache Spark are among the most useful and popular data analytics tools that help engineers and researchers extend the abilities to store, analyze, transform, and visualize data for business use. IBM Streams is capable of ingesting, filtering, analyzing, and associating massive volumes of continuous data streams and the Streams Processing Language (SPL) enables coding custom stream graphs to process data and handle real-time events. Apache Spark has unified analytics edge for large scale data processing with high performance for both batch and streaming data. We developed adapters without using third party tools to facilitate data transfer between IBM Streams and Apache Spark to support new and legacy data analytic systems. An example use case would be IBM Streams ingesting and processing realtime data streams, and then passing the data to Spark to train or update machine learning algorithms in real time that can be re-deployed in the IBM Streams data processing pipeline. This paper provides an overview of the structure of the data processing pipeline, describes the implementation details and the principle behind the design."
439,excluded,http://arxiv.org/abs/1811.04871v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1811.04871v1,2018-11-12 00:00:00,characterizing machine learning process: a maturity framework,"Academic literature on machine learning modeling fails to address how to make
machine learning models work for enterprises. For example, existing machine
learning processes cannot address how to define business use cases for an AI
application, how to convert business requirements from offering managers into
data requirements for data scientists, and how to continuously improve AI
applications in term of accuracy and fairness, and how to customize general
purpose machine learning models with industry, domain, and use case specific
data to make them more accurate for specific situations etc. Making AI work for
enterprises requires special considerations, tools, methods and processes. In
this paper we present a maturity framework for machine learning model lifecycle
management for enterprises. Our framework is a re-interpretation of the
software Capability Maturity Model (CMM) for machine learning model development
process. We present a set of best practices from our personal experience of
building large scale real-world machine learning models to help organizations
achieve higher levels of maturity independent of their starting point."
440,excluded,10.1109/mipr49039.2020.00055,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9175568/,2020-08-08 00:00:00,end-edge-cloud collaborative system: a video big data processing and analysis architecture,"In the current surveillance system, video streams are firstly captured and compressed at the cameras, and then transmitted to the backend severs or cloud for big data analysis. It is impractical to aggregate all video streams from hundreds of thousands of cameras for big data analysis. Transcoding the videos to low-bitrate ones is the conventional solution to solve the aggregation bottleneck. However, it is recognized that transcoding will inevitably affect visual feature extraction, consequently degrading the subsequent analysis performance. To address these challenges, we thus propose a new video big data analysis framework, called end-edge-cloud collaborative system. Under the end-edge-cloud collaborative framework, a camera can output two streams simultaneously, including a compressed video stream for viewing and data storage, and a compact feature stream extracted from the original video signals for visual analysis. Video stream and feature stream are synchronized by unified identification. We identify three key technologies to enable the end-edge-cloud collaborative system, including analysis-friendly video coding, visual feature compact descriptor, and user-defined neural network and parameter updating. By real-time feeding only the feature streams into the cloud center, these cameras thus form a large-scale brain-like vision system for the smart city. A prototype has been implemented to demonstrate its feasibility. Experiment results show that our system can achieve high efficient video compression and guarantee the analysis performance. Furthermore, our system makes the big data analysis feasible which only need aggregate low bit-rate compressed feature stream."
441,unknown,10.3390/s21092993,'MDPI AG',core,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-05-01 00:00:00,iktishaf+: a big data tool with automatic labeling for road traffic social sensing and event detection using distributed machine learning,"Digital societies could be characterized by their increasing desire to express themselves and interact with others. This is being realized through digital platforms such as social media that have increasingly become convenient and inexpensive sensors compared to physical sensors in many sectors of smart societies. One such major sector is road transportation, which is the backbone of modern economies and costs globally 1.25 million deaths and 50 million human injuries annually. The cutting-edge on big data-enabled social media analytics for transportation-related studies is limited. This paper brings a range of technologies together to detect road traffic-related events using big data and distributed machine learning. The most specific contribution of this research is an automatic labelling method for machine learning-based traffic-related event detection from Twitter data in the Arabic language. The proposed method has been implemented in a software tool called Iktishaf+ (an Arabic word meaning discovery) that is able to detect traffic events automatically from tweets in the Arabic language using distributed machine learning over Apache Spark. The tool is built using nine components and a range of technologies including Apache Spark, Parquet, and MongoDB. Iktishaf+ uses a light stemmer for the Arabic language developed by us. We also use in this work a location extractor developed by us that allows us to extract and visualize spatio-tem-poral information about the detected events. The specific data used in this work comprises 33.5 million tweets collected from Saudi Arabia using the Twitter API. Using support vector machines, naïve Bayes, and logistic regression-based classifiers, we are able to detect and validate several real events in Saudi Arabia without prior knowledge, including a fire in Jeddah, rains in Makkah, and an accident in Riyadh. The findings show the effectiveness of Twitter media in detecting important events with no prior knowledge about them.</p"
442,unknown,http://arxiv.org/abs/1805.05491v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1805.05491v1,2018-05-14 00:00:00,"crowdbreaks: tracking health trends using public social media data and
  crowdsourcing","In the past decade, tracking health trends using social media data has shown
great promise, due to a powerful combination of massive adoption of social
media around the world, and increasingly potent hardware and software that
enables us to work with these new big data streams. At the same time, many
challenging problems have been identified. First, there is often a mismatch
between how rapidly online data can change, and how rapidly algorithms are
updated, which means that there is limited reusability for algorithms trained
on past data as their performance decreases over time. Second, much of the work
is focusing on specific issues during a specific past period in time, even
though public health institutions would need flexible tools to assess multiple
evolving situations in real time. Third, most tools providing such capabilities
are proprietary systems with little algorithmic or data transparency, and thus
little buy-in from the global public health and research community. Here, we
introduce Crowdbreaks, an open platform which allows tracking of health trends
by making use of continuous crowdsourced labelling of public social media
content. The system is built in a way which automatizes the typical workflow
from data collection, filtering, labelling and training of machine learning
classifiers and therefore can greatly accelerate the research process in the
public health domain. This work introduces the technical aspects of the
platform and explores its future use cases."
443,unknown,10.1109/isaect.2018.8618860,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8618860/,2018-11-23 00:00:00,application of machine learning model on streaming health data event in real-time to predict health status using spark,"In healthcare field, a huge amount of data collected in real-time by IoT systems, remote sensing device and other data collection tools brings new challenges that focus primarily on data size and the fast growth rate of such large data. Applying machine learning model on this voluminous data having varying velocity becomes extremely complex for traditional methods of data mining. To deal with this challenge, Apache Spark, a powerful big data processing tool can be used successfully for streaming data event against machine learning through in-memory and distributed computations. This work aims at developing a real-time health status prediction system with breast cancer use case using spark streaming framework with machine learning especially Decision Tree. The system focus on applying machine learning model on streaming data coming with rapid rate to predict health status based on several input variables. Based on this, the system first preprocesses the dataset and analyzes it to create an offline model for learning system, the model then deployed on system and use it in real-time to predict health status."
444,unknown,http://arxiv.org/abs/1811.06672v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1811.06672v1,2018-11-16 00:00:00,detecting irregular patterns in iot streaming data for fall detection,"Detecting patterns in real time streaming data has been an interesting and
challenging data analytics problem. With the proliferation of a variety of
sensor devices, real-time analytics of data from the Internet of Things (IoT)
to learn regular and irregular patterns has become an important machine
learning problem to enable predictive analytics for automated notification and
decision support. In this work, we address the problem of learning an irregular
human activity pattern, fall, from streaming IoT data from wearable sensors. We
present a deep neural network model for detecting fall based on accelerometer
data giving 98.75 percent accuracy using an online physical activity monitoring
dataset called ""MobiAct"", which was published by Vavoulas et al. The initial
model was developed using IBM Watson studio and then later transferred and
deployed on IBM Cloud with the streaming analytics service supported by IBM
Streams for monitoring real-time IoT data. We also present the systems
architecture of the real-time fall detection framework that we intend to use
with mbientlabs wearable health monitoring sensors for real time patient
monitoring at retirement homes or rehabilitation clinics."
445,excluded,10.3390/s21196688,Sensors,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/fb294c1a30ed274387227158dc7df608a63291a4,2021-01-01 00:00:00,building an iot platform based on service containerisation,"IoT platforms have become quite complex from a technical viewpoint, becoming the cornerstone for information sharing, storing, and indexing given the unprecedented scale of smart services being available by massive deployments of a large set of data-enabled devices. These platforms rely on structured formats that exploit standard technologies to deal with the gathered data, thus creating the need for carefully designed customised systems that can handle thousands of heterogeneous data sensors/actuators, multiple processing frameworks, and storage solutions. We present the SCoT2.0 platform, a generic-purpose IoT Platform that can acquire, process, and visualise data using methods adequate for both real-time processing and long-term Machine Learning (ML)-based analysis. Our goal is to develop a large-scale system that can be applied to multiple real-world scenarios and is potentially deployable on private clouds for multiple verticals. Our approach relies on extensive service containerisation, and we present the different design choices, technical challenges, and solutions found while building our own IoT platform. We validate this platform supporting two very distinct IoT projects (750 physical devices), and we analyse scaling issues within the platform components."
446,unknown,10.1109/indin45523.2021.9557363,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9557363/,2021-07-23 00:00:00,multi-robot multiple camera people detection and tracking in automated warehouses,"In this work a multi-robot system is presented for people detection and tracking in automated warehouses. Each Automated Guided Vehicle (AGV) is equipped with multiple RGB cameras that can track the workers’ current locations on the floor thanks to a neural network that provides human pose estimation. Based on the local perception of the environment each AGV can exploit information about the tracked people for self-motion planning or collision avoidance.Additionally, data collected from each robot contributes to a global people detection and tracking system. A warehouse central management software fuses information received from all AGVs into a map of the current locations of workers. The estimated locations of workers are sent back to the AGVs to prevent potential collision. The proposed method is based on two-level hierarchy of Kalman filters. Experiments performed in a real warehouse show the viability of the proposed approach."
447,unknown,10.1109/iciscae48440.2019.221655,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9075573/,2019-09-30 00:00:00,the construction of portrait identification tracking system based on mask r-cnn,"A portrait identification and tracking system with strong real-time performance, good flexibility and controllable cost is designed and implemented in the paper. Firstly, Mask R-CNN neural network is used to extract the features of the target, and the COCO dataset is used to train and establish the portrait data model. As a result, accuracy of the portrait recognition is improved. Then, a portrait tracking system including monocular camera, data acquisition module, data processing module, steering gear and control system is built. And the ""Raspberry Pi"" control method is used to control the MG955 steering gear group. Finally, the recognition and tracking of characters can be realized through wired, WIFI, Bluetooth and other ways, which improves the universality of the system. The designed system has simple structure, complete functions and can be used for automatic aiming and tracking of other objects. The modular system can also be used for unmanned aerial vehicles, robots and other platforms."
448,unknown,http://arxiv.org/abs/2003.13174v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2003.13174v1,2020-03-30 00:00:00,"mip an ai distributed architectural model to introduce cognitive
  computing capabilities in cyber physical systems (cps)","This paper introduces the MIP Platform architecture model, a novel AI-based
cognitive computing platform architecture. The goal of the proposed application
of MIP is to reduce the implementation burden for the usage of AI algorithms
applied to cognitive computing and fluent HMI interactions within the
manufacturing process in a cyber-physical production system. The cognitive
inferencing engine of MIP is a deterministic cognitive module that processes
declarative goals, identifies Intents and Entities, selects suitable actions
and associated algorithms, and invokes for the execution a processing logic
(Function) configured in the internal Function-as-aService or Connectivity
Engine. Constant observation and evaluation against performance criteria assess
the performance of Lambda(s) for many and varying scenarios. The modular design
with well-defined interfaces enables the reusability and extensibility of FaaS
components. An integrated BigData platform implements this modular design
supported by technologies such as Docker, Kubernetes for virtualization and
orchestration of the individual components and their communication. The
implementation of the architecture is evaluated using a real-world use case
later discussed in this paper."
449,unknown,10.1109/icassp.1993.319249,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/319249/,1993-04-30 00:00:00,gisting conversational speech in real time,"The authors describe additions and modifications to a prototype system for analyzing air traffic contol (ATC) communication. The primary goal of the effort was to achieve real-time performance. This involved both system architectural and algorithmic modifications. The task of the system is to extract the gist of activity as it is monitored. In the ATC domain this involves identifying those flights that are present and classifying each flight as a departure, arrival, or other. The system combines a variety of techniques from speaker-identification, speech recognition, natural-language processing, and artificial intelligence. Continuous processing versions of the algorithms have been constructed and it has been demonstrated that real-time performance is possible by distributing the processing over a small number of workstations. To accomplish this task, a flexible software task-construction tool that allows simple specification of complex systems, supporting both dataflow and client/server models, has been developed.<>"
450,unknown,10.1109/icactm.2019.8776776,IEEE,ieeexplore,finance,'finance' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8776776/,2019-04-26 00:00:00,strengthening people analytics through wearable iot device for real-time data collection,"The organizations are evolving through technological advancements and building a novel tech- culture by implementing the People Analytics, Machine Learning, IOT and Artificial Intelligence. The digitalization of both structured and unstructured data has helped organizations to predict future events across all the functional areas, mainly - Marketing, Operations, Finance, Productions, and Human resource management. People analytics is leveraging the Internet of Things (IoT) for improving business decisions related to acquisition, motivation, utilization, and retention of talented employees in the organizations. This paper attempts to study how organizations are leveraging technology and tools to radically transform the way HR and business leaders are using people data. In this paper, we have discussed a framework for real-time data analysis with the help of wearable IoT devices. We have considered wearable IOT technology supported wristband which will be enabled with movement sensors, GPS, accelerometer, heart rate related sensor, thermometer sensor for real time data collection. These sensors continuously collect data and can send to the nearby designed system as per the communication network. This data can give much-hidden insight about an employee. The paper suggests a model for the Human Resource Management system through wearable IoT device to provide unbiased and transparent results."
451,excluded,10.1109/ic-etite47903.2020.450,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9077792/,2020-02-25 00:00:00,an iot based smart water quality monitoring system using cloud,"The Internet of Things (IoT) is the network of physical devices, vehicles, home appliances, and other items embedded with electronics, software, sensors, actuators and connectivity which enables these things to connect and exchange data. The number of IoT devices has increased 31% year-over-year to 8.4 billion in 2017 and it is estimated that there will be 30 billion devices by 2020. Water pollution is a major environmental problem in India. The largest source of water pollution in India is untreated sewage. Other sources of pollution include agricultural runoff and unregulated small scale industry that results in polluting, most of the rivers, lakes and surface water in India. In this paper, An IoT Based Smart Water Quality Monitoring System using Cloud and Deep Learning is proposed to monitor the quality of the water in water-bodies. In conventional systems, the monitoring process involves the manual collection of sample water from various regions, followed by laboratory testing and analysis. This process is ineffective, as this process is arduous and time-consuming and it does not provide real-time results. The quality of water should be monitored continuously, to ensure the safe supply of water from any water bodies and water resources. Hence, the design and development of a low-cost system for real-time monitoring of water quality using the Internet of Things (IoT) is essential. Monitoring water quality in water bodies using Internet of Things (IoT) helps in combating environmental issues and improving the health and living standards of all living things. The proposed system monitors the quality of water relentlessly with the help of IoT devices, such as, NodeMCU. The in-built Wi-Fi module is attached in NodeMCU which enables internet connectivity transfers the measured data from sensors to the Cloud. The prototype is designed in such a way that it can monitor the number of pollutants in the water. Multiple sensors are used to measure various parameters to assess the quality of water from water bodies. The results are stored in the Cloud, deep learning techniques are used to predict whether the water suitable or not."
452,unknown,10.1109/icesc48915.2020.9155625,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9155625/,2020-07-04 00:00:00,fire and gun violence based anomaly detection system using deep neural networks,"Real-time object detection to improve surveillance methods is one of the sought-after applications of Convolutional Neural Networks (CNNs). This research work has approached the detection of fire and handguns in areas monitored by cameras. Home fires, industrial explosions, and wildfires are a huge problem that cause adverse effects on the environment. Gun violence and mass shootings are also on the rise in certain parts of the world. Such incidents are time-sensitive and can cause a huge loss to life and property. Hence, the proposed work has built a deep learning model based on the YOLOv3 algorithm that processes a video frame-by-frame to detect such anomalies in real-time and generate an alert for the concerned authorities. The final model has a validation loss of 0.2864, with a detection rate of 45 frames per second and has been benchmarked on datasets like IMFDB, UGR, and FireNet with accuracies of 89.3%, 82.6% and 86.5% respectively. Experimental result satisfies the goal of the proposed model and also shows a fast detection rate that can be deployed indoor as well as outdoors."
453,excluded,http://arxiv.org/abs/1908.11319v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1908.11319v2,2019-08-29 00:00:00,"machine learning and the internet of things enable steam flood
  optimization for improved oil production","Recently developed machine learning techniques, in association with the
Internet of Things (IoT) allow for the implementation of a method of increasing
oil production from heavy-oil wells. Steam flood injection, a widely used
enhanced oil recovery technique, uses thermal and gravitational potential to
mobilize and dilute heavy oil in situ to increase oil production. In contrast
to traditional steam flood simulations based on principles of classic physics,
we introduce here an approach using cutting-edge machine learning techniques
that have the potential to provide a better way to describe the performance of
steam flood. We propose a workflow to address a category of time-series data
that can be analyzed with supervised machine learning algorithms and IoT. We
demonstrate the effectiveness of the technique for forecasting oil production
in steam flood scenarios. Moreover, we build an optimization system that
recommends an optimal steam allocation plan, and show that it leads to a 3%
improvement in oil production. We develop a minimum viable product on a cloud
platform that can implement real-time data collection, transfer, and storage,
as well as the training and implementation of a cloud-based machine learning
model. This workflow also offers an applicable solution to other problems with
similar time-series data structures, like predictive maintenance."
454,unknown,10.1016/j.enbuild.2018.12.034,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85059816255,2019-02-15,"intellimav: a cloud computing measurement and verification 2.0 application for automated, near real-time energy savings quantification and performance deviation detection","Energy conservation measures (ECMs) are implemented in all sectors with the objective of improving the efficiency with which energy is consumed. Measurement and verification (M&V) is required to verify the performance of every ECM to ensure its successful implementation and operation. The methodologies implemented to achieve this are currently evolving to a more dynamic state, known as measurement and verification 2.0, through the use of automated and advanced analytics. The primary barrier to the adoption of M&V 2.0 practices are the tools available to practitioners. This paper aims to populate the knowledge gap in the industrial buildings sector by presenting a novel cloud computing-based application, IntelliMaV, that applies advanced machine learning techniques on large datasets to automatically verify the performance of ECMs in near real-time. Additionally, a performance deviation detection system is incorporated, ensuring persistence of savings beyond the typical period of analysis in M&V.
                  IntelliMaV allows M&V practitioners to quantify energy savings with minimum levels of uncertainty by applying powerful analytics to data readily available in industrial facilities. The use of a cloud computing-based architecture reduces the resources required on-site and decreases the time required to train the baseline energy model through the use of parallel processing. The robust nature of the application ensures it is applicable across the broad spectrum of ECMs in the industrial buildings sector. A case study carried out in a large biomedical manufacturing facility demonstrates the ease of use of the application and the benefits realised through its adoption. The energy savings from an ECM were calculated to be 2,353,225 kWh/yr with 25.5% uncertainty at a 90% confidence interval."
455,unknown,10.23919/apnoms52696.2021.9562691,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9562691/,2021-09-10 00:00:00,real-time license plate recognition and vehicle tracking system based on deep learning,"Traditional license plate recognition technology mostly uses traditional image processing methods to find out the characteristics of the license plate, and then crop and recognize the characters. The process needs to be modified due to the different environments, scenes and conditions. In recent years, many studies have implemented license plate and character recognition by using deep learning algorithms. Although it has a good recognition accuracy, the calculation speed still cannot reach the level of real-time recognition. This research proposes a real-time license plate recognition system based on YOLOv3, which uses deep learning model to realize the vehicle license plate recognition, lane identification and vehicle trajectory tracking. In this study, a web-based platform is established to present the result of license plate recognition and trajectory, and the streaming roadside video in the campus. In the platform, license plates of driving vehicles can be identified in real-time, and the user can search and track specific vehicle intuitively. In the experiment, the average accuracy of the system performs 84.3&#x0025; in real-time license plate recognition, and 100&#x0025; in lane identification. The system can process in 40 FPS, which can meet the level of real-time system. In the future, the system can cooperate with traffic access control in campus or community to improve the efficiency of traffic control."
456,unknown,10.2118/207732-ms,"Day 3 Wed, November 17, 2021",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/f9e88fe7795b80a79afdd884e79e8d075df7e5f2,2021-01-01 00:00:00,autonomous corrosion and scale management in electric submersible pump wells,"
 Producers find a considerable amount of their operating expense (OPEX) comes from managing risks associated with corrosion and scale. Monitoring and chemical adjustment workflows are typically manual, and performed at low frequencies, leading to delays in event detection. As a result, the potential for negative events such as production shutdowns and well failures increase. This project's scope integrates chemistry domain experience with edge analytics, machine learning models, and intelligent equipment, to transform manual processes into an autonomous solution. The goal is to optimize operations, reduce well failures and workover costs, and maximize production. This solution is currently deployed in an oilfield, that has been historically challenged with a high number of electric submersible pump (ESP) failures due to corrosion and scale that resulted in significant production losses and unforeseen workover costs. The designed digital architecture supports autonomous management of scale and corrosion through remote monitoring and automated chemical injection. Real-time data is acquired from connected equipment, processed in an edge device running artificial intelligence, and autonomously sent to chemical pumps. Data from sensors, connected devices, and models are visualized in cloud applications, or integrated into existing client systems for end user analysis and full visibility of the entire process. The results show highly accurate models, precise chemical injection, and a reduction of well failures."
457,excluded,10.1109/metroind4.0iot51437.2021.9488536,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9488536/,2021-06-09 00:00:00,cnn-lstm neural network applied for thermal infrared underground water leakage,"In this paper is proposed a methodological approach for the detection of leaks in water pipelines. The approach is based on the use of Infrared Thermography (IRT) for the real time monitoring, and of Artificial Neural Networks (ANNs) for the identification of potential leaks not easy visible by IRT. The input data source consists of radiometric data processed by Convolutional Neural Networks (CNNs) providing as output information about the presence or absence of the water leakages. A preliminary study has been carried out about a fixed monitoring station created to identify leaks in underground pipelines. In addition, has been implemented a platform to remotely acquire the thermograms and to analyze them by CNN networks detecting leakages, combined with Long Short-Term Memory (LSTM) neural networks, and image filtering algorithms, such as image segmentation and active contour snake approach. The LSTM network allows the prediction and calculation of the propagation trend of the leak plume. Finally, image filtering improves the visualization of leaks as it allows to draw the contours of the pixel clusters representing the leakages areas in the thermograms. The work was developed within the research framework of an industrial project. The proposed approach is suitable also for oil spill and gas leakages detections."
458,unknown,10.1109/coins51742.2021.9524186,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9524186/,2021-08-25 00:00:00,an edge ai based robot system for search and rescue applications,"In this work, we propose an edge AI based robot system that contains drones and multi-legged robots for search and rescue applications. To accurately search for survivors in real-time, we integrate Tiny-YOLO into the drone design. Instead of adopting a microprocessor usually used in a robot, the FPGA device is adopted as the main hardware computing architecture of the multi-legged robot. A resource-efficient quantized neural network is implemented as a hardware module and integrated into the multi-legged robot for real-time detection. When a survivor is detected from robots, the corresponding information about GPS and the triangulation localization is thus delivered to the edge server. Then, rescuers can receive the notification message from the edge server by using their mobile devices. For survivor detection, experiments show the drone and the multi-legged robot can achieve 2.164 fps and 2.404 fps, respectively."
459,unknown,http://arxiv.org/abs/2105.15041v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2105.15041v1,2021-05-31 00:00:00,"scorpion detection and classification systems based on computer vision
  and deep learning for health security purposes","In this paper, two novel automatic and real-time systems for the detection
and classification of two genera of scorpions found in La Plata city
(Argentina) were developed using computer vision and deep learning techniques.
The object detection technique was implemented with two different methods, YOLO
(You Only Look Once) and MobileNet, based on the shape features of the
scorpions. High accuracy values of 88% and 91%, and high recall values of 90%
and 97%, have been achieved for both models, respectively, which guarantees
that they can successfully detect scorpions. In addition, the MobileNet method
has been shown to have excellent performance to detect scorpions within an
uncontrolled environment and to perform multiple detections. The MobileNet
model was also used for image classification in order to successfully
distinguish between dangerous scorpion (Tityus) and non-dangerous scorpion
(Bothriurus) with the purpose of providing a health security tool. Applications
for smartphones were developed, with the advantage of the portability of the
systems, which can be used as a help tool for emergency services, or for
biological research purposes. The developed systems can be easily scalable to
other genera and species of scorpions to extend the region where these
applications can be used."
460,excluded,http://arxiv.org/abs/2005.08076v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2005.08076v1,2020-05-16 00:00:00,"a deep learning based wearable healthcare iot device for ai-enabled
  hearing assistance automation","With the recent booming of artificial intelligence (AI), particularly deep
learning techniques, digital healthcare is one of the prevalent areas that
could gain benefits from AI-enabled functionality. This research presents a
novel AI-enabled Internet of Things (IoT) device operating from the ESP-8266
platform capable of assisting those who suffer from impairment of hearing or
deafness to communicate with others in conversations. In the proposed solution,
a server application is created that leverages Google's online speech
recognition service to convert the received conversations into texts, then
deployed to a micro-display attached to the glasses to display the conversation
contents to deaf people, to enable and assist conversation as normal with the
general population. Furthermore, in order to raise alert of traffic or
dangerous scenarios, an 'urban-emergency' classifier is developed using a deep
learning model, Inception-v4, with transfer learning to detect/recognize
alerting/alarming sounds, such as a horn sound or a fire alarm, with texts
generated to alert the prospective user. The training of Inception-v4 was
carried out on a consumer desktop PC and then implemented into the AI based IoT
application. The empirical results indicate that the developed prototype system
achieves an accuracy rate of 92% for sound recognition and classification with
real-time performance."
461,excluded,10.1109/tii.2014.2302638,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6725615/,2014-05-01 00:00:00,an integrated system for regional environmental monitoring and management based on internet of things,"Climate change and environmental monitoring and management have received much attention recently, and an integrated information system (IIS) is considered highly valuable. This paper introduces a novel IIS that combines Internet of Things (IoT), Cloud Computing, Geoinformatics [remote sensing (RS), geographical information system (GIS), and global positioning system (GPS)], and e-Science for environmental monitoring and management, with a case study on regional climate change and its ecological effects. Multi-sensors and web services were used to collect data and other information for the perception layer; both public networks and private networks were used to access and transport mass data and other information in the network layer. The key technologies and tools include real-time operational database (RODB); extraction–transformation–loading (ETL); on-line analytical processing (OLAP) and relational OLAP (ROLAP); naming, addressing, and profile server (NAPS); application gateway (AG); application software for different platforms and tasks (APPs); IoT application infrastructure (IoT-AI); GIS and e-Science platforms; and representational state transfer/Java database connectivity (RESTful/JDBC). Application Program Interfaces (APIs) were implemented in the middleware layer of the IIS. The application layer provides the functions of storing, organizing, processing, and sharing of data and other information, as well as the functions of applications in environmental monitoring and management. The results from the case study show that there is a visible increasing trend of the air temperature in Xinjiang over the last 50 years (1962–2011) and an apparent increasing trend of the precipitation since the early 1980s. Furthermore, from the correlation between ecological indicators [gross primary production (GPP), net primary production (NPP), and leaf area index (LAI)] and meteorological elements (air temperature and precipitation), water resource availability is the decisive factor with regard to the terrestrial ecosystem in the area. The study shows that the research work is greatly benefited from such an IIS, not only in data collection supported by IoT, but also in Web services and applications based on cloud computing and e-Science platforms, and the effectiveness of monitoring processes and decision-making can be obviously improved. This paper provides a prototype IIS for environmental monitoring and management, and it also provides a new paradigm for the future research and practice; especially in the era of big data and IoT."
462,excluded,10.1007/s00521-020-05189-8,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00521-020-05189-8,2020-12-01 00:00:00,anomaly detection via blockchained deep learning smart contracts in industry 4.0,"The complexity of threats in the ever-changing environment of modern industry is constantly increasing. At the same time, traditional security systems fail to detect serious threats of increasing depth and duration. Therefore, alternative, intelligent solutions should be used to detect anomalies in the operating parameters of the infrastructures concerned, while ensuring the anonymity and confidentiality of industrial information. Blockchain is an encrypted, distributed archiving system designed to allow for the creation of real-time log files that are unequivocally linked. This ensures the security and transparency of transactions. This research presents, for the first time in the literature, an innovative Blockchain Security Architecture that aims to ensure network communication between traded Industrial Internet of Things devices, following the Industry 4.0 standard and based on Deep Learning Smart Contracts . The proposed smart contracts are implementing (via computer programming) a bilateral traffic control agreement to detect anomalies based on a trained Deep Autoencoder Neural Network. This architecture enables the creation of a secure distributed platform that can control and complete associated transactions in critical infrastructure networks, without the intervention of a single central authority. It is a novel approach that fuses artificial intelligence in the Blockchain, not as a supportive framework that enhances the capabilities of the network, but as an active structural element, indispensable and necessary for its completion."
463,excluded,10.1007/978-3-030-86261-9_18,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-86261-9_18,2022-01-01 00:00:00,service-oriented architecture for data-driven fault detection,"Predictive maintenance approaches result in shorter interventions, less downtime and less maintenance and production costs, leading to more sustainable manufacturing and increasing earnings. Machine learning and data mining approaches are now popular in this field, producing data-driven models that can provide insights regarding equipment behavior, identify anomalies, and generate predictions that can be used to support decision-making processes. In this paper, we propose a service-oriented architecture that adapts a predictive maintenance reference architecture to the domain of the flexible packaging market. Focusing on the data analytics services required for predictive maintenance, the proposed architecture was assessed by conducting a case-study using real-world data. To detect faults in the company’s coextrusion film blowing machines, we devised a predictive maintenance methodology whereby the isolation forest algorithm is used to detect potentially anomalous data points and the anomaly threshold is defined by computing the interquartile range of anomaly scores. To compensate for the fact that single anomalies are unlikely to be of significance, when deploying the isolation forest model in real time the percentage of anomalies detected within a given time interval is monitored. An alarm is issued by the system should that percentage exceed a specific cutoff. Experimental results show the described approach is successful in distinguishing anomalous data from normal data in non-stationary time-series."
464,unknown,10.1109/ehb52898.2021.9657633,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9657633/,2021-11-19 00:00:00,modular telepresence robot for distance medical education,"In special conditions such as pandemics, wars or any other problem can lead to the physical suspension of the education system including courses in medical schools. Telepresence robotics is a solution through which courses and laboratories can be held remotely where medical students can interact physically, through robots, with teaching or laboratory equipment, thus actively participating in the educational process. Although there are various variants of telepresence robots, this paper presents the study of the possibility to design and make a telepresence robot kit, low cost, containing modules easy to assemble and adapted for various teaching situations, respectively for the development of remote laboratory experiments. The proposed kit has mechanical components that can be printed on a 3D printer. The electronic components are compatible with the Arduino environment and include three modules: sensory, navigation and a communications and artificial intelligence module. The software elements are also modularly designed and allow adaptation to various study and laboratory situations. Robotic elements compatible with the Arduino environment such as arms or lifting or throwing systems can be added to the robot. The results of the experimental measurements showed that the robot has acceptable travel errors for unpretentious applications, but to increase its accuracy in laboratory conditions, the software was programmed so that, with the help of artificial intelligence, the robot can perform real-time measurements and correct automatically. This mechanism is useful in medical applications such as assisting and guiding blind patients."
465,excluded,10.1109/icoris52787.2021.9649589,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9649589/,2021-10-26 00:00:00,design and implementation of internet of things and cloud technology in flood risk mitigation,"In the last decade, floods have frequently occurred in many big cities due land use mismanagement, and poor water management and control. With the climate change, the flood disaster becomes more intensive and frequent causing a lot of damages on public infrastructure and human casualties. The rapid development of advanced information technology has contributed on how to manage and control the flood events through integrated information system and advanced flood modelling techniques. This paper focuses on designing and implementing Internet of Things (IoT) and Cloud Technology (CT) for supporting the flood risk mitigation system in the area of Jabodetabek. The IoT based water sensor network with automated data acquisition system were implemented and simulated in the upstream and downstream water gates at Katulampa and Manggarai, respectively. The integration of IoT system based on Arduino and Raspberry Pi, and AWS Cloud allows for better real-time data collection and management that could be used for accurate flood model forecasting with data assimilation and machine learning techniques. The presentation and dissemination of flood data and modelling results using IoT and CT systems through web/mobile application could support the flood risk mitigation and preparedness."
466,unknown,10.2523/iptc-21968-ms,"Day 2 Tue, February 22, 2022",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1e3c9858a7d6dd1bbc0a36d0b3edf901a6141c00,2022-01-01 00:00:00,autonomous viscosity/density sensing system for drilling edge-computing system,"
 Current mud monitoring practices are limited due to their reliance on manual measurements such as funnel viscometers, weight balances, or basic field rheometers. These manual practices impose restraints on the quantity and quality of the available data that are essential to ensure optimal and safe drilling operations. In this study, we introduce a new autonomous mud viscosity/density system based on an electromechanical tuning fork resonator. The system was integrated into an edge-computing system for improved data collection and deployment of machine learning models. The system was tested during a live drilling campaign. The viscosity/density sensor is based on an electromechanical tuning fork resonator. The sensor was integrated into a submergible housing for in-tank measurements. Two systems were developed for simultaneous measurements at inflow (possum belly) and outflow (suction pit). The data from the two systems were broadcast wirelessly to the central computer room at the rig for real-time display and data aggregation by the edge-computing system for the development of time-series analysis models using machine learning. During initial field testing, data from a single sensor were collected for various hours at a rate less than a sample per second. The test allowed for continuous monitoring of the mud consistency not accessible by current measurement practices. The data demonstrated the potential to perform real-time calculation and display of drilling parameters and to detect anomalies in the fluid that might be indicative of developing operational problems, which would enable the instrument to be used as an early-warning system and real-time calculation of drilling parameters. The system detailed here provides an essential building block to enable drilling automation. The robustness and compactness of the instrument allow it to be installed at various points in the mud circulation system for the generation of large data sets that can be processed using modern analytics algorithms in an edge-computing framework."
467,unknown,10.1007/978-981-19-0707-4_23,Springer,springer,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-981-19-0707-4_23,2022-01-01 00:00:00,development of a robust framework for surveillance vehicle,"Advances in Surveillance Vehicles have opened forums for applications, one of which is military supervision. Its adaptability and monitoring of a highly secret area allow it to swiftly identify targets and any safety issues. Identifying targets or in simple words detecting them with their 3-dimensional characteristics, gives us a much-evolved vehicle. The goal of this research is to recognize items in rural regions that are highly complicated, as well as to evaluate precision and computational complexity, with the region of relevance presented in a 3-dimensional image for clarity. A low-power integrated Graphics Processing Time (NVIDIA Jetson) is chosen for this objective since it permits numerous neural networks to operate at almost the same instant as well as a computer vision methodology is used for picture identification. Deep neural networks with condition-of-the-art characteristics like Faster-RCNN and SSD are implemented in TensorFlow and their performance is evaluated on real-world circumstances, where Faster-RCNN proves to be the most suitable choice. Using an imaging system like the laser scanner or any 3-dimensional modelling application to recreate 3-dimensional models of different items is likewise high in price and needs a high degree of expertise. While a much handy photograph-based 3-dimensional modelling software is an affordable choice that will implement the task with relative ease. MeshLab and COLMAP have proved to provide a relatively accurate 3-dimensional model of the target region. So, the project combines all the above-mentioned techniques to acquire a high-end vehicle that can multi-task."
468,unknown,10.1038/s41591-021-01384-9,Nature,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s41591-021-01384-9,2021-07-01 00:00:00,deep learning of hiv field-based rapid tests,"Although deep learning algorithms show increasing promise for disease diagnosis, their use with rapid diagnostic tests performed in the field has not been extensively tested. Here we use deep learning to classify images of rapid human immunodeficiency virus (HIV) tests acquired in rural South Africa. Using newly developed image capture protocols with the Samsung SM-P585 tablet, 60 fieldworkers routinely collected images of HIV lateral flow tests. From a library of 11,374 images, deep learning algorithms were trained to classify tests as positive or negative. A pilot field study of the algorithms deployed as a mobile application demonstrated high levels of sensitivity (97.8%) and specificity (100%) compared with traditional visual interpretation by humans—experienced nurses and newly trained community health worker staff—and reduced the number of false positives and false negatives. Our findings lay the foundations for a new paradigm of deep learning–enabled diagnostics in low- and middle-income countries, termed REASSURED diagnostics 1 , an acronym for real-time connectivity, ease of specimen collection, affordable, sensitive, specific, user-friendly, rapid, equipment-free and deliverable. Such diagnostics have the potential to provide a platform for workforce training, quality assurance, decision support and mobile connectivity to inform disease control strategies, strengthen healthcare system efficiency and improve patient outcomes and outbreak management in emerging infections. In a pilot field study conducted in rural South Africa, deep learning algorithms can accurately classify rapid HIV tests as positive or negative, highlighting the potential of deep learning–enabled diagnostics for use in low- and middle-income countries."
469,included,http://arxiv.org/abs/1911.06633v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1911.06633v1,2019-11-15 00:00:00,"healthfog: an ensemble deep learning based smart healthcare system for
  automatic diagnosis of heart diseases in integrated iot and fog computing
  environments","Cloud computing provides resources over the Internet and allows a plethora of
applications to be deployed to provide services for different industries. The
major bottleneck being faced currently in these cloud frameworks is their
limited scalability and hence inability to cater to the requirements of
centralized Internet of Things (IoT) based compute environments. The main
reason for this is that latency-sensitive applications like health monitoring
and surveillance systems now require computation over large amounts of data
(Big Data) transferred to centralized database and from database to cloud data
centers which leads to drop in performance of such systems. The new paradigms
of fog and edge computing provide innovative solutions by bringing resources
closer to the user and provide low latency and energy-efficient solutions for
data processing compared to cloud domains. Still, the current fog models have
many limitations and focus from a limited perspective on either accuracy of
results or reduced response time but not both. We proposed a novel framework
called HealthFog for integrating ensemble deep learning in Edge computing
devices and deployed it for a real-life application of automatic Heart Disease
analysis. HealthFog delivers healthcare as a fog service using IoT devices and
efficiently manages the data of heart patients, which comes as user requests.
Fog-enabled cloud framework, FogBus is used to deploy and test the performance
of the proposed model in terms of power consumption, network bandwidth,
latency, jitter, accuracy and execution time. HealthFog is configurable to
various operation modes that provide the best Quality of Service or prediction
accuracy, as required, in diverse fog computation scenarios and for different
user requirements."
470,excluded,10.1109/r10-htc.2018.8629835,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8629835/,2018-12-08 00:00:00,foody - smart restaurant management and ordering system,"Customers play a vital role in the contemporary food industry when determining the quality of the restaurant and its food. Restaurants give considerable attention to customers’ feedback about their service, since the reputation of the business depends on it. Key factors of evaluating customer satisfaction are, being able to deliver the services effectively to lessen the time of consumption, as well as maintaining a high quality of service. In most cases of selecting a prominent restaurant, customers focus on their choice of favorite food in addition to available seating and space options. Long waiting times and serving the wrong order is a common mistake that happens in every restaurant that eventually leads to customer dissatisfaction. Objectives of this online application “Foody” is to address these deficiencies and provide efficient and accurate services to the customer, by providing unique menus to each customer considering their taste. This concept is implemented as a mobile application using latest IT concepts such as Business Intelligence, Data Mining, Predictive Analysis and Artificial Intelligence. This includes graphics and 3D modeling that provide existent physical information related to food such as colors, sizes and further user can view the ingredients of the meal as well as the available tables. In addition, the app shows the real-time map to the restaurant. Current table reservation status is indicated by the color change of the table. Unique food recommendation and it’s order for each customer is generated by analyzing their social media information and the system notifies the customer the wait time by calculating it. Preparation of food and allocation is done subjectively. The expected outcome of the research is to develop a fully automated restaurant management system with the mentioned features as well as to avoid confusions between orders, provide better view of food and allow the customer to choose the menu according to their taste in a minimum time."
471,unknown,10.1016/j.puhe.2016.01.006,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84957716366,2016-05-01,id-viewer: a visual analytics architecture for infectious diseases surveillance and response management in pakistan,"Objectives
                  Globally, disease surveillance systems are playing a significant role in outbreak detection and response management of Infectious Diseases (IDs). However, in developing countries like Pakistan, epidemic outbreaks are difficult to detect due to scarcity of public health data and absence of automated surveillance systems. Our research is intended to formulate an integrated service-oriented visual analytics architecture for ID surveillance, identify key constituents and set up a baseline for easy reproducibility of such systems in the future.
               
                  Study design
                  This research focuses on development of ID-Viewer, which is a visual analytics decision support system for ID surveillance. It is a blend of intelligent approaches to make use of real-time streaming data from Emergency Departments (EDs) for early outbreak detection, health care resource allocation and epidemic response management.
               
                  Methods
                  We have developed a robust service-oriented visual analytics architecture for ID surveillance, which provides automated mechanisms for ID data acquisition, outbreak detection and epidemic response management. Classification of chief-complaints is accomplished using dynamic classification module, which employs neural networks and fuzzy-logic to categorize syndromes. Standard routines by Center for Disease Control (CDC), i.e. c1-c3 (c1-mild, c2-medium and c3-ultra), and spatial scan statistics are employed for detection of temporal and spatio-temporal disease outbreaks respectively. Prediction of imminent disease threats is accomplished using support vector regression for early warnings and response planning. Geographical visual analytics displays are developed that allow interactive visualization of syndromic clusters, monitoring disease spread patterns, and identification of spatio-temporal risk zones.
               
                  Results
                  We analysed performance of surveillance framework using ID data for year 2011–2015. Dynamic syndromic classifier is able to classify chief-complaints to appropriate syndromes with high classification accuracy. Outbreak detection methods are able to detect the ID outbreaks in start of epidemic time zones. Prediction model is able to forecast dengue trend for 20 weeks ahead with nominal normalized root mean square error of 0.29. Interactive geo-spatiotemporal displays, i.e. heat-maps, and choropleth are shown in respective sections.
               
                  Conclusion
                  The proposed framework will set a standard and provide necessary details for future implementation of such a system for resource-constrained regions. It will improve early outbreak detection attributable to natural and man-made biological threats, monitor spatio-temporal epidemic trends and provide assurance that an outbreak has, or has not occurred. Advanced analytics features will be beneficial in timely organization/formulation of health management policies, disease control activities and efficient health care resource allocation."
472,excluded,10.1145/3460418.3479277,'Association for Computing Machinery (ACM)',core,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-01-01 00:00:00,"air quality sensor network data acquisition, cleaning, visualization, and analytics: a real-world iot use case","Monitoring and analyzing air quality is of primary importance to encourage more sustainable lifestyles and plan corrective actions. This paper presents the design and end-To-end implementation1 of a real-world urban air quality data collection and analytics use case which is a part of the TRAFAIR (Understanding Traffic Flows to Improve Air Quality) European project [1, 2]. This implementation is related to the project work done in Modena city, Italy, starting from distributed low-cost multi-sensor IoT devices installation, LoRa network setup, data collection at LoRa server database, ML-based anomaly measurement detection plus cleaning, sensor calibration, central control and visualization using designed SenseBoard [3]"
473,excluded,10.1109/access.2018.2877523,IEEE Access,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/fd9d86dde5f536e3f549d7c42c79dff22317f194,2018-01-01 00:00:00,smart audio sensors in the internet of things edge for anomaly detection,"Everyday objects are becoming smart enough to directly connect to other nearby and remote objects and systems. These objects increasingly interact with machine learning applications that perform feature extraction and model inference in the cloud. However, this approach poses several challenges due to latency, privacy, and dependency on network connectivity between data producers and consumers. To alleviate these limitations, computation should be moved as much as possible towards the IoT edge, that is on gateways, if not directly on data producers. In this paper, we propose a design framework for smart audio sensors able to record and pre-process raw audio streams, before wirelessly transmitting the computed audio features to a modular IoT gateway. In this paper, an anomaly detection algorithm executed as a micro-service is capable of analyzing the received features, hence detecting audio anomalies in real-time. First, to assess the effectiveness of the proposed solution, we deployed a real smart environment showcase. More in detail, we adopted two different anomaly detection algorithms, namely Elliptic Envelope and Isolation Forest, that were purposely trained and deployed on an affordable IoT gateway to detect anomalous sound events happening in an office environment. Then, we numerically compared both the deployments, in terms of end-to-end latency and gateway CPU load, also deriving some ideal capacity bounds."
474,unknown,10.1109/aiiot54504.2022.9817374,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9817374/,2022-06-09 00:00:00,violence detection using computer vision approaches,"Violent crime has always been a major social problem. The rise of violent behavior in public areas can be attributed to a variety of factors. Greed, frustration, and hostility among individuals, as well as social and economic anxieties, are the primary causes of increased violence. It is critical to protect our possessions, as well as our lives, from threats such as robbery or homicide. It is impossible to prevent crime and violent acts unless brain signals are studied and a certain pattern deduced from criminal ideas is detected in real-time. Due to its technological viability, it has yet to be realized. However, We can identify violent activity in public spaces by using the concepts of computer vision (a subfield of deep learning) technology. The goal of this project is to build a real-time violent activity monitoring system that will be capable of detecting violence very quickly and efficiently. The public of any city can benefit from it, as it will allow the people of the law enforcement department to take necessary actions to prevent violent activities. When the system is implemented, it will be able to detect the speed of the movements of people and their distances from other people walking in public places by using cameras. The system will mainly detect the speed of hand and leg movements of a person who will be very close to another person. If anyone is identified as a violent maker, the server-side of the system will notify the people who will be responsible for preventing violence in a very short time. The system was built using the concepts of computer vision and neural networks. The system has been developed and tested initially on the personal computing devices of the system developers. This system is very easy to design and develop, making it very easy to use for any kind of public area surveillance. At the same time, the system gives its desired output due to its high accuracy."
475,excluded,10.1109/percomworkshops53856.2022.9767292,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9767292/,2022-03-25 00:00:00,you work we care: sitting posture assessment based on point cloud data,"The technology of 3D recognition is evolving rapidly, enabling novel applications towards human-centric intelligent environments. On top of these applications, tracking human sitting posture is essential for realizing human comfort and healthy environments. However, existing techniques rely on cameras or chair-attached sensors, which are privacy-invading or non-common technology in every environment. This paper introduces a ubiquitous portable technology for tracking sitting posture with a plug-and-play concept. Specifically, at the core of the proposed system, we leverage our proprietary LiDAR device to scan the human&#x2019;s sitting posture and render it in a privacy-keeping point cloud representation.The capture point cloud samples are leveraged to train an efficient deep neural network for enabling accurate recognition of the sitting posture. The proposed network significantly reduces the computational complexity of the model by learning special features that simplify the classification task. We implemented and evaluated the proposed system on nine different human postures in a real-world environment. The results show that it obtains an accuracy of 87% with a drastically reduced processing time."
476,excluded,10.1016/j.rsase.2022.100787,scopus,sciencedirect,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85132238263,2022-08-01,a novel drone-based system for accurate human temperature measurement and disease symptoms detection using thermography and ai,"The world continues to witness several waves of COVID-19 spread due to the emergence of new variants of the SARS-CoV-2 virus. Stopping the spread requires synergistic efforts that include the use of technologies such as unmanned aerial vehicles and machine learning. This paper presents a novel system for detecting disease symptoms from a distance using unmanned aerial vehicles equipped with thermal and visual image sensors. A hardware/software system that uses thermography to accurately calculate the skin temperature of targeted individuals using thermal cameras is developed. In addition, machine vision algorithms are developed to recognize human actions such as coughing and sneezing, which are paramount symptoms of respiratory infections. The proposed system is implemented and tested in outdoor environments. The results of experiments showed that the system can determine the skin temperature of multiple targeted individuals simultaneously with an error of less than 1 °C. The field experiments showed that the developed system is capable of simultaneously measuring the temperature of more than 10 individuals in less than 5 seconds. Just to give a perspective, it takes at least 3 seconds to measure one individual's temperature if this was done using traditional methods. Furthermore, the results showed that the system has accurately detected actions such as coughing and sneezing with almost 96% accuracy at a real-time performance of 28 frames/second."
477,unknown,10.1007/978-3-030-64912-8_3,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-64912-8_3,2020-01-01 00:00:00,low-cost optical tracking of soccer players,"Sports analytics are on the rise in European football, however, due to the high cost so far only the top tier leagues and championships have had the privilege of collecting high precision data to build upon. We believe that this opportunity should be available for everyone especially for youth teams, to develop and recognize talent earlier. We therefore set the goal of creating a low-cost player tracking system that could be applied in a wide base of football clubs and pitches, which in turn would widen the reach for sports analytics, ultimately assisting the work of scouts and coaches in general. In this paper, we present a low-cost optical tracking solution based on cheap action cameras and cloud-deployed data processing. As we build on existing research results in terms of methods for player detection, i.e., background-foreground separation, and for tracking, i.e., Kalman filter, we adapt those algorithms with the aim of sacrificing as least as possible on accuracy while keeping costs low. The results are promising: our system yields significantly better accuracy than a standard deep learning based tracking model at the fraction of its cost. In fact, at a cost of $2.4 per match spent on cloud processing of videos for real-time results, all players can be tracked with a 11-meter precision on average."
478,excluded,10.1109/icccnt51525.2021.9580153,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9580153/,2021-07-08 00:00:00,a deep learning model to snore detection using smart phone,"Snoring is a harsh and irritating sound in our throats as wind streams past loosen tissues, allowing the tissues to vibrate when we inhale. Once in a while, nearly everybody snores, enabling the tissues to vibrate as we breathe. Although everybody snores, sometimes it could very well be an ongoing problem for particular people. It may even indicate an actual medical condition now and then. Helpless rest will cause colossal physical, emotional, and monetary problems caused by snoring. Resting as an afterthought rather than dozing on either the back is a simple and popular solution to snoring. In this paper, a deep learning model for snore detection and implementation of the software on an Android smartphone is discussed. An Android app is produced to decide if the audio sample is snore/non-snore using the real-time audio piece captured using the unit's internal microphone. It also operates on any phone-connected audio input unit. The Snore Detection deep learning model has achieved an accuracy of 98%. A prototype app consisting of the input device (microphone), Android Smart Phone (for Android app deployment) have been developed and tested successfully."
479,unknown,10.1109/access.2020.2977846,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9020166/,2020-01-01 00:00:00,big data driven edge-cloud collaboration architecture for cloud manufacturing: a software defined perspective,"In the practice of cloud manufacturing, there still exist some major challenges, including: 1) cloud based big data analytics and decision-making cannot meet the requirements of many latency-sensitive applications on shop floors; 2) existing manufacturing systems lack enough reconfigurability, openness and evolvability to deal with shop-floor disturbances and market changes; and 3) big data from shop-floors and the Internet has not been effectively utilized to guide the optimization and upgrade of manufacturing systems. This paper proposes an open evolutionary architecture of the intelligent cloud manufacturing system with collaborative edge and cloud processing. Hierarchical gateways connecting and managing shop-floor <i>things</i> at the &#x201C;edge&#x201D; side are introduced to support latency-sensitive applications for real-time responses. Big data processed both at the gateways and in the cloud will be used to guide continuous improvement and evolution of edge-cloud systems for better performance. As software tools are becoming dominant as the &#x201C;brain&#x201D; of manufacturing control and decision-making, this paper also proposes a new mode - &#x201C;AI-Mfg-Ops&#x201D; (AI enabled Manufacturing Operations) with a supporting software defined framework, which can promote fast operation and upgrading of cloud manufacturing systems with smart monitoring-analysis-planning-execution in a closed loop. This research can contribute to the rapid response and efficient operation of cloud manufacturing systems."
480,unknown,10.1109/ises.2018.00039,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8719325/,2018-12-19 00:00:00,a smart sensor in the iomt for stress level detection,"Psychological stress is a sense of pressure which affects the physiological parameters in a person. In this paper a novel stress detection system, iStress is proposed which monitors stress levels through body temperature, rate of motion and sweat during physical activity. The implementation of the iStress system uses a neural network approach utilizing a Mamdani-type fuzzy logic controller with more than 150 instances as the model. The collected data are sent and stored in the cloud, which can help in real time monitoring of the person's stress level thereby reducing risks to health. This system consumes low energy although operating in real time. The proposed system has an ability to produce results with 97% accuracy, low system complexity and moderate cost."
481,unknown,http://arxiv.org/abs/2203.12111v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2203.12111v1,2022-03-23 00:00:00,"muscle vision: real time keypoint based pose classification of physical
  exercises","Recent advances in machine learning technology have enabled highly portable
and performant models for many common tasks, especially in image recognition.
One emerging field, 3D human pose recognition extrapolated from video, has now
advanced to the point of enabling real-time software applications with robust
enough output to support downstream machine learning tasks. In this work we
propose a new machine learning pipeline and web interface that performs human
pose recognition on a live video feed to detect when common exercises are
performed and classify them accordingly. We present a model interface capable
of webcam input with live display of classification results. Our main
contributions include a keypoint and time series based lightweight approach for
classifying a selected set of fitness exercises and a web-based software
application for obtaining and visualizing the results in real time."
482,excluded,10.2118/194136-ms,"Day 1 Tue, March 05, 2019",semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/4b868acf097dcc9357c6c633e03f37f8d12fea79,2019-01-01 00:00:00,building a rig state classifier using supervised machine learning to support invisible lost time analysis,"
 This paper covers the development of a key component of an internal system to report invisible lost time (ILT) metrics across drilling operations. Specifically this paper covers the development of a generalizable rig state engine based on the application of supervised machine learning. The same steps used in the creation of the production rig state engine are appled here to a smaller data set to demonstrate both the tractability of the problem and the methods used to create the rig state engine in the production system.
 The project objective was to provide efficiency and engineering metrics in a central repository covering operated regions. The system is designed to require minimal user configuration and management and provides both historic and near real time analysis to deliver a rich resource for offset comparison and benchmarking.
 Identifying rig-state is at the heart of every performance and engineering analysis system. This can be thought of as a machine learning classification problem. A large supervised learning set was constructed and used to train classification models which were compared for accuracy. A key success metric was the ability to generalise the selected model across different operations. Output from the rig-state classifier was then used to derive KPI data which was presented through a web based front end. A pilot system was then developed using agile principles allowing for rapid user engagement. Testing demonstrated that the system can support all real time operations within the company simultaneously and rapidly process historic well data for offset benchmarking. The cloud-based architecture allows rapid deployment of the system to new groups significantly reducing deployment costs. The system provides a foundation for onward data science and more advanced functionality.
 Minimal configuration, cloud storage and processing, combining contextual data with real-time rig data, near-real-time and historic analysis capabilities, rapid deployment, low cost, high accuracy and consistent metrics are all key and proven value drivers for the system. The output data is aso a valuable resource for additional machine learning and data science projects."
483,unknown,http://arxiv.org/abs/2007.15153v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2007.15153v1,2020-07-29 00:00:00,"fast, structured clinical documentation via contextual autocomplete","We present a system that uses a learned autocompletion mechanism to
facilitate rapid creation of semi-structured clinical documentation. We
dynamically suggest relevant clinical concepts as a doctor drafts a note by
leveraging features from both unstructured and structured medical data. By
constraining our architecture to shallow neural networks, we are able to make
these suggestions in real time. Furthermore, as our algorithm is used to write
a note, we can automatically annotate the documentation with clean labels of
clinical concepts drawn from medical vocabularies, making notes more structured
and readable for physicians, patients, and future algorithms. To our knowledge,
this system is the only machine learning-based documentation utility for
clinical notes deployed in a live hospital setting, and it reduces keystroke
burden of clinical concepts by 67% in real environments."
484,excluded,10.5555/3535850.3536145,AAMAS,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/36c13393098f73060ab8ad3df0f3827a4e07ee46,2022-01-01 00:00:00,a multi-agent system for automated machine learning,"Machine Learning (ML) focuses on giving machines the ability to forecast, predict, or classify without being explicitly programmed to do so. To achieve such goals, large amounts of data are used to conceive models that can adapt to unseen data and to new scenarios. However, applying ML models to real-world business domains is a resource-intensive and time-consuming effort. Automated machine learning (AutoML) emerged as a way to ease such processes. With this in mind, this study introduces a multi-agent system (MAS) that autonomously go through the entire ML pipeline, with different entities being responsible for the data collection process, for preprocessing the data, and for deploying the best candidate ML model. The conceived MAS is currently implemented in a real-world setting, addressing important societal challenges raised by big urban centers. The obtained results show that this solution proved to be beneficial not only for the data collection and pre-processing tasks, but also for the automated execution of ML models."
485,excluded,10.1109/21.135684,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/135684/,1991-11-01 00:00:00,an intelligent agent framework for enterprise integration,"The authors present a framework in which human and intelligent agents (IAs) can interact to facilitate the information flow and decision making in real-world enterprises. Underlying the framework is the notion of an enterprise model that is built by dividing complex enterprise operations into a collection of elementary tasks or activities. Each such task is then modeled in cognitive terms and entrusted to an IA for execution. Tasks that require human involvement are referred to the appropriate person through their personal assistant, a special type of IA that knows how to communicate both with humans, through multimedia interfaces, and with other IAs and the shared knowledge base. The computer-aided software engineering tools supported by a library of activity models permit every individual in an enterprise to model the activities with which they are personally most familiar. The preliminary experimental results suggest that this divide-and-conquer strategy, leading to cognitive models that are buildable and maintainable by end-users, is a viable approach to real-world distributed artificial intelligence.<>"
486,excluded,10.1109/gucon48875.2020.9231104,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9231104/,2020-10-04 00:00:00,prediction and monitoring of stored food grains health using iot enable nodes,"Accompanying the rapid urbanization, A developing country like India, still monitoring the food grain's health using conventional methods, which detect insects at a very later stage due to which food grain get spoil. The major parameter that should be observed for maintaining the quality of grains are temperature, relative humidity, and carbon dioxide concentration. The observation of these parameters regularly can lead to early detection of the insects in the grains. This paper suggests the state of the art Internet of Things (IoT) based real-time system which observes the warehouse parameter i.e Temperature, Relative Humidity, and Carbon dioxide and predicts the type of insect activity. The hardware Device used in the experiment is designed by keeping in mind the Indian warehouse's conditions and for large scale implementation of this model. The hardware Device contains two sensors (DHT22 and CDM7160), ESP8266, and battery for power supply. DHT22 and CDM7160 sensors are adopted to detect the above parameters i.e temperature, relative humidity, and CO2. The online portal has been created for data analysis and real-time data collection from the sensor nodes. This portal can be easily integrated with the digital portal already present in most of the warehouses. The machine learning algorithm has been deployed on the online server to predict relative humidity on the basis of collected data of relative humidity at a particular temperature. The prediction of relative humidity has been done for the next five days. The predicted relative humidity is used for predicting the type of insect that could attack the stack of the warehouse in the coming future. Based on the predicted results further action will be taken for controlling the food grain quality and prevent the wastage of food grain."
487,excluded,10.1109/phm2022-london52454.2022.00044,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9808809/,2022-05-29 00:00:00,a fault diagnosis platform of actuators on embedded iot microcontrollers,"In the process of monitoring and fault diagnosis of complex electromechanical equipment, the close coupling between the fault diagnosis process and the front-end equipment can effectively reduce the occurrence of serious faults and significantly improve the economic benefits. In this paper, an Internet of Things (IoT) framework for monitoring and diagnosing industrial equipment is designed and implemented for complex electromechanical equipment running in real-time. All the procedures are physically implemented on a hardware prototype, which includes hardware selection, software configuration, transplanting of machine learning (ML) model and data communication. The framework of the physical platform is universal and flexible. It can be deployed in various monitoring scenarios, and flexibly customize the deployed artificial intelligence (AI) models according to their applications. Three typical machine learning algorithms of SVM, ANN and LSTM models are transplanted to STM32 MCU to compare the results. Finally, the proposed method is experimentally validated on NASA Electro-mechanical actuators (EMAs) data set."
488,unknown,10.1109/sest.2018.8495711,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8495711/,2018-09-12 00:00:00,from m&v to m&t: an artificial intelligence-based framework for real-time performance verification of demand-side energy savings,"The European Union’s Energy Efficiency Directive is placing an increased focus on the measurement and verification (M&V) of demand side energy savings. The objective of M&V is to quantify energy savings with minimum uncertainty. M&V is currently undergoing a transition to practices, known as M&V 2.0, that employ automated advanced analytics to verify performance. This offers the opportunity to effectively manage the transition from short-term M&V to long-term monitoring and targeting (M&T) in industrial facilities. The original contribution of this paper consists of a novel, robust and technology agnostic framework that not only satisfies the requirements of M&V 2.0, but also bridges the gap between M&V and M&T by ensuring persistence of savings. The approach features a unique machine learning-based energy modelling methodology, model deployment and an exception reporting system that ensures early identification of performance degradation. A case study demonstrates the effectiveness of the approach. Savings from a real-world project are found to be 177,962 +/- 12,334 kWh with a 90% confidence interval. The uncertainty associated with the savings is 8.6% of the allowable uncertainty, thus highlighting the viability of the framework as a reliable and effective tool."
489,excluded,10.1109/ojcoms.2020.3009023,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9139304/,2020-01-01 00:00:00,enabling remote human-to-machine applications with ai-enhanced servers over access networks,"The recent research trends for achieving ultra-reliable and low-latency communication networks are largely driven by smart manufacturing and industrial Internet-of-Things applications. Such applications are being realized through Tactile Internet that allows users to control remote things and involve the bidirectional transmission of video, audio, and haptic data. However, the end-to-end propagation latency presents a stubborn bottleneck, which can be alleviated by using various artificial intelligence-based application layer and network layer prediction algorithms, e.g., forecasting and preempting haptic feedback transmission. In this paper, we study the experimental data on traffic characteristics of control signals and haptic feedback samples obtained through virtual reality-based human-to-machine teleoperation. Moreover, we propose the installation of edge-intelligence servers between master and slave devices to implement the preemption of haptic feedback from control signals. Harnessing virtual reality-based teleoperation experiments, we further propose a two-stage artificial intelligence-based module for forecasting haptic feedback samples. The first-stage unit is a supervised binary classifier that detects if haptic sample forecasting is necessary and the second-stage unit is a reinforcement learning unit that ensures haptic feedback samples are forecasted accurately when different types of material are present. Furthermore, by evaluating analytical expressions, we show the feasibility of deploying remote human-to-machine teleoperation over fiber backhaul by using our proposed artificial intelligence-based module, even under heavy traffic intensity."
490,excluded,10.1007/s11554-021-01110-1,Springer,springer,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11554-021-01110-1,2021-08-01 00:00:00,nighttime object detection system with lightweight deep network for internet of vehicles,"Autonomous driving systems in internet of vehicles (IoV) applications usually adopt a cloud computing mode. In these systems, information got at the edge of the cloud computing center for data analysis and situation response. However, the conventional IoV face enormous challenges to meet the requirements in terms of storage, communication, and computing problems because of the considerable amount of information on the traffic environment. The environment perception during the nighttime is poorer than that during the daytime that this problem also requires addressing. To solve these problems, we propose a nighttime object detection scheme based on a lightweight deep learning model in the edge computing mode. First, the pedestrian detection and the vehicle detection algorithm that using the thermal images based on the YOLO architecture. We can implement the model on edge devices that can achieve real-time detection through the designed lightweight strategy. Next, a spatial prior information and temporal prior information into the detection algorithm and divide the frames into key and non-key frames to increase the performance and speed of the system simultaneously. Finally, we implemented the detection network for performance and feasibility verification on the Jetson TX2 edge device. The experimental results show that the proposed system can achieve real-time and high-accuracy object detection on edge devices."
491,excluded,10.1109/telecom50385.2020.9299566,IEEE,ieeexplore,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9299566/,2020-10-30 00:00:00,forest monitoring system for early fire detection based on convolutional neural network and uav imagery,"Forest fires are one of the main reasons for environmental degradation. In their early stages, the fires are hard to discover, so a faster and more accurate detection method can help minimize the amount of damage they can inflict. In this paper, we present an approach for autonomous early fire detection, which is based on a system with high degree of reliability and with no need of service or human interaction. To provide the autonomous capabilities to the proposed system, we have developed an object detection method, based on a convolutional neural network, which is presented in the main part of the paper. In order to have a better field of view over the observed area, instead of traditional lookout towers and satellite based monitoring, we use live video feed from an unmanned aerial vehicle (UAV), which patrols over the risky area. To make better predictions on the fire probability, we use not only the optical camera of the UAV, but also an on-board thermal camera. With the help of the software platform Node-RED, we have developed a web-based platform, which can present the acquired data in real-time and can notify the interested parties. The workflow for the development of the web-platform is also described in this paper."
492,excluded,10.1109/gucon50781.2021.9573606,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9573606/,2021-09-26 00:00:00,possible role of ai and iot in smart buildings,"There are an increasing number of Artificial Intelligence (AI) and Internet of Things (IoT) devices and applications which are used across the globe in different areas. One such area is IoT-enabled smart buildings. There is a growing interest among government and non-government organizations in IoT-enabled smart buildings. Despite the perceived benefits of being smart, there is also a key concern of human safety in an emergency situation. This research aims to address this important concern and will develop an initial framework, called here the Safe'Tech, for human safety in IoT enabled smart buildings. The SafeTech will be developed and evaluated using the well-known design science research method. The SafeTech will comprise of both a design and software prototype solution, which can be used to ensure the safety of humans during an emergency through the collection, processing and communication of real-time data from the IoT sensors embedded in the smart building such as smart campus and shopping centre. The SafeTech will interact with the smart building through sensors and collect the state of the physical environment and sense any emergency and then also interact with the humans in the smart building and inform and direct them about the emergency and guide them for the safe evacuation. The applicability and evaluation of the framework software prototype will be demonstrated with the help of smart building scenarios in both Australia and Qatar, involving the fire emergency exercise and automatic and directed evacuation using the SafeTech. This research has several implications, in particular, the impact of reducing human life loss or injury reduction."
493,unknown,10.1109/asiancon51346.2021.9544683,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9544683/,2021-08-29 00:00:00,covid-19 safety protocol monitoring system using ai,"The entire globe is going through a severe health crisis due to the rapid spread of Covid-19 Disease, which in turn creating a deep impact on the human lives and their day-to-day livelihood. Only hope of preventing it from further spread of disease is by following all the precautionary measures provided by the WHO. One of the most important safety measures is to wear face mask and maintain social distancing. Hence, we have proposed to develop a system can monitor whether a person is wearing a facemask correctly/not in real time. This will help to reduce the rapid spread of the disease in public places and various other organizations. We have proposed a solution that uses Artificial Intelligence and has a capability to detect the violation of wearing a face mask in real-time using Image Recognition and Video Techniques. The main intention of the proposed work is to provide the front-end software (Web or mobile application) for administrators to monitor the violation. The system will capture the violated instance and store the data. Our system will also include a User registration form where the users have to register their information along with their face images. This will be an input to the face recognition model, which will train and alert the user whenever violation occurs."
494,excluded,10.1109/icc42927.2021.9500499,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9500499/,2021-06-23 00:00:00,bcovx: blockchain-based covid diagnosis scheme using chest x-ray for isolated location,"The COVID-19 pandemic has adversely affected the lives of millions of people worldwide. With an alarming increase in COVID-19 cases, it is important to detect and diagnose COVID-19 in its early stages to prevent its spread. To diagnose remote patients, the Internet can be useful for accessing data of that patient. But, the Internet has also had issues related to data security, reliability, and privacy. Motivated by these challenges, in this paper, we propose a Blockchain (BC) based COVID-19 detection scheme (BCovX) for fast and reliable diagnosis of COVID-19 using chest X-Ray (CXR) images. For fast and accurate detection of COVID-19 using CXR, BCovX consists of a Convolutional Neural Network (CNN) model, using which a patient can be diagnosed for COVID-19 remotely. CNNs have performed successfully in medical imaging classification. BCovX provides reliable and secure data access and exchange using BC and smart contracts (SC). To solve issues related to data storage and its associated cost, the InterPlanetary File System (IPFS) protocol is used to store medical data. We also present a real-time SC developed in Solidity to govern the transaction between the patient and the doctor. The SC has been compiled and deployed on Remix Integrated Development Environment (IDE). Finally, we have evaluated the performance of BCovX with traditional schemes in terms of storage cost, bandwidth requirements, and accuracy of the CNN model."
495,excluded,10.3390/s21082595,Sensors,semantic_scholar,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/07f55daeccb3adf401a88c0c3d39b89bbf3b3bab,2021-01-01 00:00:00,deep learning based pavement inspection using self-reconfigurable robot,"The pavement inspection task, which mainly includes crack and garbage detection, is essential and carried out frequently. The human-based or dedicated system approach for inspection can be easily carried out by integrating with the pavement sweeping machines. This work proposes a deep learning-based pavement inspection framework for self-reconfigurable robot named Panthera. Semantic segmentation framework SegNet was adopted to segment the pavement region from other objects. Deep Convolutional Neural Network (DCNN) based object detection is used to detect and localize pavement defects and garbage. Furthermore, Mobile Mapping System (MMS) was adopted for the geotagging of the defects. The proposed system was implemented and tested with the Panthera robot having NVIDIA GPU cards. The experimental results showed that the proposed technique identifies the pavement defects and litters or garbage detection with high accuracy. The experimental results on the crack and garbage detection are presented. It is found that the proposed technique is suitable for deployment in real-time for garbage detection and, eventually, sweeping or cleaning tasks."
496,unknown,http://arxiv.org/abs/2101.04086v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2101.04086v1,2021-01-11 00:00:00,"system design for a data-driven and explainable customer sentiment
  monitor","The most important goal of customer services is to keep the customer
satisfied. However, service resources are always limited and must be
prioritized. Therefore, it is important to identify customers who potentially
become unsatisfied and might lead to escalations. Today this prioritization of
customers is often done manually. Data science on IoT data (esp. log data) for
machine health monitoring, as well as analytics on enterprise data for customer
relationship management (CRM) have mainly been researched and applied
independently. In this paper, we present a framework for a data-driven decision
support system which combines IoT and enterprise data to model customer
sentiment. Such decision support systems can help to prioritize customers and
service resources to effectively troubleshoot problems or even avoid them. The
framework is applied in a real-world case study with a major medical device
manufacturer. This includes a fully automated and interpretable machine
learning pipeline designed to meet the requirements defined with domain experts
and end users. The overall framework is currently deployed, learns and
evaluates predictive models from terabytes of IoT and enterprise data to
actively monitor the customer sentiment for a fleet of thousands of high-end
medical devices. Furthermore, we provide an anonymized industrial benchmark
dataset for the research community."
497,excluded,10.1109/icnsc.2019.8743331,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8743331/,2019-05-11 00:00:00,real-time sentiment analysis on e-commerce application,"Opinion mining is one of the most important tasks of natural language processing, which is also known as sentiment analysis, used to identify about what people have an impression about their services and products in social media platforms. To improve marketing strategies using product reviews, an effective method should be used for predicting the sentiment polarity. In this research article, a Machine learning technique called Support Vector Machine (SVM) is used to design a model and this model has been implemented on an E-commerce application. The data used in this study are online product reviews which are collected from Amazon.com. The experiments of sentiment analysis are performed for two levels of categorization: review level and sentence level. The main focus of this paper is to present a real-time sentimental analysis on the product reviews of e-commerce application so that the user experience can be enhanced."
498,unknown,10.1145/3362743.3362963,SenSys-ML,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d455a285dca63774864bf31c36228bf2a17d0bd1,2019-01-01 00:00:00,efficient sparse processing in smart home applications,"In recent years, smart home technology has become prevalant and important for various applications. A typical smart home system consists of sensing nodes sending raw data to a cloud server which performs inference using a Machine Learning (ML) model trained offline. This approach suffers from high energy and communication costs and raises privacy concerns. To address these issues researchers proposed hierarchy aware models which distributes the inference computations across the sensor network with each node processing a part of the inference. While hierarchical models reduce these overheads significantly they are computationally intensive to run on resource constrained devices which are typical to smart home deployments. In this work we present a novel approach combining Hierarchy aware Neural Networks (HNN) with variational dropout technique to generate sparse models which have low computational overhead allowing them to be run on edge devices with limited resources. We evaluate our approach using an extensive real-world smart home deployment consisting of several edge devices. Measurements across different devices show that without significant loss of accuracy, energy consumption can be reduced by up to 35% over state-of-the-art."
499,unknown,10.1109/aero50100.2021.9438171,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9438171/,2021-03-13 00:00:00,considerations in the deployment of machine learning algorithms on spaceflight hardware,"Recent advances in artificial intelligence (AI) and machine learning (ML) have revolutionized many fields. ML has many potential applications in the space domain. Next generation space instruments are producing data at rates that exceed the capabilities of current spacecraft to store or transmit to ground stations. Deployment of ML algorithms onboard future spacecraft could perform processing of sensor data as it is gathered, reducing data volume and providing a dramatic increase in throughput of meaningful data. ML techniques may also be used to enhance the autonomy of space missions. However ML techniques have not yet been widely deployed in space environments, primarily due to limitations on the computational capabilities of spaceflight hardware. The need to verify that high-performance computational hardware can reliably operate in this environment delays the adoption of these technologies. Nevertheless, the availability of advanced processing capabilities onboard spacecraft is increasing. These platforms may not provide the processing power of terrestrial equivalents, but they do provide the resources necessary for deploying real-time execution of ML algorithms. In this paper, we present results exploring the implementation of ML techniques on computationally-constrained, high-reliability spacecraft hardware. We show two ML algorithms utilizing deep learning techniques which illustrate the utility of these approaches for space applications. We describe implementation considerations when tailoring these algorithms for execution on computationally-constrained hardware and present a workflow for performing these optimizations. We also present initial results on characterizing the trade space between algorithm accuracy, throughput, and reliability on a variety of hardware platforms with current and anticipated paths to spaceflight."
500,unknown,http://arxiv.org/abs/1906.07391v3,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1906.07391v3,2019-06-18 00:00:00,"the breakthrough listen search for intelligent life: public data,
  formats, reduction and archiving","Breakthrough Listen is the most comprehensive and sensitive search for
extraterrestrial intelligence (SETI) to date, employing a collection of
international observational facilities including both radio and optical
telescopes. During the first three years of the Listen program, thousands of
targets have been observed with the Green Bank Telescope (GBT), Parkes
Telescope and Automated Planet Finder. At GBT and Parkes, observations have
been performed ranging from 700 MHz to 26 GHz, with raw data volumes averaging
over 1PB / day. A pseudo-real time software spectroscopy suite is used to
produce multi-resolution spectrograms amounting to approximately 400 GB hr^-1
GHz^-1 beam^-1. For certain targets, raw baseband voltage data is also
preserved. Observations with the Automated Planet Finder produce both
2-dimensional and 1-dimensional high resolution (R~10^5) echelle spectral data.
  Although the primary purpose of Listen data acquisition is for SETI, a range
of secondary science has also been performed with these data, including studies
of fast radio bursts. Other current and potential research topics include
spectral line studies, searches for certain kinds of dark matter, probes of
interstellar scattering, pulsar searches, radio transient searches and
investigations of stellar activity. Listen data are also being used in the
development of algorithms, including machine learning approaches to modulation
scheme classification and outlier detection, that have wide applicability not
just for astronomical research but for a broad range of science and
engineering.
  In this paper, we describe the hardware and software pipeline used for
collection, reduction, archival, and public dissemination of Listen data. We
describe the data formats and tools, and present Breakthrough Listen Data
Release 1.0 (BLDR 1.0), a defined set of publicly-available raw and reduced
data totalling 1 PB."
501,unknown,10.1007/s00170-020-05548-8,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00170-020-05548-8,2020-06-01 00:00:00,a tool wear monitoring and prediction system based on multiscale deep learning models and fog computing,"Tool condition monitoring (TCM) during the manufacturing process is of great significance for ensuring product quality and plays an important role in intelligent manufacturing. Current TCM systems deployed in the local device or cloud computing environment unable meet the requirements of low response latency and high accuracy at the same time. The emerging fog computing provides new solutions for the above problem. This paper presents a tool wear monitoring and prediction (TWMP) system based on deep learning models and fog computing. In order to improve monitoring and prediction accuracy, we propose a multiscale convolutional long short-term memory model (MCLSTM) to complete the tool wear monitoring task and a bi-directional LSTM model (BiLSTM) to complete the tool wear prediction task. To reduce the response latency of the TWMP system, we deploy the MCLSTM model and the BiLSTM model in a fog computing architecture. The fog computing architecture consists of an edge computing layer, a fog computing layer, and a cloud computing layer. The edge computing layer undertakes real-time signal collection task. The fog computing layer undertakes real-time tool wear monitoring task. The cloud computing layer with powerful computing resources undertakes intensive computing and latency-insensitive tasks such as data storage, tool wear prediction, and model training. A twist drill wear monitoring and prediction experiment is conducted to test the performance of the proposed system in terms of accuracy, response time, and network bandwidth consumption."
502,unknown,10.1109/icbats54253.2022.9759087,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9759087/,2022-02-17 00:00:00,smart waste management and classification system for smart cities using deep learning,"For modern city environments to be renewable and clean, waste management and recycling are essential. Solid waste management, disposal, and recycling are issues in many Pakistani cities, particularly Karachi and Lahore. The combination of the IoTs and deep learning offers a modular technique to data categorization and real-time examining. This article illustrates a capable &#x201C;Smart trash management and categorization system&#x201D; based on the &#x201C;internet of things (IoT)&#x201D; and DL. The article provides an architectural idea for a microchips-based garbage bin that uses numerous measuring instruments to connect with the method to gather wastes as quickly as possible. The &#x201C;Internet of Things (IoT)&#x201D; is used in the suggested data monitoring solution to offer real-time data control. In addition, in this smart waste management and categorization scheme, a waste classification model based on convolutional neural networks was deployed. This waste classification technique will be used to sort rubbish into several categories at the waste-collecting plant to increase recycling. This proposed system offers complete trash management and recycling solution in smart cities, from waste collection to waste management and classification."
503,excluded,10.1109/iccad51958.2021.9643557,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9643557/,2021-11-04 00:00:00,a general hardware and software co-design framework for energy-efficient edge ai,"A huge number of edge applications including self-driving cars, mobile health, robotics, and augmented reality / virtual reality are enabled by deep neural networks (DNNs). Currently, much of this computation for these applications happens in the cloud, but there are several good reasons to perform the processing on local edge platforms such as smartphones: improved accessibility to different parts of the world, low latency, and data privacy. In this paper, we present a general hardware and software co-design framework for energy-efficient edge AI for both simple classification and structured output prediction tasks (e.g., 3D shapes from images). This framework relies on two key ideas. First, we design a space of DNNs of increasing complexity (coarse to fine) and perform input-specific adaptive inference by selecting a DNN of appropriate complexity depending on the hardness of input examples. Second, we execute the selected DNN on the target edge platform using a resource management policy to save energy. We also provide instantiations of our co-design framework for three qualitatively different problem settings: convolutional neural networks for image classification, graph convolutional networks for predicting 3D shapes from images, and generative adversarial networks on photo-realistic unconditional image generation. Our experiments on real-world benchmarks and mobile platforms show the effectiveness of our co-design framework in achieving significant gain in energy with little to no loss in accuracy of predictions."
504,unknown,10.23919/cinc53138.2021.9662709,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9662709/,2021-09-15 00:00:00,mobile app for the digitization and deep-learning-based classification of electrocardiogram printed records,"The interpretation of electrocardiograms (ECG) plays an important role in diagnosis and monitoring cardiovascular diseases, where 75% of deaths are in low and middle-income countries, whose access to experienced cardiologists is more limited. Moreover, the implementation of computer-assisted clinical decision making systems is hampered by old ECG equipment, which does not allow for exporting a digital copy of the trace. These environments need new ECG interpretation techniques that enable a rapid and simple diagnosis. This could be provided by mobile phone applications where ECG classification algorithms, based on machine learning (ML) techniques, could be embedded. In this work, firstly, we present an user-friendly Android-based mobile app for the embedding of the algorithms and the ECG record capture. Secondly, an algorithm for ECG digitisation and ML-based classification, considering different orientations and illuminations defaults requiring some processing for the extraction of the signal. Afterwards, the post processed ECG is introduced into a deep learning algorithm, specifically a residual neural network, pre-trained on the China Physiological Challenge database. The proposed methodology was tested on a set of synthetic and 50 real ECGs, achieving an accuracy of 88%. These preliminary results pave the way for improved ECG interpretation in clinical environments such as emergency units."
505,excluded,http://arxiv.org/abs/2111.12142v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2111.12142v1,2021-11-23 00:00:00,"phenomenological classification of the zwicky transient facility
  astronomical event alerts","The Zwicky Transient Facility (ZTF), a state-of-the-art optical robotic sky
survey, registers on the order of a million transient events - such as
supernova explosions, changes in brightness of variable sources, or moving
object detections - every clear night, and generates associated real-time
alerts. We present Alert-Classifying Artificial Intelligence (ACAI), an
open-source deep-learning framework for the phenomenological classification of
ZTF alerts. ACAI uses a set of five binary classifiers to characterize objects
which, in combination with the auxiliary/contextual event information available
from alert brokers, provides a powerful tool for alert stream filtering
tailored to different science cases, including early identification of
supernova-like and anomalous transient events. We report on the performance of
ACAI during the first months of deployment in a production setting."
506,excluded,10.1109/iccpeic.2017.8290335,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8290335/,2017-03-23 00:00:00,smart personalized learning system for energy management in buildings,"Integration of energy management systems into existing buildings brings in several challenges and financial constraints. Some of the challenges in the existing smart building solutions are that they require large-scale deployment of sensors, high rate of data collection, real-time data analysis in short span of time, and lack of knowledge about the energy usage with respect to the behavior of individuals and groups. This work proposes an affordable wearable device system as an alternative for large-scale deployment of sensors in industrial buildings. For effective energy management in the buildings, a personalized behavior analysis has been done in machine learning and neural networks algorithm and integrated with the proposed system. The complete system is implemented and tested extensively. The results show that the proposed system could provide 85% user comfort and 23% energy savings."
507,excluded,10.1016/j.promfg.2020.10.053,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85099870958,2020-01-01,enabling real-time quality inspection in smart manufacturing through wearable smart devices and deep learning,"In this paper, we present a novel method for utilising wearable devices with Convolutional Neural Networks (CNN) trained on acoustic and accelerometer signals in smart manufacturing environments in order to provide real-time quality inspection during manual operations. We show through our framework how recorded or streamed sound and accelerometer data gathered from a wrist-attached device can classify certain user actions as successful or unsuccessful. The classification is designed with a Deep CNN model trained on Mel-frequency Cepstral Coefficients (MFCC) from the acoustic input signals. The wearable device provides feedback on three different modalities: audio, visual and haptic; thus ensuring the worker’s awareness at all time. We validate our findings through deployments of the complete AI-enabled device in production facilities of Mercedes-Benz AG. From the conducted experiments it is concluded that the use of acoustic and accelerometer data is valuable to train a classifier with the purpose of action examination during industrial assembly operations, and provides an intuitive interface for ensuring continued and improved quality inspection."
508,excluded,10.1007/978-3-030-00916-8_21,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-00916-8_21,2018-01-01 00:00:00,retail: a machine learning-based item-level localization system in retail environment,"Radio-frequency identification (RFID) technology has become the key focus of indoor localization recently. The low cost and flexibility allow numbers of passive RFID-based algorithms been proposed for indoor localization. However, in a real-world environment including retail store and supermarket with large-scale item-level deployment of RFID tags and complex surroundings, these algorithms may not be available due to the collision and interference. Existing algorithms either require extra hardware or only take a small number of tags into consideration, facing difficulty in applying to these places. In this paper, we propose a novel machine learning-based REal-Time and Item-Level (RETaIL) indoor localization system, which is designed to tolerate various interference. RETaIL incorporates three machine learning algorithm, J48, SVM and cloth grouping, for indoor localization. Validations in both complex laboratory environment and real-world Levis outlet store demonstrate the accuracy and efficiency of RETaIL and its capability of dealing with interference in retail environment."
509,unknown,10.1016/j.trpro.2022.02.037,scopus,sciencedirect,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85127522953,2022-01-01,a novel image and audio-based artificial intelligence service for security applications in autonomous vehicles,"Autonomous Vehicles (AVs) can potentially reduce the accident risk while a human is driving. They can also improve the public transportation by connecting city centers with main mass transit systems. Creating a system that can provide a sense of security to the passenger, when the driver is missing, remains a challenging task. In this work, an image and audio-based approach, supported by novel Artificial Intelligence (AI) algorithms, is proposed as a service to increase the security and trust inside an autonomous shuttle. The two modalities, running in real-time, can detect petty crimes scenarios such as screaming, bag snatching, people fighting and vandalism and enable notifications to authorized personnel for proper actions. The proposed solution is deployed on a Jetson AGX Xavier to favor power efficiency and seamless integration and achieves up to 96% accuracy. Thus, the envisioned system exhibits high potential for transforming security and safety in emerging autonomous public transportation infrastructure."
510,included,10.22323/1.372.0041,,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/946e25b009baa067d99f6c2397a5fed72de260db,2020-01-01 00:00:00,machine learning-based system for the availability and reliability assessment and management of critical infrastructures (caso),"A critical infrastructure is a complex interconnected system of systems providing basic and essential services to support the operation of particle accelerators but also industries and households for which they must guarantee high reliability of critical functions. 
Model-based approaches are usually adopted to provide an early identiﬁcation of failures and to reveal hidden dependencies among subsystems. System models are complex and require constant updating to be reactive to system changes and real operating conditions, wear and aging. The interconnections between the different systems and the functional dependencies between their components are in many cases modified at both physical and functional levels while their degraded performances impact the overall system availability and reliability. 
A novel approach is proposed which combines model-based and Big Data analytics by machine learning techniques to extract descriptive and predictive models directly from data. The objective is to foresee and react in time to failures to reduce downtimes as well as to optimize maintenance and operation costs. 
The Computer-Aided System for critical infrastructure Operation (CASO) is designed to significantly and efficiently enhance the quality, safety, reliability and availability of critical infrastructures. 
We report on the design of CASO, its implementation and on the preliminary results inferred on historical and live stream data recorded from CERN’s technical infrastructure. Proposal for the full deployment and expected long-term capabilities will also be discussed."
511,unknown,10.1007/978-3-319-62392-4_24,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-319-62392-4_24,2017-01-01 00:00:00,prescstream: a framework for streaming soft real-time predictive and prescriptive analytics,"All the data volume generated by modern applications brings opportunities for knowledge extraction and value creation. In this sense, the integration of predictive and prescriptive analytics may help the industry and users to be more productive and successful. It means not only to estimate an outcome but also to act on it in the real world. Nonetheless, mastering these concepts and providing their integration is not an easy task. This work proposes PrescStream, a proof of concept framework that uses machine learning based prediction, and process this outcome result to do prescriptive analytics, allowing researchers to integrate predictive and prescriptive analytics into their experiments. It has a scalable, fault-tolerant microservices based architecture, making it ideal for cloud deployment and IoT (internet of things) applications. The paper describes the general architecture of the system, as well as a validation usage with result analysis."
512,excluded,10.1109/ccwc47524.2020.9031201,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9031201/,2020-01-08 00:00:00,the self- upgrading mobile application for the automatic malaria detection,"The UN World Health Organization (WHO) set an ambitious vision of malaria control which includes reducing malaria case incidence by 90% by 2030. Many tools and approaches have been considered to enable the progress toward malaria vision. The use of portable smartphones and machine learning (ML) software is a promising one among them. Recently, many ML models have been proposed for malaria detection. From the interview with health workers in the field, we realized ML model should be continuously improved to provide higher accuracy and/or more capability to cover practical issues found in a real setting such as malaria mosquitoes with drug resistance. In this paper, we propose the mobile application for malaria detection which upgrades ML model on its own without depending on internet connection. Unavailability of internet connection is commonly observed in malaria epidemic countries. We also learned that ML model should be not only accurate but also resource-efficient. This motivated us to set up performance metrics for ML model. Based on the metrics, we chose the optimal ML model of Resnet-50. While most of the prior art ML models were optimized in terms of accuracy only, the optimal model of our choice satisfies both accuracy and resource efficiency. With adopting the model, we architect self-upgrading malaria-detection application and it is validated using ATAM (Architecture Tradeoff Analysis Method) to ensure the application works in the resource-constrained setting as desired. Lastly, we develop the prototype application and show it diagnoses malaria parasites as expected. To collect more blood samples and feedback from prospect users, we plan to do testing at local clinics in India and Myanmar."
513,unknown,10.1109/southeastcon42311.2019.9113415,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9113415/,2019-04-14 00:00:00,advanced signal processing for decision making and decision-fusion software systems for aircraft structural health monitoring,"Tracking the structure health status of the aircraft fleet is one of the important factors for improving safety in the aviation industry. US Airforce Research Lab is working hard to develop technology to monitor the structural health status of aircraft before and after takeoff. This technology is called Structural Health Monitoring or Structural Health Management (SHM), and uses advanced signal processing techniques. The advantages of this include improving the technology and safety factors of the aircraft by increasing the robustness of the aircraft structure as well as reduce the cost of the labor maintenance. This research paper focuses on developing a software system that is capable of detecting a crack in the aircraft structure at an early stage. Also, the developed software will be used to estimate the crack lengths within 90% accuracy. The developed software uses the Matlab environment for all algorithms developed: calculating and finding the crack, and estimating the crack length. Artificial Intelligent (AI) techniques such as fuzzy logic and neural networks are used to support the decision regarding the length of the crack. Also, developed decision fusion frameworks are designed to increase the accurate percentage rate of the decision making results. The results obtained from this research are compared with the baseline developed by The Air Force Research Laboratory (AFRL). Finally, the results of this research along with the capabilities of using advanced signal processing integrated with AI, show that the calculations for the crack and the crack length produce very acceptable results using a real data gathered by AFRL. Also, the percentage average error performance analysis of the developed software's system algorithms is provided in the research paper."
514,unknown,10.1109/icssit46314.2019.8987908,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8987908/,2019-11-29 00:00:00,real-time two way communication system for speech and hearing impaired using computer vision and deep learning,"Sign Language is the most expressive form of communication for speech and hearing impaired people to communicate with normal person but a normal person cannot understand sign language. So in order to break this barrier of communication there needs to be a system that can enable conversion of sign language to voice or text and voice or text to sign language and do it in real time. The systems that currently exist are not real time, do not facilitate two-way communication, require static surrounding conditions or have low recognition accuracy. There exist systems that have good accuracy but require external hardware like gloves [3] which increases the cost. Our contribution to solving this problem consists of a Sign Language Communication System. It is a real-time communications system built using the advancements in Image Processing, Deep Learning and Computer Vision that provides real-time sign language to text and text to sign language conversion. The project is software-based which can be installed on any computer with good specifications. It is also a two-way communication system allowing not just speech and hearing impaired to communicate with normal people but also other way around. The primary goal of our system is to enable hearing and speech impaired people to communicate with people that are not disabled in real time by interpreting alphabets, numbers and words in the Indian sign language. The system is able to predict 17600 test images in 14 seconds with an average prediction time of 0.000805 seconds with an accuracy of 99%."
515,unknown,10.1109/icdmw.2019.00065,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8955638/,2019-11-11 00:00:00,mímir: building and deploying an ml framework for industrial iot,"In this paper we describe Mímir, a production grade cloud and edge spanning ML framework for Industrial IoT applications. We first describe our infrastructure for optimized capture, streaming and multi-resolution storage of manufacturing data and its context. We then describe our workflow for scalable ML model training, validation, and deployment that leverages a manufacturing taxonomy and parameterized ML pipelines to determine the best metrics, hyper-parameters and models to use for a given task. We also discuss our design decisions on model deployment for real-time and batch data in the cloud and at the edge. Finally, we describe the use of the framework in building and deploying an application for Predictive Quality monitoring during a Plastics Extrusion manufacturing process."
516,unknown,10.1109/jiot.2021.3120640,IEEE Internet of Things Journal,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/951d6f7735768e5f6237502df2ddbbdae2716fa5,2021-01-01 00:00:00,"fast data: a fair, secure and trusted decentralized iiot data marketplace enabled by blockchain","As the world calls it, data is the new oil. With vast installments of Industrial Internet-of-Things (IIoT) infrastructure, data is produced at a rate like never before. Similarly, artificial intelligence (AI) and machine learning (ML) solutions are getting integrated to numerous services, making them ""smarter"". However, the data remains fragmented in individual organizational silos inhibiting data value extraction to it’s full potential. Digital marketplaces are emerging to allow data owners to monetize this data. Yet concerns like privacy, security and unfair payment settlement deter adoption of such platforms. In addition, the state-of-the-art platforms are under the control of large multinational corporations with no transparency between buyer and seller in terms of payment details, listing, data discovery and storage. In this work, a novel decentralized platform of digital data marketplace for IoT data has been proposed. The platform leverages a decentralized data streaming network to host IoT data in a reliable and fault tolerant manner. The platform ensures fair trading, data storage and delivery in a privacy preserving manner and trust metric calculation for actors in the network. In order to study the feasibility of the proposed platform, an open source library is developed using Hyperledger Fabric and data network layer built on VerneMQ, the library is deployed on a real-time Google cloud platform. The library is tested and results are analysed for throughput, overheads and scalability."
517,unknown,10.1109/naecon.2018.8556744,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8556744/,2018-07-26 00:00:00,onboard image processing for small satellites,"In general, the computational ability of spacecraft and satellites has lagged behind terrestrial computers by several generations. Moore's Law turns the supercomputers of yesterday into the laptops of today, but space computing remains relatively underpowered due to the harsh radiation environment and low risk-tolerance of most space missions. Space missions are generally low risk because of the high cost of components and launch. However, launch costs are drastically decreasing and innovations such as CubeSats are changing the risk equation. By accepting more risk and utilizing commercial of the shelf (COTS) parts, it is possible to cheaply build and launch extremely capable computing platforms into space. High performance satellites will be required for advanced interplanetary exploration due to latency challenges. The long transmission times between planets means satellites or robotic explorers need onboard processing to perform tasks in real-time. This paper explores one possible application that could be hosted onboard the next generation of high performance satellites, performing object classification on satellite imagery. Automation of satellite imagery processing is currently performed by servers or workstations on Earth, but this paper will show that those algorithms can be moved onboard satellites by using COTS components. First traditional computer vision techniques such as edge detection and sliding windows are used to detect possible objects on the open ocean. Then a modern neural network architecture is used to classify the object as a ship or not. This application is implemented on a Nvidia Jetson TX2 and measurements of the application's power use confirm that it fits within the Size Weight and Power (SWAP) requirements of SmallSats and possibly even CubeSats."
518,unknown,10.1109/biosmart.2019.8734185,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8734185/,2019-04-26 00:00:00,an iot-cloud based solution for real-time and batch processing of big data: application in healthcare,"With the large use of Internet of Things (IoT) today, everything around us seems to generate data. The ever increasing number of connected things or objects (IoT) is coupled with a growing volume of data generated at a continually increasing rate. Especially where data is big or there is a need to process it, cloud infrastructures, with their scalability and easy access, are becoming the solution of choice for storage and processing. In the context of healthcare applications, where medical sensors collect health data from patients and send it to the cloud, two issues frequently appear in relation to “Big Data”. The first issue is related to real-time analysis introduced by the increasing velocity at which data is generated especially from connected devices (IoT). This data should be analyzed continuously in real-time in order to take appropriate actions regarding the patient's care plan. Moreover, medical data accumulated from different patients over time constitutes an important training dataset that can be used to train machine learning models in order to perform smarter disease prediction and treatment. This gives rise to another issue regarding long-term batch processing of often huge volumes of stored data. To deal with these issues, we propose an IoT-Cloud based framework for real-time and batch processing of Big Data in the healthcare domain. We implement the proposed solution on Amazon Cloud operator known as Amazon Web Services (AWS) and use a Raspberry pi as an IoT device to generate data in real time. We test the solution with the specific application of ECG monitoring and abnormality reporting. We analyze the performance of the implemented system in terms of response time by varying the velocity and volume of the analyzed data. We also discuss how the cloud resources should be provisioned in order to guarantee processing performance for both long-term and real-time scenarios. To ensure a good tradeoff between cost and processing performance, resources provision should be adapted to the exact needs and characteristics of the considered application."
519,unknown,10.1007/s12530-015-9133-5,Springer,springer,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s12530-015-9133-5,2016-03-01 00:00:00,real-time vessel behavior prediction,"Vessel traffic management systems (VTMS) and vessel traffic monitoring information systems (VTMIS) have been available for a number of years now. These systems have significantly contributed to increasing the efficiency and safety of operations at sea. However, nowadays, risks at sea are once again on the rise, thus demanding an evolution in VTMS and VTMIS, such that they can support a human operator’s better understanding of the complex reality at sea and enhance his or her decision-making in light of danger. A critical requirement of such systems, is that they exhibit the ability to for-see unfolding cautious and potentially hazardous situations, so as to propose measures of danger avoidance. In this study, we employ machine learning, and specifically artificial neural networks, as a tool to add predictive capacity to VTMIS. The main objective of this study is to implement a publicly accessible, web-based system capable of real time learning and accurately predicting any vessels future behavior in low computational time. This work describes our approach, design choices, implementation and evaluation details, while we present a proof of concept prototype system. Our proposal can potentially be used as the predictive foundation for various intelligent systems, including vessel collision prevention, vessel route planning, operation efficiency estimation and even anomaly detection systems."
520,excluded,10.1109/smartcomp52413.2021.00052,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9556275/,2021-08-27 00:00:00,swims: the smart wastewater intelligent management system,"Wastewater treatment is a critical process in urban and industrial settlements aiming to clean and protect the water as well as the overall environment. Wastewater management systems are conceived explicitly for purifying wastewater, providing clean water efficiently, but this is a hard task due to frequent and quite unpredictable fluctuations of inlet wastewater flows, arising from (random) rain water or (periodical, e.g. day-night) sewage sources, sometimes also leading to failures and outages. To ensure the quality of the clean water out above a threshold and keep the overall system operating, this paper proposes the smart wastewater intelligent management system (SWIMS). It monitors and controls inlet and outlet flows as well as the water quality and parts of the plant as a cyber-physical system (CPS), starting from an Environmental Internet of Things (EIoT) platform. The data generated from the treatment plant is collected in an information system hosted by a server together with an intelligent system that processes this information in a real-time fashion and provides the feedback for optimizing the plant to maintain a good quality of water over time. Such an intelligent system exploits deep learning approaches to control the behaviour of the wastewater treatment system through anomaly detection, supporting decision making on it. SWIMS has been implemented in a real case study deployed in Briatico, Italy. The data and results collected from such a case study are presented, analyzed and discussed in this paper, demonstrating the feasibility and the effectiveness of the SWIMS solution."
521,unknown,http://arxiv.org/abs/1804.05839v4,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1804.05839v4,2018-04-16 00:00:00,bigdl: a distributed deep learning framework for big data,"This paper presents BigDL (a distributed deep learning framework for Apache
Spark), which has been used by a variety of users in the industry for building
deep learning applications on production big data platforms. It allows deep
learning applications to run on the Apache Hadoop/Spark cluster so as to
directly process the production data, and as a part of the end-to-end data
analysis pipeline for deployment and management. Unlike existing deep learning
frameworks, BigDL implements distributed, data parallel training directly on
top of the functional compute model (with copy-on-write and coarse-grained
operations) of Spark. We also share real-world experience and ""war stories"" of
users that have adopted BigDL to address their challenges(i.e., how to easily
build end-to-end data analysis and deep learning pipelines for their production
data)."
522,excluded,10.1109/sam53842.2022.9827835,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9827835/,2022-06-23 00:00:00,nemo: internet of things based real-time noise and emissions monitoring system for smart cities,"With the advent of ubiquitous sensors and Internet of Things (IoT) applications, research and development initiatives on smart cities are ramping up worldwide. It enables remote monitoring, management, and control of devices and the generation of fresh and actionable insight from huge quantities of real-time data. Real-time noise and emissions monitoring of vehicles remain indispensable in a smart city context. Effective management and control of noise and emissions of vehicles on the road are necessary and possible through analyzing lots of sensor data in real-time to take an actionable insight. To contribute to this, as part of an ongoing effort of the European Union project called &#x201C;NEMO: Noise and Emissions Monitoring and Radical Mitigation&#x201D;, in this paper, we present the design and development of an IoT-based real-time noise and emissions monitoring system for vehicles in a smart city context. Real-world sensor data of the vehicles in some European cities are collected during the pilot tests. We have developed a complete application for infrastructure managers and analysts to monitor the sensor data related to noise and emissions of vehicles in real-time. The data of the individual road vehicles and trains in selected EU cities and from trains on a track in the Netherlands are collected in the cloud and analyzed with artificial intelligence (AI) algorithms for classification such as high emitter, medium emitter, and normal emitters. We present the development of a complete software solution that can be integrated with existing intelligent transportation systems in smart cities. Finally, we report the initial vehicle classification results from the Rotterdam (Netherlands) pilot test as a representative example for the NEMO monitoring system."
523,unknown,10.1109/bigdataservice49289.2020.00009,2020 IEEE Sixth International Conference on Big Data Computing Service and Applications (BigDataService),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/46b9300c507e50ce45c60e49297c42ed3927b370,2020-01-01 00:00:00,distributed fog computing architecture for real-time anomaly detection in smart meter data,"The use of Fog Computing for real-time Big Data monitoring of power consumption is gaining popularity. In traditional systems, Cloud servers receive sensor Big Data, perform predictions and detect anomalies or any threat patterns and then raise the alarms. With exponentially increasing sensor data, Cloud servers are becoming impractical to process this data because of the issues of volume, velocity, variety, network bandwidth, real-time support and security issues. Fog Computing is introduced as a Distributed Computing paradigm that uses intermediate Computing infrastructure for processing to overcome the limitations of Cloud Computing. In this paper, we propose a hierarchically Distributed Fog Computing architecture to deploy machine learning based anomaly detection models for generating insights from the collected Smart meter sensor data from the household. The anomaly detection is divided into two steps: model training and anomaly detection. We perform detailed analysis and evaluation of the models using standard open datasets obtained from UCI machine learning repository. The results confirm the efficacy of our proposed architecture. We used open source framework and software for our experiments."
524,excluded,10.1038/s41598-022-06201-y,Nature,springer,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1038/s41598-022-06201-y,2022-02-14 00:00:00,cost-effective filtering of unreliable proximity detection results based on ble rssi and imu readings using smartphones,"Indoor environments are a major challenge in the domain of location-based services due to the inability to use GPS. Currently, Bluetooth Low Energy has been the most commonly used technology for such services due to its low cost, low power consumption, ubiquitous availability in smartphones and the dependence of the signal strength on the distance between devices. The article proposes a system that detects the proximity between static (anchors) and moving objects, evaluates the quality of this prediction and filters out the unreliable results based on custom metrics. We define three metrics: two matrics based on RSSI and Intertial Measurement Unit (IMU) readings and one joint metric. This way the filtering is based on both, the external information (RSSI) and the internal information (IMU). To process the IMU data, we use machine learning activity recognition models (we apply feature selection and compare three models and choose the best one—Gradient Boosted Decision Trees). The proposed system is flexible and can be easily customized. The great majority of operations can be conducted directly on smartphones. The solution is easy to implement, cost-efficient and can be deployed in real-life applications (MICE industry, museums, industry)."
525,excluded,10.1080/09720529.2021.1932915,Journal of Discrete Mathematical Sciences and Cryptography,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/bfd2beb862cdbedd2b96d402fea599843283e47c,2021-01-01 00:00:00,machine learning based mac protocol design for pipeline leakage detection in smart city project,"Abstract The integration of WSN with IoT is growing exponentially for implementing real time applications in today’s world. Also an Intelligent WSN design is required for implementing these applications. This paper discusses a MWSN based system for detection and alert of the pipeline leakage. In this method all the nodes are not allowed to communicate with the central server directly. Whenever a node receives a data of some irregularities in the system, the node sends it in multihop fashion to the nearest cluster node, which is the only node that is allowed to communicate with the central server. In the remote areas with no infrastructure of internet connection, this system can used to transmit the data to the server, thus the system is cost effective and efficient. As the number of sensor nodes increases and time bound data is required at the IoT devices and cloud for implementing smart applications. In the first phase the paper discussed the deployment strategy of wireless sensor nodes on pipe. In Phase 2, the paper discussed the ML based MAC protocol design. Here ML algorithm is discussed in the paper for energy efficient information transmission node to node. The duty cycle will be tuned based on ML models as discussed. The decision tree classifier will be finally chosen to tune the duty cycle of wireless sensor node. In the final phase the IoT application and website designing is discussed to collect the data and send it to cloud for observing the status of leakage in each zone and sending the alerts to area manager. The work is novel in terms of intelligent model development for pipeline leakage detection and has can be modified for number of mission critical scenarios."
526,unknown,10.1109/avss.2018.8639168,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8639168/,2018-11-30 00:00:00,a practical person monitoring system for city security,"Recent progress in Deep Learning(DL) has brought many breakthroughs with incredible performance, which have not been achieved with traditional machine learning algorithms. In computer vision, DL-based methods have started to outperform humans in certain tasks and are going to impact our daily lives. We present our case study of an implementation and evaluation of our prototype real-time person-monitoring system using cutting-edge DL computer vision techniques. We used a fast and lightweight stream-processing engine for its flexibility and portability, packaged all of DL software stacks as docker containers for portability and ease of deployment, and evaluated our prototype's performance using realistic scenarios in which one hundred camera streams are gathered at centered GPU servers. We confirmed that our prototype system can monitor one hundred video streams in real-time. We also report lessons learned through our prototype implementation and discuss the future direction of person monitoring."
527,unknown,http://arxiv.org/abs/2006.06082v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2006.06082v3,2020-06-10 00:00:00,towards integrating fairness transparently in industrial applications,"Numerous Machine Learning (ML) bias-related failures in recent years have led
to scrutiny of how companies incorporate aspects of transparency and
accountability in their ML lifecycles. Companies have a responsibility to
monitor ML processes for bias and mitigate any bias detected, ensure business
product integrity, preserve customer loyalty, and protect brand image.
Challenges specific to industry ML projects can be broadly categorized into
principled documentation, human oversight, and need for mechanisms that enable
information reuse and improve cost efficiency. We highlight specific roadblocks
and propose conceptual solutions on a per-category basis for ML practitioners
and organizational subject matter experts. Our systematic approach tackles
these challenges by integrating mechanized and human-in-the-loop components in
bias detection, mitigation, and documentation of projects at various stages of
the ML lifecycle. To motivate the implementation of our system -- SIFT (System
to Integrate Fairness Transparently) -- we present its structural primitives
with an example real-world use case on how it can be used to identify potential
biases and determine appropriate mitigation strategies in a participatory
manner."
528,unknown,10.1155/2020/8894705,'Hindawi Limited',core,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),,2020-01-01 00:00:00,scalable system for smart urban transport management,"Efficient management of smart transport systems requires the integration of various sensing technologies, as well as fast processing of a high volume of heterogeneous data, in order to perform smart analytics of urban networks in real time. However, dynamic response that relies on intelligent demand-side transport management is particularly challenging due to the increasing flow of transmitted sensor data. In this work, a novel smart service-driven, adaptable middleware architecture is proposed to acquire, store, manipulate, and integrate information from heterogeneous data sources in order to deliver smart analytics aimed at supporting strategic decision-making. The architecture offers adaptive and scalable data integration services for acquiring and processing dynamic data, delivering fast response time, and offering data mining and machine learning models for real-time prediction, combined with advanced visualisation techniques. The proposed solution has been implemented and validated, demonstrating its ability to provide real-time performance on the existing, operational, and large-scale bus network of a European capital city"
529,excluded,10.1109/iccoins49721.2021.9497209,2021 International Conference on Computer & Information Sciences (ICCOINS),semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/b943738687ec6af33a0282cba1646c918690c5df,2021-01-01 00:00:00,"a new framework of smart system for inventory management, stock item locator and navigation","The present paper discloses a patent framework of iNavig (Smart System for Inventory Management, Stock Item Locator and Navigation) for locating stock items stored in a building and navigating to the stock items on shelves. The enhanced system comprises a plurality of weight sensors for detecting a weight of the stock items placed on a storing platform, a video camera assembly to provide a real-time video data of the stock items and a visual appearance of the stock items. A service processing module for receiving a service request from a user via a portable computing device to procure one of the stock items is also presented. A plurality of anchor nodes comprising node microcontrollers (NodeMCU) is deployed in a vicinity of the stock items on the storing platform at a predefined set of locations. For smart navigation features, the system is equipped with an AR navigation map using a camera assembly of the portable computing device itself. Moreover, a stock item indicator utilizing fog computing decentralized system which implements machine learning and intelligent of things will be provided in terms of visual and/or audio notification to the user upon arrival at the said one of the stock items location."
530,excluded,10.1117/12.2218707,SPIE Smart Structures and Materials + Nondestructive Evaluation and Health Monitoring,semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/5c3cffd4545666e09c5b5192186284037693eccc,2016-01-01 00:00:00,the application of data mining and cloud computing techniques in data-driven models for structural health monitoring,"Recently, data-driven models for Structural Health Monitoring (SHM) have been of great interest among many researchers. In data-driven models, the sensed data are processed to determine the structural performance and evaluate the damages of an instrumented structure without necessitating the mathematical modeling of the structure. A framework of data-driven models for online assessment of the condition of a structure has been developed here. The developed framework is intended for automated evaluation of the monitoring data and structural performance by the Internet technology and resources. The main challenges in developing such framework include: (a) utilizing the sensor measurements to estimate and localize the induced damage in a structure by means of signal processing and data mining techniques, and (b) optimizing the computing and storage resources with the aid of cloud services. The main focus in this paper is to demonstrate the efficiency of the proposed framework for real-time damage detection of a multi-story shear-building structure in two damage scenarios (change in mass and stiffness) in various locations. Several features are extracted from the sensed data by signal processing techniques and statistical methods. Machine learning algorithms are deployed to select damage-sensitive features as well as classifying the data to trace the anomaly in the response of the structure. Here, the cloud computing resources from Amazon Web Services (AWS) have been used to implement the proposed framework."
531,included,10.1109/lra.2020.2998414,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9103220/,2020-07-01 00:00:00,rilaas: robot inference and learning as a service,"Programming robots is complicated due to the lack of `plug-and-play' modules for skill acquisition. Virtualizing deployment of deep learning models can facilitate large-scale use/re-use of off-the-shelf functional behaviors. Deploying deep learning models on robots entails real-time, accurate and reliable inference service under varying query load. This letter introduces a novel Robot-Inference-and-Learning-as-a-Service (RILaaS) platform for low-latency and secure inference serving of deep models that can be deployed on robots. Unique features of RILaaS include: 1) low-latency and reliable serving with gRPC under dynamic loads by distributing queries over multiple servers on Edge and Cloud, 2) SSH based authentication coupled with SSL/TLS based encryption for security and privacy of the data, and 3) front-end REST API for sharing, monitoring and visualizing performance metrics of the available models. We report experiments to evaluate the RILaaS platform under varying loads of batch size, number of robots, and various model placement hosts on Cloud, Edge, and Fog for providing benchmark applications of object recognition and grasp planning as a service. We address the complexity of load balancing with a reinforcement learning algorithm that optimizes simulated profiles of networked robots; outperforming several baselines including round robin, least connections, and least model time with 68.30% and 14.04% decrease in round-trip latency time across models compared to the worst and the next best baseline respectively. Details and updates are available at: https://sites.google.com/view/rilaas."
532,unknown,10.1007/978-3-030-82136-4_25,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-82136-4_25,2021-01-01 00:00:00,a research study on running machine learning algorithms on big data with spark,"The design and implementation of proactive fault diagnosis systems concerning the bearings during their manufacturing process requires the selection of robust representation learning techniques, which belong to the broader scope of the machine learning techniques. Particular systems, such as those that are based on machine learning libraries like Scikit-learn, favor the actual processing of the data, while essentially disregarding relevant computational parameters, such as the speed of the data processing, or the consideration of scalability as an important design and implementation feature. This paper describes an integrated machine learning-based data analytics system, which processes the large amounts of data that are generated by the bearings manufacturing processes using a multinode cluster infrastructure. The data analytics system uses an optimally configured and deployed Spark environment. The proposed data analytics system is thoroughly assessed using a large dataset that stores real manufacturing data, which is generated by the respective bearings manufacturing processes. The performance assessment demonstrates that the described approach ensures the timely and scalable processing of the data. This achievement is relevant, as it exceeds the processing capabilities of significant existing data analytics systems."
533,excluded,http://arxiv.org/abs/2005.13601v1,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2005.13601v1,2020-05-27 00:00:00,"the adversarial resilience learning architecture for ai-based modelling,
  exploration, and operation of complex cyber-physical systems","Modern algorithms in the domain of Deep Reinforcement Learning (DRL)
demonstrated remarkable successes; most widely known are those in game-based
scenarios, from ATARI video games to Go and the StarCraft~\textsc{II} real-time
strategy game. However, applications in the domain of modern Cyber-Physical
Systems (CPS) that take advantage a vast variety of DRL algorithms are few. We
assume that the benefits would be considerable: Modern CPS have become
increasingly complex and evolved beyond traditional methods of modelling and
analysis. At the same time, these CPS are confronted with an increasing amount
of stochastic inputs, from volatile energy sources in power grids to broad user
participation stemming from markets. Approaches of system modelling that use
techniques from the domain of Artificial Intelligence (AI) do not focus on
analysis and operation. In this paper, we describe the concept of Adversarial
Resilience Learning (ARL) that formulates a new approach to complex environment
checking and resilient operation: It defines two agent classes, attacker and
defender agents. The quintessence of ARL lies in both agents exploring the
system and training each other without any domain knowledge. Here, we introduce
the ARL software architecture that allows to use a wide range of model-free as
well as model-based DRL-based algorithms, and document results of concrete
experiment runs on a complex power grid."
534,excluded,10.1007/s42991-021-00221-3,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s42991-021-00221-3,2022-04-05 00:00:00,flukebook: an open-source ai platform for cetacean photo identification,"Determining which species are at greatest risk, where they are most vulnerable, and what are the trajectories of their communities and populations is critical for conservation and management. Globally distributed, wide-ranging whales and dolphins present a particular challenge in data collection because no single research team can record data over biologically meaningful areas. Flukebook.org is an open-source web platform that addresses these gaps by providing researchers with the latest computational tools. It integrates photo-identification algorithms with data management, sharing, and privacy infrastructure for whale and dolphin research, enabling the global collaborative study of these global species. With seven automatic identification algorithms trained for 15 different species, resulting in 37 species-specific identification pipelines, Flukebook is an extensible foundation that continually incorporates emerging AI techniques and applies them to cetacean photo identification through continued collaboration between computer vision researchers, software engineers, and biologists. With over 2.0 million photos of over 52,000 identified individual animals submitted by over 250 researchers, the platform enables a comprehensive understanding of cetacean populations, fostering international and cross-institutional collaboration while respecting data ownership and privacy. We outline the technology stack and architecture of Flukebook, its performance on real-world cetacean imagery, and its development as an example of scalable, extensible, and reusable open-source conservation software. Flukebook is a step change in our ability to conduct large-scale research on cetaceans across biologically meaningful geographic ranges, to rapidly iterate population assessments and abundance trajectories, and engage the public in actions to protect them."
535,excluded,10.1109/cbd54617.2021.00060,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9816215/,2022-03-27 00:00:00,a hybrid framework for mobile augmented reality,"Augmented reality (AR) aims to combine real and virtual worlds, offering real-time 3D interactions between users and virtual information. Existed AR systems detect specific markers in environments and augmented them with virtual contents such as 3D objects and videos. With the explosive growth of powerful, less expensive mobile devices, AR systems are extended to mobile devices. However, they fail to achieve the accurate and real-time integration of the real world and virtual contents because of the insufficient detection accuracy to objects in real world and computational resource shortage. In this paper, we propose a hybrid AR framework that integrates cloud computing, 5G communication and deep learning technology to achieve accurate tracking and ultra-low latency mobile AR applications. Our AR framework contains three blocks, an image capturing block, a deep learning based objects tracking block and a WebGL based rendering block. We use cloud computing technology to deploy them on mobile devices and a cloud server separately according to calculation amounts and use 5G to achieve communication between the cloud server and mobiles devices. The experiment results show that the accuracy and running speed of our framework can meet the requirements of most mobile AR applications."
536,unknown,10.1016/s0531-5131(03)00353-4,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/12144250151,2003-06-01,the nasa smart probe project for real-time multiple microsensor tissue recognition: update,"The NASA Smart Probe combines information from multiple microsensors—using fuzzy logic and neural network software—to provide a unique tissue “signature” in real time. This report presents recent advances in the probe architecture itself plus clinical information gathered from women undergoing biopsy for suspected breast cancer by the NASA licensee, BioLuminate (Dublin, CA, USA). The multiparameter Smart Probe for breast cancer—1 mm in diameter—can clearly differentiate normal breast, benign lesions, and breast carcinoma. The sensors employed in the Smart Probe for breast cancer include electrical impedance and optical spectroscopy (OS) (both broadband or white light, and laser light (infrared and blue/fluorescence)). Data are acquired 100 times per second; a typical breast “biopsy” typically generates 500 MB of data. Potential applications of nanoelectrode arrays and the Smart Probe concept for deep brain recording and stimulation in neurosurgery are also noted."
537,excluded,http://arxiv.org/abs/1709.03008v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1709.03008v1,2017-09-09 00:00:00,"identifying irregular power usage by turning predictions into
  holographic spatial visualizations","Power grids are critical infrastructure assets that face non-technical losses
(NTL) such as electricity theft or faulty meters. NTL may range up to 40% of
the total electricity distributed in emerging countries. Industrial NTL
detection systems are still largely based on expert knowledge when deciding
whether to carry out costly on-site inspections of customers. Electricity
providers are reluctant to move to large-scale deployments of automated systems
that learn NTL profiles from data due to the latter's propensity to suggest a
large number of unnecessary inspections. In this paper, we propose a novel
system that combines automated statistical decision making with expert
knowledge. First, we propose a machine learning framework that classifies
customers into NTL or non-NTL using a variety of features derived from the
customers' consumption data. The methodology used is specifically tailored to
the level of noise in the data. Second, in order to allow human experts to feed
their knowledge in the decision loop, we propose a method for visualizing
prediction results at various granularity levels in a spatial hologram. Our
approach allows domain experts to put the classification results into the
context of the data and to incorporate their knowledge for making the final
decisions of which customers to inspect. This work has resulted in appreciable
results on a real-world data set of 3.6M customers. Our system is being
deployed in a commercial NTL detection software."
538,unknown,10.1109/pyhpc51966.2020.00010,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9307950/,2020-11-13 00:00:00,accelerating microstructural analytics with dask for volumetric x-ray images,"While X-ray microtomography has become indispensable in 3D inspections of materials, efficient processing of such volumetric datasets continues to be a challenge. This paper describes a computational environment for HPC to facilitate parallelization of algorithms in computer vision and machine learning needed for microstructure characterization and interpretation. The contribution is to accelerate microstructural analytics by employing Dask high-level parallel abstractions, which scales Numpy workflows to enable multi-dimensional image analysis of diverse specimens. We illustrate our results using an example from materials sciences, emphasizing the benefits of parallel execution of image-dependent tasks. Preliminary results show that the proposed environment configuration and scientific software stack deployed using JupyterLab at NERSC Cori enables near-real time analyses of complex, high-resolution experiments."
539,unknown,10.1109/bigdata.2017.8258392,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8258392/,2017-12-14 00:00:00,a multi-task machine learning approach for comorbid patient prioritization,"With the advent of Internet of Things, remote patient monitoring and diagnosis is becoming the more advanced and accessible feature in healthcare industry regardless of age and disorder. With the increase in patient population, remotely prioritizing and diagnosing patients becomes a major problem. Research has been conducted to address this issue. However, many models fail to support the condition of comorbidity. In this paper, we developed a multi-task machine learning model that supports remote patient prioritization based on disease severity. This model not only considers patients diagnosed with a single disease, but also acknowledges comorbidity of a patient to calculate their severity utilizing physiological data. Classifying a comorbid patient based on their disease(s) severity is a challenging task. We achieve this by exploiting the commonalities and differences between the diseases using multi-task machine learning. Based on the correlation between diseases, we classify the patient into appropriate severity level. In addition to this, our model also supports continuous patient monitoring based on their severity levels, which is made possible by implementing in Apache Spark. Apache Spark also serves as a real-time streaming processor suitable for remote patient monitoring and diagnosis."
540,unknown,10.3233/shti220352,dHealth,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/7e9a68155b4d14fed1d126ac2f51ca969b1ea304,2022-01-01 00:00:00,promoting the importance of recall visits among dental patients in india using a semi-autonomous ai system,"In many developing countries like India, there is a widespread lack of general awareness about the importance of good oral health, which causes dental patients to neglect their oral hygiene, thus precipitating many long-term ailments. We developed an application that promotes the significance of regular dental checkups and oral health care by explaining to patients how these are intrinsic to overall health. Our application, in essence, extracts relevant health information from published scientific studies according to a patient's medical history and shares it with the patient at the discretion of the supervising dentist, thereby empowering patients to make more informed decisions. We present a detailed overview of our semi- autonomous machine learning-based solution, along with the complex challenges involved in the design, development, and real-world deployment of our application. Finally, we conducted a randomized parallel-group study in India with 224 dental patients over two years to assess the utility of our proposed solution. Results show our application improved the patient recall rate from 21.1% to 37.8% (p-value = 0.024)."
541,included,10.1109/icsssm.2016.7538620,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7538620/,2016-06-26 00:00:00,emotion-based social computing platform for streaming big-data: architecture and application,"Exploration of user generated content in the epoch of Web 2.0 brings unprecedented challenge to the social computing, which has to provide real-time solution in the circumstance of massive data volumes and evolving application scenarios. This paper presents an emotion-based social computing platform namely ESC for streaming big-data. The main aim of ESC is to provide sentiment analysis as the foundation of social computing and enable both real-time computation on streaming big-data and batch computation on off-line big-data with high performance and low risk. Different from conventional data processing technologies, ESC is designed as a scalable and QoS-optimized adaptive platform for developers to only focus on business models instead of being distracted by details of the computing infrastructure. In addition, continuous streaming computing is emphasized in ESC to keep tracking on long term dynamic evolution in social media, which can provide a valuable proxy for in-depth social analytics. The architecture of ESC is implemented by distributed storage, sentiment analysis, data parallelism and routing, real-time streaming computation, batch computation and distributed machine learning. And the evaluation results from real-time and batch computations testify the high performance and scalability of ESC. Moreover, a few applications based on it further demonstrates its usability in enacting on different streaming big-data and variety of social computations."
542,unknown,10.1109/icde.2019.00205,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8731598/,2019-04-11 00:00:00,fair: fraud aware impression regulation system in large-scale real-time e-commerce search platform,"Fraud sellers in e-commerce usually promote their products via fake transactions. Such behaviors damage the reputation of the e-commerce platform and jeopardize the business environment in the platform. The search engine of existing e-commerce platforms mainly focuses on generating transactions by matching users' queries and sellers' products. The most common method to defense fraud sellers is to set up a blacklist based on fraud detection and manual investigation, and then punish those sellers in the list, which is inefficient and can only cover a small fraction of potential fraud sellers. In this paper, we propose the first fraud aware impression regulation system (FAIR) which is data-driven and can work in large-scale e-commerce platforms. Its main function is to actively regulate the impressions received by all potential fraud sellers in a real-time fashion. It utilizes the reinforcement learning architecture to dynamically adjust the impression regulation strategy under different reward settings, which can not only promote the impression regulation effects, but also improve the revenue of the platform simultaneously. We deploy FAIR on the Taobao platform of Alibaba, one of the world's largest e-commerce search platform, and perform an A/B test for two weeks. The results show that FAIR can effectively reduce the fraud impressions and improve the overall platform revenue at the same time."
543,unknown,10.1109/qsic.2009.26,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5381489/,2009-08-25 00:00:00,application of metamorphic testing to supervised classifiers,"Many applications in the field of scientific computing - such as computational biology, computational linguistics, and others - depend on Machine Learning algorithms to provide important core functionality to support solutions in the particular problem domains. However, it is difficult to test such applications because often there is no ""test oracle"" to indicate what the correct output should be for arbitrary input. To help address the quality of such software, in this paper we present a technique for testing the implementations of supervised machine learning classification algorithms on which such scientific computing software depends. Our technique is based on an approach called ""metamorphic testing"", which has been shown to be effective in such cases. More importantly, we demonstrate that our technique not only serves the purpose of verification, but also can be applied in validation. In addition to presenting our technique, we describe a case study we performed on a real-world machine learning application framework, and discuss how programmers implementing machine learning algorithms can avoid the common pitfalls discovered in our study. We also discuss how our findings can be of use to other areas outside scientific computing, as well."
544,excluded,10.1109/iemcon53756.2021.9623141,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9623141/,2021-10-30 00:00:00,design and development of an integrated internet of audio and video sensors for covid-19 coughing and sneezing recognition,"There are a lot of ongoing efforts to combat the COVID-19 pandemic using different combinations of low-cost sensing technologies, information/communication technologies, and smart computation. To provide COVID-19 situational awareness and early warnings, a scalable, real-time sensing solution is needed to recognize risky behaviors in COVID-19 virus spreading such as coughing and sneezing. Various coughing and sneezing recognition methods use audio-only or video-only sensors and Deep Learning (DL) algorithms for smart event recognition. However, each of these recognition processes experiences several types of failure behaviors due to false detection. Sensor integration is a solution to overcome such failures. Moreover, it improves event recognition precision. With the wide availability of low-cost audio and video sensors, we proposed a real-time integrated Internet of Things (IoT) architecture to improve the results of coughing and sneezing recognition. Implemented architecture joins edge and cloud computing. In edge computing, the microphone and camera are connected to the internet and embedded with a DL engine. Audio and video streams are fed to edge computing to detect coughing and sneezing actions in realtime. Cloud computing, which is developed based on the Amazon Web Service (AWS), combines the results of audio and video processing. In this paper, a scenario of a person coughing and sneezing was developed to demonstrate the capabilities of the proposed architecture. The experimental results show that the proposed architecture improved the reliability of coughing and sneezing recognition in the integrated cloud system compared to audio-only and video-only detectors. Three factors have been considered to compare the results of the proposed architecture: F-score, precision, and recall. The precision and recall of the cloud detector are improved on average by &#x0025;43 and &#x0025;15, respectively, compared to audio-only and video-only detectors. The F-score improved on average 1.24 times."
545,unknown,10.4043/29576-ms,"Day 1 Mon, May 06, 2019",semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/a11b6a988b1342c928f400d229207a5d6ffb586c,2019-01-01 00:00:00,extracting value from data using an industrial data platform to provide a foundational digital twin,"
 Oil & Gas data currently exists within a world of data silos. Lack of data is not the challenge. A wide variety of data is collected, including sensor values, P&IDs, ERP, and depth-based trajectories. Rather, the challenge pertains to data usefulness. The root of the problem is a combination of factors, including poor data infrastructure, incompatible operational data systems, and restricted data access. All this translates to a low maturity of digitalization across the Oil & Gas industry. To date, digitalization efforts have been limited to pilot projects, proofs of concept and case studies, with no large-scale operationalized projects.
 Aker BP, one of Europe's largest independent Oil & Gas companies, has broken through the typical roadblocks by deploying an industrial data platform across all five of its operational assets. The platform aggregates and processes data from sensors and contextualizes it, structuring it in relation to process diagrams, production information, 3D-models, and event data (maintenance, incidents). Everything linked in the real world is also linked in the platform. This has dramatically reduced the cost of integration and maintenance, while simultaneously enabling scalability, speed of development, and data openness throughout the Aker BP organization. The data platform handles live and historical data for close to 200,000 sensors, with a peak transfer of 800,000 data points per second. Internal and external experts are able to apply state-of-the-art algorithms to visualize and solve critical business problems. A range of third-party applications and data scientists also use the 1+ trillion data points in the platform to create value and support Aker BP's strategy for day-to-day operations and long-term digital transformation.
 To realize the promise of digitalization, unlocking the value of data must be made a priority within the Oil & Gas industry. This paper will describe the implementation of the industrial data platform, explaining how data streamed from many, disparate, underlying systems is contextualized in the data platform to provide a holistic view of all processes and operations, thus creating a foundational digital twin for each asset, ready to empower machine learning applications for optimization and automatization, as well as human-facing applications, such as advanced visualizations and apps for the digital field worker."
546,excluded,10.1007/978-3-7643-8900-0_6,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-7643-8900-0_6,2009-01-01 00:00:00,a methodology for developing environmental information systems with software agents,"This article presents a unifying methodology for developing environmental information systems with software agents. Based on the experience reported in recent literature, we abstract common requirements of environmental information systems into agent types, combine state-of-the-art tools from computer science, service-oriented software engineering and artificial intelligence domains, as software agents and machine learning, and illustrate their potential for solving real-world problems. Specifically, two generic agent types are specified that behave as information carriers and decision makers, which provide an appropriate abstraction for deployment of added-value services in environmental information systems. A concrete pathway for applying these instruments throughout the software life cycle of an environmental information system is outlined, along with suggestions for software specification and deployment tools. The method is demonstrated in two application domains: one for air quality assessment and another for meteorological radar data surveillance."
547,excluded,10.1109/iota.2016.7562703,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7562703/,2016-01-24 00:00:00,iot and distributed machine learning powered optimal state recommender solution,"Recommender systems add significant benefits to E-commerce in terms of sale conversion, revenues, customer experience, loyalty and lifetime value. But the recommendations from these systems do not change on inputs beyond user and item profile and transaction data. There have been some attempts in the past to optimize on more varied data in recommenders, example of which is the location based recommenders. But location is just one dimension of the state that a user could have shared with GPS/GLONASS/BaiDeu sensor available in most Smartphones. With an upcoming era of Smart-wears and pervasive IoTs, there are a lot many other dimensions of a user state which can be utilized to optimize upon the concept of Optimal State Recommender Solutions. This paper suggests upgrading from conventional recommendations that are based on user/ item preferences alone with systems that provide the best recommendation at the most optimal state when the user is most receptive to accept the recommendation, the “optimal state recommendation solution” and proposes solutions and architectures to overcome the challenges of dealing with real time, distributed machine learning on IoT scale data in implementing this solution. The paper leverages some of the advance distributed machine learning algorithms like variants of Distributed Kalman Filters, Distributed Alternating Least Square Recommenders, Distributed Mini-Batch Stochastic Gradient Descent(SGD) based Classifiers, and highly scalable distributed computation and machine learning platforms like Apache Spark, (Apache) Spark MLlib, Spark Streaming, Python/PySpark, R/SparkR, Apache Kafka in an high performance, distributed, fault tolerant architecture. The solution also aspires to be compliant with upcoming IoT standards and architectures like IEEE P2413 to provide a standard solution for such problems beyond the current scope of this paper."
548,excluded,http://arxiv.org/abs/2207.01137v1,arxiv,arxiv,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2207.01137v1,2022-07-03 00:00:00,"promotheus: an end-to-end machine learning framework for optimizing
  markdown in online fashion e-commerce","Managing discount promotional events (""markdown"") is a significant part of
running an e-commerce business, and inefficiencies here can significantly
hamper a retailer's profitability. Traditional approaches for tackling this
problem rely heavily on price elasticity modelling. However, the partial
information nature of price elasticity modelling, together with the
non-negotiable responsibility for protecting profitability, mean that machine
learning practitioners must often go through great lengths to define strategies
for measuring offline model quality. In the face of this, many retailers fall
back on rule-based methods, thus forgoing significant gains in profitability
that can be captured by machine learning. In this paper, we introduce two novel
end-to-end markdown management systems for optimising markdown at different
stages of a retailer's journey. The first system, ""Ithax"", enacts a rational
supply-side pricing strategy without demand estimation, and can be usefully
deployed as a ""cold start"" solution to collect markdown data while maintaining
revenue control. The second system, ""Promotheus"", presents a full framework for
markdown optimization with price elasticity. We describe in detail the specific
modelling and validation procedures that, within our experience, have been
crucial to building a system that performs robustly in the real world. Both
markdown systems achieve superior profitability compared to decisions made by
our experienced operations teams in a controlled online test, with improvements
of 86% (Promotheus) and 79% (Ithax) relative to manual strategies. These
systems have been deployed to manage markdown at ASOS.com, and both systems can
be fruitfully deployed for price optimization across a wide variety of retail
e-commerce settings."
549,excluded,10.17762/turcomat.v12i7.2670,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/c17d2d329f367c18c091afa4ebe046c2b2b1ac47,2021-01-01 00:00:00,frame work to classify data in interactive system to enhance decision making,"Decision-making is a process of choosing among alternative courses of action for the purpose of attaining a goal or goals. The ultimate objective of data analytics is to ease the decision making process but this has lot of challenges and proper planning is the only way to overcome. The idea behind this research work is to propose a novel framework for data analytics to make effective decisions in an organization by aiding the various stages of the decision making mechanism. According to the design science methodology, the research has been formulated and used in the frame work design process. A novel framework was proposed that combines different aspects of data analytics, needed architectures and tools are incorporated in the various stages of decision making process. Based on the Simons the decision-making process, a new framework was designed with 4 phases namely Data, analytical, model deployment and visualization. The decisive objective of the proposed framework is to ease the process of decision making and also to take effective decision. 
In the process of future planning by the organization, it needs simple accurate estimation techniques for predictions to make effective decisions. Predictions always deal with the future events based on past incidents or records. Different kinds of predictions have been done regularly in many fields for the benefit of an individual, a group of people, an organization or a country. Support Vector Machines can be used to create a powerful prediction model because of its capability in classification and regression. The purpose of this research work is to develop a decision support system model was developed using novel algorithm. The newly developed framework has been proposed for the purpose of data analytics and for prediction.  In this work, the machine learning algorithms Support Vector Machines (SVM), Random Forest, Decision Tree, Naive Bayes and the newly proposed algorithm has been analyzed and results are compared. The outcome of this research work proves that the proposed framework model provides better result than other model. 
The objective of designing and developing the proposed framework is to ease the process of decision making in the scenario of interactive system. A student performance assessment is used to evaluate the proposed framework using real data. To test the correctness of proposed framework, an experiment was done with the student data to predict student performance using newly proposed machine learning framework. The result justifies the proposed framework for the decision making process, gives added value."
550,excluded,10.1016/j.jbi.2021.103922,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85116551006,2021-11-01,predicting potential palliative care beneficiaries for health plans: a generalized machine learning pipeline,"Recognizing that palliative care improves the care quality and reduces the healthcare costs for individuals in their end of life, health plan providers strive to better enroll the appropriate target population for palliative care. Current research has not adequately addressed challenges related to proactively select potential palliative care beneficiaries from a population health perspective. This study presents a Generalized Machine Learning Pipeline (GMLP) to predict palliative needs in patients using administrative claims data. The GMLP has five steps: data cohort creation, feature engineering, predictive modeling, scoring beneficiaries, and model maintenance. It encapsulates principles of population health management, business domain knowledge, and machine learning (ML) process knowledge with an innovative data pull strategy. The GMLP was applied in a regional health plan using a data cohort of 17,197 patients. Multiple ML models were turned and evaluated against a custom performance metric based on the business requirement. The best model was an AdaBoost model with a precision of 71.43% and a recall of 67.98%. The post-implementation evaluation of the GMLP showed that it increased the recall of high mortality risk patients, improved their quality of life, and reduced the overall cost. The GMLP is a novel approach that can be applied agnostically to the data and specific ML algorithms. To the best of our knowledge, it is the first attempt to continuously score palliative care beneficiaries using administrative data. The GMLP and its use case example presented in the paper can serve as a methodological guide for different health plans and healthcare policymakers to apply ML in solving real-world clinical challenges, such as palliative care management and other similar risk-stratified care management workflows."
551,unknown,10.1109/vr.2011.5759457,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5759457/,2011-03-23 00:00:00,simulator x: a scalable and concurrent architecture for intelligent realtime interactive systems,"This article presents a platform for software technology research in the area of intelligent Realtime Interactive Systems. Simulator X is targeted at Virtual Reality, Augmented Reality, Mixed Reality, and computer games. It provides a foundation and testbed for a variety of different application models. The current research architecture is based on the actor model to support fine grained concurrency and parallelism. Its design follows the minimize coupling and maximize cohesion software engineering principle. A distributed world state and execution scheme is combined with an object-centered world view based on an entity model. Entities conceptually aggregate properties internally represented by state variables. An asynchronous event mechanism allows intra- and interprocess communication between the simulation actors. An extensible world interface uses an ontology-based semantic annotation layer to provide a coherent world view of the resulting distributed world state and execution scheme to application developers. The world interface greatly simplifies configurability and the semantic layer provides a solid foundation for the integration of different Artificial Intelligence components. The current architecture is implemented in Scala using the Java virtual machine. This choice additionally fosters low-level scalability, portability, and reusability."
552,unknown,10.1109/ecrime.2012.6489521,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6489521/,2012-10-24 00:00:00,phishari: automatic realtime phishing detection on twitter,"With the advent of online social media, phishers have started using social networks like Twitter, Facebook, and Foursquare to spread phishing scams. Twitter is an immensely popular micro-blogging network where people post short messages of 140 characters called tweets. It has over 100 million active users who post about 200 million tweets everyday. Phishers have started using Twitter as a medium to spread phishing because of this vast information dissemination. Further, it is difficult to detect phishing on Twitter unlike emails because of the quick spread of phishing links in the network, short size of the content, and use of URL obfuscation to shorten the URL. Our technique, PhishAri, detects phishing on Twitter in realtime. We use Twitter specific features along with URL features to detect whether a tweet posted with a URL is phishing or not. Some of the Twitter specific features we use are tweet content and its characteristics like length, hashtags, and mentions. Other Twitter features used are the characteristics of the Twitter user posting the tweet such as age of the account, number of tweets, and the follower-followee ratio. These twitter specific features coupled with URL based features prove to be a strong mechanism to detect phishing tweets. We use machine learning classification techniques and detect phishing tweets with an accuracy of 92.52%. We have deployed our system for end-users by providing an easy to use Chrome browser extension. The extension works in realtime and classifies a tweet as phishing or safe. In this research, we show that we are able to detect phishing tweets at zero hour with high accuracy which is much faster than public blacklists and as well as Twitter's own defense mechanism to detect malicious content. We also performed a quick user evaluation of PhishAri in a laboratory study to evaluate the usability and effectiveness of PhishAri and showed that users like and find it convenient to use PhishAri in real-world. To the best of our knowledge, this is the first realtime, comprehensive and usable system to detect phishing on Twitter."
553,unknown,10.1109/atc.2016.7764786,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7764786/,2016-10-14 00:00:00,automatic license plate recognition using mobile device,"The rapid development of intelligent transport systems (ITS) brings us a safer and more convenient life. In this field, automatic license plate recognition (ALPR) plays an important role in many applications which have been deployed in reality such as stolen car detection, parking system management, and automatic transport charging system. The traditional ALPR methods usually need a high resolution camera to capture good quality images and a powerful computer to process the complex algorithms. However, with many great scientific breakthroughs in mobile device technology, we can easily buy smartphone/tablet which equipped strong central processing unit (CPU) and excellent camera to meet the requirements of ALPR. In this paper, we develop an Android program which processes the image captured by built-in camera of mobile device to have license plate number and save it into database for further applications. Open-source OpenCV libraries are imported into project for some image processing steps with purpose of programming time saving. We apply Tesseract engine and neural network which are two optical character recognition (OCR) methods to convert license plate image to machine-encoded text."
554,unknown,10.13052/jmm1550-4646.1811,J. Mobile Multimedia,semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/ea7dc642f21359301b3d62a50b734328ac01b177,2021-01-01 00:00:00,service orchestration for object detection on edge and cloud in dependable industrial vehicles,"Industrial applications, including autonomous systems and vehicles, rely on processing data on multiple physical devices. The composition of functionality across heterogeneous computing infrastructure is challenging, and will likely get even more challenging in the future as software in vehicles is updated to introduce new features and ensure the safety. New soft real-time use cases emerge and in such cases the model of offloading processing from a limited or malfunctioning device is a viable solution. This study examines orchestration of services across edge and cloud for an industrial vehicle application use case involving image based object detection using machine learning (ML) based models. First, service orchestration requirements are defined taking into account the dependable nature of industrial vehicle applications. Second, an implementation based on Arrowhead framework is presented and evaluated. The open Arrowhead framework offers means for dynamic service discovery, authorization and late binding of computational units. The feasibility of object detection as a service and the suitability of Arrowhead framework to support such orchestrations across edge and cloud is assessed."
555,excluded,10.1109/icmcce51767.2020.00464,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9421705/,2020-12-27 00:00:00,edge-based fashion detection by transfer learning,"Fashion detection is a technology of detection, classification, and localization that is applied to fashion attributes. This technology is usually implemented by object detection which is becoming especially popular recently. It can be applied in real life, such as extracting fashion attribute of fashion in a certain picture, which is the first step to achieve fashion retrieval, or used as a detection module of the dress-changing application, which helps consumer transform their garment to allow them to know whether this costume suits them or not and further boosts e-commerce platform's sales volume. Therefore, fashion detection has become a new emerging promising research field that has captured many researchers' attention. Transfer-learning is a recently developed new technology and extensively applied in multiple fields. This paper applies transfer-learning-based object detection on a fashion dataset, which is fashion detection. Specifically, first the operation is packing training and testing data into a specific file format which is “.tfrecord”, then uploading them and its' corresponding bounding box labels on Google Cloud Storage. After configuring the pre-trained model and training parameter, the training process follows with Tensorflow Object Detection which is Google's deep learning open-source API. Finally, under particular conditions, this model achieves good prediction performance on Color-Fashion Dataset. Besides, this work includes employing a light-weight model on the Android platform with Android Studio, which allows visualization of this work."
556,unknown,10.1109/ic-etite47903.2020.203,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9077722/,2020-02-25 00:00:00,automated detection of driving pathway using image processing,"Image data is one of the most popular real world input data that can be used for variety of applications ranging from robotics and computer vision to security systems. In combination with other methods such as neural network, Artificial neural network and image processing techniques, manipulation of image data can lead to applications such as detection of objects, tracking, identification and vision based robotics and so on. Advanced Driver Assistance System (ADAS) also use image for camera based driver assistance systems.The report covers a hardware model system that tests the software work of detection of traffic signs and path for it own ADAS systems. Different problems were tackled, including the choice of OS, and additional hardware components needed to tackle. The choice of programming languages, equipment, OS and methods were based on simplicity and practicality. Artificial neural network in combination with Open CV libraries were used for stop sign, traffic light and path road detection. The hardware model consisted of RC Car attached to raspberry pi board with a mounted pi camera for video streaming and an arduino controller attached to a radio transmitter for controlling through Open CV running in windows PC."
557,excluded,10.3390/s22082821,Sensors,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d3eb93449328ee7b9913f0f9a54bc46f12c312a7,2022-01-01 00:00:00,data twin-driven cyber-physical factory for smart manufacturing,"Because of the complex production processes and technology-intensive operations that take place in the aerospace and defense industry, introducing Industry 4.0 into the manufacturing processes of aircraft composite materials is inevitable. Digital Twin and Cyber-Physical Systems in Industry 4.0 are key techniques to develop digital manufacturing. Since it is very difficult to create high-fidelity virtual models, the development of digital manufacturing for aircraft manufacturers is challenging. In this study, we provide a view from a data simulation perspective and adopt machine learning approaches to simplify the high-fidelity virtual models in Digital Twin. The novel concept is called Data Twin, and the deployable service to support the simulation is known as the Data Twin Service (DTS). Relying on the DTS, we also propose a microservice software architecture, Cyber-Physical Factory (CPF), to simulate the shop floor environment. Additionally, there are two war rooms in the CPF that can be used to establish a collaborative platform: one is the Physical War Room, used to integrate real data, and the other is the Cyber War Room for handling simulation data and the results of the CPF."
558,unknown,10.1109/cloudcom.2019.00066,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8968845/,2019-12-13 00:00:00,performance analysis of data parallelism technique in machine learning for human activity recognition using lstm,"Human activity recognition (HAR), driven by large deep learning models, has received a lot of attention in recent years due to its high applicability in diverse application domains, manipulate time-series data to speculate on activities. Meanwhile, the cloud term ""as-a-service"" has essentially revolutionized the information technology industry market over the last ten years. These two trends somehow are incorporating to inspire a new model for the assistive living application: HAR as a service in the cloud. However, with frequently updates deep learning frameworks in open source communities as well as various new hardware features release, which make a significant software management challenge for deep learning model developers. To address this problem, container techniques are widely employed to facilitate the deep learning software development cycle. In addition, models and the available datasets are being larger and more complicated, and so, an expanding amount of computing resources is desired so that these models are trained in a feasible amount of time. This requires an emerging distributed training approach, called data parallelism, to achieve low resource utilization and faster execution in training time. Therefore, in this paper, we apply the data parallelism to build an assistive living HAR application using LSTM model, deploying in containers within a Kubernetes cluster to enable the real-time recognition as well as prediction of changes in human activity patterns. We then systematically measure the influence of this technique on the performance of the HAR application. Firstly, we evaluate our system performance with regard to CPU and GPU when deployed in containers and host environment, then analyze the outcomes to verify the difference in terms of the model learning performance. Through the experiments, we figure out that data parallelism strategy is efficient for improving model learning performance. In addition, this technique helps to increase the scaling efficiency in our system."
559,excluded,10.1016/j.ifacol.2015.07.010,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/84992490572,2015-06-01,development of handheld cardiac event monitoring system,"This paper contributes the development, prototyping and analysis of proposed methodology on ARM (Advanced RISC Machine) in laboratory for automatic detection of arrhythmia beat in real-time for diagnosis of cardiovascular diseases. The methodology involves the integration of R peak detection algorithm, Principal Component Analysis for feature extraction and feedforward neural network architecture to classify generic heartbeats into six classes. The proposed methodology is implemented on ARM-based SoC (System-on-Chip) platform for diagnosis of six heartbeats. This developed system is validated by generating realtime ECG beats using MIT-BIH database and the output of the proposed system is monitored in the displaying device. The performance metrics of the developed system yields an overall accuracy of 92.81% with average sensitivity, specificity and positive predictivity of 92.68%, 98.51% and 92.42% respectively. Moreover, the developed system can be fabricated into a handheld device for automatic ECG beat monitoring."
560,unknown,10.1007/978-3-030-90176-9_32,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-90176-9_32,2021-01-01 00:00:00,the design and evaluation of a chatbot for human resources,"Technological innovations in artificial intelligence and machine learning enable business operators to engage with their customers 24/7 through chatbots. Many customers expect around-the-clock support which puts a strain on human resources; augmenting human resources with a chatbot can reduce costs for an organization and increase customer satisfaction. This work presents the HR Chatbot: a chatbot that answers general questions about human resource topics (i.e. payroll, benefits) for a private university. The research involves a collaboration between computer scientists, user experience researchers, and human resource administration. This work addresses two research questions: What are employees at a private university looking for from a chatbot for human resources?; and what are the appropriate methods to evaluate and measure the success of a chatbot for human resources? The HR Chatbot uses IBM Watson Assistant services, and an initial prototype was designed from a document of 31 frequently asked questions. Three rounds of user testing were conducted with employees of the university. The initial tests revealed that the chatbot was perceived as useful, but many were dissatisfied with the responses, specifically the lack of responses. Errors in the chatbot were classified into different categories; the most common being that the question was not in the content scope for the chatbot. Thus, data from the initial studies informed the scope of the chatbot; the number of unique questions grew to 157 and the total number of questions increased to 463. The HR Chatbot has 90% accuracy and an average sustained usability score of 69.5, surpassing the benchmark score. Following the initial tests, the HR Chatbot was deployed in real-time on the human resources website. This work describes how a chatbot was created, evaluated, and deployed online. We hope that this work inspires and informs others to explore similar use cases with chatbot technologies."
561,excluded,10.1109/iske54062.2021.9755365,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9755365/,2021-11-28 00:00:00,a feasible system of automatic flame detection and tracking for fire-fighting robot,"Traditional fire-fighting robots are limited by fire detection and location technology. The detection and location accuracy are greatly affected by the environment, resulting in poor performance, complex deployment and low intellectualization. In this work, a set of automatic fire location and tracking system is designed and implemented based on deep learning, which is also integrated with video image processing technology and open source computer vision library (OpenCV). The system uses a high-real-time deep learning model for fire detection, and it eliminates false alarms by calculating and comparing the structural similarity ratio of the images, as well as combined with the dynamic characteristics of the flame, thereby further improving the detection accuracy. Additionally, our work facilitates the deployment by taking advantage of the monocular camera to locate and track the fire source. Experimental results have demonstrated that the system delivers advantages such as high detection accuracy, good real-time performance, long monitoring distance, and fast response speed. These results also allow the proposed system to be a prime candidate for fire-fighting robots in various complex environments."
562,unknown,10.1109/i4cs.2015.7294480,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7294480/,2015-07-10 00:00:00,happy hour - improving mood with an emotionally aware application,"Mobile sensing in Cyber-Physical Systems has been evolving proportionally with smartphones. In fact, we are witnessing a tremendous increase in systems that sense various facets of human beings and their surrounding environments. In particular, the detection of human emotions can lead to emotionally-aware applications that use this information to benefit people's daily lives. This work presents the implementation of a Human-inthe- loop emotionally-aware Cyber-Physical System that attempts to positively impact its user's mood through moderate walking exercise. Data from smartphone sensors, a smartshirt's electrocardiogram and weather information from a web API are processed through a machine learning algorithm to infer emotional states. When negative emotions are detected, the application timely suggests walking exercises, while providing real-time information regarding nearby points of interest. This information includes events, background music, attendance, agitation and general mood. In addition, the system also dynamically adapts privacy and networking configurations based on emotions. The sharing of the user's location on social networks and the device's networking interfaces are configured according to user-defined rules in order to reduce frustration and provide a better Quality of Experience."
563,excluded,10.1109/tlt.2018.2833111,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8354804/,2019-09-01 00:00:00,inquiry-based learning with robogen: an open-source software and hardware platform for robotics and artificial intelligence,"It has often been found that students appreciate hands-on work, and find that they learn more with courses that include a project than those relying solely on conventional lectures and tests. This type of project driven learning is a key component of “Inquiry-based learning” (IBL), which aims at teaching methodology as well as content by incorporating the student as an actor rather than a spectator. Robotics applications are especially well-suited for IBL due to the value of trial and error experience, the multiple possibilities for students to implement their own ideas, and the importance of programming, problem-solving, and electro-mechanical skills in real world engineering and science jobs. Furthermore, robotics platforms can be useful teaching media and learning tools for a variety of topics. Here, we present RoboGen: an open-source, web-based, software, and hardware platform for Robotics and Artificial Intelligence with a particular focus on Evolutionary Robotics. We describe the platform in detail, compare it to existing alternatives, and present results of its use as a platform for Inquiry-based learning within a master's level course at the Ecole Polytechnique Fédérale de Lausanne."
564,excluded,10.15123/uel.8990y,,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1e17a7e5cab1feb5ff3d832c8ead8b430100ff45,2021-01-01 00:00:00,data quality management in large-scale cyber-physical systems,"Cyber-Physical Systems (CPSs) are cross-domain, multi-model, advance information systems that play a significant role in many large-scale infrastructure sectors of smart cities public services such as traffic control, smart transportation control, and environmental and noise monitoring systems. Such systems, typically, involve a substantial number of sensor nodes and other devices that stream and exchange data in real-time and usually are deployed in uncontrolled, broad environments. 
Thus, unexpected measurements may occur due to several internal and external factors, including noise, communication errors, and hardware failures, which may compromise these systems quality of data and raise serious concerns related to safety, reliability, performance, and security. In all cases, these unexpected measurements need to be carefully interpreted and managed based on domain knowledge and computational models. 
Therefore, in this research, data quality challenges were investigated, and a comprehensive, proof of concept, data quality management system was developed to tackle unaddressed data quality challenges in large-scale CPSs. The data quality management system was designed to address data quality challenges associated with detecting: sensor nodes measurement errors, sensor nodes hardware failures, and mismatches in sensor nodes spatial and temporal contextual attributes. Detecting sensor nodes measurement errors associated with the primary data quality dimensions of accuracy, timeliness, completeness, and consistency in large-scale CPSs were investigated using predictive and anomaly analysis models via utilising statistical and machine-learning techniques. Time-series clustering techniques were investigated as a feasible mean for detecting long-segmental outliers as an indicator of sensor nodes’ continuous halting and incipient hardware failures. Furthermore, the quality of the spatial and temporal contextual attributes of sensor nodes observations was investigated using timestamp analysis techniques. 
The different components of the data quality management system were tested and calibrated using benchmark time-series collected from a high-quality, temperature sensor network deployed at the University of East London. Furthermore, the effectiveness of the proposed data quality management system was evaluated using a real-world, large-scale environmental monitoring network consisting of more than 200 temperature sensor nodes distributed around London. 
The data quality management system achieved high accuracy detection rate using LSTM predictive analysis technique and anomaly detection associated with DBSCAN. It successfully identified timeliness and completeness errors in sensor nodes’ measurements using periodicity analysis combined with a rule engine. It achieved up to 100% accuracy in detecting potentially failed sensor nodes using the characteristic-based time-series clustering technique when applied to two days or longer time-series window. Timestamp analysis was adopted effectively for evaluating the quality of temporal and spatial contextual attributes of sensor nodes observations, but only within CPS applications in which using gateway modules is possible."
565,excluded,10.1016/j.micpro.2019.102929,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85075315081,2020-02-01,a fog-cloud based cyber physical system for ulcerative colitis diagnosis and stage classification and management,"Ulcerative Colitis is a fairly common, chronic or long-term disease that causes inflammation of the large intestine. It can be debilitating and can sometimes lead to life threatening complications. Therefore, its diagnosis in nascent stages is important. Healthcare services based on Fog-Cloud assisted Cyber-Physical Systems are emerging as a proactive and efficacious solution to provide remote monitoring of individuals for early detection and consequent management of several diseases. This paper presents a novel IoT-Fog-Cloud assisted Cyber Physical System for diagnosis and stage classification of Ulcerative Colitis using Naïve Bayes classifier and Deep Neural Network respectively. A vital point of this paper is real-time alert generation from Fog Layer in case the user need emergency treatment if he/she is already diagnosed with UC. Finally, analysis results and compiled medical information of each user is stored on cloud. Implementation results of the proposed framework proves its efficiency in diagnosis and subsequent stage classification of Ulcerative Colitis with real-time classification mechanism at fog layer. Furthermore, alert generation improves the efficacy of the proposed system."
566,unknown,10.1109/ic-etite47903.2020.394,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9077698/,2020-02-25 00:00:00,real-time acoustic based depression detection using machine learning techniques,"Depression disorder is predicted to rise to the second leading cause of disability by 2030 as per the identifications of the World Health organization (WHO). Though well trained clinicians, medical and psychological treatments are available for depression treatment, persons or families are reluctant to speak out/reach doctors about this disorder for various social reasons. Diagnosis of depression disorder includes numerous interviews with patient and family, clinical analysis, questionnaires which is time consuming and also demands well trained clinicians. In the present era of Machine learning, automation of depression detection is not complicated and can easily be deployed. However, the automation should use fewer resources, provide accurate results with more reachability. In this paper, acoustic features are used to train a classification model to categorize a human as Depressed or not-Depressed. DIAC-WOZ database available with AVEC2016 challenge is considered for training the classifiers. Prosodic, Spectral and Voice control features are extracted using the COVAREP toolbox and are feature fused. SMOTE analysis is used for overcoming the class imbalance and 93% accuracy is obtained with the SVM algorithm resulting in Depression Classification Model (DCM). An android application cureD Deployed on Cloud is developed to self assess depression using DCM and PHQ-8 questionnaire. The application is tested on real time data of 50 subjects under the supervision of a qualified psychiatrist and an accuracy of 90% is obtained."
567,unknown,10.1109/iciteed.2016.7863311,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7863311/,2016-10-06 00:00:00,android based real-time static indonesian sign language recognition system prototype,"Sign language uses gestures instead of speech sounds to communicate. But in general, normal people rarely trying to learn sign language to interact with the deaf community. Recently, there are many sign language recognition system that had been developed. But most of them were implemented using desktop and laptop computer, which is impractical due to its weight and size. This paper presents a prototype of real time static Indonesian sign language recognition using the Android Smart Phone, so it can be used anywhere and anytime. YCrCb color space combined with skin color detection is used to remove the background image and form a segmented image. Detection contour with convex hull algorithms are used to localize and save an area of the hand. Convexity defect algorithms then are used to extract the hand gesture's features using a radiant line from the center of the palm to the fingertips. The classification of hand gesture that performs sign alphabets is accomplished using back propagation neural network algorithm in order to determine a suitable alphabet. The performance test of the system is done by recognizing some variation hand gesture poses for Indonesian sign language alphabet. The results show that the system can detect the position of the user's hand. Furthermore, the system can recognize the alphabet sign from user hand gesture input, reaching 91.66% success rate in testing using Android devices in real time."
568,unknown,http://arxiv.org/abs/2205.06234v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2205.06234v1,2022-05-12 00:00:00,"sibila: high-performance computing and interpretable machine learning
  join efforts toward personalised medicine in a novel decision-making tool","Background and Objectives: Personalised medicine remains a major challenge
for scientists. The rapid growth of Machine learning and Deep learning has made
it a feasible alternative for predicting the most appropriate therapy for
individual patients. However, the lack of interpretation of their results and
high computational requirements make many reluctant to use these methods.
  Methods: Several Machine learning and Deep learning models have been
implemented into a single software tool, SIBILA. Once the models are trained,
SIBILA applies a range of interpretability methods to identify the input
features that each model considered the most important to predict. In addition,
all the features obtained are put in common to estimate the global attribution
of each variable to the predictions. To facilitate its use by non-experts,
SIBILA is also available to all users free of charge as a web server at
https://bio-hpc.ucam.edu/sibila/.
  Results: SIBILA has been applied to three case studies to show its accuracy
and efficiency in classification and regression problems. The first two cases
proved that SIBILA can make accurate predictions even on uncleaned datasets.
The last case demonstrates that SIBILA can be applied to medical contexts with
real data.
  Conclusion: With the aim of becoming a powerful decision-making tool for
clinicians, SIBILA has been developed. SIBILA is a novel software tool that
leverages interpretable machine learning to make accurate predictions and
explain how models made those decisions. SIBILA can be run on high-performance
computing platforms, drastically reducing computing times."
569,unknown,10.1109/percomworkshops53856.2022.9767294,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9767294/,2022-03-25 00:00:00,a novel iot system for patient-centric pressure ulcer prevention using sensor embedded dressings,"Pressure ulcers/injuries (PU/Is) are localized damage to the skin and/or underlying tissue caused by prolonged pressure to an area of the body. PU/Is affect over 2.5 million individuals in the United States annually, are associated with increased morbidity and mortality, and incur a cost of approximately $11 billion to the US healthcare system. Mitigating PU/Is continues to be a challenging task using traditional methods due to their time-and labor-intensive nature, and existing technological solutions tend to be prohibitively expensive, inefficiently implemented, or ineffective. Thus, there is a clear unmet need for a holistic, end-to-end, hospital-integrated, patient-centric system for PU/I prevention. Such a system can monitor pressure at high-risk areas and uses real-time sensor data, analysis, and visualization to guide clinicians and caregivers to perform effective preventative measures. In this paper, we describe a proof of concept of this system, which integrates: (A) a prototype ""smart wound dressing"" capable of detecting of changes in interface pressure and patient angle over time, including during routine patient repositioning maneuvers; and (B) an open software infrastructure that collects pressure-over-time data, stores, analyzes, and displays them to clinicians and caregivers. We present preliminary results obtained using our current prototype which uses machine learning algorithms to infer a patient's current position based on data from the pressure sensor."
570,unknown,10.1109/iciea49774.2020.9101915,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9101915/,2020-04-21 00:00:00,"distributed and context aware application of deep neural networks in mobile 3d-multi-sensor systems based on cloud-, edge- and fpga-computing","The use of deep neural networks (DNN) for 3Dimage processing significantly enhances visual cognition of mobile systems by considering spatial information. However, training and execution require high computing power. This is crucial in applications with real-time constraints since mobile systems have limited resources. Current approaches do not consider the usage of 3D-sensing. Furthermore, suggested system architectures solely focus on cloud- and edge-computing in combination with load balancing and parallelization for a distributed execution of DNNs. In contrast, we propose a novel system architecture for the distributed and context aware usage of DNNs for image processing tasks in mobile 3D-multi-sensor systems. Thereby, the scalable cloud- and edge-infrastructure is complemented by realtime capable and energy-efficient FPGA-computing. The publishsubscriber pattern facilitates the distributed execution of DNNs as well as their dynamic deployment. Moreover, context information is considered. Thus, a rule-based context model dynamically loads specialized DNNs and selects appropriate devices for execution. Finally, a case-study on a mobile 3D-multi-sensor system for wheeled walkers demonstrates applicability and benefits of the proposed approach."
571,unknown,10.1109/wowmom54355.2022.00031,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9842849/,2022-06-17 00:00:00,context aware adaptive ml inference in mobile-cloud applications,"With the emergence of mobile devices having enough resources to execute real-time ML inference, deployment opportunities arise on mobile devices while keeping privacy-sensitive data close to the source and reducing server load. Moreover, offloading inference to a cloud server facilitates deployment of neural network-based applications on resource-constrained devices. Depending on the application goals and execution context of the application, the optimal deployment on either cloud server or mobile device varies during the lifetime of an application. In this paper, we propose a context-aware middleware that enables optimization of deployed application software to satisfy the application&#x2019;s functional goals in accordance with changing execution context and environmental conditions. We facilitate system design through the abstraction of deployed software components as states and make use of finite state machines and contextual triggers to model the reconfiguration of the system. We evaluate our framework using a real-world nutritional monitoring application via food image recognition deployed in a two-tier mobile and cloud architecture. We compare the proposed solution with various static deployments of the application and show that our approach can react to changing application goals at run-time in order to reduce server load and thereby increase scalability."
572,included,10.1109/icde51399.2021.00211,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9458658/,2021-04-22 00:00:00,catching them red-handed: real-time aggression detection on social media,"Aggression on social media has evolved into a major point of concern. However, recently proposed machine learning (ML) approaches to detect various types of aggressive behavior fall short, due to the fast and increasing pace of content generation as well as evolution of such behavior over time. This work introduces the first, practical, real-time framework for detecting aggression on Twitter via embracing the streaming ML paradigm. This method adapts its ML binary classifiers in an incremental fashion, while receiving new annotated examples, and achieves similar performance as batch-based ML models, with 82-93% accuracy, precision, and recall. Experimental analysis on real Twitter data reveals how this framework, implemented in Spark Streaming, easily scales to process millions of tweets in minutes."
573,unknown,10.1109/icqr2mse.2012.6246340,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6246340/,2012-06-18 00:00:00,a kind of software fault diagnosing framework based on multi-agent,"One of the most expensive and time-consuming activities of the software is locating the fault. Therefore, the software fault diagnosis technology and the Spectrum-based Fault Localization (SFL) is very significant to software quality assurance. Recently, the fault diagnosis technology based on the artificial intelligence theory attracts more and more attention. Multi-Agent Systems (MAS) have autonomy, intelligence and social ability, which is well-suited for diagnosing software systems, and which can easily be integrated with existing software testing schemes of software intensive equipment. In this paper, a kind of 4-layer Framework based on MAS for fault diagnosis is innovative presented. The proposed Framework is applied to real software diagnosis to validate its effectiveness."
574,excluded,10.1016/j.dcan.2021.04.001,'Elsevier BV',core,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-08-01 00:00:00,towards asynchronous federated learning for heterogeneous edge-powered internet of things,"The advancement of the Internet of Things (IoT) brings new opportunities for collecting real-time data and deploying machine learning models. Nonetheless, an individual IoT device may not have adequate computing resources to train and deploy an entire learning model. At the same time, transmitting continuous real-time data to a central server with high computing resource incurs enormous communication costs and raises issues in data security and privacy. Federated learning, a distributed machine learning framework, is a promising solution to train machine learning models with resource-limited devices and edge servers. Yet, the majority of existing works assume an impractically synchronous parameter update manner with homogeneous IoT nodes under stable communication connections. In this paper, we develop an asynchronous federated learning scheme to improve training efficiency for heterogeneous IoT devices under unstable communication network. Particularly, we formulate an asynchronous federated learning model and develop a lightweight node selection algorithm to carry out learning tasks effectively. The proposed algorithm iteratively selects heterogeneous IoT nodes to participate in the global learning aggregation while considering their local computing resource and communication condition. Extensive experimental results demonstrate that our proposed asynchronous federated learning scheme outperforms the state-of-the-art schemes in various settings on independent and identically distributed (i.i.d.) and non-i.i.d. data distribution"
575,unknown,10.1109/aero.1997.577990,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/577990/,1997-02-13 00:00:00,integrating autonomy technologies into an embedded spacecraft system-flight software system engineering for new millennium,"Deep Space 1 (DS1) is the first deep-space mission of NASA's New Millennium technology validation program. The DS1 flight software will validate five autonomy technologies: 1) Planner/Scheduler, which receives ground or on-board requests for spacecraft activities and schedules them to resolve any resource conflicts or timing constraints; 2) Smart Executive, which expands planned activities into lower-level commands, deduces required hardware configurations or other actions, and provides detection and avoidance of constraint violations; 3) Mode Identification and Reconfiguration engine, which incorporates models of hardware and software behavior, detects discrepancies due to hardware or software failures, and requests recovery actions via the Smart Executive. 4) Autonomous Navigation, which determines the spacecraft trajectory from images of asteroids against the celestial sphere, and autonomously adjusts the trajectory to reach the target asteroid or comet. 5) Beacon Monitoring, which uses radio carrier modification and telemetry summarization to simplify ground monitoring of spacecraft health. Integration of these technologies into the spacecraft flight software architecture has presented a number of system engineering challenges, Some of these technologies were developed in a research-oriented, non-real-time, artificial intelligence organizational culture while spacecraft software is typically developed in a strong real-time, algorithmically-oriented culture. The Navigation technology has been developed in a ground-based environment. Integration of these different cultures and mutual education of the software team has been achieved. An early rapid prototype of an existing spacecraft design proved very valuable in educating the team members and in working out the development process."
576,excluded,10.1109/icspis.2018.8700563,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8700563/,2018-12-27 00:00:00,design and implementation of a parcel sorter using deep learning,"Automation in industrial environment reduces the cost of the operation while increasing the overall performance. Having an automation mechanism in the e-commerce warehouses to sort the parcels based on their destinations or shipping method will reduce the parcel processing time significantly. To automate parcel processing in Digikala's warehouse, a parcel sorter system is designed and implemented. In this system shipment method of the parcel is indicated by a set of markers. A computer vision system is developed to identify these markers using deep learning algorithms. The parcels are identified while they are moving on the conveyor belt in a relatively high speed (1 m/s). The computer vision system is capable of processing 1.3MP pictures in real-time with a rate of 100FPS. To sort the parcels an omni wheel roller mechanism is designed and utilized. To achieve the best results in a practical environment, a gap optimization mechanism and pack positioning conveyor are implemented and placed before the sorter. This system is successfully installed in the Digikala's warehouse."
577,unknown,10.1145/3528416.3530986,CF,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/70dea8bedbdafd831b18befc7c63764557e62eff,2022-01-01 00:00:00,benchmarking and feasibility aspects of machine learning in space systems,"Compute in space, e.g., in miniaturized satellites, requires dealing with special physical and boundary constraints, including the limited energy budget. These constraints impose strict operational conditions on the on-board data processing system and its capability in dealing with sophisticated workloads suchlike Machine Learning (ML). In the meantime, the breakthroughs in ML based on Deep Neural Networks (DNNs) in the last decade promise innovative solutions to expand the functional capabilities of on-board data processing and to drive the space industry forward. Therefore, due to the aforementioned special requirements, performance- and power-efficient, and novel solutions and architectures for deploying ML via, e.g., FPGA-enabled SoC, particularly Commercial-Off-The-Shelf (COTS) solutions, are gaining significant interest in the space industry. Therefore it is essential to conduct extensive benchmarking and feasibility and efficiency analyses in different aspects: such analyses would require the investigation of options for programming and deployment as well as the investigation of various real-world models and datasets. To this end, a research and development activity is funded by the European Space Agency (ESA) General Support Technology Programme and is led by Airbus Defence and Space GmbH with the goal of developing an ML Application Benchmark (MLAB) that covers benchmarking aspects mentioned above. In this invited paper, we provide an overview of the MLAB project and discuss development and progress in various directions, including framework analyses, model, and dataset investigation. We elaborate on a benchmarking methodology developed in the context of this project to enable the analysis of various hardware platforms and options. In the end, focus on a particular use case of aircraft detection as a real-world example and provide an analysis of various performance and accuracy indicators including, accuracy, throughput, latency, and power consumption."
578,excluded,10.1007/978-3-030-69514-9_4,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-69514-9_4,2021-01-01 00:00:00,deep learning at the edge for operation and maintenance of large-scale solar farms,"Real-time monitoring of large-scale solar farms is one important aspect of reliable and secure deployment of 100% renewable energy-based grids. The ability to observe sensors on solar panels using Internet of Things (IoT) technologies makes it possible to study the behavior of solar panels under various conditions and to detect anomalous behaviors in real-time. Such technologies make it possible for grid administrators to make informed decisions in reacting to anomalies such as panel damage, electrical errors, monitoring hardware decay, or malicious data injection attacks. Smart edge devices offer an opportunity to reduce the cost of continuously sending data for anomaly detection by performing analytics on local edge device within a given farm and sending only the result of the analysis back to datacenters. This paper presents the design and evaluation of a low-cost edge-based anomaly detection system for remote solar farms using Raspberry Pi and deep learning. The design was implemented and tested using real-life observations from a solar monitoring system under soiling conditions. The experiments showed that it is possible to run real-time anomaly detection algorithms on edge devices with little overhead in terms of power consumption and utilization of computational resources, making it an ideal system for large-scale implementation."
579,excluded,10.1109/hpca.2018.00018,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8327001/,2018-02-28 00:00:00,in-situ ai: towards autonomous and incremental deep learning for iot systems,"Recent years have seen an exploration of data volumes from a myriad of IoT devices, such as various sensors and ubiquitous cameras. The deluge of IoT data creates enormous opportunities for us to explore the physical world, especially with the help of deep learning techniques. Traditionally, the Cloud is the option for deploying deep learning based applications. However, the challenges of Cloud-centric IoT systems are increasing due to significant data movement overhead, escalating energy needs, and privacy issues. Rather than constantly moving a tremendous amount of raw data to the Cloud, it would be beneficial to leverage the emerging powerful IoT devices to perform the inference task. Nevertheless, the statically trained model could not efficiently handle the dynamic data in the real in-situ environments, which leads to low accuracy. Moreover, the big raw IoT data challenges the traditional supervised training method in the Cloud. To tackle the above challenges, we propose In-situ AI, the first Autonomous and Incremental computing framework and architecture for deep learning based IoT applications. We equip deep learning based IoT system with autonomous IoT data diagnosis (minimize data movement), and incremental and unsupervised training method (tackle the big raw IoT data generated in ever-changing in-situ environments). To provide efficient architectural support for this new computing paradigm, we first characterize the two In-situ AI tasks (i.e. inference and diagnosis tasks) on two popular IoT devices (i.e. mobile GPU and FPGA) and explore the design space and tradeoffs. Based on the characterization results, we propose two working modes for the In-situ AI tasks, including Single-running and Co-running modes. Moreover, we craft analytical models for these two modes to guide the best configuration selection. We also develop a novel two-level weight shared In-situ AI architecture to efficiently deploy In-situ tasks to IoT node. Compared with traditional IoT systems, our In-situ AI can reduce data movement by 28-71%, which further yields 1.4X-3.3X speedup on model update and contributes to 30-70% energy saving."
580,excluded,10.1007/978-3-030-64148-1_14,Springer,springer,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-64148-1_14,2020-01-01 00:00:00,an end-to-end framework for productive use of machine learning in software analytics and business intelligence solutions,"Nowadays, machine learning (ML) is an integral component in a wide range of areas, including software analytics (SA) and business intelligence (BI). As a result, the interest in custom ML-based software analytics and business intelligence solutions is rising. In practice, however, such solutions often get stuck in a prototypical stage because setting up an infrastructure for deployment and maintenance is considered complex and time-consuming. For this reason, we aim at structuring the entire process and making it more transparent by deriving an end-to-end framework from existing literature for building and deploying ML-based software analytics and business intelligence solutions. The framework is structured in three iterative cycles representing different stages in a model’s lifecycle: prototyping, deployment, update. As a result, the framework specifically supports the transitions between these stages while also covering all important activities from data collection to retraining deployed ML models. To validate the applicability of the framework in practice, we compare it to and apply it in a real-world ML-based SA/BI solution."
581,excluded,10.5220/0006707903950402,VEHITS,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/0b9e5a7bd29b4302a4c661f5f0a7fcc9fce43b0c,2018-01-01 00:00:00,an information system for bus travelling and performance evaluation,"A wide vehicular network has a huge potential to collect city-data, specially with respect to city mobility, one of the top concerns of the municipalities. In this work, we propose the use of the mobility data generated by the movement of the connected buses to deliver a new set of tools to support both the bus passengers and bus fleet operator use cases. Considering the bus passengers, it is possible to build smart schedules, which deliver an estimated time of arrival based on the city dynamics along time, and that can be accessed directly in the smartphone. Considering the bus fleet operator, it is possible to characterize the behaviour of buses and bus lines. Using the GPS trace of buses and map-matching algorithm, we are able to discover the line each bus is assigned to. Estimated times of arrival and predictions are implemented recurring to time estimations and predictions, using both data mining and machine learning approaches. Proof-of-concept applications were implemented to demonstrate the real-life applicability, including a mobile app for the citizens, and a web dashboard for the fleet operator."
582,unknown,10.23919/ilrn52045.2021.9459415,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9459415/,2021-06-10 00:00:00,exploring the real-time touchless hand interaction and intelligent agents in augmented reality learning applications,"During the last decade, there has been a surge in research studies exploring the adoption of Augmented Reality (AR) in educational settings. Within these multiple research studies, AR's capability to extend the teaching and learning environment with augmented 3D learning objects with enhanced interactive capabilities have been demonstrated. This new technology has not been widely adopted in the mainstream but with the recent unprecedented circumstances of COVID-19, there has been an increasing societal willingness to adopt these technologies. AR has been a desirable technology due to its inherent touchless nature which facilitates social distancing at this time but AR applications crucially offer so much more. They can provide interactive functionality through augmentation of the teaching and learning environment within an immersive user experience including 3D interactions with learning objects, gestures, hand interaction, tangible and multi-modal interaction. This paper presents the results of a review of touchless interaction studies in educational applications and proposes the implementation of real-time touchless hand interaction within kinesthetic learning and utilization of machine learning agents. The architecture of two AR applications with real-time hand interaction and machine learning agents are demonstrated within this paper enabling engaged kinesthetic learning as an alternative learning interface."
583,excluded,10.1016/j.suscom.2022.100743,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85129609524,2022-09-01,improving sugarcane production in saline soils with machine learning and the internet of things,"The Indian sugar industry is the second largest in the world. Sugar is an essential domestic grocery item required in India producing over 25 million tonnes per annum. Sugarcane is the root of sugar products that grow in over 5 million hectares all over India. However, nearly 1.5 million hectares of overall farms are saline soil lands (high salt content). This leads to lower yields in sugarcane agriculture than what would be expected. Therefore, tackling the salinity problem is crucial to achieve strong food security as well as tackle the sustainability of farming practices in India that have reach beyond just sugarcane. This research proposes efficient, sustainable, smart farming techniques for sugarcane cultivation in salt-affected lands with the help of the Internet of Things (IoT) and Machine Learning (ML). The proposed model has been implemented in a real-world two hectare sugar cane field cultivated from saline soils using Raspberry PI IoT nodes to control the drip irrigation (water supply). The Naïve Bayes model has been used to train and predict the leaching requirement suggested by Food and Agriculture Organization of the United Nations (FAO) and United Nations Educational, Scientific and Cultural Organization (UNESCO) for efficient leaching water requirements. The performance of the proposed model has been evaluated in terms of sugar cane growth, cost of cultivation, as well as water requirements leading to an improved outlook for future use. Moreover, our results have been compared with regular sugar cane cultivation to show their effectiveness."
584,unknown,10.1109/biocas.2019.8919141,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8919141/,2019-10-19 00:00:00,an aiot wearable ecg patch with decision tree for arrhythmia analysis,"This paper proposes a novel electrocardiogram (ECG) monitoring system with an artificial intelligence of things (AIoT) design and is based on a decision tree [1]. The proposed system includes a front-end device and a software system. The front-end device includes a solar charging circuit, a wireless charging circuit, and an analog front-end circuit. The analog front-end block is composed of an ECG sensing circuit, a microcontroller unit (MCU), and a Bluetooth low-energy (BLE) module. This paper also proposes an iOS-based app with decision-tree model. Real-time ECG data are classified by the decision-tree model into different arrhythmias, and the average delay latency is 500 ms. Meanwhile, with 4G or Wi-Fi, the collected ECG data are uploaded to the cloud server for training the decision tree, and the coefficients of the pre-trained decision tree will be sent back to update the decision-tree model in the app. An accuracy of 98.22% is achieved. Through the proposed AIoT system, doctors and users can perform long-term ECG monitoring, which is valuable for cardiovascular-disease diagnosis. Doctors can also instantly assist users with the web-user interface to meet the needs of telemedicine. The proposed AIoT system is also used in human trials in the National Cheng Kung University Hospital. The power consumption of the proposed front-end device is 8.25 mW, and it can be continuously used for up to 32 h with a 120 mAh lithium-ion battery. When operated on solar charging, the device can continue working until the solar cell runs out of power."
585,excluded,http://arxiv.org/abs/2012.10342v3,arxiv,arxiv,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2012.10342v3,2020-12-18 00:00:00,exploring and interrogating astrophysical data in virtual reality,"Scientists across all disciplines increasingly rely on machine learning
algorithms to analyse and sort datasets of ever increasing volume and
complexity. Although trends and outliers are easily extracted, careful and
close inspection will still be necessary to explore and disentangle detailed
behavior, as well as identify systematics and false positives. We must
therefore incorporate new technologies to facilitate scientific analysis and
exploration. Astrophysical data is inherently multi-parameter, with the
spatial-kinematic dimensions at the core of observations and simulations. The
arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as
well as the availability of versatile development tools for video games, has
enabled scientists to deploy such technology to effectively interrogate and
interact with complex data. In this paper we present development and results
from custom-built interactive VR tools, called the iDaVIE suite, that are
informed and driven by research on galaxy evolution, cosmic large-scale
structure, galaxy-galaxy interactions, and gas/kinematics of nearby galaxies in
survey and targeted observations. In the new era of Big Data ushered in by
major facilities such as the SKA and LSST that render past analysis and
refinement methods highly constrained, we believe that a paradigm shift to new
software, technology and methods that exploit the power of visual perception,
will play an increasingly important role in bridging the gap between
statistical metrics and new discovery. We have released a beta version of the
iDaVIE software system that is free and open to the community."
586,included,http://arxiv.org/abs/2103.13452v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.13452v1,2021-03-24 00:00:00,"a portable, self-contained neuroprosthetic hand with deep learning-based
  finger control","Objective: Deep learning-based neural decoders have emerged as the prominent
approach to enable dexterous and intuitive control of neuroprosthetic hands.
Yet few studies have materialized the use of deep learning in clinical settings
due to its high computational requirements. Methods: Recent advancements of
edge computing devices bring the potential to alleviate this problem. Here we
present the implementation of a neuroprosthetic hand with embedded deep
learning-based control. The neural decoder is designed based on the recurrent
neural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a
compacted yet powerful edge computing platform for deep learning inference.
This enables the implementation of the neuroprosthetic hand as a portable and
self-contained unit with real-time control of individual finger movements.
Results: The proposed system is evaluated on a transradial amputee using
peripheral nerve signals (ENG) with implanted intrafascicular microelectrodes.
The experiment results demonstrate the system's capabilities of providing
robust, high-accuracy (95-99%) and low-latency (50-120 msec) control of
individual finger movements in various laboratory and real-world environments.
Conclusion: Modern edge computing platforms enable the effective use of deep
learning-based neural decoders for neuroprosthesis control as an autonomous
system. Significance: This work helps pioneer the deployment of deep neural
networks in clinical applications underlying a new class of wearable biomedical
devices with embedded artificial intelligence."
587,unknown,10.1007/978-3-319-26925-2_18,Springer,springer,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-319-26925-2_18,2015-01-01 00:00:00,a novel data dissemination model for organic data flows,"The number of computing devices of the IoT are expected to grow exponentially. To address the communication needs of the IoT, research is being done to develop new networking architectures and to extend existing architectures. An area that lacks attention in these efforts is the emphasis on utilisation of omnipresent local data. There are a number of issues (e.g., underutilisation of local resources and dependence on cloud based data) that need to be addressed to exploit the benefits of utilising local data. We present a novel data dissemination model, called the Organic Data Dissemination (ODD) model to utilise the omni-present data around us, where devices deployed with the ODD model are able to operate even without the existence of networking infrastructure. The realisation of the ODD model requires innovations in many different area including the areas of opportunistic communications, naming of information, direct peer-to-peer communications and reinforcement learning. This paper focuses on highlighting the usage of the ODD model in real application scenarios and the details of the architectural components."
588,unknown,10.2118/198963-ms,,semantic_scholar,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/20831632f67745592fd1938fd0f15891191d10db,2020-01-01 00:00:00,enabling witsml stream analytics through open source big data technologies,"
 This work presents a set of interconnected open source big data technologies through an example case to demonstrate the processes used to generate, process, store, and consume real-time wellsite information transfer standard markup language (WITSML) data from drilling and completions (D&C) operations. The new proposed approach to manage real-time data is based on the use of distributed storage and processing technologies to simultaneously analyze large volumes of information, previously considered only on an individual well basis. This new approach leverages the open source technology available in the market to target the gathering of the data, its secure database storage, and its future processing in a single workflow, to obtain the most value added from the oilfield-generated data during D&C operations. This work presents a set of big data tools and their application.
 For this case study, a dynamic, open, easily configurable, and fully scalable work environment was obtained. Open source big data technologies prove to be different when handling operations data, as compared to traditional technologies. Traditional technologies typically require several manual inputs and configurations to enter the data. Data models for these technologies are usually closed or proprietary, and the provided visualizations are limited either to single well analysis or to basic analytical dashboards. The technologies used for this case fully enable data input and storage, as well as allow the generation and real-time deployment of machine learning and data science algorithms and models.
 As a different approach, this paper introduces the use of big data architectures for the D&C business to obtain better results for gathering, storing, and analyzing real-time information than the business has traditionally achieved."
589,unknown,10.1109/smartcomp55677.2022.00041,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9821056/,2022-06-24 00:00:00,"a demo of a software platform for ubiquitous big data engineering, visualization, and analytics, via reconfigurable micro-services, in smart factories","Intelligent, smart, Cloud, reconfigurable manufac-turing, and remote monitoring, all intersect in modern industry and mark the path toward more efficient, effective, and sustain-able factories. Many obstacles are found along the path, including legacy machineries and technologies, security issues, and software that is often hard, slow, and expensive to adapt to face unforeseen challenges and needs in this fast-changing ecosystem. Light-weight, portable, loosely coupled, easily monitored, variegated software components, supporting Edge, Fog and Cloud computing, that can be (re)created, (re)configured and operated from remote through Web requests in a matter of milliseconds, and that rely on libraries of ready-to-use tasks also extendable from remote through sub-second Web requests, constitute a fertile technological ground on top of which fourth-generation industries can be built. In this demo it will be shown how starting from a completely virgin Docker Engine, it is possible to build, configure, destroy, rebuild, operate, exclusively from remote, exclusively via API calls, computation networks that are capable to (i) raise alerts based on configured thresholds or trained ML models, (ii) transform Big Data streams, (iii) produce and persist Big Datasets on the Cloud, (iv) train and persist ML models on the Cloud, (v) use trained models for one-shot or stream predictions, (vi) produce tabular visualizations, line plots, pie charts, histograms, at real-time, from Big Data streams. Also, it will be shown how easily such computation networks can be upgraded with new functionalities at real-time, from remote, via API calls."
590,unknown,10.1016/j.procs.2020.07.028,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85094582861,2020-01-01,a mobile clinical dss based on augmented reality and deep learning for the home cares of patients afflicted by bedsores,"A bedsore, also known as pressure sore, pressure ulcer or decubitus ulcer, is the result of constant pressure on skin occurring in bedridden patients and paraplegics continuously sitting in chair. All patients who are immobile for a long time due to any cause are likely to get bedsores. Effective and efficient management of processes related to the treatment of bedsores is an important issue for healthcare organizations as it heavily affects the quality of life of patients and the costs for such organizations. Therefore organizations need and look for more and more to provide their field workforce with smart mobile tools able to support such processes. In such a context, this paper proposes a mobile app implementing a Clinical Decision Support System (CDSS) to help field operators to measure the bedsore, classify its status, trace its evolution along the timeline and making correct decisions about the course of actions to effectively treat it. The mobile app is mostly based on Augmented Reality supported by Deep Learning, thus it requires an adequate system architecture to be effectively deployed, adopted and used. From the conceptual viewpoint, the defined CDSS model lays on three important considerations: providing automatic support to classify the status of a bedsore does not do all the work but help operators to improve the quality of their decisions, augmented reality allows to build a situated environment for decision-making supporting the operators’ cognitive processes, operators should use only one tool to execute all their tasks in order to be more focused on the real problem which is to improve the quality of life of their patients."
591,excluded,10.1007/s00521-019-04386-4,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s00521-019-04386-4,2019-12-01 00:00:00,a versatile hardware/software platform for personalized driver assistance based on online sequential extreme learning machines,"In the present scenario of technological breakthroughs in the automotive industry, machine learning is greatly contributing to the development of safer and more comfortable vehicles. In particular, personalization of the driving experience using machine learning is an innovative trend that comprises the development of both customized driver assistance systems and in-cabin comfort features. In this work, a versatile hardware/software platform for personalized driver assistance, using online sequential extreme learning machines (OS-ELM), is presented. The system, based on a programmable system-on-chip (SoC), is able to recognize the driver and personalize the behavior of the car. The platform provides high speed, small size, efficient power consumption, and true capability for real-time adaptation (i.e., on-chip self-learning). In addition, due to the plasticity and scalability of the OS-ELM algorithm and the programmable nature of the SoC, this solution is flexible enough to cope with the incremental changes that the new generation of vehicles are demanding. The implementation details of a system, suitable for current levels of driving automation, are provided."
592,excluded,10.1149/ma2022-01251219mtgabs,ECS Meeting Abstracts,semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/1a0cf3eba22c053ad21503e034baf3178899258a,2022-01-01 00:00:00,design and deployment of a data lake at a pilot plant scale for a smart electropolishing process,"In order to remain competitive and satisfy the demands of today’s customers in a timely manner, manufacturing industries are embracing the Industry 4.0 philosophy where automation is pushed beyond robotics to new technologies emerging from data science and artificial intelligence. The aim is to reduce time spent on none added value tasks and help learning from past experience in order to enhance efficiency and quality of manufacturing processes.
 Traditional industries, such as electropolishing, need to find ways to automate their, often heavily artisanal-based techniques and develop an intelligent network of machines and processes taking advantage of information and communication technology such as Big Data, IoT (Internet of Things), or Artificial Intelligence (AI). This digital transition can be realized through the application of an IIoT (Industrial Internet of Things) platform that constructs a massive, sophisticated information network of interconnected sensors, equipment, and processes known as cyber-physical systems.
 Within this network, large amounts of data (for example process bath attributes such as temperature or viscosity and part characteristics such as roughness or brightness) can be collected automatically via sensors and through user-friendly applications from manual measurements and observations. All data are uploaded automatically into a cloud-based data storage system. In order for this collected information to be useful, the data needs to be processed to allow pattern discovery and extraction of useful information regarding the system performance, probable faults in the process, and product quality. Besides others, machine learning algorithms play a key role in extracting useful information.
 Classification and processing of such massive, diverse, and rapidly arriving data sets are known to be challenging. As a result, the concept of data lake has arisen in the last decade as an appealing and cost-effective approach for companies to manage large amounts of data. It consists of a large repository of datasets designed to transform raw and unstructured data into structured, usable information to allow further processing. A data lake, organized typically in four layers (ingestion, distillation, processing, and insights layers), stores both old and near real-time data in one location for initial assessment, with comprehensive data organization, analysis, and visualization being performed only when necessary 1,2. This promotes agility by allowing data to be accessed by everyone in the company. 2
 
 In this work, a data lake is designed and implemented in conjunction with a pilot plant to demonstrate how in the electropolishing process of stainless-steel samples in an aging electrolyte, data can be collected and organized for further processing using machine learning techniques in order to optimize the process and part quality based on the data analysis results.
 References: 
 
 N. Miloslavskaya and A. Tolstoy, Procedia Comput. Sci., 88, 300–305 (2016).
 
 
 H. Fang, in 2015 IEEE International Conference on Cyber Technology in Automation, Control, and Intelligent Systems (CYBER),, p. 820–824 (2015).
 
 
 
 
 
 
 
 
 Figure 1
"
593,unknown,10.14419/ijet.v7i3.12.16561,International Journal of Engineering & Technology,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/21be6397934e0675286a423f153019fb2a65e8fa,2018-01-01 00:00:00,"vcin vehicular interconnected informatics network “a interconnected network of vehicular information, data, and interface”","With market moving towards smart city development, the importance of data and its analysis has increased. According to a report by HIS Market, data generated from IOT devices will increase to 125 billion in 2030. Such huge amount of data is going to bring opportunities for all level of industries and markets. Thus, intelligent collection of data along with storage, processing and analysis will be a key to develop smart city. The information generated through data can target both users and market. It will be a result of various machine learning, data mining algorithms and distributed data storage technologies. The users can access information through web/mobile applications. The real time analysis of data and delivering it to user with no time will reduce accidents. This will increase productivity in hardware and software markets .VCIN the system of interconnected network of vehicles and infrastructure model in which, it focuses on four core modules. The interconnecting modules makes the network of data coming in and out of the system and finally getting displayed on one of our dashboard.  "
594,included,10.1109/embc44109.2020.9175947,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9175947/,2020-07-24 00:00:00,aidex - an open-source platform for real-time forecasting sepsis and a case study on taking ml algorithms to production,"Sepsis, a dysregulated immune response to infection, has been the leading cause of morbidity and mortality in critically ill patients. Multiple studies have demonstrated improved survival outcomes when early treatment is initiated for septic patients. In our previous work, we developed a real-time machine learning algorithm capable of predicting onset of sepsis four to six hours prior to clinical recognition. In this work, we develop AIDEx, an open-source platform that consumes data as FHIR resources. It is capable of consuming live patient data, securely transporting it into a cloud environment, and monitoring patients in real-time. We build AIDEx as an EHR vendor-agnostic open-source platform that can be easily deployed in clinical environments. Finally, the computation of the sepsis risk scores uses a common design pattern that is seen in streaming clinical informatics and predictive analytics applications. AIDEx provides a comprehensive case study in the design and development of a production-ready ML platform that integrates with Healthcare IT systems."
595,excluded,http://arxiv.org/abs/2205.00258v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2205.00258v1,2022-04-30 00:00:00,"easynlp: a comprehensive and easy-to-use toolkit for natural language
  processing","The success of Pre-Trained Models (PTMs) has reshaped the development of
Natural Language Processing (NLP). Yet, it is not easy to obtain
high-performing models and deploy them online for industrial practitioners. To
bridge this gap, EasyNLP is designed to make it easy to build NLP applications,
which supports a comprehensive suite of NLP algorithms. It further features
knowledge-enhanced pre-training, knowledge distillation and few-shot learning
functionalities for large-scale PTMs, and provides a unified framework of model
training, inference and deployment for real-world applications. Currently,
EasyNLP has powered over ten business units within Alibaba Group and is
seamlessly integrated to the Platform of AI (PAI) products on Alibaba Cloud.
The source code of our EasyNLP toolkit is released at GitHub
(https://github.com/alibaba/EasyNLP)."
596,excluded,10.1109/smartcomp52413.2021.00027,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9556252/,2021-08-27 00:00:00,aris: a real time edge computed accident risk inference system,"To deploy an intelligent transport system in urban environment, an effective and real-time accident risk prediction method is required that can help maintain road safety, provide adequate level of medical assistance and transport in case of an emergency. Reducing traffic accidents is an important problem for increasing public safety, so accident analysis and prediction have been a subject of extensive research in recent time. Even if a traffic hazard occurs, a readily deployable structure with an accurate prediction of accident can contribute to better management of rescue resources. But the significant shortcomings of current studies are the use of small-scale datasets with minimal scope, being based on extensive data sets, and not being applicable for real-time purposes. To overcome these challenges, we propose ARIS: a system for real-time traffic accident prediction built on a traffic accident dataset named ‘US-Accidents’ which covers 49 states of United States, collected from February 2016 to June 2020. Our approach is based on a deep neural network model that utilizes a variety of data characteristics, such as time-sensitive weather data, textual information, and discerning factors. We have tested ARIS against multiple baselines through a comprehensive series of experiments across several major cities of USA, and we have noticed significant improvement during inference especially in detecting accident classes. Additionally, to make our model edge-implementable we have compressed our model using a joint technique of magnitude-based weight pruning and model quantization. We have also demonstrated the inference results along with power consumption profiling after deploying the model on a resource constrained environment that consists of Intel Neural Compute Stick 2 (NCS2) with Raspberry Pi 4B (RPi4). Our investigation and observations indicate major improvements to predict unusual traffic accident event even after model compression and deployment. We have managed to reduce the model size and inference time by ≈ 6x, and ≈ 70 % respectively with insignificant drop in performance. Furthermore, to better understand the importance of each individual type of variables used in our analysis, we have showcased a comprehensive ablation study."
597,unknown,10.1109/icacsis.2018.8618144,IEEE,ieeexplore,e-commerce,'e-commerce' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8618144/,2018-10-28 00:00:00,protagoras: a service for tagging e-commerce products at scale,"Despite widespread adoption of machine learning to solve real world problems, the implementation of ML solutions in production environment is more complicated than it seems. It is quite straightforward to write machine learning codes these days but they are not designed to be deployed in production scale where millions of requests per day is a norm. In this paper, we describe our implementation of a ML service for large scale product tagging in e-commerce called Protagoras. The problem of tagging products can be seen as multi-label classification where the labels are product tags. By performing the classification within each product category, the precision can be increased and the inference can be performed faster. Protagoras combined the scalability and speed of microservice implementation in Golang and robust machine learning implementation in Python. We present the architecture of the system with all its components including API endpoints, job queue, database, and monitoring. The benchmark shows that, even with 1000 classifiers in one category, the average latency for online inference is below 300 millisecond. The throughput can be further maximized by replicating the service into multiple servers."
598,excluded,http://arxiv.org/abs/2108.07856v1,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2108.07856v1,2021-08-17 00:00:00,"oncopetnet: a deep learning based ai system for mitotic figure counting
  on h&e stained whole slide digital images in a large veterinary diagnostic
  lab setting","Background: Histopathology is an important modality for the diagnosis and
management of many diseases in modern healthcare, and plays a critical role in
cancer care. Pathology samples can be large and require multi-site sampling,
leading to upwards of 20 slides for a single tumor, and the human-expert tasks
of site selection and and quantitative assessment of mitotic figures are time
consuming and subjective. Automating these tasks in the setting of a digital
pathology service presents significant opportunities to improve workflow
efficiency and augment human experts in practice. Approach: Multiple
state-of-the-art deep learning techniques for histopathology image
classification and mitotic figure detection were used in the development of
OncoPetNet. Additionally, model-free approaches were used to increase speed and
accuracy. The robust and scalable inference engine leverages Pytorch's
performance optimizations as well as specifically developed speed up techniques
in inference. Results: The proposed system, demonstrated significantly improved
mitotic counting performance for 41 cancer cases across 14 cancer types
compared to human expert baselines. In 21.9% of cases use of OncoPetNet led to
change in tumor grading compared to human expert evaluation. In deployment, an
effective 0.27 min/slide inference was achieved in a high throughput veterinary
diagnostic pathology service across 2 centers processing 3,323 digital whole
slide images daily. Conclusion: This work represents the first successful
automated deployment of deep learning systems for real-time expert-level
performance on important histopathology tasks at scale in a high volume
clinical practice. The resulting impact outlines important considerations for
model development, deployment, clinical decision making, and informs best
practices for implementation of deep learning systems in digital histopathology
practices."
599,unknown,10.1016/s0273-1177(99)00318-x,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/0033374124,1999-01-01,development of autonomous control in a closed microbial bioreactor,"Space-based life support systems which include ecological components will rely on sophisticated hardware and software to monitor and control key system parameters. Autonomous closed artificial ecosystems are useful for research in numerous fields. We are developing a bioreactor designed to study both microbe-environment interactions and autonomous control systems. Currently we are investigating N-cycling and N-mass balance in closed microbial systems. The design features of the system involve real-time monitoring of physical parameters (e.g. temperature, light), growth solution composition (e.g. pH, NOx, CO2), cell density and the status of important hardware components. Control of key system parameters is achieved by incorporation of artificial intelligence software tools that permit autonomous decision-making by the instrument. These developments provide a valuable research tool for terrestrial microbial ecology, as well as a testbed for implementation of artificial intelligence concepts. Autonomous instrumentation will be necessary for robust operation of space-based life support systems, and for use on robotic spacecraft. Sample data acquired from the system, important features of software components, and potential applications for terrestrial and space research will be presented."
600,unknown,10.1109/ests.2005.1524673,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/1524673/,2005-07-27 00:00:00,operational experience with intelligent software agents for shipboard diesel and gas turbine engine health monitoring,"The power distribution network aboard future navy warships are vital to reliable operations and survivability. Power distribution involves delivering electric power from multiple generation sources to a dynamic set of load devices. Advanced power electronics, intelligent controllers, and a communications infrastructure form a shipboard power distribution network, much like the domestic electric utility power grid. Multiple electric generation and storage devices distributed throughout the ship will eliminate dependence on any single power source through dynamic load management and power grid connectivity. Although new technologies are under development, gas turbine and diesel generators remain as the likely near-term power sources for the future all-electric ship integrated power system (IPS). Health monitoring of these critical IPS power sources are essential to achieving reliability and survivability goals. System complexity, timing constraints, and manning constraints will shift both control and equipment health monitoring functions from human operators to intelligent machines. Drastic manning reductions coupled with a large increase in the number of sensor monitoring points makes automated condition-based maintenance (CBM) a stringent requirement. CBM has traditionally been labor intensive and expensive to implement, relying on human experiential knowledge, interactive data processing, information management, and cognitive processing. The diagnostic robustness and accuracy of these embedded software agents are essential, as false or missed diagnostic calls have severe ramifications within the intelligent, automated control environment. This paper presents some of the first reported results of intelligent diagnostic software agents operating in real-time onboard naval ships with gas turbine and diesel machinery plants. The agents are shown to perform a substantial amount of CBM-related data processing and analysis that would not otherwise be performed by the crew, including real-time, neural network diagnostic inferencing. The agents are designed to diagnose existing system faults and to predict machinery problems at their earliest stage of development. The results reported herein should be of particular interest to those involved with future all-electric ship designs that includes both gas turbine and diesel engines as primary electrical power sources."
601,unknown,10.24251/hicss.2019.507,HICSS,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/e925a7ee2cd8914325d44d92f020e54448144632,2019-01-01 00:00:00,halleyassist: a personalised internet of things technology to assist the elderly in daily living,"Ambient Assisted Living (AAL) research has received extensive attention in recent years. AAL applications combine aspects of Internet of Things (IoT), smart platform design and machine learning to produce an intelligent system. In this paper we describe a personalised IoT-based AAL system that enables an independent and safe life for elderly people within their own home via real-time monitoring and intervention. The system, HalleyAssist underpinned by smart home automation functions includes a novel approach for monitoring the wellbeing and detecting abnormal changes in behavioral patterns of an elderly person. The significance of the approach is in the use of machine learning models to automatically learn normal behavioral pattern for the person from IoT sensor data and using the models derived to detect significant changes in behavioral pattern should they occur. The architecture and developed proof of concept of the proposed system is presented along with discussion of how privacy and security concerns are addressed. We also report on outcomes of real-world in-home trials of an early version of the system where it was installed in four older people’s home for a period of six weeks. The response from the older people to the deployed system was very positive. Finally, the paper presents a discussion and an analysis of the results using the data collected during the in-home trials."
602,excluded,10.1609/aaai.v31i2.19093,AAAI,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/c120eba62b3c5e85596bc0860cee810ee7bb1756,2017-01-01 00:00:00,real-time indoor localization in smart homes using semi-supervised learning,"
 
 
Long-term automated monitoring of residential or small in- dustrial properties is an important task within the broader scope of human activity recognition. We present a device- free wifi-based localization system for smart indoor spaces, developed in a collaboration between McGill University and Aerˆıal Technologies. The system relies on existing wifi net- work signals and semi-supervised learning, in order to au- tomatically detect entrance into a residential unit, and track the location of a moving subject within the sensing area. The implemented real-time monitoring platform works by detect- ing changes in the characteristics of the wifi signals collected via existing off-the-shelf wifi-enabled devices in the environ- ment. This platform has been deployed in several apartments in the Montreal area, and the results obtained show the poten- tial of this technology to turn any regular home with an ex- isting wifi network into a smart home equipped with intruder alarm and room-level location detector. The machine learn- ing component has been devised so as to minimize the need for user annotation and overcome temporal instabilities in the input signals. We use a semi-supervised learning framework which works in two phases. First, we build a base learner for mapping wifi signals to different physical locations in the en- vironment from a small amount of labeled data; during its lifetime, the learner automatically re-trains when the uncer- tainty level rises significantly, without the need for further supervision. This paper describes the technical and practical issues arising in the design and implementation of such a sys- tem for real residential units, and illustrates its performance during on-going deployment. 
 
 
"
603,unknown,10.3390/w11010009,'MDPI AG',core,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),,,building an intelligent hydroinformatics integration platform for a regional flood inundation warning systems,"[[abstract]]Flood disasters have had a great impact on city development. Early flood warning systems (EFWS) are promising countermeasures against flood hazards and losses. Machine learning (ML) is the kernel for building a satisfactory EFWS. This paper first summarizes the ML methods proposed in this special issue for flood forecasts and their significant advantages. Then, it develops an intelligent hydroinformatics integration platform (IHIP) to derive a user-friendly web interface system through the state-of-the-art machine learning, visualization and system developing techniques for improving online forecast capability and flood risk management. The holistic framework of the IHIP includes five layers (data access, data integration, servicer, functional subsystem, and end-user application) and one database for effectively dealing with flood disasters. The IHIP provides real-time flood-related data, such as rainfall and multi-step-ahead regional flood inundation maps. The interface of Google Maps fused into the IHIP significantly removes the obstacles for users to access this system, helps communities in making better-informed decisions about the occurrence of floods, and alerts communities in advance. The IHIP has been implemented in the Tainan City of Taiwan as the study case. The modular design and adaptive structure of the IHIP could be applied with similar efforts to other cities of interest for assisting the authorities in flood risk management.[[notice]]補正完"
604,unknown,10.1109/smartcomp55677.2022.00036,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9821037/,2022-06-24 00:00:00,toward an api-driven infinite cyber-screen for custom real-time display of big data streams,"Graphical User Interfaces (GUI) and real-time in-teractive Big Data charts play a key role in a wide variety of Big Data applications. The software libraries that are available at today are not suitable for displaying huge volumes of data in a single chart, because they require all the data to be collected at a single node. In this work, an innovative approach to the problem is presented, that consists in using a network of cyber-devices that is created and configured via API calls and that interfaces with a Scala Spark server application through a multiplicity of communication technologies, to produce and display a variety of time-space- infinite Big Data stream visualizations, including line plots, pie charts, histograms, that are updated at real-time as new data come, without ever collecting the data or the charts markup at a single node. The proposed approach characterizes for being (i) Web-based, (ii) API-based, (iii) Cloud-based, (iv) portable, (v) customizable/extendable, (vi) plug and play, and for relying on (i) Node-RED, (ii) MQTT, (iii) Scala, (iv) Akka HTTP, (v) Spark, (vi) Kafka, (vii) Docker. Remarkably, the same network used for Big Data visualization can be reconfigured in a matter of milliseconds and used for Big Data (streams) filtering, transformation, merge, analytics, and for training Machine Learning models, storing trained models on a Cloud storage, using stored models for one-shot or stream predictions, and much more. Although being at an advanced stage, we consider this research as a work in progress, since an extensive benchmarking and application to variegated real-world scenarios are still to be carried out."
605,unknown,10.1109/tsmc.1985.6313366,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/6313366/,1985-05-01 00:00:00,an architecture for control and communications in distributed artificial intelligence systems,"An architecture and implementation for a distributed artificial intelligence (DAI) system are presented, with emphasis given to the control and communication aspects. Problem solving by this system occurs as an iterative refinement of several mechanisms, including problem decomposition, kernel-subproblem solving, and result synthesis. In order for all related nodes to make optimum use of the information obtained from these problem-solving mechanisms, the system dynamically reconfigures itself, thereby improving its performance during operation. This approach offers the possibilities of increased real-time response, improved reliability and flexibility, and lower processing costs. A major component in the node architecture is a database of metaknowledge about the expertise of a node's own expert systems and those of the other processing nodes. This information is gradually accumulated during problem solving. Each node also has a dynamic-planning ability, which guides the problem-solving process in the most promising direction and a focus-control mechanism, which restricts the size of the explored solution space at the task level while reducing the communication bandwidths required. It also has a question-and-answer mechanism, which handles internode communications. Examples in the domain of digital-logic design are given to demonstrate the operation of the system."
606,unknown,10.2118/191626-ms,"Day 1 Mon, September 24, 2018",semantic_scholar,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/341c37c9cf41623610f79cf1a95c04fa20f027ba,2018-01-01 00:00:00,integrating downhole temperature sensing datasets and visual analytics for improved gas lift well surveillance,"
 Given the near ubiquity of fiber-optic, information and communication technologies in reservoir and well management, there is a significant need for one-stop shop downhole distributed sensing data analysis methods together with machine learning techniques towards autonomous analysis of such data sources. However, traditional approaches of converting distributed temperature sensor (DTS) data to actionable insights for optimizing gas lift well operations management remain dependent on training based on human annotations. Annotation of downhole distributed temperature sensor data is a laborious task that is not feasible in practice to train a big data classification algorithm for accurate and reliable anomaly detection of gas lift valves. Furthermore, even obtaining training examples for event diagnosis is challenging due to the rarity of some gas lift valve problems. In gas lift well surveillance, it is essential to generate real-time results to allow a swift response by an engineer to prevent harmful consequences of gas lift valve failure onsets on well performance. The online learning capabilities, also mean that the data classification model can be continuously updated to accommodate reservoir changes in the well environment. In this paper, we propose a novel online real-time DTS data visual analytics platform for gas lift wells using big data tools. The proposed system combines Apache Kafka for data ingestion, Apache Spark for in-memory data processing and analytics, Apache Cassandra for storing raw data and processed results, and INT geo toolkit for data visualization. Specifically, the data analytics pipeline uses data mining algorithms to statistically learn features from the DTS measurements. The learned features are used as inputs to a k-means algorithm and then use supervised learning to predict the performance status of gas lift valves and raise alarms based on analytics-based intelligent warning system. The performance of the proposed system architecture for detecting gas lift valve anomaly is evaluated under varying deployment scenarios. To the best of our knowledge, DTS data analytics pipeline system has not been used for real-time anomaly detection in gas lift well operations."
607,included,10.1109/bigdata50022.2020.9377837,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9377837/,2020-12-13 00:00:00,a sentiment analysis service platform for streamed multilingual tweets,"Micro-blogging and social-media platforms are now prominent forums for disseminating information, opinions and commentaries. Among these, Twitter enjoys an in-excess of 330M base of users who continually produce and consume information snippets. Users collectively create a voluminous and multi-lingual corpus in a very broad range of topics on a daily basis. The discourse generated in the blogosphere is often of prime interest and importance to individuals, organizations, and companies. These actors would certainly like to periodically receive an overall assessment of demonstrated ""sentiments"" on specific issues by automatically classifying tweets expressed in different languages in conjunction with big-data analytics. In this paper, we propose a scalable service platform that employs multilingual sentiment analysis to classify streamed-tweets and yields analytics for selected topics in real-time. We discuss the main component of our Spark-enabled platform as we seek to offer an effective big-data service that can: 1) dynamically handle voluminous as well as high-rate tweettraffic through a multi-component application exploiting the latest software developments, 2) accurately identify messages originated by non-genuine user-accounts, and 3) utilize the Spark machine-learning library (MLib) to successfully classify streamed multi-lingual messages in real-time, using multiple potentially distributed executors. To empower our service platform, we have adopted training sets and developed sentiment analysis (SA) models for English, French, and Greek that help classify streamed tweetswith high accuracy. While experimenting with our distributed analytical platform, we establish both accurate and real-time classification for tweetsexpressed in the above European languages."
608,excluded,10.1109/icoac44903.2018.8939110,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8939110/,2018-12-15 00:00:00,designing an efficient framework for violence detection in sensitive areas using computer vision and machine learning techniques,"Human security against violence is one of the major concern in sensitive areas like ATMs, Government offices, Hospitals etc. For such incidents, there is a need of generating timely and automated alerts to concern officials to take further action. For this, surveillance cameras deployed in sensitive areas can be very much helpful. However, existing systems face the problems of low accuracy, high false alerts and high computational cost in monitoring and analyzing the video streams from surveillance cameras and making the decision about violence in real-time. In this paper, we propose an efficient framework to detect violence in sensitive areas using feasible computer vision and machine learning techniques. It collects the video streams of human activities and generates the violence related features by applying motion tracking which slices the video frames based on the presence of moving objects. To find the violent flow descriptors, it calculates the optical flow for each pixel of the frame. These violent flow descriptors are applied to different machine learning techniques to detect the violence events. For the feasibility analysis, different machine learning techniques are analyzed and compared. In addition, the fusion of feasible techniques is tested for improving the accuracy and reducing the error rate."
609,unknown,10.1007/978-3-030-96498-6_13,Springer,springer,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-96498-6_13,2022-01-01 00:00:00,"nrel stratus - enabling workflows to fuse data streams, modeling, simulation, and machine learning","Integrating cloud services into advanced computing facilities provides significant new capabilities and offers several advantages over focusing solely on traditional high performance computing (HPC) workloads. The integration of cloud services is especially potent for workflows that fuse data streams, modeling and simulation (“modsim”), and machine learning. A key challenge to adopting a hybrid edge-cloud-HPC model is aligning optimal capability, data, and user intent on the right resources for each step in a workflow. The National Renewable Energy Laboratory (NREL) Stratus service provides a basis for this alignment. Stratus layers the capabilities needed to make cloud services accessible to a lab-based scientific community on commercial offerings, and currently supports upward of 200 projects, ranging from Internet of Things (IoT) integration to traditional modeling and simulation. This provides a real-world inventory of scientific workflow elements, which enables placing these elements appropriately between the edge, cloud, and traditional HPC. This paper outlines a vision via reference architecture and the application of that architecture in a typical workflow. We highlight multiple components, including sensor data intake, cleaning and transforming (edge/cloud suitable), generation of synthetic data through modsim, computationally heavy machine learning training and hyperparameter optimization (HPC suitable), and inference and deployment (cloud ideal). Every step in such a workflow involves a cost-benefit analysis of the data movement, computational efficiency, availability, latency, and resource capabilities. The reference architecture and examples outlined in this paper allow for better understanding of new opportunities in the context of emerging workflows that combine IOT, cloud, and HPC to bolster scientific productivity."
610,excluded,10.1155/2021/8885671,'Hindawi Limited',core,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),,2021-01-01 00:00:00,recurrent neural-based vehicle demand forecasting and relocation optimization for car-sharing system: a real use case in thailand,"A car-sharing system has been playing an important role as an alternative transport mode in order to avoid traffic congestion and pollution due to a quick growth of usage of private cars. In this paper, we propose a novel vehicle relocation system with a major improvement in threefolds: (i) data preprocessing, (ii) demand forecasting, and (iii) relocation optimization. The data preprocessing is presented in order to automatically remove fake demands caused by search failures and application errors. Then, the real demand is forecasted using a deep learning approach, Bidirectional Gated Recurrent Unit. Finally, the Minimum Cost Maximum Flow algorithm is deployed to maximize forecasted demands, while minimizing the amount of relocations. Furthermore, the system is deployed in the real use case, entitled “CU Toyota Ha:mo,” which is a car-sharing system in Chulalongkorn University. It is based on a web application along with rule-based notification via Line. The experiment was conducted based on the real vehicle usage data in 2019. By comparing in real environment in November of 2019, the results show that our model even outperforms the manual relocation by experienced staff. It achieved a 3% opportunity loss reduction and 3% less relocation trips, reducing human effort by 17 man-hours/week"
611,excluded,10.1016/j.eswa.2010.12.080,scopus,sciencedirect,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/79951576718,2011-06-01,an intelligent framework to manage robotic autonomous agents,"In this paper a joint application of Artificial Intelligence (AI), robotics and Web services is described. The aim of the work presented here was to create a new integrated framework that keeps advantage on one side of the sensing and exploring capabilities of the robotic systems that work in the real world and, on the other side, of the information available via Web. Robots are conceived like (semi-)autonomous systems able to explore and manipulate a portion of their environment in order to find and collect information and data. On the other hand, the Web, that in a robotic domain is usually considered like a channel of communication (e.g. tele-operation, tele-manipulation), here is conceived also like a source of knowledge. This allows to define a new framework able to manage robotic agents in order to get precise, real-time information from the real world. Besides, software agents may search for and get additional information from the Web logical world. The intelligent administration of these services can be applied in different environments and leads to optimize procedures and solve practical problems. To this end a traffic control application has been defined and a simplified test-case implemented."
612,excluded,10.1109/comsnets53615.2022.9668605,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9668605/,2022-01-08 00:00:00,dynamic split computing of posenet inference for fitness applications in home iot-edge platform,"In the next generation wireless networks, implementations of advanced computation tasks such as running Deep Neural Network (DNN) models in Internet of Things (IoT) devices is a challenging task due to their limited computation &#x0026; processing capabilities. To address this issue, we propose an Extended Dynamic Split Computation (E-DSC) algorithm which finds an optimal partition point of DNN inference layer based on the available network bandwidth; where one inference sub-model is computed at IoT device and the other inference sub-model is computed by a home edge. To validate the E-DSC algorithm, we consider a fitness application as a use-case. The application captures live video of a person performing an arm exercise using a camera attached to a Raspberry-Pi (RPi) considered as an IoT device. The PoseNet DNN model is used to estimate the pose of the person using split-inference of the DNN are partitioned among RPi and Samsung Galaxy S20 device (considered as a home edge). The S20 device sends the final inference result back to the RPi which then checks if the exercise is done correctly and displays the exercise count on a television/monitor. We conduct extensive experiments to compare the performance of PoseNet inference execution on RPi device, Galaxy S20 device and performing dynamic split using E-DSC algorithm over Wi-Fi networks in a real time deployment scenario."
613,excluded,http://arxiv.org/abs/2103.15348v2,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2103.15348v2,2021-03-29 00:00:00,"layoutparser: a unified toolkit for deep learning based document image
  analysis","Recent advances in document image analysis (DIA) have been primarily driven
by the application of neural networks. Ideally, research outcomes could be
easily deployed in production and extended for further investigation. However,
various factors like loosely organized codebases and sophisticated model
configurations complicate the easy reuse of important innovations by a wide
audience. Though there have been on-going efforts to improve reusability and
simplify deep learning (DL) model development in disciplines like natural
language processing and computer vision, none of them are optimized for
challenges in the domain of DIA. This represents a major gap in the existing
toolkit, as DIA is central to academic research across a wide range of
disciplines in the social sciences and humanities. This paper introduces
layoutparser, an open-source library for streamlining the usage of DL in DIA
research and applications. The core layoutparser library comes with a set of
simple and intuitive interfaces for applying and customizing DL models for
layout detection, character recognition, and many other document processing
tasks. To promote extensibility, layoutparser also incorporates a community
platform for sharing both pre-trained models and full document digitization
pipelines. We demonstrate that layoutparser is helpful for both lightweight and
large-scale digitization pipelines in real-word use cases. The library is
publicly available at https://layout-parser.github.io/."
614,unknown,10.1109/icc40277.2020.9148684,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9148684/,2020-06-11 00:00:00,machine learning for predictive diagnostics at the edge: an iiot practical example,"Edge Computing is becoming more and more essential for the Industrial Internet of Things (IIoT) for data acquisition from shop floors. The shifting from central (cloud) to distributed (edge nodes) approaches will enhance the capabilities of handling real-time big data from IoT. Furthermore, these paradigms allow moving storage and network resources at the edge of the network closer to IoT devices, thus ensuring low latency, high bandwidth, and location-based awareness. This research aims at developing a reference architecture for data collecting, smart processing, and manufacturing control system in an IIoT environment. In particular, our architecture supports data analytics and Artificial Intelligence (AI) techniques, in particular decentralized and distributed hybrid twins, at the edge of the network. In addition, we claim the possibility to have distributed Machine Learning (ML) by enabling edge devices to learn local ML models and to store them at the edge. Furthermore, edges have the possibility of improving the global model (stored at the cloud) by sending the reinforced local models (stored in different shop floors) towards the cloud. In this paper, we describe our architectural proposal and show a predictive diagnostics case study deployed in an edge-enabled IIoT infrastructure. Reported experimental results show the potential advantages of using the proposed approach for dynamic model reinforcement by using real-time data from IoT instead of using an offline approach at the cloud infrastructure."
615,unknown,10.1109/smc42975.2020.9283261,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9283261/,2020-10-14 00:00:00,a real-time hand motion detection system for unsupervised home training,"Hand motion tracking plays a vital role in health-care like Physical therapy (PT) rehabilitation that helps the patients restore their physical movement of the hand. These treatments are taken by patients suffering from stroke, accidents, and any other kind of neurological disorder. We have developed a low-cost system to track hand movements and detect the gestures of hand for unsupervised home training. The system integrated a convolutional neural network-based hand motion system and a gesture detection system to serve a training session sequential for hand movement rehabilitation. We combined part of open datasets and our novel dataset(total: 16605, 4 labels) for the final training, six directions, and four gestures were predicted in real-time based on our proposed model. An adaptive GUI application was developed to respond to individual performance patterns. This system can be deployed easily on a laptop/PC with a web camera, which makes it easier and low-costs for a doctor to track the hand movement of the patients and also support to quantify the improvements index after several training sessions. These tracked data can also be stored and sent to the remote clinic center and used for further studies. This kind of system would be beneficial to the patients and the training center intelligently. It could be deployed at home or the clinic with more optimization of functions such as safety measures."
616,unknown,10.1109/conmedia46929.2019.8981830,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8981830/,2019-10-11 00:00:00,using naïve bayes classifier for application feedback classification and management in bahasa indonesia,"The world keeps moving, software products too. An application's objectives, structures, requirements, and assumptions that have been elicited and analyzed previously may need to be reassessed and updated. In order to fully understand these requirements evolutions, what changes are necessary, and why those changes are needed, one essential source of requirements is user feedback. However, handling and analyzing so many user feedbacks can be time-consuming. Using natural language processing tools for Bahasa Indonesia and Naive Bayes classifier, this research aims to develop a tool to process natural language and classify user feedbacks. The developed tool is expected to make feedback classification less time-consuming so that developers can project their energy to more productive and creative works. The machine learning models are built using the feedback dataset taken from an up-and-running university e-learning system and show promising results. The highest confusion matrix scores are 92.5% for accuracy, 85.6% precision, 85.1% recall, and lastly, 85.4% for the F-measure score. The resulting web application for feedback management is then evaluated to the users, and even though it still needs to be further polished and improved for real industrial use, it is perceived to be useful and easy to use."
617,unknown,10.1109/access.2020.2991225,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9081933/,2020-01-01 00:00:00,design and performance evaluation of an ai-based w-band suspicious object detection system for moving persons in the iot paradigm,"The threat of terrorism has spread all over the world, and the situation has become grave. Suspicious object detection in the Internet of Things (IoT) is an effective way to respond to global terrorist attacks. The traditional solution requires performing security checks one by one at the entrance of each gate, resulting in bottlenecks and crowding. In the IoT paradigm, it is necessary to be able to perform suspicious object detection on moving people. Artificial intelligence (AI) and millimeter-wave imaging are advanced technologies in the global security field. However, suspicious object detection for moving persons in the IoT, which requires the integration of many different imaging technologies, is still a challenge in both academia and industry. Furthermore, increasing the recognition rate of suspicious objects and controlling network congestion are two main issues for such a suspicious object detection system. In this paper, an AI-based W-band suspicious object detection system for moving persons in the IoT paradigm is designed and implemented. In this system, we establish a suspicious object database to support AI technology for improving the probability of identifying suspicious objects. Moreover, we propose an efficient transmission mechanism to reduce system network congestion since a massive amount of data will be generated by 4K cameras during real-time monitoring. The evaluation results indicate that the advantages and efficiency of the proposed scheme are significant."
618,unknown,10.3390/s20040998,Sensors,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/84c2265a8c4a8e693b6fff967877f4afd76f1489,2020-01-01 00:00:00,moreair: a low-cost urban air pollution monitoring system,"MoreAir is a low-cost and agile urban air pollution monitoring system. This paper describes the methodology used in the development of this system along with some preliminary data analysis results. A key feature of MoreAir is its innovative sensor deployment strategy which is based on mobile and nomadic sensors as well as on medical data collected at a children’s hospital, used to identify urban areas of high prevalence of respiratory diseases. Another key feature is the use of machine learning to perform prediction. In this paper, Moroccan cities are taken as case studies. Using the agile deployment strategy of MoreAir, it is shown that in many Moroccan neighborhoods, road traffic has a smaller impact on the concentrations of particulate matters (PM) than other sources, such as public baths, public ovens, open-air street food vendors and thrift shops. A geographical information system has been developed to provide real-time information to the citizens about the air quality in different neighborhoods and thus raise awareness about urban pollution."
619,unknown,http://arxiv.org/abs/2204.03998v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2204.03998v1,2022-04-08 00:00:00,"snapmode: an intelligent and distributed large-scale fashion image
  retrieval platform based on big data and deep generative adversarial network
  technologies","Fashion is now among the largest industries worldwide, for it represents
human history and helps tell the worlds story. As a result of the Fourth
Industrial Revolution, the Internet has become an increasingly important source
of fashion information. However, with a growing number of web pages and social
data, it is nearly impossible for humans to manually catch up with the ongoing
evolution and the continuously variable content in this domain. The proper
management and exploitation of big data can pave the way for the substantial
growth of the global economy as well as citizen satisfaction. Therefore,
computer scientists have found it challenging to handle e-commerce fashion
websites by using big data and machine learning technologies. This paper first
proposes a scalable focused Web Crawler engine based on the distributed
computing platforms to extract and process fashion data on e-commerce websites.
The role of the proposed platform is then described in developing a
disentangled feature extraction method by employing deep convolutional
generative adversarial networks (DCGANs) for content-based image indexing and
retrieval. Finally, the state-of-the-art solutions are compared, and the
results of the proposed approach are analyzed on a standard dataset. For the
real-life implementation of the proposed solution, a Web-based application is
developed on Apache Storm, Kafka, Solr, and Milvus platforms to create a
fashion search engine called SnapMode."
620,unknown,http://arxiv.org/abs/2012.10610v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2012.10610v3,2020-12-19 00:00:00,"spaceml: distributed open-source research with citizen scientists for
  the advancement of space technology for nasa","Traditionally, academic labs conduct open-ended research with the primary
focus on discoveries with long-term value, rather than direct products that can
be deployed in the real world. On the other hand, research in the industry is
driven by its expected commercial return on investment, and hence focuses on a
real world product with short-term timelines. In both cases, opportunity is
selective, often available to researchers with advanced educational
backgrounds. Research often happens behind closed doors and may be kept
confidential until either its publication or product release, exacerbating the
problem of AI reproducibility and slowing down future research by others in the
field. As many research organizations tend to exclusively focus on specific
areas, opportunities for interdisciplinary research reduce. Undertaking
long-term bold research in unexplored fields with non-commercial yet great
public value is hard due to factors including the high upfront risk, budgetary
constraints, and a lack of availability of data and experts in niche fields.
Only a few companies or well-funded research labs can afford to do such
long-term research. With research organizations focused on an exploding array
of fields and resources spread thin, opportunities for the maturation of
interdisciplinary research reduce. Apart from these exigencies, there is also a
need to engage citizen scientists through open-source contributors to play an
active part in the research dialogue. We present a short case study of SpaceML,
an extension of the Frontier Development Lab, an AI accelerator for NASA.
SpaceML distributes open-source research and invites volunteer citizen
scientists to partake in development and deployment of high social value
products at the intersection of space and AI."
621,unknown,10.1109/ieeeconf49454.2021.9382732,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9382732/,2021-01-14 00:00:00,2d laser and 3d camera data integration and filtering for human trajectory tracking,"This paper addresses a robust human trajectory tracking method through the data integration of 2D laser scanner and 3D camera. Mapping the deep learning-based 3D camera human detection onto the point cloud of the depth information to build up the 3D bounding box-represented human and using the state-of-the-art 2D laser-based leg detection are the main data streams of the human tracking system. The human-oriented global nearest neighbour (HOGNN) data association, inspired from the Hall's proxemics, was developed to improve both the 3D camera-based and 2D laser-based human detection techniques. The dual Kalman filters are employed to track the human trajectory in parallel. The integration of the 3D camera-based and 2D laser-based human tracking is the key function of the system providing the real-time feedback for both the HOGNN to reduce false-positives of the camera-based and laser-based human detection and the Kalman filter to enhance the quality of the human trajectory tracking under uncertain environmental conditions. We implemented the sensor integration on ROS and validated it through real-world experiments."
622,excluded,http://arxiv.org/abs/2010.07029v2,arxiv,arxiv,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2010.07029v2,2020-09-29 00:00:00,"basic principles and concept design of a real-time clinical decision
  support system for managing medical emergencies on missions to mars","Space agencies and private companies prepare the beginning of human space
exploration for the 2030s with missions to put the first human on the Mars
surface. The absence of gravity and radiation, along with distance, isolation
and hostile environments, are expected to increase medical events where
previously unseen manifestations may arise. The current healthcare strategy
based on telemedicine and the possibility to stabilize and transport the
injured crewmember to a terrestrial definitive medical facility is not
applicable in exploration class missions. Therefore, the need for deploying the
full autonomous capability to solve medical emergencies may guide the design of
future onboard healthcare systems. We present ten basic principles and concept
design of a software suite to bring onboard decision support to help the crew
dealing with medical emergencies taking into consideration physiological
disturbances in space and spaceflight restrictions. 1) give real-time support
for emergency medical decision making, 2) give patient-specific advice for
executive problem-solving, 3) take into account available information from life
support and monitoring of crewmembers, 4) be fully autonomous from remote
facilities, 5) continuously adapt predictions to physiological disturbance and
changing conditions, 6) optimize emergency medical decision making in terms of
mission fundamental priorities, 7) take into account medical supplies and
equipment on board, 8) apply health standards for the level of care V, 9)
implement ethics responsibilities for spaceflights, and 10) apply ethical
standards for artificial intelligence. Based on these principles, we propose an
autonomous clinical decision support system (CDSS) to provide real-time advice
for emergency medical interventions on board of space exploration missions."
623,unknown,10.1007/978-3-030-81716-9_7,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-3-030-81716-9_7,2022-01-01 00:00:00,fault diagnosis in structural health monitoring systems using signal processing and machine learning techniques,"Smart structures leverage intelligent structural health monitoring (SHM) systems, which comprise sensors and processing units deployed to transform sensor data into decisions. Faulty sensors may compromise the reliability of SHM systems, causing data corruption, data loss, and erroneous judgment of structural conditions. Fault diagnosis (FD) of SHM systems encompasses the detection, isolation, identification, and accommodation of sensor faults, aiming to ensure the reliability of SHM systems. Typically, FD is based on “analytical redundancy,” utilizing correlated sensor data inherent to the SHM system. However, most analytical redundancy FD approaches neglect the fault identification step and are tailored to specific types of sensor data. In this chapter, an analytical redundancy FD approach for SHM systems is presented, coupling methods for processing any type of sensor data and two machine learning (ML) techniques, (i) an ML regression algorithm used for fault detection, fault isolation, and fault accommodation, and (ii) an ML classification algorithm used for fault identification. The FD approach is validated using an artificial neural network as ML regression algorithm and a convolutional neural network as ML classification algorithm. Validation is performed through a real-world SHM system in operation at a railway bridge. The results demonstrate the suitability of the FD approach for ensuring reliable SHM systems."
624,excluded,10.1007/978-981-16-7167-8_25,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-981-16-7167-8_25,2022-01-01 00:00:00,e-irrigation solutions for forecasting soil moisture and real-time automation of plants watering,"This study deals Rahman, Md Mijanur  with the development of e-irrigation solution Majher, Sonda  for forecasting soil conditions and real- time Jannat, Tanzin Ara  automation of watering in smart agriculture applications. There are several factors that affect agriculture, including limitation of water resources, proper usage of pesticides, inaccuracy in the prediction of soil moisture, inefficient irrigation management, etc. Moreover, traditional irrigation procedures lack proper managing of plants/crops watering that causes wastage of water. As smart farming is an emerging concept, this research work aims of using evolving technologies, such as the “Internet of Things (IoT)”, “Information and Communication Technology (ICT)”, wireless sensor networks, cloud computing, machine learning, big data, etc. This paper presents a smart irrigation system for predicting irrigation requirements and performing automatic watering process with the help of IoT tools, ICT protocols, and machine learning approaches. The e-irrigation system is developed based on an intelligent irrigation algorithm considering sensors and weather forecasting data. The e-irrigation solution has been implemented in small scale that can predict the soil condition and irrigate plants effectively without human intervention."
625,unknown,10.1109/cbms.2019.00129,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8787485/,2019-06-07 00:00:00,analysing the performance of a real-time healthcare 4.0 system using shared frailty time to event models,"This paper introduces the real-time Healthcare 4.0 system, the VILIAlert system and a new approach that we propose for the robust assessment of it's performance. The VILIAlert system alerts clinicians when a patient's tidal volume value rises above the clinically accepted level of 8 ml/kg as beyond this point (> 8 ml/kg), a patient is considered high risk of permanent damage to their lungs. In order to ensure success with the VILIAlert system, the ideal scenario is to ensure that as soon as patients in the Intensive Care Unit experience tidal volume values beyond the 8 ml/kg level, a clinical intervention can be carried out so to minimise the risk of patients ever having permanent damage. The approach has been implemented in the Intensive Care Unit at the Royal Victoria Hospital Belfast, Northern Ireland demonstrating the potential for such an approach to be used across all hospitals in the region."
626,unknown,10.1109/embc46164.2021.9629820,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9629820/,2021-11-05 00:00:00,ubiei-edge: human big data decoding using deep learning on mobile edge,"Human big data decoding is of great potential to reveal the complex patterns of human dynamics like physiological and biomechanical signals. In this study, we take special interest in brain visual dynamics, e.g., eye movement signals, and investigate how to leverage eye signal decoding to provide a voice-free communication possibility for ALS patients who lose ability to control their muscles. Due to substantial complexity of visual dynamics, we propose a deep learning framework to decode the visual dynamics when the user performs eye-writing tasks. Further, to enable real-time inference of the eye signals, we design and develop a mobile edge computing platform, called UbiEi-Edge, which can wirelessly receive the eye signals via low-energy Bluetooth, execute the deep learning algorithm, and visualize decoding results. This real word implementation, developed on an Android Phone, aims to provide real-time data streaming and automatic, real-time decoding of brain visual dynamics, thereby enabling a new paradigm for ALS patients to communicate with the external world. Our experiment has demonstrated the feasibility and effectiveness of the proposed novel mobile edge computing prototype. The study, by innovatively bridging AI, edge computing, and mobile health, will greatly advance the brain dynamics decoding-empowered human-centered computing and smart health big data applications."
627,unknown,10.1109/access.2018.2873907,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8482108/,2018-01-01 00:00:00,sdn-based multi-tier computing and communication architecture for pervasive healthcare,"A large spectrum of healthcare applications, ranging from continuous blood sugar level monitoring to sleep apnea detection are nowadays facilitated by modern mobile gadgets. Wearable and ambient sensors generate enormous amounts of physiological data that demand high computation power for real-time processing and large storage area for recording the personal data. In order to conserve the energy on the battery-limited mainstream mobile devices of the end-users, the execution of the healthcare applications may be offloaded to a remote server. While cloud computing provides unlimited pool of resources for latency-tolerant services such as training a machine learning model, the personalization of healthcare services and the delay sensitivity of continuous health assessment necessitate a computation infrastructure in the vicinity of the end-users. As a remedy to address various demands of a wide range of pervasive healthcare applications, we propose a multi-tier computing and communication architecture composed of end-user devices, edge servers, and legacy cloud data-centers. The dynamic management of this architecture, policies to be applied within the network and the orchestration of the healthcare services are carried out by the concept of programmable networks, in the form of software-defined networking (SDN). As a concrete demonstration of our ideas, a fall risk assessment service is implemented and an experimental study is conducted to evaluate its accuracy and the performance of the multi-tier architecture. The results indicate that the proposed architecture is feasible to enable real-time healthcare services and has significant performance advantages over traditional cloud-based approaches."
628,excluded,10.1186/s13638-021-01971-x,Springer,springer,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1186/s13638-021-01971-x,2021-05-04 00:00:00,caramel: results on a secure architecture for connected and autonomous vehicles detecting gps spoofing attacks,"The main goal of the H2020-CARAMEL project is to address the cybersecurity gaps introduced by the new technological domains adopted by modern vehicles applying, among others, advanced Artificial Intelligence and Machine Learning techniques. As a result, CARAMEL enhances the protection against threats related to automated driving, smart charging of Electric Vehicles, and communication among vehicles or between vehicles and the roadside infrastructure. This work focuses on the latter and presents the CARAMEL architecture aiming at assessing the integrity of the information transmitted by vehicles, as well as at improving the security and privacy of communication for connected and autonomous driving. The proposed architecture includes: (1) multi-radio access technology capabilities, with simultaneous 802.11p and LTE-Uu support, enabled by the connectivity infrastructure; (2) a MEC platform, where, among others, algorithms for detecting attacks are implemented; (3) an intelligent On-Board Unit with anti-hacking features inside the vehicle; (4) a Public Key Infrastructure that validates in real-time the integrity of vehicle’s data transmissions. As an indicative application, the interaction between the entities of the CARAMEL architecture is showcased in case of a GPS spoofing attack scenario. Adopted attack detection techniques exploit robust in-vehicle and cooperative approaches that do not rely on encrypted GPS signals, but only on measurements available in the CARAMEL architecture."
629,unknown,10.2118/192513-ms,"Day 2 Thu, November 29, 2018",semantic_scholar,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/a9e3fae6da524ae16fce0a30cc77a25010d1bdea,2018-01-01 00:00:00,iiot edge analytics: deploying machine learning at the wellhead to identify rod pump failure,"
 Oil and Gas operators now have the possibility to collect and leverage significant amounts of data directly at the extremities of their production networks. Data combined with Industrial Internet of Things (IIoT) architecture is an opportunity to improve maintenance of assets, increase their up-time, reduce safety risks and optimize operational costs. However, to turn data into meaningful insights, Oil and Gas industry needs to fully take benefit of Machine Learning (ML) models which are able to consume real-time data and provide insights in isolated locations with scarce connectivity. These ML models need to be precise, robust and compatible with Edge computing capabilities.
 This paper presents an analytics solution for rod pumps, capable of automated Dynagraph Card recognition at the wellhead leveraging an ensemble of ML models deployed at the Edge. The proposed solution does not require Internet connectivity to generate alarms and addresses confidentiality requirements of Oil and Gas industry. An overview of the employed ML models as well as the computing and communication infrastructure is given. We believe the given outline is insightful for the petroleum industry on its road to digitization and optimization of Artificial Lift systems."
630,unknown,10.1109/upcon47278.2019.8980238,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8980238/,2019-11-10 00:00:00,iot based ehealth management system using arduino and google cloud firestore,"With the global population crossing 7 billion, the number of deaths from cardiovascular diseases is on the rise. According to WHO report, 17.9 million people die each year from CVDs, representing 31% of all global deaths. Of these deaths, 85% are due to heart attack and stroke. Cardiovascular diseases, the leading cause of premature death in the world include heart attacks, strokes and other circulatory diseases. More than 75% of CVD deaths occur in low-income and middle-income countries. So the challenge is how to provide a health-care monitoring system especially for rural areas which lacks in the established set up of cardio health centers. The scarcity of specialist can partially be solved with sharing the people medical information with professionals, specialists and artificial intelligence health management system on cloud to give information at early stage, which can increase the quality of life and also lessen the risk of death. In this paper we discuss the implementation and design of a monitoring and measurement system based on IoT (internet of things) with Arduino. This system uses Arduino microcontroller with ESP8266 NodeMCU as a WI-FI module which helps to send and store the real time information of patient health to Google firebase."
631,excluded,10.1109/miucc52538.2021.9447632,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9447632/,2021-05-27 00:00:00,edge/fog-based architecture design for intelligent surveillance systems in smart cities: a software perspective,"Surveillance systems are critical for the growth of smart cities. These systems can be thought of as the cities&#x2019; vision organs. It is anticipated that smart cities will produce a massive amount of data. Thus, to ensure the safety of its people, it is essential to conduct an efficient and real-time analysis of these data in order to receive timely responses in the event of catastrophic incidents. As a result, the process of transferring this vast data to the cloud for processing is relatively slow. In this paper, we present the software perspective to design and implement EFISS, a multilayer computing-based architecture for an intelligent, resourceefficient, and real-time surveillance system for smart cities. The framework, which is comprised of edge-fog computational layers, would help in the prevention of crime and the prediction of criminal incidents in smart cities. The EFISS can identify and validate crimes in real-time, using artificial intelligence (AI) and an event-driven method to transmit crime data to protective services and police units, allowing rapid intervention while conserving resources."
632,unknown,http://arxiv.org/abs/1908.10001v1,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1908.10001v1,2019-08-27 00:00:00,real-world conversational ai for hotel bookings,"In this paper, we present a real-world conversational AI system to search for
and book hotels through text messaging. Our architecture consists of a
frame-based dialogue management system, which calls machine learning models for
intent classification, named entity recognition, and information retrieval
subtasks. Our chatbot has been deployed on a commercial scale, handling tens of
thousands of hotel searches every day. We describe the various opportunities
and challenges of developing a chatbot in the travel industry."
633,excluded,10.1109/iros.2017.8206437,IEEE,ieeexplore,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8206437/,2017-09-28 00:00:00,a multimodal execution monitor with anomaly classification for robot-assisted feeding,"Activities of daily living (ADLs) are important for quality of life. Robotic assistance offers the opportunity for people with disabilities to perform ADLs on their own. However, when a complex semi-autonomous system provides real-world assistance, occasional anomalies are likely to occur. Robots that can detect, classify and respond appropriately to common anomalies have the potential to provide more effective and safer assistance. We introduce a multimodal execution monitor to detect and classify anomalous executions when robots operate near humans. Our system builds on our past work on multimodal anomaly detection. Our new monitor classifies the type and cause of common anomalies using an artificial neural network. We implemented and evaluated our execution monitor in the context of robot-assisted feeding with a general-purpose mobile manipulator. In our evaluations, our monitor outperformed baseline methods from the literature. It succeeded in detecting 12 common anomalies from 8 able-bodied participants with 83% accuracy and classifying the types and causes of the detected anomalies with 90% and 81% accuracies, respectively. We then performed an in-home evaluation with Henry Evans, a person with severe quadriplegia. With our system, Henry successfully fed himself while the monitor detected, classified the types, and classified the causes of anomalies with 86%, 90%, and 54% accuracy, respectively."
634,unknown,10.1109/itsc.2017.8317664,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8317664/,2017-10-19 00:00:00,a new modelling framework over temporal graphs for collaborative mobility recommendation systems,"Over the years, collaborative mobility proved to be an important but challenging component of the smart cities paradigm. One of the biggest challenges in the smart mobility domain is the use of data science as an enabler for the implementation of large scale transportation sharing solutions. In particular, the next generation of Intelligent Transportation Systems (ITS) requires the combination of artificial intelligence and discrete simulations when exploring the effects of what-if decisions in complex scenarios with millions of users. In this paper, we address this challenge by presenting an innovative data modelling framework that can be used for ITS related problems. We demonstrate that the use of graphs and time series in multi-dimensional data models can satisfy the requirements of descriptive and predictive analytics in real-world case studies with massive amounts of continuously changing data. The features of the framework are explained in a case study of a complex collaborative mobility system that combines carpooling, carsharing and shared parking. The performance of the framework is tested with a large-scale dataset, performing machine learning tasks and interactive realtime data visualization. The outcome is a fast, efficient and complete architecture that can be easily deployed, tested and used for research as well in an industrial environment."
635,unknown,10.1109/icccworkshops52231.2021.9538856,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9538856/,2021-07-30 00:00:00,implementation of an intelligent target detection system for edge node,"With the rapid development of the artificial intelligence industry, the application of object detection technology in real life is becoming increasingly widespread, such as intelligent monitoring, autonomous driving, and augmented reality. In this paper, object detection system is deployed on edge devices which is low complexity and low-cost device such as the Raspberry Pi. Implement Mobilenet-SSD based on the deep learning framework and deploy on edge devices, such as image acquisition, object detection and result display. The results show that the object detection technology can also be achieved on devices with scarce computing resources such as the Raspberry Pi and satisfies actual business requirements."
636,excluded,http://arxiv.org/abs/2009.11722v1,arxiv,arxiv,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2009.11722v1,2020-09-23 00:00:00,"cloud2edge elastic ai framework for prototyping and deployment of ai
  inference engines in autonomous vehicles","Self-driving cars and autonomous vehicles are revolutionizing the automotive
sector, shaping the future of mobility altogether. Although the integration of
novel technologies such as Artificial Intelligence (AI) and Cloud/Edge
computing provides golden opportunities to improve autonomous driving
applications, there is the need to modernize accordingly the whole prototyping
and deployment cycle of AI components. This paper proposes a novel framework
for developing so-called AI Inference Engines for autonomous driving
applications based on deep learning modules, where training tasks are deployed
elastically over both Cloud and Edge resources, with the purpose of reducing
the required network bandwidth, as well as mitigating privacy issues. Based on
our proposed data driven V-Model, we introduce a simple yet elegant solution
for the AI components development cycle, where prototyping takes place in the
cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment
and evaluation on the target ECUs (Electronic Control Units) is performed as
Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework
is demonstrated using two real-world use-cases of AI inference engines for
autonomous vehicles, that is environment perception and most probable path
prediction."
637,included,10.17863/cam.45198,Journal of Management in Engineering,core,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),,2020-05-01 00:00:00,developing a dynamic digital twin at building and city levels: a case study of the west cambridge campus,"A Digital Twin (DT) refers to a digital replica of physical assets, processes and systems. DTs integrate artificial intelligence, machine learning and data analytics to create living digital simulation models that are able to learn and update from multiple sources, and to represent and predict the current and future conditions of physical counterparts. However, the current activities related to DTs are still at an early stage with respect to buildings and other infrastructure assets from an architectural and engineering/construction point of view. Less attention has been paid to the operation & maintenance (O&M) phase, which is the longest time span in the asset life cycle. A systematic and clear architecture verified with practical use cases for constructing a DT would be the foremost step for effective operation and maintenance of buildings and cities. According to current research about multi-tier architectures, this paper presents a system architecture for DTs which is specifically designed at both the building and city levels. Based on this architecture, a DT demonstrator of the West Cambridge site of the University of Cambridge was developed, which integrates heterogeneous data sources, supports effective data querying and analysing, supports decision-making processes in O&M management, and further bridges the gap between human relationships with buildings/cities. This paper aims at going through the whole process of developing DTs in building and city levels from the technical perspective and sharing lessons learnt and challenges involved in developing DTs in real practices. Through developing this DT demonstrator, the results provide a clear roadmap and present particular DT research efforts for asset management practitioners, policymakers and researchers to promote the implementation and development of DT at the building and city levels.Centre for Digital Built Britain (Innovate UK); Centre for Smart Infrastructure and Construction (Innovate UK/EPSRC"
638,unknown,10.1109/wf-iot.2016.7845462,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7845462/,2016-12-14 00:00:00,namatad: inferring occupancy from building sensors using machine learning,"Driven by the need to improve efficiency, modern buildings are instrumented with numerous sensors to monitor utilization and regulate environmental conditions. While these sensor systems serve as valuable tools for managing the comfort and health of occupants, there is an increasing need to expand the deployment of sensors to provide additional insights. Because many of these desired insights have high temporal value, such as occupancy during emergency situations, such insights are needed in real time. However, augmenting buildings with new sensors is often expensive and requires a significant capital investment. In this paper, we propose and describe the real-time, streaming system called Namatad that we developed to infer insights from many sensors typical of Internet of Things (IoT) deployments. We evaluate the effectiveness of this platform by leveraging machine learning to infer new insights from environmental sensors within buildings. We describe how we built the components of our system leveraging several open source, streaming frameworks. We also describe how we ingest and aggregate from building sensors and sensing platforms, route data streams to appropriate models, and make predictions using machine learning techniques. Using our system, we have been able to predict the occupancy of rooms within a building on the University of Washington campus over the last three months, in real time, at accuracies of up to 95%."
639,unknown,10.1016/j.measurement.2022.110819,scopus,sciencedirect,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85124237105,2022-03-15,chicktrack – a quantitative tracking tool for measuring chicken activity,"The automatic detection, counting and tracking of individual and flocked chickens in the poultry industry is of paramount to enhance farming productivity and animal welfare. Due to methodological difficulties, such as the complex background of images, varying lighting conditions, and occlusions from e.g., feeding stations, water nipple stations and barriers in the chicken rearing production floor, it is a challenging task to automatically recognize and track birds using computer software. Here, a deep learning model based on You Only Look Once (Yolov5) is proposed for detecting domesticated chickens from videos with varying complex backgrounds. A multiscale feature is being adapted to the Yolov5 network for mapping modules in the counting and tracking of the trajectories of the chickens. The Yolov5 network was trained and tested on our dataset which resulted in an enhanced tracking precision accuracy. Using Kalman Filter, the proposed model was able to track multiple chickens simultaneously with the focus to associate individual chickens across the frames of the video for real time and online applications. By being able to detect the chickens amid diverse background interference and counting them precisely along with tracking the movement and measuring their travelled path and direction, the proposed model provides excellent performance for on-farm applications. Artificial intelligence enabled automatic measurements of chicken behavior on-farm using cameras offers continuous monitoring of the chicken's ability to perch, walk, interact with other birds and the farm environment, as well as the assessment of dustbathing, thigmotaxis, and foraging frequency, which are important indicators for their ability to express natural behaviors. This study highlights the potential of automated monitoring of poultry through the usage of ChickTrack model as a digital tool in enabling science-based animal husbandry practices and thereby promote positive welfare for chickens in animal farming."
640,excluded,http://arxiv.org/abs/2202.06622v3,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2202.06622v3,2022-02-14 00:00:00,"an edge-cloud based reference architecture to support cognitive
  solutions in the process industry","Process Industry (PI e.g. Steel, Metals, Chemicals, Cement, Asphalt,
Ceramics) is one of the leading sectors of the world economy, characterized
however by intense environmental impact, and very high energy consumption. In
spite of a traditional low innovation pace in PI, in the recent years a strong
push at worldwide level towards the dual objective of improving the efficiency
of plants and the quality of products, significantly reducing the consumption
of electricity and CO2 emissions has taken momentum. Digital Technologies
(namely Smart Embedded Systems, IoT, Data, AI and Edge-to-Cloud Technologies)
are enabling drivers for a Twin Digital-Green Transition, as well as
foundations for human centric, safe, comfortable and inclusive work places.
Currently, digital sensors in plants produce a large amount of data which in
most cases constitutes just a potential and not a real value for Process
Industry, often locked-in in close proprietary systems and seldomly exploited.
Digital technologies, with process modelling-simulation via digital twins, can
build a bridge between the physical and the virtual worlds, bringing innovation
with great efficiency and drastic reduction of waste. In accordance with the
guidelines of Industrie 4.0, the H2020 funded CAPRI project aims to innovate
the process industry, with a modular and scalable Reference Architecture, based
on open source software, which can be implemented both in brownfield and
greenfield scenarios. The ability to distribute processing between the edge,
where the data is created, and the cloud, where the greatest computational
resources are available, facilitates the development of integrated digital
solutions with cognitive capabilities. The reference architecture is being
validated in the asphalt, steel & pharma pilot plants, allowing the development
of integrated planning solutions, with scheduling and control of the plants."
641,unknown,http://arxiv.org/abs/2204.13103v1,arxiv,arxiv,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2204.13103v1,2022-04-27 00:00:00,"flowgnn: a dataflow architecture for universal graph neural network
  inference via multi-queue streaming","Graph neural networks (GNNs) have recently exploded in popularity thanks to
their broad applicability to graph-related problems such as quantum chemistry,
drug discovery, and high energy physics. However, meeting demand for novel GNN
models and fast inference simultaneously is challenging because of the gap
between developing efficient accelerators and the rapid creation of new GNN
models. Prior art focuses on the acceleration of specific classes of GNNs, such
as Graph Convolutional Network (GCN), but lacks the generality to support a
wide range of existing or new GNN models. Meanwhile, most work rely on graph
pre-processing to exploit data locality, making them unsuitable for real-time
applications. To address these limitations, in this work, we propose a generic
dataflow architecture for GNN acceleration, named FlowGNN, which can flexibly
support the majority of message-passing GNNs. The contributions are three-fold.
First, we propose a novel and scalable dataflow architecture, which flexibly
supports a wide range of GNN models with message-passing mechanism. The
architecture features a configurable dataflow optimized for simultaneous
computation of node embedding, edge embedding, and message passing, which is
generally applicable to all models. We also propose a rich library of
model-specific components. Second, we deliver ultra-fast real-time GNN
inference without any graph pre-processing, making it agnostic to dynamically
changing graph structures. Third, we verify our architecture on the Xilinx
Alveo U50 FPGA board and measure the on-board end-to-end performance. We
achieve a speed-up of up to 51-254x against CPU (6226R) and 1.3-477x against
GPU (A6000) (with batch sizes 1 through 1024); we also outperform the SOTA GNN
accelerator I-GCN by 1.03x and 1.25x across two datasets. Our implementation
code and on-board measurement are publicly available on GitHub."
642,excluded,10.1109/educon52537.2022.9766555,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9766555/,2022-03-31 00:00:00,internet of things meets machine learning: a water usage alert example,"The rapid growth of the electronics industry resulted in numerous, amazing and cheap devices, while fluent documentation and user-friendly programming environments are available for them. Modern educational systems worldwide have exploited this dynamic by including in their didactic curricula innovative practices that are usually called STEM actions. Added to this, enriching educational methods with real-world problem solving techniques increases students&#x2019; interest and prepares them for their future role in the society. Apparently, such challenging problems are not missing, with the depletion of natural resources to be one of the most intense ones. In this context, promising modern technological flavors like Internet of Things (IoT) and Machine Learning (ML) can join their potential to form educationally fruitful and also practically important activities targeted at increasing the social awareness for the water misuse problem, like the ones proposed herein. These activities also encourage the deployment of low-cost appliances that, only with minor modifications, can respond to a wide variety real problems in either urban or rural environments."
643,unknown,http://arxiv.org/abs/2111.13657v2,arxiv,arxiv,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2111.13657v2,2021-11-26 00:00:00,"amazon sagemaker model monitor: a system for real-time insights into
  deployed machine learning models","With the increasing adoption of machine learning (ML) models and systems in
high-stakes settings across different industries, guaranteeing a model's
performance after deployment has become crucial. Monitoring models in
production is a critical aspect of ensuring their continued performance and
reliability. We present Amazon SageMaker Model Monitor, a fully managed service
that continuously monitors the quality of machine learning models hosted on
Amazon SageMaker. Our system automatically detects data, concept, bias, and
feature attribution drift in models in real-time and provides alerts so that
model owners can take corrective actions and thereby maintain high quality
models. We describe the key requirements obtained from customers, system design
and architecture, and methodology for detecting different types of drift.
Further, we provide quantitative evaluations followed by use cases, insights,
and lessons learned from more than 1.5 years of production deployment."
644,excluded,10.1109/wcmeim52463.2020.00019,IEEE,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9409493/,2020-12-06 00:00:00,a portable system for tof camera based human body detection and pose estimation,"In recent years, human motion detection and analysis has been a research hotspot in the field of machine vision. It has been widely used in sports fitness, sports rehabilitation, sports analysis and virtual reality. Most of the existing human motion detection systems are based on optical equipment using retroreflective markers, which have many defects such as high deployment cost and complex operation. To this end, this article develops a household fitness motion evaluation system based on ToF camera with a mobile phone. The system firstly uses the ToF camera to collect the 3D human body point cloud information. Then Huawei augmented reality engine is employed to track the human skeleton. As all the spatial coordinates of joints are acquired, virtual human model can be reconstructed remotely on the phone. The mobile phone, working as the edge node between the person and the cloud server, transmits the captured human motion model parameters to the cloud platform. Then a hierarchical co-occurrence neural network recognizes and scores the detected user actions online. The experimental results show that the system proposed in this paper is portable and inexpensive. Real-time capabilities and measurement accuracy can also satisfy the requirements of evaluation for physical movements in sports rehabilitation."
645,excluded,10.1109/jiot.2019.2902410,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8656525/,2019-10-01 00:00:00,experiences with iot and ai in a smart campus for optimizing classroom usage,"Increasing demand for university education is putting pressure on campuses to make better use of their real-estate resources. Evidence indicates that enrollments are rising, yet attendance is falling due to diverse demands on student time and easy access to online content. This paper outlines our efforts to address classroom under-utilization in a real university campus arising from the gap between enrollment and attendance. We do so by instrumenting classrooms with Internet of Things (IoT) sensors to measure real-time usage, using AI to predict attendance, and performing optimal allocation of rooms to courses so as to minimize space wastage. Our first contribution undertakes an evaluation of several IoT sensing approaches for measuring class occupancy, and comparing them in terms of cost, accuracy, privacy, and ease of deployment/operation. Our second contribution instruments nine lecture halls of varying capacity across campus, collects and cleans live occupancy data spanning about 250 courses over two sessions, and draws insights into attendance patterns, including identification of canceled lectures and class tests, while also releasing our data openly to the public. Our third contribution is to use AI techniques for predicting classroom attendance, applying them to real data, and accurately predicting future attendance with an root-mean-square error as low as 0.16. Our final contribution is to develop an optimal allocation of classes to rooms based on predicting attendance rather than enrollment, resulting in over 10% savings in room costs with very low risk of room overflows."
646,unknown,10.1007/s11277-020-07040-8,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s11277-020-07040-8,2021-03-01 00:00:00,an optimized integrated framework of big data analytics managing security and privacy in healthcare data,"Big data analytics has anonymously changed the overall global scenario to discover knowledge trends for future decision making. In general, potential area of big data application tends to be healthcare, where the global burden is to improve patient diagnostic system and providing patterns to assure the privacy of the end users. However, data constraints exists on real data which needs to be accessed while preserving the security of patients for further diagnostic analysis. This advancement in big data needs to addressed where the patient right needs to maintained while the disclosure of knowledge discovery for future needs are also addressed. To, embark and acknowledge the big data environment its adherently important to determine the cutting-edge research which can benefit end users and healthcare practioners to discover overall prognosis and diagnosis of disease while maintaining the concerns for privacy and security of patient data. In current state of art, we tried to address the big data analytics approach while maintain privacy of healthcare databases for future knowledge discovery. The current objective was to design and develop a novel framework which can integrate the big data with privacy and security concerns and determine knowledgably patterns for future decision making. In the current study we have utilized big data analytical technique for patients suffering from Human Immunodeficiency Virus (HIV) and Tuberculosis (TB) coinfection to develop trends and detect patterns with socio economic factors. Further, a novel framework was implemented using unsupervised learning technique in STATA and MATLAB 7.1 to develop patterns for knowledge discovery process while maintain the privacy and security of data. The study overall can benefit end users to predict future prognosis of disease and combinatorial effects to determining varied policies which can assist patients with needs."
647,excluded,10.29019/enfoqueute.v10n1.445,Enfoque UTE,semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/d3a04a3a23ebaa63f5680054ee7a4d473f36fa2c,2019-01-01 00:00:00,"where to park? architecture and implementation of an empty parking lot, automatic recognition system","The traffic congestion present in practically every city has, among its increasing factors, the unavailability of enough parking spaces. A typical driver invests a considerable part of the total trip time looking for a free space where to park his vehicle; in many cases, this leads to delays and the consequent discomfort due to the undesired consequences of the generated tardiness. For this problem it is possible to find partial palliative solutions, minimizing the time spent searching for parking by applying Internet of Things techniques, oriented to smart cities and buildings. This research has been focused on finding an appropriate computer architecture that will allow the implementation of a distributed system, which, thanks to the use of computer vision and machine learning techniques, detects free parking spaces inside a parking lot, and provides real time information to the driver, allowing him to go as directly as possible to a vacant parking space."
648,excluded,10.1145/2208828.2208857,"2012 Third International Conference on Future Systems: Where Energy, Computing and Communication Meet (e-Energy)",semantic_scholar,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/18dc6cb94eaa53a69a9af4b562733e98e22e7e33,2012-01-01 00:00:00,smartcharge: cutting the electricity bill in smart homes with energy storage,"Market-based electricity pricing provides consumers an opportunity to lower their electric bill by shifting consumption to low price periods. In this paper, we explore how to lower electric bills without requiring consumer involvement using an intelligent charging system, called SmartCharge, and an on-site battery array to store low-cost energy for use during high-cost periods. SmartCharge's algorithm reduces electricity costs by determining when to switch the home's power supply between the grid and the battery array. The algorithm leverages a prediction model we develop, which forecasts future demand using statistical machine learning techniques. We evaluate SmartCharge in simulation using data from real homes to quantify its potential to lower bills in a range of scenarios. We show that typical savings today are 10-15%, but increase linearly with rising electricity prices. We also find that SmartCharge deployed at only 22% of 435 homes reduces the aggregate demand peak by 20%. Finally, we analyze SmartCharge's installation and maintenance costs. Our analysis shows that battery advancements, combined with an expected rise in electricity prices, have the potential to make the return on investment positive for the average home within the next few years."
649,unknown,10.2196/preprints.23368,,semantic_scholar,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://www.semanticscholar.org/paper/cc06199ccc2db316894c13599324634f77fc3fcd,2020-01-01 00:00:00,design and deployment of e-health system in perspective of developing countries: machine learning based approach (preprint),"
 BACKGROUND
 We are living in a world where data science and machine learning is tightening its grasp on many sectors of modern life. The medical sector is not an exception. In developing countries, healthcare is one of the domains that need immediate attention. Due to the lack of manpower and technical resources, a large number of people in these regions do not receive proper medical care. Designing an E-health system with the help of machine learning and web technologies would be a great aid in such circumstances.
 
 
 OBJECTIVE
 This proposed E-health System will assist the medical professionals in determining diseases. Moreover, the system will be also helpful for the patients to check whether they have been diagnosed correctly. Based on their diagnosis results they can get medical specialist recommendation and medicine suggestions from the system. The automation of identifying the diseases and suggestion models with the help of machine learning will be cost-efficient and time-saving compared to the traditional methods. The main objective of this E-health system is to provide health care with the help of sustainable and realistic machine learning technologies.
 
 
 METHODS
 In this research, for the disease identification part, machine learning techniques have been applied to identify three diseases which are Dengue, Diabetes, and Thyroid. Decision Tree, Gaussian Naive-Bayes, Random Forest, Logistic Regression, k-Nearest Neighbors, Multilayer Perceptron, and Support Vector Machine Classifiers have been used for all three diseases. The E-health system comprised of disease identification model, medical specialist recommendation model, and the medicine suggestion model has been deployed on the web. The medical specialist recommendation model and the medicine suggestion model results are based on the finding of the disease identification model. Any user can insert their disease-specific data to use these three features of the E-health system.
 
 
 RESULTS
 For the disease identification model, Multilayer Perceptron for Dengue, Logistic Regression for Diabetes, and Random Forest for Thyroid performed the best with accuracies of 88.3%, 82.5%, and 98.5% respectively. These classifiers also showed good precision, recall, and F1 score.
 
 
 CONCLUSIONS
 The E-health system has performed well with real-time data. By making the dataset more enriched, the disease identification model will be more robust and thorough. Moreover, usability and acceptance tests can help us in finding different real-time scenarios of the E-health system.
"
650,unknown,10.1007/s42797-021-00025-1,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s42797-021-00025-1,2020-10-01 00:00:00,"design of a dynamic and self-adapting system, supported with artificial intelligence, machine learning and real-time intelligence for predictive cyber risk analytics in extreme environments – cyber risk in the colonisation of mars","Multiple governmental agencies and private organisations have made commitments for the colonisation of Mars. Such colonisation requires complex systems and infrastructure that could be very costly to repair or replace in cases of cyber-attacks. This paper surveys deep learning algorithms, IoT cyber security and risk models, and established mathematical formulas to identify the best approach for developing a dynamic and self-adapting system for predictive cyber risk analytics supported with Artificial Intelligence and Machine Learning and real-time intelligence in edge computing. The paper presents a new mathematical approach for integrating concepts for cognition engine design, edge computing and Artificial Intelligence and Machine Learning to automate anomaly detection. This engine instigates a step change by applying Artificial Intelligence and Machine Learning embedded at the edge of IoT networks, to deliver safe and functional real-time intelligence for predictive cyber risk analytics. This will enhance capacities for risk analytics and assists in the creation of a comprehensive and systematic understanding of the opportunities and threats that arise when edge computing nodes are deployed, and when Artificial Intelligence and Machine Learning technologies are migrated to the periphery of the internet and into local IoT networks."
651,excluded,10.1109/icspis48872.2019.9066130,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9066130/,2019-12-19 00:00:00,deep vision for navigation of autonomous motorcycle in urban and semi-urban environments,"Deep neural networks are currently the best solution for road and traffic scene interpretation for autonomous and self-driving vehicles. Compared to the autonomous cars, motorcycles have significant flexibility and advantages in crowded traffic situations and especially in non-urban and off-road areas. Many off-road tracks especially for agriculture and environment management tasks are only traversable with motorcycles. In this paper, a deep neural network is used for design and implementation of the vision system for navigation of an autonomous motorcycle. The proposed framework is evaluated using real world scenarios captured by a real motorcycle in various complex situations. The experimental results show that the proposed framework is capable of highly accurate interpretation of various environments for autonomous navigation of a motorcycle."
652,unknown,http://arxiv.org/abs/2206.06957v2,arxiv,arxiv,robotics,'robotics' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/2206.06957v2,2022-06-14 00:00:00,"continual-learning-as-a-service (claas): on-demand efficient adaptation
  of predictive models","Predictive machine learning models nowadays are often updated in a stateless
and expensive way. The two main future trends for companies that want to build
machine learning-based applications and systems are real-time inference and
continual updating. Unfortunately, both trends require a mature infrastructure
that is hard and costly to realize on-premise. This paper defines a novel
software service and model delivery infrastructure termed Continual
Learning-as-a-Service (CLaaS) to address these issues. Specifically, it
embraces continual machine learning and continuous integration techniques. It
provides support for model updating and validation tools for data scientists
without an on-premise solution and in an efficient, stateful and easy-to-use
manner. Finally, this CL model service is easy to encapsulate in any machine
learning infrastructure or cloud system. This paper presents the design and
implementation of a CLaaS instantiation, called LiquidBrain, evaluated in two
real-world scenarios. The former is a robotic object recognition setting using
the CORe50 dataset while the latter is a named category and attribute
prediction using the DeepFashion-C dataset in the fashion domain. Our
preliminary results suggest the usability and efficiency of the Continual
Learning model services and the effectiveness of the solution in addressing
real-world use-cases regardless of where the computation happens in the
continuum Edge-Cloud."
653,unknown,10.1016/j.ifacol.2020.12.751,scopus,sciencedirect,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85119604308,2020-01-01,intelligent comprehensive occupational health monitoring system for mine workers,The objective of this work is to present a comprehensive occupational health monitoring system which provides the current state of the occupational health for mine workers. The hearing threshold shift and dust exposure of each individual mine worker is monitored using this system. The data obtained from the system is transmitted via Internet of Things to storage which may be cloud or a server. The novelty of this model lies in its dual ability to monitor both Noise Induced Hearing Loss and Pneumoconiousis which is caused by inhalation of dust particles. The output of this dual system is further processed using Machine learning and artificial intelligence techniques. Recommendations are then provided to the mine worker with regards to their state of health. This system forms part of an early intervention system in the mines. The model was validated using real data from a Platinum mine in South Africa. Future improvement to this work would entail refinement of the current preliminary implementation plan and carrying out the first phase of the implementation.
654,included,10.1109/bigdata.2015.7363884,IEEE,ieeexplore,science,'science' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/7363884/,2015-11-01 00:00:00,pairs: a scalable geo-spatial data analytics platform,"Geospatial data volume exceeds hundreds of Petabytes and is increasing exponentially mainly driven by images/videos/data generated by mobile devices and high resolution imaging systems. Fast data discovery on historical archives and/or real time datasets is currently limited by various data formats that have different projections and spatial resolution, requiring extensive data processing before analytics can be carried out. A new platform called Physical Analytics Integrated Repository and Services (PAIRS) is presented that enables rapid data discovery by automatically updating, joining, and homogenizing data layers in space and time. Built on top of open source big data software, PAIRS manages automatic data download, data curation, and scalable storage while being simultaneously a computational platform for running physical and statistical models on the curated datasets. By addressing data curation before data being uploaded to the platform, multi-layer queries and filtering can be performed in real time. In addition, PAIRS offers a foundation for developing custom analytics. Towards that end we present two examples with models which are running operationally: (1) high resolution evapo-transpiration and vegetation monitoring for agriculture and (2) hyperlocal weather forecasting driven by machine learning for renewable energy forecasting."
655,excluded,10.1007/978-981-16-0171-2_21,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/978-981-16-0171-2_21,2021-01-01 00:00:00,deep learning-based software tools for tuberculosis detection in chest x-ray images,"Deep learning is a rising phenomenon in data analysis and one of the ten innovative technologies. Deep learning allows for multi-layered computational models to learn data representations with multiple levels of abstraction. Deep learning provides the best solution to visual object recognition. Deep learning offers the best solution to tuberculosis (TB) detection in X-ray images. In a shortage of qualified radiologists, these new technologies increase the capacity to improve overall TB diagnosis and treatment. This paper aims to review deep learning-based software techniques, such as CAD4TB, qXR, and Lunit INSIGHT used for detecting chest X-ray (CXR) abnormalities. Deep learning-based CAD4TB software highlights the abnormal region in the form of a heat map image. qXR software identifies 15 abnormalities from abnormal chest X-ray images. Lunit INSIGHT CXR software identifies ten abnormalities from chest radiography. The human observer must validate deep learning-based computer-aided detection (CAD) systems used in real diagnosis service. The accuracy of CAD software is analysed by using the receiver operating characteristic (ROC) curve. Deep learning-based systems in medicine must meet ethical values and validate the results."
656,excluded,10.1109/access.2021.3056577,IEEE,ieeexplore,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/9344674/,2021-01-01 00:00:00,identification of tobacco crop based on machine learning for a precision agricultural sprayer,"Agrochemicals, which are very efficacious in protecting crops, also cause environmental pollution and pose serious threats to farmers’ health upon exposure. In order to cut down the environmental and human health risks associated with agrochemical application, there is a need to develop intelligent application equipment that could detect and recognize crops/weeds, and spray precise doses of agrochemical at the right place and right time. This paper presents a machine-learning based crop/weed detection system for a tractor-mounted boom sprayer that could perform site-specific spraying on tobacco crop in fields. An SVM classifier with a carefully chosen feature combination (texture, shape, and color) for tobacco plant has been proposed and 96% classification accuracy has been achieved. The algorithm has been trained and tested on a real dataset collected in local fields with diverse changes in scale, orientation, background clutter, outdoor lighting conditions, and variation between tobacco and weeds. Performance comparison of the proposed algorithm has been made with a deep learning based classifier (customized for real-time inference). Both algorithms have been deployed on a tractor-mounted boom sprayer in tobacco fields and it has been concluded that the SVM classifier performs well in terms of accuracy (96%) and real-time inference (6 FPS) on an embedded device (Raspberry Pi 4). In comparison, the customized deep learning-based classifier has an accuracy of 100% but performs much slower (0.22 FPS) on the Raspberry Pi 4."
657,unknown,http://arxiv.org/abs/1904.01576v2,arxiv,arxiv,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),http://arxiv.org/abs/1904.01576v2,2019-04-02 00:00:00,"barista: efficient and scalable serverless serving system for deep
  learning prediction services","Pre-trained deep learning models are increasingly being used to offer a
variety of compute-intensive predictive analytics services such as fitness
tracking, speech and image recognition. The stateless and highly parallelizable
nature of deep learning models makes them well-suited for serverless computing
paradigm. However, making effective resource management decisions for these
services is a hard problem due to the dynamic workloads and diverse set of
available resource configurations that have their deployment and management
costs. To address these challenges, we present a distributed and scalable
deep-learning prediction serving system called Barista and make the following
contributions. First, we present a fast and effective methodology for
forecasting workloads by identifying various trends. Second, we formulate an
optimization problem to minimize the total cost incurred while ensuring bounded
prediction latency with reasonable accuracy. Third, we propose an efficient
heuristic to identify suitable compute resource configurations. Fourth, we
propose an intelligent agent to allocate and manage the compute resources by
horizontal and vertical scaling to maintain the required prediction latency.
Finally, using representative real-world workloads for urban transportation
service, we demonstrate and validate the capabilities of Barista."
658,unknown,10.1109/intlec.2009.5352090,IEEE,ieeexplore,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/5352090/,2009-10-22 00:00:00,implementation of a network-based online monitoring system for substation power facilities in urban rail transit,"This paper is focused on implementing an online monitoring system that checks power facility status by applying network based technology to Urban Transit substation power facilities and the results of its on-field tests. This system is composed of a sensor part, a measurement part, a transceiver part, a host computer, and a power source part. The system is designed to collect, save, analyze, and display the online state power facility AI (analog input). This system measures voltage and current from positive feeders and negative feeders where it is possible to check abnormalities of the substation's main power facilities such as transformers, circuit breakers, and rectifiers. By monitoring abnormal data of the Urban transit power facilities real-time and analyzing stored data, establishing procedures of optimized maintenance is possible."
659,excluded,10.1109/vlsid.2018.20,IEEE,ieeexplore,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy'),https://ieeexplore.ieee.org/document/8326883/,2018-01-10 00:00:00,tutorial t2a: safe autonomous systems: real-time error detection and correction in safety-critical signal processing and control algorithms,"While the last two decades have seen revolutions in computing and communications systems, the next few decades will see a revolution in the use of every-day robotics and artificial intelligence in broad societal applications. Examples of such systems include sensor networks, the smart power grid, self-driven cars and autonomous drones. Such systems are driven by signal processing, control and learning algorithms that process sensor data, actuate control functions and learn about the environment in which these systems operate. The trustworthiness and safety of such systems is of paramount importance and has significant impact on the commercial viability of the underlying technology. As a consequence, anomalies in system operation due to computation errors in on-board processors, degradation and failure of embedded sensors, actuators and electro-mechanical subsystems and unforeseen changes in their operation environment need to detected with minimum latency. Such anomalies also need to be mitigated in ways that ensure the safety of such systems under all possible failure scenarios. Many future systems will be selflearning in the field. It is necessary to ensure that such learning does not compromise the safety of all human personnel involved in the operation of such systems. To enable safe operation of such systems, the underlying hardware needs to be tuned in the field to maximize performance, reliability and error-resilience while minimizing power consumption. To enable such dynamic adaptation, device operating conditions and the onset of soft errors are sensed using post-manufacture and real-time checking mechanisms. These mechanisms rely on the use of built-in sensors and/or low-overhead function encoding techniques to detect anomalies in system functions. A key capability is that of being able to deduce multiple performance parameters of the system-under-test using compact optimized stimulus using learning algorithms. The sensors and function encodings assess the loss in performance of the relevant systems due to workload uncertainties, manufacturing process imperfections, soft errors and hardware malfunction and failures induced by electromechanical degradation. These are then mitigated through the use of algorithm-through-circuit level compensation techniques based on pre-deployment simulation and post-deployment self-learning. These techniques continuously trade off performance vs. power of the individual software and hardware modules in such a way as to deliver the end-to-end desired application level Quality of Service (QoS), while minimizing energy/power consumption and maximizing reliability and safety. Applications to signal processing, and control algorithms for example autonomous systems will be discussed."
660,unknown,10.1016/j.scs.2020.102582,scopus,sciencedirect,smart cities,'smart cities' AND 'machine learning' AND ('real-world' AND 'deploy'),https://api.elsevier.com/content/abstract/scopus_id/85096158767,2021-01-01,towards the sustainable development of smart cities through mass video surveillance: a response to the covid-19 pandemic,"Sustainable smart city initiatives around the world have recently had great impact on the lives of citizens and brought significant changes to society. More precisely, data-driven smart applications that efficiently manage sparse resources are offering a futuristic vision of smart, efficient, and secure city operations. However, the ongoing COVID-19 pandemic has revealed the limitations of existing smart city deployment; hence; the development of systems and architectures capable of providing fast and effective mechanisms to limit further spread of the virus has become paramount. An active surveillance system capable of monitoring and enforcing social distancing between people can effectively slow the spread of this deadly virus. In this paper, we propose a data-driven deep learning-based framework for the sustainable development of a smart city, offering a timely response to combat the COVID-19 pandemic through mass video surveillance. To implementing social distancing monitoring, we used three deep learning-based real-time object detection models for the detection of people in videos captured with a monocular camera. We validated the performance of our system using a real-world video surveillance dataset for effective deployment."
661,excluded,10.1007/s12652-021-03232-7,Springer,springer,health,'health' AND 'machine learning' AND ('real-world' AND 'deploy'),http://dx.doi.org/10.1007/s12652-021-03232-7,2021-04-09 00:00:00,healthsaver: a neural network based hospital recommendation system framework on flask webapplication with realtime database and rfid based attendance system,"Good healthcare systems are crucial to the socio-economic growth of a country. In the current scenario, with a rapidly growing population, it is not always possible to find good hospitals nearby which has beds and doctors available, especially in times of emergencies. However, no reliable hospital recommendation system exists which compute these parameters and suggests hospitals. The proposed model utilizes neural networks to recommend hospitals based on hospital ratings, doctors available and distance to ensure that in time of emergencies, because crucial time is not wasted and the people can go to the best suited hospital. A comparative study among different neural network models were carried out in terms of accuracy and computing time. The most efficient model was chosen to be implemented in the final recommendation system. The dataset used to train the neural network models was the Hospital rating dataset provided by the Centers for Medicare and Medicaid services available on Kaggle. A real-time database has been made in Google Firebase, which contains information about hospitals like location, ratings, doctors and beds available. An attendance system for doctors has also been designed using Radio Frequency Identification (RFID) cards and NodeMCU. A web-application has also been designed where the user can obtain the name, address and phone number of the recommended hospital or hospitals."
