doi,type,publication,publisher,publication_date,database,title,url,abstract,domain,id
10.1016/j.trc.2021.103499,filtered,Transportation Research Part C: Emerging Technologies,scopus,2022-01-01,sciencedirect,do autonomous vehicles drive like humans? a turing approach and an application to sae automation level 2 cars,https://api.elsevier.com/content/abstract/scopus_id/85120490088,"Fully automated vehicles (AVs) are set to become a reality in future decades and changes are to be expected in user perceptions and behavior. While AV acceptability has been widely studied, changes in human drivers’ behavior and in passengers’ reactions have received less attention. It is not yet possible to ascertain the risk of driver behavioral changes such as overreaction, and the corresponding safety problems, in mixed traffic with partially AVs. Nor has there been proper investigation of the potential unease of car occupants trained for human control, when exposed to automatic maneuvers. The conjecture proposed in this paper is that automation Level 2 vehicles do not induce potentially adverse effects in traditional vehicle drivers’ behavior or in occupants’ reactions, provided that they are indistinguishable from human-driven vehicles. To this end, the paper proposes a Turing approach to test the “humanity” of automation Level 2 vehicles. The proposed test was applied to the results of an experimental campaign carried out in Italy: 546 car passengers were interviewed on board Level 2 cars in which they could not see the driver. They were asked whether a specific driving action (braking, accelerating, lane keeping) had been performed by the human driver or by the automatic on-board software under different traffic conditions (congestion and speed). Estimation results show that in most cases the interviewees were unable to distinguish the Artificial Intelligence (AI) from the human driver by observing random responses with a 95% significance level (proportion of success statistically equal to 50%). However, in the case of moderate braking and lane keeping at >100 km/h and in high traffic congestion, respondents recognized AI control from the human driver above pure chance, with 62–69% correct response rates. These findings, if confirmed in other case studies, could significantly impact on AVs acceptability, also contributing to their design as well as to long-debated ethical questions. AI driving software could be designed and tested for “humanity”, as long as safety is guaranteed, and autonomous cars could be allowed to circulate as long as they cannot be distinguished from human-driven vehicles in recurrent driving conditions.",autonomous vehicle,1
10.1007/978-3-030-64573-1_164,filtered,Artificial Intelligence in Medicine,Springer,2022-01-01 00:00:00,springer,aim in endoscopy procedures,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-64573-1_164,"Artificial intelligence (AI) is revolutionizing the way medicine is practiced. In this context, the application of AI algorithms in endoscopy is gaining increasing attention so that modern endoscopy is moving towards more and more assisted/automatic solutions. Several approaches have been carried out in order to improve accuracy in diagnosis and surgical procedures. In this chapter, a general overview of the main contributions in the field is surveyed. Four main categories of applications were identified, namely, (i) detection and diagnosis during endoscopic procedure, (ii) informative frame selection, (iii) mosaicking and surface reconstruction, (iv) augmented reality systems for intraoperative assistance and surgeon training. Discussions on future research directions and implementation in clinical practice are provided.",health,2
10.1109/ccnc49033.2022.9700515,filtered,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,cave-vr and unity game engine for visualizing city scale 3d meshes,https://ieeexplore.ieee.org/document/9700515/,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",smart cities,3
10.1109/jiot.2021.3097768,filtered,IEEE Internet of Things Journal,IEEE,2022-03-01 00:00:00,ieeexplore,user-aware and flexible proactive caching using lstm and ensemble learning in iot-mec networks,https://ieeexplore.ieee.org/document/9488291/,"To meet the stringent demands of emerging Internet-of-Things (IoT) applications, such as smart home, smart city, and virtual reality in 5G/6G IoT networks, edge content caching for mobile/multiaccess edge computing (MEC) has been identified as a promising approach to improve the quality of services in terms of latency and energy consumption. However, the limitations of cache capacity make it difficult to develop an effective common caching framework that satisfies diverse user preferences. In this article, we propose a new content caching strategy that maximizes the cache hit ratio through flexible prediction in dynamically changing network and user environments. It is based on a hierarchical deep learning architecture: long short-term memory (LSTM)-based local learning and ensemble-based meta-learning. First, as a local learning model, we employ an LSTM method with seasonal-trend decomposition using loess (STL)-based preprocessing. It identifies the attributes for demand prediction on the contents in various demographic user groups. Second, as a metalearning model, we employ a regression-based ensemble learning method, which uses an online convex optimization framework and exhibits sublinear “regret” performance. It orchestrates the obtained multiple demographic user preferences into a unified caching strategy in real time. Extensive experiments were conducted on the popular MovieLens data sets. It was shown that the proposed control provides up to a 30% higher cache hit ratio than conventional representative algorithms and a near-optimal cache hit ratio within approximately 9% of the optimal caching scheme with perfect prior knowledge of content popularity. The proposed learning and caching control can be implemented as a core function of the 5G/6G standard’s network data analytic function (NWDAF) module.",smart cities,4
10.1109/jsac.2021.3118405,filtered,IEEE Journal on Selected Areas in Communications,IEEE,2022-02-01 00:00:00,ieeexplore,"learning-based prediction, rendering and transmission for interactive virtual reality in ris-assisted terahertz networks",https://ieeexplore.ieee.org/document/9565222/,"The quality of experience (QoE) requirements of wireless virtual reality (VR) can only be satisfied with high data rate, high reliability, and low VR interaction latency. This high data rate over short transmission distances may be achieved via the abundant bandwidth in the terahertz (THz) band. However, THz waves experience severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with programmable reflecting elements. Meanwhile, the low VR interaction latency can be achieved with the mobile edge computing (MEC) network architecture due to its computation capabilities. Motivated by these considerations, in this paper, we propose an MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by taking into account the uplink viewpoint prediction and position transmission, the MEC rendering, and the downlink transmission. We propose two methods, which are referred to as centralized online gated recurrent unit (GRU) and distributed federated averaging (FedAvg), to predict the viewpoints of the VR users. In the uplink, an algorithm that integrates online long-short term memory (LSTM) and convolutional neural networks (CNN) is deployed to predict the locations and the line-of-sight and non-line-of-sight statuses of the VR users over time. In the downlink, we develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS under latency constraints. Simulation results show that our proposed learning architecture achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.",multimedia,5
10.1109/tmrb.2021.3129113,filtered,IEEE Transactions on Medical Robotics and Bionics,IEEE,2022-02-01 00:00:00,ieeexplore,learning a generic olfactory search strategy from silk moths by deep inverse reinforcement learning,https://ieeexplore.ieee.org/document/9619462/,"Despite their simple nervous systems, insects efficiently search for and find sources of odorants. Hence, it is necessary to model and implement such behavior in artificial agents (robots), to enable them to detect dangerous substances such as drugs, gas leaks, and explosives. Previous studies have approached behavioral modeling with either statistical or machine-learning methods. In this study, we determined the behavior trajectories of male silk moths using a virtual reality (VR) system. We then modeled these trajectories as a Markov decision process (MDP) and employed inverse reinforcement learning (IRL) to learn their reward function. Furthermore, we estimated the optimal policy from the learned reward function. We then conducted olfactory search simulations and determined that the IRL-based policy could locate odor sources with a high success rate. This was also investigated under environmental conditions different from those faced by real moths on the VR system. The obtained results indicate that IRL can generically represent olfactory search strategies that are adaptable to various environments.",multimedia,6
10.1007/978-3-030-85365-5_17,filtered,"Advances in Deep Learning, Artificial Intelligence and Robotics",Springer,2022-01-01 00:00:00,springer,robust model for rural education using deep learning and robotics,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-85365-5_17,"Rural Education is important for the overall development of villages. National Achievement Survey (NAS) has surveyed and reported in many States of India consistent decline in the learning levels of students in mathematics, language and science from class III to class VIII studying in the government school system. Smart Villages is only possible if the literacy level and infrastructure improves considerably. This paper aims to perform a reality check of the situation by comparing different rural areas of various countries including India and study of related work done. The paper proposes a Robust Model for Rural Education by developing an intelligent humanoid robot using the Deep Learning approach, Human recognition, Object Recognition and Speech Recognition. The data set consists of Primary and Secondary Student data of around 10,000 Students (5 years) from 5 villages. The Proposed Model would be compared with existing models on the parameters of Learnability, Decision making, Flexibility and Cost-effectiveness. The implementation of this Model will help in decreasing the drop out rate, evaluate Students and give them a Learning platform based on their characteristics, increase adaptive and self paced learning. This Model can also be executed for Rural Adult Education and Skill building so that the Smart Village concept can become a reality.",multimedia,7
10.1007/978-3-030-93564-1_38,filtered,7th International Conference on Advancements of Medicine and Health Care through Technology,Springer,2022-01-01 00:00:00,springer,programing a robotic ambulance with virtual reality,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-93564-1_38,"By applying artificial intelligence and virtual reality, this study presents results and challenges for robotic ambulances. Programming vehicle dynamics and testing protocol is intended to support task of developing an autonomous ambulance and engineering efforts. Testing parameters are controlled with automated driver. Using software will diminish human input in driving. Actual values of robotic ambulance in testing are displacement, speed, and acceleration. Ambulance’s velocity in testing on a virtual track with corners and straight lines is an important kinematic parameter that influences safety of transport and some program sections. Accelerations are also important to be programmed. Objective of this paper is to highlight sequences of programming robotic ambulance using virtual reality and artificial intelligence. Results are consisting in testing scenarios, ambulance automated driving program on virtual track, refined program code, solutions for challenges.",multimedia,8
http://arxiv.org/abs/2201.07312v1,filtered,arxiv,arxiv,2022-01-18 00:00:00,arxiv,model-driven cluster resource management for ai workloads in edge clouds,http://arxiv.org/abs/2201.07312v1,"Since emerging edge applications such as Internet of Things (IoT) analytics
and augmented reality have tight latency constraints, hardware AI accelerators
have been recently proposed to speed up deep neural network (DNN) inference run
by these applications. Resource-constrained edge servers and accelerators tend
to be multiplexed across multiple IoT applications, introducing the potential
for performance interference between latency-sensitive workloads. In this
paper, we design analytic models to capture the performance of DNN inference
workloads on shared edge accelerators, such as GPU and edgeTPU, under different
multiplexing and concurrency behaviors. After validating our models using
extensive experiments, we use them to design various cluster resource
management algorithms to intelligently manage multiple applications on edge
accelerators while respecting their latency constraints. We implement a
prototype of our system in Kubernetes and show that our system can host 2.3X
more DNN applications in heterogeneous multi-tenant edge clusters with no
latency violations when compared to traditional knapsack hosting algorithms.",multimedia,9
http://arxiv.org/abs/2201.02279v1,filtered,arxiv,arxiv,2022-01-06 00:00:00,arxiv,de-rendering 3d objects in the wild,http://arxiv.org/abs/2201.02279v1,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.",multimedia,10
10.1016/j.vrih.2022.01.004,filtered,Virtual Reality and Intelligent Hardware,scopus,2022-02-01,sciencedirect,virtual-reality-based digital twin of office spaces with social distance measurement feature,https://api.elsevier.com/content/abstract/scopus_id/85124517698,"Background
                  Social distancing is an effective way to reduce the spread of the SARS-CoV-2 virus. Many students and researchers have already attempted to use computer vision technology to automatically detect human beings in the field of view of a camera and help enforce social distancing. However, because of the present lockdown measures in several countries, the validation of computer vision systems using large-scale datasets is a challenge.
               
                  Methods
                  In this paper, a new method is proposed for generating customized datasets and validating deep-learning-based computer vision models using virtual reality (VR) technology. Using VR, we modeled a digital twin (DT) of an existing office space and used it to create a dataset of individuals in different postures, dresses, and locations. To test the proposed solution, we implemented a convolutional neural network (CNN) model for detecting people in a limited-sized dataset of real humans and a simulated dataset of humanoid figures.
               
                  Results
                  We detected the number of persons in both the real and synthetic datasets with more than 90% accuracy, and the actual and measured distances were significantly correlated (r=0.99). Finally, we used intermittent-layer- and heatmap-based data visualization techniques to explain the failure modes of a CNN.
               
                  Conclusions
                  A new application of DTs is proposed to enhance workplace safety by measuring the social distance between individuals. The use of our proposed pipeline along with a DT of the shared space for visualizing both environmental and human behavior aspects preserves the privacy of individuals and improves the latency of such monitoring systems because only the extracted information is streamed.",multimedia,11
http://arxiv.org/abs/2201.01369v1,filtered,arxiv,arxiv,2022-01-04 00:00:00,arxiv,"using simulation optimization to improve zero-shot policy transfer of
  quadrotors",http://arxiv.org/abs/2201.01369v1,"In this work, we show that it is possible to train low-level control policies
with reinforcement learning entirely in simulation and, then, deploy them on a
quadrotor robot without using real-world data to fine-tune. To render zero-shot
policy transfers feasible, we apply simulation optimization to narrow the
reality gap. Our neural network-based policies use only onboard sensor data and
run entirely on the embedded drone hardware. In extensive real-world
experiments, we compare three different control structures ranging from
low-level pulse-width-modulated motor commands to high-level attitude control
based on nested proportional-integral-derivative controllers. Our experiments
show that low-level controllers trained with reinforcement learning require a
more accurate simulation than higher-level control policies.",robotics,12
10.1109/iccece54139.2022.9712792,filtered,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),IEEE,2022-01-16 00:00:00,ieeexplore,design of deep learning based autonomous driving control algorithm,https://ieeexplore.ieee.org/document/9712792/,"In recent years, with the continuous development of the field of artificial intelligence, autonomous driving technology has gained widespread attention. In order to meet the purpose of changing driving behavior and completing driving tasks in real time without human intervention. In this paper, we study the design and implementation process of end-to-end autonomous driving algorithms based on computer vision and deep learning, and explain the elements of algorithm design from a theoretical perspective. The method of continuous steering angle prediction for autonomous driving based on convolutional neural network is proposed, as well as the method of network pre-training and overfitting prevention to improve the training effect and generalization ability. The difference with the traditional end-to-end control methods is that the traditional methods study the problem abstractly as a classification problem, describing the motion in terms of direction with a coarser granularity. The method proposed in this paper treats it as a regression problem, describing the motion in terms of steering angles, which provides a more accurate description of the motion and is more adaptive.",autonomous vehicle,13
10.1109/wacv51458.2022.00206,filtered,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,plugging self-supervised monocular depth into unsupervised domain adaptation for semantic segmentation,https://ieeexplore.ieee.org/document/9707096/,"Although recent semantic segmentation methods have made remarkable progress, they still rely on large amounts of annotated training data, which are often infeasible to collect in the autonomous driving scenario. Previous works usually tackle this issue with Unsupervised Domain Adaptation (UDA), which entails training a network on synthetic images and applying the model to real ones while minimizing the discrepancy between the two domains. Yet, these techniques do not consider additional information that may be obtained from other tasks. Differently, we propose to exploit self-supervised monocular depth estimation to improve UDA for semantic segmentation. On one hand, we deploy depth to realize a plug-in component which can inject complementary geometric cues into any existing UDA method. We further rely on depth to generate a large and varied set of samples to Self-Train the final model. Our whole proposal allows for achieving state-of-the-art performance (58.8 mIoU) in the GTA5 → CS benchmark. Code is available at https://github.com/CVLAB-Unibo/d4-dbst.",autonomous vehicle,14
10.1007/s11042-021-11437-3,filtered,Multimedia Tools and Applications,Springer,2022-01-01 00:00:00,springer,deep reinforcement learning based control for autonomous vehicles in carla,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s11042-021-11437-3,"Nowadays, Artificial Intelligence (AI) is growing by leaps and bounds in almost all fields of technology, and Autonomous Vehicles (AV) research is one more of them. This paper proposes the using of algorithms based on Deep Learning (DL) in the control layer of an autonomous vehicle. More specifically, Deep Reinforcement Learning (DRL) algorithms such as Deep Q-Network (DQN) and Deep Deterministic Policy Gradient (DDPG) are implemented in order to compare results between them. The aim of this work is to obtain a trained model, applying a DRL algorithm, able of sending control commands to the vehicle to navigate properly and efficiently following a determined route. In addition, for each of the algorithms, several agents are presented as a solution, so that each of these agents uses different data sources to achieve the vehicle control commands. For this purpose, an open-source simulator such as CARLA is used, providing to the system with the ability to perform a multitude of tests without any risk into an hyper-realistic urban simulation environment, something that is unthinkable in the real world. The results obtained show that both DQN and DDPG reach the goal, but DDPG obtains a better performance. DDPG perfoms trajectories very similar to classic controller as LQR. In both cases RMSE is lower than 0.1m following trajectories with a range 180-700m. To conclude, some conclusions and future works are commented.",autonomous vehicle,15
http://arxiv.org/abs/2201.05797v1,filtered,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"finding label and model errors in perception data with learned
  observation assertions",http://arxiv.org/abs/2201.05797v1,"ML is being deployed in complex, real-world scenarios where errors have
impactful consequences. In these systems, thorough testing of the ML pipelines
is critical. A key component in ML deployment pipelines is the curation of
labeled training data. Common practice in the ML literature assumes that labels
are the ground truth. However, in our experience in a large autonomous vehicle
development center, we have found that vendors can often provide erroneous
labels, which can lead to downstream safety risks in trained models.
  To address these issues, we propose a new abstraction, learned observation
assertions, and implement it in a system called Fixy. Fixy leverages existing
organizational resources, such as existing (possibly noisy) labeled datasets or
previously trained ML models, to learn a probabilistic model for finding errors
in human- or model-generated labels. Given user-provided features and these
existing resources, Fixy learns feature distributions that specify likely and
unlikely values (e.g., that a speed of 30mph is likely but 300mph is unlikely).
It then uses these feature distributions to score labels for potential errors.
We show that FIxy can automatically rank potential errors in real datasets with
up to 2$\times$ higher precision compared to recent work on model assertions
and standard techniques such as uncertainty sampling.",autonomous vehicle,16
10.1016/j.engappai.2021.104514,filtered,Engineering Applications of Artificial Intelligence,scopus,2022-01-01,sciencedirect,instance-based defense against adversarial attacks in deep reinforcement learning,https://api.elsevier.com/content/abstract/scopus_id/85118104144,"Deep Reinforcement Learning systems are now a hot topic in Machine Learning for their effectiveness in many complex tasks, but their application in safety-critical domains (e.g., robot control or self-autonomous driving) remains dangerous without mechanism to detect and prevent risk situations. In Deep RL, such risk is mostly in the form of adversarial attacks, which introduce small perturbations to sensor inputs with the aim of changing the network-based decisions and thus cause catastrophic situations. In the light of these dangers, a promising line of research is that of providing these Deep RL algorithms with suitable defenses, especially when deploying in real environments. This paper suggests that this line of research could be greatly improved by the concepts from the existing research field of Safe Reinforcement Learning, which has been postulated as a family of RL algorithms capable of providing defenses against many forms of risks. However, the connections between Safe RL and the design of defenses against adversarial attacks in Deep RL remain largely unexplored. This paper seeks to explore precisely some of these connections. In particular, this paper proposes to reuse some of the concepts from existing Safe RL algorithms to create a novel and effective instance-based defense for the deployment stage of Deep RL policies. The proposed algorithm uses a risk function based on how far a state is from the state space known by the agent, that allows identifying and preventing adversarial situations. The success of the proposed defense has been evaluated in 4 Atari games.",autonomous vehicle,17
10.1109/jiot.2021.3096637,filtered,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,an integrated framework for health state monitoring in a smart factory employing iot and big data techniques,https://ieeexplore.ieee.org/document/9481251/,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",health,18
10.1109/tie.2021.3065616,filtered,IEEE Transactions on Industrial Electronics,IEEE,2022-03-01 00:00:00,ieeexplore,health management of dry-type transformer based on broad learning system,https://ieeexplore.ieee.org/document/9380956/,"This article presents a novel health management method of the dry-type transformer to diagnose the early unhealthy behavior and evaluate the transformer's health condition by health score. The health condition diagnosis implemented by a proposed dynamic-weighted-feed-back broad learning system (BLS) (DW-FB-BLS) method, which helps to determine the BLS network structure effectively, and adjusts the weight of features in the online application to avoid reduction of accuracy caused by concept drift. Then, a rational score rule is set to evaluate the health condition of the dry-type transformer by health score, which allows intuitive presentation and preservation of transformer's health condition over a long period. Finally, the effectiveness and validity of the proposed method are verified based on the real field data of dry-type transformer. Satisfactory results for unhealthy behavior diagnosis and health evaluation are obtained, it shows that health management of this article can reflect the real health condition of dry-type transformer appropriately.",health,19
10.1109/access.2022.3141913,filtered,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,decentralized federated learning for healthcare networks: a case study on tumor segmentation,https://ieeexplore.ieee.org/document/9676574/,"Smart healthcare relies on artificial intelligence (AI) functions for learning and analysis of patient data. Since large and diverse datasets for training of Machine Learning (ML) models can rarely be found in individual medical centers, classical centralized AI requires moving privacy-sensitive data from medical institutions to data centers that process the fused information. Training on data centers thus requires higher communication resource/energy demands while violating privacy. This is considered today as a significant bottleneck in pursuing scientific collaboration across trans-national clinical medical research centers. Recently, federated learning (FL) has emerged as a distributed AI approach that enables the cooperative training of ML models, without the need of sharing patient data. This paper dives into the analysis of different FL methods and proposes a real-time distributed networking framework based on the Message Queuing Telemetry Transport (MQTT) protocol. In particular, we design a number of solutions for ML over networks, based on FL tools relying on a parameter server (PS) and fully decentralized paradigms driven by consensus methods. The proposed approach is validated in the context of brain tumor segmentation, using a modified version of the popular U-NET model with representative clinical datasets obtained from the daily clinical workflow. The FL process is implemented on multiple physically separated machines located in different countries and communicating over the Internet. The real-time test-bed is used to obtain measurements of training accuracy vs. latency trade-offs, and to highlight key operational conditions that affect the performance in real deployments.",health,20
10.1007/978-3-030-80928-7_10,filtered,Machine Learning for Critical Internet of Medical Things,Springer,2022-01-01 00:00:00,springer,aiiomt: iomt-based system-enabled artificial intelligence for enhanced smart healthcare systems,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-80928-7_10,"The healthcare system has been on the frontline in recent years, researchers have tried to find solutions to different illnesses and sickness by applying various modern methods. But the major difference among them is that in recent years, other powerful new tools have emerged, which could be used as an instrument in the healthcare system and keeping it within reasonable limits. One of those technological tools is the Internet of Medical Things (IoMT) and Artificial intelligence (AI). Recently, AI enabled with IoMT-based systems is causing a paradigm shift in the healthcare zone, and the applicability might yield profit especially in diagnosis, prediction, and treatment of different diseases outbreak. The application of AI enabled with IoT-based systems in the healthcare system can be expediting the diagnoses and monitoring of disease and minimizes the burden of medical processes. Therefore, this chapter reviews the applicability of AiIoMT-based system in healthcare systems and the research challenges in deployment of AiIoMT system. The chapter also proposed an AiIoMT-based framework for diagnosis and monitoring of patients in real time. The model was tested using cytology image dataset and evaluated based on accuracy, sensitivity, specificity, F-score, and precision. The findings show a greater diagnosis accuracy of 99.5%, which shows that the AI model is a promising algorithm for the diagnosis of diseases in an IoMT-based system. The diagnosis, prediction, treatment, screening, and medication in the healthcare system have significantly improved with the continuing expansion in the methods having seriously reduced human intervention in medical practice.",health,21
http://arxiv.org/abs/2202.01176v1,filtered,arxiv,arxiv,2022-02-02 00:00:00,arxiv,epidemic dreams: dreaming about health during the covid-19 pandemic,http://arxiv.org/abs/2202.01176v1,"The continuity hypothesis of dreams suggests that the content of dreams is
continuous with the dreamer's waking experiences. Given the unprecedented
nature of the experiences during COVID-19, we studied the continuity hypothesis
in the context of the pandemic. We implemented a deep-learning algorithm that
can extract mentions of medical conditions from text and applied it to two
datasets collected during the pandemic: 2,888 dream reports (dreaming life
experiences), and 57M tweets mentioning the pandemic (waking life experiences).
The health expressions common to both sets were typical COVID-19 symptoms
(e.g., cough, fever, and anxiety), suggesting that dreams reflected people's
real-world experiences. The health expressions that distinguished the two sets
reflected differences in thought processes: expressions in waking life
reflected a linear and logical thought process and, as such, described
realistic symptoms or related disorders (e.g., nasal pain, SARS, H1N1); those
in dreaming life reflected a thought process closer to the visual and emotional
spheres and, as such, described either conditions unrelated to the virus (e.g.,
maggots, deformities, snakebites), or conditions of surreal nature (e.g., teeth
falling out, body crumbling into sand). Our results confirm that dream reports
represent an understudied yet valuable source of people's health experiences in
the real world.",health,22
http://arxiv.org/abs/2201.05115v1,filtered,arxiv,arxiv,2022-01-13 00:00:00,arxiv,functional anomaly detection: a benchmark study,http://arxiv.org/abs/2201.05115v1,"The increasing automation in many areas of the Industry expressly demands to
design efficient machine-learning solutions for the detection of abnormal
events. With the ubiquitous deployment of sensors monitoring nearly
continuously the health of complex infrastructures, anomaly detection can now
rely on measurements sampled at a very high frequency, providing a very rich
representation of the phenomenon under surveillance. In order to exploit fully
the information thus collected, the observations cannot be treated as
multivariate data anymore and a functional analysis approach is required. It is
the purpose of this paper to investigate the performance of recent techniques
for anomaly detection in the functional setup on real datasets. After an
overview of the state-of-the-art and a visual-descriptive study, a variety of
anomaly detection methods are compared. While taxonomies of abnormalities (e.g.
shape, location) in the functional setup are documented in the literature,
assigning a specific type to the identified anomalies appears to be a
challenging task. Thus, strengths and weaknesses of the existing approaches are
benchmarked in view of these highlighted types in a simulation study. Anomaly
detection methods are next evaluated on two datasets, related to the monitoring
of helicopters in flight and to the spectrometry of construction materials
namely. The benchmark analysis is concluded by recommendation guidance for
practitioners.",health,23
http://arxiv.org/abs/2201.01943v1,filtered,arxiv,arxiv,2022-01-06 00:00:00,arxiv,"machine learning: algorithms, models, and applications",http://arxiv.org/abs/2201.01943v1,"Recent times are witnessing rapid development in machine learning algorithm
systems, especially in reinforcement learning, natural language processing,
computer and robot vision, image processing, speech, and emotional processing
and understanding. In tune with the increasing importance and relevance of
machine learning models, algorithms, and their applications, and with the
emergence of more innovative uses cases of deep learning and artificial
intelligence, the current volume presents a few innovative research works and
their applications in real world, such as stock trading, medical and healthcare
systems, and software automation. The chapters in the book illustrate how
machine learning and deep learning algorithms and models are designed,
optimized, and deployed. The volume will be useful for advanced graduate and
doctoral students, researchers, faculty members of universities, practicing
data scientists and data engineers, professionals, and consultants working on
the broad areas of machine learning, deep learning, and artificial
intelligence.",health,24
10.1016/j.physc.2021.1354007,filtered,Physica C: Superconductivity and its Applications,scopus,2022-02-15,sciencedirect,optical fibre based quench detection in hts applications using machine learning classifiers,https://api.elsevier.com/content/abstract/scopus_id/85122309535,"A Mach-Zehnder Interferometer (MZI) based optical fibre sensing technique, developed and patented by EPFL, is an efficient and economical way to detect hotspots in High Temperature Superconductor (HTS) applications. Due to the MZI sensitivity being a composite of strain sensitive and temperature sensitive contributions, the MZI gives an instantaneous response to a quench (within 10 ms), because of the quick strain transfer to the optical fibre. However, the MZI output signal can also manifest the environmental noise caused by mechanical vibrations, bubbling in the cryostat and temperature variations, along with the response to the quench. This presents the problems of false alarms and indiscernible response to a quench. Discrete wavelet transform (DWT) has been proven to be a useful tool for feature extraction in different fields requiring signal categorization and hence holds the potential to enable quench recognition in the MZI output. This paper proposes an effective approach of performing DWT based feature extraction on experimental data and subsequently using the extracted features for the MZI response classification using two machine learning based classification techniques: k-nearest neighbours (KNN) and Artificial Neural Network (ANN). For this manuscript, experiments were performed using MZI for quench detection in an HTS tape. Feature extraction was then implemented on these experimental measurements using discrete wavelet coefficients extracted at different decomposition levels from the MZI output; these features were then used to train the KNN and ANN models for identifying quench in the MZI signal. This method could be a valuable supplement to the MZI technique by enabling the development of a real time application that can process the MZI output data as well as eliminate the occurrences of false alarms; thereby facilitating reliable quench detection. With this development, the MZI technique would become an even more attractive solution for the health monitoring of HTS applications.",health,25
10.1016/j.ergon.2021.103234,filtered,International Journal of Industrial Ergonomics,scopus,2022-01-01,sciencedirect,industrial intelligence in the care of workers’ mental health: a review of status and challenges,https://api.elsevier.com/content/abstract/scopus_id/85120173556,"Mental health is a current concern because people worldwide have been committed to disorders that impair lives as a whole, affecting emotional states, behaviors, and body responses. These disorders decrease worker's productivity, impact industries economically, and cause serious psycho-physical conditions. However, technological advances have leveraged the industry to a novel phase where digitalization and automation provide a new reality. Hence, this industrial transformation may contribute to assists human beings in the workplace with a focus on mental health. This article presents a systematic literature review to investigate studies regarding technologies employed in the care of worker's mental health and the industrial role in this scenario. Three general, three focused, and three descriptive questions highlight the academic progress of industrial concern on mental health, implemented systems and cases, and research challenges. As a result, the review discussed 31 studies, extracted from an initial corpus of 25269, ranging from January 2010 to November 2020. The studies approached stress as the most frequent mental issue in the industry and Support Vector Machine (SVM) as the most used machine learning algorithm, where biomarkers presented the primary data extractors to deal with this theme. Moreover, information fusion methods improved the accuracy of specific cases. However, a growing interest in mental health care has emerged only in recent years, and several challenges require efforts before applying systems in real industrial environments.",health,26
10.1109/tcyb.2020.2964011,filtered,IEEE Transactions on Cybernetics,IEEE,2022-01-01 00:00:00,ieeexplore,hierarchical granular computing-based model and its reinforcement structural learning for construction of long-term prediction intervals,https://ieeexplore.ieee.org/document/8972350/,"As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.",industry,27
10.1109/access.2021.3138990,filtered,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"microgrid digital twins: concepts, applications, and future trends",https://ieeexplore.ieee.org/document/9663369/,"Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the <italic>digital twinning</italic> concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs’ operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed.",industry,28
10.1109/tii.2021.3081417,filtered,IEEE Transactions on Industrial Informatics,IEEE,2022-03-01 00:00:00,ieeexplore,early classification of industrial alarm floods based on semisupervised learning,https://ieeexplore.ieee.org/document/9435070/,"Early classification of ongoing alarm floods in industrial monitoring systems is crucial to provide a safe and efficient operation. It can provide online decision support for plant operators to take timely action, without waiting for the end of an alarm flood. In this article, a data-driven approach is proposed to address the early classification problem with unlabeled historical data. To prioritize earlier activated alarms and take advantage of the triggering time information of alarms, a vector representation called exponentially attenuated component (EAC) is used to represent alarm floods. This makes alarm sequences fit for different powerful machine learning algorithms, which can be easily implemented online with acceptable computational complexities. A method based on the time information of unlabeled historical alarm floods is formulated to determine the attenuation coefficient for EAC representation. With the Gaussian mixture model, an efficient semisupervised approach is proposed to provide an early classification of alarm floods using unlabeled historical data. It includes two phases: offline clustering and online classification, where the clustering step is automated in terms of choosing the optimal number of clusters by applying an efficient cluster validity index. The efficiency of the proposed method is validated by the Tennessee Eastman process benchmark and a real industrial dataset.",industry,29
10.1109/tii.2021.3124848,filtered,IEEE Transactions on Industrial Informatics,IEEE,2022-06-01 00:00:00,ieeexplore,qos and privacy-aware routing for 5g-enabled industrial internet of things: a federated reinforcement learning approach,https://ieeexplore.ieee.org/document/9601174/,"The development and maturity of the fifth-generation (5G) wireless communication technology provides the industrial Internet of Things (IIoT) with ultra-reliable and low-latency communications and massive machine-type communications, and forms a novel IIoT architecture, 5G-IIoT. However, massive data transfer between interconnecting industrial devices also brings new challenges for the 5G-IIoT routing process in terms of latency, load balancing, and data privacy, which affect the development of 5G-IIoT applications. Moreover, the existing research works on IIoT routing mostly focus on the latency and the reliability of the routing, disregarding the privacy security in the routing process. To solve these problems, in this article, we propose a quality of service (QoS) and data privacy-aware routing protocol, named QoSPR, for 5G-IIoT. Specifically, we improve the community detection algorithm info-map to divide the routing area into optimal subdomains, based on which the deep reinforcement learning algorithm is applied to build the gateway deployment model for latency reduction and load-balancing improvement. To eliminate areal differences, while considering the privacy preservation of the routing data, the federated reinforcement learning is applied to obtain the universal gateway deployment model. Then, based on the gateway deployment, the QoS and data privacy-aware routing is accomplished by establishing communications along the load-balancing routes of the minimum latencies. The validation experiment is conducted on real datasets. The experiment results show that as a data privacy-aware routing protocol, the QoSPR can significantly reduce both average latency and maximum latency, while maintaining excellent load balancing in 5G-IIoT.",industry,30
10.1109/access.2022.3149050,filtered,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,design and implementation of traffic generation model and spectrum requirement calculator for private 5g network,https://ieeexplore.ieee.org/document/9703352/,"This paper proposes a neural 5G traffic generation model and a methodology for calculating the spectrum requirements of private 5G networks to provide various industrial communication services. To accurately calculate the spectral requirements, it is necessary to analyze the actual data volume and traffic type of industrial cases. However, because there is currently no suitable traffic model to test loads in private 5G networks, we have developed a generative adversarial network (GAN)-based traffic generator that can generate realistic traffic by learning actual traffic traces collected by mobile network operators. In addition, in the case of industrial applications, probability-based traffic models were used in parallel as there were not enough real data to be learned. The proposed 5G traffic generation model is combined with the proposed 5G spectrum calculation methodology, enabling more accurate spectrum requirements calculation through traffic simulation similar to a real-life environment. In this paper, the spectrum requirements are calculated differently according to two types of duplexing, namely frequency division duplexing (FDD) and time division duplexing (TDD). As a guide for companies aiming to provide advanced wireless connectivity for a wide variety of vertical industries using 5G networks, eight use cases defined in the 5G Alliance for Connected Industries and Automation (ACIA) white paper were simulated. The spectrum requirements were calculated under various simulation conditions considering varying traffic loads, deployment scenarios, and duplexing types. Various simulation results confirmed that a bandwidth of at least 22.0 MHz to a maximum of 397.8 MHz is required depending on the deployment scenario.",industry,31
10.1109/tmech.2021.3065522,filtered,IEEE/ASME Transactions on Mechatronics,IEEE,2022-02-01 00:00:00,ieeexplore,federated transfer learning for intelligent fault diagnostics using deep adversarial networks with data privacy,https://ieeexplore.ieee.org/document/9376674/,"Intelligent data-driven machinery fault diagnosis methods have been popularly developed in the past years. While fairly high diagnosis accuracies have been obtained, large amounts of labeled training data are mostly required, which are difficult to collect in practice. The promising collaborative model training solution with multiple users poses high demands on data privacy due to conflict of interests. Furthermore, in the real industries, the data from different users can be usually collected from different machine operating conditions. The domain shift phenomenon and data privacy concern make the joint model training scheme quite challenging. To address this issue, a federated transfer learning method for fault diagnosis is proposed in this article. Different models can be used by different users to enhance data privacy. A federal initialization stage is introduced to keep similar data structures in distributed feature extractions, and a federated communication stage is further implemented using deep adversarial learning. A prediction consistency scheme is also adopted to increase model robustness. Experiments on two real-world datasets suggest the proposed federated transfer learning method is promising for real industrial applications.",industry,32
10.1007/978-3-030-42462-6_123,filtered,The Palgrave Handbook of Climate Resilient Societies,Springer,2021-01-01 00:00:00,springer,water 4.0: enhancing climate resilience,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-42462-6_123,"For this chapter, water 4.0 is defined as the industry 4.0 concept applied to the water sector. As industry 4.0 reflects the fourth industrial revolution , water 4.0 reflects the fourth water revolution . Based on the literature review and case studies, this chapter examines a proposition that water 4.0 will increase not only the sector’s economic effectiveness but also sustainability including climate resilience. Relevant technologies include digital twins , visualization, wireless monitoring sensors, industrial internet of things (IoT/IIoT), cloud computing, and predictive or prescriptive analytics but also blockchain , drones, and cybersecurity. For water 4.0 becoming a reality, water utility companies need not only collect more data but also to have proper analytical tools in place to convert data into information supporting optimal decisions. The current tools should preferably be replaced by machine learning algorithms that are nonlinear, nonstationary, and dynamic and thus aligned closely with the real world. It has been suggested in this chapter that such disruptive technologies be introduced through an ISO 55001-based asset management system (AMS). ISO 19650 series supplements ISO 55001 and contains additional requirements for the AMS development by focusing particularly on asset information. For this purpose, the series provides assistance with big data and digital twins . Two approaches are applicable to the implementation of water 4.0 through AMS: adaptability and more traditional continuous improvement with the former considered in this chapter as preferred but requires a sufficient level of asset management maturity. Therefore, it might be prudent that every organization sets their own water 4.0 -related standards and objectives in their own AMS and considers the preferred level of adaptability . Adaptability is arguably required for water 4.0 with adaptation bringing the greatest value .",industry,33
http://arxiv.org/abs/2202.10075v1,filtered,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"icsml: industrial control systems machine learning inference framework
  natively executing on iec 61131-3 languages",http://arxiv.org/abs/2202.10075v1,"Industrial Control Systems (ICS) have played a catalytic role in enabling the
4th Industrial Revolution. ICS devices like Programmable Logic Controllers
(PLCs), automate, monitor and control critical processes in industrial, energy
and commercial environments. The convergence of traditional Operational
Technology (OT) with Information Technology (IT) has opened a new and unique
threat landscape. This has inspired defense research that focuses heavily on
Machine Learning (ML) based anomaly detection methods that run on external IT
hardware which means an increase in costs and the further expansion of the
threat landscape. To remove this requirement, we introduce the ICS Machine
Learning inference framework (ICSML) which enables the execution of ML models
natively on the PLC. ICSML is implemented in IEC 61131-3 code and works around
the limitations imposed by the domain-specific languages, providing a complete
set of components for the creation of fully fledged ML models in a way similar
to established ML frameworks. We then demonstrate a complete end-to-end
methodology for creating ICS ML models using an external framework for training
and ICSML for the PLC implementation. To evaluate our contributions we run a
series of benchmarks studying memory and performance and compare our solution
to the TFLite inference framework. Finally, to demonstrate the abilities of
ICSML and to verify its non-intrusive nature, we develop and evaluate a case
study of a real defense for process aware attacks against a Multi Stage Flash
(MSF) desalination plant.",industry,34
http://arxiv.org/abs/2201.06735v1,filtered,arxiv,arxiv,2022-01-18 00:00:00,arxiv,ai augmented digital metal component,http://arxiv.org/abs/2201.06735v1,"The aim of this work is to propose a new paradigm that imparts intelligence
to metal parts with the fusion of metal additive manufacturing and artificial
intelligence (AI). Our digital metal part classifies the status with real time
data processing with convolutional neural network (CNN). The training data for
the CNN is collected from a strain gauge embedded in metal parts by laser
powder bed fusion process. We implement this approach using additive
manufacturing, demonstrate a self-cognitive metal part recognizing partial
screw loosening, malfunctioning, and external impacting object. The results
indicate that metal part can recognize subtle change of multiple fixation state
under repetitive compression with 89.1% accuracy with test sets. The proposed
strategy showed promising potential in contributing to the hyper-connectivity
for next generation of digital metal based mechanical systems",industry,35
http://arxiv.org/abs/2201.06616v2,filtered,arxiv,arxiv,2022-01-17 00:00:00,arxiv,improving the quality control of seismic data through active learning,http://arxiv.org/abs/2201.06616v2,"In image denoising problems, the increasing density of available images makes
an exhaustive visual inspection impossible and therefore automated methods
based on machine-learning must be deployed for this purpose. This is
particulary the case in seismic signal processing. Engineers/geophysicists have
to deal with millions of seismic time series. Finding the sub-surface
properties useful for the oil industry may take up to a year and is very costly
in terms of computing/human resources. In particular, the data must go through
different steps of noise attenuation. Each denoise step is then ideally
followed by a quality control (QC) stage performed by means of human expertise.
To learn a quality control classifier in a supervised manner, labeled training
data must be available, but collecting the labels from human experts is
extremely time-consuming. We therefore propose a novel active learning
methodology to sequentially select the most relevant data, which are then given
back to a human expert for labeling. Beyond the application in geophysics, the
technique we promote in this paper, based on estimates of the local error and
its uncertainty, is generic. Its performance is supported by strong empirical
evidence, as illustrated by the numerical experiments presented in this
article, where it is compared to alternative active learning strategies both on
synthetic and real seismic datasets.",industry,36
10.1016/j.compag.2022.106688,filtered,Computers and Electronics in Agriculture,scopus,2022-02-01,sciencedirect,implementation of a decision support system for prediction of the total soluble solids of industrial tomato using machine learning models,https://api.elsevier.com/content/abstract/scopus_id/85122635918,"Tomato is the second most important vegetable in the world, both in terms of production and consumption. Especially for the cultivation of industrial tomato, harvest is conducted when the total soluble solids, a major quality characteristic, are as high as possible. Advancements in technology have made Decision Support Systems simpler and more applicable in an everyday basis. Data Analysis, combined with Machine Learning algorithms are considered the future of sustainable agriculture, allowing farmers to be advised about the best possible decisions for their cultivation. Farmers need to adopt this kind of technology in order to be able to know when the quality of tomatoes is at its peak, in order to gather their product from the field. The implementation of a Decision Support System to predict the total soluble solids was conducted,based on data from previous years, including quality data (pH, Bostwick, L, a/b, Mean Weight, °Brix), the type of hybrid used, weather data and soil data from the fields. Data derived from fields in 6 different regions in the northwestern Peloponnese, Greece over 6 cultivation periods, created a dataset of 33 different inputs. Thirteen different algorithms were put into evaluation in order to find the best one in terms of speed and efficiency. In this research, we developed a Decision Support System using the K-nearest algorithm, which proved to be the best for our dataset. The predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. This DSS which is using real time weather data as an input is expected to be a valuable tool for the farmers.",industry,37
10.1016/j.autcon.2021.104088,filtered,Automation in Construction,scopus,2022-02-01,sciencedirect,vision-based high-precision intelligent monitoring for shield tail clearance,https://api.elsevier.com/content/abstract/scopus_id/85120874971,"Real-time shield tail clearance measurement and monitoring is a key task during shield tunneling construction. The shield tail clearance measurement and monitoring technology development is still in its infancy, the current methods are mainly designed manually based on intuition. In order to fill the gap between the requirement of shield tail clearance measurement and monitoring and the limitations of the current methods, this paper systematically studies the existing mechanisms related to shield tail clearance measurement and monitoring, and develops a high-precision intelligent monitoring system for shield tail clearance. The proposed monitoring system includes four components: 1) two types of shield tail clearance calculation models, 2) the integrated hardware of the monitoring system which is composed of a data acquisition unit, a signal transmission unit and a control unit, 3) the region of interest (ROI) extraction method based on deep neural network, and the image processing algorithms for image enhancement and feature extraction, 4) the custom-developed software built on mature integrated development environment (IDE). After the calculation model of shield tail clearance is established, the system uses monitoring devices equipped with industrial cameras to obtain the on-site image, and then applies image processing technologies along with deep learning approach to extract the key features, which are brought into the model to calculate the values of shield tail clearance, finally displays these values and simulates the current tunneling attitude of the shield machine in real time. The experimental results show that the system proposed in this paper achieves the goal of high precision measuring and real-time monitoring of the shield tail clearance.",industry,38
10.1016/j.apenergy.2021.118127,filtered,Applied Energy,scopus,2022-02-01,sciencedirect,data-driven control of room temperature and bidirectional ev charging using deep reinforcement learning: simulations and experiments,https://api.elsevier.com/content/abstract/scopus_id/85118721393,"The control of modern buildings is a complex multi-loop problem due to the integration of renewable energy generation, storage devices, and electric vehicles (EVs). Additionally, it is a complex multi-criteria problem due to the need to optimize overall energy use while satisfying users’ comfort. Both conventional rule-based (RB) controllers, which are difficult to apply in multi-loop settings, and advanced model-based controllers, which require an accurate building model, cannot fulfil the requirements of the building automation industry to solve this problem optimally at low development and commissioning costs. This work presents a fully data-driven pipeline to obtain an optimal control policy from historical building and weather data, thus avoiding the need for complex physics-based modelling. We demonstrate the potential of this method by jointly controlling a room temperature and an EV to minimize the cost of electricity while retaining the comfort of the occupants. We model the room temperature with a recurrent neural network and use it as a simulation environment to learn a deep reinforcement learning (DRL) control policy. It achieves on average 17% energy savings and 19% better comfort satisfaction than a standard RB room temperature controller. When a bidirectional EV is connected additionally and a two-tariff electricity pricing is applied, it successfully leverages the battery and decreases the overall cost of electricity. Finally, we deployed it on a real building, where it achieved up to 30% energy savings while maintaining similar comfort levels compared to a conventional RB room temperature controller.",industry,39
10.1016/j.ssci.2021.105529,filtered,Safety Science,scopus,2022-02-01,sciencedirect,a novel decision support system for managing predictive maintenance strategies based on machine learning approaches,https://api.elsevier.com/content/abstract/scopus_id/85118705579,"Nowadays, the industrial environment is characterised by growing competitiveness, short response times, cost reduction and reliability of production to meet customer needs. Thus, the new industrial paradigm of Industry 4.0 has gained interest worldwide, leading many manufacturers to a significant digital transformation. Digital technologies have enabled a novel approach to decision-making processes based on data-driven strategies, where knowledge extraction relies on the analysis of a large amount of data from sensor-equipped factories. In this context, Predictive Maintenance (PdM) based on Machine Learning (ML) is one of the most prominent data-driven analytical approaches for monitoring industrial systems aiming to maximise reliability and efficiency. In fact, PdM aims not only to reduce equipment failure rates but also to minimise operating costs by maximising equipment life. When considering industrial applications, industries deal with different issues and constraints relating to process digitalisation. The main purpose of this study is to develop a new decision support system based on decision trees (DTs) that guides the decision-making process of PdM implementation, considering context-aware information, quality and maturity of collected data, severity, occurrence and detectability of potential failures (identified through FMECA analysis) and direct and indirect maintenance costs. The decision trees allow the study of different scenarios to identify the conditions under which a PdM policy, based on the ML algorithm, is economically profitable compared to corrective maintenance, considered to be the current scenario. The results show that the proposed methodology is a simple and easy way to implement tool to support the decision process by assessing the different levels of occurrence and severity of failures. For each level, savings and the potential costs have been evaluated at leaf nodes of the trees aimed at defining the most suitable maintenance strategy implementation. Finally, the proposed DTs are applied to a real industrial case to illustrate their applicability and robustness.",industry,40
10.1016/j.eswa.2021.116045,filtered,Expert Systems with Applications,scopus,2022-02-01,sciencedirect,posimnet-r: an immunologic resilient approach to position routers in industrial wireless sensor networks,https://api.elsevier.com/content/abstract/scopus_id/85117584055,"Industry 4.0 has increased the interest in employing Industrial Wireless Sensor Network (IWSN) technologies in industrial automation. The advantages range from ease of installation and maintenance to reduced deployment time and infrastructure costs. However, industrial automation has critical requirements regarding network infrastructure, such as reliability and failure tolerance. Therefore, it is imperative to have an adequate placement of sensor and router nodes, to obtain a network with multiple paths, allowing the data to reach management systems within a reasonable time, even in the event of failures. The placement of router nodes has to consider latency, network lifespan, connectivity, and failure tolerance aspects in a possibly hostile environment, with classified areas and obstacles such as silos, tanks and buildings. We present a new approach, called POSIMNET-R, to place IWSN routing nodes in an industrial configuration, which circumvents forbidden areas and obstacles, based on Artificial Immunological Networks. The resulting network offers low failure rates and path redundancy criteria. The results have shown that POSIMNET-R was capable of providing a reliable network with multiple paths and resilience of the used routers equal to 81.50% in the basic case study and 73.66% in the real case scenario.",industry,41
10.1016/j.softx.2021.100956,filtered,SoftwareX,scopus,2022-01-01,sciencedirect,tx2_fcnn_node: an open-source ros compatible tool for monocular depth reconstruction,https://api.elsevier.com/content/abstract/scopus_id/85121968187,"We present tx2_fcnn_node – a Robot Operating System (ROS) compatible tool that is aimed at seamless integration of various monocular depth reconstruction neural networks to the robotic software based on ROS (which is a de-facto standard in the area of robotics). Our tool simplifies the process of deploying, evaluating, and comparing depth reconstruction neural networks both on real robots and in simulation. We complement our software with a set of the precompiled neural networks which can be used off the shelf, with some of them being able to demonstrate near real-time performance when running onboard compact embedded platforms, e.g. Nvidia Jetson TX2, that are often used nowadays both in academia and industry.",industry,42
10.1016/j.compag.2021.106635,filtered,Computers and Electronics in Agriculture,scopus,2022-01-01,sciencedirect,intelligent iot-multiagent precision irrigation approach for improving water use efficiency in irrigation systems at farm and district scales,https://api.elsevier.com/content/abstract/scopus_id/85121511874,"The fourth industrial revolution in agriculture seeks the automation of traditional practices, using modern smart technologies. Advances in electronics, computation and the internet of things are integrated for improving field inputs management. The aim of this paper is to present the design and implementation of an intelligent IoT-multiagent precision irrigation approach for improving water use efficiency in irrigation systems. The study site was the large-scale irrigation and drainage district of Chicamocha and Firavitoba (Usochicamocha) located in Boyacá - Colombia, where water is distributed from the Chicamocha riverbed. In the proposed system, irrigation is supervised and controlled in each field by an intelligent irrigation agent that autonomously prescribes and applies water amounts with agronomical criteria. The methodology was applied with real (cyber-physical) and virtual (simulated) intelligent agents and was extended to eleven pump stations that supply water to 5911 fields. Using a MQTT protocol, hundreds of irrigation intelligent agents report water prescriptions and crop characteristics to a master agent in each pump station, who creates a regional irrigation map to manage georeferenced field information and performs negotiation of water resources between agents according to supply availability. Field maps and intelligent irrigation agents can be visualized using devices with internet access. Results demonstrated that irrigation amounts were correctly applied on the fields, thus improving the water use efficiency. This technology is a novel support to decision-making in water resources management applications at field and district scales.",industry,43
10.1016/j.enconman.2021.115030,filtered,Energy Conversion and Management,scopus,2022-01-01,sciencedirect,deep reinforcement learning based energy management strategy of fuel cell hybrid railway vehicles considering fuel cell aging,https://api.elsevier.com/content/abstract/scopus_id/85119905389,"In the rail transportation industry, growing energy and environmental awareness requires the use of alternatives to combustion engines. These include hybrid electrically driven railway vehicles powered by fuel cells and batteries. The cost of hydrogen consumption and the lifetime of fuel cells are currently the main challenges that need to be addressed before widespread deployment of fuel cell railway vehicles can be realized. With this in mind, this work focuses on the energy management system with emphasis on optimizing the energy distribution to reduce the overall operational cost. The presented energy management strategy (EMS) aims at minimizing hydrogen consumption and fuel cell aging costs while achieving a favorable balance between battery charging and discharging. In order to take fuel cell aging into account in energy management and mitigate fuel cell aging trough power distribution, an online fuel cell aging estimation model based on four operation modes is introduced and applied. Moreover, the advanced deep reinforcement learning method Twin Delayed Deep Deterministic Policy Gradient (TD3) is used to obtain a promising EMS. To improve the adaptability of the strategy, a stochastic training environment, which is based on real measured speed profiles considering passenger numbers is used for training. Assuming different environmental and passenger transport volumes, the results confirm that the proposed TD3-EMS achieves battery charge-sustaining at low hydrogen consumption while slowing down fuel cell degradation.",industry,44
10.1016/j.ijpe.2021.108339,filtered,International Journal of Production Economics,scopus,2022-01-01,sciencedirect,age-based preventive maintenance with multiple printing options,https://api.elsevier.com/content/abstract/scopus_id/85118549755,"In today's economic context, production systems must be readily available and machinery downtime kept to a minimum. Maintenance and spare parts inventory management play a vital role in achieving these goals, and preventive maintenance has increasingly been considered in maintenance policies. Additive manufacturing (AM) has recently been combined with preventive maintenance, and thus represents an emerging research direction. However, few studies have as yet been conducted in this research stream, and we intend to fill this gap. Our study makes three main contributions. First, we address the main limitations of two current models (i.e., assuming that no failure occurs during the replenishment lead time of the spare parts). Second, we propose a new maintenance policy that considers two printing options with different levels of reliability and unitary purchase costs. Third, we develop a decision support system (DSS) to assist managers in deciding whether to implement a preventive maintenance policy that includes AM or conventional manufacturing (CM) parts. We take an interdisciplinary approach to conducting a parametrical analysis where we consider real data on the reliability of CM and AM parts, in addition to the impact of post-processing operations and optimization routines. We find that AM-based preventive maintenance policies are favored when the MTTF and the backorder costs are low and when the failure and maintenance costs are high. These findings have been incorporated into the DSS, which provides thresholds for every parameter to guide practitioners in choosing between AM and CM parts for preventive maintenance, without requiring time-expensive calculations.",industry,45
10.1109/access.2021.3137031,filtered,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,autonomous detection and deterrence of pigeons on buildings by drones,https://ieeexplore.ieee.org/document/9656717/,"Pigeons may transmit diseases to humans and cause damages to buildings, monuments, and other infrastructure. Therefore, several control strategies have been developed, but they have been found to be either ineffective or harmful to animals and often depend on human operation. This study proposes a system capable of autonomously detecting and deterring pigeons on building roofs using a drone. The presence and position of pigeons were detected in real time by a neural network using images taken by a video camera located on the roof. Moreover, a drone was utilized to deter the animals. Field experiments were conducted in a real-world urban setting to assess the proposed system by comparing the number of animals and their stay durations for over five days against the 21-day-trial experiment without the drone. During the five days of experiments, the drone was automatically deployed 55 times and was significantly effective in reducing the number of birds and their stay durations without causing any harm to them. In conclusion, this study has proven the effectiveness of this system in deterring birds, and this approach can be seen as a fully autonomous alternative to the already existing methods.",smart cities,46
10.1109/ccnc49033.2022.9700579,filtered,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),IEEE,2022-01-11 00:00:00,ieeexplore,demo: an experimental environment based on mini-pcs for federated learning research,https://ieeexplore.ieee.org/document/9700579/,"There is a growing research interest in Federated Learning (FL), a promising approach for data privacy preservation and proximity of training to the network edge, where data is generated. Resource consumption for Machine Learning (ML) training and inference is important for edge nodes, but most of the proposed protocols and algorithms for FL are evaluated by simulations. In this demo paper, we present an environment based on distributed mini-PCs to enable experimental study of FL protocols and algorithms. We have installed low-capacity mini-PCs within a wireless city-level mesh network and deployed container-based FL components on these nodes. We show the deployed FL clients and server at different nodes in the city and demonstrate how an FL experiment can be set and run in a real environment.",smart cities,47
10.1109/wacv51458.2022.00308,filtered,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),IEEE,2022-01-08 00:00:00,ieeexplore,multi-branch neural networks for video anomaly detection in adverse lighting and weather conditions,https://ieeexplore.ieee.org/document/9706717/,"Automated anomaly detection in surveillance videos has attracted much interest as it provides a scalable alternative to manual monitoring. Most existing approaches achieve good performance on clean benchmark datasets recorded in well-controlled environments. However, detecting anomalies is much more challenging in the real world. Adverse weather conditions like rain or changing brightness levels cause a significant shift in the input data distribution, which in turn can lead to the detector model incorrectly reporting high anomaly scores. Additionally, surveillance cameras are usually deployed in evolving environments such as a city street of which the appearance changes over time because of seasonal changes or roadworks. The anomaly detection model will need to be updated periodically to deal with these issues. In this paper, we introduce a multi-branch model that is equipped with a trainable preprocessing step and multiple identical branches for detecting anomalies during day and night as well as in sunny and rainy conditions. We experimentally validate our approach on a distorted version of the Avenue dataset and provide qualitative results on real-world surveillance camera data. Experimental results show that our method outperforms the existing methods in terms of detection accuracy while being faster and more robust on scenes with varying visibility.",smart cities,48
10.1109/tits.2020.3029537,filtered,IEEE Transactions on Intelligent Transportation Systems,IEEE,2022-02-01 00:00:00,ieeexplore,spatial positioning token (sptoken) for smart mobility,https://ieeexplore.ieee.org/document/9238413/,"We introduce a permissioned distributed ledger technology (DLT) design for crowdsourced smart mobility applications. This architecture is based on a directed acyclic graph architecture (similar to the IOTA tangle) and uses both Proof-of-Work and Proof-of-Position mechanisms to provide protection against spam attacks and malevolent actors. In addition to enabling individuals to retain ownership of their data and to monetize it, the architecture is also suitable for distributed privacy-preserving machine learning algorithms, is lightweight, and can be implemented in simple internet-of-things (IoT) devices. To demonstrate its efficacy, we apply this framework to reinforcement learning settings where a third party is interested in acquiring information from agents. In particular, one may be interested in sampling an unknown vehicular traffic flow in a city, using a DLT-type architecture and without perturbing the density, with the idea of realizing a set of virtual tokens as surrogates of real vehicles to explore geographical areas of interest. These tokens, whose authenticated position determines write access to the ledger, are thus used to emulate the probing actions of commanded (real) vehicles on a given planned route by “jumping” from a passing-by vehicle to another to complete the planned trajectory. Consequently, the environment stays unaffected (i.e., the autonomy of participating vehicles is not influenced by the algorithm), regardless of the number of emitted tokens. The design of such a DLT architecture is presented, and numerical results from large-scale simulations are provided to validate the proposed approach.",smart cities,49
10.1109/tnse.2021.3072911,filtered,IEEE Transactions on Network Science and Engineering,IEEE,2022-02-01 00:00:00,ieeexplore,prioritized content determination and dissemination using reinforcement learning in dtns,https://ieeexplore.ieee.org/document/9403937/,"In a battlefield, several groups of soldiers are deployed with different mission goals by the command and control center (CC). To continue the missions appropriately and get a better understanding of the situation, the soldiers, as well as the CC, need to collect information of interest generated in different battle zones. However, due to the damaged network infrastructure in the hostile areas, it is a challenge to determine the topics of interest associated with the events and missions, and efficiently forward the associated content to the CC. Hence, the devices of the soldiers (nodes) generate, store and forward content hop by hop using a Delay Tolerant Network (DTN). While forwarding content, nodes avoid congestion so that meaningful content related to prioritized mission goals can be disseminated. In this dynamic surrounding, any sudden but important event-related content should also be sent to the CC with the help of intermediate nodes regardless of their own mission interests. We design a scheme to forward contents generated by the nodes to the CC using Reinforcement Learning (RL) while maximizing the number of interesting data in the respective nodes' buffer, and avoiding congestion. In this forwarding process, we focus on identifying the trending topics/keywords among changing missions and their related data at the node level, and the changes of interest of the nodes based on their mobility and connectivity patterns. Experiments are conducted using real datasets and ONE simulator to show the effectiveness of Reinforcement Learning (RL) on the prioritized content dissemination in a DTN.",smart cities,50
10.1109/jsen.2021.3132460,filtered,IEEE Sensors Journal,IEEE,2001-02-01 20:22:00,ieeexplore,automatic rail component detection based on attnconv-net,https://ieeexplore.ieee.org/document/9634063/,"The automatic detection of major rail components using railway images is beneficial to ensure the rail transport safety. In this paper, we propose an attention-powered deep convolutional network (AttnConv-net) to detect multiple rail components including the rail, clips, and bolts. The proposed method consists of a deep convolutional neural network (DCNN) as the backbone, cascading attention blocks (CAB), and two feed forward networks (FFN). Two types of positional embedding are applied to enrich information in latent features extracted from the backbone. Based on processed latent features, the CAB aims to learn the local context of rail components including their categories and component boundaries. Final categories and bounding boxes are generated via two FFN implemented in parallel. To enhance the detection of small components, various data augmentation methods are employed in training process. The effectiveness of the proposed AttnConv-net is validated with one real dataset and another synthesized dataset. Compared with classic convolutional neural network based methods, our proposed method simplifies the detection pipeline by eliminating the need of prior- and post-processing, which offers a new speed-quality solution to enable faster and more accurate image-based rail component detections.",smart cities,51
http://arxiv.org/abs/2202.05118v1,filtered,arxiv,arxiv,2022-02-10 00:00:00,arxiv,"reinforcement learning in the wild: scalable rl dispatching algorithm
  deployed in ridehailing marketplace",http://arxiv.org/abs/2202.05118v1,"In this study, a real-time dispatching algorithm based on reinforcement
learning is proposed and for the first time, is deployed in large scale.
Current dispatching methods in ridehailing platforms are dominantly based on
myopic or rule-based non-myopic approaches. Reinforcement learning enables
dispatching policies that are informed of historical data and able to employ
the learned information to optimize returns of expected future trajectories.
Previous studies in this field yielded promising results, yet have left room
for further improvements in terms of performance gain, self-dependency,
transferability, and scalable deployment mechanisms. The present study proposes
a standalone RL-based dispatching solution that is equipped with multiple
mechanisms to ensure robust and efficient on-policy learning and inference
while being adaptable for full-scale deployment. A new form of value updating
based on temporal difference is proposed that is more adapted to the inherent
uncertainty of the problem. For the driver-order assignment, a customized
utility function is proposed that when tuned based on the statistics of the
market, results in remarkable performance improvement and interpretability. In
addition, for reducing the risk of cancellation after drivers' assignment, an
adaptive graph pruning strategy based on the multi-arm bandit problem is
introduced. The method is evaluated using offline simulation with real data and
yields notable performance improvement. In addition, the algorithm is deployed
online in multiple cities under DiDi's operation for A/B testing and is
launched in one of the major international markets as the primary mode of
dispatch. The deployed algorithm shows over 1.3% improvement in total driver
income from A/B testing. In addition, by causal inference analysis, as much as
5.3% improvement in major performance metrics is detected after full-scale
deployment.",smart cities,52
http://arxiv.org/abs/2202.01862v1,filtered,arxiv,arxiv,2022-02-03 00:00:00,arxiv,practical imitation learning in the real world via task consistency loss,http://arxiv.org/abs/2202.01862v1,"Recent work in visual end-to-end learning for robotics has shown the promise
of imitation learning across a variety of tasks. Such approaches are expensive
both because they require large amounts of real world training demonstrations
and because identifying the best model to deploy in the real world requires
time-consuming real-world evaluations. These challenges can be mitigated by
simulation: by supplementing real world data with simulated demonstrations and
using simulated evaluations to identify high performing policies. However, this
introduces the well-known ""reality gap"" problem, where simulator inaccuracies
decorrelate performance in simulation from that of reality. In this paper, we
build on top of prior work in GAN-based domain adaptation and introduce the
notion of a Task Consistency Loss (TCL), a self-supervised loss that encourages
sim and real alignment both at the feature and action-prediction levels. We
demonstrate the effectiveness of our approach by teaching a mobile manipulator
to autonomously approach a door, turn the handle to open the door, and enter
the room. The policy performs control from RGB and depth images and generalizes
to doors not encountered in training data. We achieve 80% success across ten
seen and unseen scenes using only ~16.2 hours of teleoperated demonstrations in
sim and real. To the best of our knowledge, this is the first work to tackle
latched door opening from a purely end-to-end learning approach, where the task
of navigation and manipulation are jointly modeled by a single neural network.",smart cities,53
http://arxiv.org/abs/2201.09419v1,filtered,arxiv,arxiv,2022-01-24 00:00:00,arxiv,"automated machine learning for secure key rate in discrete-modulated
  continuous-variable quantum key distribution",http://arxiv.org/abs/2201.09419v1,"Continuous-variable quantum key distribution (CV QKD) with discrete
modulation has attracted increasing attention due to its experimental
simplicity, lower-cost implementation and compatibility with classical optical
communication. Correspondingly, some novel numerical methods have been proposed
to analyze the security of these protocols against collective attacks, which
promotes key rates over one hundred kilometers of fiber distance. However,
numerical methods are limited by their calculation time and resource
consumption, for which they cannot play more roles on mobile platforms in
quantum networks. To improve this issue, a neural network model predicting key
rates in nearly real time has been proposed previously. Here, we go further and
show a neural network model combined with Bayesian optimization. This model
automatically designs the best architecture of neural network computing key
rates in real time. We demonstrate our model with two variants of CV QKD
protocols with quaternary modulation. The results show high reliability with
secure probability as high as $99.15\%-99.59\%$, considerable tightness and
high efficiency with speedup of approximately $10^7$ in both cases. This
inspiring model enables the real-time computation of unstructured quantum key
distribution protocols' key rate more automatically and efficiently, which has
met the growing needs of implementing QKD protocols on moving platforms.",smart cities,54
10.1109/wacvw54805.2022.00069,filtered,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),IEEE,2022-01-08 00:00:00,ieeexplore,aa3dnet: attention augmented real time 3d object detection,https://ieeexplore.ieee.org/document/9707544/,"In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Bird’s Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at &gt;30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",multimedia,55
10.1109/lra.2022.3147337,filtered,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,sim2air - synthetic aerial dataset for uav monitoring,https://ieeexplore.ieee.org/document/9699390/,"In this letter, we propose a novel approach to generate a synthetic aerial dataset for application in UAV monitoring. We propose to accentuate shape-based object representation by applying texture randomization. A diverse dataset with photorealism in all parameters such as shape, pose, lighting, scale, viewpoint, etc. except for atypical textures is created in a 3D modelling software Blender. Our approach specifically targets two conditions in aerial images where texture of objects is difficult to detect, namely challenging illumination and objects occupying only a small portion of the image. Experimental evaluation of YOLO and Faster R-CNN detectors trained on synthetic data with randomized textures confirmed our approach by increasing the mAP value (17 and 3.7 percentage points for YOLO; 20 and 1.1 percentage points for Faster R-CNN) on two test datasets of real images, both containing UAV-to-UAV images with motion blur. Testing on different domains, we conclude that the more the generalisation ability is put to the test, the more apparent are the advantages of the shape-based representation.",multimedia,56
10.1109/lra.2021.3116700,filtered,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,sim2real learning of obstacle avoidance for robotic manipulators in uncertain environments,https://ieeexplore.ieee.org/document/9555228/,"Obstacle avoidance for robotic manipulators can be challenging when they operate in unstructured environments. This problem is probed with the sim-to-real (sim2real) deep reinforcement learning, such that a moving policy of the robotic arm is learnt in a simulator and then adapted to the real world. However, the problem of sim2real adaptation is notoriously difficult. To this end, this work proposes (1) a unified representation of obstacles and targets to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model combining the unified representation with the deep reinforcement learning control module that can be trained by interacting with the environment. Such a representation is agnostic to the shape and appearance of the underlying objects, which simplifies and unifies the scene representation in both simulated and real worlds. We implement this idea with a vision-based actor-critic framework by devising a bounding box predictor module. The predictor estimates the 3D bounding boxes of obstacles and targets from the RGB-D input. The features extracted by the predictor are fed into the policy network, and all the modules are jointly trained. This makes the policy learn object-aware scene representation, which leads to a data-efficient learning of the obstacle avoidance policy. Our experiments in simulated environment and the real-world show that the end-to-end model of the unified representation achieves better sim2real adaption and scene generalization than state-of-the-art techniques.",multimedia,57
10.1109/jiot.2021.3089080,filtered,IEEE Internet of Things Journal,IEEE,2001-02-01 20:22:00,ieeexplore,audio-visual autoencoding for privacy-preserving video streaming,https://ieeexplore.ieee.org/document/9453730/,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",multimedia,58
10.1109/comsnets53615.2022.9668498,filtered,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),IEEE,2022-01-08 00:00:00,ieeexplore,tele-driving an electric vehicle over a private lte network,https://ieeexplore.ieee.org/document/9668498/,"We demonstrate tele-driving operation for an electric vehicle capable of stopping itself in case of system failure over a captive LTE network deployed in a university campus. Our electronically controlled vehicle is driven remotely by an operator from a control room which receives the multi-camera real-time video feed from the vehicle over this network. Our primary contribution includes the responsive emergency braking mechanism for the vehicle, modular vehicle design based on CAN bus, low latency LTE MAC scheduler design, and modifications to popular video tool, FFMPEG to support low latency real time video streaming. Our demonstration shows complete integration of the different components, i.e., the vehicle, the LTE network and the remote driving application. Another salient feature of our system is the O-RAN compliant RAN awareness module and KPI (Key Performance Indicator) application which enables real-time network performance monitoring.",multimedia,59
10.1109/tcsvt.2021.3066675,filtered,IEEE Transactions on Circuits and Systems for Video Technology,IEEE,2022-02-01 00:00:00,ieeexplore,spatio-temporal online matrix factorization for multi-scale moving objects detection,https://ieeexplore.ieee.org/document/9380454/,"Detecting moving objects from the video sequences has been treated as a challenging computer vision task, since the problems of dynamic background, multi-scale moving objects and various noise interference impact the corresponding feasibility and efficiency. In this paper, a novel spatio-temporal online matrix factorization (STOMF) method is proposed to detect multi-scale moving objects under dynamic background. To accommodate a wide range of the real noise distractions, we apply a specific mixture of exponential power (MoEP) distributions to the framework of low-rank matrix factorization (LRMF). For the optimization of solution algorithm, a temporal difference motion prior (TDMP) model is proposed, which estimates the motion matrix and calculates the weight matrix. Moreover, a partial spatial motion information (PSMI) post-processing method is further designed to implement multi-scale objects extraction in varieties of complex dynamic scenes, which utilizes partial background and motion information. The superiority of the STOMF method is validated by massive experiments on practical datasets, as compared with state-of-the-art moving objects detection approaches.",multimedia,60
10.1109/access.2021.3139537,filtered,IEEE Access,IEEE,2000-01-01 00:00:00,ieeexplore,"automatic adaptation of open educational resources: an approach from a multilevel methodology based on students’ preferences, educational special needs, artificial intelligence and accessibility metadata",https://ieeexplore.ieee.org/document/9669174/,"The need for adaptive e-learning environments that respond to learning variability is now a fundamental requirement in education, as it helps to ensure that students learn and pass their courses within a set time frame. Although guidelines, techniques and methods have been established in recent years to contribute to the development of accessible and adaptable e-learning environments that promote digital inclusion, their implementation is challenging due to the lack of knowledge of an adequate way to do it and because it is considered more of a technological competence for scholars in the area. In this context, automated support for adapting material that responds to the correct use of accessibility metadata not only provides a way to improve the description of adapted educational resources, but also facilitates their search according to the needs and preferences of students, particularly those with disabilities. In this article, we carry out a multilevel methodological proposal for the automatic adaptation of open educational resources, in order to provide a tool that contributes to the accessibility and correct use of their metadata in e-learning environments. A research is conducted with students with disabilities to establish their real needs and preferences, highlighting the need to strengthen the adequate description and coherent alternative text in images, the correct subtitling in videos and the conversion of audio to text, data that are relevant to our proposal. The research conducted aims to contribute with an automated support tool in the generation of accessible educational resources that are correctly labeled for search and reuse. This research also aims to support researchers in artificial intelligence applications to address challenges and opportunities in the field of virtual education, in addition to providing an overview that could help those who generate educational resources and maintain their interest in making them accessible.",multimedia,61
10.1109/taslp.2021.3126947,filtered,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",IEEE,2000-01-01 00:00:00,ieeexplore,end-to-end neural based modification of noisy speech for speech-in-noise intelligibility improvement,https://ieeexplore.ieee.org/document/9611022/,"Intelligibility of speech can be significantly reduced when it is presented in adverse near-end listening conditions, like background noise. Multiple approaches have been suggested to improve the perception of speech in such conditions. However, most of these approaches were designed to work with clean input speech. Therefore, they have serious limitations when deployed in real world applications like telephony and hearing aids, where noisy input speech is quite common. In this paper we present an end-to-end neural network approach for the above problem, which effectively reduces the input noise and improves the intelligibility for listeners in adverse conditions. To that end, a convolutional neural network topology with variable dilation factors is proposed and evaluated both in a causal and a non-causal configuration using raw speech as input. A Teacher-Student training strategy is employed, where the Teacher is a well-established speech-in-noise intelligibility enhancer based on spectral shaping followed by dynamic range compression (SSDRC). The evaluation is performed both objectively using the speech intelligibility in bits metric (SIIB), and subjectively on the Greek Harvard corpus. A noise robust multi-band version of SSDRC was used as a baseline. Compared with the baseline, at 0 dB input SNR, the suggested neural network system achieved about 380% and 230% relative SIIB improvements in fluctuating and stationary backgrounds, respectively. Subjectively, the suggested model increased listeners’ keyword correct rate in stationary noise from 25% to 60% at 0 dB input SNR, and from about 52% to 75% at 5 dB input SNR, compared with the baseline.",multimedia,62
10.1007/s00530-021-00881-8,filtered,Multimedia Systems,Springer,2022-01-29 00:00:00,springer,an object detection-based few-shot learning approach for multimedia quality assessment,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00530-021-00881-8,"A large portion of the global population generates various multimedia data such as texts, images, videos, etc. One of the most common categories which influences the public at large is visual multimedia content. Due to the different social media platforms (e.g., Whatsapp, Twitter, Facebook, Instagram, and YouTube), these materials are passed without censorship and national boundaries. Multimedia data containing any violent or vulgar objects could trigger public unrest, and thus, it is a serious threat to the law and order of the land. Children and teenagers use social media like never before in previous generations and create lots of multimedia data. It is important to assess the quality of multimedia content without any bias and prejudices. Although the mainstream social media platforms use different filters and moderation using human experts, it is impossible to verify the terabytes of uploaded images and videos. Thus, it is inevitable to automate the content assessment phase without incurring an increase in upload time. This study aims to prevent uploading or to tag an image/video with a reasonable percentage of a gun as content. In this paper, object detection architectures such as Faster RCNN, EfficientDet, and YOLOv5 have been used to demonstrate how these techniques can efficiently detect human faces and different types of guns in given multimedia data (images/videos). The models are tested on various test images and video clips. A comparative analysis has also been discussed based on mean average precision and frames per second metric. The YOLOv5 provides the best-performing results as high as 80.39% and 35.22% at $$\text{mAP}_{0.5}$$ mAP 0.5 and $$\text{mAP}_{[0.50:0.95]}$$ mAP [ 0.50 : 0.95 ] , respectively. A face recognition task requires thousands of samples and the usual deep learning models are data-driven. On the contrary, a few-shot learning approach has been implemented to recognize the detected faces categorizing the content as real or reel.",multimedia,63
10.1007/s00371-021-02347-4,filtered,The Visual Computer,Springer,2022-01-13 00:00:00,springer,a detailed analysis of image and video forgery detection techniques,http://link.springer.com/openurl/fulltext?id=doi:10.1007/s00371-021-02347-4,"With the recent advancement in modern technology, one can easily manipulate a digital image or video using computer software or a mobile application. The purpose of editing visual media could be as simple as to look good before sharing to the social networking site’s or can be as malicious as to defame or hurt one’s reputation in the real world through such morphed visual imagery. Identity theft is one of the examples where one’s identity get stolen by some impersonator who can access the personal and financial information of an innocent person. To avoid such drastic situations, law enforcement authorities must use some automatic tools and techniques to find out whether a person is innocent or the culprit. One major question that arises here is how and what parts of visual imagery can be manipulated or edited. The answer to this question is important to distinguish the authentic images/videos from the doctored multimedia. This survey provides a detailed analysis of image and video manipulation types, popular visual imagery manipulation methods, and state-of-the-art image and video forgery detection techniques. It also surveys different fake image and video datasets used in tampering. The goal is to develop a sense of privacy and security in the research community. Finally, it focuses to motivate researchers to develop generalized methods to capture artificial visual imagery which is capable of detecting any type of manipulation in given visual imagery.",multimedia,64
10.1007/978-3-030-92127-9_68,filtered,"11th International Conference on Theory and Application of Soft Computing, Computing with Words and Perceptions and Artificial Intelligence - ICSCCW-2021",Springer,2022-01-01 00:00:00,springer,application of digital twin theory for improvement of natural gas treatment unit,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-92127-9_68,"This paper describes fundamental principles of Digital Twins theory and provides exact investigation results in application of Digital Twins theory in upstream branch of oil and gas industry, namely based on example of natural gas treatment plant’s performance increase. As one of key process units of natural gas treatment which allows to implement most powerful functions of digital twin gas sweetening unit with set of membranes is considered as object of investigation. On the base of membrane technology manipulated variables are defined as inputs to digital twin model. Some theoretical results as well as real references of model’s engine calculations are reflected in the paper. Details of technical dashboards to visualize calculated results of running model based on manipulated variables are presented including monthly key performance indicators report dashboard, process flow diagram dashboard and high-level management dashboard. Paper also demonstrates data flow between digital twin model and real process unit and also inside digital twin model.",multimedia,65
10.1007/978-3-030-95405-5_9,filtered,Advanced Data Mining and Applications,Springer,2022-01-01 00:00:00,springer,smart online exam proctoring assist for cheating detection,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-3-030-95405-5_9,"Online exams are the most preferred mode of exams in online learning environment. This mode of exam has been even more prevalent and a necessity in the event of a forced closure of face-to-face teaching such as the recent Covid-19 pandemic. Naturally, conducting online exams poses much greater challenge to preserving academic integrity compared to conducting on-site face-to-face exams. As there is no human proctor for policing the examinee on site, the chances of cheating are high. Various online exam proctoring tools are being used by educational institutes worldwide, which offer different solutions to reduce the chances of cheating. The most common technique followed by these tools is recording of video and audio of the examinee during the whole duration of exam. These videos can be analyzed later by human examiner to detect possible cheating case. However, viewing hours of exam videos for each student can be impractical for a large class and thus detecting cheating would be next to impossible. Although some AI-based tools are being used by some proctoring software to raise flags, they are not always very useful. In this paper we propose a cheating detection technique that analyzes an exam video to extract four types of event data, which are then fed to a pre-trained classification model for detecting cheating activity. We formulate the cheating detection problem as a multivariate time-series classification problem by transforming each video into a multivariate time-series representing the time-varying event data extracted from each frame of the video. We have developed a real dataset of cheating videos and conduct extensive experiments with varying video lengths, different deep learning and traditional machine learning models and feature sets, achieving prediction accuracy as high as 97.7%.",multimedia,66
10.1007/978-981-16-5689-7_12,filtered,Advances in Data and Information Sciences,Springer,2022-01-01 00:00:00,springer,applications of high dimensional neural networks: a survey,http://link.springer.com/openurl/fulltext?id=doi:10.1007/978-981-16-5689-7_12,"The evolution of artificial neural networks has always been inspired by enormous power of human brain. This survey can be an eye-opener for researchers as its diverse applications of HDNNs in present scenario shows an intelligent way to mimic human brain without creating a complex neuronal architecture having large number of layers. HDNN’s urgency is evident because in science many quantities are measured not by single values for each of them but a group of values defines 1 single unit as: signal has two values: amplitude and phase. The deficiency in present literature on the questions allied with HDNN application looks like sluggish down research focal point and growth in the area. Hence, there exists a call for state-of-the-art addressing high-dimensional problems in neural networks. The study equips readers with a lucid acquaintance of the existing and novel inclination in HDNN replicas. A lot of applications of HDNNs in various disciplines like: healthcare, climate, security, speech recognition, computer vision, music signal processing, production, stock, science, etc. are covered here to confirm advancement in HDNNs. Study divulges that HDNNs is prevalently known as: CVNN, QVNN, 3D VVNN, and OVNN. This paper also reveals that HDNNs have outperformed real valued neural networks in terms of resource utilization, training data set requirement, and accuracy of results. To see a comparative picture of significance and possible implementation of different HDNNs few charts are provided. A motivational message and suggestions for future researches in this area of High-Dimensionality will conclude this paper.",multimedia,67
http://arxiv.org/abs/2202.05940v1,filtered,arxiv,arxiv,2022-02-12 00:00:00,arxiv,automatic curriculum generation for learning adaptation in networking,http://arxiv.org/abs/2202.05940v1,"As deep reinforcement learning (RL) showcases its strengths in networking and
systems, its pitfalls also come to the public's attention--when trained to
handle a wide range of network workloads and previously unseen deployment
environments, RL policies often manifest suboptimal performance and poor
generalizability.
  To tackle these problems, we present Genet, a new training framework for
learning better RL-based network adaptation algorithms. Genet is built on the
concept of curriculum learning, which has proved effective against similar
issues in other domains where RL is extensively employed. At a high level,
curriculum learning gradually presents more difficult environments to the
training, rather than choosing them randomly, so that the current RL model can
make meaningful progress in training. However, applying curriculum learning in
networking is challenging because it remains unknown how to measure the
""difficulty"" of a network environment.
  Instead of relying on handcrafted heuristics to determine the environment's
difficulty level, our insight is to utilize traditional rule-based (non-RL)
baselines: If the current RL model performs significantly worse in a network
environment than the baselines, then the model's potential to improve when
further trained in this environment is substantial. Therefore, Genet
automatically searches for the environments where the current model falls
significantly behind a traditional baseline scheme and iteratively promotes
these environments as the training progresses. Through evaluating Genet on
three use cases--adaptive video streaming, congestion control, and load
balancing, we show that Genet produces RL policies which outperform both
regularly trained RL policies and traditional baselines in each context, not
only under synthetic workloads but also in real environments.",multimedia,68
http://arxiv.org/abs/2201.10369v1,filtered,arxiv,arxiv,2022-01-25 00:00:00,arxiv,winograd convolution for deep neural networks: efficient point selection,http://arxiv.org/abs/2201.10369v1,"Convolutional neural networks (CNNs) have dramatically improved the accuracy
of tasks such as object recognition, image segmentation and interactive speech
systems. CNNs require large amounts of computing resources because
ofcomputationally intensive convolution layers. Fast convolution algorithms
such as Winograd convolution can greatly reduce the computational cost of these
layers at a cost of poor numeric properties, such that greater savings in
computation exponentially increase floating point errors.
  A defining feature of each Winograd convolution algorithm is a set of
real-value points where polynomials are sampled. The choice of points impacts
the numeric accuracy of the algorithm, but the optimal set of points for small
convolutions remains unknown. Existing work considers only small integers and
simple fractions as candidate points. In this work, we propose a novel approach
to point selection using points of the form {-1/c , -c, c, 1/c } using the full
range of real-valued numbers for c. We show that groups of this form cause
cancellations in the Winograd transform matrices that reduce numeric error. We
find empirically that the error for different values of c forms a rough curve
across the range of real-value numbers helping to localize the values of c that
reduce error and that lower errors can be achieved with non-obvious real-valued
evaluation points instead of integers or simple fractions. We study a range of
sizes for small convolutions and achieve reduction in error ranging from 2% to
around 59% for both 1D and 2D convolution. Furthermore, we identify patterns in
cases when we select a subset of our proposed points which will always lead to
a lower error. Finally we implement a complete Winograd convolution layer and
use it to run deep convolution neural networks on real datasets and show that
our proposed points reduce error, ranging from 22% to 63%.",multimedia,69
http://arxiv.org/abs/2201.09550v1,filtered,arxiv,arxiv,2022-01-24 00:00:00,arxiv,crowd tracking and monitoring middleware via map-reduce,http://arxiv.org/abs/2201.09550v1,"This paper presents the design, implementation, and operation of a novel
distributed fault-tolerant middleware. It uses interconnected WSNs that
implement the Map-Reduce paradigm, consisting of several low-cost and low-power
mini-computers (Raspberry Pi). Specifically, we explain the steps for the
development of a novice, fault-tolerant Map-Reduce algorithm which achieves
high system availability, focusing on network connectivity. Finally, we
showcase the use of the proposed system based on simulated data for crowd
monitoring in a real case scenario, i.e., a historical building in Greece (M.
Hatzidakis' residence).The technical novelty of this article lies in presenting
a viable low-cost and low-power solution for crowd sensing without using
complex and resource-intensive AI structures or image and video recognition
techniques.",multimedia,70
http://arxiv.org/abs/2201.04833v1,filtered,arxiv,arxiv,2022-01-13 00:00:00,arxiv,"snapshotnet: self-supervised feature learning for point cloud data
  segmentation using minimal labeled data",http://arxiv.org/abs/2201.04833v1,"Manually annotating complex scene point cloud datasets is both costly and
error-prone. To reduce the reliance on labeled data, a new model called
SnapshotNet is proposed as a self-supervised feature learning approach, which
directly works on the unlabeled point cloud data of a complex 3D scene. The
SnapshotNet pipeline includes three stages. In the snapshot capturing stage,
snapshots, which are defined as local collections of points, are sampled from
the point cloud scene. A snapshot could be a view of a local 3D scan directly
captured from the real scene, or a virtual view of such from a large 3D point
cloud dataset. Snapshots could also be sampled at different sampling rates or
fields of view (FOVs), thus multi-FOV snapshots, to capture scale information
from the scene. In the feature learning stage, a new pre-text task called
multi-FOV contrasting is proposed to recognize whether two snapshots are from
the same object or not, within the same FOV or across different FOVs. Snapshots
go through two self-supervised learning steps: the contrastive learning step
with both part and scale contrasting, followed by a snapshot clustering step to
extract higher level semantic features. Then a weakly-supervised segmentation
stage is implemented by first training a standard SVM classifier on the learned
features with a small fraction of labeled snapshots. The trained SVM is used to
predict labels for input snapshots and predicted labels are converted into
point-wise label assignments for semantic segmentation of the entire scene
using a voting procedure. The experiments are conducted on the Semantic3D
dataset and the results have shown that the proposed method is capable of
learning effective features from snapshots of complex scene data without any
labels. Moreover, the proposed method has shown advantages when comparing to
the SOA method on weakly-supervised point cloud semantic segmentation.",multimedia,71
http://arxiv.org/abs/2201.03804v1,filtered,arxiv,arxiv,2022-01-11 00:00:00,arxiv,"ci-avsr: a cantonese audio-visual speech dataset for in-car command
  recognition",http://arxiv.org/abs/2201.03804v1,"With the rise of deep learning and intelligent vehicle, the smart assistant
has become an essential in-car component to facilitate driving and provide
extra functionalities. In-car smart assistants should be able to process
general as well as car-related commands and perform corresponding actions,
which eases driving and improves safety. However, there is a data scarcity
issue for low resource languages, hindering the development of research and
applications. In this paper, we introduce a new dataset, Cantonese In-car
Audio-Visual Speech Recognition (CI-AVSR), for in-car command recognition in
the Cantonese language with both video and audio data. It consists of 4,984
samples (8.3 hours) of 200 in-car commands recorded by 30 native Cantonese
speakers. Furthermore, we augment our dataset using common in-car background
noises to simulate real environments, producing a dataset 10 times larger than
the collected one. We provide detailed statistics of both the clean and the
augmented versions of our dataset. Moreover, we implement two multimodal
baselines to demonstrate the validity of CI-AVSR. Experiment results show that
leveraging the visual signal improves the overall performance of the model.
Although our best model can achieve a considerable quality on the clean test
set, the speech recognition quality on the noisy data is still inferior and
remains as an extremely challenging task for real in-car speech recognition
systems. The dataset and code will be released at
https://github.com/HLTCHKUST/CI-AVSR.",multimedia,72
http://arxiv.org/abs/2201.00768v1,filtered,arxiv,arxiv,2022-01-03 00:00:00,arxiv,"robust natural language processing: recent advances, challenges, and
  future directions",http://arxiv.org/abs/2201.00768v1,"Recent natural language processing (NLP) techniques have accomplished high
performance on benchmark datasets, primarily due to the significant improvement
in the performance of deep learning. The advances in the research community
have led to great enhancements in state-of-the-art production systems for NLP
tasks, such as virtual assistants, speech recognition, and sentiment analysis.
However, such NLP systems still often fail when tested with adversarial
attacks. The initial lack of robustness exposed troubling gaps in current
models' language understanding capabilities, creating problems when NLP systems
are deployed in real life. In this paper, we present a structured overview of
NLP robustness research by summarizing the literature in a systemic way across
various dimensions. We then take a deep-dive into the various dimensions of
robustness, across techniques, metrics, embeddings, and benchmarks. Finally, we
argue that robustness should be multi-dimensional, provide insights into
current research, identify gaps in the literature to suggest directions worth
pursuing to address these gaps.",multimedia,73
10.1016/j.enconman.2022.115217,filtered,Energy Conversion and Management,scopus,2022-02-15,sciencedirect,robopv: an integrated software package for autonomous aerial monitoring of large scale pv plants,https://api.elsevier.com/content/abstract/scopus_id/85122793867,"In this paper, a novel software package, called RoboPV, is introduced for autonomous aerial monitoring of PV plants. RoboPV automatically performs aerial monitoring of PV plants, from optimal trajectory planning to image processing and pattern recognition for real-time fault detection and analysis. RoboPV consists of four integrated components: boundary area detection, path planning, dynamic processing, and fault detection. To design an optimal flight path, aerial images of PV plants, which have been collected from experimental flights, are given as inputs to a developed encoder-decoder deep learning architecture to extract boundary points of PV plants automatically. Then, a novel path planning algorithm is conducted by RoboPV to design an optimal flight path with full coverage of whole regions of the PV plant. Aerial images are analysed in real-time during the flight by a high precise neural network trained for automatic fault detection. In this study, several decision-making and maneuver algorithms were developed for various real-world flight conditions to improve the performance of RoboPV during an autonomous aerial inspection. RoboPV is a modular processing library that can be installed on any micro-computer processor with a low computational power. Moreover, supporting the MAVLink communication protocol enables RoboPV to connect with an intelligent Pixhawk flight autopilot and navigate a wide range of multi-rotors. To demonstrate the performance of RoboPV, a six degrees of freedom dynamic model of a multi-rotor is developed in a SIMULINK environment with a defined aerial monitoring mission on three different real megawatt-scale PV plants. The results prove that RoboPV can execute the autonomous aerial inspection with an overall accuracy of 93% for large-scale PV plants.",multimedia,74
10.1016/j.cviu.2021.103339,filtered,Computer Vision and Image Understanding,scopus,2022-02-01,sciencedirect,snapshotnet: self-supervised feature learning for point cloud data segmentation using minimal labeled data,https://api.elsevier.com/content/abstract/scopus_id/85122523188,"Manually annotating complex scene point cloud datasets is both costly and error-prone. To reduce the reliance on labeled data, a new model called SnapshotNet is proposed as a self-supervised feature learning approach, which directly works on the unlabeled point cloud data of a complex 3D scene. The SnapshotNet pipeline includes three stages. In the snapshot capturing stage, snapshots, which are defined as local collections of points, are sampled from the point cloud scene. A snapshot could be a view of a local 3D scan directly captured from the real scene, or a virtual view of such from a large 3D point cloud dataset. Snapshots could also be sampled at different sampling rates or fields of view (FOVs), thus multi-FOV snapshots, to capture scale information from the scene. In the feature learning stage, a new pre-text task called multi-FOV contrasting is proposed to recognize whether two snapshots are from the same object or not, within the same FOV or across different FOVs. Snapshots go through two self-supervised learning steps: the contrastive learning step with both part contrasting and scale contrasting, followed by a snapshot clustering step to extract higher level semantic features. Then a weakly-supervised segmentation stage is implemented by first training a standard SVM classifier on the learned features with a small fraction of labeled snapshots. Then trained SVM is further used to predict labels for input snapshots and predicted labels are converted into point-wise label assignments for semantic segmentation of the entire scene using a voting procedure. The experiments are conducted on the Semantic3D dataset and the results have shown that the proposed method is capable of learning effective features from snapshots of complex scene data without any labels. Moreover, the proposed weakly-supervised method has shown advantages when comparing to the state of the art method on weakly-supervised point cloud semantic segmentation.",multimedia,75
10.1109/tifs.2021.3131026,filtered,IEEE Transactions on Information Forensics and Security,IEEE,2000-01-01 00:00:00,ieeexplore,poligraph: intrusion-tolerant and distributed fake news detection system,https://ieeexplore.ieee.org/document/9627681/,"We present Poligraph, an intrusion-tolerant and decentralized fake news detection system. Poligraph aims to address architectural, system, technical, and social challenges of building a practical, long-term fake news detection platform. We first conduct a case study for fake news detection at authors’ institute, showing that machine learning-based reviews are less accurate but timely, while human reviews, in particular, experts reviews, are more accurate but time-consuming. This justifies the need for combining both approaches. At the core of Poligraph is two-layer consensus allowing seamlessly combining machine learning techniques and human expert determination. We construct the two-layer consensus using Byzantine fault-tolerant (BFT) and asynchronous threshold common coin protocols. We prove the correctness of our system in terms of conventional definitions of security in distributed systems (agreement, total order, and liveness) as well as new review validity (capturing the accuracy of news reviews). We also provide theoretical foundations on parameter selection for our system. We implement Poligraph and evaluate its performance on Amazon EC2 using a variety of news from online publications and social media. We demonstrate Poligraph achieves throughput of more than 5,000 transactions per second and latency as low as 0.05 second. The throughput of Poligraph is only marginally (<inline-formula> <tex-math notation=""LaTeX"">${4\%}$ </tex-math></inline-formula>–<inline-formula> <tex-math notation=""LaTeX"">${7\%}$ </tex-math></inline-formula>) slower than that of an unreplicated, single-server implementation. In addition, we conduct a real-world case study for the review of fake and real news among both experts and non-experts, which validates the practicality of our approach.",science,76
http://arxiv.org/abs/2202.10335v1,filtered,arxiv,arxiv,2022-02-21 00:00:00,arxiv,explainability in machine learning: a pedagogical perspective,http://arxiv.org/abs/2202.10335v1,"Given the importance of integrating of explainability into machine learning,
at present, there are a lack of pedagogical resources exploring this.
Specifically, we have found a need for resources in explaining how one can
teach the advantages of explainability in machine learning. Often pedagogical
approaches in the field of machine learning focus on getting students prepared
to apply various models in the real world setting, but much less attention is
given to teaching students the various techniques one could employ to explain a
model's decision-making process. Furthermore, explainability can benefit from a
narrative structure that aids one in understanding which techniques are
governed by which questions about the data.
  We provide a pedagogical perspective on how to structure the learning process
to better impart knowledge to students and researchers in machine learning,
when and how to implement various explainability techniques as well as how to
interpret the results. We discuss a system of teaching explainability in
machine learning, by exploring the advantages and disadvantages of various
opaque and transparent machine learning models, as well as when to utilize
specific explainability techniques and the various frameworks used to structure
the tools for explainability. Among discussing concrete assignments, we will
also discuss ways to structure potential assignments to best help students
learn to use explainability as a tool alongside any given machine learning
application.
  Data science professionals completing the course will have a birds-eye view
of a rapidly developing area and will be confident to deploy machine learning
more widely. A preliminary analysis on the effectiveness of a recently
delivered course following the structure presented here is included as evidence
supporting our pedagogical approach.",science,77
http://arxiv.org/abs/2202.10144v1,filtered,arxiv,arxiv,2022-02-21 00:00:00,arxiv,"inferring network structure with unobservable nodes from time series
  data",http://arxiv.org/abs/2202.10144v1,"Network structures play important roles in social, technological and
biological systems. However, the observable nodes and connections in real cases
are often incomplete or unavailable due to measurement errors, private
protection issues, or other problems. Therefore, inferring the complete network
structure is useful for understanding human interactions and complex dynamics.
The existing studies have not fully solved the problem of inferring network
structure with partial information about connections or nodes. In this paper,
we tackle the problem by utilizing time-series data generated by network
dynamics. We regard the network inference problem based on dynamical time
series data as a problem of minimizing errors for predicting states of
observable nodes and proposed a novel data-driven deep learning model called
Gumbel-softmax Inference for Network (GIN) to solve the problem under
incomplete information. The GIN framework includes three modules: a dynamics
learner, a network generator, and an initial state generator to infer the
unobservable parts of the network. We implement experiments on artificial and
empirical social networks with discrete and continuous dynamics. The
experiments show that our method can infer the unknown parts of the structure
and the initial states of the observable nodes with up to 90\% accuracy. The
accuracy declines linearly with the increase of the fractions of unobservable
nodes. Our framework may have wide applications where the network structure is
hard to obtain and the time series data is rich.",science,78
http://arxiv.org/abs/2202.07475v1,filtered,arxiv,arxiv,2022-02-14 00:00:00,arxiv,"a real-time system for detecting landslide reports on social media using
  artificial intelligence",http://arxiv.org/abs/2202.07475v1,"This paper presents an online system that leverages social media data in real
time to identify landslide-related information automatically using
state-of-the-art artificial intelligence techniques. The designed system can
(i) reduce the information overload by eliminating duplicate and irrelevant
content, (ii) identify landslide images, (iii) infer geolocation of the images,
and (iv) categorize the user type (organization or person) of the account
sharing the information. The system was deployed in February 2020 online at
https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter
data stream and has been running continuously since then to provide
time-critical information to partners such as British Geological Survey and
European Mediterranean Seismological Centre. We trust this system can both
contribute to harvesting of global landslide data for further research and
support global landslide maps to facilitate emergency response and decision
making.",science,79
10.1016/j.comcom.2021.11.011,filtered,Computer Communications,scopus,2022-02-01,sciencedirect,ran energy efficiency and failure rate through ann traffic predictions processing,https://api.elsevier.com/content/abstract/scopus_id/85120657977,"In this paper, we focus on the application of ML tools to resource management in a portion of a Radio Access Network (RAN) and, in particular, to Base Station (BS) activation and deactivation, aiming at reducing energy consumption while providing enough capacity to satisfy the variable traffic demand generated by end users. In order to properly decide on BS (de)activation, traffic predictions are needed, and Artificial Neural Networks (ANN) are used for this purpose. Since critical BS (de)activation decisions are not taken in proximity of minima and maxima of the traffic patterns, high accuracy in the traffic estimation is not required at those times, but only close to the times when a decision is taken. This calls for careful processing of the ANN traffic predictions to increase the probability of correct decision. Numerical performance results in terms of energy saving and traffic lost due to incorrect BS deactivations are obtained by simulating algorithms for traffic predictions processing, using real traffic as input. Results suggest that good performance trade-offs can be achieved even in presence of non-negligible traffic prediction errors, if these forecasts are properly processed. The impact of forecast processing for dynamic resource allocation on the BS failure rate is also investigated. Results reveal that conservative approaches better prevent BSs from hardware failure. Nevertheless, the deployment of newer devices, designed for fast dynamic networks, allows the adoption of approaches which frequently activate and deactivate BSs, thus achieving higher energy saving.",science,80
10.1016/j.apenergy.2021.118085,filtered,Applied Energy,scopus,2022-02-01,sciencedirect,ship energy management system development and experimental evaluation utilizing marine loading cycles based on machine learning techniques,https://api.elsevier.com/content/abstract/scopus_id/85120648001,"In order to develop energy management systems for hybrid ship propulsion plants that are truly optimal and robust, it is important that the test conditions in experimental facilities are as close as possible to real world applications. In this context, a framework for the design and experimental evaluation of power-split control systems for ship propulsion is proposed. Using machine learning, data from ship operation are processed and 20 loading patterns are recognized; representative templates are extracted to be used as marine loading cycles in the energy management system development and testing. A ship propulsion model with wave disturbance is utilized to simulate realistic loading scenarios on the experimental facility. A predictive energy management system is presented, that controls the diesel engine and the electric motor/generator based on a strategy that defines the trade-off between fuel consumption and NOx emissions minimization. In addition the propeller load characteristics that are estimated and a speed predictor are utilized to aid the optimization within the 10 s prediction time window. A parametric simulation study is performed for the trade-off evaluation between fuel consumption and NOx emissions reduction potential of the control scheme. Finally, utilizing an extracted loading cycle, the energy management system is experimentally implemented and tested in real-time operation, where it has to cope with environmental disturbance rejection and follow the desired speed profile while performing the power-split control in respect to the fuel to NOx weighting strategy. Based on the experimental results in a hybrid diesel–electric marine powertrain with a 260 kW diesel engine and a 90 kW electric machine, fuel consumption and NOx emissions reduction by 6% and 8.5% respectively, were achieved over the tested profile. In this framework, the capabilities of the energy management system in realistic operation conditions can be exploited and evaluated.",science,81
10.1016/j.jwpe.2021.102452,filtered,Journal of Water Process Engineering,scopus,2022-02-01,sciencedirect,polyamine-modified polyacrylonitrile fibers for efficient removal of u(vi) from real fluorine-contained low-level radioactive wastewater,https://api.elsevier.com/content/abstract/scopus_id/85119972768,"It is of great significance to develop an adsorbent with high adsorption capacity and excellent resistance to anion and cation interference toward the removal of U(VI). Herein, a novel polyamine-modified polyacrylonitrile-based fiber (PANPA) has been synthesized through hydrothermal method, which can validly remove U(VI) from solution. Combined with mesoscopic, spectral characterization and simulation method, the removal behavior and mechanism of U(VI) from high fluorine uranium-containing wastewater by PANPA are systematically investigated. The results show that, based on the strong coordination principle of polyamine group and UO2
                     2+, PANPA can selectively remove U(VI) from wastewater. In addition, the q
                     
                        max
                      of 459.27 mg g−1 was more than that of many other adsorbent materials. More importantly, PANPA is not affected by high concentration of F−, and exhibits higher distribution coefficient (559,900 mL g−1) and removal efficiency (99.5%) to U(VI) than other coexisting ions in real wastewater. Furthermore, the column experiment was also implemented to remove U(VI). The results indicate that PANPA is a promising material to effectively remove U(VI) from real wastewater produced during the fabrication of nuclear fuel elements.",science,82
10.1016/j.postharvbio.2021.111741,filtered,Postharvest Biology and Technology,scopus,2022-01-01,sciencedirect,multi-output 1-dimensional convolutional neural networks for simultaneous prediction of different traits of fruit based on near-infrared spectroscopy,https://api.elsevier.com/content/abstract/scopus_id/85115232057,"In spectral data predictive modelling of fresh fruit, often the models are calibrated to predict multiple responses. A common method to deal with such a multi-response predictive modelling is the partial least-squares (PLS2) regression. Recently, deep learning (DL) has shown to outperform partial least-squares (PLS) approaches for single fruit traits prediction. The DL can also be adapted to perform multi-response modelling. This study presents an implementation of DL modelling for multi-response prediction for spectral data of fresh fruit. To show this, a real NIR data set related to SSC and MC measurements in pear fruit was used. Since DL models perform better with larger data sets, a data augmentation procedure was performed prior to data modelling. Furthermore, a comparative study was also performed between two of the most used DL architectures for spectral analysis, their multi-output and single-output variants and a classic baseline model using PLS2. A key point to note that all the DL modelling presented in this study is performed using novel automated optimisation tools such as Bayesian optimisation and Hyperband. The results showed that DL models can be easily adapted by changing the output of the fully connected layers to perform multi-response modelling. In comparison to the PLS2, the multi-response DL model showed ∼13 % lower root mean squared error (RMSE), showing the ease and superiority of handling multi-response by DL models for spectral calibration.",science,83
10.1109/lra.2022.3146945,filtered,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,"tacto: a fast, flexible, and open-source simulator for high-resolution vision-based tactile sensors",https://ieeexplore.ieee.org/document/9697425/,"Simulators perform an important role in prototyping, debugging, and benchmarking new advances in robotics and learning for control. Although many physics engines exist, some aspects of the real world are harder than others to simulate. One of the aspects that have so far eluded accurate simulation is touch sensing. To address this gap, we present TACTO – a fast, flexible, and open-source simulator for vision-based tactile sensors. This simulator allows to render realistic high-resolution touch readings at hundreds of frames per second, and can be easily configured to simulate different vision-based tactile sensors, including DIGIT and OmniTact. In this letter, we detail the principles that drove the implementation of TACTO and how they are reflected in its architecture. We demonstrate TACTO on a perceptual task, by learning to predict grasp stability using touch from 1 million grasps, and on a marble manipulation control task. Moreover, we provide a proof-of-concept that TACTO can be successfully used for Sim2Real applications. We believe that TACTO is a step towards the widespread adoption of touch sensing in robotic applications, and to enable machine learning practitioners interested in multi-modal learning and control.",robotics,84
10.1109/tro.2021.3084374,filtered,IEEE Transactions on Robotics,IEEE,2022-02-01 00:00:00,ieeexplore,cat-like jumping and landing of legged robots in low gravity using deep reinforcement learning,https://ieeexplore.ieee.org/document/9453856/,"In this article, we show that learned policies can be applied to solve legged locomotion control tasks with extensive flight phases, such as those encountered in space exploration. Using an off-the-shelf deep reinforcement learning algorithm, we train a neural network to control a jumping quadruped robot while solely using its limbs for attitude control. We present tasks of increasing complexity leading to a combination of 3-D (re)orientation and landing locomotion behaviors of a quadruped robot traversing simulated low-gravity celestial bodies. We show that our approach easily generalizes across these tasks and successfully trains policies for each case. Using sim-to-real transfer, we deploy trained policies in the real world on the SpaceBok robot placed on an experimental testbed designed for 2-D microgravity experiments. The experimental results demonstrate that repetitive controlled jumping and landing with natural agility is possible.",robotics,85
10.1109/tfuzz.2020.3033141,filtered,IEEE Transactions on Fuzzy Systems,IEEE,2022-01-01 00:00:00,ieeexplore,fuzzy double deep q-network-based gait pattern controller for humanoid robots,https://ieeexplore.ieee.org/document/9237162/,"In this article, the adaptive-network-based fuzzy inference system (ANFIS) is combined with the double deep <italic>Q</italic>-network (DDQN) to realize a fuzzy DDQN (FDDQN) such that a humanoid robot can generate a linear inverted pendulum model-based gait pattern in real time. The FDDQN not only allows the humanoid robot to correct the gait pattern instantly but also improves its stability. The proposed scheme is designed and implemented in a toddler-sized humanoid robot called Louis. First, four pressure sensors are installed on the bottom of the sole and one inertial measurement unit is set up on the trunk of the robot. A wireless communication chip is employed to transfer the data to a computer to determine the required parameters for the robot. Next, a control system based on the Linux operating system is developed. The values of the center of pressure and acceleration obtained with the ANFIS are adopted to train the DDQN. The proposed neural network comprises four layers, and the model is cautiously selected to avoid overfitting. The proposed scheme is verified using a robot simulator and then real-time-tested on Louis. The experimental results indicate that the FDDQN can provide the robot timely feedback during walking as well as helps it in adjusting the gait pattern independently. The balancing of the robot through effective dynamic feedback is similar to the balancing ability of an infant learning to walk.",robotics,86
10.1109/lra.2021.3129136,filtered,IEEE Robotics and Automation Letters,IEEE,2022-01-01 00:00:00,ieeexplore,ocrtoc: a cloud-based competition and benchmark for robotic grasping and manipulation,https://ieeexplore.ieee.org/document/9619915/,"In this paper, we propose a cloud-based benchmark for robotic grasping and manipulation, called the OCRTOC benchmark. The benchmark focuses on the object rearrangement problem, specifically table organization tasks. We provide a set of identical real robot setups and facilitate remote experiments of standardized table organization scenarios in varying difficulties. In this workflow, users upload their solutions to our remote server and their code is executed on the real robot setups and scored automatically. After each execution, the OCRTOC team resets the experimental setup manually. We also provide a simulation environment that researchers can use to develop and test their solutions. With the OCRTOC benchmark, we aim to lower the barrier of conducting reproducible research on robotic grasping and manipulation and accelerate progress in this field. Executing standardized scenarios on identical real robot setups allows us to quantify algorithm performances and achieve fair comparisons. Using this benchmark we held a competition in the 2020 International Conference on Intelligence Robots and Systems (IROS 2020). In total, 59 teams took part in this competition worldwide. We present the results and our observations of the 2020 competition, and discuss our adjustments and improvements for the upcoming OCRTOC 2021 competition. The homepage of the OCRTOC competition is <uri>www.ocrtoc.org</uri>, and the OCRTOC software package is available at <uri>https://github.com/OCRTOC/OCRTOC_software_package</uri>.",robotics,87
10.1109/lra.2022.3143289,filtered,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,visuotactile 6d pose estimation of an in-hand object using vision and tactile sensor data,https://ieeexplore.ieee.org/document/9682507/,"Knowledge of the 6D pose of an object can benefit in-hand object manipulation. Existing 6D pose estimation methods use vision data. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot’s grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this letter, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot’s hand.The main challenges of this research include 1) lack of standard representation for tactile sensor data, 2) fusion of sensor data from heterogeneous sources—vision and tactile, and 3) a need for large training datasets. To address these challenges, first, we propose use of point clouds to represent object surfaces that are in contact with the tactile sensor. Second, we present a network architecture based on pixel-wise dense fusion to fuse vision and tactile data to estimate the 6D pose of an object. Third, we extend NVIDIA’s Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and the corresponding tactile point clouds for 11 objects from the YCB Object and Model Set in Unreal Engine 4. We present results of simulated experiments suggesting that using tactile data in addition to vision data improves the 6D pose estimate of an in-hand object. We also present qualitative results of experiments in which we deploy our network on real physical robots showing successful transfer of a network trained on synthetic data to a real system.",robotics,88
10.1109/sii52469.2022.9708882,filtered,2022 IEEE/SICE International Symposium on System Integration (SII),IEEE,2022-01-12 00:00:00,ieeexplore,reinforcement learning based hierarchical control for path tracking of a wheeled bipedal robot with sim-to-real framework,https://ieeexplore.ieee.org/document/9708882/,"We propose a reinforcement learning (RL) based hierarchical control framework for path tracking of a wheeled bipedal robot. The framework consists of three control levels. 1) The high-level RL is used to obtain an optimal policy through trial and error in a simulated environment. 2) The middle-level Lyapunov-based non-linear controller is utilized to track a desired line with strong robustness and high stability. 3) The low-level PID-based controller is implemented to simultaneously achieve both balancing and velocity tracking for a physical wheeled bipedal robot in real world. Thanks to the middle-level controller, the offline trained policy in simulation can be directly employed on the physical robot in real time without tuning any parameters. Moreover, the high-level policy network is able to improve optimality and generality for the task of path tracking, as well to avoid the cumbersome process of manually tuning control gains. The experiment results in both simulation and real world demonstrate that the proposed hierarchical control framework can achieve quick, robust, and stable path tracking for a wheeled bipedal robot.",robotics,89
10.1109/lra.2022.3141150,filtered,IEEE Robotics and Automation Letters,IEEE,2022-04-01 00:00:00,ieeexplore,reve-ce: remote embodied visual referring expression in continuous environment,https://ieeexplore.ieee.org/document/9674225/,"Ithas always been a great challenge for the robot to navigate in the visual world following natural language instructions. Recently, several tasks such as the Vision-and-Language Navigation (VLN) and Remote Embodied Visual Referring Expression in Real Indoor Environments (REVERIE) are proposed trying to solve this challenge. And the most significant difference between VLN and REVERIE tasks is that REVERIE uses a higher guidance level instruction. However, the navigation process of REVERIE is implemented in a discrete environment, which is unrealistic in real world scenarios. To make the REVERIE task more consistent with the real physical world, we develop a new task of Remote Embodied Visual Referring Expression in Continuous Environment, namely REVE-CE, in which the agent executes a much longer sequence of low-level actions given language instructions. Furthermore, we propose a multi-branch cross modal attention (MBCMA) framework to solve the proposed REVE-CE task. Extensive experiments are conducted demonstrating that the proposed framework greatly outperforms the state-of-the-art VLN baselines and a new benchmark for the proposed REVE-CE task is built.",robotics,90
http://arxiv.org/abs/2201.05753v1,filtered,arxiv,arxiv,2022-01-15 00:00:00,arxiv,"parameter identification and motion control for articulated rigid body
  robots using differentiable position-based dynamics",http://arxiv.org/abs/2201.05753v1,"Simulation modeling of robots, objects, and environments is the backbone for
all model-based control and learning. It is leveraged broadly across dynamic
programming and model-predictive control, as well as data generation for
imitation, transfer, and reinforcement learning. In addition to fidelity, key
features of models in these control and learning contexts are speed, stability,
and native differentiability. However, many popular simulation platforms for
robotics today lack at least one of the features above. More recently,
position-based dynamics (PBD) has become a very popular simulation tool for
modeling complex scenes of rigid and non-rigid object interactions, due to its
speed and stability, and is starting to gain significant interest in robotics
for its potential use in model-based control and learning. Thus, in this paper,
we present a mathematical formulation for coupling position-based dynamics
(PBD) simulation and optimal robot design, model-based motion control and
system identification. Our framework breaks down PBD definitions and
derivations for various types of joint-based articulated rigid bodies. We
present a back-propagation method with automatic differentiation, which can
integrate both positional and angular geometric constraints. Our framework can
critically provide the native gradient information and perform gradient-based
optimization tasks. We also propose articulated joint model representations and
simulation workflow for our differentiable framework. We demonstrate the
capability of the framework in efficient optimal robot design, accurate
trajectory torque estimation and supporting spring stiffness estimation, where
we achieve minor errors. We also implement impedance control in real robots to
demonstrate the potential of our differentiable framework in human-in-the-loop
applications.",robotics,91
