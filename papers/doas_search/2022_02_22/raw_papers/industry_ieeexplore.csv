doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/SII52469.2022.9708896,Integration of a reconfigurable robotic workcell for assembly operations in automotive industry,IEEE,Conferences,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",https://ieeexplore.ieee.org/document/9708896/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore
10.1109/CCNC49033.2022.9700522,A Deep Reinforcement Learning-based Resource Management Scheme for SDN-MEC-supported XR Applications,IEEE,Conferences,"The Multi-Access Edge Computing (MEC) paradigm provides a promising solution for efficient computing services at edge nodes, such as base stations (BS), access points (AP), etc. By offloading highly intensive computational tasks to MEC servers, critical benefits in terms of reducing energy consumption at mobile devices and lowering processing latency can be achieved to support high Quality of Service (QoS) to many applications. Among the services which would benefit from MEC deployments are eXtended Reality (XR) applications which are receiving increasing attention from both academia and industry. XR applications have high resource requirements, mostly in terms of network bandwidth, computation and storage. Often these resources are not available in classic network architectures and especially not when XR applications are run by mobile devices. This paper leverages the concepts of Software Defined Networking (SDN) and Network Function Virtualization (NFV) to propose an innovative resource management scheme considering heterogeneous QoS requirements at the MEC server level. The resource assignment is formulated by employing a Deep Reinforcement Learning (DRL) technique to support high quality of XR services. The simulation results show how our proposed solution outperforms other state-of-the-art resource management-based schemes.",https://ieeexplore.ieee.org/document/9700522/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore
10.1109/SII52469.2022.9708896,Integration of a reconfigurable robotic workcell for assembly operations in automotive industry,IEEE,Conferences,"This paper deals with the integration of a flexible, reconfigurable work cell performing assembly of parts in the automotive industry. The unique feature of the developed cell is that it can function in two modes: a) entirely autonomously or b) in cooperation with a human, where the operation of the robot dynamically adapts to human actions. We have implemented technologies for online recognition of human intention and for real-time learning of robust assembly policies to achieve the desired outcome. This challenging goals dictate the integration of modern deep learning algorithms, statistical learning, and compliant robot control into a unique ROS-based robot control system.",https://ieeexplore.ieee.org/document/9708896/,2022 IEEE/SICE International Symposium on System Integration (SII),9-12 Jan. 2022,ieeexplore
10.1109/JIOT.2021.3096637,An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques,IEEE,Journals,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",https://ieeexplore.ieee.org/document/9481251/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/JIOT.2021.3079440,Deep-Learning-Enabled Automatic Optical Inspection for Module-Level Defects in LCD,IEEE,Journals,"Liquid crystal display (LCD) defects detection on module level is increasingly important for flat-panel displays (FPD) industry to increase the production capacity via machine vision technology. However, it is an overwhelmingly challenging issue due to various difficulties. This article discloses a practical automatic optical inspection (AOI) system consisting of hardware structure and software algorithm to detect module-level defects. The AOI system is the core component to build a distributed integrated inspection system with the help of the Internet of Things (IoT). Starting from the analysis of the challenges encountered in module-level defects inspection, a delicate photograph scheme is proposed to reveal different kinds of defects. In order to robustly work on the module-level defects detection with complex situations, a novel framework based on YOLOV3 detection unit is proposed in this article, including the preprocessing module, detection module, defects definition module, and interferences elimination module. To the best of our knowledge, this is the first work that designs a practical AOI system for module-level defects detection. In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on the manufacturing lines. The evaluation of the detection performance of the AOI system in comparison with a manual scheme indicates that the proposed system is practical for module-level defects detection. Currently, the proposed system has been deployed in a real-world LCD manufacturing line from a major player in the world.",https://ieeexplore.ieee.org/document/9429707/,IEEE Internet of Things Journal,"15 Jan.15, 2022",ieeexplore
10.1109/TCYB.2020.2964011,Hierarchical Granular Computing-Based Model and Its Reinforcement Structural Learning for Construction of Long-Term Prediction Intervals,IEEE,Journals,"As one of the most essential sources of energy, byproduct gas plays a pivotal role in the steel industry, for which the flow tendency is generally regarded as the guidance for planning and scheduling in real production. In order to obtain the numeric estimation along with its reliability, the construction of prediction intervals (PIs) is highly demanded by any practical applications as well as being long term for providing more information on future trends. Bearing this in mind, in this article, a hierarchical granular computing (HGrC)-based model is established for constructing long-term PIs, in which probabilistic modeling gives rise to a long horizon of numeric prediction, and the deployment of information granularities hierarchically extends the result to be interval-valued format. Considering that the structure of this model has a direct impact on its performance, Monte-Carlo search with a policy gradient technique is then applied for reinforcement structure learning. Compared with the existing methods, the size (length) of the granules in the proposed approach is unequal so that it becomes effective for not only periodic but also nonperiodic data. Furthermore, with the use of parallel strategy, the efficiency can be also guaranteed for real-world applications. The experimental results demonstrate that the proposed method is superior to other commonly encountered techniques, and the stability of the structure learning process behaves better when compared with other reinforcement learning approaches.",https://ieeexplore.ieee.org/document/8972350/,IEEE Transactions on Cybernetics,Jan. 2022,ieeexplore
10.1109/ACCESS.2021.3138990,"Microgrid Digital Twins: Concepts, Applications, and Future Trends",IEEE,Journals,"Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the <italic>digital twinning</italic> concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs’ operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed.",https://ieeexplore.ieee.org/document/9663369/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3086149,Toward a Web-Based Digital Twin Thermal Power Plant,IEEE,Journals,"As a crucial part of cyber-physical systems, a digital twin can process data, visualize processes, and send commands to the control system, which can be used for the research on thermal power plants that are vital for providing energy for manufacturing and industry, and also daily consumptions. This article introduces the methodologies and techniques toward a web-based digital twin thermal power plant. To implement a web-based digital twin thermal power plant, the architecture, modeling, control algorithm, rule model, and physical-digital twin control are explored. The potential functionalities of the web-based digital twin including real-time monitoring, visualization and interactions, and provided services for physical thermal plants and universities are also presented. A case study has been provided to illustrate the web-based digital twin power plant. The research in this article can provide potential solutions for web-based digital twin research and education.",https://ieeexplore.ieee.org/document/9446630/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/JIOT.2021.3096637,An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques,IEEE,Journals,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",https://ieeexplore.ieee.org/document/9481251/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/TII.2021.3093388,An Adaptive Deep Learning Framework for Fast Recognition of Integrated Circuit Markings,IEEE,Journals,"Fast recognition of integrated circuit (IC) markings is an essential but challenging task in electronic device manufacturing lines. This article develops an adaptive deep learning framework to facilitate the fast marking recognition of IC chips. The proposed framework contains four deep learning components, namely, chip segmentation, orientation correction, character extraction, and character recognition. The four components utilize different convolutional neural network structures to guarantee excellent adaptivity to a wide range of IC types and mitigate the influence of the low-quality chip images. In particular, the character extraction model is comprised of two improved label generation strategies and a proposed border correction method, so as to accommodate tiny scale chips and compactly printed markings. Experiments from the chip image dataset of a real laptop manufacturing line reached a recognition Precision of 91.73% and the Recall of 92.93%. The results demonstrate the superiority of the proposed framework to the state-of-the-art models and the effectiveness of handling a great diversity of chips with different scales, shapes, text fonts, marking colors, and layouts.",https://ieeexplore.ieee.org/document/9468418/,IEEE Transactions on Industrial Informatics,April 2022,ieeexplore
10.1109/JIOT.2021.3096637,An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques,IEEE,Journals,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",https://ieeexplore.ieee.org/document/9481251/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/JIOT.2021.3079440,Deep-Learning-Enabled Automatic Optical Inspection for Module-Level Defects in LCD,IEEE,Journals,"Liquid crystal display (LCD) defects detection on module level is increasingly important for flat-panel displays (FPD) industry to increase the production capacity via machine vision technology. However, it is an overwhelmingly challenging issue due to various difficulties. This article discloses a practical automatic optical inspection (AOI) system consisting of hardware structure and software algorithm to detect module-level defects. The AOI system is the core component to build a distributed integrated inspection system with the help of the Internet of Things (IoT). Starting from the analysis of the challenges encountered in module-level defects inspection, a delicate photograph scheme is proposed to reveal different kinds of defects. In order to robustly work on the module-level defects detection with complex situations, a novel framework based on YOLOV3 detection unit is proposed in this article, including the preprocessing module, detection module, defects definition module, and interferences elimination module. To the best of our knowledge, this is the first work that designs a practical AOI system for module-level defects detection. In order to demonstrate the effectiveness of the proposed method, extensive experiments have been conducted on the manufacturing lines. The evaluation of the detection performance of the AOI system in comparison with a manual scheme indicates that the proposed system is practical for module-level defects detection. Currently, the proposed system has been deployed in a real-world LCD manufacturing line from a major player in the world.",https://ieeexplore.ieee.org/document/9429707/,IEEE Internet of Things Journal,"15 Jan.15, 2022",ieeexplore
10.1109/TII.2021.3131355,Dynamic Network Slicing Orchestration for Remote Adaptation and Configuration in Industrial IoT,IEEE,Journals,"As an emerging and prospective paradigm, the industrial Internet of Things (IIoT) enable intelligent manufacturing through the interconnection and interaction of industrial production elements. The traditional approach that transmits data in a single physical network is undesirable because such a scheme cannot meet the network requirements of different industrial applications. To address this problem, in this article, we propose a network slicing orchestration system for remote adaptation and configuration in smart factories. We exploit software-defined networking and network functions virtualization to slice the physical network into multiple virtual networks. Different applications can use a dedicated network that meets its requirements with limited network resources with this scheme. To optimize network resource allocation and adapt to the dynamic network environments, we propose two heuristic algorithms with the assistance of artificial intelligence and the theoretical analysis of the network slicing system. We conduct numerical simulations to learn the performance of the proposed algorithms. Our experimental results show the effectiveness and efficiency of our proposed algorithms when multiple network services are concurrently running in the IIoT. Finally, we use a case study to verify the feasibility of the proposed network slicing orchestration system on a real smart manufacturing testbed.",https://ieeexplore.ieee.org/document/9629333/,IEEE Transactions on Industrial Informatics,June 2022,ieeexplore
10.1109/TII.2021.3128972,Guest Editorial: Security and Privacy of Federated Learning Solutions for Industrial IoT Applications,IEEE,Journals,"The Industrial Internet of Things (IoT) typically consists of several thousands of heterogeneous devices, such as sensors, actuators, access points, machinery, end-users' handheld equipment, and supply chain. In such an industrial environment, a multitude of data is generated from massive IoT devices, e.g., sensors for monitoring the environment, reading temperature, and gauging pressure. Most of the data are from delay-sensitive and computation-intensive applications, such as real-time manufacturing and automated diagnostics, which require big data analytics with low latency. Machine learning (ML) has been witnessed as an efficient solution for big data analytics. The majority of such ML algorithms are centralized methods, meaning that they first gather data from different users for use as a training dataset, which is placed on the ML server, and then build a model to classify the new data samples by applying the ML algorithms to this training dataset. However, the access to these datasets in the centralized ML methods raises concerns about data privacy for users. Federated learning (FL) was designed to protect data privacy to address a part of these issues. In FL, each participant uses a global training model without uploading their private data to a third-party server. Compared with the conventional ML, FL can preserve data security, especially in terms of participant data during the learning process. In particular, FL can also help in updating server-side data for the global model, and the participant is not required to provide their data. However, in FL, individual computing units may show abnormal actions, such as faulty software, hardware invasions, unreliable communication channels, and malicious samples deliberately crafting the model. To mitigate these challenges, we require robust policies to control the learning phases in FL. Motivated by the abovementioned issues, this special section solicits original research and practical contributions that advance the security and privacy of the FL solutions for industrial IoT applications as follows.",https://ieeexplore.ieee.org/document/9619939/,IEEE Transactions on Industrial Informatics,May 2022,ieeexplore
10.1109/TII.2021.3086149,Toward a Web-Based Digital Twin Thermal Power Plant,IEEE,Journals,"As a crucial part of cyber-physical systems, a digital twin can process data, visualize processes, and send commands to the control system, which can be used for the research on thermal power plants that are vital for providing energy for manufacturing and industry, and also daily consumptions. This article introduces the methodologies and techniques toward a web-based digital twin thermal power plant. To implement a web-based digital twin thermal power plant, the architecture, modeling, control algorithm, rule model, and physical-digital twin control are explored. The potential functionalities of the web-based digital twin including real-time monitoring, visualization and interactions, and provided services for physical thermal plants and universities are also presented. A case study has been provided to illustrate the web-based digital twin power plant. The research in this article can provide potential solutions for web-based digital twin research and education.",https://ieeexplore.ieee.org/document/9446630/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/JIOT.2021.3096637,An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques,IEEE,Journals,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",https://ieeexplore.ieee.org/document/9481251/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/JIOT.2021.3096637,An Integrated Framework for Health State Monitoring in a Smart Factory Employing IoT and Big Data Techniques,IEEE,Journals,"With the rapid growth in the use of various smart digital sensors, the Internet of Things (IoT) is a swiftly growing technology, which has contributed significantly to Industry 4.0 and the promotion of IoT-based smart factories, which gives rise to the new challenges of big data analytics and the implementation of machine learning techniques. This article proposes a practical framework that combines IoT techniques, a data lake, data analysis, and cloud computing for manufacturing equipment health-state monitoring and diagnostics in smart manufacturing. It addresses all the required aspects in the realization of such a system and allows the seamless interchange of data and functionality. Due to the specific characteristics of IoT sensor data (low quality, redundant multisources, partial labeling), we not only provide a promising framework but also give detailed insights and pay considerable attention to data quality issues. In the proposed framework, an ingestion procedure is designed to manage data collection, data security, data transformation and data storage issues. To improve the quality of IoT big data, a high-noise feature filter is proposed for automated preliminary sensor selection to suppress noisy features, followed by a noisy data cleaning module to provide good quality data for unbiased diagnosis modeling. The proposed framework can achieve seamless integration between IoT big data ingestion from the physical factory and machine learning-based data analytics in the virtual systems. It is built on top of the Apache Spark processing engine, being capable of working in both big data and real-time environments. One case study has been conducted based on a four-stage syngas compressor from real industries, which won the Best Industry Application of IoT at the BigInsights Data &amp; AI Innovation Awards. The experimental results demonstrate the effectiveness of both the proposed IoT-architecture and techniques to address the data quality issues.",https://ieeexplore.ieee.org/document/9481251/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/ACCESS.2022.3140595,Integrating Artificial Intelligence Internet of Things and 5G for Next-Generation Smartgrid: A Survey of Trends Challenges and Prospect,IEEE,Journals,"Smartgrid is a paradigm that was introduced into the conventional electricity network to enhance the way generation, transmission, and distribution networks interrelate. It involves the use of Information and Communication Technology (ICT) and other solution in fault and intrusion detection, mere monitoring of energy generation, transmission, and distribution. However, on one hand, the actual and earlier smartgrid, do not integrate more advanced features such as automatic decision making, security, scalability, self-healing and awareness, real-time monitoring, cross-layer compatibility, etc. On the other hand, the emergence of the digitalization of the communication infrastructure to support the economic sector which among them are energy generation and distribution grid with Artificial Intelligence (AI) and large-scale Machine to Machine (M2M) communication. With the future Massive Internet of Things (MIoT) as one of the pillars of 5G/6G network factory, it is the enabler to support the next generation smart grid by providing the needed platform that integrates, in addition to the communication infrastructure, the AI and IoT support, providing a multitenant system. This paper aim at presenting a comprehensive review of next smart grid research trends and technological background, discuss a futuristic next-generation smart grid driven by artificial intelligence (AI) and leverage by IoT and 5G. In addition, it discusses the challenges of next-generation smart-grids as it relate to the integration of AI, IoT and 5G for better smart grid architecture. Also, proffers possible solutions to some of the challenges and standards to support this novel trend. A corresponding future work will dwell on the implementation of the discussed integration of AI, IoT and 5G for next-generation smart grid, using Matlab, NS2/NS3, Open-daylight and Mininet as soft tools and compare with related literature.",https://ieeexplore.ieee.org/document/9672084/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3077865,A Data Stream Cleaning System Using Edge Intelligence for Smart City Industrial Environments,IEEE,Journals,"Cities are becoming smarter because of recent advances in artificial intelligence and the Internet of Things. However, heterogeneous data source in smart cities are continuously producing low-quality data, and ever-growing applications have greater real-time requirements. Therefore, this article proposes a data stream cleaning system (named DSCS) using edge intelligence to utilize the advantages of cloud servers and edge devices. The DSCS in edge nodes consists of a dynamic protocol interpreter, a structure parser, and a cleaning model activator. Meanwhile, a cloud server, which has pools of protocol and structured programs and cleaning models, supports the edge nodes to adapt massive heterogeneous data sources. To validate the proposed data cleaning system, we applied it to two scenarios: monitoring the injection molding machines, and base stations. The DSCS can have a stable processing time when the number of accessed edge devices is increased, as well as a good cleaning effect.",https://ieeexplore.ieee.org/document/9424956/,IEEE Transactions on Industrial Informatics,Feb. 2022,ieeexplore
10.1109/ACCESS.2022.3145236,Colonization by Algorithms in the Fourth Industrial Revolution,IEEE,Journals,"Data gathering and information processing have evolved to where it is almost unfathomable how much exists in digital form today. The generation thereof also no longer involves an explicit instruction from human to machine but can happen in real-time without human intervention. Artificial intelligence, machine learning, and cognitive computing are being utilized to mine data from a variety of sources. One such (profitable) source is human beings. Digital algorithms are designed to harness the power of technology to gather information. There has always been a sense of secrecy regarding some information (classified, top secret, confidential, etc.) but the Fourth Industrial Revolution has created the means to gather extremely large amounts of data, unknown to its sources. Anthropological value systems should become a fundamental foundation of digital algorithms. Such an approach could prevent software from exploiting its sources, especially minorities. Value systems together with ethics are guided by people’s culture. In ethically aligned algorithm design, value systems and digital technologies intersect and govern how algorithms are developed, the way data is engaged, and further the discipline of digital humanities.",https://ieeexplore.ieee.org/document/9690490/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3131355,Dynamic Network Slicing Orchestration for Remote Adaptation and Configuration in Industrial IoT,IEEE,Journals,"As an emerging and prospective paradigm, the industrial Internet of Things (IIoT) enable intelligent manufacturing through the interconnection and interaction of industrial production elements. The traditional approach that transmits data in a single physical network is undesirable because such a scheme cannot meet the network requirements of different industrial applications. To address this problem, in this article, we propose a network slicing orchestration system for remote adaptation and configuration in smart factories. We exploit software-defined networking and network functions virtualization to slice the physical network into multiple virtual networks. Different applications can use a dedicated network that meets its requirements with limited network resources with this scheme. To optimize network resource allocation and adapt to the dynamic network environments, we propose two heuristic algorithms with the assistance of artificial intelligence and the theoretical analysis of the network slicing system. We conduct numerical simulations to learn the performance of the proposed algorithms. Our experimental results show the effectiveness and efficiency of our proposed algorithms when multiple network services are concurrently running in the IIoT. Finally, we use a case study to verify the feasibility of the proposed network slicing orchestration system on a real smart manufacturing testbed.",https://ieeexplore.ieee.org/document/9629333/,IEEE Transactions on Industrial Informatics,June 2022,ieeexplore
10.1109/TII.2021.3081417,Early Classification of Industrial Alarm Floods Based on Semisupervised Learning,IEEE,Journals,"Early classification of ongoing alarm floods in industrial monitoring systems is crucial to provide a safe and efficient operation. It can provide online decision support for plant operators to take timely action, without waiting for the end of an alarm flood. In this article, a data-driven approach is proposed to address the early classification problem with unlabeled historical data. To prioritize earlier activated alarms and take advantage of the triggering time information of alarms, a vector representation called exponentially attenuated component (EAC) is used to represent alarm floods. This makes alarm sequences fit for different powerful machine learning algorithms, which can be easily implemented online with acceptable computational complexities. A method based on the time information of unlabeled historical alarm floods is formulated to determine the attenuation coefficient for EAC representation. With the Gaussian mixture model, an efficient semisupervised approach is proposed to provide an early classification of alarm floods using unlabeled historical data. It includes two phases: offline clustering and online classification, where the clustering step is automated in terms of choosing the optimal number of clusters by applying an efficient cluster validity index. The efficiency of the proposed method is validated by the Tennessee Eastman process benchmark and a real industrial dataset.",https://ieeexplore.ieee.org/document/9435070/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/TII.2021.3128972,Guest Editorial: Security and Privacy of Federated Learning Solutions for Industrial IoT Applications,IEEE,Journals,"The Industrial Internet of Things (IoT) typically consists of several thousands of heterogeneous devices, such as sensors, actuators, access points, machinery, end-users' handheld equipment, and supply chain. In such an industrial environment, a multitude of data is generated from massive IoT devices, e.g., sensors for monitoring the environment, reading temperature, and gauging pressure. Most of the data are from delay-sensitive and computation-intensive applications, such as real-time manufacturing and automated diagnostics, which require big data analytics with low latency. Machine learning (ML) has been witnessed as an efficient solution for big data analytics. The majority of such ML algorithms are centralized methods, meaning that they first gather data from different users for use as a training dataset, which is placed on the ML server, and then build a model to classify the new data samples by applying the ML algorithms to this training dataset. However, the access to these datasets in the centralized ML methods raises concerns about data privacy for users. Federated learning (FL) was designed to protect data privacy to address a part of these issues. In FL, each participant uses a global training model without uploading their private data to a third-party server. Compared with the conventional ML, FL can preserve data security, especially in terms of participant data during the learning process. In particular, FL can also help in updating server-side data for the global model, and the participant is not required to provide their data. However, in FL, individual computing units may show abnormal actions, such as faulty software, hardware invasions, unreliable communication channels, and malicious samples deliberately crafting the model. To mitigate these challenges, we require robust policies to control the learning phases in FL. Motivated by the abovementioned issues, this special section solicits original research and practical contributions that advance the security and privacy of the FL solutions for industrial IoT applications as follows.",https://ieeexplore.ieee.org/document/9619939/,IEEE Transactions on Industrial Informatics,May 2022,ieeexplore
10.1109/TII.2021.3130279,Imitation Learning Based Heavy-Hitter Scheduling Scheme in Software-Defined Industrial Networks,IEEE,Journals,"To realize flexible networking and on-demand topology reconstructing, software-defined industrial networks (SDINs) are increasingly embracing the flat structure. Similar to software defined networks (SDN), SDIN suffers from low traffic scheduling efficiency caused by large and imbalanced flows, known as the heavy hitters problem. Due to such heavy hitters, industrial networks may fail to satisfy application’s QoS requirements, which results in more severe damages. To improve flow scheduling efficiency under heavy hitters, this article introduces a novel imitation learning-based flow scheduling (ILFS) method. ILFS utilizes P4-based In-band Network Telemetry (INT) to collect fine-grained, real-time traffic data from SDIN’s data plane. In the control plane, it integrates the Generative Adversarial Imitation Learning (GAIL) model with a soft actor critic to preserve the experiences of flow, thereby better scheduling large flows. Our experiments thoroughly compare ILFS’s performance with several state-of-the-art traffic scheduling strategies. The results indicate that ILFS successfully controls the link bandwidth the utilization between 10<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and 80<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and significantly improves the average network throughput and link utilization rate.",https://ieeexplore.ieee.org/document/9626609/,IEEE Transactions on Industrial Informatics,June 2022,ieeexplore
10.1109/TII.2021.3093905,"Modeling, Detecting, and Mitigating Threats Against Industrial Healthcare Systems: A Combined Software Defined Networking and Reinforcement Learning Approach",IEEE,Journals,"The rise of the Internet of Medical Things introduces the healthcare ecosystem in a new digital era with multiple benefits, such as remote medical assistance, real-time monitoring, and pervasive control. However, despite the valuable healthcare services, this progression raises significant cybersecurity and privacy concerns. In this article, we focus our attention on the IEC 60 870-5-104 protocol, which is widely adopted in industrial healthcare systems. First, we investigate and assess the severity of the IEC 60 870-5-104 cyberattacks by providing a quantitative threat model, which relies on Attack Defence Trees and Common Vulnerability Scoring System v3.1. Next, we introduce an intrusion detection and prevention system (IDPS), which is capable of discriminating and mitigating automatically the IEC 60 870-5-104 cyberattacks. The proposed IDPS takes full advantage of the machine learning (ML) and software defined networking (SDN) technologies. ML is used to detect the IEC 60 870-5-104 cyberattacks, utilizing 1) Transmission Control Protocol/Internet Protocol network flow statistics and 2) IEC 60 870-5-104 payload flow statistics. On the other side, the automated mitigation is transformed into a multiarmed bandit problem, which is solved through a reinforcement learning method called Thomson sampling and SDN. The evaluation analysis demonstrates the efficiency of the proposed IDPS in terms of intrusion detection accuracy and automated mitigation performance. The detection accuracy and the F1 score of the proposed IDPS reach 0.831 and 0.8258, respectively, while the mitigation accuracy is calculated at 0.923.",https://ieeexplore.ieee.org/document/9470933/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/TII.2021.3124848,QoS and Privacy-Aware Routing for 5G-Enabled Industrial Internet of Things: A Federated Reinforcement Learning Approach,IEEE,Journals,"The development and maturity of the fifth-generation (5G) wireless communication technology provides the industrial Internet of Things (IIoT) with ultra-reliable and low-latency communications and massive machine-type communications, and forms a novel IIoT architecture, 5G-IIoT. However, massive data transfer between interconnecting industrial devices also brings new challenges for the 5G-IIoT routing process in terms of latency, load balancing, and data privacy, which affect the development of 5G-IIoT applications. Moreover, the existing research works on IIoT routing mostly focus on the latency and the reliability of the routing, disregarding the privacy security in the routing process. To solve these problems, in this article, we propose a quality of service (QoS) and data privacy-aware routing protocol, named QoSPR, for 5G-IIoT. Specifically, we improve the community detection algorithm info-map to divide the routing area into optimal subdomains, based on which the deep reinforcement learning algorithm is applied to build the gateway deployment model for latency reduction and load-balancing improvement. To eliminate areal differences, while considering the privacy preservation of the routing data, the federated reinforcement learning is applied to obtain the universal gateway deployment model. Then, based on the gateway deployment, the QoS and data privacy-aware routing is accomplished by establishing communications along the load-balancing routes of the minimum latencies. The validation experiment is conducted on real datasets. The experiment results show that as a data privacy-aware routing protocol, the QoSPR can significantly reduce both average latency and maximum latency, while maintaining excellent load balancing in 5G-IIoT.",https://ieeexplore.ieee.org/document/9601174/,IEEE Transactions on Industrial Informatics,June 2022,ieeexplore
10.1109/TII.2021.3077005,Verifiable Data Mining Against Malicious Adversaries in Industrial Internet of Things,IEEE,Journals,"With the large-scaled data generated from various interconnected machines and networks, Industrial Internet of Things (IIoT) provides unprecedented opportunities for facilitating data mining for industrial applications. The current IIoT architecture tends to adopt cloud computing for further timely mining IIoT data, however, the openness of security-critical IIoT becomes challenging in terms of unbearable privacy issues. Most existing privacy-preserving data mining (PPDM) techniques are designed to resist honest-but-curious adversaries (i.e., cloud servers and data users). Due to the complexity and openness in IIoT, PPDM is significantly difficult with the presence of malicious adversaries in IIoT who may incur incorrect learned models and inference results. To solve the aforementioned issues, we propose a framework to extend existing PPDM to guard linear regression against malicious behaviors (hereafter referred to as GuardLR). To prevent dishonest computations of cloud servers and inconsistent inputs of data users, we first design a privacy-preserving verifiable learning scheme for linear regression, which guarantees the correctness of learning. In this article, to avoid malicious clouds from returning incorrect inference results, we design a privacy-preserving prediction scheme with lightweight verification. Our formal security analysis shows that GuardLR achieves privacy, completeness, and soundness. Empirical experiments using real-world datasets also demonstrate that GuardLR has high computational efficiency and accuracy.",https://ieeexplore.ieee.org/document/9422191/,IEEE Transactions on Industrial Informatics,Feb. 2022,ieeexplore
10.1109/TNSE.2021.3075428,AI-Assisted Energy-Efficient and Intelligent Routing for Reconfigurable Wireless Networks,IEEE,Journals,"Intelligent network management for reconfigurable wireless networks such as 5G and beyond applications is crucial for many industrial applications, and has been the subject of ongoing research. This paper proposes an Artificial Intelligence(AI)-Assisted energy-efficient and intelligent routing, based on both energy efficiency prioritization and AI theory, in order to meet the exacting demands particularly in a real-world scenario. Specifically, to achieve network intelligence and quality of service (QoS), we use the AI theory to enhance routing adaptivity for intelligent network management in reconfigurable wireless networks. The software-defined networking idea is used to achieve this goal from a network-level perspective. To facilitate self-awareness, self-study, self-decision making, and self-configuration, we construct a mathematical model to convert the energy-efficient and intelligent routing problem into a multi-constraint optimal problem. Then an AI-assisted intelligent routing algorithm is designed to dynamically and adaptively change link weighs, which allows us to achieve optimal energy efficiency. Findings from our simulation suggest the potential of our proposed approach.",https://ieeexplore.ieee.org/document/9416866/,IEEE Transactions on Network Science and Engineering,1 Jan.-Feb. 2022,ieeexplore
10.1109/TNSE.2021.3055835,Cloud Versus Edge Deployment Strategies of Real-Time Face Recognition Inference,IEEE,Journals,"Choosing the appropriate deployment strategy for any Deep Learning (DL) project in a production environment has always been the most challenging problem for industrial practitioners. There are several conflicting constraints and controversial approaches when it comes to deployment. Among these problems, the deployment on cloud versus the deployment on edge represents a common dilemma. In a nutshell, each approach provides benefits where the other would have limitations. This paper presents a real-world case study on deploying a face recognition application using MTCNN detector and FaceNet recognizer. We report the challenges faced to decide on the best deployment strategy. We propose three inference architectures for the deployment, including cloud-based, edge-based, and hybrid. Furthermore, we evaluate the performance of face recognition inference on different cloud-based and edge-based GPU platforms. We consider different models of Jetson boards for the edge (Nano, TX2, Xavier NX, Xavier AGX) and various GPUs for the cloud (GTX 1080, RTX 2080Ti, RTX 2070, and RTX 8000). We also investigate the effect of deep learning model optimization using TensorRT and TFLite compared to a standard Tensorflow GPU model, and the effect of input resolution. We provide a benchmarking study for all these devices in terms of frames per second, execution times, energy and memory usages. After conducting a total of 294 experiments, the results demonstrate that the TensorRT optimization provides the fastest execution on all cloud and edge devices, at the expense of significantly larger energy consumption (up to +40% and +35% for edge and cloud devices, respectively, compared to Tensorflow). Whereas TFLite is the most efficient framework in terms of memory and power consumption, while providing significantly less (-4% to -62%) processing acceleration than TensorRT. <italic>Practitioners Note:</italic> The study reported in this paper presents the real-challenges that we faced during our development and deployment of a face-recognition application both on the edge and on the cloud, and the solutions we have developed to solve these problems. The code, results, and interactive analytic dashboards of this paper will be put public upon publication.",https://ieeexplore.ieee.org/document/9350171/,IEEE Transactions on Network Science and Engineering,1 Jan.-Feb. 2022,ieeexplore
10.1109/ACCESS.2022.3145236,Colonization by Algorithms in the Fourth Industrial Revolution,IEEE,Journals,"Data gathering and information processing have evolved to where it is almost unfathomable how much exists in digital form today. The generation thereof also no longer involves an explicit instruction from human to machine but can happen in real-time without human intervention. Artificial intelligence, machine learning, and cognitive computing are being utilized to mine data from a variety of sources. One such (profitable) source is human beings. Digital algorithms are designed to harness the power of technology to gather information. There has always been a sense of secrecy regarding some information (classified, top secret, confidential, etc.) but the Fourth Industrial Revolution has created the means to gather extremely large amounts of data, unknown to its sources. Anthropological value systems should become a fundamental foundation of digital algorithms. Such an approach could prevent software from exploiting its sources, especially minorities. Value systems together with ethics are guided by people’s culture. In ethically aligned algorithm design, value systems and digital technologies intersect and govern how algorithms are developed, the way data is engaged, and further the discipline of digital humanities.",https://ieeexplore.ieee.org/document/9690490/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3075464,DNNOff: Offloading DNN-Based Intelligent IoT Applications in Mobile Edge Computing,IEEE,Journals,"A deep neural network (DNN) has become increasingly popular in industrial Internet of Things scenarios. Due to high demands on computational capability, it is hard for DNN-based applications to directly run on intelligent end devices with limited resources. Computation offloading technology offers a feasible solution by offloading some computation-intensive tasks to the cloud or edges. Supporting such capability is not easy due to two aspects: <italic>Adaptability:</italic> offloading should dynamically occur among computation nodes. <italic>Effectiveness:</italic> it needs to be determined which parts are worth offloading. This article proposes a novel approach, called DNNOff. For a given DNN-based application, DNNOff first rewrites the source code to implement a special program structure supporting on-demand offloading and, at runtime, automatically determines the offloading scheme. We evaluated DNNOff on a real-world intelligent application, with three DNN models. Our results show that, compared with other approaches, DNNOff saves response time by 12.4–66.6% on average.",https://ieeexplore.ieee.org/document/9416166/,IEEE Transactions on Industrial Informatics,April 2022,ieeexplore
10.1109/ACCESS.2022.3149050,Design and Implementation of Traffic Generation Model and Spectrum Requirement Calculator for Private 5G Network,IEEE,Journals,"This paper proposes a neural 5G traffic generation model and a methodology for calculating the spectrum requirements of private 5G networks to provide various industrial communication services. To accurately calculate the spectral requirements, it is necessary to analyze the actual data volume and traffic type of industrial cases. However, because there is currently no suitable traffic model to test loads in private 5G networks, we have developed a generative adversarial network (GAN)-based traffic generator that can generate realistic traffic by learning actual traffic traces collected by mobile network operators. In addition, in the case of industrial applications, probability-based traffic models were used in parallel as there were not enough real data to be learned. The proposed 5G traffic generation model is combined with the proposed 5G spectrum calculation methodology, enabling more accurate spectrum requirements calculation through traffic simulation similar to a real-life environment. In this paper, the spectrum requirements are calculated differently according to two types of duplexing, namely frequency division duplexing (FDD) and time division duplexing (TDD). As a guide for companies aiming to provide advanced wireless connectivity for a wide variety of vertical industries using 5G networks, eight use cases defined in the 5G Alliance for Connected Industries and Automation (ACIA) white paper were simulated. The spectrum requirements were calculated under various simulation conditions considering varying traffic loads, deployment scenarios, and duplexing types. Various simulation results confirmed that a bandwidth of at least 22.0 MHz to a maximum of 397.8 MHz is required depending on the deployment scenario.",https://ieeexplore.ieee.org/document/9703352/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3131355,Dynamic Network Slicing Orchestration for Remote Adaptation and Configuration in Industrial IoT,IEEE,Journals,"As an emerging and prospective paradigm, the industrial Internet of Things (IIoT) enable intelligent manufacturing through the interconnection and interaction of industrial production elements. The traditional approach that transmits data in a single physical network is undesirable because such a scheme cannot meet the network requirements of different industrial applications. To address this problem, in this article, we propose a network slicing orchestration system for remote adaptation and configuration in smart factories. We exploit software-defined networking and network functions virtualization to slice the physical network into multiple virtual networks. Different applications can use a dedicated network that meets its requirements with limited network resources with this scheme. To optimize network resource allocation and adapt to the dynamic network environments, we propose two heuristic algorithms with the assistance of artificial intelligence and the theoretical analysis of the network slicing system. We conduct numerical simulations to learn the performance of the proposed algorithms. Our experimental results show the effectiveness and efficiency of our proposed algorithms when multiple network services are concurrently running in the IIoT. Finally, we use a case study to verify the feasibility of the proposed network slicing orchestration system on a real smart manufacturing testbed.",https://ieeexplore.ieee.org/document/9629333/,IEEE Transactions on Industrial Informatics,June 2022,ieeexplore
10.1109/TII.2021.3081417,Early Classification of Industrial Alarm Floods Based on Semisupervised Learning,IEEE,Journals,"Early classification of ongoing alarm floods in industrial monitoring systems is crucial to provide a safe and efficient operation. It can provide online decision support for plant operators to take timely action, without waiting for the end of an alarm flood. In this article, a data-driven approach is proposed to address the early classification problem with unlabeled historical data. To prioritize earlier activated alarms and take advantage of the triggering time information of alarms, a vector representation called exponentially attenuated component (EAC) is used to represent alarm floods. This makes alarm sequences fit for different powerful machine learning algorithms, which can be easily implemented online with acceptable computational complexities. A method based on the time information of unlabeled historical alarm floods is formulated to determine the attenuation coefficient for EAC representation. With the Gaussian mixture model, an efficient semisupervised approach is proposed to provide an early classification of alarm floods using unlabeled historical data. It includes two phases: offline clustering and online classification, where the clustering step is automated in terms of choosing the optimal number of clusters by applying an efficient cluster validity index. The efficiency of the proposed method is validated by the Tennessee Eastman process benchmark and a real industrial dataset.",https://ieeexplore.ieee.org/document/9435070/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/TMECH.2021.3065522,Federated Transfer Learning for Intelligent Fault Diagnostics Using Deep Adversarial Networks With Data Privacy,IEEE,Journals,"Intelligent data-driven machinery fault diagnosis methods have been popularly developed in the past years. While fairly high diagnosis accuracies have been obtained, large amounts of labeled training data are mostly required, which are difficult to collect in practice. The promising collaborative model training solution with multiple users poses high demands on data privacy due to conflict of interests. Furthermore, in the real industries, the data from different users can be usually collected from different machine operating conditions. The domain shift phenomenon and data privacy concern make the joint model training scheme quite challenging. To address this issue, a federated transfer learning method for fault diagnosis is proposed in this article. Different models can be used by different users to enhance data privacy. A federal initialization stage is introduced to keep similar data structures in distributed feature extractions, and a federated communication stage is further implemented using deep adversarial learning. A prediction consistency scheme is also adopted to increase model robustness. Experiments on two real-world datasets suggest the proposed federated transfer learning method is promising for real industrial applications.",https://ieeexplore.ieee.org/document/9376674/,IEEE/ASME Transactions on Mechatronics,Feb. 2022,ieeexplore
10.1109/TII.2021.3128972,Guest Editorial: Security and Privacy of Federated Learning Solutions for Industrial IoT Applications,IEEE,Journals,"The Industrial Internet of Things (IoT) typically consists of several thousands of heterogeneous devices, such as sensors, actuators, access points, machinery, end-users' handheld equipment, and supply chain. In such an industrial environment, a multitude of data is generated from massive IoT devices, e.g., sensors for monitoring the environment, reading temperature, and gauging pressure. Most of the data are from delay-sensitive and computation-intensive applications, such as real-time manufacturing and automated diagnostics, which require big data analytics with low latency. Machine learning (ML) has been witnessed as an efficient solution for big data analytics. The majority of such ML algorithms are centralized methods, meaning that they first gather data from different users for use as a training dataset, which is placed on the ML server, and then build a model to classify the new data samples by applying the ML algorithms to this training dataset. However, the access to these datasets in the centralized ML methods raises concerns about data privacy for users. Federated learning (FL) was designed to protect data privacy to address a part of these issues. In FL, each participant uses a global training model without uploading their private data to a third-party server. Compared with the conventional ML, FL can preserve data security, especially in terms of participant data during the learning process. In particular, FL can also help in updating server-side data for the global model, and the participant is not required to provide their data. However, in FL, individual computing units may show abnormal actions, such as faulty software, hardware invasions, unreliable communication channels, and malicious samples deliberately crafting the model. To mitigate these challenges, we require robust policies to control the learning phases in FL. Motivated by the abovementioned issues, this special section solicits original research and practical contributions that advance the security and privacy of the FL solutions for industrial IoT applications as follows.",https://ieeexplore.ieee.org/document/9619939/,IEEE Transactions on Industrial Informatics,May 2022,ieeexplore
10.1109/TII.2021.3130279,Imitation Learning Based Heavy-Hitter Scheduling Scheme in Software-Defined Industrial Networks,IEEE,Journals,"To realize flexible networking and on-demand topology reconstructing, software-defined industrial networks (SDINs) are increasingly embracing the flat structure. Similar to software defined networks (SDN), SDIN suffers from low traffic scheduling efficiency caused by large and imbalanced flows, known as the heavy hitters problem. Due to such heavy hitters, industrial networks may fail to satisfy application’s QoS requirements, which results in more severe damages. To improve flow scheduling efficiency under heavy hitters, this article introduces a novel imitation learning-based flow scheduling (ILFS) method. ILFS utilizes P4-based In-band Network Telemetry (INT) to collect fine-grained, real-time traffic data from SDIN’s data plane. In the control plane, it integrates the Generative Adversarial Imitation Learning (GAIL) model with a soft actor critic to preserve the experiences of flow, thereby better scheduling large flows. Our experiments thoroughly compare ILFS’s performance with several state-of-the-art traffic scheduling strategies. The results indicate that ILFS successfully controls the link bandwidth the utilization between 10<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and 80<inline-formula><tex-math notation=""LaTeX"">$\%$</tex-math></inline-formula> and significantly improves the average network throughput and link utilization rate.",https://ieeexplore.ieee.org/document/9626609/,IEEE Transactions on Industrial Informatics,June 2022,ieeexplore
10.1109/TIE.2021.3057030,KDnet-RUL: A Knowledge Distillation Framework to Compress Deep Neural Networks for Machine Remaining Useful Life Prediction,IEEE,Journals,"Machine remaining useful life (RUL) prediction is vital in improving the reliability of industrial systems and reducing maintenance cost. Recently, long short-term memory (LSTM) based algorithms have achieved state-of-the-art performance for RUL prediction due to their strong capability of modeling sequential sensory data. In many cases, the RUL prediction algorithms are required to be deployed on edge devices to support real-time decision making, reduce the data communication cost, and preserve the data privacy. However, the powerful LSTM-based methods which have high complexity cannot be deployed to edge devices with limited computational power and memory. To solve this problem, we propose a knowledge distillation framework, entitled KDnet-RUL, to compress a complex LSTM-based method for RUL prediction. Specifically, it includes a generative adversarial network based knowledge distillation (GAN-KD) for disparate architecture knowledge transfer, a learning-during-teaching based knowledge distillation (LDT-KD) for identical architecture knowledge transfer, and a sequential distillation upon LDT-KD for complicated datasets. We leverage simple and complicated datasets to verify the effectiveness of the proposed KDnet-RUL. The results demonstrate that the proposed method significantly outperforms state-of-the-art KD methods. The compressed model with 12.8 times less weights and 46.2 times less total float point operations even achieves a comparable performance with the complex LSTM model for RUL prediction.",https://ieeexplore.ieee.org/document/9351733/,IEEE Transactions on Industrial Electronics,Feb. 2022,ieeexplore
10.1109/ACCESS.2021.3138990,"Microgrid Digital Twins: Concepts, Applications, and Future Trends",IEEE,Journals,"Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the <italic>digital twinning</italic> concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs’ operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed.",https://ieeexplore.ieee.org/document/9663369/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3093905,"Modeling, Detecting, and Mitigating Threats Against Industrial Healthcare Systems: A Combined Software Defined Networking and Reinforcement Learning Approach",IEEE,Journals,"The rise of the Internet of Medical Things introduces the healthcare ecosystem in a new digital era with multiple benefits, such as remote medical assistance, real-time monitoring, and pervasive control. However, despite the valuable healthcare services, this progression raises significant cybersecurity and privacy concerns. In this article, we focus our attention on the IEC 60 870-5-104 protocol, which is widely adopted in industrial healthcare systems. First, we investigate and assess the severity of the IEC 60 870-5-104 cyberattacks by providing a quantitative threat model, which relies on Attack Defence Trees and Common Vulnerability Scoring System v3.1. Next, we introduce an intrusion detection and prevention system (IDPS), which is capable of discriminating and mitigating automatically the IEC 60 870-5-104 cyberattacks. The proposed IDPS takes full advantage of the machine learning (ML) and software defined networking (SDN) technologies. ML is used to detect the IEC 60 870-5-104 cyberattacks, utilizing 1) Transmission Control Protocol/Internet Protocol network flow statistics and 2) IEC 60 870-5-104 payload flow statistics. On the other side, the automated mitigation is transformed into a multiarmed bandit problem, which is solved through a reinforcement learning method called Thomson sampling and SDN. The evaluation analysis demonstrates the efficiency of the proposed IDPS in terms of intrusion detection accuracy and automated mitigation performance. The detection accuracy and the F1 score of the proposed IDPS reach 0.831 and 0.8258, respectively, while the mitigation accuracy is calculated at 0.923.",https://ieeexplore.ieee.org/document/9470933/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/TSMC.2020.3018102,Novel Fast and Automatic Condition Monitoring Strategy Based on Small Amount of Labeled Data,IEEE,Journals,"Signal-based automatic condition monitoring techniques are one of the effective ways to ensure the operational safety of modern industrial systems. Currently, the main challenge is to increase their effectiveness under noisy environment and with limited labeled data. In this article, a novel fast and automatic signal-based fault diagnosis procedure that does not require any kinds of machine learning algorithms is proposed. This procedure is based on the measurement of the similarity in the frequency domain between the collected data and a small amount of labeled reference signals (LRSs). The LRSs are obtained under various known operation conditions using fast Fourier transform (FFT) algorithm. The proposed approach is suitable for a real-time implementation and capable of successfully overcoming the challenge of condition monitoring of rotating machines under noisy environment and with limited labeled data. The merits, fastness, and also robustness against noise of the proposed technique are demonstrated experimentally through different practical applications, as well as compared to state-of-the-art procedures on a database of vibration signals measured under a variety of machine health and working conditions.",https://ieeexplore.ieee.org/document/9194082/,"IEEE Transactions on Systems, Man, and Cybernetics: Systems",Feb. 2022,ieeexplore
10.1109/TII.2021.3124848,QoS and Privacy-Aware Routing for 5G-Enabled Industrial Internet of Things: A Federated Reinforcement Learning Approach,IEEE,Journals,"The development and maturity of the fifth-generation (5G) wireless communication technology provides the industrial Internet of Things (IIoT) with ultra-reliable and low-latency communications and massive machine-type communications, and forms a novel IIoT architecture, 5G-IIoT. However, massive data transfer between interconnecting industrial devices also brings new challenges for the 5G-IIoT routing process in terms of latency, load balancing, and data privacy, which affect the development of 5G-IIoT applications. Moreover, the existing research works on IIoT routing mostly focus on the latency and the reliability of the routing, disregarding the privacy security in the routing process. To solve these problems, in this article, we propose a quality of service (QoS) and data privacy-aware routing protocol, named QoSPR, for 5G-IIoT. Specifically, we improve the community detection algorithm info-map to divide the routing area into optimal subdomains, based on which the deep reinforcement learning algorithm is applied to build the gateway deployment model for latency reduction and load-balancing improvement. To eliminate areal differences, while considering the privacy preservation of the routing data, the federated reinforcement learning is applied to obtain the universal gateway deployment model. Then, based on the gateway deployment, the QoS and data privacy-aware routing is accomplished by establishing communications along the load-balancing routes of the minimum latencies. The validation experiment is conducted on real datasets. The experiment results show that as a data privacy-aware routing protocol, the QoSPR can significantly reduce both average latency and maximum latency, while maintaining excellent load balancing in 5G-IIoT.",https://ieeexplore.ieee.org/document/9601174/,IEEE Transactions on Industrial Informatics,June 2022,ieeexplore
10.1109/TPDS.2021.3104255,Taskflow: A Lightweight Parallel and Heterogeneous Task Graph Computing System,IEEE,Journals,"Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community.",https://ieeexplore.ieee.org/document/9511796/,IEEE Transactions on Parallel and Distributed Systems,1 June 2022,ieeexplore
10.26599/TST.2020.9010055,Underground pipeline surveillance with an algorithm based on statistical time-frequency acoustic features,TUP,Journals,"Underground pipeline networks suffer from severe damage by earth-moving devices due to rapid urbanization. Thus, designing a round-the-clock intelligent surveillance system has become crucial and urgent. In this study, we develop an acoustic signal-based excavation device recognition system for underground pipeline protection. The front-end hardware system is equipped with an acoustic sensor array, an Analog-to-Digital Converter (ADC) module (ADS1274), and an industrial processor Advanced RISC Machine (ARM) cortex-A8 for signal collection and algorithm implementation. Then, a novel Statistical Time-Frequency acoustic Feature (STFF) is proposed, and a fast Extreme Learning Machine (ELM) is adopted as the classifier. Experiments on real recorded data show that the proposed STFF achieves better discriminative capability than the conventional acoustic cepstrum features. In addition, the surveillance platform is applicable for encountering big data owing to the fast learning speed of ELM.",https://ieeexplore.ieee.org/document/9552663/,Tsinghua Science and Technology,April 2022,ieeexplore
10.1109/TII.2021.3077005,Verifiable Data Mining Against Malicious Adversaries in Industrial Internet of Things,IEEE,Journals,"With the large-scaled data generated from various interconnected machines and networks, Industrial Internet of Things (IIoT) provides unprecedented opportunities for facilitating data mining for industrial applications. The current IIoT architecture tends to adopt cloud computing for further timely mining IIoT data, however, the openness of security-critical IIoT becomes challenging in terms of unbearable privacy issues. Most existing privacy-preserving data mining (PPDM) techniques are designed to resist honest-but-curious adversaries (i.e., cloud servers and data users). Due to the complexity and openness in IIoT, PPDM is significantly difficult with the presence of malicious adversaries in IIoT who may incur incorrect learned models and inference results. To solve the aforementioned issues, we propose a framework to extend existing PPDM to guard linear regression against malicious behaviors (hereafter referred to as GuardLR). To prevent dishonest computations of cloud servers and inconsistent inputs of data users, we first design a privacy-preserving verifiable learning scheme for linear regression, which guarantees the correctness of learning. In this article, to avoid malicious clouds from returning incorrect inference results, we design a privacy-preserving prediction scheme with lightweight verification. Our formal security analysis shows that GuardLR achieves privacy, completeness, and soundness. Empirical experiments using real-world datasets also demonstrate that GuardLR has high computational efficiency and accuracy.",https://ieeexplore.ieee.org/document/9422191/,IEEE Transactions on Industrial Informatics,Feb. 2022,ieeexplore
