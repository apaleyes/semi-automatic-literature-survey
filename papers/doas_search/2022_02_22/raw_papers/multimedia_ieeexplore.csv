doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database
10.1109/JSAC.2021.3118405,"Learning-Based Prediction, Rendering and Transmission for Interactive Virtual Reality in RIS-Assisted Terahertz Networks",IEEE,Journals,"The quality of experience (QoE) requirements of wireless virtual reality (VR) can only be satisfied with high data rate, high reliability, and low VR interaction latency. This high data rate over short transmission distances may be achieved via the abundant bandwidth in the terahertz (THz) band. However, THz waves experience severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with programmable reflecting elements. Meanwhile, the low VR interaction latency can be achieved with the mobile edge computing (MEC) network architecture due to its computation capabilities. Motivated by these considerations, in this paper, we propose an MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by taking into account the uplink viewpoint prediction and position transmission, the MEC rendering, and the downlink transmission. We propose two methods, which are referred to as centralized online gated recurrent unit (GRU) and distributed federated averaging (FedAvg), to predict the viewpoints of the VR users. In the uplink, an algorithm that integrates online long-short term memory (LSTM) and convolutional neural networks (CNN) is deployed to predict the locations and the line-of-sight and non-line-of-sight statuses of the VR users over time. In the downlink, we develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS under latency constraints. Simulation results show that our proposed learning architecture achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.",https://ieeexplore.ieee.org/document/9565222/,IEEE Journal on Selected Areas in Communications,Feb. 2022,ieeexplore
10.1109/CCNC49033.2022.9700515,CAVE-VR and Unity Game Engine for Visualizing City Scale 3D Meshes,IEEE,Conferences,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",https://ieeexplore.ieee.org/document/9700515/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore
10.1109/TNSRE.2022.3147772,Improving Automatic Control of Upper-Limb Prosthesis Wrists Using Gaze-Centered Eye Tracking and Deep Learning,IEEE,Journals,"Many upper-limb prostheses lack proper wrist rotation functionality, leading to users performing poor compensatory strategies, leading to overuse or abandonment. In this study, we investigate the validity of creating and implementing a data-driven predictive control strategy in object grasping tasks performed in virtual reality. We propose the idea of using gaze-centered vision to predict the wrist rotations of a user and implement a user study to investigate the impact of using this predictive control. We demonstrate that using this vision-based predictive system leads to a decrease in compensatory movement in the shoulder, as well as task completion time. We discuss the cases in which the virtual prosthesis with the predictive model implemented did and did not make a physical improvement in various arm movements. We also discuss the cognitive value in implementing such predictive control strategies into prosthetic controllers. We find that gaze-centered vision provides information about the intent of the user when performing object reaching and that the performance of prosthetic hands improves greatly when wrist prediction is implemented. Lastly, we address the limitations of this study in the context of both the study itself as well as any future physical implementations.",https://ieeexplore.ieee.org/document/9698069/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,2022,ieeexplore
10.1109/TMRB.2021.3129113,Learning a Generic Olfactory Search Strategy From Silk Moths by Deep Inverse Reinforcement Learning,IEEE,Journals,"Despite their simple nervous systems, insects efficiently search for and find sources of odorants. Hence, it is necessary to model and implement such behavior in artificial agents (robots), to enable them to detect dangerous substances such as drugs, gas leaks, and explosives. Previous studies have approached behavioral modeling with either statistical or machine-learning methods. In this study, we determined the behavior trajectories of male silk moths using a virtual reality (VR) system. We then modeled these trajectories as a Markov decision process (MDP) and employed inverse reinforcement learning (IRL) to learn their reward function. Furthermore, we estimated the optimal policy from the learned reward function. We then conducted olfactory search simulations and determined that the IRL-based policy could locate odor sources with a high success rate. This was also investigated under environmental conditions different from those faced by real moths on the VR system. The obtained results indicate that IRL can generically represent olfactory search strategies that are adaptable to various environments.",https://ieeexplore.ieee.org/document/9619462/,IEEE Transactions on Medical Robotics and Bionics,Feb. 2022,ieeexplore
10.1109/JSAC.2021.3118405,"Learning-Based Prediction, Rendering and Transmission for Interactive Virtual Reality in RIS-Assisted Terahertz Networks",IEEE,Journals,"The quality of experience (QoE) requirements of wireless virtual reality (VR) can only be satisfied with high data rate, high reliability, and low VR interaction latency. This high data rate over short transmission distances may be achieved via the abundant bandwidth in the terahertz (THz) band. However, THz waves experience severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with programmable reflecting elements. Meanwhile, the low VR interaction latency can be achieved with the mobile edge computing (MEC) network architecture due to its computation capabilities. Motivated by these considerations, in this paper, we propose an MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by taking into account the uplink viewpoint prediction and position transmission, the MEC rendering, and the downlink transmission. We propose two methods, which are referred to as centralized online gated recurrent unit (GRU) and distributed federated averaging (FedAvg), to predict the viewpoints of the VR users. In the uplink, an algorithm that integrates online long-short term memory (LSTM) and convolutional neural networks (CNN) is deployed to predict the locations and the line-of-sight and non-line-of-sight statuses of the VR users over time. In the downlink, we develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS under latency constraints. Simulation results show that our proposed learning architecture achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.",https://ieeexplore.ieee.org/document/9565222/,IEEE Journal on Selected Areas in Communications,Feb. 2022,ieeexplore
10.1109/JIOT.2021.3097768,User-Aware and Flexible Proactive Caching Using LSTM and Ensemble Learning in IoT-MEC Networks,IEEE,Journals,"To meet the stringent demands of emerging Internet-of-Things (IoT) applications, such as smart home, smart city, and virtual reality in 5G/6G IoT networks, edge content caching for mobile/multiaccess edge computing (MEC) has been identified as a promising approach to improve the quality of services in terms of latency and energy consumption. However, the limitations of cache capacity make it difficult to develop an effective common caching framework that satisfies diverse user preferences. In this article, we propose a new content caching strategy that maximizes the cache hit ratio through flexible prediction in dynamically changing network and user environments. It is based on a hierarchical deep learning architecture: long short-term memory (LSTM)-based local learning and ensemble-based meta-learning. First, as a local learning model, we employ an LSTM method with seasonal-trend decomposition using loess (STL)-based preprocessing. It identifies the attributes for demand prediction on the contents in various demographic user groups. Second, as a metalearning model, we employ a regression-based ensemble learning method, which uses an online convex optimization framework and exhibits sublinear “regret” performance. It orchestrates the obtained multiple demographic user preferences into a unified caching strategy in real time. Extensive experiments were conducted on the popular MovieLens data sets. It was shown that the proposed control provides up to a 30% higher cache hit ratio than conventional representative algorithms and a near-optimal cache hit ratio within approximately 9% of the optimal caching scheme with perfect prior knowledge of content popularity. The proposed learning and caching control can be implemented as a core function of the 5G/6G standard’s network data analytic function (NWDAF) module.",https://ieeexplore.ieee.org/document/9488291/,IEEE Internet of Things Journal,"1 March1, 2022",ieeexplore
10.1109/CCNC49033.2022.9700515,CAVE-VR and Unity Game Engine for Visualizing City Scale 3D Meshes,IEEE,Conferences,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",https://ieeexplore.ieee.org/document/9700515/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore
10.1109/WACVW54805.2022.00069,AA3DNet: Attention Augmented Real Time 3D Object Detection,IEEE,Conferences,"In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Bird’s Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at &gt;30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",https://ieeexplore.ieee.org/document/9707544/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),4-8 Jan. 2022,ieeexplore
10.1109/CCNC49033.2022.9700515,CAVE-VR and Unity Game Engine for Visualizing City Scale 3D Meshes,IEEE,Conferences,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",https://ieeexplore.ieee.org/document/9700515/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore
10.1109/WACV51458.2022.00380,Fast-CLOCs: Fast Camera-LiDAR Object Candidates Fusion for 3D Object Detection,IEEE,Conferences,"When compared to single modality approaches, fusion-based object detection methods often require more complex models to integrate heterogeneous sensor data, and use more GPU memory and computational resources. This is particularly true for camera-LiDAR based multimodal fusion, which may require three separate deep-learning networks and/or processing pipelines that are designated for the visual data, LiDAR data, and for some form of a fusion framework. In this paper, we propose Fast Camera-LiDAR Object Candidates (Fast-CLOCs) fusion network that can run high-accuracy fusion-based 3D object detection in near real-time. Fast-CLOCs operates on the output candidates before Non-Maximum Suppression (NMS) of any 3D detector, and adds a lightweight 3D detector-cued 2D image detector (3D-Q-2D) to extract visual features from the image domain to improve 3D detections significantly. The 3D detection candidates are shared with the proposed 3D-Q-2D image detector as proposals to reduce the network complexity drastically. The superior experimental results of our Fast-CLOCs on the challenging KITTI and nuScenes datasets illustrate that our Fast-CLOCs outperforms state-of-the-art fusion-based 3D object detection approaches. We will release the code upon publication.",https://ieeexplore.ieee.org/document/9706631/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore
10.1109/WACV51458.2022.00037,Occlusion Resistant Network for 3D Face Reconstruction,IEEE,Conferences,"3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of 0.77 against 5.84 of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of 6.67 in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.",https://ieeexplore.ieee.org/document/9706716/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore
10.1109/LRA.2022.3142439,Anytime 3D Object Reconstruction Using Multi-Modal Variational Autoencoder,IEEE,Journals,"For effective human-robot teaming, it is important for the robots to be able to share their visual perception with the human operators. In a harsh remote collaboration setting, data compression techniques such as autoencoder can be utilized to obtain and transmit the data in terms of latent variables in a compact form. In addition, to ensure real-time runtime performance even under unstable environments, an anytime estimation approach is desired that can reconstruct the full contents from incomplete information. In this context, we propose a method for imputation of latent variables whose elements are partially lost. To achieve the anytime property with only a few dimensions of variables, exploiting prior information of the category-level is essential. A prior distribution used in variational autoencoders is simply assumed to be isotropic Gaussian regardless of the labels of each training datapoint. This type of flattened prior makes it difficult to perform imputation from the category-level distributions. We overcome this limitation by exploiting a category-specific multi-modal prior distribution in the latent space. The missing elements of the partially transferred data can be sampled, by finding a specific modal according to the remaining elements. Since the method is designed to use partial elements for anytime estimation, it can also be applied for data over-compression. Based on the experiments on the ModelNet and Pascal3D datasets, the proposed approach shows consistently superior performance over autoencoder and variational autoencoder up to 70% data loss. The software is open source and is available from our repository<sup>1</sup>.",https://ieeexplore.ieee.org/document/9681277/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/ACCESS.2022.3140332,Spatial-Importance-Based Computation Scheme for Real-Time Object Detection From 3D Sensor Data,IEEE,Journals,"Three-dimensional (3D) sensor networks using multiple light-detection-and-ranging (LIDAR) sensors are good for smart monitoring of spots, such as intersections, with high potential risk of road-traffic accidents. The image sensors must share the strictly limited computation capacity of an edge computer. To have the computation speeds required from real-time applications, the system must have a short computation delay while maintaining the quality of the output, e.g., the accuracy of the object detection. This paper proposes a spatial-importance-based computation scheme that can be implemented on an edge computer of image-sensor networks composed of 3D sensors. The scheme considers regions where objects exist as more likely to be ones of higher spatial importance. It processes point-cloud data from each region according to the spatial importance of that region. By prioritizing regions with high spatial importance, it shortens the computation delay involved in the object detection. A point-cloud dataset obtained by a moving car equipped with a LIDAR unit was used to numerically evaluate the proposed scheme. The results indicate that the scheme shortens the delay in object detection.",https://ieeexplore.ieee.org/document/9668921/,IEEE Access,2022,ieeexplore
10.1109/WACVW54805.2022.00069,AA3DNet: Attention Augmented Real Time 3D Object Detection,IEEE,Conferences,"In this work, we address the problem of 3D object detection from point cloud data in real time. For autonomous vehicles to work, it is very important for the perception component to detect the real world objects with both high accuracy and fast inference. We propose a novel neural network architecture along with the training and optimization details for detecting 3D objects using point cloud data. We present anchor design along with custom loss functions used in this work. A combination of spatial and channel wise attention module is used in this work. We use the Kitti 3D Bird’s Eye View dataset for benchmarking and validating our results. Our method surpasses previous state of the art in this domain both in terms of average precision and speed running at &gt;30 FPS. Finally, we present the ablation study to demonstrate that the performance of our network is generalizable. This makes it a feasible option to be deployed in real time applications like self driving cars.",https://ieeexplore.ieee.org/document/9707544/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),4-8 Jan. 2022,ieeexplore
10.1109/WACV51458.2022.00380,Fast-CLOCs: Fast Camera-LiDAR Object Candidates Fusion for 3D Object Detection,IEEE,Conferences,"When compared to single modality approaches, fusion-based object detection methods often require more complex models to integrate heterogeneous sensor data, and use more GPU memory and computational resources. This is particularly true for camera-LiDAR based multimodal fusion, which may require three separate deep-learning networks and/or processing pipelines that are designated for the visual data, LiDAR data, and for some form of a fusion framework. In this paper, we propose Fast Camera-LiDAR Object Candidates (Fast-CLOCs) fusion network that can run high-accuracy fusion-based 3D object detection in near real-time. Fast-CLOCs operates on the output candidates before Non-Maximum Suppression (NMS) of any 3D detector, and adds a lightweight 3D detector-cued 2D image detector (3D-Q-2D) to extract visual features from the image domain to improve 3D detections significantly. The 3D detection candidates are shared with the proposed 3D-Q-2D image detector as proposals to reduce the network complexity drastically. The superior experimental results of our Fast-CLOCs on the challenging KITTI and nuScenes datasets illustrate that our Fast-CLOCs outperforms state-of-the-art fusion-based 3D object detection approaches. We will release the code upon publication.",https://ieeexplore.ieee.org/document/9706631/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore
10.1109/WACV51458.2022.00037,Occlusion Resistant Network for 3D Face Reconstruction,IEEE,Conferences,"3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of 0.77 against 5.84 of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of 6.67 in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.",https://ieeexplore.ieee.org/document/9706716/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore
10.1109/OJITS.2021.3139393,NAPC: A Neural Algorithm for Automated Passenger Counting in Public Transport on a Privacy-Friendly Dataset,IEEE,Journals,"Real-time load information in public transport is of high importance for both passengers and service providers. Neural algorithms have shown a high performance on various object counting tasks and play a continually growing methodological role in developing automated passenger counting systems. However, the publication of public-space video footage is often contradicted by legal and ethical considerations to protect the passengers’ privacy. This work proposes an end-to-end Long Short-Term Memory network with a problem-adapted cost function that learned to count boarding and alighting passengers on a publicly available, comprehensive dataset of approx.13,000 manually annotated low-resolution 3D LiDAR video recordings (depth information only) from the doorways of a regional train. These depth recordings do not allow the identification of single individuals. For each door opening phase, the trained models predict the correct passenger count (ranging from 0 to 67) in approx.96% of boarding and alighting, respectively. Repeated training with different training and validation sets confirms the independence of this result from a specific test set.",https://ieeexplore.ieee.org/document/9665722/,IEEE Open Journal of Intelligent Transportation Systems,2022,ieeexplore
10.1109/LSP.2022.3144074,Rethinking Lightweight: Multiple Angle Strategy for Efficient Video Action Recognition,IEEE,Journals,"Video action recognition task involves modeling spatiotemporal information, and efficiency is critical to capture spatiotemporal dependencies in the video. Most existing models rely on optical flow information to capture the dynamic visual tempos between consecutive video frames. Although impressive performance can be achieved by combining optical flow with RGB, the time-consuming nature of optical flow computation cannot be ignored. Moreover, 3D CNN has successfully modeled spatiotemporal information, yet the enormous computational volume is unsuitable for real-time action recognition. In this letter, we propose a novel lightweight video feature extraction strategy that achieves better recognition performance with lower FLOPs. In particular, we perform convolution on the video cube from three orthogonal angles to learn its appearance and motion features. Compared with the computational volume of 3D CNN, our proposed method is more economical and thus meets the lightweight requirements. Extensive experimental results on public Something Something-V1<inline-formula><tex-math notation=""LaTeX"">$\&amp;$</tex-math></inline-formula>V2 and Diving48 datasets show our approach achieves the state-of-the-art performance.",https://ieeexplore.ieee.org/document/9684992/,IEEE Signal Processing Letters,2022,ieeexplore
10.1109/LRA.2022.3147337,Sim2Air - Synthetic Aerial Dataset for UAV Monitoring,IEEE,Journals,"In this letter, we propose a novel approach to generate a synthetic aerial dataset for application in UAV monitoring. We propose to accentuate shape-based object representation by applying texture randomization. A diverse dataset with photorealism in all parameters such as shape, pose, lighting, scale, viewpoint, etc. except for atypical textures is created in a 3D modelling software Blender. Our approach specifically targets two conditions in aerial images where texture of objects is difficult to detect, namely challenging illumination and objects occupying only a small portion of the image. Experimental evaluation of YOLO and Faster R-CNN detectors trained on synthetic data with randomized textures confirmed our approach by increasing the mAP value (17 and 3.7 percentage points for YOLO; 20 and 1.1 percentage points for Faster R-CNN) on two test datasets of real images, both containing UAV-to-UAV images with motion blur. Testing on different domains, we conclude that the more the generalisation ability is put to the test, the more apparent are the advantages of the shape-based representation.",https://ieeexplore.ieee.org/document/9699390/,IEEE Robotics and Automation Letters,April 2022,ieeexplore
10.1109/LRA.2021.3116700,Sim2real Learning of Obstacle Avoidance for Robotic Manipulators in Uncertain Environments,IEEE,Journals,"Obstacle avoidance for robotic manipulators can be challenging when they operate in unstructured environments. This problem is probed with the sim-to-real (sim2real) deep reinforcement learning, such that a moving policy of the robotic arm is learnt in a simulator and then adapted to the real world. However, the problem of sim2real adaptation is notoriously difficult. To this end, this work proposes (1) a unified representation of obstacles and targets to capture the underlying dynamics of the environment while allowing generalization to unseen goals and (2) a flexible end-to-end model combining the unified representation with the deep reinforcement learning control module that can be trained by interacting with the environment. Such a representation is agnostic to the shape and appearance of the underlying objects, which simplifies and unifies the scene representation in both simulated and real worlds. We implement this idea with a vision-based actor-critic framework by devising a bounding box predictor module. The predictor estimates the 3D bounding boxes of obstacles and targets from the RGB-D input. The features extracted by the predictor are fed into the policy network, and all the modules are jointly trained. This makes the policy learn object-aware scene representation, which leads to a data-efficient learning of the obstacle avoidance policy. Our experiments in simulated environment and the real-world show that the end-to-end model of the unified representation achieves better sim2real adaption and scene generalization than state-of-the-art techniques.",https://ieeexplore.ieee.org/document/9555228/,IEEE Robotics and Automation Letters,Jan. 2022,ieeexplore
10.1109/ACCESS.2022.3140332,Spatial-Importance-Based Computation Scheme for Real-Time Object Detection From 3D Sensor Data,IEEE,Journals,"Three-dimensional (3D) sensor networks using multiple light-detection-and-ranging (LIDAR) sensors are good for smart monitoring of spots, such as intersections, with high potential risk of road-traffic accidents. The image sensors must share the strictly limited computation capacity of an edge computer. To have the computation speeds required from real-time applications, the system must have a short computation delay while maintaining the quality of the output, e.g., the accuracy of the object detection. This paper proposes a spatial-importance-based computation scheme that can be implemented on an edge computer of image-sensor networks composed of 3D sensors. The scheme considers regions where objects exist as more likely to be ones of higher spatial importance. It processes point-cloud data from each region according to the spatial importance of that region. By prioritizing regions with high spatial importance, it shortens the computation delay involved in the object detection. A point-cloud dataset obtained by a moving car equipped with a LIDAR unit was used to numerically evaluate the proposed scheme. The results indicate that the scheme shortens the delay in object detection.",https://ieeexplore.ieee.org/document/9668921/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3086149,Toward a Web-Based Digital Twin Thermal Power Plant,IEEE,Journals,"As a crucial part of cyber-physical systems, a digital twin can process data, visualize processes, and send commands to the control system, which can be used for the research on thermal power plants that are vital for providing energy for manufacturing and industry, and also daily consumptions. This article introduces the methodologies and techniques toward a web-based digital twin thermal power plant. To implement a web-based digital twin thermal power plant, the architecture, modeling, control algorithm, rule model, and physical-digital twin control are explored. The potential functionalities of the web-based digital twin including real-time monitoring, visualization and interactions, and provided services for physical thermal plants and universities are also presented. A case study has been provided to illustrate the web-based digital twin power plant. The research in this article can provide potential solutions for web-based digital twin research and education.",https://ieeexplore.ieee.org/document/9446630/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/ACCESS.2021.3138990,"Microgrid Digital Twins: Concepts, Applications, and Future Trends",IEEE,Journals,"Following the fourth industrial revolution, and with the recent advances in information and communication technologies, the <italic>digital twinning</italic> concept is attracting the attention of both academia and industry worldwide. A microgrid digital twin (MGDT) refers to the digital representation of a microgrid (MG), which mirrors the behavior of its physical counterpart by using high-fidelity models and simulation platforms as well as real-time bi-directional data exchange with the real twin. With the massive deployment of sensor networks and IoT technologies in MGs, a huge volume of data is continuously generated, which contains valuable information to enhance the performance of MGs. MGDTs provide a powerful tool to manage the huge historical data and real-time data stream in an efficient and secure manner and support MGs’ operation by assisting in their design, operation management, and maintenance. In this paper, the concept of the digital twin (DT) and its key characteristics are introduced. Moreover, a workflow for establishing MGDTs is presented. The goal is to explore different applications of DTs in MGs, namely in design, control, operator training, forecasting, fault diagnosis, expansion planning, and policy-making. Besides, an up-to-date overview of studies that applied the DT concept to power systems and specifically MGs is provided. Considering the significance of situational awareness, security, and resilient operation for MGs, their potential enhancement in light of digital twinning is thoroughly analyzed and a conceptual model for resilient operation management of MGs is presented. Finally, future trends in MGDTs are discussed.",https://ieeexplore.ieee.org/document/9663369/,IEEE Access,2022,ieeexplore
10.1109/TII.2021.3086149,Toward a Web-Based Digital Twin Thermal Power Plant,IEEE,Journals,"As a crucial part of cyber-physical systems, a digital twin can process data, visualize processes, and send commands to the control system, which can be used for the research on thermal power plants that are vital for providing energy for manufacturing and industry, and also daily consumptions. This article introduces the methodologies and techniques toward a web-based digital twin thermal power plant. To implement a web-based digital twin thermal power plant, the architecture, modeling, control algorithm, rule model, and physical-digital twin control are explored. The potential functionalities of the web-based digital twin including real-time monitoring, visualization and interactions, and provided services for physical thermal plants and universities are also presented. A case study has been provided to illustrate the web-based digital twin power plant. The research in this article can provide potential solutions for web-based digital twin research and education.",https://ieeexplore.ieee.org/document/9446630/,IEEE Transactions on Industrial Informatics,March 2022,ieeexplore
10.1109/CCNC49033.2022.9700636,Balancing Latency and Accuracy on Deep Video Analytics at the Edge,IEEE,Conferences,"Real-time deep video analytic at the edge is an enabling technology for emerging applications, such as vulnerable road user detection for autonomous driving, which requires highly accurate results of model inference within a low latency. In this paper, we investigate the accuracy-latency trade-off in the design and implementation of real-time deep video analytic at the edge. Without loss of generality, we select the widely used YOLO-based object detection and WebRTC-based video streaming for case study. Here, the latency consists of both networking latency caused by video streaming and the processing latency for video encoding/decoding and model inference. We conduct extensive measurements to figure out how the dynamically changing settings of video streaming affect the achieved latency, the quality of video, and further the accuracy of model inference. Based on the findings, we propose a mechanism for adapting video streaming settings (i.e. bitrate, resolution) online to optimize the accuracy of video analytic within latency constraints. The mechanism has proved, through a simulated setup, to be efficient in searching the optimal settings.",https://ieeexplore.ieee.org/document/9700636/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore
10.1109/WACV51458.2022.00308,Multi-branch Neural Networks for Video Anomaly Detection in Adverse Lighting and Weather Conditions,IEEE,Conferences,"Automated anomaly detection in surveillance videos has attracted much interest as it provides a scalable alternative to manual monitoring. Most existing approaches achieve good performance on clean benchmark datasets recorded in well-controlled environments. However, detecting anomalies is much more challenging in the real world. Adverse weather conditions like rain or changing brightness levels cause a significant shift in the input data distribution, which in turn can lead to the detector model incorrectly reporting high anomaly scores. Additionally, surveillance cameras are usually deployed in evolving environments such as a city street of which the appearance changes over time because of seasonal changes or roadworks. The anomaly detection model will need to be updated periodically to deal with these issues. In this paper, we introduce a multi-branch model that is equipped with a trainable preprocessing step and multiple identical branches for detecting anomalies during day and night as well as in sunny and rainy conditions. We experimentally validate our approach on a distorted version of the Avenue dataset and provide qualitative results on real-world surveillance camera data. Experimental results show that our method outperforms the existing methods in terms of detection accuracy while being faster and more robust on scenes with varying visibility.",https://ieeexplore.ieee.org/document/9706717/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore
10.1109/JIOT.2021.3089080,Audio-Visual Autoencoding for Privacy-Preserving Video Streaming,IEEE,Journals,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",https://ieeexplore.ieee.org/document/9453730/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/LSP.2022.3144074,Rethinking Lightweight: Multiple Angle Strategy for Efficient Video Action Recognition,IEEE,Journals,"Video action recognition task involves modeling spatiotemporal information, and efficiency is critical to capture spatiotemporal dependencies in the video. Most existing models rely on optical flow information to capture the dynamic visual tempos between consecutive video frames. Although impressive performance can be achieved by combining optical flow with RGB, the time-consuming nature of optical flow computation cannot be ignored. Moreover, 3D CNN has successfully modeled spatiotemporal information, yet the enormous computational volume is unsuitable for real-time action recognition. In this letter, we propose a novel lightweight video feature extraction strategy that achieves better recognition performance with lower FLOPs. In particular, we perform convolution on the video cube from three orthogonal angles to learn its appearance and motion features. Compared with the computational volume of 3D CNN, our proposed method is more economical and thus meets the lightweight requirements. Extensive experimental results on public Something Something-V1<inline-formula><tex-math notation=""LaTeX"">$\&amp;$</tex-math></inline-formula>V2 and Diving48 datasets show our approach achieves the state-of-the-art performance.",https://ieeexplore.ieee.org/document/9684992/,IEEE Signal Processing Letters,2022,ieeexplore
10.1109/CCNC49033.2022.9700636,Balancing Latency and Accuracy on Deep Video Analytics at the Edge,IEEE,Conferences,"Real-time deep video analytic at the edge is an enabling technology for emerging applications, such as vulnerable road user detection for autonomous driving, which requires highly accurate results of model inference within a low latency. In this paper, we investigate the accuracy-latency trade-off in the design and implementation of real-time deep video analytic at the edge. Without loss of generality, we select the widely used YOLO-based object detection and WebRTC-based video streaming for case study. Here, the latency consists of both networking latency caused by video streaming and the processing latency for video encoding/decoding and model inference. We conduct extensive measurements to figure out how the dynamically changing settings of video streaming affect the achieved latency, the quality of video, and further the accuracy of model inference. Based on the findings, we propose a mechanism for adapting video streaming settings (i.e. bitrate, resolution) online to optimize the accuracy of video analytic within latency constraints. The mechanism has proved, through a simulated setup, to be efficient in searching the optimal settings.",https://ieeexplore.ieee.org/document/9700636/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore
10.1109/WACV51458.2022.00135,Detecting Tear Gas Canisters With Limited Training Data,IEEE,Conferences,"Human rights investigations often entail triaging large volumes of open source images and video in order to find moments that are relevant to a given investigation and warrant further inspection. Searching for instances of tear gas usage online manually is laborious and time-consuming. In this paper, we study various object detection models for their potential use in the discovery and identification of tear gas canisters for human rights monitors. CNN based object detection typically requires large volumes of training data, and prior to our work, an appropriate dataset of tear gas canisters did not exist. We benchmark methods for training object detectors using limited labelled data: we fine-tune different object detection models on the limited labelled data and compare performance to a few shot detector and augmentation strategies using synthetic data. We provide a dataset for evaluating and training tear gas canister detectors and indicate how such detectors can be deployed in real-world contexts for investigating human rights violations. Our experiments show that various techniques can improve results, including fine-tuning state of the art detectors, using few shot detectors, and including synthetic data as part of the training set.",https://ieeexplore.ieee.org/document/9706699/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore
10.1109/COMSNETS53615.2022.9668364,Open-air Off-street Vehicle Parking Management System Using Deep Neural Networks: A Case Study,IEEE,Conferences,"Smart parking solution aims to output real-time parking occupancy information. It helps to reduce parking bay search time, traffic, fuel consumption, and thereby vehicular emissions with increased road safety. A computer vision-based solution using camera video data is most reliable and rational since it allows monitoring the entire open-air parking area at once. A real-time parking solution (cloud-based, server processing, or onboard processing) helps bring the occupancy information to the end-user. It comes with many challenges such as viewing angles, lighting conditions, model optimization, reducing inference time, and many more real-world challenges. Hence, this paper presents a case study on real-time open-air off-street intelligent parking management using a deep neural network. Also, most of the earlier research works focus on day-time data and do not discuss the night data. So, in this work, we perform experiments on realtime 24-hour data from an input camera video source mounted to monitor parking at IIT Hyderabad (IITH) parking lot. Our experiments demonstrate the real-world challenges and can help improve parking performance, deployment at IITH, and relevant parking systems in general.",https://ieeexplore.ieee.org/document/9668364/,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),4-8 Jan. 2022,ieeexplore
10.1109/COMSNETS53615.2022.9668498,Tele-driving an electric vehicle over a private LTE network,IEEE,Conferences,"We demonstrate tele-driving operation for an electric vehicle capable of stopping itself in case of system failure over a captive LTE network deployed in a university campus. Our electronically controlled vehicle is driven remotely by an operator from a control room which receives the multi-camera real-time video feed from the vehicle over this network. Our primary contribution includes the responsive emergency braking mechanism for the vehicle, modular vehicle design based on CAN bus, low latency LTE MAC scheduler design, and modifications to popular video tool, FFMPEG to support low latency real time video streaming. Our demonstration shows complete integration of the different components, i.e., the vehicle, the LTE network and the remote driving application. Another salient feature of our system is the O-RAN compliant RAN awareness module and KPI (Key Performance Indicator) application which enables real-time network performance monitoring.",https://ieeexplore.ieee.org/document/9668498/,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),4-8 Jan. 2022,ieeexplore
10.1109/ACCESS.2022.3147519,A Deep Learning-Based Approach for Inappropriate Content Detection and Classification of YouTube Videos,IEEE,Journals,"The exponential growth of videos on YouTube has attracted billions of viewers among which the majority belongs to a young demographic. Malicious uploaders also find this platform as an opportunity to spread upsetting visual content, such as using animated cartoon videos to share inappropriate content with children. Therefore, an automatic real-time video content filtering mechanism is highly suggested to be integrated into social media platforms. In this study, a novel deep learning-based architecture is proposed for the detection and classification of inappropriate content in videos. For this, the proposed framework employs an ImageNet pre-trained convolutional neural network (CNN) model known as EfficientNet-B7 to extract video descriptors, which are then fed to bidirectional long short-term memory (BiLSTM) network to learn effective video representations and perform multiclass video classification. An attention mechanism is also integrated after BiLSTM to apply attention probability distribution in the network. These models are evaluated on a manually annotated dataset of 111,156 cartoon clips collected from YouTube videos. Experimental results demonstrated that EfficientNet-BiLSTM (accuracy = 95.66%) performs better than attention mechanism-based EfficientNet-BiLSTM (accuracy = 95.30%) framework. Secondly, the traditional machine learning classifiers perform relatively poor than deep learning classifiers. Overall, the architecture of EfficientNet and BiLSTM with 128 hidden units yielded state-of-the-art performance (f1 score = 0.9267). Furthermore, the performance comparison against existing state-of-the-art approaches verified that BiLSTM on top of CNN captures better contextual information of video descriptors in network architecture, and hence achieved better results in child inappropriate video content detection and classification.",https://ieeexplore.ieee.org/document/9696242/,IEEE Access,2022,ieeexplore
10.1109/JIOT.2021.3089080,Audio-Visual Autoencoding for Privacy-Preserving Video Streaming,IEEE,Journals,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",https://ieeexplore.ieee.org/document/9453730/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/ACCESS.2021.3137031,Autonomous Detection and Deterrence of Pigeons on Buildings by Drones,IEEE,Journals,"Pigeons may transmit diseases to humans and cause damages to buildings, monuments, and other infrastructure. Therefore, several control strategies have been developed, but they have been found to be either ineffective or harmful to animals and often depend on human operation. This study proposes a system capable of autonomously detecting and deterring pigeons on building roofs using a drone. The presence and position of pigeons were detected in real time by a neural network using images taken by a video camera located on the roof. Moreover, a drone was utilized to deter the animals. Field experiments were conducted in a real-world urban setting to assess the proposed system by comparing the number of animals and their stay durations for over five days against the 21-day-trial experiment without the drone. During the five days of experiments, the drone was automatically deployed 55 times and was significantly effective in reducing the number of birds and their stay durations without causing any harm to them. In conclusion, this study has proven the effectiveness of this system in deterring birds, and this approach can be seen as a fully autonomous alternative to the already existing methods.",https://ieeexplore.ieee.org/document/9656717/,IEEE Access,2022,ieeexplore
10.1109/TMM.2021.3050086,Improving Robustness of DASH Against Unpredictable Network Variations,IEEE,Journals,"Most video players use adaptive bitrate (ABR) algorithms to provide good quality-of-experience (QoE) in dynamic network conditions. To deal with the adaptation challenges, many ABR algorithms select bitrate by optimizing a defined QoE function. Within the framework, various algorithms mainly differ in how the optimization problem is solved, including prediction-based approaches and learn-based approaches. However, these algorithms suffer from limited performance in the current popular mobile streaming which has limited resources and rapidly changing link rates. Existing machine-learning approaches face deployment difficulties on mobile devices, and prediction-based approaches that rely on throughput prediction experience large buffer occupancy variations in cellular networks, resulting in rebuffering frequently. To provide a robust and lightweight ABR algorithm for mobile streaming, this work improves the robustness of prediction-based scheme against unpredictable network variations and develops RBC (Robust Bitrate Controller) algorithm. Rather than optimizing QoE over the entire buffer capacity, RBC creates buffer margins to absorb the impact of throughput jitters and solves QoE maximization on the narrowed buffer range. The amount of buffer margin is dynamically adjusted based on the real-time throughput fluctuation to ensure sufficient de-jitter space. For online lightweight deployment, RBC provides a closed-form solution of the desired bitrate with small computation complexity by using adaptive control approach. Trace-driven experiments and real-world tests show that RBC effectively reduces the playback freezing and gains an improvement in overall QoE.",https://ieeexplore.ieee.org/document/9317786/,IEEE Transactions on Multimedia,2022,ieeexplore
10.1109/OJITS.2021.3139393,NAPC: A Neural Algorithm for Automated Passenger Counting in Public Transport on a Privacy-Friendly Dataset,IEEE,Journals,"Real-time load information in public transport is of high importance for both passengers and service providers. Neural algorithms have shown a high performance on various object counting tasks and play a continually growing methodological role in developing automated passenger counting systems. However, the publication of public-space video footage is often contradicted by legal and ethical considerations to protect the passengers’ privacy. This work proposes an end-to-end Long Short-Term Memory network with a problem-adapted cost function that learned to count boarding and alighting passengers on a publicly available, comprehensive dataset of approx.13,000 manually annotated low-resolution 3D LiDAR video recordings (depth information only) from the doorways of a regional train. These depth recordings do not allow the identification of single individuals. For each door opening phase, the trained models predict the correct passenger count (ranging from 0 to 67) in approx.96% of boarding and alighting, respectively. Repeated training with different training and validation sets confirms the independence of this result from a specific test set.",https://ieeexplore.ieee.org/document/9665722/,IEEE Open Journal of Intelligent Transportation Systems,2022,ieeexplore
10.1109/LSP.2022.3144074,Rethinking Lightweight: Multiple Angle Strategy for Efficient Video Action Recognition,IEEE,Journals,"Video action recognition task involves modeling spatiotemporal information, and efficiency is critical to capture spatiotemporal dependencies in the video. Most existing models rely on optical flow information to capture the dynamic visual tempos between consecutive video frames. Although impressive performance can be achieved by combining optical flow with RGB, the time-consuming nature of optical flow computation cannot be ignored. Moreover, 3D CNN has successfully modeled spatiotemporal information, yet the enormous computational volume is unsuitable for real-time action recognition. In this letter, we propose a novel lightweight video feature extraction strategy that achieves better recognition performance with lower FLOPs. In particular, we perform convolution on the video cube from three orthogonal angles to learn its appearance and motion features. Compared with the computational volume of 3D CNN, our proposed method is more economical and thus meets the lightweight requirements. Extensive experimental results on public Something Something-V1<inline-formula><tex-math notation=""LaTeX"">$\&amp;$</tex-math></inline-formula>V2 and Diving48 datasets show our approach achieves the state-of-the-art performance.",https://ieeexplore.ieee.org/document/9684992/,IEEE Signal Processing Letters,2022,ieeexplore
10.1109/TCSVT.2021.3066675,Spatio-Temporal Online Matrix Factorization for Multi-Scale Moving Objects Detection,IEEE,Journals,"Detecting moving objects from the video sequences has been treated as a challenging computer vision task, since the problems of dynamic background, multi-scale moving objects and various noise interference impact the corresponding feasibility and efficiency. In this paper, a novel spatio-temporal online matrix factorization (STOMF) method is proposed to detect multi-scale moving objects under dynamic background. To accommodate a wide range of the real noise distractions, we apply a specific mixture of exponential power (MoEP) distributions to the framework of low-rank matrix factorization (LRMF). For the optimization of solution algorithm, a temporal difference motion prior (TDMP) model is proposed, which estimates the motion matrix and calculates the weight matrix. Moreover, a partial spatial motion information (PSMI) post-processing method is further designed to implement multi-scale objects extraction in varieties of complex dynamic scenes, which utilizes partial background and motion information. The superiority of the STOMF method is validated by massive experiments on practical datasets, as compared with state-of-the-art moving objects detection approaches.",https://ieeexplore.ieee.org/document/9380454/,IEEE Transactions on Circuits and Systems for Video Technology,Feb. 2022,ieeexplore
10.1109/ICCECE54139.2022.9712814,A Lightweight SAR Image Recognition Algorithm Based On Deep Convolutional Neural Network,IEEE,Conferences,"Synthetic Aperture Radar (SAR) can provide large-scale, all-time, all-weather imaging and therefore plays an important role in both military reconnaissance and battlefield perception. In recent years, due to the outstanding performance of deep convolutional neural networks (CNNs) on image recognition, the approaches that apply CNNs to SAR target recognition has attracted widespread attention among domestic and foreign scholars. The CNNs can achieve high accuracy, yet they often contain huge number of parameters and occupy too much memory to be deployed on devices with memory constraint. In this work, we build light-weight SAR image recognition models by performing iterative pruning and retraining on three representative architectures. The algorithm can lead to a 50% reduction in the number of parameters, while still obtaining a high accuracy of 98.5%. Our algorithm and results provide a potential direction for building light-weight SAR image model that can be deployed for real-world applications.",https://ieeexplore.ieee.org/document/9712814/,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),14-16 Jan. 2022,ieeexplore
10.1109/ICCECE54139.2022.9712814,A Lightweight SAR Image Recognition Algorithm Based On Deep Convolutional Neural Network,IEEE,Conferences,"Synthetic Aperture Radar (SAR) can provide large-scale, all-time, all-weather imaging and therefore plays an important role in both military reconnaissance and battlefield perception. In recent years, due to the outstanding performance of deep convolutional neural networks (CNNs) on image recognition, the approaches that apply CNNs to SAR target recognition has attracted widespread attention among domestic and foreign scholars. The CNNs can achieve high accuracy, yet they often contain huge number of parameters and occupy too much memory to be deployed on devices with memory constraint. In this work, we build light-weight SAR image recognition models by performing iterative pruning and retraining on three representative architectures. The algorithm can lead to a 50% reduction in the number of parameters, while still obtaining a high accuracy of 98.5%. Our algorithm and results provide a potential direction for building light-weight SAR image model that can be deployed for real-world applications.",https://ieeexplore.ieee.org/document/9712814/,2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE),14-16 Jan. 2022,ieeexplore
10.1109/TIM.2021.3132332,Finger Vein Recognition Algorithm Based on Lightweight Deep Convolutional Neural Network,IEEE,Journals,"Even though the deep neural networks have strong feature representation capability and high recognition accuracy in finger vein recognition, the deep models are computationally intensive and poor in timeliness. To address these issues, this article proposes a lightweight algorithm for finger vein image recognition and matching. The proposed algorithm uses a lightweight convolutional model in the backbone network and employs a triplet loss function to train the model, which not only improves the matching accuracy, but also satisfies the real-time matching requirements. In addition, the Mini-region of interest (RoI) and finger vein pattern feature extraction also effectively solve the problems of large amounts of calculation and background noise. Moreover, the present model recognizes new categories based on the feature vector space constructed by the finger vein recognition system, so that new categories can be recognized without retraining the model. The results show that the finger vein recognition and matching algorithm proposed in this article achieves 99.3% and 99.6% in recognition accuracy and 14.2 and 16.5 ms in matching time for the dataset Shandong University Machine Learning and Applications Laboratory-Homologous Multimodal Biometric Traits (SDUMLA-HMT) and Peking University Finger Vein Dataset (PKU-FVD), respectively. These metrics show that our approach is time-saving and more effective than previous algorithms. Compared with the state-of-the-art finger vein recognition algorithm, the proposed algorithm improves 1.45% in recognition accuracy while saving 45.7% in recognition time.",https://ieeexplore.ieee.org/document/9633979/,IEEE Transactions on Instrumentation and Measurement,2022,ieeexplore
10.1109/JIOT.2021.3089080,Audio-Visual Autoencoding for Privacy-Preserving Video Streaming,IEEE,Journals,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",https://ieeexplore.ieee.org/document/9453730/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/TASLP.2021.3129994,SoundStream: An End-to-End Neural Audio Codec,IEEE,Journals,"We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.",https://ieeexplore.ieee.org/document/9625818/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2022,ieeexplore
10.1109/JIOT.2021.3089080,Audio-Visual Autoencoding for Privacy-Preserving Video Streaming,IEEE,Journals,"The demand of sharing video streaming extremely increases due to the proliferation of Internet of Things (IoT) devices in recent years, and the explosive development of artificial intelligent (AI) detection techniques has made visual privacy protection more urgent and difficult than ever before. Although a number of approaches have been proposed, their essential drawbacks limit the effect of visual privacy protection in real applications. In this article, we propose a cycle vector-quantized variational autoencoder (cycle-VQ-VAE) framework to encode and decode the video with its extracted audio, which takes the advantage of multiple heterogeneous data sources in the video itself to protect individuals’ privacy. In our cycle-VQ-VAE framework, a fusion mechanism is designed to integrate the video and its extracted audio. Particularly, the extracted audio works as the random noise with a nonpatterned distribution, which outperforms the noise that follows a patterned distribution for hiding visual information in the video. Under this framework, we design two models, including the frame-to-frame (F2F) model and video-to-video (V2V) model, to obtain privacy-preserving video streaming. In F2F, the video is processed as a sequence of frames; while, in V2V, the relations between frames are utilized to deal with the video, greatly improving the performance of privacy protection, video compression, and video reconstruction. Moreover, the video streaming is compressed in our encoding process, which can resist side-channel inference attack during video transmission and reduce video transmission time. Through the real-data experiments, we validate the superiority of our models (F2F and V2V) over the existing methods in visual privacy protection, visual quality preservation, and video transmission efficiency. The codes of our model implementation and more experimental results are now available at <uri>https://github.com/ahahnut/cycle-VQ-VAE</uri>.",https://ieeexplore.ieee.org/document/9453730/,IEEE Internet of Things Journal,"1 Feb.1, 2022",ieeexplore
10.1109/ACCESS.2021.3139537,"Automatic Adaptation of Open Educational Resources: An Approach From a Multilevel Methodology Based on Students’ Preferences, Educational Special Needs, Artificial Intelligence and Accessibility Metadata",IEEE,Journals,"The need for adaptive e-learning environments that respond to learning variability is now a fundamental requirement in education, as it helps to ensure that students learn and pass their courses within a set time frame. Although guidelines, techniques and methods have been established in recent years to contribute to the development of accessible and adaptable e-learning environments that promote digital inclusion, their implementation is challenging due to the lack of knowledge of an adequate way to do it and because it is considered more of a technological competence for scholars in the area. In this context, automated support for adapting material that responds to the correct use of accessibility metadata not only provides a way to improve the description of adapted educational resources, but also facilitates their search according to the needs and preferences of students, particularly those with disabilities. In this article, we carry out a multilevel methodological proposal for the automatic adaptation of open educational resources, in order to provide a tool that contributes to the accessibility and correct use of their metadata in e-learning environments. A research is conducted with students with disabilities to establish their real needs and preferences, highlighting the need to strengthen the adequate description and coherent alternative text in images, the correct subtitling in videos and the conversion of audio to text, data that are relevant to our proposal. The research conducted aims to contribute with an automated support tool in the generation of accessible educational resources that are correctly labeled for search and reuse. This research also aims to support researchers in artificial intelligence applications to address challenges and opportunities in the field of virtual education, in addition to providing an overview that could help those who generate educational resources and maintain their interest in making them accessible.",https://ieeexplore.ieee.org/document/9669174/,IEEE Access,2022,ieeexplore
10.1109/TASLP.2021.3129994,SoundStream: An End-to-End Neural Audio Codec,IEEE,Journals,"We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.",https://ieeexplore.ieee.org/document/9625818/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2022,ieeexplore
10.1109/TNSE.2021.3050781,VFChain: Enabling Verifiable and Auditable Federated Learning via Blockchain Systems,IEEE,Journals,"Advanced artificial intelligence techniques, such as federated learning, has been applied to broad areas, e.g., image classification, speech recognition, smart city, and healthcare. Despite intensive research on federated learning, existing schemes are vulnerable to attacks and can hardly meet the security requirements for real-world applications. The problem of designing a secure federated learning framework to ensure the correctness of training procedure has not been sufficiently studied and remains open. In this paper, we propose VFChain, a verifiable and auditable federated learning framework based on the blockchain system. First, to provide the verifiability, a committee is selected through the blockchain to collectively aggregate models and record verifiable proofs in the blockchain. Then, to provide the auditability, a novel authenticated data structure is proposed for blockchain to improve the search efficiency of verifiable proofs and support a secure rotation of committee. Finally, to further improve the search efficiency, an optimization scheme is proposed to support multiple-model learning tasks. We implement VFChain and conduct extensive experiments by utilizing the popular deep learning models over the public real-world dataset. The evaluation results demonstrate the effectiveness of our proposed VFChain system.",https://ieeexplore.ieee.org/document/9321132/,IEEE Transactions on Network Science and Engineering,1 Jan.-Feb. 2022,ieeexplore
10.1109/ACCESS.2022.3140901,A Two-Stage Deep Neuroevolutionary Technique for Self-Adaptive Speech Enhancement,IEEE,Journals,"This paper presents a novel self-adaptive approach for speech enhancement in the context of highly nonstationary noise. A two-stage deep neuroevolutionary technique for speech enhancement is proposed. The first stage is composed of a deep neural network (DNN) method for speech enhancement. Two DNN methods were tested at this stage, namely, both a deep complex convolution recurrent network (DCCRN) and a residual long short-term memory neural network (ResLSTM). The ResLSTM method was combined with a minimum mean-square error method to perform a preliminary enhancement. The ResLSTM network is used as an <italic>a priori</italic> signal-to-noise ratio (SNR) estimator. The second stage implements a self-adaptive multiband spectral subtraction enhancement method using tuning optimization based on a genetic algorithm. The proposed two-stage technique is evaluated using objective measures of speech quality and intelligibility. The experiments are carried out using the NOIZEUS noisy speech corpus using conditions of real-world stationary, colored, and nonstationary noise sources at multiple SNR levels. These experiments demonstrate the advantage of building a cooperative approach using evolutionary and deep learning-based techniques that are capable of achieving robust speech enhancement in adverse conditions. Indeed, the experimental tests show that the proposed two-stage technique outperformed a baseline implementation using a state-of-the-art deep learning approach by an average 13% and 6% improvement for six noise conditions at a −5 dB and a 0 dB input SNR, respectively.",https://ieeexplore.ieee.org/document/9672141/,IEEE Access,2022,ieeexplore
10.1109/TASLP.2021.3126947,End-to-End Neural Based Modification of Noisy Speech for Speech-in-Noise Intelligibility Improvement,IEEE,Journals,"Intelligibility of speech can be significantly reduced when it is presented in adverse near-end listening conditions, like background noise. Multiple approaches have been suggested to improve the perception of speech in such conditions. However, most of these approaches were designed to work with clean input speech. Therefore, they have serious limitations when deployed in real world applications like telephony and hearing aids, where noisy input speech is quite common. In this paper we present an end-to-end neural network approach for the above problem, which effectively reduces the input noise and improves the intelligibility for listeners in adverse conditions. To that end, a convolutional neural network topology with variable dilation factors is proposed and evaluated both in a causal and a non-causal configuration using raw speech as input. A Teacher-Student training strategy is employed, where the Teacher is a well-established speech-in-noise intelligibility enhancer based on spectral shaping followed by dynamic range compression (SSDRC). The evaluation is performed both objectively using the speech intelligibility in bits metric (SIIB), and subjectively on the Greek Harvard corpus. A noise robust multi-band version of SSDRC was used as a baseline. Compared with the baseline, at 0 dB input SNR, the suggested neural network system achieved about 380% and 230% relative SIIB improvements in fluctuating and stationary backgrounds, respectively. Subjectively, the suggested model increased listeners’ keyword correct rate in stationary noise from 25% to 60% at 0 dB input SNR, and from about 52% to 75% at 5 dB input SNR, compared with the baseline.",https://ieeexplore.ieee.org/document/9611022/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2022,ieeexplore
10.1109/TASLP.2021.3133190,VACE-WPE: Virtual Acoustic Channel Expansion Based on Neural Networks for Weighted Prediction Error-Based Speech Dereverberation,IEEE,Journals,"Speech dereverberation is an important issue for many real-world speech processing applications. Among the techniques developed, the weighted prediction error (WPE) algorithm has been widely adopted and advanced over the last decade, which blindly cancels out the late reverberation component from the reverberant mixture of microphone signals. In this study, we extend the neural-network-based virtual acoustic channel expansion (VACE) framework for the WPE-based speech dereverberation, a variant of the WPE that we recently proposed to enable the use of dual-channel WPE algorithm in a single-microphone speech dereverberation scenario. Based on the previous study, some ablation studies are conducted regarding the constituents of the VACE-WPE in an offline processing scenario. These studies reveal the characteristics of the system, thereby simplifying the architecture and leading to the introduction of new strategies for training the neural network for the VACE. Experimental results demonstrate that VACE-WPE (our PyTorch implementation and pre-trained models are available from <uri>https://github.com/dreadbird06/vace_wpe</uri>) considerably outperforms its single-channel counterpart in simulated noisy reverberant environments in terms of objective speech quality and is superior to the single-channel WPE as well as several fully neural speech dereverberation methods when employed as the front-end for the far-field automatic speech recognizer.",https://ieeexplore.ieee.org/document/9640471/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2022,ieeexplore
10.1109/ACCESS.2022.3140901,A Two-Stage Deep Neuroevolutionary Technique for Self-Adaptive Speech Enhancement,IEEE,Journals,"This paper presents a novel self-adaptive approach for speech enhancement in the context of highly nonstationary noise. A two-stage deep neuroevolutionary technique for speech enhancement is proposed. The first stage is composed of a deep neural network (DNN) method for speech enhancement. Two DNN methods were tested at this stage, namely, both a deep complex convolution recurrent network (DCCRN) and a residual long short-term memory neural network (ResLSTM). The ResLSTM method was combined with a minimum mean-square error method to perform a preliminary enhancement. The ResLSTM network is used as an <italic>a priori</italic> signal-to-noise ratio (SNR) estimator. The second stage implements a self-adaptive multiband spectral subtraction enhancement method using tuning optimization based on a genetic algorithm. The proposed two-stage technique is evaluated using objective measures of speech quality and intelligibility. The experiments are carried out using the NOIZEUS noisy speech corpus using conditions of real-world stationary, colored, and nonstationary noise sources at multiple SNR levels. These experiments demonstrate the advantage of building a cooperative approach using evolutionary and deep learning-based techniques that are capable of achieving robust speech enhancement in adverse conditions. Indeed, the experimental tests show that the proposed two-stage technique outperformed a baseline implementation using a state-of-the-art deep learning approach by an average 13% and 6% improvement for six noise conditions at a −5 dB and a 0 dB input SNR, respectively.",https://ieeexplore.ieee.org/document/9672141/,IEEE Access,2022,ieeexplore
10.1109/TASLP.2021.3126947,End-to-End Neural Based Modification of Noisy Speech for Speech-in-Noise Intelligibility Improvement,IEEE,Journals,"Intelligibility of speech can be significantly reduced when it is presented in adverse near-end listening conditions, like background noise. Multiple approaches have been suggested to improve the perception of speech in such conditions. However, most of these approaches were designed to work with clean input speech. Therefore, they have serious limitations when deployed in real world applications like telephony and hearing aids, where noisy input speech is quite common. In this paper we present an end-to-end neural network approach for the above problem, which effectively reduces the input noise and improves the intelligibility for listeners in adverse conditions. To that end, a convolutional neural network topology with variable dilation factors is proposed and evaluated both in a causal and a non-causal configuration using raw speech as input. A Teacher-Student training strategy is employed, where the Teacher is a well-established speech-in-noise intelligibility enhancer based on spectral shaping followed by dynamic range compression (SSDRC). The evaluation is performed both objectively using the speech intelligibility in bits metric (SIIB), and subjectively on the Greek Harvard corpus. A noise robust multi-band version of SSDRC was used as a baseline. Compared with the baseline, at 0 dB input SNR, the suggested neural network system achieved about 380% and 230% relative SIIB improvements in fluctuating and stationary backgrounds, respectively. Subjectively, the suggested model increased listeners’ keyword correct rate in stationary noise from 25% to 60% at 0 dB input SNR, and from about 52% to 75% at 5 dB input SNR, compared with the baseline.",https://ieeexplore.ieee.org/document/9611022/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2022,ieeexplore
10.1109/TASLP.2021.3129994,SoundStream: An End-to-End Neural Audio Codec,IEEE,Journals,"We present <italic>SoundStream</italic>, a novel neural audio codec that can efficiently compress speech, music and general audio at bitrates normally targeted by speech-tailored codecs. <italic>SoundStream</italic> relies on a model architecture composed by a fully convolutional encoder/decoder network and a residual vector quantizer, which are trained jointly end-to-end. Training leverages recent advances in text-to-speech and speech enhancement, which combine adversarial and reconstruction losses to allow the generation of high-quality audio content from quantized embeddings. By training with structured dropout applied to quantizer layers, a single model can operate across variable bitrates from 3 kbps to 18 kbps, with a negligible quality loss when compared with models trained at fixed bitrates. In addition, the model is amenable to a low latency implementation, which supports streamable inference and runs in real time on a smartphone CPU. In subjective evaluations using audio at 24 kHz sampling rate, <italic>SoundStream</italic> at 3 kbps outperforms Opus at 12 kbps and approaches EVS at 9.6 kbps. Moreover, we are able to perform joint compression and enhancement either at the encoder or at the decoder side with no additional latency, which we demonstrate through background noise suppression for speech.",https://ieeexplore.ieee.org/document/9625818/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2022,ieeexplore
10.1109/TASLP.2021.3133190,VACE-WPE: Virtual Acoustic Channel Expansion Based on Neural Networks for Weighted Prediction Error-Based Speech Dereverberation,IEEE,Journals,"Speech dereverberation is an important issue for many real-world speech processing applications. Among the techniques developed, the weighted prediction error (WPE) algorithm has been widely adopted and advanced over the last decade, which blindly cancels out the late reverberation component from the reverberant mixture of microphone signals. In this study, we extend the neural-network-based virtual acoustic channel expansion (VACE) framework for the WPE-based speech dereverberation, a variant of the WPE that we recently proposed to enable the use of dual-channel WPE algorithm in a single-microphone speech dereverberation scenario. Based on the previous study, some ablation studies are conducted regarding the constituents of the VACE-WPE in an offline processing scenario. These studies reveal the characteristics of the system, thereby simplifying the architecture and leading to the introduction of new strategies for training the neural network for the VACE. Experimental results demonstrate that VACE-WPE (our PyTorch implementation and pre-trained models are available from <uri>https://github.com/dreadbird06/vace_wpe</uri>) considerably outperforms its single-channel counterpart in simulated noisy reverberant environments in terms of objective speech quality and is superior to the single-channel WPE as well as several fully neural speech dereverberation methods when employed as the front-end for the far-field automatic speech recognizer.",https://ieeexplore.ieee.org/document/9640471/,"IEEE/ACM Transactions on Audio, Speech, and Language Processing",2022,ieeexplore
10.1109/TNSE.2021.3050781,VFChain: Enabling Verifiable and Auditable Federated Learning via Blockchain Systems,IEEE,Journals,"Advanced artificial intelligence techniques, such as federated learning, has been applied to broad areas, e.g., image classification, speech recognition, smart city, and healthcare. Despite intensive research on federated learning, existing schemes are vulnerable to attacks and can hardly meet the security requirements for real-world applications. The problem of designing a secure federated learning framework to ensure the correctness of training procedure has not been sufficiently studied and remains open. In this paper, we propose VFChain, a verifiable and auditable federated learning framework based on the blockchain system. First, to provide the verifiability, a committee is selected through the blockchain to collectively aggregate models and record verifiable proofs in the blockchain. Then, to provide the auditability, a novel authenticated data structure is proposed for blockchain to improve the search efficiency of verifiable proofs and support a secure rotation of committee. Finally, to further improve the search efficiency, an optimization scheme is proposed to support multiple-model learning tasks. We implement VFChain and conduct extensive experiments by utilizing the popular deep learning models over the public real-world dataset. The evaluation results demonstrate the effectiveness of our proposed VFChain system.",https://ieeexplore.ieee.org/document/9321132/,IEEE Transactions on Network Science and Engineering,1 Jan.-Feb. 2022,ieeexplore
