id,datePublished,description,publisher,title,doi,downloadUrl,journals,database,query_name,query_value
289160157,2020-01-01T00:00:00,"Hyper-heuristic is a new methodology for the adaptive hybridization of meta-heuristic algorithms to derive a general algorithm for solving optimization problems. This work focuses on the selection type of hyper-heuristic, called the exponential Monte Carlo with counter (EMCQ). Current implementations rely on the memory-less selection that can be counterproductive as the selected search operator may not (historically) be the best performing operator for the current search instance. Addressing this issue, we propose to integrate the memory into EMCQ for combinatorial t-wise test suite generation using reinforcement learning based on the Q-learning mechanism, called Q-EMCQ. The limited application of combinatorial test generation on industrial programs can impact the use of such techniques as Q-EMCQ. Thus, there is a need to evaluate this kind of approach against relevant industrial software, with a purpose to show the degree of interaction required to cover the code as well as finding faults. We applied Q-EMCQ on 37 real-world industrial programs written in Function Block Diagram (FBD) language, which is used for developing a train control management system at Bombardier Transportation Sweden AB. The results show that Q-EMCQ is an efficient technique for test case generation. Addition- ally, unlike the t-wise test suite generation, which deals with the minimization problem, we have also subjected Q-EMCQ to a maximization problem involving the general module clustering to demonstrate the effectiveness of our approach. The results show the Q-EMCQ is also capable of outperforming the original EMCQ as well as several recent meta/hyper-heuristic including modified choice function, Tabu high-level hyper-heuristic, teaching learning-based optimization, sine cosine algorithm, and symbiotic optimization search in clustering quality within comparable execution time",'Springer Science and Business Media LLC',An evaluation of Monte Carlo-based hyper-heuristic for interaction testing of industrial embedded software applications,10.1007/s00500-020-04769-z,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
304693960,2020-01-01T00:00:00,Fault detection and fault diagnosis are crucial subsystems to be integrated within the control architecture of modern industrial processes to ensure high quality standards. In this paper we present a two-stage unsupervised approach for fault detection and diagnosis in household appliances. In particular a suitable testing procedure has been implemented on a real industrial production line in order to extract the most meaningful features that allow to efficiently classify different types of fault by consecutively exploiting deep autoencoder neural network and k-means or hierarchical clustering techniques,,A deep learning unsupervised approach for fault diagnosis of household appliances,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
389889861,2019-11-21T00:00:00,"The problem of inefficient processing in the Big Data industry is touched upon. A detailed analysis of the various means to increase the percentage of processed data is provided and the experimental implementation of a way to obtain and preprocess data in a mobile device network in real-time mode is shown.During the analysis of the subject, the next fields of research were observed: Deep Learning, Machine Learning, Big Data, and GPGPU technology. The increasing numbers of publications, especially of papers that include mobile networks, were emphasized as evidence of rapid growth of the industry and a transition of neuron network algorithms toward mobile operating systems.Further analysis was focused on highlighting the most relevant and perspective objects for the research. The analysis showed that amidst currently most innovative and broadly widespread operation systems and frameworks to implement and engage neuron network algorithms Android operation system and TensorFlow framework has the most significant advantages.Due to the purpose of developing an experimental solution based on mobile device network and neuron network, different classes and types of neuron network architectures were explored. Two major types of mobile neuron networks such as quantized and integer neuron networks and the principal dissimilarity between them were described. Various neuron networks were tested on mobile devices with Internet connection via specially developed auxiliary software using GPGPU technology. Experimental results had shown that modern smartphones such as Huawei P20-Pro are capable to analyze, store and transmit the incoming from its camera sensor information at a rate of up to 40 frames per second. The usage of mobile GPU for improving the performance of the neuron networks was proved to be effective as such the number of frames processed by a neuron network per second can be elevated up to 10 times.The experimental software that task was to search for the given by user object in a real-time mode using mobile device network as both — server and processing nodes — proved to be a violable solution for the increasing amount of preprocessed up-to-the-minute information in the Big Data industry after the diligent research.As a summary, it needs to be stated that current progress in mobile device industry is making possible to bring neuron network technologies on mobile platforms and expand the capability of data aggregating services through the use of modern Deep Learning frameworks and GPGPU technology.Проаналізовано напрямки досліджень у галузі концептів Big Data, розподілених мереж мобільних пристроїв і Deep Learning. Кількісно охарактеризовано та порівняно інтенсивність розвитку сучасних бібліотек нейронних мереж для використання технологій Deep Learning, Big Data, GPGPU. Розроблено застосування для дослідження роботи згорткових нейронних мереж різної архітектури із використанням потуж-ностей мобільних CPU та GPU і з використанням API для нейронних мереж на операційній системі Android. Розроблено застосування для агрегації проаналізованих у реальному часі даних, структуризації даних на сервері та досліджено роботу застосування в мережах WiFi, 3G та 4G. Проведено аналіз різних шляхів агрегації даних",Інститут проблем реєстрації інформації НАН України,Агрегація та аналіз графічних даних  у розподіленій мережі мобільних пристроїв,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
390009408,2020-02-20T00:00:00,"Methods of machine intelligence with training contribute their specifics to the creation and commissioning of a gaming system.One of the main problems is the need to anticipate the entire set of input situations and possible answers at the time of design and the impossibility of expanding their list withoutretraining. This leads to a narrowing of the possibility of their use in real gaming systems.  The object of research is the processes of creating and training game agents based on the evolutionary approaches of artificial intelligence. The purpose of the work is the development and justification of a formal model of a game agent based on machine learning methods, software implementation of the game process using neural networks and evolutionary optimization methods for multiple generations of game agent populations. To achieve this goal, the theoretical base, the existing research and development in the industry; a game scenario was designed, the main agents were identified, their main capabilities and expected behavior; training of game agents by various types of neural networks; development testing, quality assessment of behavior and decision-making by artificial intelligence using neural networks were performed; a comparative analysis of various types of neural networks, the proposed recommendations for their use for given conditions. To display the artificial neural network, the template components of the layer, neuron, and bridge were used. The created software module will allow game agents built using various neural networks to compete with each other (and not with a person, as in the normal game mode), and will reveal more prepared ones.  The scientific novelty of the study lies in the fact that a model of a game agent is formalized, based on machine learning methods. The results obtained in this work can be used in the development of video games built on the basis of artificial intelligence of game agents based on machine learning methods, as well as in other scientific studies.Методы машинного интеллекта с обучением вносят свою специфику к созданию и наладке игровой системы.Одна из главных проблем — это необходимость предвидения всего набора входных ситуаций и возможных ответов на моменте проектирования и невозможность расширения их списка без переобучения. Это приводит к сужению возможности их использования в реальных игровых системах.Объектом исследования являются процессы создания и обучения игровых агентов на основе эволюционных подходов искусственного интеллекта. Цель работы - разработка и обоснование формальной модели игрового агента, основанного на методах машинного обучения, программная реализация игрового процесса сприменением нейронных сетей и эволюционных методов оптимизации при множественных поколениях популяций игровых агентов.Для осуществления поставленной цели исследована теоретическая база, существующие исследования и разработки в отрасли; спроектирован игровой сценарий, определены основные агенты, их основные возможности и ожидаемое поведение;реализовано обучение игровых агентов различными типами нейронных сетей; выполнено тестирование разработок, оценка качества поведения и принятия решений искусственныминтеллектом с использованием нейронных сетей; проведен сравнительный анализ различных типов нейронных сетей, предлагаемых рекомендаций по их использованию для заданныхусловий. Для отображения искусственной нейронной сети использованы шаблонные компоненты слоя, нейрона и моста.Созданный программный модуль позволит игровым агентам, построенным с использованием различных нейронных сетей, соперничать друг с другом (а не с человеком,как в обычном режиме игры), позволит выявить более подготовленного из них. Научная новизна исследования заключается в том, что формализована модель игрового агента, в основе которой методы машинного обучения. Полученные в работе результаты могут быть использованы при разработке видеоигр, построенных на базе искусственного интеллекта игровых агентов, в основекоторых методы машинного обучения, а также в рамках других научных исследований.Методи машинного інтелекту з навчанням привносять свою специфіку до створення і налагодження ігрової системи.  Одна з головних проблем - це необхідність передбачення усього набору вхідних ситуацій і можливих відповідей на моменті проектування і неможливість розширення їх списку без перенавчання. Це призводить до звуження можливості їх використання у реальних ігрових системах. Об'єктом дослідження є процеси створення і навчання ігрових агентів на основі еволюційних підходів штучного інтелекту. Мета роботи - розробка і обґрунтування формальної моделі ігрового агенту, заснованої на методах машинного навчання, програмна реалізація ігрового процесу з застосуванням нейронних мереж та еволюційних методів оптимізації при множинних генераціях популяцій ігрових агентів. Для здійснення поставленої мети досліджена теоретична база, існуюча дослідження та розробки в галузі; спроектовано ігровий сценарій, визначені основні агенти, їх основні можливості та очікувана поведінка; реалізоване навчання ігрових агентів різними типами нейронних мереж; виконане тестування розробок, оцінка якості поведінки й прийняття рішень штучним інтелектом з використанням нейронних мереж; проведений порівняльний аналіз різних типів нейронних мереж, запропонування рекомендацій щодо їх використання для заданих умов. Для відображення штучної нейронної мережі використані шаблоні компоненти слою, нейрону і мосту. Створений програмний модуль дозволить ігровим агентам, побудованим з використанням різних нейронних мереж, суперничати один з іншим (а не з людиною, як у звичайному режимі гри), дозволить виявити більш підготовленого з них. Наукова новизна одержаних результатів полягає у тому, що формалізовано модель ігрового агенту, в основі якої методи машинного навчання. Отримані в роботі результати можуть бути використані при розробці відеоігор, побудованих на базі штучного інтелекту ігрових агентів, що гуртуються на методах машинного навчання, а також в рамках інших наукових досліджень",ДВНЗ «Приазовський державний технічний університет»,ДОСЛІДЖЕННЯ ТА МОДЕЛЮВАННЯ ІГРОВОГО ПРОЦЕСУ НА БАЗІ МЕТОДІВ МАШИННОГО НАВЧАННЯ,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
287429785,2019-01-01T00:00:00,"International audienceThe use of wireless sensor networks, which are the key ingredient in the growing Internet of Things (IoT), has surged over the past few years with a widening range of applications in the industry, healthcare, agriculture, with a special attention to monitoring and tracking, often tied with security issues. In some applications, sensors can be deployed in remote, large unpopulated areas, whereas in others, they serve to monitor confined busy spaces. In either case, clustering the sensor network’s nodes into several clusters is of fundamental benefit for obvious scalability reasons, and also for helping to devise maintenance or usage schedules that might greatly improve the network’s lifetime. In the present paper, we survey and compare popular and advanced clustering schemes and provide a detailed analysis of their performance as a function of scale, type of collected data or their heterogeneity, and noise level. The testing is performed on real sensor data provided by the UCI Machine Learning Repository, using various external validation metrics",HAL CCSD,Introducing and Comparing Recent Clustering Methods for Massive Data Management in the Internet of Things,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226908873,2019-07-08T00:00:00,"International audienceThe Industry 4.0 framework needs new intelligent approaches. Thus, the manufacturing industries more and more pay close attention to artificial intelligence (AI). For example, smart monitoring and diagnosis, real time evaluation and optimization of the whole production and raw materials management can be improved by using machine learning and big data tools. An accurate milling process implies a high quality of the obtained material surface (roughness, flatness). With the involvement of AI-based algorithms, milling process is expected to be more accurate during complex operations.In this work, a smart milling diagnosis has been developed for composite sandwich structures based on honey-comb core. The use of such material has grown considerably in recent years, especially in the aeronautic, aerospace, sporting and automotive industries. But the precise milling of such material presents many difficulties. The objective of this work is to develop a data-driven industrial surface quality diagnosis for the milling of honey-comb material, by using supervised machine learning methods. Therefore, cutting forces and workpiece material vibrations are online measured in order to predict the resulting surface flatness.The workpiece material studied in this investigation is Nomex® honeycomb cores with thin cell walls. The Nomex® honeycomb machining presents several defects related to its composite nature (uncut fiber, tearing of the walls), the cutting conditions and to the alveolar geometry of the structure which causes vibration on the different components of the cutting effort.Given the low level of cutting forces, the quality of the obtained machined surface allows to establish criteria for determining the machinability of the honeycomb structures. Nearly 40 features are calculated in time domain and frequency domain from the raw signal in steady state behavior (transient zones are not taken into account). The features are then normalized. The input parameters for each experiment are: the tool rotation speed, the cutting speed and the depth of cut. It is then necessary to make a dimensional reduction of that feature table in order to avoid overfitting and to reduce the computing time of the learning algorithm.In this work, several classification algorithms have been implemented such as : k-nearest neighbor (kNN), Decision trees (DT), Support Vector Machine (SVM). The different supervised learning algorithms have been implemented and compared. Each AI-based model has been applied to a set of features. From the prediction results, SVM algorithm seems to be the most efficient algorithm in this application",HAL CCSD,Milling diagnosis using machine learning approaches,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
227326055,2019-08-13,"International audienceRequests to improve the quality of software are increasing due to the competition in software industry and the complexity of software development integrating multiple technology domains (e.g., IoT, Big Data, Cloud, Artificial Intelligence, Security Technologies). Measurements collection and analysis is key activity to assess software quality during its development live-cycle. To optimize this activity, our main idea is to periodically select relevant measures to be executed (among a set of possible measures) and automatize their analysis by using a dedicated tool. The proposed solution is integrated in a whole PaaS platform called MEASURE. The tools supporting this activity are Software Metric Suggester tool that recommends metrics of interest according several software development constraints and based on artificial intelligence and MINT tool that correlates collected measurements and provides near real-time recommendations to software development stakeholders (i.e. DevOps team, project manager, human resources manager etc.) to improve the quality of the development process. To illustrate the efficiency of both tools, we created different scenarios on which both approaches are applied. Results show that both tools are complementary and can be used to improve the software development process and thus the final software qualit",Springer,Smart measurements and analysis for software quality enhancement,10.1007/978-3-030-29157-0_9,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479999354,2021-01-01T00:00:00,"The Software Defined Networking (SDN) paradigm enables the development of systems that centrally monitor and manage network traffic, providing support for the deployment of machine learning-based systems that automatically detect and mitigate network intrusions. This paper presents an intelligent system capable of deciding which countermeasures to take in order to mitigate an intrusion in a software defined network. The interaction between the intruder and the defender is posed as a Markov game and MuZero algorithm is used to train the model through self-play. Once trained, the model is integrated with an SDN controller, so that it is able to apply the countermeasures of the game in a real network. To measure the performance of the model, attackers and defenders with different training steps have been confronted and the scores obtained by each of them, the duration of the games and the ratio of games won have been collected. The results show that the defender is capable of deciding which measures minimize the impact of the intrusion, isolating the attacker and preventing it from compromising key machines in the network.This work was supported in part by the Spanish Centre for the Development of Industrial Technology (CDTI) through the Project EGIDA-RED DE EXCELENCIA EN TECNOLOGIAS DE SEGURIDAD Y PRIVACIDAD under Grant CER20191012, in part by the Spanish Ministry of Science and Innovation under Grant PID2019-104966GB-I00, in part by the Basque Business Development Agency (SPRI)-Basque Country Government ELKARTEK Program through the projects TRUSTIND under Grant KK-2020/00054 and 3KIA under Grant KK-2020/00049, and in part by the Basque Country Program of Grants for Research Groups under Grant IT-1244-19",'Institute of Electrical and Electronics Engineers (IEEE)',Towards Autonomous Defense of SDN Networks Using MuZero Based Intelligent Agents,10.1109/ACCESS.2021.3100706,https://core.ac.uk/download/479999354.pdf,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
480038968,2021-01-01T00:00:00,"The evaluation of groundwater quality in the Dammam formation, Faddak farm, Karbala Governorate, Iraq proved that the sulfate (SO42−) concentrations have high values; so, this water is not suitable for livestock, poultry and irrigation purposes. For reclamation of this water, manufacturing of new sorbent for permeable reactive barrier was required through precipitation of Mg and Fe hydroxides nanoparticles on the activated carbon (AC) surface with best Mg/Fe molar ratio of 7.5/2.5. Mixture of 50% coated AC and 50% scrap iron was applied to eliminate SO42− from contaminated water with efficiency of 59% and maximum capacity of adsorption equals to 9.5 mg/g for a time period of 1 h, sorbent dosage 40 g/L, and initial pH = 5 at 50 mg/L initial SO42− concentration and 200 rpm shaking speed. Characterization analyses certified that the plantation of Mg and Fe nanoparticles onto AC was achieved. Continuous tests showed that the longevity of composite sorbent is increased with thicker bed and lower influent concentration and flow rate. Computer solution (COMSOL) software was well simulated for continuous measurements. The reclamation of real contaminated groundwater was achieved in column set-up with efficiency of 70% when flow rate was 5 mL/min, bed depth was 50 cm and inlet SO42− concentration was 2301 mg/L.Validerad;2021;Nivå 2;2021-07-29 (beamah);Forskningsfinansiär: Taif University (TURSP-2020/49)</p",'MDPI AG',New Composite Sorbent for Removal of Sulfate Ions from Simulated and Real Groundwater in the Batch and Continuous Tests,10.3390/molecules26144356,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226738815,2018-01-01T00:00:00,"It is with great pleasure that we present this Special Issue of
the Journal of Signal Processing Systems (JSPS) dedicated to
Embedded Computer Vision! We are pleased to include six
state-of-the-art papers from the leaders in this field, both from
industry and academia, who keep pushing the embedded computer vision technology forward.
While the idea for this special issue originated between the Guest Editors at one of the CVPR workshops
on the same topic that we have organized, it is the work
of the contributing authors that makes it a success. The
papers were solicited from the workshop participants
and through an open call for papers, so the initial submissions were in many ways already pre-filtered. Out of
24 submitted papers, the highly selective review process
yielded the six papers included here. They cover a
broad range of challenges that are encountered in practical deployment of embedded vision systems, especially
when high computational performance needs meet limited resources. We present papers describing a range of
novel solutions: a deep learning accelerator, a robust
aerial tracking system, an FPGA-based aerial visual
servoing task solution, an approach to use low-cost
hardware for real-time vision, a real-time motion detector, and an image enhancement approach based on human vision",'Springer Science and Business Media LLC',Guest Editorial: Special Issue on Embedded Computer Vision,10.1007/s11265-018-1365-8,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
352949441,2019-01-01T00:00:00,"Complexity is a fundamental part of product design and manufacturing today, owing to increased demands for customization and advances in digital design techniques. Assembling and repairing such an enormous variety of components means that workers are cognitively challenged, take longer to search for the relevant information and are prone to making mistakes. Although in recent years deep learning approaches to object recognition have seen rapid advances, the combined potential of deep learning and augmented reality in the industrial domain remains relatively under explored. In this paper we introduce AR-ProMO, a combined hardware/software solution that provides a generalizable assistance system for identifying mistakes during product assembly and repair",'Association for Computing Machinery (ACM)',Handling Work Complexity with AR/Deep Learning,10.1145/3369457.3370919,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
323194776,2019-11-02T00:00:00,"This paper outlines real-world control challenges faced by modern-day biopharmaceutical facilities through the extension of a previously developed industrial-scale penicillin fermentation simulation (IndPenSim). The extensions include the addition of a simulated Raman spectroscopy device for the purpose of developing, evaluating and implementation of advanced and innovative control solutions applicable to biotechnology facilities. IndPenSim can be operated in fixed or operator controlled mode and generates all the available on-line, off-line and Raman spectra for each batch. The capabilities of IndPenSim were initially demonstrated through the implementation of a QbD methodology utilising the three stages of the PAT framework. Furthermore, IndPenSim evaluated a fault detection algorithm to detect process faults occurring on different batches recorded throughout a yearly campaign. The simulator and all data presented here are available to download at www.industrialpenicillinsimulation.com and acts as a benchmark for researchers to analyse, improve and optimise the current control strategy implemented on this facility. Additionally, a highly valuable data resource containing 100 batches with all available process and Raman spectroscopy measurements is freely available to download. This data is highly suitable for the development of big data analytics, machine learning (ML) or artificial intelligence (AI) algorithms applicable to the biopharmaceutical industry",,Modern day monitoring and control challenges outlined on an industrial-scale benchmark fermentation process,,https://core.ac.uk/download/323194776.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479718027,2021-01-01T00:00:00,"The evaluation of groundwater quality in the Dammam formation, Faddak farm, Karbala Governorate, Iraq proved that the sulfate (SO42−) concentrations have high values; so, this water is not suitable for livestock, poultry and irrigation purposes. For reclamation of this water, manufacturing of new sorbent for permeable reactive barrier was required through precipitation of Mg and Fe hydroxides nanoparticles on the activated carbon (AC) surface with best Mg/Fe molar ratio of 7.5/2.5. Mixture of 50% coated AC and 50% scrap iron was applied to eliminate SO42− from contaminated water with efficiency of 59% and maximum capacity of adsorption equals to 9.5 mg/g for a time period of 1 h, sorbent dosage 40 g/L, and initial pH = 5 at 50 mg/L initial SO42− concentration and 200 rpm shaking speed. Characterization analyses certified that the plantation of Mg and Fe nanoparticles onto AC was achieved. Continuous tests showed that the longevity of composite sorbent is increased with thicker bed and lower influent concentration and flow rate. Computer solution (COMSOL) software was well simulated for continuous measurements. The reclamation of real contaminated groundwater was achieved in column set-up with efficiency of 70% when flow rate was 5 mL/min, bed depth was 50 cm and inlet SO42− concentration was 2301 mg/L.Validerad;2021;Nivå 2;2021-07-29 (beamah);Forskningsfinansiär: Taif University (TURSP-2020/49)</p",'MDPI AG',New Composite Sorbent for Removal of Sulfate Ions from Simulated and Real Groundwater in the Batch and Continuous Tests,10.3390/molecules26144356,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
480201885,2021-08-01T00:00:00,"The increasing digitalization and advancement in information communication technologies has greatly changed how humans interact with digital information. Nowadays, it is not sufficient to only display relevant data in production activities, as the enormous amount of data generated from smart devices can overwhelm operators without being fully utilized. Operators often require extensive knowledge of the machines in use to make informed decisions during processes such as maintenance and production. To enable novice operators to access such knowledge, it is important to reinvent the way of interacting with digitally enhanced smart devices. In this research, a mobile augmented reality remote monitoring system is proposed to help operators with low knowledge and experience level comprehend digital twin data of a device and interact with the device. It analyses both historic logs as well as real-time data through a cloud server and enriches 2D data with 3D models and animations in the 3D physical space. A cloud-based machine learning algorithm is applied to transform learned knowledge into live presentations on a mobile device for users to interact with. A scaled-down case study is conducted using a tower crane model to demonstrate the potential benefits as well as implications when the system is deployed in industrial environments. This user study verifies that the proposed solution yields consistent measurable improvements for novice users in human-device interaction that is statistically significant",'MDPI AG',An Integrated Mobile Augmented Reality Digital Twin Monitoring System,10.3390/computers10080099,,"[{'title': 'Computers', 'identifiers': ['issn:2073-431X', '2073-431x']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
401914974,2021-03-29T00:00:00,"Purpose: This paper discusses the impact of COVID-19 on the Visual Arts industry in Malaysia. In general, this pandemic has affected various forms of artistic activities and the income of visual arts artists and galleries. The cancellation of art projects and exhibitions has greatly affected the artist's source of income as well as disrupted the sale of works and forms of art appreciation. The crisis has also opened up a new form to the visual arts industry by looking at alternative approaches to the continuity of the arts field by switching to virtual or online methods. This emerging crisis of COVID-19 might be the starting point for all art practitioners including artists, art critics, galleries/museums, collectors, and curators in using the online space to continue to capitalize on and expand the Visual Arts industry.
Design/methodology/approach: Review approach.
Findings: The COVID-19 pandemic has made a huge impact on the country's Visual Arts industry where a wide range of art activities cannot be implemented and opened up opportunities for online activities
Practical implications: Exhibition and sale of works through online approach has become one of the main methods that support the Visual Arts industry with the application of a combination of the latest technologies such as VR and AI that enable the representation of real experiences in the context of art appreciation.
Originality/value: This paper is original.
Paper type: This paper can be categorized as a viewpoin",'Narotama University',Covid -19: The Impact On Malaysian Visual Arts Scene,10.29138/ijebd.v4i2.1117,https://core.ac.uk/download/401914974.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479723532,2021-01-01T00:00:00,"An increase in unplanned downtime of machines disrupts and degrades the industrial business, which results in substantial credibility damage and monetary loss. The cutting tool is a critical asset of the milling machine; the failure of the cutting tool causes a loss in industrial productivity due to unplanned downtime. In such cases, a proper predictive maintenance strategy by real-time health monitoring of cutting tools becomes essential. Accurately predicting the useful life of equipment plays a vital role in the predictive maintenance arena of industry 4.0. Many active research efforts have been done to estimate tool life in varied directions. However, the consolidated study of the implemented techniques and future pathways is still missing. So, the purpose of this paper is to provide a systematic and comprehensive literature survey on the data-driven approach of Remaining Useful Life (RUL) estimation of cutting tools during the milling process. The authors have summarized different monitoring techniques, feature extraction methods, decision-making models, and available sensors currently used in the data-driven model. The authors have also presented publicly available datasets related to milling under various operating conditions to compare the accuracy of the prediction model for tool wear estimation. Finally, the article concluded with the challenges, limitations, recent advancements in RUL prognostics techniques using Artificial Intelligence (AI), and future research scope to explore more in this area",'Institute of Electrical and Electronics Engineers (IEEE)',"Data-Driven Remaining Useful Life Estimation for Milling Process: Sensors, Algorithms, Datasets, and Future Directions",10.1109/ACCESS.2021.3101284,,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
200955322,2019-04-01T00:00:00,"Introduction. It is shown that the digital twin (electronic passport) of a CNC machine is developed as a cyber-physical system. The work objective is to create neural network models to determine the operation of a CNC machine, its performance and dynamic stability under cutting.Materials and Methods. The development of mathematical models of machining processes using a sensor system and the Industrial Internet of Things is considered. Machine learning methods valid for the implementation of the above tasks are evaluated. A neural network model of dynamic stability of the cutting process is proposed, which enables to optimize the machining process at the stage of work preparation. On the basis of nonlinear dynamics approaches, the attractors of the dynamic cutting system are reconstructed, and their fractal dimensions are determined. Optimal characteristics of the equipment are selected by input parameters and debugging of the planned process based on digital twins.Research Results. Using machine learning methods allowed us to create and explore neural network models of technological systems for cutting, and the software for their implementation. The possibility of applying decision trees for the problem of diagnosing and classifying malfunctions of CNC machines is shown.Discussion and Conclusions. In real production, the technology of digital twins enables to optimize processing conditions considering the technical and dynamic state of CNC machines. This provides a highly accurate assessment of the production capacity of the enterprise under the development of the production program. In addition, equipment failures can be identified in real time on the basis of the intelligent analysis of the distributed sensor system data",'FSFEI HE Don State Technical University',Development of digital twin of CNC unit based on machine learning methods,10.23947/1992-5980-2019-19-1-45-55,,"[{'title': None, 'identifiers': ['issn:2687-1653', '2687-1653']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
220155640,2019-01-01T00:00:00,"Adequate testing of AI applications is essential to ensure their quality. However, it is often prohibitively difficult to generate realistic test cases or to check software correctness. This paper proposes a new method called datamorphic testing, which consists of three components: a set of seed test cases, a set of datamorphisms for transforming test cases, and a set of metamorphisms for checking test results. With an example of face recognition application, the paper demonstrates how to develop datamorphic test frameworks, and illustrates how to perform testing in various strategies, and validates the approach using an experiment with four real industrial applications of face recognition",,Datamorphic testing: A method for testing intelligent applications,,https://core.ac.uk/download/220155640.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
437429008,2022-03-31T00:00:00,"The Internet of Things (IoT), in combination with advancements in Big Data, communications and networked systems, offers a positive impact across a range of sectors including health, energy, manufacturing and transport. By virtue of current business models adopted by manufacturers and ICT operators, IoT devices are deployed over various networked infrastructures with minimal security, opening up a range of new attack vectors. Conventional rule-based intrusion detection mechanisms used by network management solutions rely on pre-defined attack signatures and hence are unable to identify new attacks. In parallel, anomaly detection solutions tend to suffer from high false positive rates due to the limited statistical validation of ground truth data, which is used for profiling normal network behaviour. In this work we go beyond current solutions and leverage the coupling of anomaly detection and Cyber Threat Intelligence (CTI) with parallel processing for the profiling and detection of emerging cyber attacks. We demonstrate the design, implementation, and evaluation of Citrus: a novel intrusion detection framework which is adept at tackling emerging threats through the collection and labelling of live attack data by utilising diverse Internet vantage points in order to detect and classify malicious behaviour using graph-based metrics as well as a range of machine learning (ML) algorithms. Citrus considers the importance of ground truth data validation and its flexible software architecture enables both the real-time and offline profiling, detection and classification of emerging cyber-attacks under optimal computational costs. Thus, establishing it as a viable and practical solution for next generation network defence and resilience strategies",'Institute of Electrical and Electronics Engineers (IEEE)',Practical Intrusion Detection of Emerging Threats,10.1109/TNSM.2021.3091517,https://core.ac.uk/download/437429008.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
384308353,2021-01-25T00:00:00,"Today’s factories are considered as smart ecosystems with humans, machines and devices interacting with each other for efficient manufacturing of products. Industry 4.0 is a suite of enabler technologies for such smart ecosystems that allow transformation of industrial processes. When implemented, Industry 4.0 technologies have a huge impact on efficiency, productivity and profitability of businesses. The adoption and implementation of Industry 4.0, however, require to overcome a number of practical challenges, in most cases, due to the lack of modernisation and automation in place with traditional manufacturers. This paper presents a first of its kind case study for moving a traditional food manufacturer, still using the machinery more than one hundred years old, a common occurrence for small- and medium-sized businesses, to adopt the Industry 4.0 technologies. The paper reports the challenges we have encountered during the transformation process and in the development stage. The paper also presents a smart production control system that we have developed by utilising AI, machine learning, Internet of things, big data analytics, cyber-physical systems and cloud computing technologies. The system provides novel data collection, information extraction and intelligent monitoring services, enabling improved efficiency and consistency as well as reduced operational cost. The platform has been developed in real-world settings offered by an Innovate UK-funded project and has been integrated into the company’s existing production facilities. In this way, the company has not been required to replace old machinery outright, but rather adapted the existing machinery to an entirely new way of operating. The proposed approach and the lessons outlined can benefit similar food manufacturing industries and other SME industries",'Springer Science and Business Media LLC',Towards design and implementation of Industry 4.0 for food manufacturing,10.1007/s00521-021-05726-z,https://core.ac.uk/download/384308353.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
288551532,2019,"In proposing a machine learning approach for a flow shop scheduling problem with alternative resources, sequence-dependent setup times, and blocking, this paper seeks to generate a tree-based priority rule in terms of a well-performing decision tree (DT) for dispatching jobs. Furthermore, generating a generic DT and RF that yields competitive results for instance scenarios that structurally differ from the training instances was another goal of our research. The proposed DT relies on high quality solutions, obtained using a constraint programming (CP) formulation. Novel aspects include a unified representation of job sequencing and machine assignment decisions, as well as the generation of random forests (RF) to counteract overfitting behaviour. To show the performance of the proposed approaches, different instance scenarios for two objectives (makespan and total tardiness minimisation) were implemented, based on randomised problem data. The background of this approach is a real-world physical system of an industrial partner that represents a typical shop floor for many production processes, such as furniture and window construction. The results of a comparison of the DT and RF approach with two priority dispatching rules, the original CP solutions and tight lower bounds retrieved from a strengthened mixed-integer programming (MIP) formulation show that the proposed machine learning approach performs well in most instance sets for the makespan objective and in all sets for the total tardiness objective.© The Author(s) 201",'Springer Science and Business Media LLC',"A machine learning approach for flow shop scheduling problems with alternative resources, sequence-dependent setup times, and blocking",10.1007/s00291-019-00567-8,,"[{'title': None, 'identifiers': [' 0171-6468', 'ISSN: 0171-6468']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
427382360,2019-09-19T00:00:00,"Driven by the demand to accommodate today’s growing mobile traffic, 5G is designed to

be a key enabler and a leading infrastructure provider in the information and communication technology

industry by supporting a variety of forthcoming services with diverse requirements. Considering the everincreasing

complexity of the network, and the emergence of novel use cases such as autonomous cars,

industrial automation, virtual reality, e-health, and several intelligent applications, machine learning (ML)

is expected to be essential to assist in making the 5G vision conceivable. This paper focuses on the potential

solutions for 5G from an ML-perspective. First, we establish the fundamental concepts of supervised,

unsupervised, and reinforcement learning, taking a look at what has been done so far in the adoption of

ML in the context of mobile and wireless communication, organizing the literature in terms of the types of

learning.We then discuss the promising approaches for how ML can contribute to supporting each target 5G

network requirement, emphasizing its specific use cases and evaluating the impact and limitations they have

on the operation of the network. Lastly, this paper investigates the potential features of Beyond 5G (B5G),

providing future research directions for how ML can contribute to realizing B5G. This article is intended

to stimulate discussion on the role that ML can play to overcome the limitations for a wide deployment of

autonomous 5G/B5G mobile and wireless communications",'Institute of Electrical and Electronics Engineers (IEEE)',"Machine Learning for 5G/B5G Mobile and Wireless Communications: Potential, Limitations, and Future Directions",10.1109/ACCESS.2019.2942390,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
347168399,2020-11-18T00:00:00,"International audienceComplex  industrial  systems  are  increasingly  software  driven,  rapidly  evolving  into autonomous,  self-adaptive  processing  at  industrial  field.  Artificial  intelligence technologies are spreading fast at industrial Edge, improving the industrial operations efficiency. However Edge computing systems involving artificial intelligence must also continuously ensure the safe operations. This paper assesses the state of the art of cognitive technologies with their relevance for artificial intelligence implementation at industrial  safety  critical  systems.  It  introduces  then  a  state of  the  art  for  artificial intelligence  safety  assurance  practices.  Implementation  at  the industrial  application stack is illustrated with the edge virtual operating system that operates the industrial fog. Edge operating system is evolving fast as kind of an AI intensive software. Industrial developments are illustrated with Slap OS, real case examples of swarm computing and advanced robotic",HAL CCSD,Industrial field autonomous systems: AI-assisted distributed applications at Edge,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
490870289,2021-03-01T00:00:00,"Recent advancements in the field of artificial intelligence have demonstrated success in a variety of clinical tasks secondary to the development and application of big data, supercomputing, sensor networks, brain science, and other technologies. However, no projects can yet be used on a large scale in real clinical practice because of the lack of standardized processes, lack of ethical and legal supervision, and other issues. We analyzed the existing problems in the field of artificial intelligence and herein propose possible solutions. We call for the establishment of a process framework to ensure the safety and orderly development of artificial intelligence in the medical industry. This will facilitate the design and implementation of artificial intelligence products, promote better management via regulatory authorities, and ensure that reliable and safe artificial intelligence products are selected for application",'SAGE Publications',"Opportunities and challenges of artificial intelligence in the medical field: current application, emerging problems, and problem-solving strategies",10.1177/03000605211000157,,"[{'title': 'Journal of International Medical Research', 'identifiers': ['issn:1473-2300', '1473-2300']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
490887727,2021-10-01T00:00:00,"Real-time dynamic monitoring of orchard grape leaf diseases can greatly improve the efficiency of disease control and is of great significance to the healthy and stable development of the grape industry. Traditional manual disease-monitoring methods are inefficient, labor-intensive, and ineffective. Therefore, an efficient method is urgently needed for real-time dynamic monitoring of orchard grape diseases. The classical deep learning network can achieve high accuracy in recognizing grape leaf diseases; however, the large amount of model parameters requires huge computing resources, and it is difficult to deploy to actual application scenarios. To solve the above problems, a cross-channel interactive attention mechanism-based lightweight model (ECA-SNet) is proposed. First, based on 6,867 collected images of five common leaf diseases of measles, black rot, downy mildew, leaf blight, powdery mildew, and healthy leaves, image augmentation techniques are used to construct the training, validation, and test set. Then, with ShuffleNet-v2 as the backbone, an efficient channel attention strategy is introduced to strengthen the ability of the model for extracting fine-grained lesion features. Ultimately, the efficient lightweight model ECA-SNet is obtained by further simplifying the network layer structure. The model parameters amount of ECA-SNet 0.5× is only 24.6% of ShuffleNet-v2 1.0×, but the recognition accuracy is increased by 3.66 percentage points to 98.86%, and FLOPs are only 37.4 M, which means the performance is significantly better than other commonly used lightweight methods. Although the similarity of fine-grained features of different diseases image is relatively high, the average F1-score of the proposed lightweight model can still reach 0.988, which means the model has strong stability and anti-interference ability. The results show that the lightweight attention mechanism model proposed in this paper can efficiently use image fine-grained information to diagnose orchard grape leaf diseases at a low computing cost",'Frontiers Media SA',Fine-Grained Grape Leaf Diseases Recognition Method Based on Improved Lightweight Attention Network,10.3389/fpls.2021.738042,https://core.ac.uk/download/490887727.pdf,"[{'title': 'Frontiers in Plant Science', 'identifiers': ['1664-462x', 'issn:1664-462X']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
295509361,2019-06,"In this study, we use unmanned aerial vehicles equipped with multispectral cameras to search for bodies in maritime rescue operations. A series of flights were performed in open‐water scenarios in the northwest of Spain, using a certified aquatic rescue dummy in dangerous areas and real people when the weather conditions allowed it. The multispectral images were aligned and used to train a convolutional neural network for body detection. An exhaustive evaluation was performed to assess the best combination of spectral channels for this task. Three approaches based on a MobileNet topology were evaluated, using (a) the full image, (b) a sliding window, and (c) a precise localization method. The first method classifies an input image as containing a body or not, the second uses a sliding window to yield a class for each subimage, and the third uses transposed convolutions returning a binary output in which the body pixels are marked. In all cases, the MobileNet architecture was modified by adding custom layers and preprocessing the input to align the multispectral camera channels. Evaluation shows that the proposed methods yield reliable results, obtaining the best classification performance when combining green, red‐edge, and near‐infrared channels. We conclude that the precise localization approach is the most suitable method, obtaining a similar accuracy as the sliding window but achieving a spatial localization close to 1 m. The presented system is about to be implemented for real maritime rescue operations carried out by Babcock Mission Critical Services Spain.This study was performed in collaboration with BabcockMCS Spain and funded by the Galicia Region Government through the Civil UAVs Initiative program, the Spanish Government’s Ministry of Economy, Industry, and Competitiveness through the RTC‐2014‐1863‐8 and INAER4‐14Y (IDI‐20141234) projects, and the grant number 730897 under the HPC‐EUROPA3 project supported by Horizon 2020",Wiley Periodicals,Detection of bodies in maritime rescue operations using unmanned aerial vehicles with multispectral cameras,10.1002/rob.21849,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
477732650,2021-07-11T00:00:00,"International audienceHybrid optimisation methods using machine learning tools are a hot topic in combinatorial optimization.The AI promise is to learn from past solutions or in real time what are the markers of good solutions and then to guide the resolution of the problem.However, for many hard problems such as VRP, a wide range of powerful solvers have already proven their efficiency whereas the most recent successes in machine learning (e.g deep learning) need a huge amount of training data before reaching a satisfactory performance level.In this paper, we study the efficiency of a features-guided multiple-neighborhood search (FG-MNS) for realistic instances of HFVRP.The solver is based on two steps.In the learning step, a powerful solver (RADOS) is used to generate a set of solutions, which are characterized by a set of features described in previous work (Lucas et al., 2019, Lucas et. al, 2020).Then, a decision tree is built on this training set to determine where are the promising areas in the features spaces.In a second step, called exploitation step, the solver uses some rules extracted from the decision tree to guide the solution toward a promising area.We present a wide range of experimentations with different variants of the FG-MNS solver (offline, online).While the results are promising for a better understanding of what makes a good solution, the real benefits of machine learning in an industrial implementation are questionable and discussed in this work",HAL CCSD,Some insights about the use of machine learning for solving VRP,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
490881795,2021-10-01T00:00:00,"Real-time business practices require huge amounts of data directly from the production assets. This new thirst for accurate and timely data has forced the convergence of the traditionally business-focused information technology (IT) environment with the production-focused operational technology (OT). Recently, software-defined network (SDN) methodologies have benefitted OT networks with enhanced situational awareness, centralized configuration, deny-by-default forwarding rules, and increased performance. What makes SDNs so innovative is the separation between the control plane and the data plane, centralizing the command in the controllers. However, due to their young age, the use of SDNs in the industry context has not yet matured comprehensive SDN-based architectures for IT/OT networks, which are also resistant to security attacks such as denial-of-service ones, which may occur in SDN-based industrial IoT (IIoT) networks. One main motivation is that the lack of comprehensive SDN-based architectures for IT/OT networks making it difficult to effectively simulate, analyze, and identify proper detection and mitigation strategies for DoS attacks in IT/OT networks. No consolidated security solutions are available that provide DoS detection and mitigation strategies in IT/OT networks. Along this direction, this paper’s contributions are twofold. On the one hand, this paper proposes a convergent IT/OT SDN-based architecture applied in a real implementation of an IT/OT support infrastructure called SIRDAM4.0 within the context of the SBDIOI40 project. On the other hand, this paper proposes a qualitative analysis on how this architecture works under DoS attacks, focusing on what the specific problems and vulnerabilities are. In particular, we simulated several distributed denial-of-service (DDoS) attack scenarios within the context of the proposed architecture to show the minimum effort needed by the attacker to hack the network, and our obtained experimental results show how it is possible to compromise the network, thus considerably worsening the performance and, in general, the functioning of the network. Finally, we conclude our analysis with a brief description on the importance of employing machine learning approaches for attack detection and for mitigation techniques",'MDPI AG',An SDN-Enabled Architecture for IT/OT Converged Networks: A Proposal and Qualitative Analysis under DDoS Attacks,10.3390/fi13100258,,"[{'title': 'Future Internet', 'identifiers': ['issn:1999-5903', '1999-5903']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
373377799,2020-01-01T00:00:00,"Object detection is arguably one of the most important and complex tasks to enable the advent of next-generation autonomous systems. Recent advancements in deep learning techniques allowed a significant improvement in detection accuracy and latency of modern neural networks, allowing their adoption in automotive, avionics and industrial embedded systems, where performances are required to meet size, weight and power constraints.Multiple benchmarks and surveys exist to compare state-of-the-art detection networks, profiling important metrics, like precision, latency and power efficiency on Commercial-off-the-Shelf (COTS) embedded platforms. However, we observed a fundamental lack of fairness in the existing comparisons, with a number of implicit assumptions that may significantly bias the metrics of interest. This includes using heterogeneous settings for the input size, training dataset, threshold confidences, and, most importantly, platform-specific optimizations, that are especially important when assessing latency and energy-related values. The lack of uniform comparisons is mainly due to the significant effort required to re-implement network models, whenever openly available, on the specific platforms, to properly configure the available acceleration engines for optimizing performance, and to re-train the model using a homogeneous dataset.This paper aims at filling this gap, providing a comprehensive and fair comparison of the best-in-class Convolution Neural Networks (CNNs) for real-time embedded systems, detailing the effort made to achieve an unbiased characterization on cutting-edge system-on-chips. Multi-dimensional trade-offs are explored for achieving a proper configuration of the available programmable accelerators for neural inference, adopting the best available software libraries. To stimulate the adoption of fair benchmarking assessments, the framework is released to the public in an open source repository",'Institute of Electrical and Electronics Engineers (IEEE)',A Systematic Assessment of Embedded Neural Networks for Object Detection,10.1109/ETFA46521.2020.9212130,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479313872,2021-07-01T00:00:00,"Abstract Rapid advancements of artificial intelligence of things (AIoT) technology pave the way for developing a digital‐twin‐based remote interactive system for advanced robotic‐enabled industrial automation and virtual shopping. The embedded multifunctional perception system is urged for better interaction and user experience. To realize such a system, a smart soft robotic manipulator is presented that consists of a triboelectric nanogenerator tactile (T‐TENG) and length (L‐TENG) sensor, as well as a poly(vinylidene fluoride) (PVDF) pyroelectric temperature sensor. With the aid of machine learning (ML) for data processing, the fusion of the T‐TENG and L‐TENG sensors can realize the automatic recognition of the grasped objects with the accuracy of 97.143% for 28 different shapes of objects, while the temperature distribution can also be obtained through the pyroelectric sensor. By leveraging the IoT and artificial intelligence (AI) analytics, a digital‐twin‐based virtual shop is successfully implemented to provide the users with real‐time feedback about the details of the product. In general, by offering a more immersive experience in human–machine interactions, the proposed remote interactive system shows the great potential of being the advanced human–machine interface for the applications of the unmanned working space",'Wiley',Artificial Intelligence of Things (AIoT) Enabled Virtual Shop Applications Using Self‐Powered Sensor Enhanced Soft Robotic Manipulator,10.1002/advs.202100230,,"[{'title': 'Advanced Science', 'identifiers': ['2198-3844', 'issn:2198-3844']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479300376,2021-07-01T00:00:00,"The evaluation of groundwater quality in the Dammam formation, Faddak farm, Karbala Governorate, Iraq proved that the sulfate (SO42−) concentrations have high values; so, this water is not suitable for livestock, poultry and irrigation purposes. For reclamation of this water, manufacturing of new sorbent for permeable reactive barrier was required through precipitation of Mg and Fe hydroxides nanoparticles on the activated carbon (AC) surface with best Mg/Fe molar ratio of 7.5/2.5. Mixture of 50% coated AC and 50% scrap iron was applied to eliminate SO42− from contaminated water with efficiency of 59% and maximum capacity of adsorption equals to 9.5 mg/g for a time period of 1 h, sorbent dosage 40 g/L, and initial pH = 5 at 50 mg/L initial SO42− concentration and 200 rpm shaking speed. Characterization analyses certified that the plantation of Mg and Fe nanoparticles onto AC was achieved. Continuous tests showed that the longevity of composite sorbent is increased with thicker bed and lower influent concentration and flow rate. Computer solution (COMSOL) software was well simulated for continuous measurements. The reclamation of real contaminated groundwater was achieved in column set-up with efficiency of 70% when flow rate was 5 mL/min, bed depth was 50 cm and inlet SO42− concentration was 2301 mg/L",'MDPI AG',New Composite Sorbent for Removal of Sulfate Ions from Simulated and Real Groundwater in the Batch and Continuous Tests,10.3390/molecules26144356,,"[{'title': 'Molecules', 'identifiers': ['1420-3049', 'issn:1420-3049']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
387260665,2020-01-01T00:00:00,"The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions. (C) 2020 The Author(s). Published by Elsevier Ltd.Funding Agencies|National Natural Science Foundation of ChinaNational Natural Science Foundation of China (NSFC) [71971030]; Shaanxi Provincial Natural Science Foundation of China [2019JM-495]; Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities [300102220203]</p",'Elsevier BV',An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode,10.1016/j.jclepro.2020.123365,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
231954004,2019-08-29T00:00:00,"The paper aims to organize and structure data collected and associated to technologies that powers the abroad concept of Industry 4.0. It starts with the historic evolution of industry, separated by date landmarks and approaches the last transition between 3.0 to 4.0. Apart from the differences between industry models, production data stats show a huge and important transformation in the amount of data related to manufacturing and how that knowledge is processed. The paper also aims to put on debate the lack of solutions regarding the knowledge extraction of data from machines and systems, needed for data analytics. Approaches with cyber-physical systems, machine learning, virtual environments, Industrial IoT 1 and augmented reality, in an
industrial scale, are some of the strategies to power the reading and interpretation of data, in order to promote industrial efficiency.
Real context industrial applications are taken into account in order to state the importance of collected data in the efficiency of a production process. Exploring technologies and concepts to improve digital twins systems, perception and perceived systems as well as maintenance processes are some of the explored implemented strategies that make Industry 4.0. Some possible strategies are presented, as well as the transition for Industry 5.0.publishe",'Association for Computing Machinery (ACM)',Industry focused in data collection: how industry 4.0 is handled by big data,10.1145/3352411.3352414,https://core.ac.uk/download/231954004.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
493120893,2019-10-08T00:00:00,"In order to create a competitive and resource-efficient transport system, the transport policy of the European Union provides for the achievement by 2030 of almost zero carbon dioxide content in the exhaust gases of vehicles used in large urban centers, and by 2050 the phasing out the use of cars, working on traditional fuels. The Republic of Belarus has a high scientific and sufficient industrial potential to participate in the process of promoting electric mobility, taking into account the use of robotics.
JSC ""Instrument-Making Plant Optron"" developed the working documentation and produced prototypes of typical representatives of the line of personal electric vehicles. However, the ongoing research focused on the creation of a preventive diagnostic system for the electric motorcycle, developed by Belarussian researches and its intelligent on-board system, focused primarily on real-time simulation processes, related specifically to the level of artificial intelligence, and on the implementation of executive level algorithms",'Zeal Press',Robotization as One of the Prospects for Electromobility in Belarus,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
427129268,2021-04-28T00:00:00,"Corn kernels detection can be implemented in industry area. This can be implemented in the selection and packaging the corn kernels before it is distributed. This technique can be implemented in the selection and packaging machine to detect corn kernels accurately. Corn kernel images was used before it is implemented in real-time. The objective of this research was corn kernel detection using Convolutional Neural Network (CNN) deep learning. This technique consists of 3 main stages, the first preprocessing or normalizing the input of corn kernels image data by wrapping and cropping, both modeling and training the system, and testing. The experiment used CNN method to classify images of dry corn kernels and to determine the accuracy value. This research used 20 dry corn kernels images as testing from 80 dry corn kernels images which used in training dataset. The accuracy of detection was dependent from the size of image and position when the image was taken. The accuracy is around 80% - 100% by using 7 convolutional layers and the average of accuracy for testing data was 0,90296. The convolutional layer which implemented in CNN has the strength to detect features in the input image. 
&nbsp;Pendeteksian bji jagung kering dapat diimplementasikan pada dunia industri. Khususnya ketika pemilahan dan pembungkusan biji jagung kering dilakukan sebelum dipasarkan. Saat ini pemilahan dan pembungkusan belum mengimplementasikan deteksi biji jagung kering sehingga terkadang di dalam kemasan biji jagung kering sering terdapat biji lainnya. Metode pendeteksian ini dapat diintegrasikan dengan mesin pemilah dan pembungkus di industri biji-bijian salah satunya untuk mendeteksi biji jagung kering. Untuk mendapatkan proses pendeteksian yang akurat, citra biji jagung kering digunakan sebagai data pada metode deep learning sebelum implementasi secara real-time. Tujuan dari penelitian ini adalah mendeteksi citra biji jagung kering dengan menerapkan metode Convolutional Neural Network (CNN) deep learning. Teknik ini terdiri dari 3 tahap utama, pertama preprocessing atau menormalkan data input citra biji jagung dengan melakukan wrapping dan cropping. Kedua, pembentukan model dan pelatihan sistem, yang terakhir adalah melakukan untuk pengujian sistem. Penelitian menggunakan CNN untuk mengenali citra biji jagung kering dan menentukan nilai akurasinya. Pada penelitian ini digunakan 20 citra biji jagung yang digunakan sebagai testing data dari 80 citra biji jagung yang digunakan pada training dataset. Nilai akurasi pendeteksian biji jagung kering dipengaruhi oleh ukuran citra dan posisi pengambilan citra dari kamera smartphone. Penggunaan 7 convolutional layer memberikan nilai akurasi berkisar antara 80% - 100% sehingga nilai rata-rata akurasi testing data sebesar 0,90296. Penggunaan convolutional layer mampu mendeteksi kekuatan bentuk dari suatu citra",'Ikatan Ahli Informatika Indonesia (IAII)',Penerapan Convolutional Neural Network Deep Learning dalam Pendeteksian Citra Biji Jagung Kering,10.29207/resti.v5i2.3040,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
322372256,2019-01-01T00:00:00,"Timely and accurate bearing fault detection and diagnosis is important for reliable and safe operation of industrial systems. In this study, performance of a generic real-time induction bearing fault diagnosis system employing compact adaptive 1D Convolutional Neural Network (CNN) classifier is extensively studied. In the literature, although many studies have developed highly accurate algorithms for detecting bearing faults, their results have generally been limited to relatively small train/test data sets. As opposed to conventional intelligent fault diagnosis systems that usually encapsulate feature extraction, feature selection and classification as distinct blocks, the proposed system takes directly raw time-series sensor data as input and it can efficiently learn optimal features with the proper training. The main advantages of the 1D CNN based approach are 1) its compact architecture configuration (rather than the complex deep architectures) which performs only 1D convolutions making it suitable for real-time fault detection and monitoring, 2) its cost effective and practical real-time hardware implementation, 3) its ability to work without any pre-determined transformation (such as FFT or DWT), hand-crafted feature extraction and feature selection, and 4) its capability to provide efficient training of the classifier with limited size of training data set and limited number of BP iterations. Effectiveness and feasibility of the 1D CNN based fault diagnosis method is validated by applying it to two commonly used benchmark real vibration data sets and comparing the results with the other competing intelligent fault diagnosis methods.Scopu",'Springer Science and Business Media LLC',A Generic Intelligent Bearing Fault Diagnosis System Using Compact Adaptive 1D CNN Classifier,10.1007/s11265-018-1378-3,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
492655583,2022-01-01T00:00:00,"Abstract

Detection techniques for massive multiple-input multiple-output (MIMO) have gained a lot of attention in both academia and industry. Detection techniques have a significant impact on the massive MIMO receivers’ performance and complexity. Although a plethora of research is conducted using the classical detection theory and techniques, the performance is deteriorated when the ratio between the numbers of antennas and users is relatively small. In addition, most of classical detection techniques are suffering from severe performance loss and/or high computational complexity in real channel scenarios. Therefore, there is a significant room for fundamental research contributions in data detection based on the deep learning (DL) approach. DL architectures can be exploited to provide optimal performance with similar complexity of conventional detection techniques. This paper aims to provide insights on DL based detectors to a generalist of wireless communications. We garner the DL based massive MIMO detectors and classify them so that a reader can find the differences between various architectures with a wider range of potential solutions and variations. In this paper, we discuss the performance-complexity profile, pros and cons, and implementation stiffness of each DL based detector’s architecture. Detection in cell-free massive MIMO is also presented. Challenges and our perspectives for future research directions are also discussed. This article is not meant to be a survey of a mature-subject, but rather serve as a catalyst to encourage more DL research in massive MIMO",'Institute of Electrical and Electronics Engineers (IEEE)',Deep learning for massive MIMO uplink detectors,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
322843592,2020-01-23T00:00:00,"International audienceResource management in SDN (e.g. network slicing) is an emerging area that attracts the attention of academia and industry. It is an indispensable technology in 5G systems. To effectively manage and optimize network resources, more intelligence needs to be deployed. Therefore, combining real network data and Machine Learning (ML) with the benefits of SDN can be a promising solution to manage the network resources in an automated and intelligent way. However, a real network dataset can have redundant and unneeded features. Also, ML algorithms are as good as the quality of data and the SDN is a time-critical system that requires real-time processing and decision. Thus, data preprocessing is a necessary task, which helps to keep the relevant features and makes the prediction quicker and more accurate.This work presents a comparative analysis between two feature selection methods, which are Recursive Feature Elimination (RFE) and Information Gain Attribute Evaluation (InfoGain), using several classifiers on different reduced versions of the network’s dataset",HAL CCSD,Network Feature Selection based on Machine Learning for Resource Management,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
234929028,2019,"In recent years, the number of Industry 4.0 enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. 

At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. 

Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. 

To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, machine learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack",'Institute of Electrical and Electronics Engineers (IEEE)',"PREMISES, a scalable data-driven service to predict alarms in slowly-degrading multi-cycle industrial processes",10.1109/BigDataCongress.2019.00032,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
355099865,2020-01-01T00:00:00,"Abstract: Poor management practices of road transport assets posed a challenge to the sustainable development of the transport system in developing countries like Nigeria. Studies in the past focused mainly on the performance of road construction process. However, few studies have evaluated the effect of the fourth industrial revolution (4.0IR) on the road transport assets in developing countries such as Nigeria. The current study aimed at assessing the effect of the fourth industrial revolution towards improving the management practice of road transport assets. Survey instruments were administered to project and facility managers in the Nigerian road construction sector of the economy using a proportionate random sampling technique. Partial least square structural equation modelling was used for data analysis utilising the Warp 7.0 PLS-SEM software algorithm. The software calculates p-values with WarpPLS based on non-parametric algorithms, resampling or stable algorithms and thus does not require that the variables to be normally distributed. The study concluded that 4.0IR drivers have a moderate effect change on the management practice of road transport assets in Nigeria at the moment. The findings imply that management of road assets in Nigeria would moderately improve due to 4.0IR technologies resulting in transport, safety and general efficiency and effectiveness of road networks in Nigeria. The study identified 4.0IR drivers to include; robotics, mobility, virtual and augmented reality, internet of things and cloud computing, machine learning, artificial intelligence, blockchain, 3D printing drones that are built with an attached 3D printer, (the drone hangs a 3D printing nozzle that's fed plastic, concrete mix or other material from a tube connected to the top of the drone's printing path that precisely plotted by software, for a promised printing accuracy of 0.1mm),and digital engineering. This study emanated from the government reports and past studies in the area of road transport asset management practice which the study investigated the major causes of poor practices and assessed the effect of the fourth industrial revolution on the practice",,Effect of the Fourth Industrial Revolution on Road Transport Asset Management Practice in Nigeria,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
490608362,2021-11-29T00:00:00,"Tomato is one of the most significant vegetables in the world. Specifically, for the industrial tomato cultivation, the product is harvested when °Brix are at their peak. Technological advancements nowadays have made Decision Support Systems, based on Machine Learning Algorithms more applicable in a daily basis. Sustainable agriculture is evolving since farmers could be advised by this technology in order to take the best decision for their crops. Farmers who adopt this kind of technology will be able to know the quality of tomatoes. The implementation of a Decision Support System capable to predict the °Brix was conducted, based on various data from previous years, such as quality characteristics, the tomato hybrid used, weather conditions and soil data from the selected fields. Data came from fields from 6 different regions in Peloponnese, Greece over 3 cultivation periods. 12 different algorithms were tested in order to find which is the best one in terms of efficiency. Results of this research showed that the predicted °Brix were following the same pattern as the actual °Brix. This means that the DSS could advise the farmer about the ideal harvesting period where the °Brix will be maximized. The use of this DSS using real time weather data as an input will be a valuable tool for the farmers",'AcademicPres (EAP) Publishing House',Bayesian Ridge Algorithm for Brix Prediction in  Industrial Tomato,10.15835/buasvmcn-hort:2021.0030,https://core.ac.uk/download/490608362.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
389476431,2021-01-01T00:00:00,"This article belongs to the Special Issue The Artificial Intelligence Technologies for Electric Power SystemsThe scheduling of tasks in a production line is a complex problem that needs to take into account several constraints, such as product deadlines and machine limitations. With innovative focus, the main constraint that will be addressed in this paper, and that usually is not considered, is the energy consumption cost in the production line. For that, an approach based on genetic algorithms is proposed and implemented. The use of local energy generation, especially from renewable sources, and the possibility of having multiple energy providers allow the user to manage its consumption according to energy prices and energy availability. The proposed solution takes into account the energy availability of renewable sources and energy prices to optimize the scheduling of a production line using a genetic algorithm with multiple constraints. The proposed algorithm also enables a production line to participate in demand response events by shifting its production, by using the flexibility of production lines. A case study using real production data that represents a textile industry is presented, where the tasks for six days are scheduled. During the week, a demand response event is launched, and the proposed algorithm shifts the consumption by changing task orders and machine usage.This work has received funding from Portugal 2020 under SPEAR project (NORTE-01-0247-FEDER-040224) and from FEDER Funds through COMPETE program and from National Funds through (FCT) under the project UIDB/00760/2020, and CEECIND/02887/2017.info:eu-repo/semantics/publishedVersio",'MDPI AG',Production Line Optimization to Minimize Energy Cost and Participate in Demand Response Events,10.3390/en14020462,https://core.ac.uk/download/389476431.pdf,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
267815482,2019-11-08T00:00:00,"Current culture-based methods for detection and determination of Campylobacter levels on processed chickens takes at least 2\ua0days. Here we sought to develop a new complete, low-cost and rapid (approximately 2·5\ua0h) detection system requiring minimal operator input.We observed a strong correlation between culture-based cell counts and our ability to detect either Campylobacter jejuni or Campylobacter coli by loop-mediated isothermal amplification from the same samples. This knowledge was used to develop a rapid and simple five-step assay to quantify Campylobacter, which was subsequently assessed for its specificity, reproducibility and accuracy in quantifying Campylobacter levels from processed chickens. The assay was found to be highly specific for C. jejuni and C. coli and was capable of distinguishing between samples that are either within or exceeding the industry set target of 6000 Campylobacter colony forming units (CFU) per carcass (equivalent to 12\ua0CFU per ml of chicken rinse) with >90% accuracy relative to culture-based methods.Our method can reliably quantify Campylobacter counts of processed chickens with an accuracy comparable to culture-based assays but provides results within hours as opposed to days.The research presented here will help improve food safety by providing fast Campylobacter detection that will enable the implementation of real-time risk management strategies in poultry processing plants to rapidly test processed chickens and identify effective intervention strategies. This technology is a powerful tool that can be easily adapted for other organisms and thus could be highly beneficial for a broad range of industries",'Wiley',"An easy‐to‐perform, culture‐free Campylobacter point‐of‐management assay for processing plant applications",10.1111/jam.14509,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
295497995,2019-02-19,"In the past, methods for hand sign recognition have been successfully tested in Human Robot Interaction (HRI) using traditional methodologies based on static image features and machine learning. However, the recognition of gestures in video sequences is a problem still open, because current detection methods achieve low scores when the background is undefined or in unstructured scenarios. Deep learning techniques are being applied to approach a solution for this problem in recent years. In this paper, we present a study in which we analyse the performance of a 3DCNN architecture for hand gesture recognition in an unstructured scenario. The system yields a score of 73% in both accuracy and F1. The aim of the work is the implementation of a system for commanding robots with gestures recorded by video in real scenarios.This work was funded by the Ministry of Economy, Industry and Competitiveness from the Spanish Government through the DPI2015-68087-R and the pre-doctoral grant BES-2016-078290, by the European Commission and FEDER funds through the project COMMANDIA (SOE2/P1/F0638), action supported by Interreg-V Sudoe",SciTePress,3DCNN Performance in Hand Gesture Recognition Applied to Robot Arm Interaction,10.5220/0007570208020806,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
328277865,2020-01-01T00:00:00,"As Digital Twins gain more traction and their adoption in industry increases, there is a need to integrate such technology with machine learning features to enhance functionality and enable decision making tasks. This has lead to the emergence of a concept known as Digital Triplet; an enhancement of Digital Twin technology through the addition of an ’intelligent activity layer’. This is a relatively new technology in Industrie 4.0 and research efforts are geared towards exploring its applicability, development and testing of means for implementation and quick adoption. This paper presents the design and implementation of a Digital Triplet for a three-floor elevator system. It demonstrates the integration of a machine learning (ML) object detection model and the system Digital Twin. This was done to introduce an additional security feature that enabled the system to make a decision, based on objects detected and take preliminary security measures. The virtual model was designed in Siemens NX and programmed via Total Integrated Automation (TIA) portal software. The corresponding physical model was fabricated and controlled using a Programmable Logic Controller (PLC) S7 1200. A control program was developed to mimic the general operations of a typical elevator system used in a commercial building setting.  Communication, between the physical and virtual models, was enabled using the OPC-Unified Architecture (OPC-UA) protocol. Object recognition using “You only look once” (YOLOV3) based machine learning algorithm was incorporated. The Digital Triplet’s functionality was tested, ensuring the virtual system duplicated actual operations of the physical counterpart through the use of sensor data. Performance testing was done to determine the impact of the ML module on the real-time functionality aspect of the system. Experiment results showed the object recognition contributed an average of 1.083s to an overall signal travel time of 1.338 s",'MDPI AG',Digital Triplet Approach for Real-Time Monitoring and Control of an Elevator Security System,10.3390/designs4020009,https://core.ac.uk/download/328277865.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
286140236,2019-08-25T08:19:25,"Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.This work was partially supported by NSFC Program (No. 61602403 and 61572426)",'Springer Science and Business Media LLC',Inference of development activities from interaction with uninstrumented applications,10.1007/s10664-017-9547-8,,"[{'title': 'Empirical Software Engineering', 'identifiers': ['issn:1382-3256', '1382-3256']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
327178337,2020,"Industrial and construction vehicles require tight periodic maintenance operations. Their schedule depends on vehicle characteristics and usage. The latter can be accurately monitored through various on-board devices, enabling the application of Machine Learning techniques to analyze vehicle usage patterns and design predictive analytics. This paper presents a data-driven application to automatically schedule the periodic maintenance operations of industrial vehicles. It aims to predict, for each vehicle and date, the actual remaining days until the next maintenance is due. Our Machine Learning solution is designed to address the following challenges: (i) the non-stationarity of the per-vehicle utilization time series, which limits the effectiveness of classic scheduling policies, and (ii) the potential lack of historical data for those vehicles that have recently been added to the fleet, which hinders the learning of accurate predictors from past data. Preliminary results collected in a real industrial scenario demonstrate the effectiveness of the proposed solution on heterogeneous vehicles. The system we propose here is currently under deployment, enabling further tests and tunings",CEUR-WS,Machine learning supported next-maintenance prediction for industrial vehicles,,https://core.ac.uk/download/pdf/327178337.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
482642854,2021-09-01T00:00:00,"The main purpose of AI governance is to take advantage of AI and reduce the risk.AI governance also aims to build a responsible AI via embracing the influencing factors such as technology,law,policy,standard,ethics,morality,safety,economy,as well as society.AI governance has three aspects:individual intelligent governance,group intelligent governance,human-computer cooperation and symbiotic system governance,which can be divided into three levels:technical level,ethical level,social and legal level.There are four key technologies for AI governance,which are intelligible AI,defense against adversarial attacks,modeling and simulation,and real-time audit.The industry is mostly concerned about developing a responsible AI in that by studying the actual practice of AI governance from leading companies like Google,IBM and Microsoft.Furthermore,tools like interpretability,privacy protection and fairness check for AI systems are already in use.At present,the main research topics on AI governance includes software-defined AI governance,key technologies of AI governance,AI governance evaluation in large-scale machine lear-ning,AI governance based on federated learning,standardization of AI governance,enhancement on artificial intelligence and human-in-the-loop AI training",Editorial office of Computer Science,AI Governance and System:Current Situation and Trend,10.11896/jsjkx.210600034,,"[{'title': None, 'identifiers': ['issn:1002-137X', '1002-137x']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
480191673,2021-08-01T00:00:00,"The provision of high data rate services to mobile users combined with improved quality of experience (i.e., zero latency multimedia content) drives technological evolution towards the design and implementation of fifth generation (5G) broadband wireless networks. To this end, a dynamic network design approach is adopted whereby network topology is configured according to service demands. In parallel, many private companies are interested in developing their own 5G networks, also referred to as non-public networks (NPNs), since this deployment is expected to leverage holistic production monitoring and support critical applications. In this context, this paper introduces a 5G NPN architectural approach, supporting among others various key enabling technologies, such as cell densification, disaggregated RAN with open interfaces, edge computing, and AI/ML-based network optimization. In the same framework, potential applications of our proposed approach in real world scenarios (e.g., support of mission critical services and computer vision analytics for emergencies) are described. Finally, scalability issues are also highlighted since a deployment framework of our architectural design in an additional real-world scenario related to Industry 4.0 (smart manufacturing) is also analyzed",'MDPI AG',"A Cost-Efficient 5G Non-Public Network Architectural Approach: Key Concepts and Enablers, Building Blocks and Potential Use Cases",10.3390/s21165578,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
482086944,2021-09-01T00:00:00,"Artificial Intelligence (AI) is one of the hottest topics in our society, especially when it comes to solving data-analysis problems. Industry are conducting their digital shifts, and AI is becoming a cornerstone technology for making decisions out of the huge amount of (sensors-based) data available in the production floor. However, such technology may be disappointing when deployed in real conditions. Despite good theoretical performances and high accuracy when trained and tested in isolation, a Machine-Learning (M-L) model may provide degraded performances in real conditions. One reason may be fragility in treating properly unexpected or perturbed data. The objective of the paper is therefore to study the robustness of seven M-L and Deep-Learning (D-L) algorithms, when classifying univariate time-series under perturbations. A systematic approach is proposed for artificially injecting perturbations in the data and for evaluating the robustness of the models. This approach focuses on two perturbations that are likely to happen during data collection. Our experimental study, conducted on twenty sensors’ datasets from the public University of California Riverside (UCR) repository, shows a great disparity of the models’ robustness under data quality degradation. Those results are used to analyse whether the impact of such robustness can be predictable—thanks to decision trees—which would prevent us from testing all perturbations scenarios. Our study shows that building such a predictor is not straightforward and suggests that such a systematic approach needs to be used for evaluating AI models’ robustness",'MDPI AG',A Systematic Approach for Evaluating Artificial Intelligence Models in Industrial Settings,10.3390/s21186195,,"[{'title': 'Sensors', 'identifiers': ['issn:1424-8220', '1424-8220']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
429696970,2020-01-01T00:00:00,"Development work within an experimental environment, in which certain properties are investigated and optimized, requires many test runs and is therefore often associated with long execution times, costs and risks. This can affect product, material and technology development in industry and research. New digital driver technologies offer the possibility to automate complex manual work steps in a cost-effective way, to increase the relevance of the results and to accelerate the processes many times over. In this context, this article presents a low-cost, modular and open-source machine vision system for test execution and evaluates it on the basis of a real industrial application. For this purpose a methodology for the automated execution of the load intervals, the process documentation and for the evaluation of the generated data by means of machine learning to classify wear levels. The software and the mechanical structure are designed to be adaptable to different conditions, components and for a variety of tasks in industry and research. The mechanical structure is required for tracking the test object and represents a motion platform with independent positioning by machine vision operators or machine learning. An evaluation of the state of the test object is performed by the transfer learning after the initial documentation run. The manual procedure for classifying the visually recorded data on the state of the test object is described for the training material. This leads to an increased resource efficiency on the material as well as on the personnel side since on the one hand the significance of the tests performed is increased by the continuous documentation and on the other hand the responsible experts can be assigned time efficiently. The presence and know-how of the experts are therefore only required for defined and decisive events during the execution of the experiments. Furthermore, the generated data are suitable for later use as an additional source of data for predictive maintenance of the developed object",'Elsevier BV',Application of Machine Learning and Vision for real-time condition monitoring and acceleration of product development cycles,10.1016/j.promfg.2020.11.012,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
491084921,2021-11-01T00:00:00,"For decades, Out-of-Stock (OOS) events have been a problem for retailers and manufacturers. In grocery retailing, an OOS event is used to characterize the condition in which customers do not find a certain commodity while attempting to buy it. This paper focuses on addressing this problem from a manufacturer’s perspective, conducting a case study in a retail packaged foods manufacturing company located in Latin America. We developed two machine learning based systems to detect OOS events automatically. The first is based on a single Random Forest classifier with balanced data, and the second is an ensemble of six different classification algorithms. We used transactional data from the manufacturer information system and physical audits. The novelty of this work is our use of new predictor variables of OOS events. The system was successfully implemented and tested in a retail packaged foods manufacturer company. By incorporating the new predictive variables in our Random Forest and Ensemble classifier, we were able to improve their system’s predictive power. In particular, the Random Forest classifier presented the best performance in a real-world setting, achieving a detection precision of 72% and identifying 68% of the total OOS events. Finally, the incorporation of our new predictor variables allowed us to improve the performance of the Random Forest by 0.24 points in the F-measure",'MDPI AG',Predicting Out-of-Stock Using Machine Learning: An Application in a Retail Packaged Foods Manufacturing Company,10.3390/electronics10222787,,"[{'title': 'Electronics', 'identifiers': ['issn:2079-9292', '2079-9292']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
288353915,2019-06-05T00:00:00,"Approximately one-third of the food produced globally is spoiled or wasted in the food supply chain (FSC). Essentially, it is lost before it even reaches the end consumer. Conventional methods of food waste tracking relying on paper-based logs to collect and analyse the data are costly, laborious, and time-consuming. Hence, an automated and real-time system based on the Internet of Things (IoT) concepts is proposed to measure the overall amount of waste as well as the reasons for waste generation in real-time within the potato processing industry, by using modern image processing and load cell technologies. The images captured through a specially positioned camera are processed to identify the damaged, unusable potatoes, and a digital load cell is used to measure their weight. Subsequently, a deep learning architecture, specifically the Convolutional Neural Network (CNN), is utilised to determine a potential reason for the potato waste generation. An accuracy of 99.79% was achieved using a small set of samples during the training test. We were successful enough to achieve a training accuracy of 94.06%, a validation accuracy of 85%, and a test accuracy of 83.3% after parameter tuning. This still represents a significant improvement over manual monitoring and extraction of waste within a potato processing line. In addition, the real-time data generated by this system help actors in the production, transportation, and processing of potatoes to determine various causes of waste generation and aid in the implementation of corrective actions",,Monitoring potato waste in food manufacturing using image processing and Internet of Things approach,,https://core.ac.uk/download/288353915.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
478595370,2021-10-01T00:00:00,"One source of learning in universities is a digital library. In the era of industry 4.0, most universities have implemented digital libraries in supporting the learning process. However, the reality shows that digital library management is still ineffective. Therefore, the implementation of digital libraries needs to be evaluated for determining the digital library effectiveness used as learning resources in supporting the learning process in universities. Many evaluation tools are used to evaluate the effectiveness of digital libraries but have not provided accurate recommendation results to support decision-making. This research presents an innovation in the form of an evaluation tool that can be used to evaluate the digital library effectiveness in universities. That evaluation tool is called the Alkin-WP-based digital library evaluation software. This software is a desktop platform that contains aspects of measuring the digital library effectiveness by referring to the components of the Alkin evaluation model and the WP (Weighted Product) method. This research aimed to show the effectiveness level of the utilization of Alkin-WP-based digital library evaluation software. This research method was R & D (Research & Development) which refers to the ten development stages of the Borg and Gall model. In this research, development was focused only on a few stages, included: usage trials, final product revision, dissemination, and implementation. The subjects involved in assessing the implementation/utilization of the Alkin-WP-based digital library evaluation software were 35 people, in the usage trials were six people, in product revision were three people, and at the stage of dissemination were 15 people. The tools used to collect data were questionnaires and interview guidelines. The data analysis technique used was descriptive quantitative. The effectiveness level of utilizing the Alkin-WP-based digital library evaluation software was 88.34%. It showed that the evaluation software had effective. The impact of this research results on the scientific field of educational evaluation is being able to show the existence of a new evaluation tool based on educational evaluation and artificial intelligence. That evaluation tool can easier for library heads to make policies for revamping digital library services based on accurate recommendations. Doi: 10.28991/esj-2021-01308 Full Text: PD",'Ital Publication',Utilization of Alkin-WP-Based Digital Library Evaluation Software as Evaluation Tool of Digital Library Effectiveness,10.28991/esj-2021-01308,https://core.ac.uk/download/478595370.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
236628368,2019-08-06T00:00:00,"Industry 4.0 refers to the new technological development occurred at the industrial production systems. It evolved as a result of integrating Internet of Things, Cyber-Physical Systems, Big-Data, Artificial Intelligence, and Cloud Computing in the industrial systems. This integration aided new capabilities to achieve a higher level of business excellence, efficiency, and effectiveness. Total Quality Management (TQM) is a managerial approach to achieve an outstanding business excellence. There are several approaches to apply TQM principles at any organization. Industry 4.0 could be utilized as a key enabler for TQM especially by integrating its techniques with the TQM best practices. This paper suggests a theoretical framework for integrating Industry 4.0 features with the TQM principles (according to ISO 9000:2015 standards family) in order to open the door for further research to address the real impact of utilizing Industry 4.0 for serving the TQM implementation approaches",'Periodica Polytechnica Budapest University of Technology and Economics',Industry 4.0 as a Key Enabler toward Successful Implementation of Total Quality Management Practices,,https://core.ac.uk/download/236628368.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
478076158,2019-01-01T00:00:00,"Mixed norms that promote structured sparsity have numerous applications in signal processing and machine learning problems. In this work, we present a new algorithm, based on a Newton root search technique, for computing the projection onto the l ∞,1 ball, which has found application in cognitive neuroscience and classification tasks. Numerical simulations show that our proposed method is between 8 and 10 times faster on average, and up to 20 times faster for very sparse solutions, than the previous state of the art. Tests on real functional magnetic resonance image data show that, for some data distributions, our algorithm can obtain speed improvements by a factor of between 10 and 100, depending on the implementation. © 2019 Society for Industrial and Applied Mathematics",'Society for Industrial & Applied Mathematics (SIAM)',"Efficient projection onto the ℓ ∞,1 mixed-norm ball using a newton root search method",10.1137/18M1212525,,"[{'title': 'SIAM Journal on Imaging Sciences', 'identifiers': ['1936-4954', 'issn:1936-4954']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
440368519,2020-01-01T00:00:00,"Indoor positioning technologies have gained great interest from both industry and academia. Variety of services and applications can be built based on the availability and accessibility of indoor positioning information, for example indoor navigation and various location-based services. Different approaches have been proposed to provide indoor positioning information to users, in which an underlying system infrastructure is usually assumed to be well deployed in advance to provide the position information to users. Among many others, one common strategy is to deploy a bunch of active sensor nodes, such as WiFi APs and Bluetooth transceivers, to the indoor environment to serve as reference landmarks. The user's current location can thus be obtained directly or indirectly according to the active sensor signals collected by the user. Different from conventional infrastructure-based approaches, which put additional sensor devices to the environment, we utilize available objects in the environment as location landmarks. Leveraging wildly available smartphone devices as customer premises equipment to the user and the cutting-edge deep-learning technology, we investigate the feasibility of an infrastructure-free intelligent indoor positioning system based on visual information only. The proposed scheme has been verified by a real case study, which is to provide indoor positioning information to users in Taipei Main Station, one of the busiest transportation stations in the world. We use available pedestrian directional signage as location landmarks, which include all of the 52 pedestrian directional signs in the testing area. The Google Objection Detection framework is applied for detection and recognition of the pedestrian directional sign. According to the experimental results, we have shown that the proposed scheme can achieve as high as 98% accuracy to successfully identify the 52 pedestrian directional signs for the three test data sets which include 6,341 test images totally. Detailed discussions of the system design and the experiments are also presented in the paper",'American Institute of Mathematical Sciences (AIMS)',An intelligent indoor positioning system based on pedestrian directional signage object detection: a case study of Taipei Main Station,10.3934/mbe.2020015,,"[{'title': 'Mathematical Biosciences and Engineering', 'identifiers': ['issn:1551-0018', '1551-0018']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
482637353,2020-02-01T00:00:00,"Models of decision making optimization in the stock market have been challenged and evaluated by researchers in recent years. Financial and economic knowledge alone will not allow to analyze and facilitate decision making and to determine the appropriate strategy, and one of the most important obstacles in this regard is the complexity of tools and methods of analysis and modeling. The multiplicity of indicators and financial ratios on the one hand and the breadth of data on the other hand are the most important obstacles in the behavioral analysis of financial markets. Accordingly, the present study aims to model the decision-making process in financial markets. In this research, a different approach is presented in conceptual modeling by combining methods and tools of artificial intelligence with financial issues. Based on this, the portfolio will be optimized by extracting appropriate financial ratios considering the effect of time, and then modeling them in a technical expert system assuming a neutral risk investor.  In addition to trying to conclude and analyze based on the realities of the stock market fundamental analysis, the system rules and the classification of companies are also distinguished from similar studies based on the dynamics of the stock market. The proposed model has been implemented using the data of companies in the real estate industry during 2007-2018. The results indicate the proper performance of the proposed model and that it has the appropriate flexibility to decide and select a portfolio",Kharazmi University,Modeling Portfolio Optimization based on Fundamental Analysis using an Expert System in the Real Estate Industry,10.22034/ijsom.2020.1.3,https://core.ac.uk/download/482637353.pdf,"[{'title': None, 'identifiers': ['2383-2525', '2383-1359', 'issn:2383-1359', 'issn:2383-2525']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
395397873,2020-01-01T00:00:00,"Manufacturing companies require efficient maintenance practices in order to improve business performance, ensure equipment availability and reduce process downtime. With the advent of new technology, manufacturing processes are evolving from the traditional ways into digitalized manufacturing. This transformation enables systems and machines to be connected in complex networks as a collaborative community through the industrial internet of things (IIoT) and cyber-physical system (CPS). Hence, advanced maintenance strategies should be developed in order to ensure the successful implementation of Industry 4.0, which aims to transform traditional product-oriented systems into product-service systems (PSS). Today, machines and systems are expected to gain self-awareness and self-predictiveness in order to provide management with more insight on the status of the factory. In this regards, real-time monitoring along with the application of advanced machine learning algorithms based on historical data will enable systems to understand the current operating conditions, predict the remaining useful life and detect anomalies in the process. This paper discusses the necessity of predictive maintenance to achieve a sustainable and service-oriented manufacturing system and provides a methodology to be followed for implementing proactive maintenance in the context of Industry 4.0",'Springer Science and Business Media LLC',Proactive Learning for Intelligent Maintenance in Industry 4.0,10.1007/978-981-15-2341-0_31,,"[{'title': None, 'identifiers': ['issn:1876-1100', '1876-1119', 'issn:1876-1119', '1876-1100']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
483832557,2021-10-01T00:00:00,"Traditional mathematics curriculum faces several issues nowadays. The gap between course materials and students’ real-life mathematical experiences, the scattering of knowledge in different courses, and the lack of mathematics applications to other subjects all hinder the learning of students. The emerg-ing trends in data science, machine learning, and artificial intelligence also impel higher education to enrich and refine mathematics education. In order to better incubate students for future, the experience of enriching undergrad-uate mathematics curriculum with computer science courses is introduced in this study. The curriculum is designed and implemented for students who major in applied mathematics to better stimulate the learning, participation, exercise, and innovation. It provides students with comprehensive theoretical and practical knowledge for the challenges and industrial requirements now-adays. Evaluations, major findings, and lessons learned from three refined courses are discussed for more insight into the following deployment and re-finement of the curriculum",'International Association of Online Engineering (IAOE)',Enriching Undergraduate Mathematics Curriculum with Computer Science Courses,10.3991/ijep.v11i5.21701,,"[{'title': 'International Journal of Engineering Pedagogy (iJEP)', 'identifiers': ['issn:2192-4880', '2192-4880']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
350944836,2020-01-01T00:00:00,"| openaire: EC/H2020/731558/EU//ANASTACIA | openaire: EC/H2020/871808/EU//INSPIRE-5GplusInternet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges.Peer reviewe",'Institute of Electrical and Electronics Engineers (IEEE)',A Machine Learning Security Framework for Iot Systems,10.1109/ACCESS.2020.2996214,,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
440766299,2020-01-01T00:00:00,"The major advantages of spot and seam welding are high speed and adaptability primarily for high-volume and/or high-rate manufacturing. However, this paradigm fails to meet the principles laid down by Industry 4.0 for real-time control towards Zero Defect Manufacturing for each individual product and intuitive technical assistance on the process parameters. In this paper, a Robust Software Platform oriented for a CPS-based Quality Assessment system for Welding is presented based on data derived from IR cameras. Imaging data are pre – processed in real-time and streamed into a module which utilizes Machine Learning algorithms to perform quality assessment. A database enables data archiving and post processing tasks along with an intuitive User Interface which provide visualization capabilities and Decision Support on the welding process parameters. The modules’ IoT-based communication is performed with 5C architecture and is in line with Web Services",'EDP Sciences',A CPS platform oriented for Quality Assessment in welding,10.1051/matecconf/202031801030,https://core.ac.uk/download/440766299.pdf,"[{'title': 'MATEC Web of Conferences', 'identifiers': ['issn:2261-236X', '2261-236x']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
478373242,2019-07-16T00:00:00,"Artificial Intelligence (AI) has wide range of applications in all areas and is gaining the understanding of the society as necessity instead of luxury. AI start ups are working to improve the quality of social interactions (social good), Education, Agriculture, manufacturing, health and medicine and public services. Hence the cost of not developing AI or developing it late is enormous. Despite the opportunities AI technologies may offer, there is a real risk that without thoughtful intervention it may in fact exacerbate structural, economic, social, and political imbalances, and further reinforce entrenched inequalities. For regulators and policymakers around the world, uneven access to technology remains a major concern because of its potential impacts on social and economic inequality. The author has conducted an exploratory research by reviewing related literatures on AI opportunities and challenges from experiences of the developed world and provided a discussion to identify the potential opportunities and expected challenges for AI adoption and implementation in Ethiopia. Finally the author has recommended what should be done.Keywords: Artificial, Intelligence,, Exploratory, Research, Skills, Infrastructure, Data, Privac",'African Journals Online (AJOL)',Artificial intelligence for Ethiopia: opportunities and challenges,10.4314/ict.v16i1.,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
478789585,2021-07-30T00:00:00,"Edge computing is difficult to deploy a complete and reliable security strategy due to its distributed computing architecture and inherent heterogeneity of equipment and limited resources. When malicious attacks occur, the loss will be immeasurable. RBF neural network has strong nonlinear representation ability and fast learning convergence speed, which is suitable for intrusion detection of edge detection industrial control network. In this paper, an improved RBF network intrusion detection model based on multi-algorithm fusion is proposed. kernel principal component analysis (KPCA) is used to extract data dimension and simplify data representation. Then subtractive clustering algorithm(SCM) and grey wolf algorithm(GWO) are used to jointly optimize RBF neural network parameters to avoid falling into local optimum, reduce the calculation of model training and improve the detection accuracy. The algorithm can better adapt to the edge computing platform with weak computing ability and bearing capacity, and realize real-time data analysis.The experimental results of BATADAL data set and Gas data set show that the accuracy of the algorithm is over 99% and the training time of larger samples is shortened by 50 times for BATADAL data set. The results show that the improved RBF network is effective in improving the convergence speed and accuracy in intrusion detection",Agora University Press,Improved RBF Network Intrusion Detection Model Based on Edge Computing with Multi-algorithm Fusion,,https://core.ac.uk/download/478789585.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
478580989,2021-09-06T00:00:00,"Implementation of the Industry 4.0 concept is considered in the context of automation of railway transport. The analysis refers to prerequisites for creation of a universal digital platform integrating automation systems at a marshalling yard.The example of JSC Russian Railways has contributed to describe the main goals of Digital Station concept, aimed at fusion of data from low-level local automation equipment. The presented functionality of the system for control and processing information on movements of wagons and locomotives at the station in real time (SCPI MWL RT) implements the set goals by integrating initial data from all automation and centralised traffic control systems operating at the station, checking it for consistency, eliminating information redundancy and generating in real time the current model of a marshalling yard regarding trains and wagons and based on data «from the wheel».Description of the existing functionality of SCPI MWL RT, implemented at a facility, is followed by the analysis of the advantages of this system for the railway cargo transportation network. The objective of the paper is to present some previously unpublished technical solutions for implementation of the specified functionality. The methods of the research are based on fusion of heterogeneous data received from floor devices, specialised video cameras, as well as from real-time wagon positioning models.It is shown that adoption of new technical solutions for SCPI MWL RT will allow to considerably improve the quality of planning of technological process of classifying railway wagons and of forecasting the need for infrastructure maintenance. Deep learning algorithms presented ensure functioning of the developed solutions in real time with high accuracy. Further steps described refer to implementation of a digital platform in the form of a digital twin of a marshalling yard, creating thus a prerequisite for development of an intelligent automatic machine to control the marshalling yard, as well as for further planned ways to implementation there-of.Реализация концепции «Индустрия 4.0» рассмотрена в контексте автоматизации железнодорожного транспорта. Проанализированы предпосылки для создания универсальной цифровой платформы, объединяющей системы автоматизации на сортировочной станции.На примере ОАО «РЖД» описаны основные цели концепции «Цифровая станция», направленные на слияние данных от низовой автоматики. Представлен функционал системы контроля и подготовки информации о перемещениях вагонов и локомотивов на станции в реальном времени (СКПИ ПВЛ РВ), реализующей поставленные цели путём обобщения исходной информации от всех действующих на станции систем автоматизации и централизации, проверки её на непротиворечивость, устранения избыточности информации и формирование в реальном времени текущей поездной и вагонной модели сортировочной станции на основе данных «от колеса».Описан существующий функционал СКПИ ПВЛ РВ, реализованный на реальном объекте внедрения, и представлены преиму щества данной системы для сети грузовых перевозок на железнодорожном транспорте. Целью работы является представление отдельных ранее не опубликованных технических решений реализации указанного функционала. Методы работы основаны на слиянии разнородных данных, получаемых от напольных устройств, специализированных видеокамер, а также моделей позиционирования вагонов реального времени.Показано, что внедрение новых технических решений в СКПИ ПВЛ РВ позволит на порядок повысить качество планирования выполнения технологического процесса сортировки железнодорожных вагонов и прогноза необходимости обслуживания инфраструктуры. Представлены алгоритмы глубокого обучения, обеспечивающие функционирование разрабатываемых решений в реальном времени с высокой точностью. Описаны дальнейшие шаги по реализации цифровой платформы в виде цифрового двойника сортировочной станции, что создаст предпосылку для разработки интеллектуального автомата управления сортировочной станцией и планируемые пути реализации",'FSBEO HPE Moscow State University of Railway Engineering (MIIT)',Концепция цифровой платформы на сортировочных станциях,10.30932/1992-3252-2021-19-1-60-73,https://core.ac.uk/download/478580989.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
395677188,2020-07-17T00:00:00,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",,https://core.ac.uk/download/395677188.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226908814,2019-07-08T00:00:00,"International audienceManufacturing companies are under a constant pressure due to multiple factors: new competition, disruptive innovations, cost reduction request, etc. To survive, they must strive to innovate and adapt their business model to improve their productivity. Recent developments based on the concept of Industry 4.0 such as big data, new communication protocols and artificial intelligence provide several new avenues to explore. In the specific context of machining, we are working toward the development of a system capable of making the prognostic of the quality (in terms of dimensional conformance) of a workpiece in real time while it is being manufactured. The goal of this paper is to showcase a prototype of the data acquisition aspect of this system and a case study presenting our first results. This case study has been conducted at our industrial partner facility (Quebec, Canada) and is based on the manufacturing of an aircraft component made from Inconel alloy 625 (AMS5666). The proposed prototype is a data acquisition system installed on a 5 axis CNC machines (GROB model G352) used to acquire and to contextualize the vibration signal obtained from the CNC machine sensor. The contextualization of the data is a key component for future work regarding the development of a prognostic system based on supervised machine learning algorithms. In the end, this paper depicts the system architecture as well as its interactions between the multiple systems and software already in place at our industrial partner. This paper also shows preliminary results describing the relationship between the workpiece quality (in terms of respect toward the dimensional requirements) and the extracted features from the sensors signals. We conclude that it is now possible to do the diagnostic of a cutting operation. Additionally, with the same information we show that it is possible to quickly do the general diagnostic of the health state of the machine. Future work regarding this project will include data acquisition from a wider range of products (i.e. different shapes, materials, processes, etc.) and the development of a machine learning based prognostic model",HAL CCSD,Toward the quality prognostic of an aircraft engine workpiece in Inconel Alloy 625: case study and proposed system architecture,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
200859620,2019-05-01T00:00:00,"This paper presents a novel diagonal recurrent neural network hybrid controller based on the shared memory of real-time database structure. The controller uses Data Engine (DE) technology, through the establishment of a unified and standardized software architecture and real-time database in different control stations, effectively solves many problems caused by technical standard, communication protocol, and programming language in actual industrial application: the advanced control algorithm and control system co-debugging difficulties, algorithm implementation and update inefficiency, and high development and operation and maintenance costs effectively fill the current technical gap. More importantly, the control algorithm development uses a unified visual graphics configuration programming environment, effectively solving the problem of integrated control of heterogeneous devices; and has the advantages of intuitive configuration and transparent data processing process, reducing the difficulty of the advanced control algorithms debugging in engineering applications. In this paper, the application of a neural network hybrid controller based on DE in motor speed measurement and control system shows that the system has excellent control characteristics and anti-disturbance ability, and provides an integrated method for neural network control algorithm in a practical industrial control system, which is the major contribution of this article",'MDPI AG',A New Method of Applying Data Engine Technology to Realize Neural Network Control,10.3390/a12050097,,"[{'title': 'Algorithms', 'identifiers': ['issn:1999-4893', '1999-4893']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
390037696,2019-09-11T00:00:00,"The article deals with the prerequisites and generalizes approaches to defining the concept of “creative economy”; on the theoretical basis and practices of creative economy in leading countries of the world an internals and the main principles of its development are identified; genesis of the concept of creative economy is considered; goals, policies, implementation practices in countries around the world are systematized. The position of Ukraine and other countries of the world in the ranking of the global index of creativity are characterized; features of the creative industry in Ukraine are revealed; index of activity of the creative industry of Ukraine is calculated, which indicates a stable tendency of its development.  Potentials of digital technologies, in particular, artificial intelligence, augmented and virtual reality, blockchain technology, to the transformation of creative economy are grounded.The article deals with the prerequisites and generalizes approaches to defining the concept of “creative economy”; on the theoretical basis and practices of creative economy in leading countries of the world an internals and the main principles of its development are identified; genesis of the concept of creative economy is considered; goals, policies, implementation practices in countries around the world are systematized. The position of Ukraine and other countries of the world in the ranking of the global index of creativity are characterized; features of the creative industry in Ukraine are revealed; index of activity of the creative industry of Ukraine is calculated, which indicates a stable tendency of its development.  Potentials of digital technologies, in particular, artificial intelligence, augmented and virtual reality, blockchain technology, to the transformation of creative economy are grounded",Черкаський навчально-науковий інститут Університету банківської справи,The theory and practice of creative economy in the conditions of digitalization,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
433806836,2021-04-30T00:00:00,"The object of research is the process of using information technology in the construction industry. One of the most problematic areas is increasing the efficiency of the construction industry through the introduction of digital technologies. The research carried out is based on the application of an approach that is implemented using artificial intelligence. The study used machine learning and fuzzy logic methods to mark visual data and analyze it for potential threats, as well as to reduce all possible risks. The main feature of this approach is that using machine learning technology, it is possible to reduce the risks of a project before they affect its profit. So, using artificial intelligence in combination with BIM technologies, it is possible to predict work on construction projects based on real-time data, past activities and other factors in such a way as to optimize construction processes. The benefits to be gained from implementing digital processes will become even more evident in future projects as AI continues to analyze company data. This is due to the fact that the proposed approach using fuzzy logic has a number of features, in particular, the more information machine learning algorithms process, the more complex they become. As a result, they provide even more useful information and allow to make even better decisions. This provides an opportunity to minimize risks and efficiently allocate resources when working on projects. Compared to conventional information technology, artificial intelligence can be used to build a knowledge-based security management system and combine statistical probabilities to help mitigate security risks in construction projects.Объектом исследования является процесс использования информационных технологий в строительной отрасли. Одним из самых проблемных мест является повышение эффективности работы строительной отрасли за счет внедрения цифровых технологий. Проведенные исследования базируются на применении подхода, который реализуется с помощью использования искусственного интеллекта. В ходе исследования использовались методы машинного обучения и нечеткой логики, позволяющие отмечать визуальные данные и анализировать их на предмет потенциальных угроз, а также для сокращения всех возможных рисков. Главная особенность данного подхода заключается в том, что с помощью технологии машинного обучения можно сокращать риски проекта до того, как они повлияют на его прибыль. Так, используя искусственный интеллект в сочетании с BIM-технологией, можно на основе данных в режиме реального времени, прошедшей деятельности и других факторов спрогнозировать работу над строительными проектами таким образом, чтобы оптимизировать строительные процессы. Преимущества, которые можно получить в результате внедрения цифровых процессов, будут еще более очевидными при работе над проектами в будущем по мере того, как искусственный интеллект продолжит анализировать данные компаний. Это связано с тем, что предложенный подход с использованием нечеткой логики имеет ряд особенностей, в частности, чем больше информации обрабатывают алгоритмы машинного обучения, тем сложнее они становятся. А в результате они предоставляют еще больше полезной информации и позволяют принимать еще более грамотные ришення. Благодаря этому обеспечивается возможность максимально снизить риски и эффективно распределить ресурсы при работе над проектами. По сравнению с обычными информационными технологиями, искусственный интеллект можно использовать для создания системы управления безопасностью, основанной на имеющихся знаниях, и объединить статистические вероятности для помощи в снижении рисков безопасности строительных проектов.Об'єктом дослідження є процес використання інформаційної технології в будівельній галузі. Одним з найбільш проблемних місць є підвищення ефективності роботи будівельної галузі за рахунок впровадження цифрових технологій. Проведені дослідження базуються на застосуванні підходу, який реалізується за допомогою використання штучного інтелекту. В ході дослідження використовувалися методи машинного навчання та нечіткої логіки, що дозволяють відзначати візуальні дані та аналізувати їх на предмет потенційних загроз, а також для скорочення всіх можливих ризиків. Головна особливість даного підходу полягає в тому, що за допомогою технології машинного навчання можна скорочувати ризики проєкту до того, як вони вплинуть на його прибуток. Так, використовуючи штучний інтелект у поєднанні з BIM-технологіями, можна на основі даних в режимі реального часу, минулої діяльності та інших факторів спрогнозувати роботу над будівельними проєктами таким чином, щоб оптимізувати будівельні процеси. Переваги, які можна отримати в результаті впровадження цифрових процесів, будуть ще більш очевидними при роботі над проєктами в майбутньому в міру того, як штучний інтелект продовжить аналізувати дані компаній. Це пов'язано з тим, що запропонований підхід з використанням нечіткої логіки має ряд особливостей, зокрема чим більше інформації обробляють алгоритми машинного навчання, тим складнішими вони стають. А в результаті вони надають ще більше корисної інформації та дозволяють приймати ще більш грамотні рішення. Завдяки цьому забезпечується можливість максимально знизити ризики та ефективно розподілити ресурси при роботі над проєктами. У порівнянні зі звичайними інформаційними технологіями, штучний інтелект можна використовувати для створення системи управління безпекою, що базується на наявних знаннях, і об'єднати статистичні ймовірності для допомоги в зниженні ризиків безпеки будівельних проєктів",РС ТЕСHNOLOGY СЕNTЕR,Впровадження штучного інтелекту в будівельну галузь та аналіз існуючих технологій,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
326652634,2020-01-01T00:00:00,"Today\u2019s factory machines are ever more connected with SCADA, MES, ERP applications as well as external systems for data analysis. Different types of network architectures must be used for this purpose. For instance, control applications at the lowest level are susceptible to delays and errors while data analysis with machine learning procedures requires to move a large amount of data without real-time constraints. Standard data formats, like Automation Markup Language (AML), have been established to document factory environment, machine placement and network deployment, however, no automatic technique is currently available in the context of Industry 4.0 to choose the best mix of network architectures according to spacial constraints, cost, and performance. We propose to fill this gap by formulating an optimization problem. First of all, spatial and communication requirements are extracted from the AML description. Then, the optimal interconnection of wired or wireless channels is obtained according to application objectives. Finally, this result is back-annotated to AML to be used in the life cycle of the production system. The proposed methodology is described through a small, but complete, smart production plant",'Institute of Electrical and Electronics Engineers (IEEE)',Network Synthesis for Industry 4.0,10.23919/DATE48585.2020.9116407,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
233572457,2020-01-01T00:00:00,"In this article, we propose an augmented reality semiautomatic labeling (ARS), a semiautomatic method which leverages on moving a 2-D camera by means of a robot, proving precise camera tracking, and an augmented reality pen (ARP) to define initial object bounding box, to create large labeled data sets with minimal human intervention. By removing the burden of generating annotated data from humans, we make the deep learning technique applied to computer vision, which typically requires very large data sets, truly automated and reliable. With the ARS pipeline, we created two novel data sets effortlessly, one on electromechanical components (industrial scenario) and other on fruits (daily-living scenario) and trained two state-of-the-art object detectors robustly, based on convolutional neural networks, such as you only look once (YOLO) and single shot detector (SSD). With respect to conventional manual annotation of 1000 frames that takes us slightly more than 10 h, the proposed approach based on ARS allows to annotate 9 sequences of about 35,000 frames in less than 1 h, with a gain factor of about 450. Moreover, both the precision and recall of object detection is increased by about 15% with respect to manual labeling. All our software is available as a robot operating system (ROS) package in a public repository alongside with the novel annotated data sets",'Institute of Electrical and Electronics Engineers (IEEE)',Semiautomatic Labeling for Deep Learning in Robotics,10.1109/TASE.2019.2938316,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
482720363,2021-10-09T00:00:00,"Industrial control systems (ICS) are the backbone for the implementation of cybersecurity solutions. They are susceptible to various attacks, due to openness in connectivity, unauthorized attempts, malicious attacks, use of more commercial off the shelf (COTS) software and hardware, and implementation of Internet protocols (IP) that exposes them to the outside world. Cybersecurity solutions for Information technology (IT) secured with firewalls, intrusion detection/protection systems do nothing much for Operational technology (OT) ICS. An innovative concept of using real operational technology network traffic-based testbed, for cyber-physical system simulation and analysis, is presented. The testbed is equipped with real-time attacks using in-house penetration test tool with reconnaissance, interception, and firmware analysis scenarios. The test cases with different real-time hacking scenarios are implemented with the ICS cyber test kit, and its industrial datasets are captured which can be utilized for Deep packet inspection (DPI). The DPI provides more visibility into the contents of OT network traffic based on OT protocols. The Machine learning (ML) techniques are deployed for cyber-attack detection of datasets from the cyber kit. The performance metrics such as accuracy, precision, recall, F1 score are evaluated and cross validated for different ML algorithms for anomaly detection. The decision tree (DT) ML technique is optimized with pruning method which provides an attack detection accuracy of 96.5%. The deep learning (DL) techniques has been used recently for enhanced OT intrusion detection performances","'Computers, Materials and Continua (Tech Science Press)'",Industrial datasets with ICS testbed and attack detection using machine learning techniques,10.32604/iasc.2022.020801,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
339161568,2020-01-01T00:00:00,"A novel approach is proposed for constructing models of anomaly detectors using supervised learning from the traces of normal and abnormal operations of an Industrial Control System (ICS). Such detectors are of value in detecting process anomalies in complex critical infrastructure such as power generation and water treatment systems. The traces are obtained by systematically “fuzzing”, i.e., manipulating the sensor readings and actuator actions in accordance with the boundaries/partitions that define the system's state. The proposed approach is tested in a Secure Water Treatment (SWaT) testbed – a replica of a real-world water purification plant, located at the Singapore University of Technology and Design. Multiple supervised classifiers are trained using the traces obtained from SWaT. The efficacy of the proposed approach is demonstrated through empirical evaluation of the supervised classifiers under various performance metrics. Lastly, it is shown that the supervised approach results in significantly lower false positive rates as compared to the unsupervised ones.Software Engineerin",'Association for Computing Machinery (ACM)',Domain-Based Fuzzing for Supervised Learning of Anomaly Detection in Cyber-Physical Systems,10.1145/3387940.3391486,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479728860,2021-01-01T00:00:00,"As the core power for the aviation industry, shipbuilding industry, and power station industry, it is essential to ensure that the gas turbines operate safely, reliably, greenly and efficiently. Learn from the advantages and disadvantages of the thermodynamic model based and data-driven artificial intelligence based gas-path diagnosis methods, a newfangled gas turbine gas-path diagnosis approach on the basis of knowledge data-driven artificial intelligence is proposed. That is a hybrid method of deep learning and gas path analysis. First, gas turbine thermodynamic model of the object to be diagnosed is constructed by adaptation modeling strategy. And the engine thermodynamic model is taken as the basal model to simulate various gas path faults. Secondly, a large number of knowledge data corresponding to component health parameters and gas turbine boundary condition parameters &#x0026; gas-path measurable parameters are simulated by setting different component health parameter values and different boundary conditions based on this basal model. And next, define the vector composed of the boundary condition parameters &#x0026; the gas path measurable parameters in the knowledge database as the input vector, and the component health parameter vector as the output vector, and a deep learning model for regression modeling of this knowledge database is designed. At last, along with the gas turbine engine runs, the trained model outputs component health parameters in real time after trained deep learning model is deployed to the corresponding gas turbine power plant. The simulation experiment results show that, accurate and quantified health parameters of each gas path component can be obtained by the proposed method in this paper, and the overall root mean square error does not exceed 0.033&#x0025;, and the maximum relative error does not exceed 0.36&#x0025;, which illustrates the proposed method has great application potential",'Institute of Electrical and Electronics Engineers (IEEE)',Gas Path Fault Diagnosis of Gas Turbine Engine Based on Knowledge Data-Driven Artificial Intelligence Algorithm,10.1109/ACCESS.2021.3101647,,"[{'title': 'IEEE Access', 'identifiers': ['issn:2169-3536', '2169-3536']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479297441,2021-07-01T00:00:00,"Inadequate management practices for solid waste and wastewater are some of the main causes of eutrophication globally, especially in regions where intensive livestock, agricultural, and industrial activities are coupled with inexistent or ineffective waste and wastewater treatment infrastructure. In this study, a methodological approach is presented to spatially assess the trophic state of large territories based on public water quality databases. The trophic state index (TSI) includes total nitrogen, total phosphorus, chlorophyll A, chemical oxygen demand, and Secchi disk depth values as water quality indicators. A geographical information system (GIS) was used to manage the spatiotemporal attributes of the water quality data, in addition to spatially displaying the results of TSI calculations. As a case study, this methodological approach was applied to determine the critical regions for mitigating eutrophication in the state of Jalisco, Mexico. Although a decreasing trend was observed for the TSI values over time for most subbasins (2012–2019), a tendency for extreme hypereutrophication was observed in some regions, such as the Guadalajara metropolitan area and the Altos region, which are of high economic relevance at the state level. A correlation analysis was performed between the TSI parameters and rainfall measurements for all subbasins under analysis, which suggested a tendency for nutrient wash-off during the rainy seasons for most subbasins; however, further research is needed to quantify the real impacts of rainfall by including other variables such as elevation and slope. The relationships between the water quality indicators and land cover were also explored. The GIS methodology proposed in this study can be used to spatially assess the trophic state of large regions over time, taking advantage of available water quality databases. This will enable the efficient development and implementation of public policies to assess and mitigate the eutrophication of water sources, as well as the efficient allocation of resources for critical regions. Further studies should focus on applying integrated approaches combining on-site monitoring data, remote sensing data, and machine learning algorithms to spatially evaluate the trophic state of territories",'MDPI AG',"A GIS Methodology to Determine the Critical Regions for Mitigating Eutrophication in Large Territories: The Case of Jalisco, Mexico",10.3390/su13148029,,"[{'title': 'Sustainability', 'identifiers': ['2071-1050', 'issn:2071-1050']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
479942346,2021-07-15T00:00:00,"This interdisciplinary article presents a concept of the 21st century and phenomena that are products of the 4th industrial revolution – big data and Artificial Intelligence technologies – as well as the opportunities of their application in public governance and social policy. This paper examines the advantages and disadvantages of big data, problems of data collection, its reliability and use. Big data can be used for the analysis and modeling of phenomena relevant to public governance and social policy. Big data consist of three main types: a) historical data, b) present data with little delay, c) prognostic data for future forecasting. The following categories of big data can be defined as: a) data from social networks, b) traditional data from business systems, c) machine-generated data, such as water extraction, pollution, satellite information. The article analyzes the advantages and disadvantages of big data. There are big data challenges such as data security, lack of cooperation in civil service and social work, in rare situations – data fragmentation, incompleteness and erroneous issues, as well as ethical issues regarding the analysis of data and its use in social policy and social administration.
Big data, covered by Artificial Intelligence, can be used in public governance and social policy by identifying “the hot spots” of various phenomena, by prognosing the meanings of variables in the future on the basis of past time rows, and by calculating the optimal motion of actions in the situations where there are possible various alternatives. The technologies of Artificial Intelligence are used more profoundly in many spheres of public policy, and in the governance of COVID-19 pandemics too.
The substantial advantages of the provided big data and Artificial Intelligence are a holistic improvement of public services, possibilities of personalization, the enhancement of citizen satisfaction, the diminishing of the costs of processing expenditure, the targeting of adopted and implemented decisions, more active involvement of citizens, the feedback of the preferences of policy formation and implementation, the observation of social phenomenas in real time, and possibilities for more detailed prognosing.
Challenges to security of data, necessary resources and competences, the lack of cooperation in public service, especially rare instances of data fragmentation, roughness, falseness, and ethical questions regarding data analysis and application can be evaluated as the most significant problems of using big data and Artificial Intelligence technologies.
Big data and their analytics conducted using Artificial Intelligence technologies can contribute to the adequacy and objectivity of decisions in public governance and social policy, effectively curbing corruption and nepotism by raising the authority and confidence of public sector organizations in governance, which is so lacking in the modern world.Šiame tarpdisciplininiame straipsnyje pateikiama XXI amžiaus ketvirtosios pramonės revoliucijos fenomenų – didžiųjų duomenų ir dirbtinio intelekto technologijų – samprata ir aptariamos jų naudojimo viešojo valdymo srityje ir socialinėje politikoje galimybės, nagrinėjami didžiųjų duomenų pranašumai ir trūkumai, jų rinkimo, patikimumo ir naudojimo problemos. Didieji duomenys gali būti naudojami su viešuoju valdymu ir socialine politika susijusių reiškinių analizei ir jiems modeliuoti. Didieji duomenys apima tris duomenų tipus: a) istorinius duomenis, b) dabarties duomenis su mažu pavėlavimu, c) prognostinius duomenis ateičiai prognozuoti. Galima apibrėžti šias didžiųjų duomenų kategorijas: a) duomenis iš socialinių tinklų, b) valdymo sistemų duomenis, c)&nbsp;mašinų generuojamus duomenis, pavyzdžiui, vandens gavybos, užterštumo, palydovų informaciją. Straipsnyje yra analizuojami didžiųjų duomenų pranašumai ir trūkumai. Galimi tokie didžiųjų duomenų iššūkiai, kaip antai: duomenų saugumas, bendradarbiavimo stoka valstybės tarnyboje, labai retai nutinkančios situacijos, duomenų fragmentacija, nebaigtumas ir klaidingumas, etiniai duomenų analizės ir naudojimo viešojo valdymo srityje ir socialinėje politikoje klausimai. Didieji duomenys ir jų analizė naudojant dirbtinio intelekto technologijas gali prisidėti prie viešojo valdymo ir socialinės politikos sprendimų adekvatumo ir objektyvumo,&nbsp;veiksmingai pažaboti korupciją ir nepotizmą didinant viešojo&nbsp;sektoriaus organizacijų autoritetą ir pasitikėjimą valdžia, o jo šiuolaikiniame pasaulyje taip trūksta",'Vilnius University Press',Didžiųjų duomenų ir dirbtinio intelekto technologijų pritaikymo galimybių viešojo valdymo srityje ir socialinėje politikoje analizė,10.15388/STEPP.2021.31,https://core.ac.uk/download/479942346.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
327017697,2020-12-15T00:00:00,"In the current era of Industry 4.0, sensor data used in connection with machine learning algorithms can help manufacturing industries to reduce costs and to predict failures in advance. This paper addresses a binary classification problem found in manufacturing engineering, which focuses on how to ensure product quality delivery and at the same time to reduce production costs. The aim behind this problem is to predict the number of faulty products, which in this case is extremely low. As a result of this characteristic, the problem is reduced to an imbalanced binary classification problem. The authors contribute to imbalanced classification research in three important ways. First, the industrial application coming from the electronic manufacturing industry is presented in detail, along with its data and modelling challenges. Second, a modified cost-sensitive classification strategy based on a combination of Voronoi diagrams and genetic algorithm is applied to tackle this problem and is compared to several base classifiers. The results obtained are promising for this specific application. Third, in order to evaluate the flexibility of the strategy, and to demonstrate its wide range of applicability, 25 real-world data sets are selected from the KEEL repository with different imbalance ratios and number of features. The strategy, in this case implemented without a predefined cost, is compared with the same base classifiers as those used for the industrial problem",,Cost-sensitive learning classification strategy for predicting product failures,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
304117829,2019-01-01T00:00:00,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search",'Institute of Electrical and Electronics Engineers (IEEE)',Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,10.23919/DATE.2019.8714959,https://core.ac.uk/download/304117829.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
237690220,2019-01-01T00:00:00,"In the Architecture, Engineering, Construction and Operations (AECO) there is a growing interest in the use of the Building Information Modelling (BIM). Through integration of information and processes in a digital model, BIM can optimise resources along the lifecycle of a physical asset. Despite the potential savings are much higher in the operational phase, BIM is nowadays mostly used in design and construction stages and there are still many barriers hindering its implementation in Facility Management (FM). Its scarce integration with live data, i.e. data that changes at high frequency, can be considered one of its major limitations in FM. The aim of this research is to overcome this limit and prove that buildings or infrastructures operations can benefit from a digital model updated with live data. The scope of the research concerns the optimisation of FM operations. The optimisation of operations can be further enhanced by the use of maintenance smart contracts allowing a better integration between users’ behaviour and maintenance implementation. In this case study research, the Image Recognition (ImR), a type of Artificial Intelligence (AI), has been used to detect users’ movements in an office building, providing real time occupancy data. This data has been stored in a BIM model, employed as single reliable source of information for FM. This integration can enhance maintenance management contracts if the BIM model is coupled with a smart contract. Far from being a comprehensive case study, this research demonstrates how the transition from BIM to the Asset Information Model (AIM) and, finally, to the Digital Twin (i.e. a near-real-time digital clone of a physical asset, of its conditions and processes) is desirable because of the outstanding benefits that have already been measured in other industrial sectors by applying the principles of Industry 4.0",'WITPRESS LTD.',Office building occupancy monitoring through image recognition sensors,10.2495/SAFE-V9-N4-371-380,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
491248240,2019-01-01T00:00:00,"Predictive Maintenance concerns the smart monitoring of machine to avoid possible future failures, since because it is better to intervene before the damage occurs, saving time and money. In this paper, a Predictive Maintenance methodology based on Machine learning approach is presented and it is applied to a real cutting machine, a woodworking machinery in a real industrial group, producing accurate estimations. This kind of strategy is important to deal with maintenance problems given the ever increasing need to reduce downtime and associated costs. The Predictive Maintenance methodology implemented allows dynamical decision rules that have to be considered for maintenance prediction using a combined approach on Azure Machine Learning Studio. The Three models (RF, GBM and XGBM) allowed the accurately predict machine down ever gripped bearing thanks to the pre-processing phases",'ASME International',An event based machine learning framework for predictive maintenance in industry 4.0,10.1115/DETC2019-97917,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
265120533,2019-12-13,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
344912383,2020-01-01T00:00:00,"Abstract

Internet of Things security is attracting a growing attention from both academic and industry communities. Indeed, IoT devices are prone to various security attacks varying from Denial of Service (DoS) to network intrusion and data leakage. This paper presents a novel machine learning (ML) based security framework that automatically copes with the expanding security aspects related to IoT domain. This framework leverages both Software Defined Networking (SDN) and Network Function Virtualization (NFV) enablers for mitigating different threats. This AI framework combines monitoring agent and AI-based reaction agent that use ML-Models divided into network patterns analysis, along with anomaly-based intrusion detection in IoT systems. The framework exploits the supervised learning, distributed data mining system and neural network for achieving its goals. Experiments results demonstrate the efficiency of the proposed scheme. In particular, the distribution of the attacks using the data mining approach is highly successful in detecting the attacks with high performance and low cost. Regarding our anomaly-based intrusion detection system (IDS) for IoT, we have evaluated the experiment in a real Smart building scenario using one-class SVM. The detection accuracy of anomalies achieved 99.71%. A feasibility study is conducted to identify the current potential solutions to be adopted and to promote the research towards the open challenges",'Institute of Electrical and Electronics Engineers (IEEE)',A machine learning security framework for IoT systems,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226752432,2019-01-01T00:00:00,"This paper reports the development of a manipulation system for electric wires, implemented by means of a commercial gripper installed on an industrial manipulator and equipped with cameras and suitably designed tactile sensors. The purpose of this system is the execution of wire insertion on commercial electromechanical components. The synergy between computer vision and tactile sensing is necessary because, in a real environment, the tight spaces very often prevent the possibility to use the vision system, also when the same task is performed by a human being. A novel technique to speed up the generation of training data sets for convolutional neural networks (CNNs) is proposed. Therefore, this technique is used to train a CNN in order to detect small objects (such as wire terminals). Moreover, aiming to prevent faults during the task and to interact with the environment safely, several machine learning approaches are used to produce an affordable output from the tactile sensor. The proposed approach shows how a cheap sensor embedded with suitable intelligence can provide information comparable to a more expensive force sensor. Note to Practitioners - This paper was motivated by the lack of commercial solution for the automatic cabling of switchgears. Existing approaches to this problem are in some way limited to specific large-scale products or simple layouts. This paper investigated a robust and flexible solution, based on the exploitation of multiple sensors and machine learning algorithms, for wire detection, grasping, and connection. The proposed approach is characterized by simple design and self-tuning capabilities, and it can be easily employed on a wide range of switchgear layouts thanks to the large workspace of the manipulator. Experimental results show that the proposed system is able to achieve a 95% success rate within a realistic admissible region. In the future research, we will integrate the proposed solution with an electromechanical component localization module and a terminal fastening system to evaluate the performance on the real production line",'Institute of Electrical and Electronics Engineers (IEEE)',Integration of robotic vision and tactile sensing for wire-terminal insertion tasks,10.1109/TASE.2018.2847222,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
442409320,2019-01-01T00:00:00,"Within the strongly regulated avionic engineering field, conventional graphical desktop hardware and software application programming interface (API) cannot be used because they do not conform to the avionic certification standards. We observe the need for better avionic graphical hardware, but system engineers lack system design tools related to graphical hardware. The endorsement of an optimal hardware architecture by estimating the performance of a graphical software, when a stable rendering engine does not yet exist, represents a major challenge. As proven by previous hardware emulation tools, there is also a potential for development cost reduction, by enabling developers to have a first estimation of the performance of its graphical engine early in the development cycle. In this paper, we propose to replace expensive development platforms by predictive software running on a desktop computer. More precisely, we present a system design tool that helps predict the rendering performance of graphical hardware based on the OpenGL Safety Critical API. First, we create nonparametric models of the underlying hardware, with machine learning, by analyzing the instantaneous frames per second (FPS) of the rendering of a synthetic 3D scene and by drawing multiple times with various characteristics that are typically found in synthetic vision applications. The number of characteristic combinations used during this supervised training phase is a subset of all possible combinations, but performance predictions can be arbitrarily extrapolated. To validate our models, we render an industrial scene with characteristic combinations not used during the training phase and we compare the predictions to those real values. We find a median prediction error of less than 4 FPS",'Hindawi Limited',Avionics Graphics Hardware Performance Prediction with Machine Learning,10.1155/2019/9195845,,"[{'title': 'Scientific Programming', 'identifiers': ['1058-9244', 'issn:1058-9244', '1875-919x', 'issn:1875-919X']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
385613724,2021-01-01T00:00:00,"This article belongs to the Special Issue The Artificial Intelligence Technologies for Electric Power SystemsThe scheduling of tasks in a production line is a complex problem that needs to take into account several constraints, such as product deadlines and machine limitations. With innovative focus, the main constraint that will be addressed in this paper, and that usually is not considered, is the energy consumption cost in the production line. For that, an approach based on genetic algorithms is proposed and implemented. The use of local energy generation, especially from renewable sources, and the possibility of having multiple energy providers allow the user to manage its consumption according to energy prices and energy availability. The proposed solution takes into account the energy availability of renewable sources and energy prices to optimize the scheduling of a production line using a genetic algorithm with multiple constraints. The proposed algorithm also enables a production line to participate in demand response events by shifting its production, by using the flexibility of production lines. A case study using real production data that represents a textile industry is presented, where the tasks for six days are scheduled. During the week, a demand response event is launched, and the proposed algorithm shifts the consumption by changing task orders and machine usage.This work has received funding from Portugal 2020 under SPEAR project (NORTE-01-0247-FEDER-040224) and from FEDER Funds through COMPETE program and from National Funds through (FCT) under the project UIDB/00760/2020, and CEECIND/02887/2017.info:eu-repo/semantics/publishedVersio",'MDPI AG',Production Line Optimization to Minimize Energy Cost and Participate in Demand Response Events,10.3390/en14020462,,"[{'title': 'Energies', 'identifiers': ['issn:1996-1073', '1996-1073']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
297906931,2019,"Predictive Maintenance concerns the smart monitoring of machine to avoid possible future failures, since because it is better to intervene before the damage occurs, saving time and money. In this paper, a Predictive Maintenance methodology based on Machine learning approach is presented and it is applied to a real cutting machine, a woodworking machinery in a real industrial group, producing accurate estimations. This kind of strategy is important to deal with maintenance problems given the ever increasing need to reduce downtime and associated costs. The Predictive Maintenance methodology implemented allows dynamical decision rules that have to be considered for maintenance prediction using a combined approach on Azure Machine Learning Studio. The Three models (RF, GBM and XGBM) allowed the accurately predict machine down ever gripped bearing thanks to the pre-processing phase",,An event based machine learning framework for predictive maintenance in industry 4.0,10.1115/DETC2019-97917,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
516350676,2021-01-01T00:00:00,"International audienceCollaborative robots are currently deployed in professional environments, in collaboration with professional human operators, helping to strike the right balance between mechanization and manual intervention in manufacturing processes required by Industry 4.0. In this paper, the contribution of gesture recognition and pose estimation to the smooth introduction of cobots into an industrial assembly line is described, with a view to performing actions in parallel with the human operators and enabling interaction between them. The proposed active vision system uses two RGB-D cameras that record different points of view of gestures and poses of the operator, to build an external perception layer for the robot that facilitates spatiotemporal adaptation, in accordance with the human's behavior. The use-case of this work is concerned with LCD TV assembly of an appliance manufacturer, comprising of two parts. The first part of the above-mentioned operation is assigned to a robot, strengthening the assembly line. The second part is assigned to a human operator. Gesture recognition, pose estimation, physical interaction, and sonic notification, create a multimodal human-robot interaction system. Five experiments are performed, to test if gesture recognition and pose estimation can reduce the cycle time and range of motion of the operator, respectively. Physical interaction is achieved using the force sensor of the cobot. Pose estimation through a skeleton-tracking algorithm provides the cobot with human pose information and makes it spatially adjustable. Sonic notification is added for the case of unexpected incidents. A real-time gesture recognition module is implemented through a Deep Learning architecture consisting of Convolutional layers, trained in an egocentric view and reducing the cycle time of the routine by almost 20%. This constitutes an added value in this work, as it affords the potential of recognizing gestures independently of the anthropometric characteristics and the background. Common metrics derived from the literature are used for the evaluation of the proposed system. The percentage of spatial adaptation of the cobot is proposed as a new KPI for a collaborative system and the opinion of the human operator is measured through a questionnaire that concerns the various affective states of the operator during the collaboration",'Frontiers Media SA',Egocentric Gesture Recognition Using 3D Convolutional Neural Networks for the Spatiotemporal Adaptation of Collaborative Robots,10.3389/fnbot.2021.703545,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
288328958,2019-01-01T00:00:00,"The industry is moving towards maintenance strategies that consider component health, which require extensive collection and analysis of data. Condition monitoring methods that require manual feature extraction and analysis, become infeasible on an industrial scale. Machine learning algorithms can be used to automatically detect and classify faults, however, obtaining sufficient data for training is required for deep learning and other data-driven classification approaches. Data from healthy machine operation is generally available in abundance, while data from representative fault- and operating conditions is limited. This limits both development and deployment of deep learning-based CM systems on an industrial scale. This paper addresses both the challenges of automated analysis and lack of training data. A deep learning classifier architecture utilizing 1-dimensional dilated convolutions is proposed. Dilation of the convolution kernel allows for analysis of raw vibration signals while simultaneously maintaining the receptive field of the classifier enough to capture temporal patterns. The proposed method performs classification in time domain on signal segments of 1 second or shorter. With knowledge of the bearing specification, artificial vibration signals with similar characteristics as an actual bearing fault can be created. In this work, generated fault signals are combined with healthy operational data to obtain training data for a deep classifier. Parameters of the vibration model is chosen as distributions rather than fixed values. By using a range parameters in the vibration model, the classifier learns to recognize temporal features from the training data that generalize to unseen data. The effectiveness of the proposed method is demonstrated by training classifiers on generated data and testing on real signals from faulty bearings at both low and high speed. One dataset containing seeded faults and three run-to-failure tests are used for the demonstration.publishedVersio",,Simulation-driven Deep Classification of Bearing Faults from Raw Vibration Data,,https://core.ac.uk/download/288328958.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
427134564,,"Due to the massive developments in information technology, the world as we know it has been massively changing and evolving. This revolution has been named Industrial Revolution 4.0. IR4.0 shrinks the gap between the digital, more technical world and the physical world. The higher education institutes are now being forced to move into a more digital form of education because if they are not, they are found institutes that are too stubborn to move on from the traditional method of education. The more modern technologies such as data analysis, artificial intelligence and other technologies such as cloud computing need to be concentrated on while going through digital transformation. This paper includes an outlook on digitization, benefits of digital transformation, challenges of implementation, how to have a smooth transition to a more digital environment and the different mechanisms that are in place, that could be improved. Because of the fact that as the generations pass, more and more of the students are getting tech savvy, so, if there is any time to implement digital transformation, it is now. This study will be focused on the Maldives and as it is a small island nation, it comes with a set of challenges that other countries most probably do not have","Journal of Applied Technology and Innovation, APU Press",Digital transformation of the higher education sector in Maldives,,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
370074721,2020-01-01T00:00:00,"The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions. (C) 2020 The Author(s). Published by Elsevier Ltd.Funding Agencies|National Natural Science Foundation of ChinaNational Natural Science Foundation of China (NSFC) [71971030]; Shaanxi Provincial Natural Science Foundation of China [2019JM-495]; Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities [300102220203]</p",'Elsevier BV',An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode,10.1016/j.jclepro.2020.123365,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
482691058,2021-11-11T00:00:00,"Artificial Intelligence of Things (AIoT) is a relatively new concept that involves the merging of Artificial Intelligence (AI) with the Internet of Things (IoT). It has emerged from the realization that Internet of Things networks could be further enhanced if they were also provided with Artificial Intelligence, enhancing the extraction of data and network operation. Prior to AIoT, the Internet of Things would consist of networks of sensors embedded in a physical environment, that collected data and sent them to a remote server. Upon reaching the server, a data analysis would be carried out which normally involved the application of a series of Artificial Intelligence techniques by experts. However, as Internet of Things networks expand in smart cities, this workflow makes optimal operation unfeasible. This is because the data that is captured by IoT is increasing in size continually. Sending such amounts of data to a remote server becomes costly, time-consuming and resource inefficient. Moreover, dependence on a central server means that a server failure, which would be imminent if overloaded with data, would lead to a halt in the operation of the smart service for which the IoT network had been deployed. Thus, decentralizing the operation becomes a crucial element of AIoT. This is done through the Edge Computing paradigm which takes the processing of data to the edge of the network. Artificial Intelligence is found at the edge of the network so that the data may be processed, filtered and analyzed there.  It is even possible to equip the edge of the network with the ability to make decisions through the implementation of AI techniques such as Machine Learning. The speed of decision making at the edge of the network means that many social, environmental, industrial and administrative processes may be optimized, as crucial decisions may be taken faster. 

Deep Intelligence is a tool that employs disruptive Artificial Intelligence techniques for data analysis i.e., classification, clustering, forecasting, optimization, visualization. Its strength lies in its ability to extract data from virtually any source type. This is a very important feature given the heterogeneity of the data being produced in the world today. Another very important characteristic is its intuitiveness and ability to operate almost autonomously.  The user is guided through the process which means that anyone can use it without any knowledge of the technical, technological and mathematical aspects of the processes performed by the platform. This means that the Deepint.net platform integrates functionalities that would normally take years to implement in any sector individually and that would normally require a group of experts in data analysis and related technologies [1-322]. 

The Deep Intelligence platform can be used to easily operate Edge Computing architectures and IoT networks. The joint characteristics of a well-designed Edge Computing platform (that is, one which brings computing resources to the edge of the network) and of the advanced Deepint.net platform deployed in a cloud environment, mean that high speed, real-time response, effective troubleshooting and management, as well as precise forecasting can be achieved. 

Moreover, the low cost of the solution, in combination with the availability of low-cost sensors, devices, Edge Computing hardware, means that deployment becomes a possibility for developing countries, where such solutions are needed most",,AIoT for Achieving Sustainable Development Goals,,https://core.ac.uk/download/482691058.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
287192453,2020-02-10,"A true artificial intelligence (AI) system is something that ""learns"" from the data it stores,  in order to perform tasks and solve problems that typically require human intelligence - either with the help of a human expert or independently. The area of AI is an interdisciplinary field, which has been designated as a strategic area in the European Union (EU) approach and a key driver of economic development that can bring solutions to many social challenges and problems. Due to its nature and its tendency to be digitally advanced and smarter with analytics, the financial sector is one of the early adopters of AI and expects multiple benefits from its application, that is, the ability to provide better service in the shortest time possible and at a lower cost. AI in the financial sector is based on an understanding of the business needs of financial organizations, institutions and markets and the ability to connect with technological capabilities. They are powerful tools that completely transform this sector. The basic idea of this paper is to consider where the real value of AI in the financial sector is, i.e. what are the practical aspects and business implications of AI in the financial sector globally. It is common knowledge that evolving technologies have always had a strong impact on the sectors in which they are applied because they give them the opportunity to improve existing manufacturing processes, services, customer experiences, operate more efficiently, achieve cost savings, etc. The aim of this paper is to identify areas of application of AI in the financial sector, and to explore leading AI applications that are changing the financial ecosystem, transforming the financial sector and that have the potential to significantly improve many of its functions. The paper further highlights other implications of AI implementation in the financial sector such as employment - job creation and termination of existing AI-influenced employment, the scope and potential of application in developing countries, the problem of regulation and use in the best interests of man, and the importance of properly managing specific AI risks","Ekonomski fakultet, Univerzitet u Istočnom Sarajevu",FINANCE AND ARTIFICIAL INTELLIGENCE: THE FIFTH INDUSTRIAL REVOLUTION AND ITS IMPACT ON THE FINANCIAL SECTOR,10.7251/ZREFIS1919067G,https://core.ac.uk/download/pdf/287192453.pdf,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
482634030,2021-09-01T00:00:00,"Access to health data, important for population health planning, basic and clinical research and health industry utilization, remains problematic. Legislation intended to improve access to personal data across national borders has proven to be a double-edged sword, where complexity and implications from misinterpretations have paradoxically resulted in data becoming more siloed. As a result, the potential for development of health specific AI and clinical decision support tools built on real-world data have yet to be fully realized. In this perspective, we propose federated networks as a solution to enable access to diverse data sets and tackle known and emerging health problems. The perspective draws on experience from the World Economic Forum Breaking Barriers to Health Data project, the Personal Health Train and Vantage6 infrastructures, and industry insights. We first define the concept of federated networks in a healthcare context, present the value they can bring to multiple stakeholders, and discuss their establishment, operation and implementation. Challenges of federated networks in healthcare are highlighted, as well as the resulting need for and value of an independent orchestrator for their safe, sustainable and scalable implementation",'Frontiers Media SA',Federated Networks for Distributed Analysis of Health Data,10.3389/fpubh.2021.712569,https://core.ac.uk/download/482634030.pdf,"[{'title': 'Frontiers in Public Health', 'identifiers': ['2296-2565', 'issn:2296-2565']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
430687431,2021-09-01T00:00:00,"Readmission of patients within a specific period after their discharge from a hospital is a cause of concern for the healthcare industry due to the cost involved. Most of the work done for predicting such readmissions using machine learning (ML) have been based on EHR, claims or authorization data from specific sources, which are mostly snapshot data at one static point in time and hence delayed. ADT being dynamic as the data is available instantaneous on occurrence of a medical event/visit adds value. Our goal is to utilize machine learning on unlabeled ADT data to identify patients who are at a high risk of being readmitted. We approached the problem in three parts. First, we labeled patient events using logical rules and finalized one of many readmission definitions that was more encapsulating of varied scenarios. Second, feature engineering was done which encapsulates the longitudinal timeline of each patient in a representative way considering all the contextual information. Third, we developed an automated machine learning pipeline which takes modeling inputs from the user, runs various models to generate readmission prediction, does a cross validation and returns the best model. We tried multiple combinations of models and cross-validation strategies and decided on a random forest model with specific hyper-parameter values and to be the most effective method to classify high risk patients. It had a test AUC-ROC of 72% which is better than quite a few industry standards. The model currently implemented in the client environment identifies the high-risk patients in real-time to care nurses who in turn take proper interventions to reduce their chances of readmission",'Elsevier BV',"Using hospital Admission, Discharge &amp; Transfer (ADT) data for predicting readmissions",10.1016/j.mlwa.2021.100055,,"[{'title': None, 'identifiers': ['issn:2666-8270', '2666-8270']}]",core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
387851240,2020-01-01T00:00:00,"The product-service system (PSS) business model has received increasing attention in equipment maintenance studies, as it has the potential to provide high value-added services for equipment users and construct ethical principles for equipment providers to support the implementation of circular economy. However, the PSS providers in equipment industry are facing many challenges when implementing Industry 4.0 technologies. One important challenge is how to fully collect and analyse the operational data of different equipment and diverse users in widely varied conditions to make the PSS providers create innovative equipment management services for their customers. To address this challenge, an active preventive maintenance approach for complex equipment is proposed. Firstly, a novel PSS operation mode was developed, where complex equipment is offered as a part of PSS and under exclusive control by the providers. Then, a solution of equipment preventive maintenance based on the operation mode was designed. A deep neural network was trained to predict the remaining effective life of the key components and thereby, it can pre-emptively assess the health status of equipment. Finally, a real-world industrial case of a leading CNC machine provider was developed to illustrate the feasibility and effectiveness of the proposed approach. Higher accuracy for predicting the remaining effective life was achieved, which resulted in predictive identification of the fault features, proactive implementation of the preventive maintenance, and reduction of the PSS providers maintenance costs and resource consumption. Consequently, the result shows that it can help PSS providers move towards more ethical and sustainable directions. (C) 2020 The Author(s). Published by Elsevier Ltd.Funding Agencies|National Natural Science Foundation of ChinaNational Natural Science Foundation of China (NSFC) [71971030]; Shaanxi Provincial Natural Science Foundation of China [2019JM-495]; Fundamental Research Funds for the Central UniversitiesFundamental Research Funds for the Central Universities [300102220203]</p",'Elsevier BV',An active preventive maintenance approach of complex equipment based on a novel product-service system operation mode,10.1016/j.jclepro.2020.123365,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
301241016,2016-06-01T00:00:00,"Purpose: The goal of this paper is to develop a pragmatic system of a production throughput

forecasting system for an automated test operation in a hard drive manufacturing plant. The

accurate forecasting result is necessary for the management team to response to any changes in

the production processes and the resources allocations.

Design/methodology/approach: In this study, we design a production throughput forecasting

system in an automated test operation in hard drive manufacturing plant. The proposed system

consists of three main stages. In the first stage, a mutual information method was adopted for

selecting the relevant inputs into the forecasting model. In the second stage, a generalized

regression neural network (GRNN) was implemented in the forecasting model development

phase. Finally, forecasting accuracy was improved by searching the optimal smoothing parameter

which selected from comparisons result among three optimization algorithms: particle swarm

optimization (PSO), unrestricted search optimization (USO) and interval halving optimization

(IHO).

Findings: The experimental result shows that (1) the developed production throughput

forecasting system using GRNN is able to provide forecasted results close to actual values, and to

projected the future trends of production throughput in an automated hard disk drive test  operation; (2) IHO algorithm performed as appropriate optimization method better than the

other two algorithms. (3) Compared with current forecasting system in manufacturing, the results

show that the proposed system’s performance is superior to the current system in prediction

accuracy and suitable for real-world application.

Originality/value: The production throughput volume is a key performance index of hard disk

drive manufacturing systems that need to be forecast. The production throughput forecasting

result is useful information for management team to respond to any changes in production

processes and resources allocation. However, a practical forecasting system for production

throughput has not been described in detail yet. The experiments were conducted on a real data

set from the final testing operation of hard disk drive manufacturing factory by using Visual Basic

Application on Microsoft Excel© to develop preliminary forecasting system for testing and

verification process. The experimental result shows that the proposed model is superior to the

performance of the current forecasting system.Peer Reviewe","[{'title': 'Journal of Industrial Engineering and Management', 'identifiers': ['issn:2013-0953', '2013-0953']}]",'Omnia Publisher SL',A production throughput forecasting system in an automated hard disk drive test operation using GRNN,10.3926/jiem.1464,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
474975486,2016-01-01T00:00:00,"With renewable energy becoming more common, energy prices fluctuate more depending on environmental factors such as the weather. Consuming energy without taking volatile prices into consideration can not only become expensive, but may also increase the peak load, which requires energy providers to generate additional energy using less environment-friendly methods. In the Netherlands, pumping stations that maintain the water levels of polder canals are large energy consumers, but the controller software currently used in the industry does not take real-time energy availability into account. We investigate if existing AI planning techniques have the potential to improve upon the current solutions. In particular, we propose a light weight but realistic simulator and investigate if an online planning method (UCT) can utilise this simulator to improve the cost-efficiency of pumping station control policies. An empirical comparison with the current control algorithms indicates that substantial cost, and thus peak load, reduction can be attained",,AAAI Press,Energy- and Cost-Efficient Pumping Station Control,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
147103496,2017-01-01T00:00:00,"Energy Storage Systems will play crucial role in controlling the grid of the future when increased penetration of renewable energy sources will take place. Especially batteries are expected to occupy a considerable share of the total energy storage market by simultaneously providing services to different stakeholders such as energy producers, transmission/distribution operators, residential, commercial and industrial consumers. Nowadays, Peak shaving and Time-of-Use applications are the most common services that standalone battery storage systems can provide to industrial consumers (without integrated PV systems and/or wind turbines). A big part of the existing literature addressing such applications aims at developing an offline algorithm for optimal battery deployment based on a known load profile (or accurately predicted) without taking into consideration real time conditions. This paper investigates the impact of industrial load forecasting errors on dispatching strategies of battery storage systems on economically driven peak shaving and Time-of-Use applications. An artificial neural network has been developed and used as a prediction model of an industrial load profile. The neural network was trained, validated and tested on historical load data with time resolution of 15 minutes, provided by the local distribution operator of the Belgian electric grid. The performance of the neural network in terms of output-target regression and mean absolute error is 0.833 and 10.02% respectively. Afterwards, a simulation was carried out comparing four different scenarios of peak shaving. The results show that the prediction accuracy of the presented neural network is not competitive enough. Peak shaving based on
predicted profiles becomes reliable for lower forecasting errors. For this purpose, further access into the process and types of loads of the user is required in order to come up with a more sophisticated prediction model",,'Institute of Electrical and Electronics Engineers (IEEE)',Assessing the impact of load forecasting accuracy on battery dispatching strategies with respect to Peak Shaving and Time-of-Use (TOU) applications for industrial consumers,10.1109/upec.2017.8231939,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
55621758,2011-01-01T00:00:00,"The impact of the initial dissolved oxygen, fermentation temperature, wort concentration and yeast pitching rate on the major fermentation process responses were evaluated by full factorial design and statistical analysis by JMP 5.01 (SAS software) software. Fermentation trials were carried out in 2L-EBC tall tubes using an industrial lager brewing yeast strain. The yeast viability, ethanol production, apparent extract and real degree of fermentation were monitored. The results obtained demonstrate that very high gravity worts at 22°P can be fermented in the same period of time as a 15°P wort, by raising the temperature to 18°C, the oxygen level to about 22 ppm, and increasing the pitching rate to 22 × 106 cell/mL. When diluting to obtain an 11.5°P beer extract, the volumetric brewing capacity increased 91% for the 22°P wort fermentation and 30% using the 15°P wort. After dilution, the fermentation of the 22°P wort resulted in a beer with higher esters levels, primarily the compound ethyl acetate.(undefined","[{'title': 'Journal of the Institute of Brewing', 'identifiers': ['2050-0416', 'issn:0046-9750', 'issn:2050-0416', '0046-9750']}]",'Wiley',Comparing the impact of environmental factors during very high gravity brewing fermentations,10.1002/j.2050-0416.2011.tb00480.x,https://core.ac.uk/download/55621758.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
217593290,2018-01-01T00:00:00,"We present the first autonomous endoscope for the visual inspection of very small ducts and cavities, up to a 6-mm diameter. The system has been designed, implemented, and tested in a challenging industrial scenario and in strict collaboration with an avionic industry partner. The inspected objects are metallic gearboxes eventually presenting different residuals (e.g., sand, machining swarfs, and metallic dust) inside the oil ducts. The automatic system is actuated by a robotic arm that moves the endoscope with a microcamera inside the gearbox duct, while a deep-learning-based spatio-temporal image analysis module detects, classifies, and localizes defects in real time. Feedback is given to the robotic arm in order to move or extract the endoscope given the detected anomalies. Evaluation provides a detection rate of nearly 98 % given different tests with different types of residuals and duct structures. \ua9 2005-2012 IEEE",,'Institute of Electrical and Electronics Engineers (IEEE)',Deep Endoscope: Intelligent Duct Inspection for the Avionic Industry,10.1109/TII.2018.2807797,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
11478250,2003,"Mechatronic design is the integrated design of a mechanical system and its embedded control system. In order to make proper choices early in the design stage, tools are required that support modelling and simulation of physical systems––together with the controllers––with parameters that are directly related to the real-world system. Such software tools are becoming available now. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a ‘process’ that can be controlled by block-diagram-based (digital) controllers. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission, a mobile robot, and an industrial linear motor with a neural-network-based learning feed-forward controller that compensates for cogging",,Pergamon,Mechatronic design,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
160081036,2017-09-18T00:00:00,"Conference of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2017 ; Conference Date: 18 September 2017 Through 22 September 2017; Conference Code:209269International audienceWe present WHODID: a turnkey intuitive web-based interface for fault detection, identification and diagnosis in production units. Fault detection and identification is an extremely useful feature and is becoming a necessity in modern production units. Moreover, the large deployment of sensors within the stations of a production line has enabled the close monitoring of products being manufactured. In this context, there is a high demand for computer intelligence able to detect and isolate faults inside production lines, and to additionally provide a diagnosis for maintenance on the identified faulty production device, with the purpose of preventing subsequent faults caused by the diagnosed faulty device behavior. We thus introduce a system which has fault detection, isolation, and identification features, for retrospective and on-the-fly monitoring and maintenance of complex dynamical production processes. It provides real-time answers to the questions: "" is there a fault? "" , "" where did it happen? "" , "" for what reason? "". The method is based on a posteriori analysis of decision sequences in XGBoost tree models, using recurrent neural networks sequential models of tree paths. The particularity of the presented system is that it is robust to missing or faulty sensor measurements, it does not require any modeling of the underlying, possibly exogenous manufacturing process, and provides fault diagnosis along with confidence level in plain English formulations. The latter can be used as maintenance directions by a human operator in charge of production monitoring and control",,'Springer Science and Business Media LLC',"WHODID: Web-based interface for Human-assisted factory Operations in fault Detection, Identification and Diagnosis",10.1007/978-3-319-71273-4_47,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
347692253,2016-11-14T00:00:00,"Nowadays, monitoring of people and events is a common matter in the street, in the industry or at home, and acoustic event detection is commonly used. This increases the knowledge of what is happening in the soundscape, and this information encourages any monitoring system to take decisions depending on the measured events. Our research in this field includes, on one hand, smart city applications, which aim is to develop a low cost sensor network for real time noise mapping in the cities, and on the other hand, ambient assisted living applications through audio event recognition at home. This requires acoustic signal processing for event recognition, which is a challenging problem applying feature extraction techniques and machine learning methods. Furthermore, when the techniques come closer to implementation, a complete study of the most suitable platform is needed, taking into account computational complexity of the algorithms and commercial platforms price. In this work, the comparative study of several platforms serving to implement this sensing application is detailed. An FPGA platform is chosen as the optimum proposal considering the application requirements and taking into account time restrictions of the signal processing algorithms. Furthermore, we describe the first approach to the real-time implementation of the feature extraction algorithm on the chosen platform",,'MDPI AG',An FPGA Platform Proposal for Real-Time Acoustic Event Detection: Optimum Platform Implementation for Audio Recognition with Time Restrictions,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
82663801,2010-04-30,"Background/PurposeThe postweaning multisystemic wasting syndrome, caused by the porcine circovirus type 2 (PCV-2), is a major disease that poses a significant threat to the global swine industry. The purpose of this study was to establish a real-time polymerase chain reaction (PCR) method for the quantification of PCV-2 and to enable the rapid differentiation of porcine circoviruses type 1 and 2 (PCV-1 and PCV-2). Such a method would significantly speed up the process of clinical diagnosis, and could also be used to study the pathogenic mechanisms of diseases associated with PCV-2.MethodsMultiplex real-time PCR, together with LightCycler PCR data analysis software, was used for the quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2. A 263-bp DNA fragment was amplified from the 3′ end of the open reading frame-2 of PCV-2 by nested PCR, and its DNA sequence was verified as having 100% identity with a PCV-2 standard (NCBI accession number: AF055394). The 263-bp DNA fragment was cloned into the pGEM-T easy vector, and the recombinant plasmid was serially diluted and quantified using real-time PCR. A standard curve was then constructed for quantification of the PCV-2 levels in field samples. The differentiation of PCV-1 and PCV-2 was carried out by analyzing the melting temperatures of the genotype-specific PCR products.ResultsTo quantify the PCV-2 levels in field samples, a standard curve (1 × 102 −1 × 109 copies/μL) was constructed. PCV-2 concentrations as low as 1 × 102 copies/mL could be detected in specimens taken from the lymph nodes or infected tissues in samples of PCV-2-infected pigs. The diagnosis of PCV-1 and PCV-2 infections and the quantification of the viral load in the field samples could be completed within 45 minutes after extracting the viral DNA using a commercial extraction kit.ConclusionThis study demonstrate that real-time PCR is a clinically feasible method for the accurate quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2",,Taiwan Society of Microbiology. Published by Elsevier Taiwan LLC.,Fast Diagnosis and Quantification for Porcine Circovirus Type 2 (PCV-2) Using Real-Time Polymerase Chain Reaction ,10.1016/S1684-1182(10)60014-X,https://core.ac.uk/download/pdf/82663801.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226738815,2018-01-01T00:00:00,"It is with great pleasure that we present this Special Issue of
the Journal of Signal Processing Systems (JSPS) dedicated to
Embedded Computer Vision! We are pleased to include six
state-of-the-art papers from the leaders in this field, both from
industry and academia, who keep pushing the embedded computer vision technology forward.
While the idea for this special issue originated between the Guest Editors at one of the CVPR workshops
on the same topic that we have organized, it is the work
of the contributing authors that makes it a success. The
papers were solicited from the workshop participants
and through an open call for papers, so the initial submissions were in many ways already pre-filtered. Out of
24 submitted papers, the highly selective review process
yielded the six papers included here. They cover a
broad range of challenges that are encountered in practical deployment of embedded vision systems, especially
when high computational performance needs meet limited resources. We present papers describing a range of
novel solutions: a deep learning accelerator, a robust
aerial tracking system, an FPGA-based aerial visual
servoing task solution, an approach to use low-cost
hardware for real-time vision, a real-time motion detector, and an image enhancement approach based on human vision",,'Springer Science and Business Media LLC',Guest Editorial: Special Issue on Embedded Computer Vision,10.1007/s11265-018-1365-8,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
163090247,2011-01-01T00:00:00,An automated pilot plant has been designed and commissioned to carry out online/real-time data acquisition and control for the Cr 6+-Fe 2+ reduction process. Simulated data from the Cr 6+-Fe 2+ model derived are validated with online data and laboratory analysis using ICP-AES analysis method. The distinctive trend or patterns exhibited in the ORP profiles for the non-equilibrium model derived have been utilized to train neural network-based controllers for the process. The implementation of this process control is to ensure sufficient Fe 2+ solution is dosed into the wastewater sample in order to reduce all Cr 6+-Cr 3+. The neural network controller has been utilized to compare the capability of set-point tracking with a PID controller in this process. For this process neural network-based controller dosed in less Fe 2+ solution compared to the PID controller which hence reduces wastage of chemicals. Industrial Cr 6+ wastewater samples obtained from an electro-plating factory has also been tested on the pilot plant using the neural network-based controller to determine its effectiveness to control the reduction process for a real plant. The results indicate the proposed controller is capable of fully reducing the Cr 6+-Cr 3+ in the batch treatment process with minimal dosage of Fe 2+,,'Elsevier BV',Neural network based controller for Cr6+–Fe2+ batch reduction process,10.1016/j.neucom.2011.06.027,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
395008361,2018-01-01T00:00:00,"Human-robot collaboration could be advanced by facilitating the intuitive, gaze-based control of robots, and enabling robots to recognize human actions, infer human intent, and plan actions that support human goals. Traditionally, gaze tracking approaches to action recognition have relied upon computer vision-based analyses of two-dimensional egocentric camera videos. The objective of this study was to identify useful features that can be extracted from three-dimensional (3D) gaze behavior and used as inputs to machine learning algorithms for human action recognition. We investigated human gaze behavior and gaze-object interactions in 3D during the performance of a bimanual, instrumental activity of daily living: the preparation of a powdered drink. A marker-based motion capture system and binocular eye tracker were used to reconstruct 3D gaze vectors and their intersection with 3D point clouds of objects being manipulated. Statistical analyses of gaze fixation duration and saccade size suggested that some actions (pouring and stirring) may require more visual attention than other actions (reach, pick up, set down, and move). 3D gaze saliency maps, generated with high spatial resolution for six subtasks, appeared to encode action-relevant information. The ""gaze object sequence"" was used to capture information about the identity of objects in concert with the temporal sequence in which the objects were visually regarded. Dynamic time warping barycentric averaging was used to create a population-based set of characteristic gaze object sequences that accounted for intra- and inter-subject variability. The gaze object sequence was used to demonstrate the feasibility of a simple action recognition algorithm that utilized a dynamic time warping Euclidean distance metric. Averaged over the six subtasks, the action recognition algorithm yielded an accuracy of 96.4%, precision of 89.5%, and recall of 89.2%. This level of performance suggests that the gaze object sequence is a promising feature for action recognition whose impact could be enhanced through the use of sophisticated machine learning classifiers and algorithmic improvements for real-time implementation. Robots capable of robust, real-time recognition of human actions during manipulation tasks could be used to improve quality of life in the home and quality of work in industrial environments",,"eScholarship, University of California",Exploiting Three-Dimensional Gaze Tracking for Action Recognition During Bimanual Manipulation to Enhance Human-Robot Collaboration.,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
132618351,2017-09-01T00:00:00,"The advent of big data has created opportunities for firms to customize their products and services to unprecedented levels of granularity. Using big data to personalize an offering in real time, however, remains a major challenge. In the mobile advertising industry, once a customer enters the network, an ad-serving decision must be made in a matter of milliseconds. In this work, we describe the design and implementation of an ad-serving algorithm that incorporates machine-learning methods to make personalized ad-serving decisions within milliseconds. We developed this algorithm for Vungle Inc., one of the largest global mobile ad networks. Our approach also addresses other important issues that most ad networks face, such as user fatigue, budget restrictions, and campaign pacing. In an A/B test versus the company’s legacy algorithm, our algorithm generated a 23 percent increase in revenue per 1,000 impressions. Across the company’s network, this increase represents a $1 million increase in monthly revenue",,'Institute for Operations Research and the Management Sciences (INFORMS)',Vungle Inc. Improves Monetization Using Big Data Analytics,10.1287/inte.2017.0903,https://core.ac.uk/download/132618351.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
323317277,2011-10-05T00:00:00,"[EN] This paper shows the development and validation of an energy  prediction model for Meliá Havana hotel located at Havana (Cuba). The model is based on the Radiant Time Series for obtaining the thermal load in room blocks of the building. The model has been implemented in Matlab®. Experimental validation of the model is performed based on real measurements for the daily energy consumption of the hotel. The model is valuable for studying the energy behaviour and for implementing advanced control strategies.[ES] Este artículo describe la obtención y validación de un modelo de predicción energética para el hotel Meliá Habana de la ciudad Habana en Cuba. El modelo obtenido emplea el método de series de tiempo radiantes para la determinación de la carga térmica de los bloques habitacionales de la instalación. El modelo es implementado en el lenguaje de programación MatLab®. La validación experimental del modelo se realiza con mediciones reales del consumo energético diario del hotel. El valor de uso del modelo obtenido es apreciable para estudios de comportamiento energético y para la implementación de estrategias avanzadas de control.Acosta, AV.; González, AI.; Zamarreño, JM.; Álvarez, V. (2011). Modelo para la Predicción Energética de una Instalación Hotelera. Revista Iberoamericana de Automática e Informática industrial. 8(4):309-322. https://doi.org/10.1016/j.riai.2011.09.001OJS3093228",,'Elsevier BV',A Hotel Building Model for Energy Prediction,10.1016/j.riai.2011.09.001,http://hdl.handle.net/10251/144437,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
323317331,2011-01-04T00:00:00,"[EN] This paper proposes a Computational Decision Support System, the Fire Emergency Manager (GCF), designed for assisting a person to make decisions in real time during emergency situations, specifically during a fire in a building. The dynamics of the GCF is based on the estimation of the final state related to each alternative via a net of concepts and some evolution functions that define how the initial state will evolve given a certain alternative. The GCF scores the alternatives computing their expected utility and so, it requires a probability value associated to each possible final state. The estimated values of the criteria that characterize a state are bounded using fuzzy sets, which provide an easy method for assigning probability values. The GCF considers that it could be necessary to carry out simultaneously more than one alternative to mitigate a fire emergency.[ES] Este trabajo propone un Sistema Computacional de Ayuda a la Decisión, el Gestor de Crisis de Fuego (GCF), diseñado para ayudar a una persona a tomar decisiones en tiempo real en situaciones de emergencia, concretamente, en caso de incendio en un edificio. El funcionamiento del GCF se basa principalmente en la estimación del estado final asociado a cada alternativa a través de una red de conceptos y unas funciones de evolución que serán aplicadas al estado inicial. El GCF tiene en cuenta que para mitigar la crisis producida por un incendio pueden ser necesarias varias alternativas ejecutándose simultáneamente.Este  trabajo  se  ha  desarrollado  dentro  del  marco  del  proyecto HESPERIA (CDTI-Programa CENIT-2005).Iglesias, Á.; Del Castillo, MD.; Serrano, JI.; Oliva, J. (2011). Sistema de Ayuda a la Decisión Aplicado a Situaciones de Emergencia en Tiempo Real. Revista Iberoamericana de Automática e Informática industrial. 8(1):80-88. https://doi.org/10.1016/S1697-7912(11)70010-3OJS808881Aamodt, A., & Plaza, E. (1994). Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches. AI Communications, 7(1), 39-59. doi:10.3233/aic-1994-7104Aleskerov, F., Say, A. I., Toker, A., Akin, H. L., & Altay, G. (2005). A cluster-based decision support system for estimating earthquake damage and casualties. Disasters, 29(3), 255-276. doi:10.1111/j.0361-3666.2005.00290.xBonazountas, M., Kallidromitou, D., Kassomenos, P., & Passas, N. (2007). A decision support system for managing forest fire casualties. Journal of Environmental Management, 84(4), 412-418. doi:10.1016/j.jenvman.2006.06.016Church, R. L. (2002). Geographical information systems and location science. Computers & Operations Research, 29(6), 541-562. doi:10.1016/s0305-0548(99)00104-5Hamalainen, R. P., Lindstedt, M. R. K., & Sinkko, K. (2000). Multiattribute Risk Analysis in Nuclear Emergency Management. Risk Analysis, 20(4), 455-468. doi:10.1111/0272-4332.204044Iliadis, L. S. (2005). A decision support system applying an integrated fuzzy model for long-term forest fire risk estimation. Environmental Modelling & Software, 20(5), 613-621. doi:10.1016/j.envsoft.2004.03.006Mowrer, F. W. (2009). Driving Forces for Smoke Movement and Management. Fire Technology, 45(2), 147-162. doi:10.1007/s10694-008-0077-1Overton, I. C. (2005). Modelling floodplain inundation on a regulated river: integrating GIS, remote sensing and hydrological models. River Research and Applications, 21(9), 991-1001. doi:10.1002/rra.867Ray, S. K., & Singh, R. P. (2007). Recent Developments and Practices to Control Fire in Undergound Coal Mines. Fire Technology, 43(4), 285-300. doi:10.1007/s10694-007-0024-6Stylios, C. D., Georgopoulos, V. C., Malandraki, G. A., & Chouliara, S. (2008). Fuzzy cognitive map architectures for medical decision support systems. Applied Soft Computing, 8(3), 1243-1251. doi:10.1016/j.asoc.2007.02.022Zadeh, L. A. (1975). The concept of a linguistic variable and its application to approximate reasoning—I. Information Sciences, 8(3), 199-249. doi:10.1016/0020-0255(75)90036-5Zadeh, L. . (1978). Fuzzy sets as a basis for a theory of possibility. Fuzzy Sets and Systems, 1(1), 3-28. doi:10.1016/0165-0114(78)90029-5Zhang, J., Delichatsios, M., & Colobert, M. (2008). Assessment of Fire Dynamics Simulator for Heat Flux and Flame Heights Predictions from Fires in SBI Tests. Fire Technology, 46(2), 291-306. doi:10.1007/s10694-008-0072-",,'Elsevier BV',A Decision Support System Applied to Emergency Situations in Real Time,10.1016/S1697-7912(11)70010-3,http://hdl.handle.net/10251/144669,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
55606136,2004-01-01T00:00:00,"A novel meso reactor based on oscillatory flow technology (Harvey et al., 2001) has been
recently presented in Harvey et al. (2003) as a new technology for reaction engineering and
particle suspension applications. Due to the demonstrated enhanced performances for fluid micro
mixing and suspension of catalyst beads and to the small volume of the reactor, this novel
miniature reactor is suitable for applications at specialist chemical manufacture and high
throughput screening. Furthermore, a high control of environment conditions (e.g. mixing
intensity, temperature) coupled with an online monitoring turns this reactor suitable for smallscale
applications to the bioengineering field, such as for fast parallel bioprocessing tasks.
This work concerns with the fluid dynamics characterisation of a novel miniature reactor.
Experimental results using state-of-art fibre-optic technology is used in order to demonstrate that
an accurate control of the residence time distribution (RTD) of liquid and solid phases can be
achieved within this reactor as well as enhanced (oxygen) mass transfer rates. Furthermore,
numerical simulations using Fluent ® software will be presented where simulated RTDs agrees
with the experimental results.
The meso reactor unit consists of 4.4 mm internal diameter and 35 cm long jacketed glass tubes,
with a unit volume of 4.5 ml and provided with smooth periodic constrictions (SPCs), with an
average baffle spacing of 13 mm. The internal diameter at the constricted zone (baffle internal
diameter) is 1.6 mm, leading to a reduction of the baffle free are of 87 %. This unit is able to
support batch or continuous operations mode, simply by configuring the tubes in parallel or in
series, according to the intended application. Mixing is achieved by oscillating the fluid at the
bottom or the top of the reactor by means of a piston pump, using oscillation amplitudes and
frequencies ranging from 0 to 4 mm centre-to-peak and 0 to 25 Hz, respectively.
Experimental studies using the Particle Image Velocimetry (PIV) technique (Harvey et al., 2003)
showed that different fluid mechanics are originated at different oscillation conditions
(oscillation amplitudes and frequencies). A plug flow or a stirred tank behaviour can be obtained
just by controlling the oscillation conditions. At low oscillatory Reynolds numbers (Reo), e.g. 10
to 100, the formation of axisymmetric eddies detached from the constrictions is coupled with low
axial velocities and makes it possible to continuously operate the reactor in a plug flow mode.
Increasing the Reo to values higher than 100, the eddy symmetry is broken and a complete
mixing state is achieved inside the meso reactor. Low oscillation amplitudes must be used if
axial dispersion is intended to be minimized, namely at plug flow setup.
Through an overall oscillation cycle, changes of the location of the main flow stream from near
the wall to the centre of each cavity and vice-versa was observed and is expected to lead to high
mass and heat transfer rates (Perry, 2002). Due to the observed high radial velocities, narrow
residence times distributions are expected to be obtained (Perry, 2002). Also high axial
circulation rates were also observed at high Reos (above 100) and it was proved to lead to an
enhanced performance on catalyst beads suspension. The relation of this fluid mechanics with
the real performance of this novel meso reactor will be demonstrated.
Tracer injection technique is applied to perform RTD studies inside a single SPC tube of the
meso reactor. Spectroscopy UV/VIS technique is used to measure the concentration of a
coloured tracer at the inlet and outlet (at continuous mode) or at the bottom and the top of the
tube (at batch mode). A fibre optic apparatus is employed in order to obtain highly accurate
online measurements of the UV/VIS absorbance. Mixing times are calculated for experiments at
batch mode. Different flow rates are used to determine the effect of the flow rate over the RTD at
continuous operation and axial dispersion is presented by the Bodenstein number, Bo.
Determination of KL.a values is achieved by online measurement of the oxygen concentration
using a special fibre optic probe. The working tip of the probe was dip-coated with a ruthenium
complex immobilised in a sol-gel matrix. This complex is excited to fluorescence by a blue led
(470 nm outpuk peak) and the level of the fluorescence is inversely related to the concentration
of the oxygen through the Stern-Volmer equation (Wang et al., 1999), which is measured by the
fibre-optic apparatus. Retention of solid phases (e.g. catalyst beads and yeast cells) inside the
meso reactor will also be tested.
Further studies using the Computation Fluid Dynamics (CFD) technique will be presented where
accurate prediction of the distribution of residence times is achieved. The use of the distributionfunctions
permits to classify the flow behaviour inside this novel meso reactor patterns and to
calculate mixing efficiencies and axial dispersion coefficients (expressed by the Bo number) at
different oscillation conditions.
A simple 2-D axisymmetric laminar model showed good agreement with flow patterns
visualisations using PIV for Reo below 100 but a 3-D model with a very fine mesh was required
to simulate breakage of axisymmetry. Consequently, 3-D models based on laminar and Large
Eddy Simulations (LES) will be used to maximize the matching of RTD at higher oscillation
conditions. Main intended application of CFDs to this novel meso reactor is the design of a meso
reactor unit, which could operate at the best oscillation conditions and flow rate for cell cultures
and biocatalyst applications",,,Residence times and mixing of a novel continuous oscillatory flow meso reactor,,https://core.ac.uk/download/55606136.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
275598738,2013-01-01T00:00:00,"The bale collecting problem (BCP) appears after harvest operations in grain and other crops. Its solution defines the sequence of collecting bales which lie scattered over the field. Current technology on navigation-aid systems or auto-steering for agricultural vehicles and machines, is able to provide accurate data to make a reliable bale collecting planning. This paper presents a hybrid genetic algorithm (HGA) approach to address the BCP pursuing resource optimization such as minimizing non-productive time, fuel consumption, or distance travelled. The algorithmic route generation provides the basis for a navigation tool dedicated to loaders and bale wagons. The approach is experimentally tested on a set of instances similar to those found in real situations. In particular, comparative results show an average improving of a 16% from those obtained by previous heuristics.This work was supported in part by the Spanish Government (research project AGL2010-15334).Gracia Calandin, CP.; Diezma Iglesias, B.; Barreiro Elorza, P. (2013). A hybrid genetic algorithm for route optimization in the bale collecting problem. Spanish Journal of Agricultural Research. 11(3):603-614. https://doi.org/10.5424/sjar/2013113-3635S603614113Amiama, C., Bueno, J., Álvarez, C. J., & Pereira, J. M. (2008). Design and field test of an automatic data acquisition system in a self-propelled forage harvester. Computers and Electronics in Agriculture, 61(2), 192-200. doi:10.1016/j.compag.2007.11.006Baker, B. M., & Ayechew, M. A. (2003). A genetic algorithm for the vehicle routing problem. Computers & Operations Research, 30(5), 787-800. doi:10.1016/s0305-0548(02)00051-5Baykasolu, A., Oumlzbakr, L., & Tapk, P. (2007). Artificial Bee Colony Algorithm and Its Application to Generalized Assignment Problem. Swarm Intelligence, Focus on Ant and Particle Swarm Optimization. doi:10.5772/5101Bentley, J. J. (1992). Fast Algorithms for Geometric Traveling Salesman Problems. ORSA Journal on Computing, 4(4), 387-411. doi:10.1287/ijoc.4.4.387Bochtis, D. D., & Sørensen, C. G. (2009). The vehicle routing problem in field logistics part I. Biosystems Engineering, 104(4), 447-457. doi:10.1016/j.biosystemseng.2009.09.003Bochtis, D. D., & Sørensen, C. G. (2010). The vehicle routing problem in field logistics: Part II. Biosystems Engineering, 105(2), 180-188. doi:10.1016/j.biosystemseng.2009.10.006Bochtis, D. D., Dogoulis, P., Busato, P., Sørensen, C. G., Berruto, R., & Gemtos, T. (2013). A flow-shop problem formulation of biomass handling operations scheduling. Computers and Electronics in Agriculture, 91, 49-56. doi:10.1016/j.compag.2012.11.015Brady, R. M. (1985). Optimization strategies gleaned from biological evolution. Nature, 317(6040), 804-806. doi:10.1038/317804a0Chen, J.-S., Pan, J. C.-H., & Lin, C.-M. (2008). A hybrid genetic algorithm for the re-entrant flow-shop scheduling problem. Expert Systems with Applications, 34(1), 570-577. doi:10.1016/j.eswa.2006.09.021Cook, S. E., & Bramley, R. G. V. (1998). Precision agriculture — opportunities, benefits and pitfalls of site-specific crop management in Australia. Australian Journal of Experimental Agriculture, 38(7), 753. doi:10.1071/ea97156Cordeau, J.-F., Gendreau, M., Laporte, G., Potvin, J.-Y., & Semet, F. (2002). A guide to vehicle routing heuristics. Journal of the Operational Research Society, 53(5), 512-522. doi:10.1057/palgrave.jors.2601319Dantzig, G., Fulkerson, R., & Johnson, S. (1954). Solution of a Large-Scale Traveling-Salesman Problem. Journal of the Operations Research Society of America, 2(4), 393-410. doi:10.1287/opre.2.4.393Dasgupta, D. (Ed.). (1999). Artificial Immune Systems and Their Applications. doi:10.1007/978-3-642-59901-9Davis L, 1985. Job shop scheduling with genetic algorithms. Proc of the First Int Conf on Genetic Algorithms and their Applications, Pittsburg, PA (USA). July 24-26. pp: 136-140.De Castro LN, Timmis J, 2002. Artificial immune systems: a new computational approach. Springer-Verlag Inc, London, UK.Dorigo, M., Birattari, M., Blum, C., Gambardella, L. M., Mondada, F., & Stützle, T. (Eds.). (2004). Ant Colony Optimization and Swarm Intelligence. Lecture Notes in Computer Science. doi:10.1007/b99492Eksioglu, B., Vural, A. V., & Reisman, A. (2009). The vehicle routing problem: A taxonomic review. Computers & Industrial Engineering, 57(4), 1472-1483. doi:10.1016/j.cie.2009.05.009Garey MR, Johnson DS, 1979. Computers and intractability: a guide to the theory of NP-completeness. WH Freeman & Company, NY.Gillett, B. E., & Miller, L. R. (1974). A Heuristic Algorithm for the Vehicle-Dispatch Problem. Operations Research, 22(2), 340-349. doi:10.1287/opre.22.2.340Goldberg DE, 1989. Genetic algorithms in search, optimization and machine learning. Kluwer Acad Publ, Boston, MA, USA.Gracia, C., Andrés, C., & Gracia, L. (2011). A hybrid approach based on genetic algorithms to solve the problem of cutting structural beams in a metalwork company. Journal of Heuristics, 19(2), 253-273. doi:10.1007/s10732-011-9187-xGrisso RD, Cundiff JS, Vaughan DH, 2007. Investigating machinery management parameters with computers tools, ASABE Conf, Paper 071030.Hameed, I. A., Bochtis, D. D., Sørensen, C. G., & Vougioukas, S. (2012). An object-oriented model for simulating agricultural in-field machinery activities. Computers and Electronics in Agriculture, 81, 24-32. doi:10.1016/j.compag.2011.11.003Holland JH, 1975. Adaptation in natural and artificial systems (Holland JH, ed.). Ann Arbor MI Univ of Michigan Press, MI, USA.Jünger M, Reinelt G, Rinaldi G, 1995. The traveling salesman problem. In: Network models. Handbooks on Operations Research and Management Science 7 (Ball MO, Magnanti TL, Monma CL, Nemhauser GL, eds.). Elsevier, Amsterdam, pp: 225-330.Kennedy JF, Kennedy J, Eberhart R, Shi Y, 2001. Swarm intelligence. Academic Press Inc, London.Laporte, G., Gendreau, M., Potvin, J.-Y., & Semet, F. (2000). Classical and modern heuristics for the vehicle routing problem. International Transactions in Operational Research, 7(4-5), 285-300. doi:10.1111/j.1475-3995.2000.tb00200.xMartin O, Otto SW, Felten EW, 1991. Large-step markov chains for the travelling salesman problem. Complex Syst 5(3): 299-326.Nikkilä, R., Seilonen, I., & Koskinen, K. (2010). Software architecture for farm management information systems in precision agriculture. Computers and Electronics in Agriculture, 70(2), 328-336. doi:10.1016/j.compag.2009.08.013Sørensen, C. G., Pesonen, L., Bochtis, D. D., Vougioukas, S. G., & Suomi, P. (2011). Functional requirements for a future farm management information system. Computers and Electronics in Agriculture, 76(2), 266-276. doi:10.1016/j.compag.2011.02.005Toth, P., & Vigo, D. (2002). 2. Branch-And-Bound Algorithms for the Capacitated VRP. The Vehicle Routing Problem, 29-51. doi:10.1137/1.9780898718515.ch2Wang, C.-H., & Lu, J.-Z. (2008). An effective evolutionary algorithm for the practical capacitated vehicle routing problems. Journal of Intelligent Manufacturing, 21(4), 363-375. doi:10.1007/s10845-008-0185-2Zhang, N., Wang, M., & Wang, N. (2002). Precision agriculture—a worldwide overview. Computers and Electronics in Agriculture, 36(2-3), 113-132. doi:10.1016/s0168-1699(02)00096-",,'Instituto Nacional de Investigacion y Tecnologia Agraria y Alimentaria (INIA)',A hybrid genetic algorithm for route optimization in the bale collecting problem,10.5424/sjar/2013113-3635,https://riunet.upv.es/bitstream/handle/10251/45759/Gracia%3bPILAR%20BARREIRO%20ELORZA%3bBel%c3%a9n%20-%20A%20hybrid%20genetic%20algorithm%20for%20route%20optimization%20in%20the%20bal....pdf?sequence=1&isAllowed=y,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
275634127,2017-01-01T00:00:00,"[EN] In this paper, we describe a new low-cost and portable electronic nose instrument, the Multisensory Odor Olfactory System MOOSY4. This prototype is based on only four metal oxide semiconductor (MOS) gas sensors suitable for IoT technology. The system architecture consists of four stages: data acquisition, data storage, data processing, and user interfacing. The designed eNose was tested with experiment for detection of volatile components in water pollution, as a dimethyl disulphide or dimethyl diselenide or sulphur. Therefore, the results provide evidence that odor information can be recognized with around 86% efficiency, detecting smells unwanted in the water and improving the quality control in bottled water factories.This work was supported by the I+D+i Program of the Generalitat Valenciana, Spain [AICO/2016/046], and the II Program UPV-La Fe [2013/0504].Climent-Martí, E.; Pelegrí Sebastiá, J.; Sogorb Devesa, T.; Talens-Felis, J.; Chilo, J. (2017). Development of the MOOSY4 eNose IoT for Sulphur-Based VOC Water Pollution Detection. Sensors. 17(8):1-10. https://doi.org/10.3390/s17081917S110178Babovic, Z. B., Protic, J., & Milutinovic, V. (2016). Web Performance Evaluation for Internet of Things Applications. IEEE Access, 4, 6974-6992. doi:10.1109/access.2016.2615181Getting Startedhttps://docs.smartcitizen.me/#/start/detailed-specificationsXu, L. D., He, W., & Li, S. (2014). Internet of Things in Industries: A Survey. IEEE Transactions on Industrial Informatics, 10(4), 2233-2243. doi:10.1109/tii.2014.2300753Huang, J., Meng, Y., Gong, X., Liu, Y., & Duan, Q. (2014). A Novel Deployment Scheme for Green Internet of Things. IEEE Internet of Things Journal, 1(2), 196-205. doi:10.1109/jiot.2014.2301819Gardner, J. W., & Bartlett, P. N. (1994). A brief history of electronic noses. Sensors and Actuators B: Chemical, 18(1-3), 210-211. doi:10.1016/0925-4005(94)87085-3Gardner, J. W., & Bartlett, P. N. (1996). Performance definition and standardization of electronic noses. Sensors and Actuators B: Chemical, 33(1-3), 60-67. doi:10.1016/0925-4005(96)01819-9Wilson, A., & Baietto, M. (2009). Applications and Advances in Electronic-Nose Technologies. Sensors, 9(7), 5099-5148. doi:10.3390/s90705099Jia, X.-M., Meng, Q.-H., Jing, Y.-Q., Qi, P.-F., Zeng, M., & Ma, S.-G. (2016). A New Method Combining KECA-LDA With ELM for Classification of Chinese Liquors Using Electronic Nose. IEEE Sensors Journal, 16(22), 8010-8017. doi:10.1109/jsen.2016.2606163Jing, Y.-Q., Meng, Q.-H., Qi, P.-F., Cao, M.-L., Zeng, M., & Ma, S.-G. (2016). A Bioinspired Neural Network for Data Processing in an Electronic Nose. IEEE Transactions on Instrumentation and Measurement, 65(10), 2369-2380. doi:10.1109/tim.2016.2578618Fine, G. F., Cavanagh, L. M., Afonja, A., & Binions, R. (2010). Metal Oxide Semi-Conductor Gas Sensors in Environmental Monitoring. Sensors, 10(6), 5469-5502. doi:10.3390/s100605469Santra, S., Guha, P. K., Ali, S. Z., Hiralal, P., Unalan, H. E., Covington, J. A., … Udrea, F. (2010). ZnO nanowires grown on SOI CMOS substrate for ethanol sensing. Sensors and Actuators B: Chemical, 146(2), 559-565. doi:10.1016/j.snb.2010.01.009Wilson, A. (2013). Diverse Applications of Electronic-Nose Technologies in Agriculture and Forestry. Sensors, 13(2), 2295-2348. doi:10.3390/s130202295Lorwongtragool, P., Sowade, E., Watthanawisuth, N., Baumann, R., & Kerdcharoen, T. (2014). A Novel Wearable Electronic Nose for Healthcare Based on Flexible Printed Chemical Sensor Array. Sensors, 14(10), 19700-19712. doi:10.3390/s141019700Son, M., Cho, D., Lim, J. H., Park, J., Hong, S., Ko, H. J., & Park, T. H. (2015). Real-time monitoring of geosmin and 2-methylisoborneol, representative odor compounds in water pollution using bioelectronic nose with human-like performance. Biosensors and Bioelectronics, 74, 199-206. doi:10.1016/j.bios.2015.06.053Gardner, J. W., Shin, H. W., Hines, E. L., & Dow, C. S. (2000). An electronic nose system for monitoring the quality of potable water. Sensors and Actuators B: Chemical, 69(3), 336-341. doi:10.1016/s0925-4005(00)00482-2Goschnick, J., Koronczi, I., Frietsch, M., & Kiselev, I. (2005). Water pollution recognition with the electronic nose KAMINA. Sensors and Actuators B: Chemical, 106(1), 182-186. doi:10.1016/j.snb.2004.05.055Guadayol, M., Cortina, M., Guadayol, J. M., & Caixach, J. (2016). Determination of dimethyl selenide and dimethyl sulphide compounds causing off-flavours in bottled mineral waters. Water Research, 92, 149-155. doi:10.1016/j.watres.2016.01.016Wilson, A. D. (2012). Review of Electronic-nose Technologies and Algorithms to Detect Hazardous Chemicals in the Environment. Procedia Technology, 1, 453-463. doi:10.1016/j.protcy.2012.02.101Becher, C., Kaul, P., Mitrovics, J., & Warmer, J. (2010). The detection of evaporating hazardous material released from moving sources using a gas sensor network. Sensors and Actuators B: Chemical, 146(2), 513-520. doi:10.1016/j.snb.2009.12.030Berrueta, L. A., Alonso-Salces, R. M., & Héberger, K. (2007). Supervised pattern recognition in food analysis. Journal of Chromatography A, 1158(1-2), 196-214. doi:10.1016/j.chroma.2007.05.024Lajara, R. J., Perez-Solano, J. J., & Pelegri-Sebastia, J. (2015). A Method for Modeling the Battery State of Charge in Wireless Sensor Networks. IEEE Sensors Journal, 15(2), 1186-1197. doi:10.1109/jsen.2014.2361151Batista, B. L., da Silva, L. R. S., Rocha, B. A., Rodrigues, J. L., Berretta-Silva, A. A., Bonates, T. O., … Barbosa, F. (2012). Multi-element determination in Brazilian honey samples by inductively coupled plasma mass spectrometry and estimation of geographic origin with data mining techniques. Food Research International, 49(1), 209-215. doi:10.1016/j.foodres.2012.07.015Benedetti, S., Mannino, S., Sabatini, A. G., & Marcazzan, G. L. (2004). Electronic nose and neural network use for the classification of honey. Apidologie, 35(4), 397-402. doi:10.1051/apido:200402",,'MDPI AG',Development of the MOOSY4 eNose IoT for Sulphur-Based VOC Water Pollution Detection,10.3390/s17081917,https://riunet.upv.es/bitstream/10251/98785/1/sensors-17-01917.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
203923069,2016-12-31,"AbstractTransferring predictive microbial models from research into real world food manufacturing or risk assessment applications is still a challenge for members of the food safety modelling community. Such knowledge transfer could be facilitated if publicly available food safety model repositories would exist.This research therefore aimed at identification of missing resources hampering the establishment of community driven food safety model repositories. Existing solutions in related scientific disciplines like Systems Biology and Data Mining were analyzed.On the basis of this analysis, some factors which would promote the establishment of community driven model repositories were identified – among them: a standardized information exchange format for models and rules for model annotation. As a consequence a proposal for a Predictive Modelling in Food Markup Language (PMF-ML) together with a prototypic implementation on the basis of the Systems Biology Markup Language (SBML) has been developed. In addition the adoption of MIRIAM guidelines for model annotation is proposed. In order to demonstrate the practicability of the proposed strategy, existing predictive models previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models are made publicly available in the first community Food Safety Model Repository called openFSMR (https://sites.google.com/site/openfsmr/).This work illustrates that a standardized information exchange format for predictive microbial models can be established by adoption of resources from Systems Biology. Harmonized description and annotation of predictive models will also contribute to increased transparency and quality of food safety models",,The Author(s). Published by Elsevier Ltd.,Towards Community Driven Food Safety Model Repositories ,10.1016/j.profoo.2016.02.098,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
200869659,2017-01-01T00:00:00,"In sustainable supply chain networks, companies are obligated to have a systematic decision support system in place to help it adopt right decisions at right times. Among strategic decisions, supplier selection and evaluation outranks other decisions in terms of importance due to its long-term impacts. Besides, the adoption of such strategic decision entails exploring several factors that contribute to the complexity of decision making in the supply chain. For the purpose of solving non-linear regression problems, a novel neural network technique known as least square-support vector machine (LS-SVM) with maximum generalization ability has successfully been implemented. However, the performance quality of the LS-SVM is recognized to notoriously vary depending on the rigorous selection of its parameters. Therefore, in this paper, a continuous general variable neighborhood search (CGVNS) which is an effective meta-heuristic algorithm to solve the real world engineering continuous optimization problems is proposed to be integrated with LS-SVM. The CGVNS is hybridized in our novel integrated LS-SVM and CGVNS model, to tune the parameters of the LS-SVM to better estimate performance rating of supplier selection and evaluation problem. To demonstrate the improved performance of our proposed integrated model, a real data set from a case study of a supplier selection and evaluation problem is presented in a cosmetics industry. Additionally, comparative evaluations between our proposed model and the conventional techniques, namely nonlinear regression, multi-layer perceptron (MLP) neural network and LS-SVM is provided. The experimental results simply manifest the outperformance of our proposed model in terms of estimation accuracy and effective prediction","[{'title': 'International Journal of Computational Intelligence Systems', 'identifiers': ['1875-6883', 'issn:1875-6883']}]",'Atlantis Press',A new enhanced support vector model based on general variable neighborhood search algorithm for supplier performance evaluation: A case study,10.2991/ijcis.2017.10.1.20,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
217586045,2018-01-01T00:00:00,"Embedded vision is a disruptive new technology in the vision industry. It is a revolutionary concept with far reaching implications, and it is opening up new applications and shaping the future of entire industries. It is applied in self-driving cars, autonomous vehicles in agriculture, digital dermascopes that help specialists make more accurate diagnoses, among many other unique and cutting-edge applications. The design of such systems gives rise to new challenges for embedded Software developers. Embedded vision applications are characterized by stringent performance constraints to guarantee real-time behaviours and, at the same time, energy constraints to save battery on the mobile platforms. In this paper, we address such challenges by proposing an overall view of the problem and by analysing current solutions. We present our last results on embedded vision design automation over two main aspects: the adoption of the model-based paradigm for the embedded vision rapid prototyping, and the application of heterogeneous programming languages to improve the system performance. The paper presents our recent results on the design of a localization and mapping application combined with image recognition based on deep learning optimized for an NVIDIA Jetson TX2",,'Institute of Electrical and Electronics Engineers (IEEE)',Rapid Prototyping of Embedded Vision Systems: Embedding Computer Vision Applications into Low-Power Heterogeneous Architectures,10.1109/RSP.2018.8631995,https://core.ac.uk/download/217586045.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
60418644,2005-11-01T00:00:00,"In this paper, we present an automatic system and algorithms for the classification of marble slabs into different groups in real time in production line, according to slabs quality. The application of the system is aimed at the marble industry, in order to automate and improve the manual classification process of marble slabs carried out at present. The system consists of a mechatronic prototype, which houses all the required physical components for the acquisition of marble slabs images in suitable light conditions, and computational algorithms, which are used to analyze the color texture of the marble surfaces and classify them into their corresponding group. In order to evaluate the color representation influence on the image analysis, four color spaces have been tested: RGB, XYZ, YIQ, and K-L. After the texture analysis performed with the sum and difference histograms algorithm, a feature extraction process has been implemented with principal component analysis. Finally, a multilayer perceptron neural network trained with the backpropagation algorithm with adaptive learning rate is used to classify the marble slabs in three categories, according to their quality. The results (successful classification rate of 98.9%) show very high performance compared with the traditional (manual) system","[{'title': 'IEEE Transactions on Systems Man and Cybernetics Part C (Applications and Reviews)', 'identifiers': ['1094-6977', 'issn:1094-6977']}]",'Institute of Electrical and Electronics Engineers (IEEE)',Automatic system for quality-based classification of marble textures,10.1109/TSMCC.2004.843236,https://core.ac.uk/download/60418644.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
286600336,2018-01-01T00:00:00,": Deep learning is the most recent approach to achieve artificial intelligence. Especially neural networks are used for solving many human problems - from repetitive operations to intelligent recognizing in image, sound and text processing. They are used in medicine, car industry, game industry and robotics. Business companies also try to find the way of exploitation of the latest technology despite the fact that it is the long way to the point where machines will be capable to replace the human intelligence. Authors of this paper explore possibilities of semi-supervised learning application in accounting. One of the latest deep learning algorithm is successfully used to reconstruct the journal entry key columns. The model was trained and tested on a real-world dataset so it could become base for developing the wide pallet of accounting and audit applications - as anomaly detection module of Enterprise Resource Planning (ERP) software or as a standalone application",,,Journal entries with deep learning model,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
275594362,2013-06-01T00:00:00,"Thanks to the built in intelligence (deployment of new intelligent devices and sensors in places where historically they were not present), the Smart Grid and Microgrid paradigms are able to take advantage from aggregated load forecasting, which opens the door for the implementation of new algorithms to seize this information for optimization and advanced planning. Therefore, accuracy in load forecasts will potentially have a big impact on key operation factors for the future Smart Grid/Microgrid-based energy network like user satisfaction and resource saving, and new methods to achieve an efficient prediction in future energy landscapes (very different from the centralized, big area networks studied so far). This paper proposes different improved models to forecast next day's aggregated load using artificial neural networks, taking into account the variables that are most relevant. In particular, seven models based on the multilayer perceptron will be proposed, progressively adding input variables after analyzing the influence of climate factors on aggregated load. The results section presents the forecast from the proposed models, obtained from real data.Hernández, L.; Baladrón Zorita, C.; Aguiar Pérez, JM.; Calavia Domínguez, L.; Carro Martínez, B.; Sanchez-Esguevillas, A.; Garcia Fernandez, P.... (2013). Experimental Analysis of the Input Variables' Relevance to Forecast Next Day's Aggregated Electric Demand Using Neural Networks. Energies. 6(6):2927-2948. doi:10.3390/en6062927S2927294866Zhang, Q., Lai, K. K., Niu, D., Wang, Q., & Zhang, X. (2012). A Fuzzy Group Forecasting Model Based on Least Squares Support Vector Machine (LS-SVM) for Short-Term Wind Power. Energies, 5(9), 3329-3346. doi:10.3390/en5093329Hsu, C.-C., & Chen, C.-Y. (2003). Regional load forecasting in Taiwan––applications of artificial neural networks. Energy Conversion and Management, 44(12), 1941-1949. doi:10.1016/s0196-8904(02)00225-xCarpaneto, E., & Chicco, G. (2008). Probabilistic characterisation of the aggregated residential load patterns. IET Generation, Transmission & Distribution, 2(3), 373. doi:10.1049/iet-gtd:20070280Shu Fan, Methaprayoon, K., & Wei-Jen Lee. (2009). Multiregion Load Forecasting for System With Large Geographical Area. IEEE Transactions on Industry Applications, 45(4), 1452-1459. doi:10.1109/tia.2009.2023569Pudjianto, D., Ramsay, C., & Strbac, G. (2007). Virtual power plant and system integration of distributed energy resources. IET Renewable Power Generation, 1(1), 10. doi:10.1049/iet-rpg:20060023Ruiz, N., Cobelo, I., & Oyarzabal, J. (2009). A Direct Load Control Model for Virtual Power Plant Management. IEEE Transactions on Power Systems, 24(2), 959-966. doi:10.1109/tpwrs.2009.2016607Hernandez, L., Baladron, C., Aguiar, J. M., Carro, B., Sanchez-Esguevillas, A., Lloret, J., … Cook, D. (2013). A multi-agent system architecture for smart grid management and forecasting of energy demand in virtual power plants. IEEE Communications Magazine, 51(1), 106-113. doi:10.1109/mcom.2013.6400446Mousavi, S. M., & Abyaneh, H. A. (2011). Effect of Load Models on Probabilistic Characterization of Aggregated Load Patterns. IEEE Transactions on Power Systems, 26(2), 811-819. doi:10.1109/tpwrs.2010.2062542Ipakchi, A., & Albuyeh, F. (2009). Grid of the future. IEEE Power and Energy Magazine, 7(2), 52-62. doi:10.1109/mpe.2008.931384Naphade, M., Banavar, G., Harrison, C., Paraszczak, J., & Morris, R. (2011). Smarter Cities and Their Innovation Challenges. Computer, 44(6), 32-39. doi:10.1109/mc.2011.187Hernández, L., Baladrón, C., Aguiar, J. M., Calavia, L., Carro, B., Sánchez-Esguevillas, A., … Gómez, J. (2012). A Study of the Relationship between Weather Variables and Electric Power Demand inside a Smart Grid/Smart World Framework. Sensors, 12(9), 11571-11591. doi:10.3390/s120911571Hernandez, L., Baladrón, C., Aguiar, J., Carro, B., Sanchez-Esguevillas, A., & Lloret, J. (2013). Short-Term Load Forecasting for Microgrids Based on Artificial Neural Networks. Energies, 6(3), 1385-1408. doi:10.3390/en6031385Perez, E., Beltran, H., Aparicio, N., & Rodriguez, P. (2013). Predictive Power Control for PV Plants With Energy Storage. IEEE Transactions on Sustainable Energy, 4(2), 482-490. doi:10.1109/tste.2012.2210255Ogliari, E., Grimaccia, F., Leva, S., & Mussetta, M. (2013). Hybrid Predictive Models for Accurate Forecasting in PV Systems. Energies, 6(4), 1918-1929. doi:10.3390/en6041918Douglas, A. P., Breipohl, A. M., Lee, F. N., & Adapa, R. (1998). The impacts of temperature forecast uncertainty on Bayesian load forecasting. IEEE Transactions on Power Systems, 13(4), 1507-1513. doi:10.1109/59.736298Sadownik, R., & Barbosa, E. P. (1999). Short-term forecasting of industrial electricity consumption in Brazil. Journal of Forecasting, 18(3), 215-224. doi:10.1002/(sici)1099-131x(199905)18:33.0.co;2-bHuang, S. R. (1997). Short-term load forecasting using threshold autoregressive models. IEE Proceedings - Generation, Transmission and Distribution, 144(5), 477. doi:10.1049/ip-gtd:19971144Infield, D. G., & Hill, D. C. (1998). Optimal smoothing for trend removal in short term electricity demand forecasting. IEEE Transactions on Power Systems, 13(3), 1115-1120. doi:10.1109/59.709108Sargunaraj, S., Sen Gupta, D. P., & Devi, S. (1997). Short-term load forecasting for demand side management. IEE Proceedings - Generation, Transmission and Distribution, 144(1), 68. doi:10.1049/ip-gtd:19970599Hong-Tzer Yang, & Chao-Ming Huang. (1998). A new short-term load forecasting approach using self-organizing fuzzy ARMAX models. IEEE Transactions on Power Systems, 13(1), 217-225. doi:10.1109/59.651639Hong-Tzer Yang, Chao-Ming Huang, & Ching-Lien Huang. (1996). Identification of ARMAX model for short term load forecasting: an evolutionary programming approach. IEEE Transactions on Power Systems, 11(1), 403-408. doi:10.1109/59.486125Yu, Z. (1996). A temperature match based optimization method for daily load prediction considering DLC effect. IEEE Transactions on Power Systems, 11(2), 728-733. doi:10.1109/59.496146Charytoniuk, W., Chen, M. S., & Van Olinda, P. (1998). Nonparametric regression based short-term load forecasting. IEEE Transactions on Power Systems, 13(3), 725-730. doi:10.1109/59.708572Taylor, J. W., & Majithia, S. (2000). Using combined forecasts with changing weights for electricity demand profiling. Journal of the Operational Research Society, 51(1), 72-82. doi:10.1057/palgrave.jors.2600856Ramanathan, R., Engle, R., Granger, C. W. J., Vahid-Araghi, F., & Brace, C. (1997). Short-run forecasts of electricity loads and peaks. International Journal of Forecasting, 13(2), 161-174. doi:10.1016/s0169-2070(97)00015-0Elman, J. L. (1990). Finding Structure in Time. Cognitive Science, 14(2), 179-211. doi:10.1207/s15516709cog1402_1Elman, J. L. (1991). Distributed representations, simple recurrent networks, and grammatical structure. Machine Learning, 7(2-3), 195-225. doi:10.1007/bf00114844Kohonen, T. (1990). The self-organizing map. Proceedings of the IEEE, 78(9), 1464-1480. doi:10.1109/5.58325Razavi, S., & Tolson, B. A. (2011). A New Formulation for Feedforward Neural Networks. IEEE Transactions on Neural Networks, 22(10), 1588-1598. doi:10.1109/tnn.2011.216316",,'MDPI AG',Experimental Analysis of the Input Variables' Relevance to Forecast Next Day's Aggregated Electric Demand Using Neural Networks,10.3390/en6062927,https://riunet.upv.es/bitstream/handle/10251/43121/Luis%3bCarlos%3bJavier%20M.%20-%20Experimental%20Analysis%20of%20the%20Input%20Variables%20%20Relevance%20to%20Forecast%20Next%20....pdf?sequence=1&isAllowed=y,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
323091055,2013-07-09T00:00:00,"[ES] En el presente artículo se muestra un esquema de identificación y control que sintoniza en línea las ganancias proporcional, integral y derivativa de un controlador PID discreto aplicado a un sistema dinámico SISO. Esto se logra empleando una red neuronal de base radial con funciones de activación wavelet hijas Morlet (wavenet) adicionalmente en cascada un filtro de respuesta infinita al impulso (IIR). Dicho esquema es aplicado en tiempo real para controlar la velocidad de un motor de inducción de CA trifásico del tipo jaula de ardilla (MIJA) alimentado con un variador de frecuencia trifásico, de esta forma se muestra cómo este esquema de identificación y control en línea, puede ser implementado en este tipo de plantas que son ampliamente utilizadas en la industria, sin la necesidad de obtener los parámetros del modelo matemático del conjunto variador de frecuencia-motor de inducción trifásico. Se presentan los resultados obtenidos en simulación numérica y experimentales, empleando para esto la plataforma de LabVIEW.[EN] This paper presents a control scheme to tune online the proportional, integral and derivative gains of a discrete PID controller, through the identification and control of a SISO stable and minimum phase dynamic system. This is accomplished using a radial basis network neural with daughter Morlet wavelets activation functions in cascaded with an infinite impulse response (IIR) filter. This scheme is applied in real time to control the speed of an AC three-phase induction motor supplied with a three-phase inverter. So in this way we show how the identification and control scheme can be implemented in this type of plants that are widely used in industry, without the need of mathematical model parameters of the induction motor. We present numerical simulation and experimental results.El autor O. Islas Gomez agradece profundamente al CONACyT por la beca otorgada para realizar estudios de posgrado, con numero de registro 266520.Ramos Velasco, L.; Ramos Fernández, J.; Islas Gómez, O.; García Lamont, J.; Espejel Rivera, M.; Márquez Vera, M. (2013). Identificación y Control Wavenet de un Motor de CA. Revista Iberoamericana de Automática e Informática industrial. 10(3):269-278. https://doi.org/10.1016/j.riai.2013.05.002OJS269278103Bocker, J., & Mathapati, S. (2007). State of the Art of Induction Motor Control. 2007 IEEE International Electric Machines & Drives Conference. doi:10.1109/iemdc.2007.383643Domínguez Mayorga, C. R., Espejel Rivera, M. A., Ramos Velasco, L. E., Ramos Fernández, J. C., & Escamilla Hernández, E. (2012). Algoritmos Wavenet con Aplicaciones en la Aproximación de Señales: un Estudio Comparativo. Revista Iberoamericana de Automática e Informática Industrial RIAI, 9(4), 347-358. doi:10.1016/j.riai.2012.09.001Farahani, M. (2013). Intelligent control of SVC using wavelet neural network to enhance transient stability. Engineering Applications of Artificial Intelligence, 26(1), 273-280. doi:10.1016/j.engappai.2012.05.006Haykin, S., 2001. Kalman Filtering and Neural Networks. Wiley, New York. Holtz, J.,;1; 2002. Sensorlees control of induction motor drives. Proceeding of IEEE International Conference in Electric Machines and Drives, IEDMC’07, Wuppertal, Germany, 1359-1394.Jahedi, G., & Ardehali, M. M. (2012). Wavelet based artificial neural network applied for energy efficiency enhancement of decoupled HVAC system. Energy Conversion and Management, 54(1), 47-56. doi:10.1016/j.enconman.2011.10.005Levin, A. U., & Narendra, K. S. (1993). Control of nonlinear dynamical systems using neural networks: controllability and stabilization. IEEE Transactions on Neural Networks, 4(2), 192-206. doi:10.1109/72.207608Levin, A. U., & Narendra, K. S. (1996). Control of nonlinear dynamical systems using neural networks. II. Observability, identification, and control. IEEE Transactions on Neural Networks, 7(1), 30-42. doi:10.1109/72.478390Lin, C.-J. (2009). Nonlinear systems control using self-constructing wavelet networks. Applied Soft Computing, 9(1), 71-79. doi:10.1016/j.asoc.2008.03.014Payakkawan, P., Klomkarn, K., Sooraksa, P., 2009. Dual-line pid controller based on pso for speed control of dc motors. The 9th International Symposium on Communication and Information Technologies (ISCIT), Incheon, Korea.Polo, M. P., Albertos, P., & Galiano, J. Á. B. (2008). Tuning of a PID controlled gyro by using the bifurcation theory. Systems & Control Letters, 57(1), 10-17. doi:10.1016/j.sysconle.2007.06.007Wu, W., & Jhao, D.-W. (2012). Control of a direct internal reforming molten carbonate fuel cell system using wavelet network-based Hammerstein models. Journal of Process Control, 22(3), 653-658. doi:10.1016/j.jprocont.2012.01.01",,'Elsevier BV',Identification and Wavenet Control of AC Motor,10.1016/j.riai.2013.05.002,http://hdl.handle.net/10251/143955,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
162022928,2018-01-01T00:00:00,"Python has evolved to become the most popular language for data science. It sports state-of-the-art libraries for analytics and machine learning, like Sci-Kit Learn. However, Python lacks the computational performance that a industrial system requires for high frequency real time predictions.

Building upon a year long research project heavily based on SciKit Learn (sklearn), we faced performance issues in deploying to production. Replacing sklearn with a better performing framework would require re-evaluating and tuning hyperparameters from scratch. Instead we developed a python embedding in a C++ based server application that increased performance by up to 20x, achieving linear scalability up to a point of convergence. Our implementation was done for mainstream cost effective hardware, which means we observed similar performance gains on small as well as large systems, from a laptop to an Amazon EC2 instance to a high-end server",,,Distributed C++-Python embedding for fast predictions and fast prototyping,,https://core.ac.uk/download/162022928.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
322372256,2019-01-01T00:00:00,"Timely and accurate bearing fault detection and diagnosis is important for reliable and safe operation of industrial systems. In this study, performance of a generic real-time induction bearing fault diagnosis system employing compact adaptive 1D Convolutional Neural Network (CNN) classifier is extensively studied. In the literature, although many studies have developed highly accurate algorithms for detecting bearing faults, their results have generally been limited to relatively small train/test data sets. As opposed to conventional intelligent fault diagnosis systems that usually encapsulate feature extraction, feature selection and classification as distinct blocks, the proposed system takes directly raw time-series sensor data as input and it can efficiently learn optimal features with the proper training. The main advantages of the 1D CNN based approach are 1) its compact architecture configuration (rather than the complex deep architectures) which performs only 1D convolutions making it suitable for real-time fault detection and monitoring, 2) its cost effective and practical real-time hardware implementation, 3) its ability to work without any pre-determined transformation (such as FFT or DWT), hand-crafted feature extraction and feature selection, and 4) its capability to provide efficient training of the classifier with limited size of training data set and limited number of BP iterations. Effectiveness and feasibility of the 1D CNN based fault diagnosis method is validated by applying it to two commonly used benchmark real vibration data sets and comparing the results with the other competing intelligent fault diagnosis methods.Scopu",,'Springer Science and Business Media LLC',A Generic Intelligent Bearing Fault Diagnosis System Using Compact Adaptive 1D CNN Classifier,10.1007/s11265-018-1378-3,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
480277648,2012-10-01T00:00:00,"A compensation scheme to reduce the inherent baseband distortion in uniform PWM modulators (UPWM) is presented in this paper. The method is based on real-time mapping of the switching times of UPWM to those of natural PWM that, as it is well-known, exhibits far less in-band distortion. Two alternatives are presented: one based on an exact, analytic algorithm recently reported in the literature, and another one that uses an artificial neural network (ANN). Both methods are designed for arbitrary, band-limited modulating signals and they are not restricted to single-frequency sinusoids, as other techniques presented in the literature. Simulation results and experimental measurements of a FPGA implementation demonstrate a significant reduction of the distortion in real-time applications. The performance of both alternatives is compared for several modulating signals, and the results of industry-standard distortion tests are also reported. Finally, some guidelines for choosing the best alternative for ASIC or FPGA implementations are providedFil: Chierchie, Fernando. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Bahía Blanca. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages"". Universidad Nacional del Sur. Departamento de Ingeniería Eléctrica y de Computadoras. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages""; ArgentinaFil: Soto, Angel Jose. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Bahía Blanca. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages"". Universidad Nacional del Sur. Departamento de Ingeniería Eléctrica y de Computadoras. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages""; ArgentinaFil: Paolini, Eduardo Emilio. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Bahía Blanca. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages"". Universidad Nacional del Sur. Departamento de Ingeniería Eléctrica y de Computadoras. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages""; ArgentinaFil: Oliva, Alejandro Raul. Consejo Nacional de Investigaciones Científicas y Técnicas. Centro Científico Tecnológico Conicet - Bahía Blanca. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages"". Universidad Nacional del Sur. Departamento de Ingeniería Eléctrica y de Computadoras. Instituto de Investigaciones en Ingeniería Eléctrica ""Alfredo Desages""; Argentin","[{'title': 'International Review of Electrical Engineering (IREE)', 'identifiers': ['1827-6660', 'issn:1827-6660']}]",'Praise Worthy Prize',Real-time low-distortion digital PWM modulator for switching converters,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
148523032,2012-01-01T00:00:00,"A number of new approaches to address the identification issues have been proposed recently, but due to the highly integrated nature of passive radio frequency identification (RFID) tags, it is difficult to evaluate them in real-world scenarios. A recurrent neural network-based hybrid approach with training through genetic algorithm has been proposed to model the performance of the RFID system with received power at the reader in the radio propagation channel as the implementable performance index. Target system is a conveyor system delivering multiple products. A method to deploy RFID technology has been developed and illustrated for smoothening flow on a conveyor. Although various analytical models have been proposed earlier, they fail to accurately predict the performance of RFID system. Proposed method incorporates various factors presented in the industrial environment, while only a few are considered in the analytical model. Such an integrated approach is a genuine extension of a previous model where only neural network model was tested to embrace the system's performance. A comparative study has been carried out to establish the better performance of proposed approach. The model proposed may be helpful to aid in the research area of simulation of RFIDs on computer for reflecting numerous factors in modelling for RFID system performance without sacrificing predictability.Department of Industrial and Systems Engineerin","[{'title': 'International Journal of Computer Integrated Manufacturing', 'identifiers': ['1362-3052', '0951-192x', 'issn:1362-3052', 'issn:0951-192X']}]",'Informa UK Limited',Monitoring the performance of conveyor system using radio frequency identification in manufacturing environment : a recurrent neural network and genetic algorithm-based approach,10.1080/0951192X.2011.646309,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
148792611,2018-04-11T00:00:00,"Evolutionary Computation (EC) has been an active research area for over 60 years, yet its commercial/home uptake has not been as prolific as we might have expected. By way of comparison, technologies such as 3D printing, which was introduced about 35 years ago, has seen much wider uptake, to the extent that it is now available to home users and is routinely used in manufacturing. Other technologies, such as immersive reality and artificial intelligence have also seen commercial uptake and acceptance by the general public. In this paper we provide a brief history of EC, recognizing the significant contributions that have been made by its pioneers. We focus on two methodologies (Genetic Programming and Hyper-heuristics), which have been proposed as being suitable for automated software development, and question why they are not used more widely by those outside of the academic community. We suggest that different research strands need to be brought together into one framework before wider uptake is possible. We hope that this position paper will serve as a catalyst for automated software development that is used on a daily basis by both companies and home users",,'Institute of Electrical and Electronics Engineers (IEEE)',Is Evolutionary Computation evolving fast enough?,10.1109/MCI.2018.2807019,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
199468428,2016-12-01T00:00:00,"4��� ������������ ��������� ������������������������ ������ ������������ ��������� ��������� ������������ ��������������� ��������� ��������� ��������� ��������������� ������������ ��������� ������������ ������������ ��������������� ������. ������ ������������������ ��������������� ������ ������(innovation)���������, ������ ��������������� ��������� ��������������������� ��������� ������ ��������� ������ ��������� ��������� ������������ ������ ��������������������������������� ��������� ��������� ������������ ���������. ��������������� ��������� ������ ������������ ������ &lsquo;������������&rsquo;��� &lsquo;���������������&rsquo;(posthuman)��� ���������, ��������� &lsquo;������&rsquo;(human) ��������� ��������� ������������ ��������� ��������������� ������ ��������� ��������������� ��������� ������������ ������������ ��������� ������ ��������� ������ ������ ��������� ������ ������������ ������ ��������� ������ ��������� ��������� ��������� ������������ ������.
������������������������ ��������������� ������������ ������������, ������������ ��� ������ ��������� ��������� ��������������� ������ ������������. ������ ��������� ��������� ������������ ������������ ��� ������ ������, ������������������ ��������� ��������� ��������������� ��������� ������������. ��������� ��������� ��� ������ ������������ ������������ ������ ������������ ��� ��� ���������, ������ ������������������������ ������ ��������� ��������������� ������������ ��������� ��� ���������, ������������ ������������ ��������� ������������ ������������ ������ ��������� ��������� ������������ ��������� ������������ ������ ������ ������������, ������ ������������ ��������� ��������� ������ ��������������� ������ ��������� ��� ������ ��������� ������������ ������, ������ ��� ��������� ��������������� ������������ ������ ��������� ������������ ������������ ��������� ��������� ��������� ��������� ������������ ������&middot;������������ ��������� ������������ ������.
��������� ������������ ������������������������ ��� ��������������� ��������������� ������ ��������� ��������� ������(device)��� ��������������� ������ ���������, ������������������������ ������ ��������� ��������� ��������� ������������ ������ ��������������� ������������ ���������������, ��������� ������������������ ������������ ������ ������������(connected) ������ ��� ��������� ��������������� ��������������������������������� ������������ ������ ���������, ������������ ��������� ��������� ��������� ������������ ������������ ������ ��������� ��������� ������������ ��������� ��� ��� ������. ��������� ������������ ������ ��������� ��������� ��������� ������������������ ��������������� ��������� ��������� ��������� ��������� ������������ ��������� ��������������� ��������� ��������������� ��������������� ��������������� ������������������ ��������� ��������� ������������ ������. ������ ������������ ������������������ ��������������� ��� ������ ������, ������������, ��������������� ��������� ������ ������, ��������� ������ ��������������� ���������, ��������������������� ������������ ��������������� ��������� ������������, ������ ��������������� ������������, ������������ ��������� ������������ ��������� ��������� ������ ��������� ��������� ��������������� ������������ ��� ������. ��������� ��������� ������ ��������� ��� ������������ ��������� ��������� ������ ������������ ������ ��������� ��������� ��� ������, ��������������� ��������������� ��������������� ������ ��������������� ��������� ��� ������ ���������, ��������� ��������� ������ ������ ������������ ��������� ������������ ��������� ��������� ��������� ��������� ������ ������������ ��������������� ��������� ��������� ������������.
��������� ������������ ��������� ��������������� ������������ ��������������� ������. ��������������� ������ ��������� ��������� ��������� ��������� ������������ ��������� ��������������� ��� ��������������� ��������� ��������� ������������ ������������ ������������������������ ��������� ��������� ��������� ������ ��������� ��������� ���������  ��������� ��������� ��������� ��������� ��� ������ ���������.


Concept of Automated Vehicle in the era of the fourth industrial revolution is transforming from the `vehicle` that had its focal point in the hardware to the `portable computer` that has its focus in the software that processes broad range of digital information. The phenomenon in one aspect is innovation through scientific technology. However on the other hand, it is raising humanistic, philosophical and legal questions about how human life will change according to the technological and industrial transition. Age of `Artificial Intelligence` and `Posthuman` emerging through the development of technology is creating new concept of human and bringing forward the issue of new danger that is occurring when reason and physical abilities thought as the essence of `human` is being substituted by scientific technology and computer. The advocates of automated vehicle assert that it provides more safety and convenience to human. Such argument cannot be completely be false, and I wish to reach the same conclusion eventually. However, as other process of introducing new technology like nuclear energy tell us, to consider whether automated vehicle is more convenient and better than the previous vehicle system and whether there are no new danger that new technology is bringing about or overlooked elements of danger is severely important. Therefore, if certain danger is expected, technological prevention should be devised as a first step, and for the sections where technology and industry is not self-regulating, it is legislations and policies` role to lead and force to a human-friendly direction. Automated vehicle that is being discussed nowadays can be neutral itself and seem to be no more than a convenient device. However, since each automated vehicle is activated by entering massive amount of digital information and since it collects, stores, and distributes a welter of personal information when connected to Internet of Things(IoT), whole new danger from the previous era is created in terms of personal information and privacy. The ironic reality of surveillance society since the modern times that has assured freedom of an individual on the one hand and has reinforced utilitarian society control on the grounds of fully protecting one`s right and expanding welfare on the other lies beneath this near future. Especially, personal information is more being exposed and distributed and collection and surveillance of digital information is becoming easier in the information age. Automated vehicle system cannot be free from such exposure and danger of violating since it is operated with geolocation, video, and communication information as well. Moreover, since violation of personal information can lead to violation of privacy and human dignity, and damage the basic order of liberal democracy, arranging the frame of public regulation relating to personal information protection by researching U.S. and European Union legislation and seems more than necessary. Being awake is what sovereign needs in the digital era. Likewise, to secure right to personal information, we should remember to turn the monitoring lamp on in advance not only as an individual, but also as a group in order to obtain convenience and prevent danger of new era of automated vehicle.��� ��������� ��������������� ������������������������������ ���������������(HY-2014������)","[{'title': 'Korean Journal of Law and Society', 'identifiers': ['1227-0954', 'issn:1227-0954']}]",������������������������,Public Law Perspectives on Personal Information Protection in the Era of Automated Vehicle,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
234929028,2019,"In recent years, the number of Industry 4.0 enabled manufacturing sites has been continuously growing, and both the quantity and variety of signals and data collected in plants are increasing at an unprecedented rate. 

At the same time, the demand of Big Data processing platforms and analytical tools tailored to manufacturing environments has become more and more prominent. 

Manufacturing companies are collecting huge amounts of information during the production process through a plethora of sensors and networks. 

To extract value and actionable knowledge from such precious repositories, suitable data-driven approaches are required. They are expected to improve the production processes by reducing maintenance costs, reliably predicting equipment failures, and avoiding quality degradation. To this aim, machine learning techniques tailored for predictive maintenance analysis have been adopted in PREMISES (PREdictive Maintenance service for Industrial procesSES), an innovative framework providing a scalable Big Data service able to predict alarming conditions in slowly-degrading processes characterized by cyclic procedures. PREMISES has been experimentally tested and validated on a real industrial use case, resulting efficient and effective in predicting alarms. The framework has been designed to address the main Big Data and industrial requirements, by being developed on a solid and scalable processing framework, Apache Spark, and supporting the deployment on modularized containers, specifically upon the Docker technology stack",,'Institute of Electrical and Electronics Engineers (IEEE)',"PREMISES, a scalable data-driven service to predict alarms in slowly-degrading multi-cycle industrial processes",10.1109/BigDataCongress.2019.00032,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
390020517,2017-12-25T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and moreРассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что ИВ создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применение в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернет речей (ІР), місця в ньому інтелектуальної власності. Показано роль держави у сприянні розвитку ІР, існуючі проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал та невикористані можливості України у розвитку технологій ІР.Робиться висновок, що у широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",,Науково-дослідний інститут інтелектуальної власності НAПрН України,ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
290044901,2008,"One of the areas that needs further improvement
within E-Learning environments via Internet (A big effort is
required in this area if progress is to be made) is allowing students
to access and practice real experiments in a real laboratory,
instead of using simulations [1]. Real laboratories allow students
to acquire methods, skills and experience related to real
equipment, in a manner that is very close to the way they are
being used in industry. The purpose of the project is the study,
development and implementation of an E-Learning environment
to allow undergraduate students to practice subjects related to
Robotics and Artificial Intelligence. The system, which is now at a
preliminary stage, will allow the remote experimentation with real
robotic devices (i.e. robots, cameras, etc.). It will enable the
student to learn in a collaborative manner (remote participation
with other students) where it will be possible to combine the onsite
activities (performed “in-situ” within the real lab during the
normal practical sessions), with the “on-line” one (performed
remotely from home via the Internet). Moreover, the remote
experiments within the E-Laboratory to control the real robots
can be performed by both, students and even scientist. This
project is under development and it is carried out jointly by two
Universities (UPC and UJI). In this article we present the system
architecture and the way students and researchers have been able
to perform a Remote Programming of Multirobot Systems via web",,Westing Publishing Co.,Remote Programming of Multirobot Systems within the UPC-UJI Telelaboratories: System Architecture and Agent-Based Multirobot Control,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
144874233,2017-11-08,"This paper will describe and present results for a local flood risk reduction system which utilises existing in-network storage capacity to attenuate flow peaks. The storage capacity is mobilised through active flow control automatically regulated by an Artificial Intelligence system using local level monitoring.

The effects of climate change, population growth and urbanisation are putting increasing pressure on sewer and drainage networks both in the UK and overseas. The capacity of networks to cope with runoff at the required rate often falls short of requirements leading to localised floods and/or increased CSO spills to receiving waters. Smart Water/ Wastewater Network technologies have the potential to deliver improved service to customers and cost-effective performance improvements for the water industry.

CENTAUR aims to provide an innovative, cost effective, local autonomous data driven in-sewer flow control system whose operation will attenuate peaks and reduce the risk of surface water flooding. The system enables the capacity of existing infrastructure to be utilised more efficiently as a very economical alternative to capital-intensive solutions, for example building extra storage capacity. The system is also quick to implement with virtually no enabling works prior to installation.

CENTAUR comprises level monitors which relay data to an intelligent controller, which instructs a flow control device regulated by a novel and robust artificial intelligence routine based on Fuzzy Logic. The level monitors and intelligent controller are located locally and utilise real time data to provide effective real time control (RTC).

The CENTAUR Fuzzy Logic control algorithm was developed in Matlab. The Matlab RTC algorithm was linked to a SWMM hydro-dynamic model of a test network to prove its efficiency. Further rigorous testing was carried out by the University of Sheffield on the full-scale test facility designed to replicate field conditions. The CENTAUR system has been further developed and it is now implemented and fully functional in trial site in Coimbra, Portugal. Results of successful testing in the laboratory and the Coimbra field trial will be presented",,,CENTAUR: Smart Utilisation of Wastewater storage capacity to prevent flooding,10.5281/zenodo.1051199,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
267815482,2019-11-08T00:00:00,"Current culture-based methods for detection and determination of Campylobacter levels on processed chickens takes at least 2\ua0days. Here we sought to develop a new complete, low-cost and rapid (approximately 2·5\ua0h) detection system requiring minimal operator input.We observed a strong correlation between culture-based cell counts and our ability to detect either Campylobacter jejuni or Campylobacter coli by loop-mediated isothermal amplification from the same samples. This knowledge was used to develop a rapid and simple five-step assay to quantify Campylobacter, which was subsequently assessed for its specificity, reproducibility and accuracy in quantifying Campylobacter levels from processed chickens. The assay was found to be highly specific for C. jejuni and C. coli and was capable of distinguishing between samples that are either within or exceeding the industry set target of 6000 Campylobacter colony forming units (CFU) per carcass (equivalent to 12\ua0CFU per ml of chicken rinse) with >90% accuracy relative to culture-based methods.Our method can reliably quantify Campylobacter counts of processed chickens with an accuracy comparable to culture-based assays but provides results within hours as opposed to days.The research presented here will help improve food safety by providing fast Campylobacter detection that will enable the implementation of real-time risk management strategies in poultry processing plants to rapidly test processed chickens and identify effective intervention strategies. This technology is a powerful tool that can be easily adapted for other organisms and thus could be highly beneficial for a broad range of industries",,'Wiley',"An easy‐to‐perform, culture‐free Campylobacter point‐of‐management assay for processing plant applications",10.1111/jam.14509,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
188688866,2018,"In the phase of industry digitalization, data are collected from many sensors and signal processing techniques play a crucial role. Data preprocessing is a fundamental step in the analysis of measurements, and a first step before applying machine learning. To reduce the influence of distortions from signals, selective digital filtering is applied to minimize or remove unwanted components. Standard software and hardware digital filtering algorithms introduce a delay, which has to be compensated for to avoid destroying signal associations. The delay from filtering becomes more crucial when the analysis involves measurements from multiple sensors, therefore in this paper we provide an overview and comparison of existing digital filtering methods with an application based on real-life marine examples. In addition, the design of special-purpose filters is a complex process and for preprocessing data from many sources, the application of digital filtering in the time domain can have a high numerical cost. For this reason we describe discrete Fourier transformation digital filtering as a tool for efficient sensor data preprocessing, which does not introduce a time delay and has low numerical cost. The discrete Fourier transformation digital filtering has a simpler implementation and does not require expert-level filter design knowledge, which is beneficial for practitioners from various disciplines. Finally, we exemplify and show the application of the methods on real signals from marine systems.acceptedVersion© 2018. This is the authors' accepted and refereed manuscript to the article. The final authenticated version is available online at: https://doi.org/10.1177%2F014233121879914",,SAGE Publications,Comparison of Delayless Digital Filtering Algorithms and Their Application to Multi-Sensor Signal Processing,10.1177/0142331218799148,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
209578960,2010-04-30,"Background/PurposeThe postweaning multisystemic wasting syndrome, caused by the porcine circovirus type 2 (PCV-2), is a major disease that poses a significant threat to the global swine industry. The purpose of this study was to establish a real-time polymerase chain reaction (PCR) method for the quantification of PCV-2 and to enable the rapid differentiation of porcine circoviruses type 1 and 2 (PCV-1 and PCV-2). Such a method would significantly speed up the process of clinical diagnosis, and could also be used to study the pathogenic mechanisms of diseases associated with PCV-2.MethodsMultiplex real-time PCR, together with LightCycler PCR data analysis software, was used for the quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2. A 263-bp DNA fragment was amplified from the 3′ end of the open reading frame-2 of PCV-2 by nested PCR, and its DNA sequence was verified as having 100% identity with a PCV-2 standard (NCBI accession number: AF055394). The 263-bp DNA fragment was cloned into the pGEM-T easy vector, and the recombinant plasmid was serially diluted and quantified using real-time PCR. A standard curve was then constructed for quantification of the PCV-2 levels in field samples. The differentiation of PCV-1 and PCV-2 was carried out by analyzing the melting temperatures of the genotype-specific PCR products.ResultsTo quantify the PCV-2 levels in field samples, a standard curve (1 × 102 −1 × 109 copies/μL) was constructed. PCV-2 concentrations as low as 1 × 102 copies/mL could be detected in specimens taken from the lymph nodes or infected tissues in samples of PCV-2-infected pigs. The diagnosis of PCV-1 and PCV-2 infections and the quantification of the viral load in the field samples could be completed within 45 minutes after extracting the viral DNA using a commercial extraction kit.ConclusionThis study demonstrate that real-time PCR is a clinically feasible method for the accurate quantification of PCV-2, and for the rapid differentiation of PCV-1 and PCV-2",,Taiwan Society of Microbiology. Published by Elsevier Taiwan LLC.,Fast Diagnosis and Quantification for Porcine Circovirus Type 2 (PCV-2) Using Real-Time Polymerase Chain Reaction ,10.1016/S1684-1182(10)60014-X,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
295497995,2019-02-19,"In the past, methods for hand sign recognition have been successfully tested in Human Robot Interaction (HRI) using traditional methodologies based on static image features and machine learning. However, the recognition of gestures in video sequences is a problem still open, because current detection methods achieve low scores when the background is undefined or in unstructured scenarios. Deep learning techniques are being applied to approach a solution for this problem in recent years. In this paper, we present a study in which we analyse the performance of a 3DCNN architecture for hand gesture recognition in an unstructured scenario. The system yields a score of 73% in both accuracy and F1. The aim of the work is the implementation of a system for commanding robots with gestures recorded by video in real scenarios.This work was funded by the Ministry of Economy, Industry and Competitiveness from the Spanish Government through the DPI2015-68087-R and the pre-doctoral grant BES-2016-078290, by the European Commission and FEDER funds through the project COMMANDIA (SOE2/P1/F0638), action supported by Interreg-V Sudoe",,SciTePress,3DCNN Performance in Hand Gesture Recognition Applied to Robot Arm Interaction,10.5220/0007570208020806,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
286140236,2019-08-25T08:19:25,"Studying developers’ behavior in software development tasks is crucial for designing effective techniques and tools to support developers’ daily work. In modern software development, developers frequently use different applications including IDEs, Web Browsers, documentation software (such as Office Word, Excel, and PDF applications), and other tools to complete their tasks. This creates significant challenges in collecting and analyzing developers’ behavior data. Researchers usually instrument the software tools to log developers’ behavior for further studies. This is feasible for studies on development activities using specific software tools. However, instrumenting all software tools commonly used in real work settings is difficult and requires significant human effort. Furthermore, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. This abstraction is often performed manually or based on simple heuristics. In this paper, we propose an approach to address the above two challenges in collecting and analyzing developers’ behavior data. First, we use our ActivitySpace framework to improve the generalizability of the data collection. ActivitySpace uses operating-system level instrumentation to track developer interactions with a wide range of applications in real work settings. Secondly, we use a machine learning approach to reduce the human effort to abstract low-level behavior data. Specifically, considering the sequential nature of the interaction data, we propose a Condition Random Field (CRF) based approach to segment and label the developers’ low-level actions into a set of basic, yet meaningful development activities. To validate the generalizability of the proposed data collection approach, we deploy the ActivitySpace framework in an industry partner’s company and collect the real working data from ten professional developers’ one-week work in three actual software projects. The experiment with the collected data confirms that with initial human-labeled training data, the CRF model can be trained to infer development activities from low-level actions with reasonable accuracy within and across developers and software projects. This suggests that the machine learning approach is promising in reducing the human efforts required for behavior data analysis.This work was partially supported by NSFC Program (No. 61602403 and 61572426)","[{'title': 'Empirical Software Engineering', 'identifiers': ['issn:1382-3256', '1382-3256']}]",'Springer Science and Business Media LLC',Inference of development activities from interaction with uninstrumented applications,10.1007/s10664-017-9547-8,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
202005127,2012-12-31,"AbstractThis paper presents an overview of Proportional Integral control (PI) and Artificial Intelligent control (AI) algorithms. AI and PI controller are analyzed using Matlab [Simulink] software. The DC motor is an attractive piece of equipment in many industrial applications requiring variable speed and load characteristics due to its ease of controllability. The main objective of this paper illustrates how the speed of the DC motor can be controlled using different controllers. The simulation results demonstrate that the responses of DC motor with an AI control which is Fuzzy Logic Control shows satisfactory well damped control performance. The results shows that Industrial DC Motor model develop using its physical parameters and controlled with an AI controller give better response, it means it can used as a controller to the real time DC Moto",,Published by Elsevier Ltd.,Modelling and Simulation for Industrial DC Motor Using Intelligent Control ,10.1016/j.proeng.2012.07.193,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201147855,2018-12-01T00:00:00,"The active implementation of digital technologies into all spheres of public life, as well as the rapid development of artificial intelligence, is assuming a serious dimension, thus requiring a special attention of the legislator. The article examines the current state of the legal regulation of the artificial intelligence. The author considers the Strategy of the Information Society Development in the Russian Federation for 2017-2030, as well as provides some clear examples of active implementation of artificial intelligence into social reality. The author also provides the McKinsey consulting group's research findings which reflect the prospects for replacing human labor by robots. It is pointed out that the issue of total computerization and the corresponding displacement of a human from the sphere of intellectual activity is rather controversial. The article also discusses the main possible problems related to the artificial intelligence technologies: the problems of responsibility that may arise in the operation of industrial robots; the continuity of digital activity can affect the psychoemotional state. The issue of a possibility for creating robots with intelligence and endowed with personality is being considered from the Philosophy perspective. The conclusion is drawn that the theoretical study of the intellect and the ""electronic person"" is one of the possible redirections of the Russian law development in modern conditions","[{'title': 'RUDN JOURNAL OF LAW', 'identifiers': ['2408-9001', 'issn:2408-9001', 'issn:2313-2337', '2313-2337']}]",'Peoples'' Friendship University of Russia',Artificial Intelligence in the Legal Space,10.22363/2313-2337-2018-22-3-315-328,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201567711,2017-05-01T00:00:00,"Abstract Background Standardized animal-free components are required for manufacturing cell-based medicinal products. Human platelet concentrates are sources of growth factors for cell expansion but such products are characterized by undesired variability. Pooling together single-donor products improves consistency, but the minimal pool sample size was never determined. Methods Supernatant rich in growth factors (SRGF) derived from n = 44 single-donor platelet-apheresis was obtained by CaCl2 addition. n = 10 growth factor concentrations were measured. The data matrix was analyzed by a novel statistical algorithm programmed to create 500 groups of random data from single-donor SRGF and to repeat this task increasing group statistical sample size from n = 2 to n = 20. Thereafter, in created groups (n = 9500), the software calculated means for each growth factor and, matching groups with the same sample size, the software retrieved the percent coefficient of variation (CV) between calculated means. A 20% CV was defined as threshold. For validation, we assessed the CV of concentrations measured in n = 10 pools manufactured according to algorithm results. Finally, we compared growth rate and differentiation potential of adipose-derived stromal/stem cells (ASC) expanded by separate SRGF pools. Results Growth factor concentrations in single-donor SRGF were characterized by high variability (mean (pg/ml)–CV); VEGF: 950–81.4; FGF-b: 27–74.6; PDGF-AA: 7883–28.8; PDGF-AB: 107834–32.5; PDGF-BB: 11142–48.4; Endostatin: 305034–16.2; Angiostatin: 197284–32.9; TGF-β1: 68382–53.7; IGF-I: 70876–38.3; EGF: 2411–30.2). In silico performed analysis suggested that pooling n = 16 single-donor SRGF reduced CV below 20%. Concentrations measured in 10 pools of n = 16 single SRGF were not different from mean values measured in single SRGF, but the CV was reduced to or below the threshold. Separate SRGF pools failed to differently affect ASC growth rate (slope pool A = 0.6; R2 = 0.99; slope pool B = 0.7; R2 0.99) or differentiation potential. Discussion Results deriving from our algorithm and from validation utilizing real SRGF pools demonstrated that pooling n = 16 single-donor SRGF products can ameliorate variability of final growth factor concentrations. Different pools of n = 16 single donor SRGF displayed consitent capability to modulate growth and differentiation potential of expanded ASC. Increasing the pool size should not further improve product composition","[{'title': 'Journal of Translational Medicine', 'identifiers': ['issn:1479-5876', '1479-5876']}]",'Springer Science and Business Media LLC',Standardization of platelet releasate products for clinical applications in cell therapy: a mathematical approach,10.1186/s12967-017-1210-z,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
36856411,2014,"Ceramic tiles, used in body armour systems, are currently inspected visually offline using an X-ray technique that is both time consuming and very expensive. The aim of this research is to develop a methodology to detect, locate and classify various manufacturing defects in Reaction Sintered Silicon Carbide (RSSC) ceramic tiles, using an ultrasonic sensing technique. Defects such as free silicon, un-sintered silicon carbide material and conventional porosity are often difficult to detect using conventional X-radiography. An alternative inspection system was developed to detect defects in ceramic components using an Artificial Neural Network (ANN) based signal processing technique. The inspection methodology proposed focuses on pre-processing of signals, de-noising, wavelet decomposition, feature extraction and post-processing of the signals for classification purposes. This research contributes to developing an on-line inspection system that would be far more cost effective than present methods and, moreover, assist manufacturers in checking the location of high density areas, defects and enable real time quality control, including the implementation of accept/reject criteria",,Elsevier,Ultrasonic sensor based defect detection and characterisation of ceramics,10.1016/j.ultras.2013.07.018,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
148683794,2015-06-03T00:00:00,"Nowadays, even though cognitive control architectures form an important area of research, there are many constraints on the broad application of cognitive control at an industrial level and very few systematic approaches truly inspired by biological processes, from the perspective of control engineering. Thus, our main purpose here is the emulation of human socio-cognitive skills, so as to approach control engineering problems in an effective way at an industrial level. The artificial cognitive control architecture that we propose, based on the shared circuits model of socio-cognitive skills, seeks to overcome limitations from the perspectives of computer science, neuroscience and systems engineering. The design and implementation of artificial cognitive control architecture is focused on four key areas: (i) self-optimization and self-leaning capabilities by estimation of distribution and reinforcement-learning mechanisms; (ii) portability and scalability based on low-cost computing platforms; (iii) connectivity based on middleware; and (iv) model-driven approaches. The results of simulation and real-time application to force control of micro-manufacturing processes are presented as a proof of concept. The proof of concept of force control yields good transient responses, short settling times and acceptable steady-state error. The artificial cognitive control architecture built into a low-cost computing platform demonstrates the suitability of its implementation in an industrial setup",,'Elsevier BV',Artificial cognitive control with self-x capabilities: A case study of a micro-manufacturing process,10.1016/j.compind.2015.05.001,https://core.ac.uk/download/148683794.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
161502228,2016,"The paper provides a state of the art review of guided wave based structural health monitoring (SHM). First, the fundamental concepts of guided wave propagation and its implementation for SHM is explained. Following sections present the different modeling schemes adopted, developments in the area of transducers for generation, and sensing of wave, signal processing and imaging technique, statistical and machine learning schemes for feature extraction. Next, a section is presented on the recent advancements in nonlinear guided wave for SHM. This is followed by section on Rayleigh and SH waves. Next is a section on real-life implementation of guided wave for industrial problems. The paper, though briefly talks about the early development for completeness,. is primarily focussed on the recent progress made in the last decade. The paper ends by discussing and highlighting the future directions and open areas of research in guided wave based SHM",,IOP PUBLISHING LTD,Guided wave based structural health monitoring: A review,10.1088/0964-1726/25/5/053001,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
288353915,2019-06-05T00:00:00,"Approximately one-third of the food produced globally is spoiled or wasted in the food supply chain (FSC). Essentially, it is lost before it even reaches the end consumer. Conventional methods of food waste tracking relying on paper-based logs to collect and analyse the data are costly, laborious, and time-consuming. Hence, an automated and real-time system based on the Internet of Things (IoT) concepts is proposed to measure the overall amount of waste as well as the reasons for waste generation in real-time within the potato processing industry, by using modern image processing and load cell technologies. The images captured through a specially positioned camera are processed to identify the damaged, unusable potatoes, and a digital load cell is used to measure their weight. Subsequently, a deep learning architecture, specifically the Convolutional Neural Network (CNN), is utilised to determine a potential reason for the potato waste generation. An accuracy of 99.79% was achieved using a small set of samples during the training test. We were successful enough to achieve a training accuracy of 94.06%, a validation accuracy of 85%, and a test accuracy of 83.3% after parameter tuning. This still represents a significant improvement over manual monitoring and extraction of waste within a potato processing line. In addition, the real-time data generated by this system help actors in the production, transportation, and processing of potatoes to determine various causes of waste generation and aid in the implementation of corrective actions",,,Monitoring potato waste in food manufacturing using image processing and Internet of Things approach,,https://core.ac.uk/download/288353915.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
236628368,2019-08-06T00:00:00,"Industry 4.0 refers to the new technological development occurred at the industrial production systems. It evolved as a result of integrating Internet of Things, Cyber-Physical Systems, Big-Data, Artificial Intelligence, and Cloud Computing in the industrial systems. This integration aided new capabilities to achieve a higher level of business excellence, efficiency, and effectiveness. Total Quality Management (TQM) is a managerial approach to achieve an outstanding business excellence. There are several approaches to apply TQM principles at any organization. Industry 4.0 could be utilized as a key enabler for TQM especially by integrating its techniques with the TQM best practices. This paper suggests a theoretical framework for integrating Industry 4.0 features with the TQM principles (according to ISO 9000:2015 standards family) in order to open the door for further research to address the real impact of utilizing Industry 4.0 for serving the TQM implementation approaches",,'Periodica Polytechnica Budapest University of Technology and Economics',Industry 4.0 as a Key Enabler toward Successful Implementation of Total Quality Management Practices,,https://core.ac.uk/download/236628368.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
478076158,2019-01-01T00:00:00,"Mixed norms that promote structured sparsity have numerous applications in signal processing and machine learning problems. In this work, we present a new algorithm, based on a Newton root search technique, for computing the projection onto the l ∞,1 ball, which has found application in cognitive neuroscience and classification tasks. Numerical simulations show that our proposed method is between 8 and 10 times faster on average, and up to 20 times faster for very sparse solutions, than the previous state of the art. Tests on real functional magnetic resonance image data show that, for some data distributions, our algorithm can obtain speed improvements by a factor of between 10 and 100, depending on the implementation. © 2019 Society for Industrial and Applied Mathematics","[{'title': 'SIAM Journal on Imaging Sciences', 'identifiers': ['1936-4954', 'issn:1936-4954']}]",'Society for Industrial & Applied Mathematics (SIAM)',"Efficient projection onto the ℓ ∞,1 mixed-norm ball using a newton root search method",10.1137/18M1212525,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
297618519,,"Eye movement behaviour is related to human brain activation either during
asleep or awake. The aim of this paper is to measure the three types of eye
movement by using the data classification of electroencephalogram (EEG)
signals. It will be illustrated and train using the artificial neural network (ANN)
method, in which the measurement of eye movement is based on eye blinks close
and open, moves to the left and right as well as eye movement upwards and
downwards. The integrated of ANN with EEG digital data signals is to train the
large-scale digital data and thus predict the eye movement behaviour with stress
activity. Since this study is using large-scale digital data, the parallelization of
integrated ANN with EEG signals has been implemented on Compute Unified
Device Architecture (CUDA) supported by heterogeneous CPU-GPU systems. The
real data set from eye therapy industry, IC Herbz Sdn Bhd was carried out in order
to validate and simulate the eye movement behaviour. Parallel performance
analyses can be captured based on execution time, speedup, efficiency, and
computational complexity",,"Jurnal Teknologi, Universiti Teknologi Malaysia",Parallel artificial neural network approaches for detecting the behaviour of eye movement using cuda software on heterogeneous cpu-gpu systems,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
92690630,2017-01-01T00:00:00,"Power grids are critical infrastructure assets that face non-technical losses (NTL) such as electricity theft or faulty meters. NTL may range up to 40% of the total electricity distributed in emerging countries. Industrial NTL detection systems are still largely based on expert knowledge when deciding whether to carry out costly on-site inspections of customers. Electricity providers are reluctant to move to large-scale deployments of automated systems that learn NTL profiles from data due to the latter's propensity to suggest a large number of unnecessary inspections. In this paper, we propose a novel system that combines automated statistical decision making with expert knowledge. First, we propose a machine learning framework that classifies customers into NTL or non-NTL using a variety of features derived from the customers' consumption data. The methodology used is specifically tailored to the level of noise in the data. Second, in order to allow human experts to feed their knowledge in the decision loop, we propose a method for visualizing prediction results at various granularity levels in a spatial hologram. Our approach allows domain experts to put the classification results into the context of the data and to incorporate their knowledge for making the final decisions of which customers to inspect. This work has resulted in appreciable results on a real-world data set of 3.6M customers. Our system is being deployed in a commercial NTL detection software",,,Identifying Irregular Power Usage by Turning Predictions into Holographic Spatial Visualizations,10.1109/icdmw.2017.40,https://core.ac.uk/download/92690630.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
477988955,2016-04-01T00:00:00,"[EN] Human-agent societies refer to applications where virtual agents and humans coexist and interact transparently into a fully integrated environment. One of the most important aspects in this kind of applications is including emotional states of the agents (humans or not) in the decision-making process. In this sense, this paper presents the applicability of the JaCalIVE framework for developing this kind of societies. Specifically, the paper presents an ambient intelligence application where humans are immersed into a system that extracts and analyzes the emotional state of a human group. A social emotional model is employed in order to try to maximize the welfare of those humans by playing the most appropriate music in every moment.Project supported by the Ministerio de Economia y Competitividad of the Spanish Government and the European Regional Development Fund of the European Union (No. TIN2015-65515-C4-1-R)Rincon, JA.; Bajo, J.; Fernandez, A.; Julian Inglada, VJ.; Carrascosa Casamayor, C. (2016). Using emotions for the development of human-agent societies. Frontiers of Information Technology & Electronic Engineering. 17(4):325-337. https://doi.org/10.1631/FITEE.1500343S325337174Ali, F., Amin, M., 2014. The influence of physical environment on emotions, customer satisfaction and behavioural intentions in Chinese resort hotel industry. J. Glob. Bus. Adv., 7(3):249–266. http://dx.doi.org/10.1504/JGBA.2014.064109Bales, R.F., 2001. Social Interaction Systems: Theory and Measurement. Transaction Publishers, USA.Barella, A., Ricci, A., Boissier, O., et al., 2012. MAM5: multi-agent model for intelligent virtual environments. Proc. 10th European Workshop on Multi-Agent Systems, p.16–30.Billhardt, H., Julian, V., Corchado, J.M., et al., 2014. An architecture proposal for human-agent societies. Proc. Int. Workshop on Highlights of Practical Applications of Heterogeneous Multi-Agent Systems, p.344–357. http://dx.doi.org/10.1007/978-3-319-07767-3_31Ducatel, K., Bogdanowicz, M., Scapolo, F., et al., 2001. Scenarios for Ambient Intelligence in 2010. Office for Official Publications of the European Communities.Fernandez, A., Ossowski, S., 2011. A multiagent approach to the dynamic enactment of semantic transportation services. IEEE Trans. Intell. Transp. Syst., 12(2):333–342. http://dx.doi.org/10.1109/TITS.2011.2106776Hale, K.S., Stanney, K.M., 2002. Handbook of Virtual Environments: Design, Implementation, and Applications. CRC Press, USA.Han, D.M., Lim, J.H., 2010. Smart home energy management system using IEEE 802.15.4 and ZigBee. IEEE Trans. Consum. Electron., 56(3):1403–1410. http://dx.doi.org/10.1109/TCE.2010.5606276Hendler, J., 2007. Where are all the intelligent agents? IEEE Intell. Syst., 22:2–3.Holzapfel, A., Stylianou, Y., 2007. A statistical approach to musical genre classification using non-negative matrix factorization. Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing, p.693–696. http://dx.doi.org/10.1109/ICASSP.2007.366330Huhns, M.N., Singh, M.P., Burstein, M., et al., 2005. Research directions for service-oriented multiagent systems. IEEE Internet Comput., 9(6):65.Intille, S.S., 2002. Designing a home of the future. IEEE Perva. Comput., 1(2):76–82.Lawrence, S., Giles, C.L., Tsoi, A.C., et al., 1997. Face recognition: a convolutional neural-network approach. IEEE Trans. Neur. Netw., 8(1):98–113. http://dx.doi.org/10.1109/72.554195Li, T., Ogihara, M., Li, Q., 2003. A comparative study on content-based music genre classification. Proc. 26th Annual Int. ACM SIGIR Conf. on Research and Development in Information Retrieval, p.282–289. http://dx.doi.org/10.1145/860435.860487Mangina, E., Carbo, J., Molina, J.M., 2009. Agent-Based Ubiquitous Computing. Atlantis Press, France. http://dx.doi.org/10.2991/978-94-91216-31-2Mehrabian, A., 1980. Basic Dimensions for a General Psychological Theory: Implications for Personality, Social, Environmental, and Developmental Studies. Oelgeschlager, Gunn & Hain, USA.Mehrabian, A., 1997. Analysis of affiliation-related traits in terms of the PAD temperament model. J. Psychol., 131(1):101–117. http://dx.doi.org/10.1080/00223989709603508Nanty, A., Gelin, R., 2013. Fuzzy controlled PAD emotional state of a NAO robot. Proc. Conf. on Technologies and Applications of Artificial Intelligence, p.90–96. http://dx.doi.org/10.1109/TAAI.2013.30Ortony, A., 1990. The Cognitive Structure of Emotions. Cambridge University Press, USA.Ossowski, S., 2013. Agreement Technologies: 8 (Law, Governance and Technology Series). Springer, the Netherlands. http://dx.doi.org/10.1007/978-94-007-5583-3Osuna, E., Freund, R., Girosit, F., 1997. Training support vector machines: an application to face detection. Proc. IEEE Computer Society Conf. on Computer Vision and Pattern Recognition, p.130–136. http://dx.doi.org/10.1109/CVPR.1997.609310Rincon, J.A., Garcia, E., Julian, V., et al., 2014. Developing adaptive agents situated in intelligent virtual environments. Proc. 9th Int. Conf. on Hybrid Artificial Intelligence Systems, p.98–109. http://dx.doi.org/10.1007/978-3-319-07617-1_9Rincon, J.A., Julian, V., Carrascosa, C., 2015a. Applying a social emotional model in human-agent societies. Proc. Int. Workshops of Practical Applications of Agents, Multi-Agent Systems, p.377–388. http://dx.doi.org/10.1007/978-3-319-19033-4_33Rincon, J.A., Julian, V., Carrascosa, C., 2015b. Social emotional model. Proc. 13th Int. Conf. on Practical Applications of Agents, Multi-Agent Systems, p.199–210. http://dx.doi.org/10.1007/978-3-319-18944-4_17Satyanarayanan, M., 2001. Pervasive computing: vision and challenges. IEEE Pers. Commun., 8(4):10–17. http://dx.doi.org/10.1109/98.943998Satyanarayanan, M., 2002. A catalyst for mobile and ubiquitous computing. IEEE Perva. Comput., 1(1):2–5. http://dx.doi.org/10.1109/MPRV.2002.993138Talupur, M., Nath, S., Yan, H., 2001. Classification of Music Genre. Project Report for 15781.Viola, P., Jones, M.J., 2004. Robust real-time face detection. Int. J. Comput. Vis., 57(2):137–154. http://dx.doi.org/10.1023/B:VISI.0000013087.49260.fbWeiser, M., 1991. The computer for the 21st century. Sci. Am., 265(3):94–104. http://dx.doi.org/10.1038/scientificamerican0991-94Zambonelli, F., Jennings, N.R., Wooldridge, M., 2003. Developing multiagent systems: the Gaia methodology. ACM Trans. Softw. Eng. Meth., 12(3):317–370. http://dx.doi.org/10.1145/958961.95896",,'Zhejiang University Press',Using emotions for the development of human-agent societies,10.1631/FITEE.1500343,http://hdl.handle.net/10251/168936,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
352949308,2013-01-01T00:00:00,"The process chain for the production of profiled strips, beams and panels includes a number of processes as mould design, fixing the number of cutting passes, tool grinding, tool measurement, moulding, sanding, profile-wrapping, coating and different types of quality assessment. This chain is still not consistently connected by means of information technologies. The design and the setup for each process in the chain are mostly done manually, using the experience of the operator. Computer aided systems (CAD/CAM) as widely used in the furniture industry are not in this field. The procedures are more complex and could not be automatized by the state of the art feature based algorithms. One attempt to solve the problem is the creation of a product data model that contains all information for the whole production process - not only geometrical data and tolerances but also semantic information (e.g. material, coating, batch size, etc.). Every process in the chain is now transforming the profile in a systematic and stochastic way. Both can be computed by the use of rules of artificial intelligence and tolerance calculations by means of statistical methods. Special CAM modules, which generate the specific process data, give also back the transformed profile and other necessary information to the superordinate product data model. The actual specific process data, for example for the process of profiling (e.g. setting parameters, pitch) can then be computed by a post processer using actual inputs as the geometry of the specified machine. The whole process chain can so be simulated before the real setup. The tools and processes can so be designed simultaneously in advance. The time for the production process can be reduced. This systematic is generating the same potentials as feature based CAD/CAM systems in the carcase furniture industry. The implementation of IT in the profiling process chain is accelerated and coordinated by the introduction of the linking product model",,Luleå Univ. of Technology,A product data model and computer aided manufacturing for the process chain of profiling,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
33222858,,"[[abstract]]This paper proposes an intelligent evolutionary algorithm that can be applied in the design of optimal automation systems, and employs a multimodal six-bar mechanism optimization design, job shop production scheduling for the fishing equipment industry, and dynamic real-time production scheduling system design cases to show how the technique developed in this paper is highly effective at resolving optimal automation system design problems. Major breakthroughs in artificial intelligence continue to be made in the wake of advanced information technology developments, and the field of intelligent evolutionary algorithms has attracted a particularly large amount of attention from researchers and users in the artificial intelligence community. The successful optimization of automation system design requires interdisciplinary integration, and further requires the use of actual cases, verification, and improvement to ensure implementation in real-world applications",,'International Journal of Automation and Smart Technology',Applications of Intelligent Evolutionary Algorithms in Optimal Automation System Design,10.5875/ausmt.v1i1.100,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
81111337,2016-12-31,"AbstractTransferring predictive microbial models from research into real world food manufacturing or risk assessment applications is still a challenge for members of the food safety modelling community. Such knowledge transfer could be facilitated if publicly available food safety model repositories would exist.This research therefore aimed at identification of missing resources hampering the establishment of community driven food safety model repositories. Existing solutions in related scientific disciplines like Systems Biology and Data Mining were analyzed.On the basis of this analysis, some factors which would promote the establishment of community driven model repositories were identified – among them: a standardized information exchange format for models and rules for model annotation. As a consequence a proposal for a Predictive Modelling in Food Markup Language (PMF-ML) together with a prototypic implementation on the basis of the Systems Biology Markup Language (SBML) has been developed. In addition the adoption of MIRIAM guidelines for model annotation is proposed. In order to demonstrate the practicability of the proposed strategy, existing predictive models previously published in the scientific literature were re-implemented using an open source software tool called PMM-Lab. The models are made publicly available in the first community Food Safety Model Repository called openFSMR (https://sites.google.com/site/openfsmr/).This work illustrates that a standardized information exchange format for predictive microbial models can be established by adoption of resources from Systems Biology. Harmonized description and annotation of predictive models will also contribute to increased transparency and quality of food safety models",,'Elsevier BV',Towards Community Driven Food Safety Model Repositories ,10.1016/j.profoo.2016.02.098,https://core.ac.uk/download/pdf/81111337.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
144707204,2016-03-23,"Electricity markets are becoming more competitive, to some extent due to the increasing number of players that have moved from other sectors to the power industry. This is essentially resulting from incentives provided to distributed generation. Relevant changes in this domain are still occurring, such as the extension of national and regional markets to continental scales. Decision support tools have thereby become essential to help electricity market players in their negotiation process. This paper presents a metalearner to support electricity market players in bidding definition. The proposed metalearner uses a dynamic artificial neural network to create its own output, taking advantage on several learning algorithms already implemented in ALBidS (Adaptive Learning strategic Bidding System). The proposed metalearner considers different weights for each strategy, based on their individual performance. The metalearner's performance is analysed in scenarios based on real electricity markets data using MASCEM (Multi-Agent Simulator for Competitive Electricity Markets). Results show that the proposed metalearner is able to provide higher profits to market players when compared to other current methodologies and that results improve over time, as consequence of its learning process",,,Metalearning to support competitive electricity market players' strategic bidding,10.1016/j.epsr.2016.03.012,https://core.ac.uk/download/pdf/144707204.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
478373242,2019-07-16T00:00:00,"Artificial Intelligence (AI) has wide range of applications in all areas and is gaining the understanding of the society as necessity instead of luxury. AI start ups are working to improve the quality of social interactions (social good), Education, Agriculture, manufacturing, health and medicine and public services. Hence the cost of not developing AI or developing it late is enormous. Despite the opportunities AI technologies may offer, there is a real risk that without thoughtful intervention it may in fact exacerbate structural, economic, social, and political imbalances, and further reinforce entrenched inequalities. For regulators and policymakers around the world, uneven access to technology remains a major concern because of its potential impacts on social and economic inequality. The author has conducted an exploratory research by reviewing related literatures on AI opportunities and challenges from experiences of the developed world and provided a discussion to identify the potential opportunities and expected challenges for AI adoption and implementation in Ethiopia. Finally the author has recommended what should be done.Keywords: Artificial, Intelligence,, Exploratory, Research, Skills, Infrastructure, Data, Privac",,'African Journals Online (AJOL)',Artificial intelligence for Ethiopia: opportunities and challenges,10.4314/ict.v16i1.,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
323090881,2015-10-15T00:00:00,"[ES] En este trabajo se presenta un nuevo esquema de control para un sistema fotovoltaico aislado (PV) utilizando un controlador de lógica borrosa (FLC). El sistema de control diseñado proporciona un buen seguimiento de la tensión de referencia óptima, a la cual se genera la máxima potencia. El sistema fotovoltaico está conectado a una carga a través de un convertidor CC/CC elevador (boost). El controlador FLC proporciona el ciclo de trabajo (D) apropiado al convertidor CC/CC para que el sistema PV genere la máxima potencia. También se propone un método de análisis de la estabilidad del sistema en lazo cerrado. Aunque el análisis de la estabilidad está basado en la metodología de Lyapunov, es un análisis semicualitativo, ya que no se dispone de un modelo del sistema en lazo cerrado para realizar un análisis analítico. Tanto los resultados de simulación como las pruebas experimentales sobre un sistema PV comercial muestran que el FLC proporciona un buen seguimiento del punto de máxima potencia (MPP). Finalmente, se ha evaluado el funcionamiento del FLC sobre un sistema PV real formado por unas placas fotovoltaicas comerciales Atersa modelo A55. Para realizar las pruebas experimentales se ha implementado la estrategia de control sobre un procesador digital de señal DS1104 de dSPACE. Los resultados experimentales obtenidos demuestran la validez del esquema de control FLC sobre un sistema fotovoltaico comercial.[EN] This paper presents a new control scheme for a standalone photovoltaic (PV) system based on a fuzzy-logic (FLC). The proposed control system provides good tracking for the optimal reference voltage, at which the maximum power generation is obtained. The photovoltaic system is connected to a load through a DC/DC boost converter. The FLC controller provides the appropriate duty cycle (D) to the DC/DC converter in order to get the maximum power from the PV system. A method for the stability analysis of the closed-loop system is also proposed. The stability analysis is based on the Lyapunov methods and is a semi-qualitative analysis because there is no closed loop system model available for the analytical analysis. Both simulation results and experimental tests on a real PV system show that the FLC provides good tracking for the maximum power point (MPP).Finally, the performance of the FLC on a real PV system consisting of a commercial solar panels Atersa model A55 is analyzed. To perform the experimental tests the proposed control strategy has been implemented on the dSPACE digital signal processor model DS1104. The experimental results demonstrate the good performance of the proposed FLC control scheme over a commercial photovoltaic system.Este trabajo ha sido realizado parcialmente gracias al apoyo de la Universidad del País Vasco (GIU13/41 y UFI11/07) y al proyecto TEP-6124 financiado por la Junta de AndalucíaFarhat, M.; Barambones, Ó.; Ramos, JA.; Durán, E.; Andújar, JM. (2015). Diseño e Implementación de un Sistema de Control estable basado en Lógica Borrosa para optimizar el rendimiento de un sistema de Generación Fotovoltaico. Revista Iberoamericana de Automática e Informática industrial. 12(4):476-487. https://doi.org/10.1016/j.riai.2015.07.006OJS476487124Fadili, A. E., Giri, F., & Magri, A. E. (2013). Reference Voltage Optimizer for Maximum Power Tracking in Single-Phase Grid-Connected Photovoltaic Systems. Journal of Control and Systems Engineering, 1(2), 57-66. doi:10.18005/jcse0102004Alajmi, B. N., Ahmed, K. H., Finney, S. J., & Williams, B. W. (2011). Fuzzy-Logic-Control Approach of a Modified Hill-Climbing Method for Maximum Power Point in Microgrid Standalone Photovoltaic System. IEEE Transactions on Power Electronics, 26(4), 1022-1030. doi:10.1109/tpel.2010.2090903Algazar, M. M., AL-monier Hamdy, EL-halim, H. A., & Salem, M. E. E. K. (2012). Maximum power point tracking using fuzzy logic control. International Journal of Electrical Power & Energy Systems, 39(1), 21-28. doi:10.1016/j.ijepes.2011.12.006Andújar, J. M., & Barragán, A. J. (2014). Hibridación de sistemas borrosos para el modelado y control. Revista Iberoamericana de Automática e Informática Industrial RIAI, 11(2), 127-141. doi:10.1016/j.riai.2014.03.004Andújar, J. M, Segura, F. (2012), Power Management Based on Sliding Control Applied to Fuel Cell System. A further Step toward the Hybrid Control Concept.Applied Energy.99, 213-225.Andújar, J. M., Barragán, A. J., Gegúndez, M. E., & Maestre, M. (2007). Control borroso multivariable basado en heurística. un caso práctico: grúa porta contenedores. Revista Iberoamericana de Automática e Informática Industrial RIAI, 4(2), 81-89. doi:10.1016/s1697-7912(07)70212-1Brunton S.L., Rowley C.W., Kulkarni S.R., Clarkson C. (2010), Maximum Power Point Tracking for Photovoltaic Optimization Using Ripple-Based Extremum Seeking Control, IEEE Trans. on Power Electronics Vol.25, No.10.Ben Salah, C., & Ouali, M. (2011). Comparison of fuzzy logic and neural network in maximum power point tracker for PV systems. Electric Power Systems Research, 81(1), 43-50. doi:10.1016/j.epsr.2010.07.005Durán, E., Andújar, J. M., Galán, J., & Sidrach-de-Cardona, M. (2009). Methodology and experimental system for measuring and displaying I
-V
 characteristic curves of PV facilities. Progress in Photovoltaics: Research and Applications, 17(8), 574-586. doi:10.1002/pip.909Farhat, M., Flah, A., & Sbita, L. (2014). Photovoltaic Maximum Power Point Tracking Based on ANN Control. International Review on Modelling and Simulations (IREMOS), 7(3), 474. doi:10.15866/iremos.v7i3.1212Farhat. M, Sbita.L, (2012), Advanced ANFIS-MPPT Control Algorithm for Sunshine photovoltaic Pumping Systems,” International Conference on Renewable Energies and Vehicular TechnologyTaeed, F., Salam, Z., & Ayob, S. (2012). FPGA Implementation of a Single-Input Fuzzy Logic Controller for Boost Converter With the Absence of an External Analog-to-Digital Converter. IEEE Transactions on Industrial Electronics, 59(2), 1208-1217. doi:10.1109/tie.2011.2161250Hannan. M. A, Ghani. Abd. Z, Mohamed.A, (2010), An Enhanced Inverter Controller for PV Applications Using the dSPACE Platform, Hindawi Publishing Corporation, International Journal of Photoenergy.Hirosato. S, (2013), Nonlinear Identification Using Single Input Connected Fuzzy Inference Model, 17th International Conference in Knowledge Based and Intelligent Information and Engineering Systems,.Park, J., & Kim, S. (2012). Maximum Power Point Tracking Controller for Thermoelectric Generators with Peak Gain Control of Boost DC–DC Converters. Journal of Electronic Materials, 41(6), 1242-1246. doi:10.1007/s11664-011-1884-6Marcelo. G .V,Jonas. R. G, Ernesto. R. F (2009), Modeling and circuit-based simulation of photovoltaic arrays, 10th Brazilian Power Electronics Conference (COBEP).Nevzat. O, (2010), Recent Developments in Maximum Power Point Tracking Technologies for Photovoltaic Systems, Hindawi Publishing Corporation International Journal of Photoenergy.Ollervides, J., Santibáñez, V., Llama, M., & Dzul, A. (2010). Aplicación de Control Borroso a un Sistema de Suspensión Magnética: Comparación Experimental. Revista Iberoamericana de Automática e Informática Industrial RIAI, 7(3), 63-71. doi:10.1016/s1697-7912(10)70043-1Patcharaprakiti, N., Premrudeepreechacharn, S., & Sriuthaisiriwong, Y. (2005). Maximum power point tracking using adaptive fuzzy logic control for grid-connected photovoltaic system. Renewable Energy, 30(11), 1771-1788. doi:10.1016/j.renene.2004.11.018Zhou, S., Kang, L., Sun, J., Guo, G., Cheng, B., Cao, B., & Tang, Y. (2010). A novel maximum power point tracking algorithms for stand-alone photovoltaic system. International Journal of Control, Automation and Systems, 8(6), 1364-1371. doi:10.1007/s12555-010-0624-7Esram, T., & Chapman, P. L. (2007). Comparison of Photovoltaic Array Maximum Power Point Tracking Techniques. IEEE Transactions on Energy Conversion, 22(2), 439-449. doi:10.1109/tec.2006.874230Villalva, M. G., Gazoli, J. R., & Filho, E. R. (2009). Comprehensive Approach to Modeling and Simulation of Photovoltaic Arrays. IEEE Transactions on Power Electronics, 24(5), 1198-1208. doi:10.1109/tpel.2009.2013862Zaidi. Z, F. Boudjema, (2010), Hybrid Control and Optimization of a Plus-Energy-House with DHWS, International Renewable Energy Congress IREC, Sousse, Tunisia",,'Elsevier BV',Design and Implementation of a Stable Control System based on Fuzzy Logic in order to optimize the performance of a Photovoltaic Generation System,10.1016/j.riai.2015.07.006,http://hdl.handle.net/10251/143677,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
395142912,2019-11-01T00:00:00,"[EN] Falls represent a major public health risk worldwide for the elderly people. A fall not assisted in time can cause functional impairment in an elderly and a significant decrease in his mobility, independence, and life quality. In this sense, we propose IoTE-Fall system, an intelligent system for detecting falls of elderly people in indoor environments that takes advantages of the Internet of Thing and the ensemble machine learning algorithm. IoTE-Fall system employs a 3D-axis accelerometer embedded into a 6LowPAN wearable device capable of capturing in real time the data of the movements of elderly volunteers. To provide high efficiency in fall detection, in this paper, four machine learning algorithms (classifiers): decision trees, ensemble, logistic regression, and Deepnets are evaluated in terms of AUC ROC, training time and testing time. The acceleration readings are processed and analyzed at the edge of the network using an ensemble-based predictor model that is identified as the most suitable predictor for fall detection. The experiment results from collection data, interoperability services, data processing, data analysis, alert emergency service, and cloud services show that our system achieves accuracy, precision, sensitivity, and specificity above 94%.Research presented in this article has been partially funded by Horizon 2020 European Project grant INTER-IoT no. 687283, ACTIVAGE project under grant agreement no. 732679, the Escuela Politecnica Nacional, Ecuador, and Secretaria de Educacion Superior Ciencia, Tecnologia e Innovacion (SENESCYT), Ecuador.Yacchirema, D.; Suárez De Puga, J.; Palau Salvador, CE.; Esteve Domingo, M. (2019). Fall detection system for elderly people using IoT and ensemble machine learning algorithm. Personal and Ubiquitous Computing. 23(5-6):801-817. https://doi.org/10.1007/s00779-018-01196-8S801817235-6He W, Goodkind D, Kowal P (2016) U.S. Census Bureau, International Population Reports, P95/16-1, An Aging World: 2015. U.S. Government Publishing Office, Washington, DCBousquet J, Kuh D, Bewick M, Standberg T, Farrell J, Pengelly R, Joel ME, Rodriguez Mañas L, Mercier J, Bringer J, Camuzat T, Bourret R, Bedbrook A, Kowalski ML, Samolinski B, Bonini S, Brayne C, Michel JP, Venne J, Viriot-Durandal P, Alonso J, Avignon A, Ben-Shlomo Y, Bousquet PJ, Combe B, Cooper R, Hardy R, Iaccarino G, Keil T, Kesse-Guyot E, Momas I, Ritchie K, Robine JM, Thijs C, Tischer C, Vellas B, Zaidi A, Alonso F, Andersen Ranberg K, Andreeva V, Ankri J, Arnavielhe S, Arshad H, Augé P, Berr C, Bertone P, Blain H, Blasimme A, Buijs GJ, Caimmi D, Carriazo A, Cesario A, Coletta J, Cosco T, Criton M, Cuisinier F, Demoly P, Fernandez-Nocelo S, Fougère B, Garcia-Aymerich J, Goldberg M, Guldemond N, Gutter Z, Harman D, Hendry A, Heve D, Illario M, Jeande C, Krauss-Etschmann S, Krys O, Kula D, Laune D, Lehmann S, Maier D, Malva J, Matignon P, Melen E, Mercier G, Moda G, Nizinkska A, Nogues M, O’Neill M, Pelissier JY, Poethig D, Porta D, Postma D, Puisieux F, Richards M, Robalo-Cordeiro C, Romano V, Roubille F, Schulz H, Scott A, Senesse P, Slagter S, Smit HA, Somekh D, Stafford M, Suanzes J, Todo-Bom A, Touchon J, Traver-Salcedo V, van Beurden M, Varraso R, Vergara I, Villalba-Mora E, Wilson N, Wouters E, Zins M (2015) Operational definition of active and healthy ageing (AHA): a conceptual framework. J Nutr Health Aging 19(9):955–960Yacchirema DC, Sarabia-Jácome D, Palau CE, Esteve M (2018) A Smart System for sleep monitoring by integrating IoT with big data analytics. IEEE Access, p 1Robie K (2010) Falls in older people: risk factors and strategies for prevention. JAMA 304(17):1958–1959Jrad RBN, Ahmed MD, Sundaram D (2014) Insider Action Design Research a multi-methodological Information Systems research approach. 2014 IEEE Eighth International Conference on Research Challenges in Information Science (RCIS). Marrakech, pp 1–12. https://doi.org/10.1109/RCIS.2014.6861053Chaccour K, Darazi R, El Hassani AH, Andrès E (2017) From fall detection to fall prevention: a generic classification of fall-related systems. IEEE Sensors J 17(3):812–822Min W, Cui H, Rao H, Li Z, Yao L (2018) Detection of human falls on furniture using scene analysis based on deep learning and activity characteristics. IEEE Access 6:9324–9335Ma X, Wang H, Xue B, Zhou M, Ji B, Li Y (2014) Depth-based human fall detection via shape features and improved extreme learning machine. IEEE J Biomed Heal Inform 18(6):1915–1922Yang L, Ren Y, Zhang W (2016) 3D depth image analysis for indoor fall detection of elderly people. Digit Commun Netw 2(1):24–34Mastorakis G, Makris D (2014) Fall detection system using Kinect’s infrared sensor. J Real-Time Image Process 9(4):635–646Kwolek B, Kepski M (2014) Human fall detection on embedded platform using depth maps and wireless accelerometer. Comput Methods Prog Biomed 117(3):489–501Wang Y, Wu K, Ni LM (2017) WiFall: device-free fall detection by wireless networks. IEEE Trans Mob Comput 16(2):581–594Sehairi K, Chouireb F, Meunier J (2018) Elderly fall detection system based on multiple shape features and motion analysis. 2018 International Conference on Intelligent Systems and Computer Vision (ISCV). Fez, pp 1–8. https://doi.org/10.1109/ISACV.2018.8354084Álvarez de la Concepción MÁ, Soria Morillo LM, Álvarez García JA, González-Abril L (2017) Mobile activity recognition and fall detection system for elderly people using Ameva algorithm. Pervasive Mob Comput 34:3–13Fortino G, Gravina R (2015) Fall-MobileGuard: a smart real-time fall detection system. In: Proceedings of the 10th EAI International Conference on Body Area Networks (BodyNets '15). ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering). ICST, Brussels, Belgium, pp 44–50. https://doi.org/10.4108/eai.28-9-2015.2261462Aguiar B, Rocha T, Silva J, Sousa I (2014) Accelerometer-based fall detection for smartphones. 2014 IEEE International Symposium on Medical Measurements and Applications (MeMeA). Lisboa, pp 1–6. https://doi.org/10.1109/MeMeA.2014.6860110Kau L, Chen C (2015) A smart phone-based pocket fall accident detection, positioning, and rescue system. IEEE J Biomed Heal Inform 19(1):44–56He J, Bai S, Wang X (2017) An Unobtrusive Fall Detection and Alerting System Based on Kalman Filter and Bayes Network Classifier. Sensors 17:1393. https://doi.org/10.3390/s17061393Santoyo-Ramón JA, Casilari E, Cano-García JM (2018) Analysis of a Smartphone-Based Architecture with Multiple Mobility Sensors for Fall Detection with Supervised Learning. Sensors 18:1155. https://doi.org/10.3390/s18041155Mao A, Ma X, He Y, Luo J (2017) Highly Portable, Sensor-Based System for Human Fall Monitoring. Sensors 17:2096. https://doi.org/10.3390/s17092096Casilari E, Oviedo-Jiménez MA (2015) Automatic fall detection system based on the combined use of a smartphone and a smartwatch. PLoS One 10(11):e0140929Dias PVGF, Costa EDM, Tcheou MP, Lovisolo L (2016) Fall detection monitoring system with position detection for elderly at indoor environments under supervision. 2016 8th IEEE Latin-American Conference on Communications (LATINCOM). Medellin, pp. 1–6. https://doi.org/10.1109/LATINCOM.2016.7811576Phu PT, Hai NT, Tam NT (2015) A Threshold Algorithm in a Fall Alert System for Elderly People. In: Toi V, Lien Phuong T (eds) 5th International Conference on Biomedical Engineering in Vietnam. IFMBE Proceedings, vol 46. Springer, Cham. https://doi.org/10.1007/978-3-319-11776-8_85 . ISBN:978-3-319-11775-1Santiago J, Cotto E, Jaimes LG, Vergara-Laurens, I (2017) Fall detection system for the elderly. 2017 IEEE 7th Annual Computing and Communication Workshop and Conference (CCWC). Las Vegas, NV, pp 1–4. https://doi.org/10.1109/CCWC.2017.7868363Malheiros L, Nze GDA, Cardoso LX (2017) Fall detection system and body positioning with heart rate monitoring. IEEE Lat Am Trans 15(6):1021–1026Ethem Alpaydin (2010) Introduction to Machine Learning, 2nd edn. The MIT PressMezghani N, Ouakrim Y, Islam MR, Yared R, Abdulrazak B (2017) Context aware adaptable approach for fall detection bases on smart textile. 2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI). Orlando, FL, pp 473–476. https://doi.org/10.1109/BHI.2017.7897308Pierleoni P, Belli A, Palma L, Pellegrini M, Pernini L, Valenti S (2015) A high reliability wearable device for elderly fall detection. IEEE Sensors J 15(8):4544–4553Aziz O, Musngi M, Park EJ, Mori G, Robinovitch SN (2017) A comparison of accuracy of fall detection algorithms (threshold-based vs. machine learning) using waist-mounted tri-axial accelerometer signals from a comprehensive set of falls and non-fall trials. Med Biol Eng Comput 55(1):45–55Nguyen LP, Saleh M, Le Bouquin Jeannès R (2018) An Efficient Design of a Machine Learning-Based Elderly Fall Detector. In: Ahmed M, Begum S, Fasquel JB (eds) Internet of Things (IoT) Technologies for HealthCare. HealthyIoT 2017. Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering, vol 225. Springer, ChamÖzdemir TA, Barshan B (2014) Detecting falls with wearable sensors using machine learning techniques. Sensors 14(6):10691–10708Tong L, Song Q, Ge Y, Liu M (2013) HMM-based human fall detection and prediction method using tri-axial accelerometer. IEEE Sensors J 13(5):1849–1856SISTEMIC: Research group on Embedded Systems and Computational Intelligence of the Electronics and Telecommunications Department at the Faculty of Engineering, University of Antioquia, “SisFall Dataset.” Online. Available: http://sistemic.udea.edu.co/investigacion/proyectos/english-falls/?lang=en . Accessed 2 Feb 2018Rubenstein L (2006) Falls in older people: epidemiology. Risk Factors and Strategies for Prev 35(Suppl 2):ii37–ii41Youn J, Okuma Y, Hwang M, Kim D, Cho JW (2017) Falling direction can predict the mechanism of recurrent falls in advanced Parkinson’s disease. Sci Rep 7(1):3921Nevitt S, Cummings MC (2018) Type of fall and risk of hip and wrist fractures: The study of osteoporotic fractures. J Am Geriatr Soc 41(11):1226–1234Karantonis DM, Narayanan MR, Mathie M, Lovell NH, Celler BG (2006) Implementation of a real-time human movement classifier using a triaxial accelerometer for ambulatory monitoring. IEEE Trans Inf Technol Biomed 10(1):156–167Khan AM, Lee YK, Kim TS (2008) Accelerometer signal-based human activity recognition using augmented autoregressive model coefficients and artificial neural nets in 2008 30th Annual International. Conf Proc IEEE Eng Med Biol Soc 2008:5172–5175Yoshida T, Mizuno F, Hayasaka T, Tsubota K, Wada S, Yamaguchi T (2005) A wearable computer system for a detection and prevention of elderly users from falling. In: Proceedings of the 12th international conference on biomedical engineering. Singapore, pp 179–182Kangas M, Konttila A, Winblad I, Jamsa T (2007) Determination of simple thresholds for accelerometry based parameters for fall detection. In: 2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. Lyon (France), pp 1367–1370. https://doi.org/10.1109/IEMBS.2007.4352552 . E- ISSN: 1558-4615Shan S, Yuan T (2010) A wearable pre-impact fall detector using feature selection and Support Vector Machine. In: IEEE 10th International Conference on Signal Processing Proceedings. Beijin (China), pp 1686–1689. https://doi.org/10.1109/ICOSP.2010.5656840 . E- ISSN: 2164-523XLombardi A, Ferri M, Rescio G, Grassi M, Malcovati P (2009) Wearable wireless accelerometer with embedded fall-detection logic for multi-sensor ambient assisted living applications. In: 2009 IEEE Sensors. Christchurch (New Zealand), pp. 1967–1970. https://doi.org/10.1109/ICSENS.2009.5398327 . E- ISSN: 1930-0395Aziz O, Klenk J, Schwickert L, Chiari L, Becker C, Park EJ, Mori G, Robinovitch SN (2017) Validation of accuracy of SVM-based fall detection system using real-world fall and non-fall datasets. PLoS One 12(7):e0180318Wang K, Delbaere K, Brodie MAD, Lovell NH, Kark L, Lord SR, Redmond SJ (2017) Differences between gait on stairs and flat surfaces in relation to fall risk and future falls. IEEE J Biomed Heal Inform 21(6):1479–1486Lindholm B, Hagell P, Hansson O, Nilsson MH (2015) Prediction of falls and/or near falls in people with mild Parkinson’s disease. PLoS One 10(1):e0117018Fan Y, Levine MD, Wen G, Qiu S (2017) A deep neural network for real-time detection of falling humans in naturally occurring scenes. Neurocomputing 260:43–58Jokanovic B, Amin M, Ahmad F (2016) Radar fall motion detection using deep learning. In: 2016 IEEE Radar Conference (RadarConf). Philadelphia (USA), pp 1–6. https://doi.org/10.1109/RADAR.2016.7485147 . E- ISSN: 2375-5318Jankowski S, Szymański Z, Dziomin U, Mazurek P, Wagner J (2015) Deep learning classifier for fall detection based on IR distance sensor data. In: 2015 IEEE 8th International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS), vol 2. Warsar (Polonia), pp. 723–727. https://doi.org/10.1109/IDAACS.2015.7341398Jokanović B, Amin M (2018) Fall detection using deep learning in range-Doppler radars. IEEE Trans Aerosp Electron Syst 54(1):180–189Shojaei-Hashemi A, Nasiopoulos P, Little JJ, Pourazad MT (2018) Video-based Human Fall Detection in Smart Homes Using Deep Learning. In: 2018 IEEE International Symposium on Circuits and Systems (ISCAS). Florence (Italy), pp 1–5. https://doi.org/10.1109/ISCAS.2018.8351648 . E- ISSN: 2379-447XLeu F-Y, Ko C-Y, Lin Y-C, Susanto H, Yu H-C (2017) Chapter 10 - Fall Detection and Motion Classification by Using Decision Tree on Mobile Phone. In: Xhafa F, Leu F-Y, Hung L-LBT-SSN (eds) Intelligent Data-Centric Systems Book. Academic Press, pp 205–237. https://doi.org/10.1016/B978-0-12-809859-2.00013-9Yacchirema D, Suárez de Puga J, Palau C, Esteve M (2018) Fall detection system for elderly people using IoT and Big Data. In: 9th International Conference on Ambient Systems, Networks and Technologies (ANT 2018), Porto (Portugal), available at Procedia Computer Science, vol 130, pp 603–610. https://doi.org/10.1016/j.procs.2018.04.110 E-ISSN:1877-0509Rougier C, Meunier J, St-Arnaud A, Rousseau J (2011) Robust video surveillance for fall detection based on human shape deformation. IEEE Trans Circuits Syst Video Technol 21(5):611–622Stone EE, Skubic M (2015) Fall detection in homes of older adults using the Microsoft Kinect. IEEE J Biomed Heal Inform 19(1):290–301Yuwono M, Moulton BD, Su SW, Celler BG, Nguyen HT (2012) Unsupervised machine-learning method for improving the performance of ambulatory fall-detection systems. Biomed Eng Online 11(1):9Friedman J, Hastie T, Tibshirani R (2001) The elements of statistical learning, vol. 1, no. 10. Springer series in statistics New York, NY, USA. https://doi.org/10.1007/b94608 . E-ISBN: 9780387848587Zhang C, Ma Y (2012) Ensemble machine learning: Methods and applications. Springer-Verlag New York, NY. https://doi.org/10.1007/978-1-4419-9326-7 . E-ISBN 978-1-4419-9326-7Big ML (2017) Inc. US “Comprehensive Machine Learning Platform”. Online. Available: https://bigml.com/features . Accessed 12 Aug 2018Ling CX, Huang J, Zhang H et al (2003) AUC: a statistically consistent and more discriminating measure than accuracy. In: 18th Int'l Joint Conf. Artificial Intelligence (IJCAI), Acapulco (mexico), vol 3, pp 519–524. ISBN:0-7695-2728-0Dai J, Bai X, Yang Z, Shen Z, Xuan D (2010) PerFallD: A pervasive fall detection system using mobile phones. In: 2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops). Mannheim (Germany), pp 292–297. https://doi.org/10.1109/PERCOMW.2010.5470652 . E- ISBN: 978-1-4244-6606-1Li Y, Ho KC, Popescu M (2012) A microphone array system for automatic fall detection. IEEE Trans Biomed Eng 59(5):1291–1301Fawcett T (2006) An introduction to ROC analysis. Pattern Recogn Lett 27(8):861–874Pease SG, Trueman R, Davies C, Grosberg J, Yau KH, Kaur N, Conway P, West A (2018) An intelligent real-time cyber-physical toolset for energy and process prediction and optimisation in the future industrial Internet of Things. Futur Gener Comput Syst 79(Part 3):815–829Breiman L (1996) Bagging predictors. Mach Learn 24(2):123–140Hanke S, Mayer C, Hoeftberger O, Boos H, Wichert R, Tazari M-R, Wolf P, Furfari F (2011) universAAL -- An Open and Consolidated AAL Platform. In: Wichert R, Eberhardt B (eds) Ambient Assisted Living: 4. AAL-Kongress 2011, Berlin, Germany, January 25–26, 2011. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 127–140. https://doi.org/10.1007/978-3-642-18167-2_10 . E-ISBN: 978-3-642-18167-2Gjoreski H, Lustrek M, Gams M (2011) Accelerometer Placement for Posture Recognition and Fall Detection. In: 2011 Seventh International Conference on Intelligent Environments. Nottingham (UK), pp 47–54. doi: https://doi.org/10.1109/IE.2011.11 . E- ISBN: 978-0-7695-4452-6Parker C (2011) An Analysis of Performance Measures for Binary Classifiers. In: 2011 IEEE 11th International Conference on Data Mining, Vancouver (Canada), pp 517–526. doi: https://doi.org/10.1109/ICDM.2011.21 . E- ISSN: 2374-8486Han J, Kamber M, Pei J (2012) Data Mining Concepts and Techniques, Third Edit. Morgan Kaufmann Publishers in The Morgan Kaufmann Series in Data Management Systems. Waltham (USA). E-ISBN: 978012381480",,'Springer Science and Business Media LLC',Fall detection system for elderly people using IoT and ensemble machine learning algorithm,10.1007/s00779-018-01196-8,http://hdl.handle.net/10251/161870,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
275622154,2006-01-01T00:00:00,"[EN] In this paper, a new CBR system for Technology Management Centers is presented. The system helps the staff of the centers to solve customer problems by finding solutions successfully applied to similar problems experienced in the past. This improves the satisfaction of customers and ensures a good reputation for the company who manages the center and thus, it may increase its profits. The CBR system is portable, flexible and multi-domain. It is implemented as a module of a help-desk application to make the CBR system as independent as
possible of any change in the help-desk. Each phase of the reasoning cycle is implemented as a series of configurable plugins, making the CBR module easy to update and maintain. This system has been introduced and tested in a real Technology Management center ran by the Spanish company TISSAT S.A.Financial support from Spanish government under grant PROFIT FIT-340001-2004-11 is gratefully acknowledgeHeras Barberá, SM.; Garcia Pardo Gimenez De Los Galanes, JA.; Ramos-Garijo Font De Mora, R.; Palomares Chust, A.; Julian Inglada, VJ.; Rebollo Pedruelo, M.; Botti, V. (2006). CBR model for the intelligent management of customer support centers. En Lecture Notes in Computer Science. Springer Verlag (Germany). 663-670. https://doi.org/10.1007/11875581_80S663670Acorn, T., Walden, S.: SMART: SupportManagement Automated Reasoning Technology for Compaq Customer Service. In: Scott, A., Klahr, P. (eds.) Proceedings of the 2 International Conference on Intelligent Tutoring Systems, ITS-92 Berlin, vol. 4, pp. 3–18. AAAI Press, Menlo Park (1992)Simoudis, E.: Using Case-Based Retrieval for Customer Technical Support. IEEE Intelligent Systems 7, 10–12 (1992)Kriegsman, M., Barletta, R.: Building a Case-Based Help Desk Application. IEEE Expert: Intelligent Systems and Their Applications 8, 18–26 (1993)Shimazu, H., Shibata, A., Nihei, K.: Case-Based Retrieval Interface Adapted to Customer-Initiated Dialogues in Help Desk Operations. In: Mylopoulos, J., Reiter, R. (eds.) Proceedings of the 12th National Conference on Artificial Intelligence, vol. 1, pp. 513–518. AAAI Press, Menlo Park (1994)Raman, R., Chang, K.H., Carlisle, W.H., Cross, J.H.: A self-improving helpdesk service system using case-based reasoning techniques. Computers in Industry 2, 113–125 (1996)Kang, B.H., Yoshida, K., Motoda, H., Compton, P.: Help Desk System with Intelligent Interface. Applied Artificial Intelligence 11, 611–631 (1997)Roth-Berghofer, T., Iglezakis, I.: Developing an Integrated Multilevel Help-Desk Support System. In: Proceedings of the 8th German Workshop on Case-Based Reasoning, pp. 145–155 (2000)Goker, M., Roth-Berghofer, T.: The development and utilization of the case-based help-desk support system HOMER. Engineering Applications of Artificial Intelligence 12, 665–680 (1999)Roth-Berghofer, T.R.: Learning from HOMER, a case-based help-desk support system. In: Melnik, G., Holz, H. (eds.) Advances in Learning Software Organizations, pp. 88–97. Springer, Heidelberg (2004)Bergmann, R., Althoff, K.D., Breen, S., Göker, M., Manago, M., Traphöner, R., Wess, S.: Developing Industrial Case-Based Reasoning Applications. In: The INRECA Methodology, 2nd edn. LNCS (LNAI), vol. 1612. Springer, Heidelberg (2003)eGain (2006), http://www.egain.comKaidara Software Corporation (2006), http://www.kaidara.com/Empolis Knowledge Management GmbH - Arvato AG (2006), http://www.empolis.com/Althoff, K.D., Auriol, E., Barletta, R., Manago, M.: A Review of Industrial Case-Based Reasoning Tools. AI Perspectives Report. Goodall, A., Oxford (1995)Watson, I.: Applying Case-Based Reasoning. Techniques for Enterprise Systems. Morgan Kaufmann Publishers, Inc. California (1997)empolis: empolis Orenge Technology Whitepaper. Technical report, empolis GmbH (2002)Tissat, S.A. (2006), http://www.tissat.esGiraud-Carrier, C., Martinez, T.R.: An integrated framework for learning and reasoning. Journal of Artificial Intelligence Research 3, 147–185 (1995)Corchado, J.M., Borrajo, M.L., Pellicer, M.A., Yanez, J.C.: Neuro-symbolic system for Business Internal Control. In: Perner, P. (ed.) ICDM 2004. LNCS (LNAI), vol. 3275, pp. 1–10. Springer, Heidelberg (2004)Aamodt, A., Plaza, E.: Case-based reasoning: foundational issues, methodological variations and system approaches. AI Communications 7(1), 39–59 (1994)Tversky, A.: Features of similarity. Psychological Review 84(4), 327–352 (1997",,'Springer Science and Business Media LLC',CBR model for the intelligent management of customer support centers,10.1007/11875581_80,https://riunet.upv.es/bitstream/10251/78887/3/ideal06.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
301258141,2018-01-01T00:00:00,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.Peer Reviewe","[{'title': 'Information Sciences', 'identifiers': ['issn:0020-0255', '0020-0255']}]",'Elsevier BV',Incorporating negative information to process discovery of complex systems,10.1016/j.ins.2017.09.027,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226908814,2019-07-08T00:00:00,"International audienceManufacturing companies are under a constant pressure due to multiple factors: new competition, disruptive innovations, cost reduction request, etc. To survive, they must strive to innovate and adapt their business model to improve their productivity. Recent developments based on the concept of Industry 4.0 such as big data, new communication protocols and artificial intelligence provide several new avenues to explore. In the specific context of machining, we are working toward the development of a system capable of making the prognostic of the quality (in terms of dimensional conformance) of a workpiece in real time while it is being manufactured. The goal of this paper is to showcase a prototype of the data acquisition aspect of this system and a case study presenting our first results. This case study has been conducted at our industrial partner facility (Quebec, Canada) and is based on the manufacturing of an aircraft component made from Inconel alloy 625 (AMS5666). The proposed prototype is a data acquisition system installed on a 5 axis CNC machines (GROB model G352) used to acquire and to contextualize the vibration signal obtained from the CNC machine sensor. The contextualization of the data is a key component for future work regarding the development of a prognostic system based on supervised machine learning algorithms. In the end, this paper depicts the system architecture as well as its interactions between the multiple systems and software already in place at our industrial partner. This paper also shows preliminary results describing the relationship between the workpiece quality (in terms of respect toward the dimensional requirements) and the extracted features from the sensors signals. We conclude that it is now possible to do the diagnostic of a cutting operation. Additionally, with the same information we show that it is possible to quickly do the general diagnostic of the health state of the machine. Future work regarding this project will include data acquisition from a wider range of products (i.e. different shapes, materials, processes, etc.) and the development of a machine learning based prognostic model",,HAL CCSD,Toward the quality prognostic of an aircraft engine workpiece in Inconel Alloy 625: case study and proposed system architecture,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
200859620,2019-05-01T00:00:00,"This paper presents a novel diagonal recurrent neural network hybrid controller based on the shared memory of real-time database structure. The controller uses Data Engine (DE) technology, through the establishment of a unified and standardized software architecture and real-time database in different control stations, effectively solves many problems caused by technical standard, communication protocol, and programming language in actual industrial application: the advanced control algorithm and control system co-debugging difficulties, algorithm implementation and update inefficiency, and high development and operation and maintenance costs effectively fill the current technical gap. More importantly, the control algorithm development uses a unified visual graphics configuration programming environment, effectively solving the problem of integrated control of heterogeneous devices; and has the advantages of intuitive configuration and transparent data processing process, reducing the difficulty of the advanced control algorithms debugging in engineering applications. In this paper, the application of a neural network hybrid controller based on DE in motor speed measurement and control system shows that the system has excellent control characteristics and anti-disturbance ability, and provides an integrated method for neural network control algorithm in a practical industrial control system, which is the major contribution of this article","[{'title': 'Algorithms', 'identifiers': ['issn:1999-4893', '1999-4893']}]",'MDPI AG',A New Method of Applying Data Engine Technology to Realize Neural Network Control,10.3390/a12050097,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
390037696,2019-09-11T00:00:00,"The article deals with the prerequisites and generalizes approaches to defining the concept of “creative economy”; on the theoretical basis and practices of creative economy in leading countries of the world an internals and the main principles of its development are identified; genesis of the concept of creative economy is considered; goals, policies, implementation practices in countries around the world are systematized. The position of Ukraine and other countries of the world in the ranking of the global index of creativity are characterized; features of the creative industry in Ukraine are revealed; index of activity of the creative industry of Ukraine is calculated, which indicates a stable tendency of its development.  Potentials of digital technologies, in particular, artificial intelligence, augmented and virtual reality, blockchain technology, to the transformation of creative economy are grounded.The article deals with the prerequisites and generalizes approaches to defining the concept of “creative economy”; on the theoretical basis and practices of creative economy in leading countries of the world an internals and the main principles of its development are identified; genesis of the concept of creative economy is considered; goals, policies, implementation practices in countries around the world are systematized. The position of Ukraine and other countries of the world in the ranking of the global index of creativity are characterized; features of the creative industry in Ukraine are revealed; index of activity of the creative industry of Ukraine is calculated, which indicates a stable tendency of its development.  Potentials of digital technologies, in particular, artificial intelligence, augmented and virtual reality, blockchain technology, to the transformation of creative economy are grounded",,Черкаський навчально-науковий інститут Університету банківської справи,The theory and practice of creative economy in the conditions of digitalization,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
297633323,,"Gate location is one of the most important design elements of an injection mould.
Injection moulding is a very complex process with several parameters having interactive
effects on each otherhence, computer-aided engineering and artificial intelligence were
utilised to optimise mould design based on weld line. Therefore, the finite element
analysis, artificial neural network and genetic algorithm were linked to find optimum gate
location in a plastic product. A reliable numerical model was developed by Moldflow
software based on a real product to simulate injection process. Moldflow was used to
predict weld line length and position of the part. Weld lines are visually undesirable and
a plastic part is structurally weak at weld line positions. This study describes how the
weld line was formed on the part. Polyamide-6 (PA-66) was used as the plastic material.
To find optimum gate location, the finite element predictions were implemented to train
a neural network, which was used later in the genetic algorithm. The optimisation
objective was to find gate location which led to minimum weld line length. This research
concluded that the developed procedure can efficiently optimise complex manufacturing
processes and prevent flaws in products and thus, can be applied practically in the
injection moulding process",,"International Journal of Automotive and Mechanical Engineering, Universiti Malaysia Pahang Publishing (UMP Publisher)","Optimisation of gate location based on weld line in plastic injection moulding using computer-aided engineering, artificial neural network, and genetic algorithm",,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
427084917,2013-09-13T00:00:00,"In  many  industrial  plants,  some  key  variables  cannot  always  be measured on-line and for the purpose of control, an alternative of sensing system is  required.  This  paper  is  concerned  with  a  development  of  an  alternative intelligent  control  strategy,  which  is  an  integration  between  the  neuro-fuzzy based  controller  and  virtual  sensing  system.  This  allows  an  immeasurable variable to be inferred and used for control. The  virtual sensor is  composed of the  Diagonal  Recurrent  Neural  Network  (DRNN)  for  plant  modeling  and  the Extended  Kalman  Filter  (EKF)  as  the  estimator  with  inputs  from  DRNN.  The integration between virtual sensor and the controller enables a development of an on-line  control  scheme  involving  the  immeasurable  variable.  The  real -time implementation  demonstrates  the  applicability  and  the  performance  of  the proposed  intelligent  control  scheme,  especially  in  dealing  with  nonlinear processes",,'The Institute for Research and Community Services (LPPM) ITB',Development of Intelligent Controller with Virtual Sensing,10.5614/itbj.eng.sci.2009.41.1.2,https://core.ac.uk/download/427084917.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
391271841,2017-09-18T00:00:00,"Conference of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2017 ; Conference Date: 18 September 2017 Through 22 September 2017; Conference Code:209269International audienceWe present WHODID: a turnkey intuitive web-based interface for fault detection, identification and diagnosis in production units. Fault detection and identification is an extremely useful feature and is becoming a necessity in modern production units. Moreover, the large deployment of sensors within the stations of a production line has enabled the close monitoring of products being manufactured. In this context, there is a high demand for computer intelligence able to detect and isolate faults inside production lines, and to additionally provide a diagnosis for maintenance on the identified faulty production device, with the purpose of preventing subsequent faults caused by the diagnosed faulty device behavior. We thus introduce a system which has fault detection, isolation, and identification features, for retrospective and on-the-fly monitoring and maintenance of complex dynamical production processes. It provides real-time answers to the questions: "" is there a fault? "" , "" where did it happen? "" , "" for what reason? "". The method is based on a posteriori analysis of decision sequences in XGBoost tree models, using recurrent neural networks sequential models of tree paths. The particularity of the presented system is that it is robust to missing or faulty sensor measurements, it does not require any modeling of the underlying, possibly exogenous manufacturing process, and provides fault diagnosis along with confidence level in plain English formulations. The latter can be used as maintenance directions by a human operator in charge of production monitoring and control",,'Springer Science and Business Media LLC',"WHODID: Web-based interface for Human-assisted factory Operations in fault Detection, Identification and Diagnosis",10.1007/978-3-319-71273-4_47,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
323317294,2011-07-10T00:00:00,"[EN] This paper presents the current situation and future perspectives on virgin olive oil elaboration process control. Regarding the current situation, a review of previous research works on the matter is made. Subsequently, the results of a Spain-wide survey are shown in order to show a precise and realistic degree of the process automation in this area. Finally, the authors present, according to their opinion, the future research lines on the olive oil elaboration process control field.[ES] En este trabajo se presenta la situación actual y perspectivas de futuro del control del proceso de elaboración del aceite de oliva virgen. Dentro del estado actual se muestra, por un lado, un análisis de los trabajos previos de investigación que tratan sobre esta problemática. Por otro lado se recogen los resultados de una encuesta realizada a nivel de toda España para conocer, de forma precisa y real, cuál es el grado de automatización actual de dicho proceso. Finalmente se indican cuáles serán, a juicio de los autores y dentro del campo de la automática, las futuras líneas de investigación de este campo.Los autores quieren agradecer la subvencion parcial de esta investigación a través de los proyectos DPI2008-05798/DPI, TEP2009-5363 y UJA 08 16 31.Cano Marchal, P.; Gómez Ortega, J.; Aguilera Puerto, D.; Gámez García, J. (2011). Situación actual y perspectivas futuras del control del proceso de elaboración del aceite de oliva virgen. Revista Iberoamericana de Automática e Informática industrial. 8(3):258-269. https://doi.org/10.1016/j.riai.2011.06.013OJS25826983Agencia para el Aceite de Oliva, 2009. Datos de información del sector del aceite de oliva en españa. http://aao.mapa.es/.Aguilera, D., Ortega, J.G., 2005. Automatización del proceso de extración del aceite de oliva. situación en la provincia de jaen. XXVI Jornadas de Autom ática.Alba, J., 1997. Elaboración de aceite de oliva virgen. In: Barranco, D., Fernández-Escobar, R., Rallo, L. (Eds.), El cultivo del olivo, 2nd Edition. Mundi-Prensa, Madrid.Bordons, C., Cueli, J., 2004. Predictive controller with estimation of mesurable disturbances. application to an olive oil mill. Journal of Process Control, 305-315.Bordons, C., & Núñez-Reyes, A. (2008). Model based predictive control of an olive oil mill. Journal of Food Engineering, 84(1), 1-11. doi:10.1016/j.jfoodeng.2007.04.011Bordons, C., Zafra, M., 2003. An inferential sensor for the olive oil industry. In: European Control Conference (1). Vol. 1. Cambridge Universtity Press.Cert, A., Alba, J., León-Camacho, M., Moreda, W., & Pérez-Camino, M. C. (1996). Effects of Talc Addition and Operating Mode on the Quality and Oxidative Stability of Virgin Olive Oils Obtained by Centrifugation. Journal of Agricultural and Food Chemistry, 44(12), 3930-3934. doi:10.1021/jf9603386Civantos, L., 1998. Obtención del aceite de oliva virgen. Editorial Agrícola Española, S.A. COI,;1; 2009. Datos de producción mundial de aceite de oliva. http://www.internationaloliveoil.org/.Covas, M.-I., Nyyssönen, K., Poulsen, H. E., Kaikkonen, J., Zunft, H.-J. F., … Kiesewetter, H. (2006). The Effect of Polyphenols in Olive Oil on Heart Disease Risk Factors. Annals of Internal Medicine, 145(5), 333. doi:10.7326/0003-4819-145-5-200609050-00006Esposto, S., GianFrancesco, M., Roberto, S., Ibanez, R., Agnese, T., Stefania, U., Maurizio, S., 2008. Monitoring of virgin olive oil volatile compounds evolution during olive malaxation by an array of metal oxide sensors. Food Chemistry 2000.Fuentes, J.M., Nickel, M.N., 2003. Desarrollo tecnológico en la industria de extracción de aceite de oliva: un análisis dinámico. In: Foro Económico y Social de Expoliva. Jaén.Furferi, R., Carfagni, M., Daou, M., 2007. Artificial neural network software for real-time estimation of olive oil qualitative parameters during continuous extraction. Computers and Electronics in Agriculture 55, 115-131.Di Giovacchino, L., Sestili, S., & Di Vincenzo, D. (2002). European Journal of Lipid Science and Technology, 104(9-10), 587-601. doi:10.1002/1438-9312(200210)104:9/103.0.co;2-mHermoso, M., Jiménez, A., Uceda, M., Morales, J., 1999. Automatizaci ón de almazaras. controles experimentales para la caracterización y regulación del proceso de elaboración www.inia.es/gcontrec/pub/970151058524220453:pdf.Jiménez Marquez, A., Aguilera Herrera, M. P., Uceda Ojeda, M., & Beltrán Maza, G. (2009). Neural network as tool for virgin olive oil elaboration process optimization. Journal of Food Engineering, 95(1), 135-141. doi:10.1016/j.jfoodeng.2009.04.021Jiménez, A., Beltrán, G., Aguilera, M., Uceda, M., 2008. A sensor-software based on artificial neural network for the optimization of olive oil elaboration process. Sensors and Actuators B: Chemical 129 (2), 985-990.Marquez, A. J., Díaz, A. M., & Reguera, M. I. P. (2005). Using optical NIR sensor for on-line virgin olive oils characterization. Sensors and Actuators B: Chemical, 107(1), 64-68. doi:10.1016/j.snb.2004.11.103Ortega Nieto, J., 1943. Cartilla de la almazara. Ministerio de Agricultura.Rodríguez-Mendez, M.L., Apetrei, C., de Saja, J.A., 2008. Evaluation of the polyphenolic content of extra virgin olive oils using an array of voltammetric sensors. Condensed Matter Physics 53, 5867-5872.Tripoli, E., Giammanco, M., Tabacchi, G., Di Majo, D., Giammanco, S., & La Guardia, M. (2005). The phenolic compounds of olive oil: structure, biological activity and beneficial effects on human health. Nutrition Research Reviews, 18(1), 98-112. doi:10.1079/nrr200495Uceda, M., Hermoso, M., 1997. Elaboración de aceite de oliva virgen. In: Barranco, D., Fernández-Escobar, R., Rallo, L. (Eds.), El cultivo del olivo, 2nd Edition. Mundi-Prensa, Madrid",,'Elsevier BV',Current situation and future perspectives on virgin olive oil elaboration process control,10.1016/j.riai.2011.06.013,http://hdl.handle.net/10251/144458,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
304117829,2019-01-01T00:00:00,"Deep Learning is increasingly being adopted by industry for computer vision applications running on embedded devices. While Convolutional Neural Networks' accuracy has achieved a mature and remarkable state, inference latency and throughput are a major concern especially when targeting low-cost and low-power embedded platforms. CNNs' inference latency may become a bottleneck for Deep Learning adoption by industry, as it is a crucial specification for many real-time processes. Furthermore, deployment of CNNs across heterogeneous platforms presents major compatibility issues due to vendor-specific technology and acceleration libraries.In this work, we present QS-DNN, a fully automatic search based on Reinforcement Learning which, combined with an inference engine optimizer, efficiently explores through the design space and empirically finds the optimal combinations of libraries and primitives to speed up the inference of CNNs on heterogeneous embedded devices. We show that, an optimized combination can achieve 45x speedup in inference latency on CPU compared to a dependency-free baseline and 2x on average on GPGPU compared to the best vendor library. Further, we demonstrate that, the quality of results and time ""to-solution"" is much better than with Random Search and achieves up to 15x better results for a short-time search",,'Institute of Electrical and Electronics Engineers (IEEE)',Learning to infer: RL-based search for DNN primitive selection on Heterogeneous Embedded Systems,10.23919/DATE.2019.8714959,https://core.ac.uk/download/304117829.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
237690220,2019-01-01T00:00:00,"In the Architecture, Engineering, Construction and Operations (AECO) there is a growing interest in the use of the Building Information Modelling (BIM). Through integration of information and processes in a digital model, BIM can optimise resources along the lifecycle of a physical asset. Despite the potential savings are much higher in the operational phase, BIM is nowadays mostly used in design and construction stages and there are still many barriers hindering its implementation in Facility Management (FM). Its scarce integration with live data, i.e. data that changes at high frequency, can be considered one of its major limitations in FM. The aim of this research is to overcome this limit and prove that buildings or infrastructures operations can benefit from a digital model updated with live data. The scope of the research concerns the optimisation of FM operations. The optimisation of operations can be further enhanced by the use of maintenance smart contracts allowing a better integration between users’ behaviour and maintenance implementation. In this case study research, the Image Recognition (ImR), a type of Artificial Intelligence (AI), has been used to detect users’ movements in an office building, providing real time occupancy data. This data has been stored in a BIM model, employed as single reliable source of information for FM. This integration can enhance maintenance management contracts if the BIM model is coupled with a smart contract. Far from being a comprehensive case study, this research demonstrates how the transition from BIM to the Asset Information Model (AIM) and, finally, to the Digital Twin (i.e. a near-real-time digital clone of a physical asset, of its conditions and processes) is desirable because of the outstanding benefits that have already been measured in other industrial sectors by applying the principles of Industry 4.0",,'WITPRESS LTD.',Office building occupancy monitoring through image recognition sensors,10.2495/SAFE-V9-N4-371-380,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
46916127,2015-01-14,"In modern distribution grids, the task of fault
localization using conventional techniques is increasingly
becoming a challenge due to the rising domination of inverterbased,
volatile, distributed power generation. Since approved
methods from high voltage level such as reactance-based methods
lack accuracy in distribution level topology, alternative
approaches for accurate fault localization are required. Within
the scope of this work, an artificial neural network (ANN) based
solution for the localization of electric faults at distribution level
has been developed, evaluated and implemented on standard
hardware from industrial automation technology i.e. a
programmable logic controller (PLC). A reduced yet
representative model of a distribution grid incorporating a
variety of aspects influencing the accuracy of fault localization
such as distributed generation, ring network topology with open
or closed loop as well as variable fault resistance has been
developed. Current and voltage measurements generated under
various fault conditions have been used for training of an ANN.
Different ANNs have been trained with various network
structures and training algorithms and after thorough analysis
and comparison of their performance, the most suitable networks
have been implemented on hardware and tested in hardware-in-the-
loop configuration. Thereby a real-time simulator suitable for
application testing and rapid prototyping provided process
values of the modeled distribution grid",,,Development of an Artificial Neural Network based Hardware Prototype for Fault Localization in Distribution Grids,10.17877/DE290R-7453,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201458389,2018-01-01T00:00:00,"Indoor and outdoor positioning lets to offer universal location services in industry and academia. Wi-Fi and Global Positioning System (GPS) are the promising technologies for indoor and outdoor positioning, respectively. However, Wi-Fi-based positioning is less accurate due to the vigorous changes of environments and shadowing effects. GPS-based positioning is also characterized by much cost, highly susceptible to the physical layouts of equipment, power-hungry, and sensitive to occlusion. In this paper, we propose a hybrid of support vector machine (SVM) and deep neural network (DNN) to develop scalable and accurate positioning in Wi-Fi-based indoor and outdoor environments. In the positioning processes, we primarily construct real datasets from indoor and outdoor Wi-Fi-based environments. Secondly, we apply linear discriminate analysis (LDA) to construct a projected vector that uses to reduce features without affecting information contents. Thirdly, we construct a model for positioning through the integration of SVM and DNN. Fourthly, we use online datasets from unknown locations and check the missed radio signal strength (RSS) values using the feed-forward neural network (FFNN) algorithm to fill the missed values. Fifthly, we project the online data through an LDA-based projected vector. Finally, we test the positioning accuracies and scalabilities of a model created from a hybrid of SVM and DNN. The whole processes are implemented using Python 3.6 programming language in the TensorFlow framework. The proposed method provides accurate and scalable positioning services in different scenarios. The results also show that our proposed approach can provide scalable positioning, and 100% of the estimation accuracies are with errors less than 1 m and 1.9 m for indoor and outdoor positioning, respectively","[{'title': 'Journal of Sensors', 'identifiers': ['1687-7268', '1687-725x', 'issn:1687-7268', 'issn:1687-725X']}]",'Hindawi Limited',An Indoor and Outdoor Positioning Using a Hybrid of Support Vector Machine and Deep Neural Network Algorithms,10.1155/2018/1253752,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
42337406,2012-01-01T00:00:00,"New embedded predictive control applications call for more efficient ways of solving quadratic programs (QPs) in order to meet demanding real-time, power and cost requirements. A single precision QP-on-a-chip controller is proposed, implemented in a field-programmable gate array (FPGA) with an iterative linear solver at its core. A novel offline scaling procedure is introduced to aid the convergence of the reduced precision solver. The feasibility of the proposed approach is demonstrated with a real-time hardware-in-the-loop (HIL) experimental setup where an ML605 FPGA board controls a nonlinear model of a Boeing 747 aircraft running on a desktop PC through an Ethernet link. Simulations show that the quality of the closed-loop control and accuracy of individual solutions is competitive with a conventional double precision controller solving linear systems using a Riccati recursion.This work was supported by the EPSRC (Grants EP/G031576/1, EP/G030308/1 and EP/I012036/1) and the EU FP7 Project EMBOCON, as well as industrial support from Xilinx, the Mathworks, and the European Space Agency.IFAC Conference on Nonlinear Model Predictive Control 2012 (NMPC'12), Noordwijkerhout, the Netherlands on August 23 - 27, 2012",,IFAC Proceedings Volumes (IFAC-PapersOnline),Predictive control of a Boeing 747 aircraft using an FPGA,,https://core.ac.uk/download/42337406.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
41534236,2005-01-01T08:00:00,"A real-time analogue recurrent neural network (RNN) can extract and learn the unknown dynamics (and features) of a typical control system such as a robot manipulator. The task at hand is a tracking problem in the presence of disturbances. With reference to the tasks assigned to an industrial robot, one important issue is to determine the motion of the joints and the effector of the robot. In order to model robot dynamics we use a neural network that can be implemented in hardware. The synaptic weights are modelled as variable gain cells that can be implemented with a few MOS transistors. The network output signals portray the periodicity and other characteristics of the input signal in unsupervised mode. For the specific purpose of demonstrating the trajectory learning capabilities, a periodic signal with varying characteristics is used. The developed architecture, however, allows for more general learning tasks typical in applications of identification and control. The periodicity of the input signal ensures convergence of the output to a limit cycle. Online versions of the synaptic update can be formulated using simple CMOS circuits. Because the architecture depends on the network generating a stable limit cycle, and consequently a periodic solution which is robust over an interval of parameter uncertainties, we currently place the restriction of a periodic format for the input signals. The simulated network contains interconnected recurrent neurons with continuous-time dynamics. The system emulates random-direction descent of the error as a multidimensional extension to the stochastic approximation. To achieve unsupervised learning in recurrent dynamical systems we propose a synapse circuit which has a very simple structure and is suitable for implementation in VLSI",,"Edith Cowan University, Research Online, Perth, Western Australia",An analogue recurrent neural networks for trajectory learning and other industrial applications,,https://core.ac.uk/download/41534236.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
81027260,2009-01-01T00:00:00,"We describe the implementation and deployment of a software decision support tool for the maintenance planning of gas turbines. The tool is used to plan the maintenance for turbines manufactured and maintained by Siemens Industrial Turbomachinery AB (SIT AB) with the goal to reduce the direct maintenance costs and the often very costly production losses during maintenance downtime. The optimization problem is formally defined, and we argue that feasibility in it is NP-complete. We outline a heuristic algorithm that can quickly solve the problem for practical purposes, and validate the approach on a real-world scenario based on an oil production facility. We also compare the performance of our algorithm with results from using mixed integer linear programming, and discuss the deployment of the application. The experimental results indicate that downtime reductions up to 65% can be achieved, compared to traditional preventive maintenance. In addition, using our tool is expected to improve availability with up to 1% and reduce the number of planned maintenance days with 12%. Compared to a mixed integer programming approach, our algorithm not optimal, but is orders of magnitude faster and produces results which are useful in practice. Our test results and SIT AB’s estimates based on operational use both indicate that significant savings can be achieved by using our software tool, compared to maintenance plans with fixed intervals.Proceedings of the Twenty-First Conference on Innovative Applications of Artificial Intelligence (IAAI'09) published by IEEE Computer SocietyCam",,IEEE Computer Society,A Tool for Gas Turbine Maintenance Scheduling,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
491248240,2019-01-01T00:00:00,"Predictive Maintenance concerns the smart monitoring of machine to avoid possible future failures, since because it is better to intervene before the damage occurs, saving time and money. In this paper, a Predictive Maintenance methodology based on Machine learning approach is presented and it is applied to a real cutting machine, a woodworking machinery in a real industrial group, producing accurate estimations. This kind of strategy is important to deal with maintenance problems given the ever increasing need to reduce downtime and associated costs. The Predictive Maintenance methodology implemented allows dynamical decision rules that have to be considered for maintenance prediction using a combined approach on Azure Machine Learning Studio. The Three models (RF, GBM and XGBM) allowed the accurately predict machine down ever gripped bearing thanks to the pre-processing phases",,'ASME International',An event based machine learning framework for predictive maintenance in industry 4.0,10.1115/DETC2019-97917,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
196646001,2018-01-01T00:00:00,"Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-toend transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%.ISBN för värdpublikation: 978-1-5386-4413-3, 978-1-5386-4414-0</p",,'Institute of Electrical and Electronics Engineers (IEEE)',Real-time Performance Evaluation of LTE for IIoT,10.1109/LCN.2018.8638081,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
265120533,2019-12-13,"International audienceThe use of Reinforcement Learning (RL) is still restricted to simulation or to enhance human-operated systems through recommendations. Real-world environments (e.g. industrial robots or power grids) are generally designed with safety constraints in mind implemented in the shape of valid actions masks or contingency controllers. For example, the range of motion and the angles of the motors of a robot can be limited to physical boundaries. Violating constraints thus results in rejected actions or entering in a safe mode driven by an external controller, making RL agents incapable of learning from their mistakes. In this paper, we propose a simple modification of a state-of-the-art deep RL algorithm (DQN), enabling learning from forbidden actions. To do so, the standard Q-learning update is enhanced with an extra safety loss inspired by structured classification. We empirically show that it reduces the number of hit constraints during the learning phase and accelerates convergence to near-optimal policies compared to using standard DQN. Experiments are done on a Visual Grid World Environment and Text-World domain",,HAL CCSD,"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-Learning From Forbidden Actions",,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
54575012,2012,"This paper presents an application of data-derived approaches for analyzing and monitoring industrial processes. The discussed methods are used in visualizing process measurements, extracting operational information, and designing estimation models for primary process variables otherwise difficult to measure in real-time. Emphasis is given to the modeling of the data with two classical machine learning paradigms; the self-organizing map (SOM) and the multi-layer perceptron (MLP). The effectiveness of the proposed approach is validated on an industrial deethanizer, where the goal is to identify operational modes and most sensitive variables for this full-scale unit, as well as design an inferential model for a critical process variable, the bottom ethane concentration. The study led to the definition of a fully automated monitoring tool to be implemented online in the plant's distributed control system. The results confirmed the potential of the data-derived approach, and based on the analysis, the existing control configuration of the unit could be redefined toward more consistent operations. Because it is general and modular by design, the tool can be easily used for other processes",,'American Chemical Society (ACS)',Data-derived analysis and inference for an industrial deethanizer,10.1021/ie202854b,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201207793,2018-08-01T00:00:00,"Introduction
Current approaches to the development and application of predictive studies is inefficient and difficult to reproduce. Thousands of predictive health algorithms have been developed; however, less than 2\% have been assessed outside their original setting and even fewer have been applied and evaluated in practice.



Objectives and Approach
Objective: To develop a standardized workflow for algorithm development, dissemination and implementation.



Existing predictive analytics workflow and open standards were adapted and expanded for health research and health care settings. The approach was designed to work within multidisciplinary teams and to improve research transparency, reproducibility, quality, efficiency and application. Key components include standardized algorithm description files, documentation and code libraries. All libraries and programming packages, which were created for/with open-source software, can be used for a wide range of statistical and machine learning models. Publicly-available repositories contain the algorithms, validation data, R code and other supporting infrastructure.



Results
Algorithm development involves variable pre-specification and documentation of model variables, followed by creation of data preprocessing code to generate model variables from the study dataset. Preprocessing uses algorithm specification documentation and a function library, building upon and integrating with existing algorithms when possible to preventing code duplication. Models are output as a Predictive Modelling Markup Language (PMML) file, a portable industry standard for describing and scoring predictive models. A separate scoring ""engine"" is used to implement PMML-described algorithms in a range of settings, including algorithm validation at other research institutions. Algorithm applications currently include the Project Big Life (www.projectbiglife.ca) online calculators, population, health services and public health planning uses and an algorithm visualization tool. An API permits use of the calculator engine by other organizations.



Conclusion/Implications
Barriers to the implementation of predictive analytics in real-world settings—such as within electronic medical records or decision aid applications—can be mitigated with well described algorithms that are easy to replicate and implement, especially as access to big health data increases and algorithms become increasingly complex","[{'title': 'International Journal for Population Data Science', 'identifiers': ['issn:2399-4908', '2399-4908']}]",'Swansea University',A Data Science Approach to Predictive Analytic Research and Knowledge Translation,10.23889/ijpds.v3i4.797,https://core.ac.uk/download/201207793.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201489487,2018-11-01T00:00:00,"The present paper reveals the second part of early research results obtained from the simulation of the annual sentiment indicator evolution under the influence of the production in industry, the intramural research and development expenditure, the turnover and volume of sales and the employment. The research uses for the process’ simulation the three dimensional representation of the above indicators.

The main goal of the research is to be able to determine the four indicators hierarchically listed based on the influence of each indicator on the sentiment indicator evolution. The secondary objective is to compare if the previous results of the research, acquired through the use of artificial neural network simulation, determined the same hierarchy and influence of the four. The data used represent the values for the Euro area (19 countries) recorded between 2006 and 2016, provided by the Eurostat – from the European Commission statistical data website. The author considered that the initial 19 countries represent the most influential ones over the economy and, thus, over the economic sentiment evolution. The method uses the real data and MathCad software for the calculus of polynomial functions that governs the graphic representation of economic sentiment trend under the positive evolution of the each of the four influencer indicators. The polynomial function was chosen by the author because of the incipient phase of research and the trends evolutions","[{'title': None, 'identifiers': ['issn:2393-2384', '2393-2384']}]",Alexandru Ioan Cuza University of Iasi,ANALYSIS OF SENTIMENT INDICATOR FOR THE EURO AREA (19 COUNTRIES) UNDER THE INFLUENCE OF FOUR MANAGEMENT INDICATORS USING GRAPHICAL REPRESENTATION,,https://core.ac.uk/download/201489487.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
56861238,2011,"The quality of welding processes is governed by the occurring induced distortions yielding an increase in production costs due to necessary reworking. Especially for more complex specimens, it is difficult to evaluate the optimal configuration of welding sequences in order to minimize the distortion. Even experienced welding operators can solve this task only by trial and error which is time and cost consuming. In modern engineering the application of welding simulation is already known to be able to analyse the heat effects of welding virtually. However, the welding process is governed by complex physical interactions. Thus, recent weld thermal models are based on many simplifications. The state of the art is to apply numerical methods in order to solve the transient heat conduction equat ion. Therefore, it is not possible to use the real process parameters as input for the mathematical model. The model parameters which allow calculating a temperature field that is in best agreement with the experiments cannot be defined directly but inversely by multiple simulations runs. In case of numerical simulation software based on finite discretization schemes this approach is very time consuming and requires expert users. The weld thermal model contains an initial weakness which has to be adapted by finding an optimal set of model parameters. This process of calibration is often done against few experiments. The range of model validity is limited. An extension can be obtained by performing a calibration against multiple experiments. The focus of the paper is to show a combined mode lling technique which provides an efficient solution of the inverse heat conduction problem mentioned above. On the one hand the inverse problem is solved by application of fast weld thermal models which are closed form solutions of the heat conduction equation. In addition, a global optimization algorithm allows an automated calibration of the weld thermal model. This technique is able to provide a temperature field automatically that fits the experimental one with high accuracy within minutes on ordinary office computers. This fast paradigm permits confirming the application of welding simulation in an industrial environment as automotive industry. On the other hand, the initial model weakness is compensated by calibrating the model against multiple experiments. The unknown relationship between model and process parameters is approximated by a neural network. The validity of the model is increased successively and enables to decrease experimental effort. For a test case, it is shown that this approach yields accurate temperature fields within very short amount of time for unknown process parameters as input data to the model contributing to the requirement to construct a substitute system of the real welding process",,,Fast temperature field generation for welding simulation and reduction of experimental effort,10.1007/bf03321324,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226752432,2019-01-01T00:00:00,"This paper reports the development of a manipulation system for electric wires, implemented by means of a commercial gripper installed on an industrial manipulator and equipped with cameras and suitably designed tactile sensors. The purpose of this system is the execution of wire insertion on commercial electromechanical components. The synergy between computer vision and tactile sensing is necessary because, in a real environment, the tight spaces very often prevent the possibility to use the vision system, also when the same task is performed by a human being. A novel technique to speed up the generation of training data sets for convolutional neural networks (CNNs) is proposed. Therefore, this technique is used to train a CNN in order to detect small objects (such as wire terminals). Moreover, aiming to prevent faults during the task and to interact with the environment safely, several machine learning approaches are used to produce an affordable output from the tactile sensor. The proposed approach shows how a cheap sensor embedded with suitable intelligence can provide information comparable to a more expensive force sensor. Note to Practitioners - This paper was motivated by the lack of commercial solution for the automatic cabling of switchgears. Existing approaches to this problem are in some way limited to specific large-scale products or simple layouts. This paper investigated a robust and flexible solution, based on the exploitation of multiple sensors and machine learning algorithms, for wire detection, grasping, and connection. The proposed approach is characterized by simple design and self-tuning capabilities, and it can be easily employed on a wide range of switchgear layouts thanks to the large workspace of the manipulator. Experimental results show that the proposed system is able to achieve a 95% success rate within a realistic admissible region. In the future research, we will integrate the proposed solution with an electromechanical component localization module and a terminal fastening system to evaluate the performance on the real production line",,'Institute of Electrical and Electronics Engineers (IEEE)',Integration of robotic vision and tactile sensing for wire-terminal insertion tasks,10.1109/TASE.2018.2847222,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
442409320,2019-01-01T00:00:00,"Within the strongly regulated avionic engineering field, conventional graphical desktop hardware and software application programming interface (API) cannot be used because they do not conform to the avionic certification standards. We observe the need for better avionic graphical hardware, but system engineers lack system design tools related to graphical hardware. The endorsement of an optimal hardware architecture by estimating the performance of a graphical software, when a stable rendering engine does not yet exist, represents a major challenge. As proven by previous hardware emulation tools, there is also a potential for development cost reduction, by enabling developers to have a first estimation of the performance of its graphical engine early in the development cycle. In this paper, we propose to replace expensive development platforms by predictive software running on a desktop computer. More precisely, we present a system design tool that helps predict the rendering performance of graphical hardware based on the OpenGL Safety Critical API. First, we create nonparametric models of the underlying hardware, with machine learning, by analyzing the instantaneous frames per second (FPS) of the rendering of a synthetic 3D scene and by drawing multiple times with various characteristics that are typically found in synthetic vision applications. The number of characteristic combinations used during this supervised training phase is a subset of all possible combinations, but performance predictions can be arbitrarily extrapolated. To validate our models, we render an industrial scene with characteristic combinations not used during the training phase and we compare the predictions to those real values. We find a median prediction error of less than 4 FPS","[{'title': 'Scientific Programming', 'identifiers': ['1058-9244', 'issn:1058-9244', '1875-919x', 'issn:1875-919X']}]",'Hindawi Limited',Avionics Graphics Hardware Performance Prediction with Machine Learning,10.1155/2019/9195845,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
83005667,2016-01-01T00:00:00,"Purpose: The goal of this paper is to develop a pragmatic system of a production throughput
forecasting system for an automated test operation in a hard drive manufacturing plant. The
accurate forecasting result is necessary for the management team to response to any changes in
the production processes and the resources allocations.
Design/methodology/approach: In this study, we design a production throughput forecasting
system in an automated test operation in hard drive manufacturing plant. The proposed system
consists of three main stages. In the first stage, a mutual information method was adopted for
selecting the relevant inputs into the forecasting model. In the second stage, a generalized
regression neural network (GRNN) was implemented in the forecasting model development
phase. Finally, forecasting accuracy was improved by searching the optimal smoothing parameter
which selected from comparisons result among three optimization algorithms: particle swarm
optimization (PSO), unrestricted search optimization (USO) and interval halving optimization
(IHO).
Findings: The experimental result shows that (1) the developed production throughput
forecasting system using GRNN is able to provide forecasted results close to actual values, and to
projected the future trends of production throughput in an automated hard disk drive test  operation; (2) IHO algorithm performed as appropriate optimization method better than the
other two algorithms. (3) Compared with current forecasting system in manufacturing, the results
show that the proposed system’s performance is superior to the current system in prediction
accuracy and suitable for real-world application.
Originality/value: The production throughput volume is a key performance index of hard disk
drive manufacturing systems that need to be forecast. The production throughput forecasting
result is useful information for management team to respond to any changes in production
processes and resources allocation. However, a practical forecasting system for production
throughput has not been described in detail yet. The experiments were conducted on a real data
set from the final testing operation of hard disk drive manufacturing factory by using Visual Basic
Application on Microsoft Excel© to develop preliminary forecasting system for testing and
verification process. The experimental result shows that the proposed model is superior to the
performance of the current forecasting system","[{'title': 'Journal of Industrial Engineering and Management', 'identifiers': ['issn:2013-0953', '2013-0953']}]",Universitat Politècnica de Catalunya,A production throughput forecasting system in an automated hard disk drive test operation using GRNN,10.3926/jiem.1464,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
41665747,2014-06-06T08:19:20,"In this article, a new and efficient multilayer neural networks learning algorithm is presented. The key concept of this new algorithm is the two-stage implementation of the steepest descendant method. At the first stage, the steepest descendant method is used to search the optimal learning constant eta and momentum term alpha for each weights updating process. At the second stage, the Delta learning rule is then employed to modify the connecting weights in terms of the optimal eta and alpha. Computer simulations show that the proposed new algorithm outmatches other learning algorithms both in converging speed and success rate. On real industrial application, we first apply the new neural network learning algorithm to the identification of a highly nonlinear injection molding barrel system. Experimental results demonstrate that the new algorithm can precisely identify the complicate injection molding barrel system. Further more, a self-tuning PID controller for precise temperature control based on the trained neural network barrel model is developed. Real experiments show that the self-tuning PID controller can precisely control the barrel temperature within +/-0.5 degrees C","[{'title': 'JSME international journal Ser C Dynamics control robotics design and manufacturing', 'identifiers': ['issn:1340-8062', '1340-8062']}]",,Cascade steepest descendant learning algorithm for multilayer feedforward neural network,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
297906931,2019,"Predictive Maintenance concerns the smart monitoring of machine to avoid possible future failures, since because it is better to intervene before the damage occurs, saving time and money. In this paper, a Predictive Maintenance methodology based on Machine learning approach is presented and it is applied to a real cutting machine, a woodworking machinery in a real industrial group, producing accurate estimations. This kind of strategy is important to deal with maintenance problems given the ever increasing need to reduce downtime and associated costs. The Predictive Maintenance methodology implemented allows dynamical decision rules that have to be considered for maintenance prediction using a combined approach on Azure Machine Learning Studio. The Three models (RF, GBM and XGBM) allowed the accurately predict machine down ever gripped bearing thanks to the pre-processing phase",,,An event based machine learning framework for predictive maintenance in industry 4.0,10.1115/DETC2019-97917,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
251177151,2018-01-01T00:00:00,"The discovery of a formal process model from event logs describing real process executions is a challenging problem that has been studied from several angles. Most of the contributions consider the extraction of a model as a one-class supervised learning problem where only a set of process instances is available. Moreover, the majority of techniques cannot generate complex models, a crucial feature in some areas like manufacturing. In this paper we present a fresh look at process discovery where undesired process behaviors can also be taken into account. This feature may be crucial for deriving process models which are
less complex, fitting and precise, but also good on generalizing the right behavior underlying an event log. The technique is based on the theory of convex polyhedra and satisfiability modulo theory (SMT) and can be combined with other process discovery approach as a post processing step to further simplify complex models. We show in detail how to apply the proposed technique in combination with a recent method that uses numerical abstract domains. Experiments performed in a new prototype implementation show the effectiveness of the technique and the ability to be combined with other discovery techniques.status: publishe","[{'title': 'Information Sciences', 'identifiers': ['1872-6291', 'issn:1872-6291', 'issn:0020-0255', '0020-0255']}]",'Elsevier BV',Incorporating negative information to process discovery of complex systems,10.1016/j.ins.2017.09.027,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
201293367,2018-12-01T00:00:00,"Interest of the authors focuses on the process of introduction and test of different types of virtual reality in sphere of mass media. In the framework of the active integration and update of new digital technologies whose development is pushed by technogenic revolution, media industry becomes a kind of platform to test innovative projects which brings the dynamics to development of the civilization based on digital economy. Currently the most prospective innovations are artificial intelligence, virtual reality (VR) and augmented reality (AR). VR considered as a new communicative format introduces fundamental changes to the media system, demands to review radically the approaches of production and consumption of media content and causes conflict situations. That is why theoretical understanding and description of those changes is required and should be based on interdisciplinary approach. The purpose of the article is to highlight and update social aspects of VR implementation, initiate the discussion what is relevant for number of humanitarian scientific studies","[{'title': 'RUDN Journal of Studies in Literature and Journalism', 'identifiers': ['2312-9220', '2312-9247', 'issn:2312-9220', 'issn:2312-9247']}]",'Peoples'' Friendship University of Russia',Virtual and media reality: trends and forecasts of media system evolution,10.22363/2312-9220-2018-23-4-410-421,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
82735414,2012-12-31,"AbstractThis paper presents an overview of Proportional Integral control (PI) and Artificial Intelligent control (AI) algorithms. AI and PI controller are analyzed using Matlab [Simulink] software. The DC motor is an attractive piece of equipment in many industrial applications requiring variable speed and load characteristics due to its ease of controllability. The main objective of this paper illustrates how the speed of the DC motor can be controlled using different controllers. The simulation results demonstrate that the responses of DC motor with an AI control which is Fuzzy Logic Control shows satisfactory well damped control performance. The results shows that Industrial DC Motor model develop using its physical parameters and controlled with an AI controller give better response, it means it can used as a controller to the real time DC Moto",,Published by Elsevier Ltd.,Modelling and Simulation for Industrial DC Motor Using Intelligent Control ,10.1016/j.proeng.2012.07.193,https://core.ac.uk/download/pdf/82735414.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
84966421,2017-01-01T00:00:00,"International audienceWood is a renewable, abundant bio-energy and environment friendly resource. Woody biomass Moisture Content (MC) is a key parameter for controlling the biofuel product qualities and properties. In this paper, we are interested in predicting MC from data. The input impedance of half-wave dipole antenna when buried in the wood pile varies according to the permittivity of wood. Hence, the measurement of reflection coefficient, that gives information about the input impedance, depends directly on the MC of wood. The relationship between the reflection coefficient measurements and the MC is studied. Based upon this relationship, MC predictive models that use machine learning techniques and feature selection methods are proposed. Numerical experiments using real world data show the relevance of the proposed approach that requires a limited computational power. Therefore, a real-time implementation for industrial processes is feasible",,'Elsevier BV',Wood moisture content prediction using feature selection techniques and a kernel method,10.1016/j.neucom.2016.09.005,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
197737176,2015-07-21T08:31:36,"none2The issue of monitoring profiles has been defined as being one of the most promising areas of research in statistical process control. One immediate difficulty is how to characterize a profile. As a matter of fact, the identification of a statistical model may become more difficult than expected, thus representing an obstacle to the introduction of profile monitoring in actual applications. For example, when a profile represents the physical dimensions of a machined surface, as it results in manufacturing applications, measurements data often exhibit complex spatial correlation.
The aim of this work is to explore a different approach for monitoring profiles, which uses the Adaptive Resonance Theory (ART) neural network. The implementation of this neural network is based on a set of profiles which are representative of the process in its natural, or in-control, state.
Throughout the paper, a real case study related to profiles data obtained by a common machining process is used. With reference to the Phase II of profile monitoring, performance of the proposed approach are compared to those of multivariate control charting of the parameters vector. Although the proposed neural network does not produce always outperforming results, it presents comparable performance in several cases. The main advantage presented by the approach is that the model of profile data is “autonomously” derived by the neural network, without requiring any further intervention by the quality practitioner. This feature may create an important bridge between profile monitoring and quality monitoring of several specifications in actual applications.www.enbis.orgPacella M.; Semeraro Q.Pacella, Massimo; Semeraro, Q",,ENBIS,A comparison of neural network and control charting for monitoring profiles in manufacturing processes,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
160450812,2018-01-01T00:00:00,"Industrial Internet of Things (IIoT) is claimed to be a global booster technology for economic development. IIoT brings bulky use-cases with a simple goal of enabling automation, autonomation or just plain digitalization of industrial processes. The abundance of interconnected IoT and CPS generate additional burden on the telecommunication networks, imposing number of challenges to satisfy the key performance requirements. In particular, the QoS metrics related to real-time data exchange for critical machine-to-machine type communication. This paper analyzes a real-world example of IIoT from a QoS perspective, such as remotely operated underground mining vehicle. As part of the performance evaluation, a software tool is developed for estimating the absolute, one-way delay in end-toend transmissions. The measured metric is passed to a machine learning model for one-way delay prediction based on LTE RAN measurements using a commercially available cutting-edge software tool. The achieved results prove the possibility to predict the delay figures using machine learning model with a coefficient of determination up to 90%.ISBN för värdpublikation: 978-1-5386-4413-3, 978-1-5386-4414-0</p",,'Institute of Electrical and Electronics Engineers (IEEE)',Real-time Performance Evaluation of LTE for IIoT,10.1109/LCN.2018.8638081,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
288328958,2019-01-01T00:00:00,"The industry is moving towards maintenance strategies that consider component health, which require extensive collection and analysis of data. Condition monitoring methods that require manual feature extraction and analysis, become infeasible on an industrial scale. Machine learning algorithms can be used to automatically detect and classify faults, however, obtaining sufficient data for training is required for deep learning and other data-driven classification approaches. Data from healthy machine operation is generally available in abundance, while data from representative fault- and operating conditions is limited. This limits both development and deployment of deep learning-based CM systems on an industrial scale. This paper addresses both the challenges of automated analysis and lack of training data. A deep learning classifier architecture utilizing 1-dimensional dilated convolutions is proposed. Dilation of the convolution kernel allows for analysis of raw vibration signals while simultaneously maintaining the receptive field of the classifier enough to capture temporal patterns. The proposed method performs classification in time domain on signal segments of 1 second or shorter. With knowledge of the bearing specification, artificial vibration signals with similar characteristics as an actual bearing fault can be created. In this work, generated fault signals are combined with healthy operational data to obtain training data for a deep classifier. Parameters of the vibration model is chosen as distributions rather than fixed values. By using a range parameters in the vibration model, the classifier learns to recognize temporal features from the training data that generalize to unseen data. The effectiveness of the proposed method is demonstrated by training classifiers on generated data and testing on real signals from faulty bearings at both low and high speed. One dataset containing seeded faults and three run-to-failure tests are used for the demonstration.publishedVersio",,,Simulation-driven Deep Classification of Bearing Faults from Raw Vibration Data,,https://core.ac.uk/download/288328958.pdf,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
390020542,2018-02-27T00:00:00,"The economic-legal aspects of the state and trends of the Internet-based technologies (IP) technology, the place of intellectual property in it are considered. It is shown that the Internet of Things creates conditions for the emergence of a synergetic effect from the combination of possibilities of artificial intelligence, cloud computing, set of sensors, mathematical algorithms for processing large data (Big Data), robotic devices of various purposes, data transmission systems (Internet), which allows to provide various services and perform various work with or without the participation of people. The role of the state in promoting the development of IP, the existing problems and ways of their solution are shown. Many governments in recent years are taking measures to analyze the state of affairs with the introduction of IP technologies, the localization of problems and threats that may or may occur in the future in order to formulate a common strategy for the development of industry for the production of IP technologies and their application in various sectors of the economy and public life. The patent landscape of the IP is analyzed, the most productive companies and inventors of IP are discovered, the dynamics of patenting in the IP environment, the value of patents, patent research problems are shown. The problems of intellectual property protection in the sphere of IP, in particular, copyright, inventions, trademarks, commercial secrets, information security are considered. The intellectual potential and untapped potential of Ukraine in the development of IP technologies are considered. It is concluded that in the widespread use of IP technologies, there is a significant potential for increasing the efficiency of any type of human activity. It concerns the real economy, industry and agriculture, health care, public administration, education, financial turnover, etc. The development of IP technologies is the most powerful stimulating factor in the innovative development of nanotechnologies, microelectronics, semiconductor technologies, microiminating of executive devices, telecommunications, radio technologies, software computing, robotics, and more.Рассмотрены экономико-правовые аспекты состояния и тенденций развития технологий Интернета вещей (ИВ), места в нем интеллектуальной собственности. Показано, что Интернет вещей создает условия для появления синергетического эффекта от сочетания возможностей искусственного интеллекта, облачных вычислений, множества сенсоров, математических алгоритмов обработки больших данных (Big Data), роботизированных устройств различного назначения, систем передачи данных (сети Интернет), что позволяет предоставлять разнообразные услуги и осуществлять различные работы с участием или без участия людей. Показана роль государства в содействии развитию ИВ, существующие проблемы и пути их решения. Правительства многих стран в последнее время принимают меры по анализу состояния дел с внедрением ИВ-технологий, локализации проблем и угроз, имеющих место или могущих возникнуть в будущем, с целью формирования общей стратегии развития промышленности производства технологий ИВ и их применения в различных секторах экономики и общественной жизни. Проанализированы патентный ландшафт ИВ, выявлены наиболее продуктивные компании и изобретатели ИВ, показана динамика патентования в среде ИВ, ценность патентов, проблемы патентного поиска. Рассмотрены проблемы охраны интеллектуальной собственности в сфере ИВ, в частности, авторских прав, изобретений, торговых марок, коммерческой тайны, информационной безопасности. Рассмотрены интеллектуальный потенциал и неиспользованные возможности Украины в развитии технологий ИВ. Делается вывод, что в широком применении технологий ИВ заложен значительный потенциал повышения эффективности любого вида человеческой деятельности. Это касается сферы реальной экономики, промышленности и сельского хозяйства, системы здравоохранения, государственного управления, образования, финансового оборота и т. п. Развитие технологий ИВ является мощным стимулирующим фактором инновационного развития нанотехнологий, микроэлектроники, полупроводниковых технологий, микроминиатюризации исполнительных устройств, телекоммуникаций, радиотехнологий, программных вычислительных средств, робототехники и многого другого.Розглянуто економіко-правові аспекти стану та тенденцій розвитку технологій Інтернету речей (ІР), місця в ньому інтелектуальної власності. Показано роль дер- жави у сприянні розвитку ІР, проблеми та шляхи їх вирішення. Проаналізовано патентний ландшафт ІР, виявлені найбільш продуктивні компанії та винахідники ІР, показано динаміку патентування в середовищі ІР, цінність патентів, проблеми патентного пошуку. Визначено проблеми охорони інтелектуальної власності у сфері ІР, зокрема, авторських прав, винаходів, торгових марок, комерційної таємниці, інформаційної безпеки. Розглянуто інтелектуальний потенціал і невикористані мож- ливості України в розвитку технологій ІР. Обґрунтовано висновок, що в широкому застосуванні технологій ІР закладено значний потенціал підвищення ефективності економіки",,Науково-дослідний інститут інтелектуальної власності НAПрН України,ІНТЕЛЕКТУАЛЬНА ВЛАСНІСТЬ В СИСТЕМІ ІНТЕРНЕТУ РЕЧЕЙ: ЕКОНОМІКО-ПРАВОВИЙ АСПЕКТ,,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
226726445,2018,"The reliability estimation of engineered components is fundamental for many optimization policies in a production process. The main goal of this paper is to study how machine learning models can fit this reliability estimation function in comparison with traditional approaches (e.g., Weibull distribution). We use a supervised machine learning approach to predict this reliability in 19 industrial components obtained from real industries. Particularly, four diverse machine learning approaches are implemented: artificial neural networks, support vector machines, random forest, and soft computing methods. We evaluate if there is one approach that outperforms the others when predicting the reliability of all the components, analyze if machine learning models improve their performance in the presence of censored data, and finally, understand the performance impact when the number of available inputs changes. Our experimental results show the high ability of machine learning to predict the component reliability and particularly, random forest, which generally obtains high accuracy and the best results for all the cases. Experimentation confirms that all the models improve their performance when considering censored data. Finally, we show how machine learning models obtain better prediction results with respect to traditional methods when increasing the size of the time-to-failure datasets",,,On the use of machine learning methods to predict component reliability from data-driven industrial case studies,10.1007/s00170-017-1039-x,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
92169625,2003-01-01T00:00:00,"Mechatronic design is the integrated design of a mechanical system and its embedded control system. In order to make proper choices early in the design stage, tools are required that support modelling and simulation of physical systems––together with the controllers––with parameters that are directly related to the real-world system. Such software tools are becoming available now. Components in various physical domains (e.g. mechanical or electrical) can easily be selected from a library and combined into a ‘process’ that can be controlled by block-diagram-based (digital) controllers. A few examples will be discussed that show the use of such a tool in various stages of the design. The examples include a typical mechatronic system with a flexible transmission, a mobile robot, and an industrial linear motor with a neural-network-based learning feed-forward controller that compensates for cogging",,'Elsevier BV',Mechatronic Design,10.1016/s0957-4158(03)00042-4,,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
275613444,2015-11-18T00:00:00,"12th International Conference on Cooperative Design, Visualization, and Engineering, CDVE 2015; Mallorca; Spain; 20 September 2015 through 23 September 2015The proper transport of hit sensitive products, such as fish and fruit,
is very important because their deterioration may cause the value lost and even
the product rejection by the buyer. For this reason, in this paper we present a
cooperative monitoring system for the delivery of fresh products. The system
consists of fixed wireless nodes and a mobile wireless node that is installed in
the packet. This mobile node is able to take data of the internal temperature,
external temperature and the 3 axis movement. With the information stored in
the network, a vendor can know the optimal conditions of the transport. Finally,
we test the maximum distance to the fixed nodes, as well as the data collected by
the sensor.Sendra, S.; Lloret, J.; Lacuesta, R.; Jimenez, JM. (2015). Cooperative Monitoring of the Delivery of Fresh Products. Lecture Notes in Computer Science. 9320:76-86. doi:10.1007/978-3-319-24132-6_10S76869320Derks, H.G., Buehler, W.S., Hall, M.B.: Real-time method and system for locating a mobile object or person in a tracking environment. US Patent 8514071 B2, 20 August 2013Witmond, R., Dutta, R., Charroppin, P.: Method for tracking a mail piece. US Patent 7003376 B2, 21 February 2006Lu, L., Liu, Y., Han, J.: ACTION: breaking the privacy barrier for RFID systems. Ad Hoc Sens. Wirel. Netw. 24(1–2), 135–159 (2015)Dhakal, S., Shin, S.: Precise time system efficiency of a frame slotted aloha based anti-collision algorithm in a RFID system. Netw. Protoc. Algorithms 5(2), 16–27 (2013)Garcia, M., Bri, D., Sendra, S., Lloret, J.: Practical deployments of wireless sensor networks: a survey. Int. J. Adv. Netw. Serv. 3(1&2), 163–178 (2010)Bri, D., Garcia, M., Lloret, J., Dini, P.: Real deployments of wireless sensor networks. In: Third International Conference on Sensor Technologies and Applications (SENSORCOMM 2009), Athens, Greece, 18–23 June 2009, pp. 415–423Karim, L., Anpalagan, A., Nasser, N., Almhana, J.: Sensor-based M2M agriculture monitoring systems for developing countries: state and challenges. Netw. Protoc. Algorithms 5(3), 68–86 (2013)Garcia, M., Lloret, J., Sendra, S., Rodrigues, J.J.: Taking cooperative decisions in group-based wireless sensor networks. In: Luo, Y. (ed.) CDVE 2011. LNCS, vol. 6874, pp. 61–65. Springer, Heidelberg (2011)Garcia-Sabater, J.P., Lloret, J., Marin-Garcia, J.A., Puig-Bernabeu, X.: Coordinating a cooperative automotive manufacturing network – an agent-based model. In: Luo, Y. (ed.) CDVE 2010. LNCS, vol. 6240, pp. 231–238. Springer, Heidelberg (2010)Li, J., Cao, J.: Survey of object tracking in wireless sensor networks. Netw. Protoc. Algorithms 25(1–2), 89–120 (2015)Vock, C.A., Larkin, A.F., Amsbury, B.W., Youngs, P.: Device for monitoring movement of shipped goods. U.S. Patent 8,280,682, 2 October 2012Jedermann, R., Schouten, R., Sklorz, A., Lang, W., Van Kooten, O.: Linking keeping quality models and sensor systems to an autonomous transport supervision system. In: The 2nd International Workshop Cold Chain Management, Bonn, Germany, 8–9 May 2006, pp. 3–18Ko, D., Kwak, Y., Song, S.: Real time traceability and monitoring system for agricultural products based on wireless sensor network. Int. J. Distrib. Sens. Netw. 2014, 1–7 (2014). Article ID 832510Ruiz-Garcia, L., Barreiro, P., Robla, J.I.: Performance of ZigBee-based wireless sensor nodes for real-time monitoring of fruit logistics. J. Food Eng. 87(3), 405–415 (2008)Shamsuzzoha, A., Addo-Tenkorang, R., Phuong, D., Helo, P.: Logistics tracking: an implementation issue for delivery network. In: PICMET 2011Conference Technology Management in the Energy Smart World, Portland, Oregon, USA, July 31–August 4 2011, pp. 1–10Torres, R.V., Sanchez, J.C., Galan, L.M.: Unmarked point and adjacency vertex, mobility models for the generation of emergency and rescue scenarios in urban areas. Ad Hoc Sens. Wirel. Netw. 23(3–4), 211–233 (2014)Waspmote features. In Digi Web Site. http://www.digi.com/products/wireless-wired-embedded-solutions/zigbee-rf-modules/point-multipoint-rfmodules/xbee-series1-module#specs . Accessed 18 April 2015Meghanathan, N., Mumford, P.: Centralized and distributed algorithms for stability-based data gathering in mobile sensor networks. Netw. Protoc. Algorithms 5(4), 84–116 (2013)Alrajeh, N.A., Khan, S., Lloret, J., Loo, J.: Artificial neural network based detection of energy exhaustion attacks in wireless sensor networks capable of energy harvesting. Ad Hoc Sens. Wirel. Netw. 22(3–4), 109–133 (2014)Garcia, M., Sendra, S., Lloret, J., Canovas, A.: Saving energy and improving communications using cooperative group-based wireless sensor networks. Telecommun. Syst. 52(4), 2489–2502 (2013",,'Springer Science and Business Media LLC',Cooperative Monitoring of the Delivery of Fresh Products,10.1007/978-3-319-24132-6_10,http://hdl.handle.net/10251/64862,core,industry,'industry' AND 'machine learning' AND ('real-world' AND 'deploy')
