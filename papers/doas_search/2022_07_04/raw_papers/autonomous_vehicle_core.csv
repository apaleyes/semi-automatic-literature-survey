id,datePublished,description,journals,publisher,title,doi,downloadUrl,database,query_name,query_value
479360462,2021-06-01T00:00:00,"Introduction. An important part of an automotive unmanned vehicle (UV) control system is the environment analysis module. This module is based on various types of sensors, e.g. video cameras, lidars and radars. The development of computer and video technologies makes it possible to implement an environment analysis module using a single video camera as a sensor. This approach is expected to reduce the cost of the entire module. The main task in video image processing is to analyse the environment as a 3D scene. The 3D trajectory of an object, which takes into account its dimensions, angle of view and movement vector, as well as the vehicle pose in a video image, provides sufficient information for assessing the real interaction of objects. A basis for constructing a 3D trajectory is vehicle pose estimation.Aim. To develop an automatic method for estimating vehicle pose based on video data analysis from a single video camera.Materials and methods. An automatic method for vehicle pose estimation from a video image was proposed based on a cascade approach. The method includes vehicle detection, key points determination, segmentation and vehicle pose estimation. Vehicle detection and determination of its key points were resolved via a neural network. The segmentation of a vehicle video image and its mask preparation were implemented by transforming it into a polar coordinate system and searching for the outer contour using graph theory.Results. The estimation of vehicle pose was implemented by matching the Fourier image of vehicle mask signatures and the templates obtained based on 3D models. The correctness of the obtained vehicle pose and angle of view estimation was confirmed by experiments based on the proposed method. The vehicle pose estimation had an accuracy of 89 % on an open Carvana image dataset.Conclusion. A new approach for vehicle pose estimation was proposed, involving the transition from end-to-end learning of neural networks to resolve several problems at once, e.g., localization, classification, segmentation, and angle of view, towards cascade analysis of information. The accuracy level of end-to-end learning requires large sets of representative data, which complicates the scalability of solutions for road environments in Russia. The proposed method makes it possible to estimate the vehicle pose with a high accuracy level, at the same time as involving no large costs for manual data annotation and training","[{'title': 'Journal of the Russian Universities Radioelectronics', 'identifiers': ['issn:1993-8985', '1993-8985', 'issn:2658-4794', '2658-4794']}]",'St. Petersburg Electrotechnical University LETI',Method for Automatic Determination of a 3D Trajectory of Vehicles in a Video Image,10.32603/1993-8985-2021-24-3-49-59,https://core.ac.uk/download/479360462.pdf,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
441223432,2020-09-01T00:00:00,"The results of the development of hardware and software system (micromodule), which detects and classifies underlying surface images of the Earth are presented. The micromodule can be installed on board of a light unmanned aerial vehicle (drone). The device has the size 5.2×7.4×3.1 cm, the weight52 g, runs on a Raspberry Pi Zero Wireless single-board microcomputer and uses a convolutional neural network based on MobileNetV2 architecture for real-time image classification. When developing the micromodule, the authors aimed to achieve a real-time image classification on inexpensive mobile equipment with low computing power so that the classification quality is  comparable  to  popular  deep  convolutional  network  architectures. The provided information could be useful for engineers and researchers who are developing compact budget mobile systems for processing, analyzing and recognition of images","[{'title': 'Informatics', 'identifiers': ['1816-0301', 'issn:1816-0301']}]",'United Institute of Informatics Problems of the National Academy of Sciences of Belarus',Recognition of underlying surface using a convolutional neural network on a single-board computer,10.37661/1816-0301-2020-17-3-36-43,https://core.ac.uk/download/441223432.pdf,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
286465896,2017-01-01T00:00:00,"Nel rilievo in ambito archeologico, per la presenza di elementi curvi e di parti aggettanti, è spesso necessario l’utilizzo di sistemi di acquisizione che permettano di ottenere una misura celere, dettagliata e di supporto alle tecniche tradizionali. L’utilizzo di sistemi di acquisizione fotogrammetrici da SAPR (Sistemi Aeromobili a Pilotaggio Remoto), unite all’utilizzo di algoritmi di Computer Vision, permettono la realizzazione di modelli tridimensionali ai quali vengono applicate texture foto-realistiche derivanti da immagini fotografiche acquisite in volo. Il caso studio che presentiamo è il rilievo fotogrammetrico del Ponte Rotto ad Apice (Bn). Per il rilievo aerofotogrammetrico è stato utilizzato un UAV (Unmanned Aerial Vehicle), con peso totale al decollo inferiore ai 2 kg (art. 12, Regolamento ENAC del 16 luglio 2015). Le immagini acquisite sono state elaborate con un software con tecnologia Structure From Motion; il modello è georeferito per mezzo di sei GCP (Ground Control Points), misurati con ricevitori GNSS (Global Navigation Satellite Systems) in modalità nRTK (Network Real Time Kinematic)",,ASITA,L’uso di SAPR per la documentazione archeologica: il caso studio del Ponte Rotto ad Apice (Bn),,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
233120453,2019-07-19T00:00:00,"Autonomous vehicle is a vehicle that can guide itself without human conduction. It is capable of sensing its environment and moving with little or no human input. This kind of vehicle has become a concrete reality and may pave the way for future systems where computers take over the art of driving. Advanced artificial intelligence control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant road signs. In this paper, we introduce an intelligent road signs classifier to help autonomous vehicles to recognize and understand road signs. The road signs classifier based on an artificial intelligence technique. In particular, a deep learning model is used, Convolutional Neural Networks (CNN). CNN is a widely used Deep Learning model to solve pattern recognition problems like image classification and object detection. CNN has successfully used to solve computer vision problems because of its methodology in processing images that are similar to the human brain decision making. The evaluation of the proposed pipeline was trained and tested using two different datasets. The proposed CNNs achieved high performance in road sign classification with a validation accuracy of 99.8% and a testing accuracy of 99.6%. The proposed method can be easily implemented for real time application",,'Bilingual Publishing Co.',To Perform Road Signs Recognition for Autonomous Vehicles Using Cascaded Deep Learning Pipeline,10.30564/aia.v1i1.569,https://core.ac.uk/download/233120453.pdf,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
323317411,2019-12-12T00:00:00,"[EN] Over the last few years, several researchers have been developing protocols and applications in order to autonomously land unmanned aerial vehicles (UAVs). However, most of the proposed protocols rely on expensive equipment or do not satisfy the high precision needs of some UAV applications such as package retrieval and delivery or the compact landing of UAV swarms. Therefore, in this work, a solution for high precision landing based on the use of ArUco markers is presented. In the proposed solution, a UAV equipped with a low-cost camera is able to detect ArUco markers sized 56×56 cm from an altitude of up to 30 m. Once the marker is detected, the UAV changes its flight behavior in order to land on the exact position where the marker is located. The proposal was evaluated and validated using both the ArduSim simulation platform and real UAV flights. The results show an average offset of only 11 cm from the target position, which vastly improves the landing accuracy compared to the traditional GPS-based landing, which typically deviates from the intended target by 1 to 3 m.This work was funded by the  Ministerio de Ciencia, Innovación y Universidades, Programa Estatal de Investigación, Desarrollo e Innovación Orientada a los Retos de la Sociedad, Proyectos I+D+I 2018 , Spain, under Grant RTI2018-096384-B-I00.Wubben, J.; Fabra Collado, FJ.; Tavares De Araujo Cesariny Calafate, CM.; Krzeszowski, T.; Márquez Barja, JM.; Cano, J.; Manzoni, P. (2019). Accurate Landing of Unmanned Aerial Vehicles Using Ground Pattern Recognition. Electronics. 8(12):1-16. https://doi.org/10.3390/electronics8121532S116812Pan, X., Ma, D., Jin, L., & Jiang, Z. (2008). Vision-Based Approach Angle and Height Estimation for UAV Landing. 2008 Congress on Image and Signal Processing. doi:10.1109/cisp.2008.78Tang, D., Li, F., Shen, N., & Guo, S. (2011). UAV attitude and position estimation for vision-based landing. Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology. doi:10.1109/emeit.2011.6023131Gautam, A., Sujit, P. B., & Saripalli, S. (2014). A survey of autonomous landing techniques for UAVs. 2014 International Conference on Unmanned Aircraft Systems (ICUAS). doi:10.1109/icuas.2014.6842377Holybro Pixhawk 4 · PX4 v1.9.0 User Guidehttps://docs.px4.io/v1.9.0/en/flight_controller/pixhawk4.htmlGarrido-Jurado, S., Muñoz-Salinas, R., Madrid-Cuevas, F. J., & Medina-Carnicer, R. (2016). Generation of fiducial marker dictionaries using Mixed Integer Linear Programming. Pattern Recognition, 51, 481-491. doi:10.1016/j.patcog.2015.09.023Romero-Ramirez, F. J., Muñoz-Salinas, R., & Medina-Carnicer, R. (2018). Speeded up detection of squared fiducial markers. Image and Vision Computing, 76, 38-47. doi:10.1016/j.imavis.2018.05.004ArUco: Augmented reality library based on OpenCVhttps://sourceforge.net/projects/aruco/Jin, S., Zhang, J., Shen, L., & Li, T. (2016). On-board vision autonomous landing techniques for quadrotor: A survey. 2016 35th Chinese Control Conference (CCC). doi:10.1109/chicc.2016.7554984Chen, X., Phang, S. K., Shan, M., & Chen, B. M. (2016). System integration of a vision-guided UAV for autonomous landing on moving platform. 2016 12th IEEE International Conference on Control and Automation (ICCA). doi:10.1109/icca.2016.7505370Nowak, E., Gupta, K., & Najjaran, H. (2017). Development of a Plug-and-Play Infrared Landing System for Multirotor Unmanned Aerial Vehicles. 2017 14th Conference on Computer and Robot Vision (CRV). doi:10.1109/crv.2017.23Shaker, M., Smith, M. N. R., Yue, S., & Duckett, T. (2010). Vision-Based Landing of a Simulated Unmanned Aerial Vehicle with Fast Reinforcement Learning. 2010 International Conference on Emerging Security Technologies. doi:10.1109/est.2010.14Araar, O., Aouf, N., & Vitanov, I. (2016). Vision Based Autonomous Landing of Multirotor UAV on Moving Platform. Journal of Intelligent & Robotic Systems, 85(2), 369-384. doi:10.1007/s10846-016-0399-zPatruno, C., Nitti, M., Petitti, A., Stella, E., & D’Orazio, T. (2018). A Vision-Based Approach for Unmanned Aerial Vehicle Landing. Journal of Intelligent & Robotic Systems, 95(2), 645-664. doi:10.1007/s10846-018-0933-2Baca, T., Stepan, P., Spurny, V., Hert, D., Penicka, R., Saska, M., … Kumar, V. (2019). Autonomous landing on a moving vehicle with an unmanned aerial vehicle. Journal of Field Robotics, 36(5), 874-891. doi:10.1002/rob.21858De Souza, J. P. C., Marcato, A. L. M., de Aguiar, E. P., Jucá, M. A., & Teixeira, A. M. (2019). Autonomous Landing of UAV Based on Artificial Neural Network Supervised by Fuzzy Logic. Journal of Control, Automation and Electrical Systems, 30(4), 522-531. doi:10.1007/s40313-019-00465-ySITL Simulator (Software in the Loop)http://ardupilot.org/dev/docs/sitl-simulator-software-in-the-loop.htmlFabra, F., Calafate, C. T., Cano, J.-C., & Manzoni, P. (2017). On the impact of inter-UAV communications interference in the 2.4 GHz band. 2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC). doi:10.1109/iwcmc.2017.7986413MAVLink Micro Air Vehicle Communication Protocolhttp://qgroundcontrol.org/mavlink/startFabra, F., Calafate, C. T., Cano, J. C., & Manzoni, P. (2018). ArduSim: Accurate and real-time multicopter simulation. Simulation Modelling Practice and Theory, 87, 170-190. doi:10.1016/j.simpat.2018.06.009Careem, M. A. A., Gomez, J., Saha, D., & Dutta, A. (2019). HiPER-V: A High Precision Radio Frequency Vehicle for Aerial Measurements. 2019 16th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON). doi:10.1109/sahcn.2019.882490",,'MDPI AG',Accurate Landing of Unmanned Aerial Vehicles Using Ground Pattern Recognition,10.3390/electronics8121532,https://riunet.upv.es/bitstream/10251/144317/1/Wubben%3bFABRA%3bTavares%20-%20Accurate%20Landing%20of%20Unmanned%20Aerial%20Vehicles%20Using%20Ground%20Pattern%20Recognit....pdf,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
227320072,2019-10-27T00:00:00,"International audienceReplacing the human driver to perform the Dynamic Driving Task (DDT)[1] will require perception, complex analysis and assessment of traffic situation. The path leading to success the deployment of fully Autonomous Vehicle (AV) depends on the resolution of a lot of challenges. Both the safety and the security aspects of AV constitute the core of regulatory compliance and technical research. The Autonomous Driving System (ADS) should be designed to ensure a safe manoeuvre and a stable behaviour despite the technological limitations, the uncertainties and hazards which characterize the real traffic conditions. In fully Autonomous Driving situation, detecting all relevant objects and agents should be sufficient to generate a warning, however the ADS requires further complex data analysis steps to quantify and improve the safety of decision making. This paper aims to improve the robustness of decision-making in order to mimic human-like decision ability. The approach is based on machine learning to identify the criticality of the dynamic situation and enabling ADS to make appropriate decision and fulfil safe manoeuvre",,HAL CCSD,Machine learning method to ensure robust decision-making of AVs,,https://core.ac.uk/download/227320072.pdf,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
387259411,2021-01-01T00:00:00,"Handling of critical situations is an important part in the architecture of an autonomous vehicle. A controller for autonomous collision avoidance is developed based on a wary strategy that assumes the least tireroad friction for which the maneuver is still feasible. Should the friction be greater, the controller makes use of this and performs better. The controller uses an acceleration-vector reference obtained from optimal control of a friction-limited particle, whose applicability is verified by using numerical optimization on a full vehicle model. By employing an analytical tire model of the tireroad friction limit, to determine slip references for steering and body-slip control, the result is a controller where the computation of its output is explicit and independent of the actual tire-road friction. When evaluated in real-time on a high-fidelity simulation model, the developed controller performs close to that achieved by offline numerical optimization.Funding: Wallenberg AI, Autonomous Systems, and Software Program (WASP) - Knut and AliceWallenberg Foundation</p",,'Institute of Electrical and Electronics Engineers (IEEE)',Autonomous Wary Collision Avoidance,10.1109/TIV.2020.3029853,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
301281319,2017-01-01T00:00:00,"Spatial perception, in which objects’ motion and positional relationship are recognized, is necessary for applications such as a walking robot and an autonomous car. One of the demanding features of spatial perception in real world applications is robustness. Neural network-based approaches, in which perception results are obtained by voting among a large number of neuronal activities, seem to be promising. We focused on a neural network model for motion stereo vision proposed by Kawakami et al. In this model, local motion in each small region of the visual field, which comprises optical flow, is detected by hierarchical neural network. Implementation of this model into a VLSI is required for real-time operation with low power consumption. In this study, we reduced the computational complexity of this model and showed cell responses of the reduced model by numerical simulation.Peer Reviewe",,'Springer Science and Business Media LLC',Complexity reduction of neural network model for local motion detection in motion stereo vision,10.1007/978-3-319-70136-3,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
429089989,2020-12-21T00:00:00,"We present an unsupervised deep learning approach for post-disaster building damage detection that can transfer to different typologies of damage or geographical locations. Previous advances in this direction were limited by insufficient qualitative training data. We propose to use a state-of-the-art Anomaly Detecting Generative Adversarial Network (ADGAN) because it only requires pre-event imagery of buildings in their undamaged state. This approach aids the post-disaster response phase because the model can be developed in the pre-event phase and rapidly deployed in the post-event phase. We used the xBD dataset, containing pre-and post-event satellite imagery of several disaster-types, and a custom made Unmanned Aerial Vehicle (UAV) dataset, containing post-earthquake imagery. Results showed that models trained on UAV-imagery were capable of detecting earthquake-induced damage. The best performing model for European locations obtained a recall, precision and F1-score of 0.59, 0.97 and 0.74, respectively. Models trained on satellite imagery were capable of detecting damage on the condition that the training dataset was void of vegetation and shadows. In this manner, the best performing model for (wild)fire events yielded a recall, precision and F1-score of 0.78, 0.99 and 0.87, respectively. Compared to other supervised and/or multi-epoch approaches, our results are encouraging. Moreover, in addition to image classifications, we show how contextual information can be used to create detailed damage maps without the need of a dedicated multi-task deep learning framework. Finally, we formulate practical guidelines to apply this single-epoch and unsupervised method to real-world applications",,'MDPI AG',Post-Disaster Building Damage Detection from Earth Observation Imagery using Unsupervised and Transferable Anomaly Detecting Generative Adversarial Networks,10.3390/rs12244193,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
387851142,2021-01-01T00:00:00,"Handling of critical situations is an important part in the architecture of an autonomous vehicle. A controller for autonomous collision avoidance is developed based on a wary strategy that assumes the least tireroad friction for which the maneuver is still feasible. Should the friction be greater, the controller makes use of this and performs better. The controller uses an acceleration-vector reference obtained from optimal control of a friction-limited particle, whose applicability is verified by using numerical optimization on a full vehicle model. By employing an analytical tire model of the tireroad friction limit, to determine slip references for steering and body-slip control, the result is a controller where the computation of its output is explicit and independent of the actual tire-road friction. When evaluated in real-time on a high-fidelity simulation model, the developed controller performs close to that achieved by offline numerical optimization.Funding: Wallenberg AI, Autonomous Systems, and Software Program (WASP) - Knut and AliceWallenberg Foundation</p",,'Institute of Electrical and Electronics Engineers (IEEE)',Autonomous Wary Collision Avoidance,10.1109/TIV.2020.3029853,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
475073071,2021-06-01T00:00:00,"An Unmanned Aerial Vehicle (UAV) can greatly reduce manpower in the agricultural plant protection such as watering, sowing, and pesticide spraying. It is essential to develop a Decision-making Support System (DSS) for UAVs to help them choose the correct action in states according to the policy. In an unknown environment, the method of formulating rules for UAVs to help them choose actions is not applicable, and it is a feasible solution to obtain the optimal policy through reinforcement learning. However, experiments show that the existing reinforcement learning algorithms cannot get the optimal policy for a UAV in the agricultural plant protection environment. In this work we propose an improved Q-learning algorithm based on similar state matching, and we prove theoretically that there has a greater probability for UAV choosing the optimal action according to the policy learned by the algorithm we proposed than the classic Q-learning algorithm in the agricultural plant protection environment. This proposed algorithm is implemented and tested on datasets that are evenly distributed based on real UAV parameters and real farm information. The performance evaluation of the algorithm is discussed in detail. Experimental results show that the algorithm we proposed can efficiently learn the optimal policy for UAVs in the agricultural plant protection environment","[{'title': 'Entropy', 'identifiers': ['1099-4300', 'issn:1099-4300']}]",'MDPI AG',Improved Q-Learning Algorithm Based on Approximate State Matching in Agricultural Plant Protection Environment,10.3390/e23060737,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
4744310,2006-05-01T00:00:00,"Published in Journal of Aerospace Computing, Information, and Communication, 3(5):187-213, May, 2006.Received 27 July 2005; revision received and accepted for publication 17 November 2005. Copyright 2005 by Henrik B.
Christophersen, R. Wayne. Pickell, James C. Neidhoefer, Adrian A. Koller, Suresh, K. Kannan and Eric N. Johnson. Published
by the American Institute of Aeronautics and Astronautics, Inc., with permission.The Flight Control System 20 (FCS20) is a compact, self-contained Guidance, Navigation,
and Control system that has recently been developed to enable advanced autonomous
behavior in a wide range of Unmanned Aerial Vehicles (UAVs). The FCS20 uses a floating
point Digital Signal Processor (DSP) for high level serial processing, a Field Programmable
Gate Array (FPGA) for low level parallel processing, and GPS and Micro Electro Mechanical
Systems (MEMS) sensors. In addition to guidance, navigation, and control functions, the
FCS20 is capable of supporting advanced algorithms such as automated reasoning, artificial
vision, and multi-vehicle interaction. The unique contribution of this paper is that it gives a
complete overview of the FCS20 GN&C system, including computing, communications, and
information aspects. Computing aspects of the FCS20 include details about the design process,
hardware components, and board configurations, and specifications. Communications
aspects of the FCS20 include descriptions of internal and external data flow. The information
section describes the FCS20 Operating System (OS), the Support Vehicle Interface Library
(SVIL) software, the navigation Extended Kalman Filter, and the neural network based
adaptive controller. Finally, simulation-based results as well as actual flight test results that
demonstrate the operation of the guidance, navigation, and control algorithms on a real
Unmanned Aerial Vehicle (UAV) are presented","[{'title': 'Journal of Aerospace Computing Information and Communication', 'identifiers': ['1542-9423', 'issn:1542-9423']}]","American Institute of Aeronautics and Astronautics, Inc.","A Compact Guidance, Navigation, and Control System for Unmanned Aerial Vehicles",,https://core.ac.uk/download/4744310.pdf,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
491013080,2021-11-01T00:00:00,"This paper presents the implementation of an autonomous electric vehicle (EV) project in the National Taiwan University of Science and Technology (NTUST) campus in Taiwan. The aim of this work was to integrate two important practices of realizing an autonomous vehicle in a campus environment, including vehicle positioning and path tracking. Such a project is helpful to the students to learn and practice key technologies of autonomous vehicles conveniently. Therefore, a laboratory-made EV was equipped with real-time kinematic GPS (RTK-GPS) to provide centimeter position accuracy. Furthermore, the model predictive control (MPC) was proposed to perform the path tracking capability. Nevertheless, the RTK-GPS exhibited some robust positioning concerns in practical application, such as a low update rate, signal obstruction, signal drift, and network instability. To solve this problem, a multisensory fusion approach using an unscented Kalman filter (UKF) was utilized to improve the vehicle positioning performance by further considering an inertial measurement unit (IMU) and wheel odometry. On the other hand, the model predictive control (MPC) is usually used to control autonomous EVs. However, the determination of MPC parameters is a challenging task. Hence, reinforcement learning (RL) was utilized to generalize the pre-trained datum value for the determination of MPC parameters in practice. To evaluate the performance of the RL-based MPC, software simulations using MATLAB and a laboratory-made, full-scale electric vehicle were arranged for experiments and validation. In a 199.27 m campus loop path, the estimated travel distance error was 0.82% in terms of UKF. The MPC parameters generated by RL also achieved a better tracking performance with 0.227 m RMSE in path tracking experiments, and they also achieved a better tracking performance when compared to that of human-tuned MPC parameters","[{'title': 'Electronics', 'identifiers': ['issn:2079-9292', '2079-9292']}]",'MDPI AG',Integrating Vehicle Positioning and Path Tracking Practices for an Autonomous Vehicle Prototype in Campus Environment,10.3390/electronics10212703,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
475072228,2021-06-01T00:00:00,"Unmanned aerial vehicle (UAV) imaging is a promising data acquisition technique for image-based plant phenotyping. However, UAV images have a lower spatial resolution than similarly equipped in field ground-based vehicle systems, such as carts, because of their distance from the crop canopy, which can be particularly problematic for measuring small-sized plant features. In this study, the performance of three deep learning-based super resolution models, employed as a pre-processing tool to enhance the spatial resolution of low resolution images of three different kinds of crops were evaluated. To train a super resolution model, aerial images employing two separate sensors co-mounted on a UAV flown over lentil, wheat and canola breeding trials were collected. A software workflow to pre-process and align real-world low resolution and high-resolution images and use them as inputs and targets for training super resolution models was created. To demonstrate the effectiveness of real-world images, three different experiments employing synthetic images, manually downsampled high resolution images, or real-world low resolution images as input to the models were conducted. The performance of the super resolution models demonstrates that the models trained with synthetic images cannot generalize to real-world images and fail to reproduce comparable images with the targets. However, the same models trained with real-world datasets can reconstruct higher-fidelity outputs, which are better suited for measuring plant phenotypes","[{'title': 'Remote Sensing', 'identifiers': ['2072-4292', 'issn:2072-4292']}]",'MDPI AG',Spatial Super Resolution of Real-World Aerial Images for Image-Based Plant Phenotyping,10.3390/rs13122308,,core,autonomous vehicle,'autonomous vehicle' AND 'machine learning' AND ('real-world' AND 'deploy')
