doi,title,publisher,content_type,abstract,html_url,publication_title,publication_date,database,query_name,query_value
10.1109/ICSC.2018.00074,A Matching Framework for Multimedia Data Integration Using Semantics and Ontologies,IEEE,Conferences,The increasing of data volume together with heterogeneity of representations are basic challenges in several application and research contexts. This issue is stressed in the Semantic Web domain where huge of information have to be accessed by users and services in a common and simple way. In order to present different data in an integrated fashion we need of efficient and effective techniques to preserve information about them. In this paper we present a complete approach to integrate very large multimedia knowledge bases using well known formalisms for knowledge representation as ontologies. The proposed approach is supported by a software framework able to create a global ontology by the integration of existing ones automatizing the whole ontology building process. We present a case study on cultural heritage domain to show a real implementation of our approach.,https://ieeexplore.ieee.org/document/8334497/,2018 IEEE 12th International Conference on Semantic Computing (ICSC),31 Jan.-2 Feb. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1995.531986,A multimedia self-service terminal with conferencing functions,IEEE,Conferences,"We developed a new platform for personal computer based self-service terminals with multimedia conferencing functions in order to provide networked multimedia capability and face-to-face interface. The platform is based on extending our desktop conferencing system called ConverStation/2. It provides ITU-T H.320 motion video audio communication, and shared chalkboard function between a self-service terminal and an operator terminal. It also provides scanned image data transmission for browsing and co-editing of images between a customer and an operator. We clearly separate the platform with application modules, by providing a string command interface between them; we enhanced our conferencing features to meet the requirements of self-service applications. The major new features include: 1) asymmetric operation and user interface of a shared chalkboard; 2) dynamic line connection/disconnection control, and 3) dynamic media (video/audio/data) control on ISDN communication. With these conferencing functions, more complex services such as consultation can be provided through self-service terminals.",https://ieeexplore.ieee.org/document/531986/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISPACS.2010.5704595,Advancing multimedia technologies for smart living and learning services,IEEE,Conferences,"The development of ICT technologies to advance smart living products and creative humanity services becomes future global consensus and industrial trends in the world. The user-driven innovations, which bring products and services naturally and smartly close to human needs, should heavily adopt many interactive multimedia technologies. The interactions through the detection of human nature touch, voice, and gestures and the responses of virtually real multimedia could be implemented in highly integrated embedded and cloud computing systems. In the talk, some designs such as 3D interactive sports, Interactive arts, interactive table, and smart living and learning products in the Technologies of Ubiquitous Computing and Humanity (TOUCH) Center, will be introduced. Through smart living and learning clouds, the future plans related to smart living and efficient learning services for the students are addressed. Linked to the city government, the humanity services through living lab open innovations delivered to the citizens will be forecasted finally.",https://ieeexplore.ieee.org/document/5704595/,2010 International Symposium on Intelligent Signal Processing and Communication Systems,6-8 Dec. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEST.2009.5276702,An artificial immune system approach to anomaly detection in multimedia ambient intelligence,IEEE,Conferences,"Artificial Immune Systems are inspired by biological immune systems, and are characterized by interesting properties such as error tolerance, adaptation and self-monitoring. An area where they found wide application is anomaly detection in information systems, including intrusion detection. In this work we propose to extend the Artificial Immune System (AIS) paradigm from its typical application domain, computer system security, to ambient intelligence. AISs can be used to respond adaptively to real word anomalies in controlled environments. Here the counterpart of perceptual functions and detection capabilities can be provided by device intelligence, e.g. in terms of multimedia interpretation.",https://ieeexplore.ieee.org/document/5276702/,2009 3rd IEEE International Conference on Digital Ecosystems and Technologies,1-3 June 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudCom.2016.0060,Animation Rendering on Multimedia Fog Computing Platforms,IEEE,Conferences,"Modern distributed multimedia applications are resource-hungry, and they often leverage on-demand cloud services to reduce their expenses. Existing cloud services deploy many servers in a few data centers, which consume a lot of electricity to power up and cold down, and thus are expensive and environmentally unfriendly. In this paper, we present a multimedia fog computing platform that utilizes resources from public crowds, edge networks, and data centers to serve distributed multimedia applications at lower costs. We use animation rendering as a case study, and identify several challenges for optimizing it on our multimedia fog computing platform. Among these challenges, we focus on the problem of predicting the completion time of each rendering job. We propose an efficient algorithm based on state-of-the-art machine learning algorithms. We also fine-tune the algorithm using multi-fold cross-validation for higher prediction accuracy. With real datasets, we conduct trace-driven simulations to quantify the performance of our prediction algorithm and that of the whole platform. The simulation results show that our proposed algorithm outperforms a state-of the-art statistical model in several aspects: completed job ratio by 20%, makespan by 2 times, and normalized deviation by 30 times, on average. Moreover, the overall performance of the platform with our proposed algorithm is fairly close to that with an Oracle of the actual job completion time: a small factor of 1.48 in terms of makespan is observed.",https://ieeexplore.ieee.org/document/7830701/,2016 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),12-15 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISM52913.2021.00039,Combining Linked Open Data and Multimedia Knowledge Base for Digital Cultural Heritage Robotic Applications,IEEE,Conferences,"The current trends in society evolution are showing rapid changes in our habitual environments and consequently affecting human interactions with them. Among the key factors of this process we can certainly cite the growth of BigData favored by the knowledge digitalization, the dissemination of sensors in environments and the advancements of connectivity capabilities. At the same time, the progress in artificial intelligence and cognitive robotics has lead to the production of sophisticated humanoid robots, which are progressively spreading to a wide public. In this way, research efforts are needed for a proper knowledge management and knowledge acquisition by machines, in order to have more natural and friendly human-robot interactions in daily tasks, usually performed by human beings. In this paper we show an approach related to a human-robot interaction in cultural heritage context, simulating a digital ecosystem where a robot plays the role of a guide for tourists and it is able to proactively interact with its interlocutors by combining both semantic and visual information. The proposed approach, its implementation and experimental results on a real robotic platform are shown and discussed.",https://ieeexplore.ieee.org/document/9666077/,2021 IEEE International Symposium on Multimedia (ISM),29 Nov.-1 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMSCNT.1997.658447,Combining multimedia collaboration and workflow management,IEEE,Conferences,"Two different areas of distributed group work are supported by workflow management systems and real time collaboration systems. Workflow management systems support work being structured in steps such that each step can be handled with the results of former steps and the expertise of the person working on that step. On the other hand, multimedia collaboration systems are best suited for unstructured group activities. Audiovisual connectivity and shared documents enable flexible group processes. All coordination tasks are left to the conference participants. The paper introduces an integration concept and prototype system which combines the advantages of both types of CSCW systems.",https://ieeexplore.ieee.org/document/658447/,Proceedings 23rd Euromicro Conference New Frontiers of Information Technology - Short Contributions -,1-4 Sept. 1997,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
,Controlling QoS at the application level for multimedia applications using artificial neural networks: Experimental results,IEEE,Conferences,"In this paper we implement a recurrent high order neural network based controller (built into a Client-Server architecture), in order to provide QoS control. The scheme is based on a recently proposed QoS control algorithm with proven stability and robustness properties. Mild assumptions have been used to construct a user satisfaction function that depends on two media characteristics (i.e. color depth and frame rate). The above, along with bandwidth measurements obtained at per frame basis, provide the essential information to the controller, which outputs the appropriate values for the media characteristics in order to achieve the required user satisfaction without violating the available bandwidth constraint. The implementation is performed on a TCP/IP network and the QoS control, which is applied at the application level, is shown to hold a real time property. Experimental results highlight its performance.",https://ieeexplore.ieee.org/document/7075266/,2000 10th European Signal Processing Conference,4-8 Sept. 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2002.1181265,Current challenges and trends of interactive multimedia in enhancing engineering education,IEEE,Conferences,"The use of multimedia computer technology as replacement, or supplement to, human educators in engineering education is becoming widespread. Such technology can be employed to demonstrate and correlate real life application and theory thereby promoting deep learning. Interactive courseware for higher learning institutions may be extremely useful where trained human resources in the engineering education sector are limited. However, although many interactive multimedia systems have been developed, only few of these systems can really be said to be successful. This paper present an overview of the status of some interactive techniques commonly used in the context of engineering education and discuss the current trends of incorporating multimedia technology in the teaching of engineering subjects.",https://ieeexplore.ieee.org/document/1181265/,"2002 IEEE Region 10 Conference on Computers, Communications, Control and Power Engineering. TENCOM '02. Proceedings.",28-31 Oct. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2007.4370883,Design and Application of Multimedia Mobile Learning Framework,IEEE,Conferences,"Due to the thrive of mobile network and portable device, distance learning is evolved from desktop computer to mobile device. This paper combines mobile computing with streaming media technology in the application of distance education to design an open multimedia mobile learning framework for university students. This framework consists of mobile learning software and custom learning device. System architecture and several implementation issues are discussed in this paper. Moreover, practical applications show that our framework can significantly increase student's interest and effect in learning as compare to the traditional mode. The system is of preferable feature of real-time, interactive and expansion.",https://ieeexplore.ieee.org/document/4370883/,2007 International Conference on Machine Learning and Cybernetics,19-22 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I-SMAC49090.2020.9243535,Design and Implementation of EDMA Controller for AI based DSP SoCs for Real- Time Multimedia Processing,IEEE,Conferences,"Processing of the multiple data streams demand highperformance multiple transfers overburden the Processor System on Chips (SoC) in real time multimedia processing applications. High performance direct memory access (DMA) controller eases the processor as it performs bulk data transfer without the intervention of processor. This is true even in most artificial intelligence (AI) based systems and interleaving functions in communication systems where high-speed bulk data transfers are required. This is achieved by the design of Enhanced Direct Memory Access (EDMA) Controller, for high speed bulk data transfers. Paper presents the design of enhanced DMA core which is synthesizable ready to integrate for high performance AI based Digital Signal Processing SoC. The EDMA core is used for flexible Memory Access and bulk data transfers. EDMA core support several methods for data transfer between an input or output (I/O) device and the core processing unit. The processor in the SoC is used to program the Direct Memory Access (DMA) transfer instructions and actual transfers are performed by the EDMA core without the interference of processor. The EDMA design supports flexible addressing modes like linear, circular, step for bulk data transfers. The EDMA core is planned to be verified with test cases as in realistic application scenarios of interleaving, real time video processing.",https://ieeexplore.ieee.org/document/9243535/,"2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC)",7-9 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVRIS.2019.00062,Design of Intelligent Campus Multimedia Interactive System Based on Internet of Things Technology,IEEE,Conferences,"In order to perfect the construction of intelligent campus multimedia interactive information network and improve the ability of campus multimedia interaction and intelligent education information management, a design scheme of intelligent campus multimedia interactive system based on Internet of things technology is proposed. The system development environment is based on Multigen Creator3.2 and embedded PCI bus development environment, and the intelligent campus multimedia interactive infrastructure is constructed by using Internet of things and radio frequency identification (RFID) construction scheme. The virtual scene simulation and multimedia information interaction design of multimedia information platform, intelligent classroom, library interactive platform and student information management system are realized on the intelligent multimedia information processing terminal to meet the needs of multimedia interaction in intelligent campus. The hardware structure and software development of the intelligent campus multimedia interactive system are carried out. the test results show that the system has high information integration and strong multimedia information processing ability. The artificial intelligence is good, the system is stable and reliable.",https://ieeexplore.ieee.org/document/8921364/,2019 International Conference on Virtual Reality and Intelligent Systems (ICVRIS),14-15 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISE51755.2020.00149,Effects of “4D Situational Multimedia Interaction” on Modern Non-commissioned Officer Education,IEEE,Conferences,"The rapid development of informatized technology has essentially exerted unavoidable and powerful influence upon modern higher education. To promote the efficiency and the effect of higher education, it is high time that higher education institutions integrated both informatized hardware (such as projectors, interactive TV set, WIFI, Virtual Reality devices) and intelligent software (such as PC/mobile-phone-based APPs with artificial intelligence, and audio-visual multimedia) with teaching practice in class. This paper dwells on the 4D situational multimedia interaction (4DSMI) based on informatized hardware including projectors, WIFI, VR and software like CoolEditPro which automatically provides assessment and grades immediately after each dubbing practice. Through the informatized hardware and software aforementioned, the 4DSMI aims at arousing sense organs, producing emotional resonance, deepening cognitive identification, and ultimately promoting linguistic competency. It researches on the relevance of 4DSMI with NCOs' linguistic competency, the design and purpose of 4DSMI, its implementation in the real class and its effects. It finds out that 4DSMI plays a vital role in getting NCOs emotionally interested, physically involved, cognitively identified and practically achieved in military English class.",https://ieeexplore.ieee.org/document/9418891/,2020 International Conference on Information Science and Education (ICISE-IE),4-6 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NAFIPS.2001.944413,From fuzzy control to fuzzy multimedia information technology,IEEE,Conferences,"In the middle of the 1980s in Japan, fuzzy technology became a central issue, mainly for process control, and the year 1990 became the so-called ""fuzzy home electronics year"". These technologies are mainly based on IF-THEN rule-based fuzzy inference with instrumentation (i.e. sensor and actuator) engineering. Then, neural network technology was merged with fuzzy technology in 1991 and again many consumer products were sent on to the real market in Japan. Such neuro-fuzzy technologies are classified into nine categories. In 1993, chaos technologies also took part in the research and development of such high-tech issues. Other technologies, e.g. genetic algorithms and artificial life, are also being studied by company engineers. These kinds of practical, technological aspects (mainly in Japan) are discussed first. Then the concept of a fuzzy multimedia intelligent communication system (FuMICS) is proposed. This is a human-friendly communication interface that is able to handle fuzzy information in a fuzzy knowledge base where multimedia information is integrated. It consists of five modules: (1) an observation unit, (2) a norm base, (3) a fuzzy knowledge base, (4) a multimedia information transmitter, and (5) a supervisor.",https://ieeexplore.ieee.org/document/944413/,Proceedings Joint 9th IFSA World Congress and 20th NAFIPS International Conference (Cat. No. 01TH8569),25-28 July 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOCPAR.2014.7008042,Hardware adaptation for multimedia application case study: Augmented reality,IEEE,Conferences,"Embedded electronic multimedia systems to a wide emerged in recent years. These systems are more complex and diversified. The SoC (System on Chip) must respect many important constraints. It must operate in extreme conditions (network problems, limited energy consumption…). In this paper, we present an adaptation technique based on the dynamic reconfiguration features of Xilinx FPGA. The purpose of this technique is to reduce the computing time of a complex multimedia application by adding a variable number of hardware computing units. A case study is presented on a real system using the Xilinx design environment to develop a 3D application. This used application is the augmented reality (AR).",https://ieeexplore.ieee.org/document/7008042/,2014 6th International Conference of Soft Computing and Pattern Recognition (SoCPaR),11-14 Aug. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin47944.2019.8966204,Hierarchical Model for Multimedia Content Classification,IEEE,Conferences,"An automatic content classification technique is an essential tool in multimedia applications. Present research for audio-based classifiers look at short- and long-term analysis of signals, using both temporal and spectral features as well as using auditory models. In this paper, we present a hierarchical scheme that maps familiar single-type streaming content sources (e.g., either music, movie, or voice) to processes in a JavaScript Object Notation (JSON) configuration file. For streaming sources containing mixed-type of content (e.g., movies and music) we employ a machine learning (ML) model to classify between the movie, music, and voice-based on metadata. Towards this end, statistical models of the various metadata are created since a large metadata dataset is not available. Subsequently, synthetic metadata are generated from these statistical models, and the synthetic metadata is input to the ML classifier as feature vectors. We demonstrate high discriminating capabilities (accuracy ≈ 90%), with very low latency (viz., ≈ on an average 7 ms), using real metadata from real-world content such as from YouTubeTM, Blu-rayTM, and user-generated content (UGC). The combined hierarchical system (comprising JSON file and ML-model) reliably identifies the content type with an accuracy greater than 90%.",https://ieeexplore.ieee.org/document/8966204/,2019 IEEE 9th International Conference on Consumer Electronics (ICCE-Berlin),8-11 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMEDIA.2010.36,Implementation and Evaluation of an Adaptive Multimedia Presentation System (AMPS) with Contextual Supplemental Support Media,IEEE,Conferences,"Requirements have been captured for a multimedia presentation learning system that adapts content through interactive interventions between the student and tutor. With the addition of contextual supplementary learning materials selected by a tutor responding to a series of email questions, supplementary video segments that personalise learning are added. A prototype has been developed using HTML, Flash and XML. Evaluation in this paper shows that adaptation was achieved but with some drawbacks. An analysis model at semantic and data level, needed to process an adaptive multimedia presentation system) in real-time is described, raising several research questions. Our results show that the addition of context-based rules to process and recommend descriptions of segmented multimedia components according to a bounded ontology can potentially produce dynamic adaptation of learning material in real-time. A new demonstrator application is under development.",https://ieeexplore.ieee.org/document/5501619/,2010 Second International Conferences on Advances in Multimedia,13-19 June 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOSE.2015.39,Linking Quality of Service and Experience in Distributed Multimedia Systems Using PROV Semantics,IEEE,Conferences,"Experimenters creating innovative applications that combine diverse distributed multimedia services with rich end user applications require enhanced insight into the relationships between the perceived quality of experience (QoE) and provided quality of service (QoS). We have implemented software which not only captures QoE and QoS measurements but, using a provenance ontology, also records the interactions between end users, the content, applications and the services. The data exploration interface provided allows an experimenter, working with participants in real-world situations, to understand the detail of the participants' usage and experience of the system and the system performance factors contributing to their quality of experience.",https://ieeexplore.ieee.org/document/7133520/,2015 IEEE Symposium on Service-Oriented System Engineering,30 March-3 April 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTTA.2008.4529967,Multimedia Learning in Advanced Computer-based Contexts: `Discovering Trier',IEEE,Conferences,"Edutainment is a neologism that expresses the marriage of education and entertainment. In particular edutainment is a form of entertainment designed to educate as well as to amuse and typically seeks to instruct or socialize its audience by embedding lessons in some familiar form of entertainment: television programs, computer and video games, films, music, websites or multimedia software. On the other hand, the design and the implementation of not boring or repetitive edutainment are not trivial or easy tasks. In this paper we propose a new approach for the design of an edutainment. The proposed environment ""translates"" the storyboards of the games in a Dynamic Bayesian Networks that are the extension of Bayesian Networks for modelling times-series data. The Bayesian approach allows a dynamic adaptation of the game to the user's profile and the establishment of several paths in the same game. We furnish some results obtained by the use of a first prototype of our tool in real academic courses.",https://ieeexplore.ieee.org/document/4529967/,2008 3rd International Conference on Information and Communication Technologies: From Theory to Applications,7-11 April 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1995.531939,Multimedia sensing system for robot,IEEE,Conferences,"The purpose of this study is to realize a multimedia sensing system for robot. Using both image and sound processing, the system makes a robot track the person who is speaking. The sound direction is calculated from the phase difference between the sounds arriving at the right and left microphones (ears) of the robot. Then by detecting the synchronization between the sound and image changes, the system identifies the speaker. Furthermore, by introducing a multi-level synchronization checking and context analysis, the action pattern of the robot can be regulated to make the robot perform in a complicated environment with plural speakers. All the processes are performed in real-time. The proposed system is implemented in the information assistant robot ""Hadaly"".",https://ieeexplore.ieee.org/document/531939/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LCN.2011.6115322,Practical machine learning based multimedia traffic classification for distributed QoS management,IEEE,Conferences,"A multi-service Internet requires routers to recognise and prioritise IP flows carrying interactive or multimedia traffic. It is increasingly problematic for legal or administrative reasons to recognise such flows using unique port numbers or deep packet inspection. New work in recent years shows that Machine Learning (ML) techniques can use externally observable statistical characteristics to usefully differentiate such IP traffic. However, most previous work has not addressed the practicality of ML-based traffic classification in terms of CPU and memory usage. Here we describe our design, implementation and performance evaluation of a distributed, ML-based traffic classification and control system for FreeBSD's IP Firewall (IPFW). On an Intel Core i7 2.8 GHz PC our system can classify up to 400 000 packets per second using only one core and our system scales well to up to 100 000 simultaneous flows. Also our implementation allows one classifier PC to control subsequent traffic shaping or blocking at multiple (potentially lower performance) routers or gateways distributed around the network.",https://ieeexplore.ieee.org/document/6115322/,2011 IEEE 36th Conference on Local Computer Networks,4-7 Oct. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPL.2010.22,Real-Time Classification of Multimedia Traffic Using FPGA,IEEE,Conferences,"Real-time classification of Internet traffic according to application types is vital for network management and surveillance. Identifying emerging applications based on well-known port numbers is no longer reliable. While deep packet inspection (DPI) solutions can be accurate, they require constant updates of signatures and become infeasible for encrypted payload especially in multimedia applications (e.g. Skype). Statistical approaches based on machine learning have thus been considered more promising and robust to encryption, privacy, protocol obfuscation, etc. However, the computation complexity of traffic classification using those statistical solutions is high, which prevents them being deployed in systems that need to manage Internet traffic in real time. This paper proposes a FPGA-based parallel architecture to accelerate the statistical identification of multimedia applications while maintaining high classification accuracy. Specifically, we base our design on the k-Nearest Neighbors (k-NN) algorithm which has been shown to be one of the most accurate machine learning algorithms for Internet traffic classification. To enable high-rate data streaming for real-time classification, we adopt the locality sensitive hashing (LSH) for approximate k-NN. The LSH scheme is carefully designed to achieve high accuracy while being efficient for implementation on FPGA. Processing components in the architecture are optimized to realize high throughput. Extensive experiments and FPGA implementation results show that our design can achieve high accuracy above 99% for classifying three main categories of multimedia applications from Internet traffic while sustaining 80 Gbps throughput for minimum size (40 bytes) packets.",https://ieeexplore.ieee.org/document/5694221/,2010 International Conference on Field Programmable Logic and Applications,31 Aug.-2 Sept. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC.2012.6181017,Real-time QoE prediction for multimedia applications in Wireless Mesh Networks,IEEE,Conferences,"As Wireless Mesh Networks (WMNs) are being increasingly deployed, there is an increasing demand for new quality assessment mechanisms that allow service operators to evaluate and optimize the utilization of network resources, while ensuring a good quality level on multimedia applications as perceived by end-users. However, existing real-time assessment schemes for WMNs are not capable of capturing the actual quality of received multimedia content with regard to user perception. Therefore, it is not possible to assure the user experience of content services. To address this problem, this paper introduces the Hybrid Quality of Experience (HyQoE) Prediction, which is a quality estimator specially designed to assess real-time multimedia applications. HyQoE is designed based on the framework of the widely used Pseudo-Subjective Quality Assessment (PSQA) Tool which exploits Random Neural Network (RNN). Crucial extension work has been implemented to achieve our objectives. A performance evaluation verifies the effectiveness and advantages of HyQoE in predicting users' perception of multimedia content in WMNs over existing subjective and hybrid methods.",https://ieeexplore.ieee.org/document/6181017/,2012 IEEE Consumer Communications and Networking Conference (CCNC),14-17 Jan. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMEDIA.2009.19,Requirements for an Adaptive Multimedia Presentation System with Contextual Supplemental Support Media,IEEE,Conferences,"Investigations into the requirements for a practical adaptive multimedia presentation system have led the writers to propose the use of a video segmentation process that provides contextual supplementary updates produced by users. Supplements consisting of tailored segments are dynamically inserted into previously stored material in response to questions from users. A proposal for the use of this technique is presented in the context of personalisation within a Virtual Learning Environment. During the investigation, a brief survey of advanced adaptive approaches revealed that adaptation may be enhanced by use of manually generated metadata, automated or semi-automated use of metadata by stored context dependent ontology hierarchies that describe the semantics of the learning domain.",https://ieeexplore.ieee.org/document/5206913/,2009 First International Conference on Advances in Multimedia,20-25 July 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAI.2016.0036,Research on the Intelligent Information System for the Multimedia Teaching Equipment Management,IEEE,Conferences,"Building the intelligent information system based on the Internet of Things technology enables the manager to achieve real-time monitoring of running status of multimedia teaching equipment. The system can improve the efficiency of multimedia teaching equipment management. To begin with, existing research on multimedia teaching equipment management and relative system is introduced, and disadvantage of the system is indicated. And then we discussed the functions of intelligent information system (IIS) for multimedia teaching equipment management, design of system overall architecture, software and hardware, and the system integration risk. Finally, the prospect of the applications of IIS in multimedia teaching equipment management is analyzed.",https://ieeexplore.ieee.org/document/7816689/,2016 International Conference on Information System and Artificial Intelligence (ISAI),24-26 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNG.2011.102,Securing IP-Multimedia Subsystem (IMS) against Anomalous Message Exploits by Using Machine Learning Algorithms,IEEE,Conferences,"Modern communication infrastructure (IP Multimedia Subsystem (IMS) and Voice over IP (VoIP)) are vulnerable to zero day attacks and unknown threats. Anomalous SIP requests can be used to remotely launch malicious activity. Furthermore, anomalous messages are capable of crashing - sometimes with one message only - servers and end points. Recently, it is shown that a malicious SIP message ""INVITE of Death"" crash a server or gain unfettered access to it. In contrast, little research is done to protect IMS against such anomalous messages. In this paper, we propose an anomalous message detection framework that extracts novel syntactical features from SIP messages at the P-CSCF of an IMS. Our framework operates in four steps: (1) analyzes the byte-level distribution of SIP message, (2) extracts spatial features from IMS messages in form of byte transition probabilities, (3) uses well-known feature selection scheme to remove redundancy in the features set, and (4) uses standard machine learning algorithms to raise the final alarm. The benefit of our framework is that it is lightweight requiring less processing and memory resources and provides high detection accuracy. We have evaluated our system on a real-world IMS dataset consisting of more than 10, 000 benign and malicious SIP messages. The results of our experiments demonstrate that using machine learning algorithms, our framework achieves detection accuracy of more than 99%. Last but not least, its testing time is 152μ seconds per packet, as a result, it can be easily deployed on IMS core.",https://ieeexplore.ieee.org/document/5945297/,2011 Eighth International Conference on Information Technology: New Generations,11-13 April 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.1998.678192,"Software MPEG-2 video decoder on a 200-MHz, low-power multimedia microprocessor",IEEE,Conferences,"This paper presents a low-power, 32-bit RISC microprocessor with a 64-bit ""single-instruction multiple-data"" multimedia coprocessor, V830R/AV, and its MPEG-2 video decoding performance. This coprocessor basically performs multimedia-oriented four 16-bit operations every clock, such as multiply-accumulate with symmetric rounding and saturation, and accelerates computationally intensive procedures of the video decoding; an 8/spl times/8 IDCT is performed in 201 clocks. The processor employs the Concurrent Rambus DRAM interface, and facilities for controlling cache behavior explicitly by software to speed up enormous memory accesses necessary for motion compensation. The 200-MHz V830R/AV processor with the 600-Mbyte/s Concurrent Rambus DRAMs decodes MPEG-2 MP@ML video in real-time (30 frames/s).",https://ieeexplore.ieee.org/document/678192/,"Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP '98 (Cat. No.98CH36181)",15-15 May 1998,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICI.2009.316,Study and Realization of Multimedia Intelligent Tourism Navigation System,IEEE,Conferences,"In this paper, we design a multimedia intelligent tourism navigation system and develop such a platform with the functions of virtual tourism, travel route planning, self-navigation and rescue. Multimedia interactive services facilitate visitors' self-service travel, and provide function of emergency process and rescue in tourism spot.",https://ieeexplore.ieee.org/document/5376250/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVRIS.2019.00109,The Marxist Chinese Political Teaching Innovation Model Based on Multimedia Database Management System,IEEE,Conferences,"In order to explore the feasibility and necessity of promoting the innovation model of Marxism in China based on multimedia database management system, this paper studies the selection of the ""office network information technology"" course as an example to carry out the practice of the innovative model of political teaching in China. The research process includes identifying and defining learning needs, formulating learning plans and evaluation strategies, designing and developing Marxist Chinese politics, implementation planning and evaluation, and using SPSS and other tools to analyze data such as learning behaviors, learning outcomes, and questionnaires. The results show that well-designed Marxist Chinese politics can effectively improve the quality of teaching; students' online learning behavior is positively correlated with academic achievement; students' acceptance and satisfaction with Marxist Chinese politics are high.",https://ieeexplore.ieee.org/document/8920809/,2019 International Conference on Virtual Reality and Intelligent Systems (ICVRIS),14-15 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSO.2009.474,The Organization and Application of Multimedia information Based on Ontology,IEEE,Conferences,"In order to use the multimedia sources efficiently, finding the semantic content behind the resources and analyzing them, a framework to uniformly describe multimedia resources is used in this paper by using the theory of ontology. Based on fifteen elementary metadata of Dublin Core Metadata Element Set, the notion of the Basic Metadata and Expand Metadata are defined to realize the unify description and separate expression. In this way, the current metadata format from most multimedia resource can be satisfied, and also the self-definition and expansion can be made according to the different applications. This framework has been satisfactorily used in real media resource management System for multimedia content organization, query and exchange.",https://ieeexplore.ieee.org/document/5193981/,2009 International Joint Conference on Computational Sciences and Optimization,24-26 April 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMPSAC.1995.524798,The multimedia desktop conference system adaptability in network traffic on LAN,IEEE,Conferences,"In this paper, we describe the design and implementation of a desktop conference system, called DeskShare, which provides a distributed and real-time conferencing facility for cooperative work. The main objective of our research is to bring users of remote systems together to collaborate in group work using readily available commodity audio and video technology, and currently used networks based on asynchronous communications, such as Ethernet and FDDI, with adaptability in network congestion. Since DeskShare is implemented as a group of cooperating processes with highly modular fashion based on on object-oriented design technique, we can easily accommodate further system configuration modifications, for example changing video and/or audio codec. Even if DeskShare was originally designed for multimedia desktop conferencing, it is possible to apply it to diverse areas, such as remote education, teleshopping, group design, and remote operation, which are necessary for various group activities.",https://ieeexplore.ieee.org/document/524798/,Proceedings Nineteenth Annual International Computer Software and Applications Conference (COMPSAC'95),9-11 Aug. 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TOCS53301.2021.9688914,Two-dimensional animation interactive demonstration teaching system based on multimedia technology---artificial intelligence and information technology,IEEE,Conferences,"With the development and innovation of computer technology and the heavy research of media, A multimedia technology that can interact with computers in real-time information has appeared. It is the technology of industrial life and education. On the other hand, this promotes the extent of computer applications and combines multimedia technologies in education. This is new. The development of the world is a way of finding new factors and increasing learning. An animation technology as a product plays an incomplete role in the production of the multimedia cursors. As we all know, the multimedia materials developed by Flash software are known for their exquisite two-dimensional animations. However, it is more difficult to implement interactive two-dimensional molecular model demonstrations in Flash. Combining the current animation technology and database technology, this article has conducted an in-depth study on it, and designed a set of interactive multimedia teaching system based on two-dimensional animation technology. This article first introduces the basic principles of animation technology and the implementation of interactive information platforms. On this basis, the system&#x0027;s hierarchical structure and development process are analyzed, and the main work is divided into two key parts. On the one hand, it uses animation technology to design and t make vivid, feasible and interactive teaching courseware, on the other hand, to build an interactive information management teaching platform combined with database principles.",https://ieeexplore.ieee.org/document/9688914/,"2021 IEEE Conference on Telecommunications, Optics and Computer Science (TOCS)",10-11 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PACRIM.1999.799575,Using formal approach to specify distributed multimedia systems,IEEE,Conferences,"A formal approach can benefit multimedia in a number of ways. For example, formal system development can aid the development of correct systems. Formal specification and verification techniques have been extensively investigated, but these techniques have generally not addressed the requirement of distributed multimedia computing. The central issue in multimedia is dealing with real-time. In particular, there is a need for software development techniques that support the specification and verification of real-time properties. In this paper, we propose a new approach to the formal specification and verification of distributed multimedia computing. This new approach is based on the extension of LOTOS to incorporate timing in it. Thus, succinct expressions of communicating concurrent processes can be made. Similarly, the emphasis on non-determinism in process algebraic techniques facilitates abstract specification, hiding implementation details. Furthermore, rich and tractable mathematical models of the semantics of process algebra have been developed. The key characteristic of the new approach lies in the specification of behavior, requirements and different types of time in a specification.",https://ieeexplore.ieee.org/document/799575/,"1999 IEEE Pacific Rim Conference on Communications, Computers and Signal Processing (PACRIM 1999). Conference Proceedings (Cat. No.99CH36368)",22-24 Aug. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GUCON48875.2020.9231241,Wireless Multimedia Sensor Networks and Data Fusion: An Overview,IEEE,Conferences,"In the era of big data, Wireless Multimedia Sensor Networks (WMSNs) have emerged as an enabling and effective technology for multimedia data sensing in physical environment. Wireless communication being resource constrained finds it difficult to support the volume and the type of big data traffic for acquisition and distribution. It poses complex technical challenges particularly in terms of computational power, transmission bandwidth, and energy consumption. In this paper, we have presented an account of important features and requirements of WMSNs for monitoring and analyzing the data recorded through multimodal sensors. Recent fusion strategies including those based on artificial intelligence for achieving improved accuracy in extracting the intelligence from big data are analyzed. Their limitations are identified and probable solutions in context with the real-field deployment of the emerging ubiquitous computing systems are outlined.",https://ieeexplore.ieee.org/document/9231241/,"2020 IEEE International Conference on Computing, Power and Communication Technologies (GUCON)",2-4 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2926206,A Fusion-Based Framework for Wireless Multimedia Sensor Networks in Surveillance Applications,IEEE,Journals,"Multimedia sensors enable monitoring applications to obtain more accurate and detailed information. However, the development of efficient and lightweight solutions for managing data traffic over wireless multimedia sensor networks (WMSNs) has become vital because of the excessive volume of data produced by multimedia sensors. As part of this motivation, this paper proposes a fusion-based WMSN framework that reduces the amount of data to be transmitted over the network by intra-node processing. This framework explores three main issues: (1) the design of a wireless multimedia sensor (WMS) node to detect objects using machine learning techniques; (2) a method for increasing the accuracy while reducing the amount of information transmitted by the WMS nodes to the base station, and; (3) a new cluster-based routing algorithm for the WMSNs that consumes less power than the currently used algorithms. In this context, a WMS node is designed and implemented using commercially available components. In order to reduce the amount of information to be transmitted to the base station and thereby extend the lifetime of a WMSN, a method for detecting and classifying objects on three different layers has been developed. A new energy-efficient cluster-based routing algorithm is developed to transfer the collected information/data to the sink. The proposed framework and the cluster-based routing algorithm are applied to our WMS nodes and tested experimentally. The results of the experiments clearly demonstrate the feasibility of the proposed WMSN architecture in the real-world surveillance applications.",https://ieeexplore.ieee.org/document/8752358/,IEEE Access,2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/90.759332,A QoS-provisioning neural fuzzy connection admission controller for multimedia high-speed networks,IEEE,Journals,"This paper proposes a neural fuzzy approach for connection admission control (CAC) with QoS guarantee in multimedia high-speed networks. Fuzzy logic systems have been successfully applied to deal with traffic-control-related problems and have provided a robust mathematical framework for dealing with real-world imprecision. However, there is no clear and general technique to map domain knowledge on traffic control onto the parameters of a fuzzy logic system. Neural networks have learning and adaptive capabilities that can be used to construct intelligent computational algorithms for traffic control. However, the knowledge embodied in conventional methods is difficult to incorporate into the design of neural networks. The proposed neural fuzzy connection admission control (NFCAC) scheme is an integrated method that combines the linguistic control capabilities of a fuzzy logic controller and the learning abilities of a neural network. It is an intelligent implementation so that it can provide a robust framework to mimic experts' knowledge embodied in existing traffic control techniques and can construct efficient computational algorithms for traffic control. We properly choose input variables and design the rule structure for the NFCAC controller so that it can have robust operation even under dynamic environments. Simulation results show that compared with a conventional effective-bandwidth-based CAC, a fuzzy-logic-based CAC, and a neural-net-based CAC, the proposed NFCAC can achieve superior system utilization, high learning speed, and simple design procedure, while keeping the QoS contract.",https://ieeexplore.ieee.org/document/759332/,IEEE/ACM Transactions on Networking,Feb. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/49.481706,A multimedia synchronization model and its implementation in transport protocols,IEEE,Journals,"An implementation of a synchronization mechanism in transport protocol to support multimedia applications over a packet or cell switched network is proposed. In designing such a mechanism for practical use, ease of implementation and capability of handling random delay of packets are two key issues for success. Since the random delay of packet or cell switched networks makes synchronization among media more complicated after the transmission across the network, a model which considers the random transmission delay is hence required to specify the temporal relationship among media. Therefore, a real-time synchronization model is presented to satisfy this requirement in the paper. Based on the proposed synchronization model, a transport protocol, namely the multimedia synchronization transport protocol (MSTP), is designed and implemented. We have implemented a prototype system using the MSTP protocol and built a simulation system for the wide area network (WAN) environment in order to evaluate the performance of the MSTP protocol.",https://ieeexplore.ieee.org/document/481706/,IEEE Journal on Selected Areas in Communications,Jan. 1996,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1093/comjnl/bxh165,Controlling Multimedia QoS in the Future Home Network Using the PSQA Metric,OUP,Journals,"Home networks are becoming ubiquitous, especially since the advent of wireless technologies such as IEEE 802.11. Coupled with this, there is an increase in the number of broadband-connected homes, and many new services are being deployed by broadband providers, such as TV and VoIP. The home network is thus becoming the ‘media hub’ of the house. This trend is expected to continue, and to expand into the Consumer Electronics (CE) market as well. This means new devices that can tap into the network in order to get their data, such as wireless TV sets, gaming consoles, tablet PCs etc. In this paper, we address the issue of evaluating the QoS provided for those media services, from the end-user's point of view. We present a performance analysis of the home network in terms of perceived quality, and show how our real-time quality assessment technique can be used to dynamically control existing QoS mechanisms. This participates to minimizing resource consumption by tuning the appropriate QoS affecting parameters in order to keep the perceived quality (the ultimate target) within acceptable bounds.",https://ieeexplore.ieee.org/document/8139365/,The Computer Journal,March 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2021.3103074,Families in Wild Multimedia: A Multimodal Database for Recognizing Kinship,IEEE,Journals,"Kinship, a soft biometric detectable in media, is fundamental for a myriad of use-cases. Despite the difficulty of detecting kinship, annual data challenges using still-images have consistently improved performances and attracted new researchers. Now, systems reach performance levels unforeseeable a decade ago, closing in on performances acceptable to deploy in practice. Like other biometric tasks, we expect systems can receive help from other modalities. We hypothesize that adding modalities to <i>Families In the Wild</i> (FIW), which has only still-images, will improve performance. Thus, to narrow the gap between research and reality and enhance the power of kinship recognition systems, we extend FIW with multimedia (MM) data (<i>i</i>.<i>e</i>., video, audio, and text captions). Specifically, we introduce the first publicly available multi-task MM kinship dataset. To build <i>FIW in Multimedia</i> (FIW MM), we developed machinery to automatically collect, annotate, and prepare the data, requiring minimal human input and no financial cost. The proposed MM corpus allows the problem statements to be more realistic template-based protocols. We show significant improvements in all benchmarks with the added modalities. The results highlight edge cases to inspire future research with different areas of improvement. FIW MM supplies the data needed to increase the potential of automated systems to detect kinship in MM. It also allows experts from diverse fields to collaborate in novel ways.",https://ieeexplore.ieee.org/document/9511098/,IEEE Transactions on Multimedia,2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2019.2893549,Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow Detection in SDN: A Social Multimedia Perspective,IEEE,Journals,"The continuous development and usage of multi-media-based applications and services have contributed to the exponential growth of social multimedia traffic. In this context, secure transmission of data plays a critical role in realizing all of the key requirements of social multimedia networks such as reliability, scalability, quality of information, and quality of service (QoS). Thus, a trust-based paradigm for multimedia analytics is highly desired to meet the increasing user requirements and deliver more timely and actionable insights. In this regard, software-defined networks (SDNs) play a vital role; however, several factors such as as-runtime security, and energy-aware networking limit its capabilities to facilitate efficient network control and management. Thus, with the view to enhance the reliability of the SDN, a hybrid deep-learning-based anomaly detection scheme for suspicious flow detection in the context of social multimedia is proposed. It consists of the following two modules: (1) an anomaly detection module that leverages improved restricted Boltzmann machine and gradient descent-based support vector machine to detect the abnormal activities, and (2) an end-to-end data delivery module to satisfy strict QoS requirements of the SDN, that is, high bandwidth and low latency. Finally, the proposed scheme has been experimentally evaluated on both real-time and benchmark datasets to prove its effectiveness and efficiency in terms of anomaly detection and data delivery essential for social multimedia. Further, a large-scale analysis over a Carnegie Mellon University (CMU)-based insider threat dataset has been conducted to identify its performance in terms of detecting malicious events such as-Identity theft, profile cloning, confidential data collection, etc.",https://ieeexplore.ieee.org/document/8613868/,IEEE Transactions on Multimedia,March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSE.2022.3153925,Multimedia Traffic Classification for Imbalanced Environment,IEEE,Journals,"With ever-increasing volume and variety of multimedia traffic on the Internet, machine learning-empowered techniques nowadays tend to become indispensable for future intelligent network management. To realize automatic traffic management with Quality of Service (QoS) guarantees, there is a pressing need for accurate traffic classification. However, the inherent characteristics of networks cause imbalanced class distribution in traffic classification, which could degrade the performance of classification, especially on the minority classes. To address the issue of class imbalance in both stationary and nonstationary environments, this paper proposes a novel scheme called CHS (chain hierarchical structure) which is able to characterize class distribution from a new perspective. By building an error model, we can compute the error propagation generated by CHS and analyze the factors that affect it. More importantly, two key methods involving classifier ranking and combination with the hierarchical structure are devised to mitigate the error propagation produced by the classifier. The effectiveness of the developed framework is validated through experiments over two real-world traffic datasets in both stationary and nonstationary environments. The experimental results demonstrate that our proposed methods outperform the state-of-the-art approaches in terms of classification accuracy and running time. The proposed methods are particularly effective in the nonstationary imbalanced environment.",https://ieeexplore.ieee.org/document/9720731/,IEEE Transactions on Network Science and Engineering,1 May-June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIP.2009.2035228,Online Reinforcement Learning for Dynamic Multimedia Systems,IEEE,Journals,"In our previous work, we proposed a systematic cross-layer framework for dynamic multimedia systems, which allows each layer to make autonomous and foresighted decisions that maximize the system's long-term performance, while meeting the application's real-time delay constraints. The proposed solution solved the cross-layer optimization offline, under the assumption that the multimedia system's probabilistic dynamics were known a priori, by modeling the system as a layered Markov decision process. In practice, however, these dynamics are unknown a priori and, therefore, must be learned online. In this paper, we address this problem by allowing the multimedia system layers to learn, through repeated interactions with each other, to autonomously optimize the system's long-term performance at run-time. The two key challenges in this layered learning setting are: (i) each layer's learning performance is directly impacted by not only its own dynamics, but also by the learning processes of the other layers with which it interacts; and (ii) selecting a learning model that appropriately balances time-complexity (i.e., learning speed) with the multimedia system's limited memory and the multimedia application's real-time delay constraints. We propose two reinforcement learning algorithms for optimizing the system under different design constraints: the first algorithm solves the cross-layer optimization in a centralized manner and the second solves it in a decentralized manner. We analyze both algorithms in terms of their required computation, memory, and interlayer communication overheads. After noting that the proposed reinforcement learning algorithms learn too slowly, we introduce a complementary accelerated learning algorithm that exploits partial knowledge about the system's dynamics in order to dramatically improve the system's performance. In our experiments, we demonstrate that decentralized learning can perform equally as well as centralized learning, while enabling the layers to act autonomously. Additionally, we show that existing application-independent reinforcement learning algorithms, and existing myopic learning algorithms deployed in multimedia systems, perform significantly worse than our proposed application-aware and foresighted learning methods.",https://ieeexplore.ieee.org/document/5299114/,IEEE Transactions on Image Processing,Feb. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2017.2692650,Overlapping Community Detection for Multimedia Social Networks,IEEE,Journals,"Finding overlapping communities from multimedia social networks is an interesting and important problem in data mining and recommender systems. However, extant overlapping community discovery with swarm intelligence often generates overlapping community structures with superfluous small communities. To deal with the problem, in this paper, an efficient algorithm (LEPSO) is proposed for overlapping communities discovery, which is based on line graph theory, ensemble learning, and particle swarm optimization (PSO). Specifically, a discrete PSO, consisting of an encoding scheme with ordered neighbors and a particle updating strategy with ensemble clustering, is devised for improving the optimization ability to search communities hidden in social networks. Then, a postprocessing strategy is presented for merging the finer-grained and suboptimal overlapping communities. Experiments on some real-world and synthetic datasets show that our approach is superior in terms of robustness, effectiveness, and automatically determination of the number of clusters, which can discover overlapping communities that have better quality than those computed by state-of-the-art algorithms for overlapping communities detection.",https://ieeexplore.ieee.org/document/7895160/,IEEE Transactions on Multimedia,Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3039695,Reliable Multi-Object Tracking Model Using Deep Learning and Energy Efficient Wireless Multimedia Sensor Networks,IEEE,Journals,"Presently, sensor-cloud based environment becomes highly beneficial due to its applicability in several domains. Wireless multimedia sensor network (WMSN) is one among them, which involves a set of multimedia sensors to collect data about the deployed region. Compared to traditional object tracking models, animal tracking in WMSN is a tedious process owing to the harsh, dynamic, and energy limited sensors. This article introduces a new Reliable Multi-Object Tracking Model using Deep Learning (DL) and Energy Efficient WMSN. Initially, the fuzzy logic technique is employed to determine the cluster heads (CHs) to attain energy efficiency. Next, in the second stage, a novel tracking algorithm by the use of Recurrent Neural Network (RNN) with a tumbling effect called RNN-T is developed. The proposed RNN-T model gets executed by every sensor node and the CHs execute the tracking algorithm to track the animals. Finally, the tracking results are transmitted to the cloud server for investigation purposes. In order to assess the performance of the presented model, an extensive experimental analysis is carried out by the use of a real-time wildlife video. The obtained results ensured that the RNN-T model has achieved better performance over the compared methods in different aspects.",https://ieeexplore.ieee.org/document/9265258/,IEEE Access,2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TKDE.2003.1161587,Semantic abstractions in the multimedia domain,IEEE,Journals,"Information searching by exactly matching content is traditionally a strong point of machine searching; this is not, however, how human memory works and is rarely satisfactory for advanced retrieval tasks in any domain-multimedia in particular, where the presentational aspects can be equally important to the semantic information content. A combined abstraction of the conceptual and presentational characteristics of multimedia applications, leading on the one hand to their conceptual structure (with classic semantics of the real-world modeled by entities, relationships, and attributes) and on the other to the presentational structure (including media type, logical structure, temporal synchronization, spatial (on the screen) ""synchronization"" and interactive behavior) is developed in this paper. Multimedia applications are construed as consisting of ""Presentational Units:"" elementary (a media object with play duration and screen position), and composite (recursive structures of PUs in the temporal, spatial, and logical dimensions). The fundamental concept introduced is that of Semantic Multimedia Abstractions (SMA): qualitative abstract descriptions of multimedia applications in terms of their conceptual and presentational properties at an adjustable level of abstraction. SMAs, which could be viewed as metadata, form an abstract space to be queried. A detailed study of possible abstractions (from multimedia applications to SMAs and SMA-to-SMA), a definition and query language for Semantic Multimedia Abstractions (SMA-L) and the corresponding SMA model (equivalent to extended OMT), as well as an implementation of a system capable of wrapping the presentational structure of XML-based documents complete this work, whose contribution lays in the classically fruitful boundary between AI, software engineering, and database research.",https://ieeexplore.ieee.org/document/1161587/,IEEE Transactions on Knowledge and Data Engineering,Jan.-Feb. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2017.2749160,Supervised Distributed Hashing for Large-Scale Multimedia Retrieval,IEEE,Journals,"Recent years have witnessed the growing popularity of hashing for large-scale multimedia retrieval. Extensive hashing methods have been designed for data stored in a single machine, that is, centralized hashing . In many real-world applications, however, the large-scale data are often distributed across different locations, servers, or sites. Although hashing for distributed data can be implemented by assembling all distributed data together as a whole dataset in theory, it usually leads to prohibitive computation, communication, and storage costs in practice. Up to now, only a few methods were tailored for distributed hashing, which are all unsupervised approaches. In this paper, we propose an efficient and effective method called supervised distributed hashing (SupDisH), which learns discriminative hash functions by leveraging the semantic label information in a distributed manner. Specifically, we cast the distributed hashing problem into the framework of classification, where the learned binary codes are expected to be distinct enough for semantic retrieval. By introducing auxiliary variables, the distributed model is then separated into a set of decentralized subproblems with consistency constraints, which can be solved in parallel on each vertex of the distributed network. As such, we can obtain high-quality distinctive unbiased binary codes and consistent hash functions with low computational complexity, which facilitate tackling large-scale multimedia retrieval tasks involving distributed datasets. Experimental evaluations on three large-scale datasets show that SupDisH is competitive to centralized hashing methods and outperforms the state-of-the-art unsupervised distributed method significantly.",https://ieeexplore.ieee.org/document/8025608/,IEEE Transactions on Multimedia,March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JIOT.2021.3075336,VID-WIN: Fast Video Event Matching With Query-Aware Windowing at the Edge for the Internet of Multimedia Things,IEEE,Journals,"Efficient video processing is a critical component in many IoMT applications to detect events of interest. Presently, many window optimization techniques have been proposed in event processing with an underlying assumption that the incoming stream has a structured data model. Videos are highly complex due to the lack of any underlying structured data model. Video stream sources, such as CCTV cameras and smartphones are resource-constrained edge nodes. At the same time, video content extraction is expensive and requires computationally intensive deep neural network (DNN) models that are primarily deployed at high-end (or cloud) nodes. This article presents VID-WIN, an adaptive 2-stage allied windowing approach to accelerate video event analytics in an edge-cloud paradigm. VID-WIN runs parallelly across edge and cloud nodes and performs the query and resource-aware optimization for state-based complex event matching. VID-WIN exploits the video content and DNN input knobs to accelerate the video inference process across nodes. This article proposes a novel content-driven microbatch resizing, query-aware caching, and microbatch-based utility filtering strategy of video frames under resource-constrained edge nodes to improve the overall system throughput, latency, and network usage. Extensive evaluations are performed over five real-world data sets. The experimental results show that VID-WIN video event matching achieves ~ 2.3× higher throughput with minimal latency and ~99% bandwidth reduction compared to other baselines while maintaining query-level accuracy and resource bounds.",https://ieeexplore.ieee.org/document/9411841/,IEEE Internet of Things Journal,"1 July1, 2021",ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct51615.2020.00044,3D human model creation on a serverless environment,IEEE,Conferences,"The creation of realistic 3D human model is traditionally timeconsuming and cumbersome, and is typically done by professionals. In recent years computer vision technologies can assist in generating human models from controlled environments, we demonstrate a different but easy capturing scenario with less constraints on the subject or the environmental setup. The reconstruction process for 3D human model consists of various intermediate process such as semantic human segmentation, human skeletal keypoint detection, and texture generation. In order to achieve easy, scalable, and flexible deployment to different cloud environments, we have chosen the serverless architecture to offload some common service functionalities to the cloud infrastructure but focused on the core task,which is the reconstruction itself. The event-driven serverless architecture eases the building of such multimedia web services with minimal coding efforts, but simply defines the APIs and declares the APIs with correspondent lambda functions. The proposed approach in this paper allow anyone with a mobile phone to generate 3D models easily and quickly in the scale of few 2-3 minutes, rather than hours.",https://ieeexplore.ieee.org/document/9287825/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2002.1045680,A MPEG-4 virtual human animation engine for interactive web based applications,IEEE,Conferences,"This paper presents a novel, MPEG-4 compliant animation engine (body player). It has been designed to synthesize virtual human full-body animations in interactive multimedia applications for the web. We believe that a full-body player can provide a more expressive and interesting interface than the use of animated faces only (talking heads). This is one of the first implementations of a MPEG-4 animation engine with deformable models (it uses the MPEG-4 Body Definition Parameters and Deformation Tables). Several potential applications are overviewed. This software tool was developed in the framework of the IST-INTERFACE European project.",https://ieeexplore.ieee.org/document/1045680/,Proceedings. 11th IEEE International Workshop on Robot and Human Interactive Communication,27-27 Sept. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2015.65,A Neural Network Based Handover Management Strategy for Heterogeneous Networks,IEEE,Conferences,"One of the key challenges for improvement of quality of services (QoS) in Heterogeneous wireless networks is the design of Vertical Handover (VHO) Management strategy. VHO is required to guide the decision for a mobile terminal (MT) to handoff between different types of networks. This is an essential task to cope with various multimedia services QoS settings. In this paper, we present a machine learning scheme based on Neural Network for calls vertical handover in heterogeneous networks. The Neural Network Based Handover Management Scheme (NNBHMS) of this paper aims toward achieving seamless connectivity and Always Best Connected (ABC) call status for group mobility over a set of heterogeneous networks. The proposed scheme evaluates and creates relationships between different decision criteria related to heterogeneous networks conditions, terminal capabilities, application requirements, and user preferences. Afterward, the estimates of each attribute are forwarded to neural network to select the optimal access network. The proposed scheme is applied for vertical handover Management in heterogeneous networks offering both real time services (voice over IP services), and data Services (packet data traffic). Through the implementation of neural networks based machine learning approach, the proposed research scheme allows solving the complexity of the handover decision process resulting from the multitude dimensions of the decision criteria and the dynamicity of many of its components. The performance results evaluated through simulation show that the use of the a neural network based machine learning scheme to carry out the Handover process can enhance the QoS perceived by both types of voice and data service while fulfilling to great extent the user preference.",https://ieeexplore.ieee.org/document/7424486/,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA),9-11 Dec. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2007.4450948,A New Color Based Optical Flow Algorithm for Environment Mapping Using a Mobile Robot,IEEE,Conferences,"Environment mapping from a video sequence is considered to be one of the most important problems in computer vision because of its application in surveillance, virtual reality, autonomous navigation, multimedia communications, medical prognosis, etc. In this paper, we have presented an optical flow based method for environment mapping. It uses a new color based optical flow computation technique. The camera, which is mounted on a mobile robot, is kept perpendicular to the direction of motion, and the captured set of images is used to compute the dense depth map. We have used a Kalman filter to denoise the depth map.",https://ieeexplore.ieee.org/document/4450948/,2007 IEEE 22nd International Symposium on Intelligent Control,1-3 Oct. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSNT53786.2021.9615487,A Novel Face Forgery Detection Method Based on Edge Details and Reused-Network,IEEE,Conferences,"In recent years, with the widespread popularity of digital media editing software and rapid development of deep learning techniques, the technology barriers for generating fake faces becomes lower and lower. As a result, forged face images have become more and more realistic, and a large number of high-quality fake face images are created and spread in our day lives. Therefore, determining the authenticity of face images is a challenging task and major focus of multimedia security research. To solve this problem, a novel face forgery detection method based on edge details and Reused-Network (RN) is proposed. Specifically, based on Sobel and Laplacian operators, we consider the differences in edge details between real and fake faces, and further learn these differences through RN to eventually extract more detailed features. The derived features are then used to train a Support Vector Machine (SVM) classifier for classification. The proposed method has a good performance on detecting fake face images, and it is experimentally verified better than some state-of-the-art works.",https://ieeexplore.ieee.org/document/9615487/,2021 IEEE 9th International Conference on Computer Science and Network Technology (ICCSNT),22-24 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOCOM.2007.144,A Novel Scheduling Algorithm for Video Traffic in High-Rate WPANs,IEEE,Conferences,"The emerging high-rate wireless personal area network (WPAN) technology is capable of supporting high-speed and high-quality real-time multimedia applications. In particular, MPEG-4 video streams are deemed to be a widespread traffic type. However, in the current IEEE 802.15.3 standard for media access control (MAC) of high-rate WPANs, the implementation details of some key issues such as scheduling and quality of service (QoS) provisioning have not been addressed. In this paper, we first propose a mathematical model for the optimal scheduling scheme for MPEG-4 flows in high-rate WPANs. We also propose an RL scheduler based on the reinforcement learning (RL) technique. Simulation results show that our proposed RL scheduler achieves nearly optimal performance and performs better than F-SRPT (Mangharam et al., 2004), EDD+SRPT (Torok et al., 2005), and PAP (Kim and Cho, 2005) scheduling algorithms in terms of a lower decoding failure rate.",https://ieeexplore.ieee.org/document/4411054/,IEEE GLOBECOM 2007 - IEEE Global Telecommunications Conference,26-30 Nov. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRI.2019.00040,A Power Efficient Neural Network Implementation on Heterogeneous FPGA and GPU Devices,IEEE,Conferences,"Deep neural networks (DNNs) have seen tremendous industrial successes in various applications, including image recognition, machine translation, audio processing, etc. However, they require massive amounts of computations and take a lot of time to process. This quickly becomes a problem in mobile and handheld devices where real-time multimedia applications such as face detection, disaster management, and CCTV require lightweight, fast, and effective computing solutions. The objective of this project is to utilize specialized devices such as Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) in a heterogeneous computing environment to accelerate the deep learning computations with the constraints of power efficiency. We investigate an efficient DNN implementation and make use of FPGA for fully-connected layer and GPU for floating-point operations. This requires the deep neural network architecture to be implemented in a model parallelism system where the DNN model is broken down and processed in a distributed fashion. The proposed heterogeneous framework idea is implemented using an Nvidia TX2 GPU and a Xilinx Artix-7 FPGA. Experimental results indicate that the proposed framework can achieve faster computation and much lower power consumption.",https://ieeexplore.ieee.org/document/8843495/,2019 IEEE 20th International Conference on Information Reuse and Integration for Data Science (IRI),30 July-1 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.38,A Secure Scheme for Image Transformation,IEEE,Conferences,"The dynamic growth in the field of Information communication has hiked the ease of Information transmission. But this type of real advancement in other sense has explored so many possibilities of information being snooped, at the time of communication in between the sender and the intended receiver. So, day by day Information Security is becoming an inseparable part of Computing and Communication. Various tools are available in order to address those issues and in them Steganography plays a significant role. In this paper, a heuristic approach for Information hiding in the form of multimedia objects or text using Steganography is proposed keeping in mind two considerations - Size and degree of security. Here, the Information is hidden behind an image in the form of an encoded matrix which is obtained from the bit matrix of the embedded object.",https://ieeexplore.ieee.org/document/4617419/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2002.1176800,A UML extension of distributed system,IEEE,Conferences,"UML provides a lack of efficient support for distributed features. This paper provides an extension of UML to solve this problem. The extension changes the use case diagram to be active and multi-level for requirement engineering of distributed systems, and adds control of time and support for the complex message mechanism in order to model the distributed applications associated with real-time and multimedia systems. The ideas derived from the paper are of interest to software engineering over the Internet.",https://ieeexplore.ieee.org/document/1176800/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTEST.2018.8397165,A classifier-based test oracle for embedded software,IEEE,Conferences,"Despite great advances in different software testing areas, one important challenge, achieving an automated test oracle, has been overlooked by academia and industry. Among various approaches for constructing a test oracle, machine learning techniques have been successful in recent years. However, there are some situations in which the existing machine learning based oracles have deficiencies. These situations include testing of applications with low observability, such as embedded software and multimedia software programs. There are also cases in testing embedded software in which explicit historical data in form of input-output relationships is not available, and situations in which the comparison between expected results and actual outputs is impossible or hard. Addressing these deficiencies, this paper proposes a new black box solution to construct automated oracles which can be applied to embedded software and other programs with low observability. To achieve this, we have employed an Artificial Neural Network (ANN) algorithm to build a model which merely requires program's input values as well as corresponding pass/fail outcome, as the training set. We have conducted extensive experiments on several benchmarks. The results manifest the applicability of the proposed approach to software systems with low observability as well as its higher accuracy in comparison to a well-known machine learning based method.",https://ieeexplore.ieee.org/document/8397165/,2018 Real-Time and Embedded Systems and Technologies (RTEST),9-10 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSM.2016.7818394,A connectionist approach to dynamic resource management for virtualised network functions,IEEE,Conferences,"Network Functions Virtualisation (NFV) continues to gain attention as a paradigm shift in the way telecommunications services are deployed and managed. By separating Network Functions (NFs) from traditional middleboxes, NFV is expected to lead to reduced CAPEX and OPEX, and to more agile services. However, one of the main challenges to achieving these objectives is on how physical resources can be efficiently, autonomously, and dynamically allocated to Virtualised Network Functions (VNFs) whose resource requirements ebb and flow. In this paper, we propose a Graph Neural Network (GNN)-based algorithm which exploits Virtual Network Function Forwarding Graph (VNF-FG) topology information to predict future resource requirements for each Virtual Network Function Component (VNFC). The topology information of each VNFC is derived from combining its past resource utilisation as well as the modelled effect on the same from VNFCs in its neighbourhood. Our proposal has been evaluated using a deployment of a virtualised IP Multimedia Subsystem (IMS), and real VoIP traffic traces, with results showing an average prediction accuracy of 90%. Moreover, compared to a scenario where resources are allocated manually and/or statically, our proposal reduces the average number of dropped calls by at least 27% and improves call setup latency by over 29%.",https://ieeexplore.ieee.org/document/7818394/,2016 12th International Conference on Network and Service Management (CNSM),31 Oct.-4 Nov. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNP.1995.524849,A neural network approach to multicast routing in real-time communication networks,IEEE,Conferences,"Real-time communication networks are designed mainly to support multimedia applications, especially the interactive ones, which require a guarantee of Quality of Service (QoS). Moreover, multicasting is needed as there are usually more than two peers who communicate together using multimedia applications. As for the routing, the network has to find an optimum (least cost) multicast route, that has enough resources to provide or guarantee the required QoS. This problem is called QoS constrained multicast routing and was proved to be an NP-complete problem. In contrast to the existing heuristic approaches, in this paper we propose a modified version of a Hopfield neural network model to solve QoS (delay) constrained multicast routing. By the massive parallel computation of neural networks, it can find a near optimal multicast route very fast, when implemented in hardware. Simulation results show that the proposed model has performance near to the optimal solution and comparable to existing heuristics.",https://ieeexplore.ieee.org/document/524849/,Proceedings of International Conference on Network Protocols,7-10 Nov. 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFCOM.2004.1354501,A new approach for serving radio network controller relocation in UMTS all-IP network,IEEE,Conferences,"To support real-time multimedia services in UMTS all-IP network, 3GPP TR 25.936 proposed two approaches to support real-time serving radio network controller (SRNC) switching, which require packet duplication during SRNC relocation. These approaches significantly consume extra system resources. This paper proposes the fast SRNC relocation (FSR) approach that does not duplicate packets. In FSR, a packet buffering mechanism is implemented to avoid packet loss at the target RNC. We propose an analytic model to investigate the performance of FSR. The numerical results show that packet loss at the source RNC can be ignored. Furthermore, the expected number of packets buffered at the target RNC is small, which does not prolong packet delay.",https://ieeexplore.ieee.org/document/1354501/,IEEE INFOCOM 2004,7-11 March 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC.2010.5583349,A new method of image data fusion based on FNN,IEEE,Conferences,"The image data are obtained by a variety of multimedia, information equipment, which include amount of information, extensive coverage, and redundancy in ubiquitous computing paradigm. In order to make use of these information reasonably and efficiently, it is necessary to fuse such massive data, therefore the multi-sensor image information fusion become a key technology to ubiquitous computing. A new approach of image data fusion based on fuzzy neural network (FNN) is presented in this paper. Here, neural network is a parallel information processing model, there are more adaptive and self-organization, and can accomplish the complexity of real-time computing and mass data retrieval, it demonstrate its unique superiority of image understanding, pattern recognition and the handling of incomplete information. In the new approach, fuzzy theory has the advantages such as: easy-to-understand, flexible and inclusive of non-precise data, as well as using expert knowledge and based on the natural language. The fuzzy neural network system developed by us has used in multi-sensor image fusion, proved that the fusion is fast, effective, good, and can meet the real-time requirements of ubiquitous computing.",https://ieeexplore.ieee.org/document/5583349/,2010 Sixth International Conference on Natural Computation,10-12 Aug. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMPDP.1999.746650,A parallel processor for neural networks,IEEE,Conferences,"We present two different algorithms implemented through neural networks on a multiprocessor device. The parallel single-chip TI TMS32C80 Multimedia Video Processor (MVP). The goal of this experimentation is to test, on real problems, the performance of this powerful unit made up by one Master Risc Processor and by four Slave Digital Signal Processors (DSPs) and to evaluate its suitability to neural network applications. The first problem implemented is a typical classification algorithm in which the network recognises which points belong to different regions inside a 2D space. The second problem is more computationally heavy and consists of a network able to recognise 'handwritten' digits. The parallel version of the first algorithm, was also tested on a commercially available supercomputer.",https://ieeexplore.ieee.org/document/746650/,Proceedings of the Seventh Euromicro Workshop on Parallel and Distributed Processing. PDP'99,3-5 Feb. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBNMT.2009.5348540,A self-learning multicast routing algorithm for multi-rate WiFi mesh network,IEEE,Conferences,"Multicast routing in multirate mesh network wireless networks is challenging, mainly due to multirate transmissions and quality of services guarantee. In this paper, we present a self-learning multicast routing algorithm for multirate WiFi (Wireless Fidelity) mesh network. First, we design a self predict routing information mechanism to predict the delay and bandwidth of every multicast tree. The trees which delay and bandwidth can meet the multimedia service needs will be chosen into the metric compare procedure. Then, we introduce the time slot, bandwidth and transmit rate into the new metric function. Moreover, a real-time automatic monitor is implemented to guarantee the delay in the time varying wireless channel. The proposed algorithm runs in the same running time as regular shortest-path algorithms and bandwidth-constraint algorithms. The experiments results show that self-learning multicast routing algorithm is capable of establishing a route which has higher time slot efficient and network performance with delay and bandwidth constraint.",https://ieeexplore.ieee.org/document/5348540/,2009 2nd IEEE International Conference on Broadband Network & Multimedia Technology,18-20 Oct. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2016.7591249,AMP: A platform for managing and mining data in the treatment of Autism Spectrum Disorder,IEEE,Conferences,"We introduce AMP (Autism Management Platform), an integrated health care information system for capturing, analyzing, and managing data associated with the diagnosis and treatment of Autism Spectrum Disorder in children. AMP's mobile application simplifies the means by which parents, guardians, and clinicians can collect and share multimedia data with one another, facilitating communication and reducing data redundancy, while simplifying retrieval. Additionally, AMP provides an intelligent web interface and analytics platform which allow physicians and specialists to aggregate and mine patient data in real-time, as well as give relevant feedback to automatically learn data filtering preferences over time. Together AMP's mobile app, web client, and analytics engine implement a rich set of features that streamline the data collection and analysis process in the context of a secure and easy-to-use system so that data may be more effectively leveraged to guide treatment.",https://ieeexplore.ieee.org/document/7591249/,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),16-20 Aug. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2009.5414103,Age regression from faces using random forests,IEEE,Conferences,Predicting the age of a person through face image analysis holds the potential to drive an extensive array of real world applications from human computer interaction and security to advertising and multimedia. In this paper the first application of the random forest for age regression is proposed. This method offers the advantage of few parameters that are relatively easy to initialize. Our method learns salient anthropometric quantities without a prior model. Significant implications include a dramatic reduction in training time while maintaining high regression accuracy throughout human development.,https://ieeexplore.ieee.org/document/5414103/,2009 16th IEEE International Conference on Image Processing (ICIP),7-10 Nov. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BMSB53066.2021.9547185,An Optimal and Lightweight Convolutional Neural Network for Performance Evaluation in Smart Cities based on CAPTCHA Solving,IEEE,Conferences,"Multimedia Internet of Things (IoT) devices, especially, the smartphones are embedded with sensors including Global Positioning System (GPS), barometer, microphone, accelerometer, etc. These sensors working together, present a fairly complete picture of the citizens' daily activities, with implications for their privacy. With the internet, Citizens in Smart Cities are able to perform their daily life activities online with their connected electronic devices. But, unfortunately, computer hackers tend to write automated malicious applications to attack websites on which these citizens perform their activities. These security threats sometime put their private information at risk. In order to prevent these security threats on websites, Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHAs) are generated, as a form of security mechanism to protect the citizens' private information. But with the advancement of deep learning, text-based CAPTCHAs can sometimes be vulnerable. As a result, it is essential to conduct performance evaluation on the CAPTCHAs that are generated before they are deployed on multimedia web applications. Therefore, this work proposed an optimal and light-weight Convolutional Neural Network (CNN) to solve both numerical and alpha-numerical complex text-based CAPTCHAs simultaneously. The accuracy of the proposed CNN model has been accelerated based on Cyclical Learning Rates (CLRs) policy. The proposed CLR-CNN model achieved a high accuracy to solve both numerical and alpha-numerical text-based CAPTCHAs of 99.87% and 99.66%, respectively. In real-time, we observed that the speed of the model has increased, the model is lightweight, stable, and flexible as compared to other CAPTCHA solving techniques. The result of this current work will increase awareness and will assist multimedia security Researchers to continue and develop more robust text-based CAPTCHAs with their security mechanisms capable of protecting the private information of citizens in Smart Cities.",https://ieeexplore.ieee.org/document/9547185/,2021 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),4-6 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAIAM53259.2021.00012,Application of children Artificial Intelligence science popularization books based on Augmented Reality technology,IEEE,Conferences,"Children’s books are enlightenment books for children’s education, and the educational significance of popular science books is even more important. With the development of global intelligent technology, parents pay more attention to children’s intelligent education, and books combined with science and technology become more popular. As one of the important tools in the process of children’s growth, books play an important role in children’s early education. However, most of the popular science books are still natural universe, animal world, military education and other subjects. Today, with the development of intelligent technology, it has become an inevitable thing to carry out artificial intelligence science popularization for children. Therefore, this article uses multimedia technology, uses Kudan SDK to develop AR applications in Unity, explores the application of AR technology in popular science books, and uses artificial intelligence as the theme for creation. It mainly includes the cognitive analysis of artificial intelligence, ar software system design and the design exploration of AR and books for 7-11 year olds, so that children can understand and learn the relevant knowledge of AI in the technical environment.",https://ieeexplore.ieee.org/document/9516578/,2021 International Symposium on Artificial Intelligence and its Application on Media (ISAIAM),21-23 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DFT.2014.6962066,Artificial intelligence based task mapping and pipelined scheduling for checkpointing on real time systems with imperfect fault detection,IEEE,Conferences,"Fault-tolerance is emerging as one of the important optimization objectives for designs in deep submicron technology nodes. This paper proposes a technique of application mapping and scheduling with checkpointing on a multiprocessor system to maximize the reliability considering transient faults. The proposed model incorporates checkpoints with imperfect fault detection probability, and pipelined execution and cyclic dependency associated with multimedia applications. This is solved using an Artificial Intelligence technique known as Particle Swarm Optimization to determine the number of checkpoints of every task of the application that maximizes the confidence of the output. The proposed approach is validated experimentally with synthetic and real-life application graphs. Results demonstrate the proposed technique improves the probability of correct result by an average 15% with imperfect fault detection. Additionally, even with 100% fault detection, the proposed technique is able to achieve better results (25% higher confidence) as compared to the existing fault-tolerant techniques.",https://ieeexplore.ieee.org/document/6962066/,2014 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT),1-3 Oct. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WICOM.2009.5301778,Audio Segmentation in AAC Domain for Content Analysis,IEEE,Conferences,"We focus the attention on the audio scene segmentation in AAC domain for audio-based multimedia indexing and retrieval applications. In particular, a MFCC extraction method is proposed, which is adaptive to the window switch in AAC encoding process, and independent of the audio sampling frequency. We discuss the fusion method of MFCC features, which came from different window type in order to keep the balance of the frequency and temporal resolution. A series of experiments via the probability distribution of MFCC were implemented to test the effective in audio scene segmentation. The experimental results show that such approach based on compression domain can approach the performance of the system based on PCM audio, and the CPU overload decreased dramatically. It is meaningful to the real time analysis of audio content.",https://ieeexplore.ieee.org/document/5301778/,"2009 5th International Conference on Wireless Communications, Networking and Mobile Computing",24-26 Sept. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CISTI.2019.8760601,Auxiliary System of Unified Recognition,IEEE,Conferences,"Auxiliary System of Unified Recognition consists of the detection/recognition of faces and automotive plates through videos captured from surveillance cameras in the Brazilian city of Três Lagoas. This system uses free software and artificial intelligence techniques to aid in local public safety. The software used is OpenCV, Tesseract-OCR, PostgreSQL, Debian GNU/Linux and C++ language. The hardware used is high resolution cameras that transmit multimedia information through the Real Time Streaming Protocol (RTSP).",https://ieeexplore.ieee.org/document/8760601/,2019 14th Iberian Conference on Information Systems and Technologies (CISTI),19-22 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BSC.2010.5472936,Cognitive mesh network resource adaptations using reinforcement learning,IEEE,Conferences,"In this paper, we consider the cognitive network (CR) that can provide a means for offering quality of service (QoS) required by real time services, streaming multimedia and other applications. We consider the approach where the licensed users (primary users, PUs) rent the surplus spectrum to unlicensed users (secondary users, SUs) to get some reward. However, when PU leases more spectrum to the SUs, its quality of service (QoS) is degraded due to a reduction of spectrum. This complex contradicting requirement is embedded in our reinforcement learning (RL) model that is developed and implemented as shown in this paper. Available spectrum is managed by the PU which executes control policy to provide end-to-end QoS connection to the SUs as well as maximizing its revenues. Maximizing profit is a key objective for the PU. In this work, we propose a novel resource management approach in the radio environment. RL is used as a means for extracting an optimal policy that helps a PU to adapt to the changing radio environment conditions, so that the PU profit is maximized continuously over time. The approach integrates different requirements such as rewards for PUs, and the cost of spectrum. Performance evaluation of the proposed spectrum sharing approach shows that the approach is effective at attaining maximized revenue under varying network conditions.",https://ieeexplore.ieee.org/document/5472936/,2010 25th Biennial Symposium on Communications,12-14 May 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISNCC.2015.7238588,Cognitive radio networks management using an ANFIS approach with QoS/QoE mapping scheme,IEEE,Conferences,"Future networks are characterized by a panoply of novel services based on multimedia services like gaming and real time video streaming. In addition, cognitive radio is considered as an emergent candidate of next generation (NextG) networks. Therefore, there is a need of some techniques that can ensure self-managed networks with learning capabilities. Our approach is based on adaptive neuro-fuzzy inference system (ANFIS) used for predicting the user video perception (e.g. MOS) and for managed decisions that can be achieved by a specific radio configuration (e.g. data rate, handover). The ANFIS model with Quality of Services/Quality of Experience (QoS/QoE) mapping is able to sense environment, decide, learn and optimize its decisions online by a learning algorithm that uses a set of experimental measurements. We used an implementation tool of the ANFIS model under MATLAB/SIMULINK environment supporting the development of real time scenarios.",https://ieeexplore.ieee.org/document/7238588/,"2015 International Symposium on Networks, Computers and Communications (ISNCC)",13-15 May 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NOMS.2018.8406297,Complex solution for VoLTE monitoring and cross-protocol data analysis,IEEE,Conferences,"Among the various challenges of network and service monitoring, complexity is one of the hardest to overcome. The practical situation for cellular mobile networks and their services are especially troublesome, since there are three generations of mobile networks working together, and services are provided across these technologies. A very good example for such operations and maintenance nightmare is the Voice over LTE (VoLTE) service, currently under deployment worldwide. In theory it merely requires the interworking of the EPC (Evolved Packet Core) of LTE and the IMS (IP Multimedia Subsystem) for call control. In reality the 2G and 3G mobile cores get involved as well, when 4G coverage is not satisfactory, and SRVCC (Single Radio Voice Call Continuity) procedures kick in. Complete monitoring, including cross- protocol data analysis of VoLTE control is a key issue that currently in the operators' interest. In this paper we introduce the problem, and provide the general solution as well as special insights for cross-protocol analysis of the VoLTE service. Such analysis results can provide a basis for the next generation of mobile core monitoring, based on supervised learning.",https://ieeexplore.ieee.org/document/8406297/,NOMS 2018 - 2018 IEEE/IFIP Network Operations and Management Symposium,23-27 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTECH.2013.6653728,Contents,IEEE,Conferences,The following topics are dealt with: innovative computing technology; digital image processing; digital video processing; interactive multimedia; network security; information security; cloud computing; e-learning; e-commerce; e-business; e-government; virtual reality; computational intelligence; data mining; network mining; soft computing; fuzzy algorithm; optimization algorithm; human-computer interaction; visualization; artificial intelligence; signal processing; software engineering; and user interface.,https://ieeexplore.ieee.org/document/6653728/,Third International Conference on Innovative Computing Technology (INTECH 2013),29-31 Aug. 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISWPC.2008.4556269,Cooperative agent delay routing approach for manets,IEEE,Conferences,"Real time and multimedia applications are widely deployed on mobile ad hoc networks in the last few years. Such applications have, often, hard requirements in terms of QoS metrics. We are interested in this paper to end-to-end delay required by some applications. We propose a new adaptive routing approach called based on reinforcement learning. Our approach is built around three parts: delay estimation, routing and flooding optimization. From simulation results on NS-2, we note that our adaptive routing algorithm performs best performance compared to QAODV and DOLSR.",https://ieeexplore.ieee.org/document/4556269/,2008 3rd International Symposium on Wireless Pervasive Computing,7-9 May 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCCM.2009.5267918,Cross-layer design of cognitive radio network for real time video streaming transmission,IEEE,Conferences,"High efficiency cross-layer resource management is desired for real-time multimedia streaming application in cognitive radio network. In this paper, a novel multi-user resource management scheme is proposed. Its working principle and information swap mechanism are illustrated in detail. The proposed RDMAL (real time distributed multi-agent learning) algorithm takes advantage of multi-agent learning approach and relies on it to dynamically exploit available frequency channel, while uses available interference information to achieve the learning efficiency. The simulation and preliminary experimental results demonstrate that the proposed RDMAL can achieve lower packet loss rate than state-of-the-art dynamic resource management algorithm and it can decrease the time overheads in terms of the cost of information exchange.",https://ieeexplore.ieee.org/document/5267918/,"2009 ISECS International Colloquium on Computing, Communication, Control, and Management",8-9 Aug. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME.2017.8019468,Deep hybrid residual learning with statistic priors for single image super-resolution,IEEE,Conferences,"This paper considers single image super-resolution (SISR), which is an important low-level vision task and has various applications in multimedia society. Recently, deep neural networks have archived good performance on this field. But most of existing deep models are based on the fully data-dependent network architecture, thus missing majority of domain-knowledge of the super-resolution task. To address this limitation, we develop a new hybrid residual learning approach to leverage priors of SISR within the maximum a posteriori framework for network architecture design. We demonstrate that it can incorporate both image priors and data fidelity into the network, leading to a novel cascaded residual learning system for SISR process. Extensive experimental results on real-world images show that the proposed algorithm performs favorably against state-of-the-art methods.",https://ieeexplore.ieee.org/document/8019468/,2017 IEEE International Conference on Multimedia and Expo (ICME),10-14 July 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS45731.2020.9180974,DeepRS: Deep-Learning Based Network-Adaptive FEC for Real-Time Video Communications,IEEE,Conferences,"As real-time multimedia streaming thriving, Forward Error Correction (FEC) methods have been studied and applied extensively these years. Most of researchers paid their attention to the coding algorithms, attempted to balance the trade off between recovery ratio and delay with fewer redundance. However, when packet loss pattern changes dynamically, the redundance waste is too serious to be ignored. In this work, we propose a novel algorithm which adjusts the redundance ratio of FEC encoder according to the prediction of packet loss. Receivers are additionally required to feedback observed packet loss pattern. Streaming sender collects the feedbacked packet loss pattern and predicts the number of packet loss in the incoming short period. As for implementation, we adopt long short-term memory (LSTM) network as our deep learning algorithm, and exquisitely embed it in our adaptive FEC system. With the extensive experiments, our proposed scheme outperforms other FEC methods greatly both in the simulations and evaluations on traces observed from the real world.",https://ieeexplore.ieee.org/document/9180974/,2020 IEEE International Symposium on Circuits and Systems (ISCAS),12-14 Oct 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS3C.2014.334,Design an Interactive User Interface with Integration of Dynamic Gesture and Handwritten Numeral Recognition,IEEE,Conferences,"The HCI (Human-Computer Interface) technology development has been an important topic because of the popularity of digital technology and the innovation of 3C product. The product of the NUI (Natural User Interface) is more popular than the design of HCI base on a joystick, mouse, keyboard and touch. It always uses the body movements to achieve the interactive in recently years. After ""Kinect"" controller was announced by the Microsoft Corporation, it open an innovative technologies in the natural user interface, which allows users to get more natural, more diverse ways to interact with machine. In this paper, the Kinect controller can complete a future digital home ITV (Interactive TV) and interactive multimedia software of control that allows users to achieve the visual and interactive operation with a more intuitive and user-friendly operation. It differs from the traditional button control. And this paper can achieve the effect of interaction HCI by Kinect. The interactive control system of this paper is included both of dynamic and handwriting recognition. The dynamic gesture uses the Kinect Sensor to achieve the real-time interaction. The handwriting digital character could be used to select TV channel. The digital character use the BPNN (Back Propagation Neural Network) to achieve the recognition and it also has a good performance in this paper.",https://ieeexplore.ieee.org/document/6846126/,"2014 International Symposium on Computer, Consumer and Control",10-12 June 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC.2014.7065004,Design and implementation of real-time image monitoring system based on Android mobile terminal,IEEE,Conferences,"This paper designs an Android-based mobile terminal real-time image monitoring system, making the remote monitoring of Android-based mobile terminal through Short Message Service (SMS) come into being, and thus gaining the image of Remote Mobile Terminals (RMT). The overall function of the process is: install this system on an Android-based RMT; register monitoring number and monitoring code; when the Local Monitor Terminal (LMT) sends SMS commands to RMT, SMS monitoring module of this system captures SMS, separates the incoming number and content and compares them respectively with the registered records in the SQLite database. If the match is successful, a message is replied and the LMT sends the information code needed to acquire according to the content of the message. The RMT takes photos and replies Multimedia Messaging Service (MMS) according to the code. This system can be operated normally after testing, and shows promising application value.",https://ieeexplore.ieee.org/document/7065004/,2014 IEEE 7th Joint International Information Technology and Artificial Intelligence Conference,20-21 Dec. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SOFTCOM.2008.4669468,Design and performance analysis of inductive QoS scheduling for dynamic network routing,IEEE,Conferences,"In the last decade, due to emerging real-time and multimedia applications, there has been much interest for developing mechanisms to take into account the quality of service required by these applications. We have proposed earlier an approach used an adaptive algorithm for packet routing using reinforcement learning called K-optimal path Q-routing algorithm (KOQRA) which optimizes simultaneously two additive QoS criteria: cumulative cost path and end-to-end delay. The approach developed here adds a new module to KOQRA dealing with the packet scheduling topic in order to achieve QoS differentiation and to optimize the queuing delay in a dynamically wireless changing environment. This module uses a multi-agent system in which each agent tries to optimize its own behaviour and communicate with other agents to make global coordination possible. This communication is done by mobile agents. In this paper, we adopt the framework of Markov decision process applied to multi-agent system and present a pheromone-Q learning approach which combines the standard Q-learning technique with a synthetic pheromone that acts as a communication medium speeding up the learning process of cooperating agents. Numerical results obtained with OPNET simulator for different levels of trafficpsilas load show that adaptive scheduling improves clearly performances of our earlier KOQRA.",https://ieeexplore.ieee.org/document/4669468/,"2008 16th International Conference on Software, Telecommunications and Computer Networks",25-27 Sept. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIS49377.2020.9194858,Design of Hardware and Software Systems for English Listening Test Platform,IEEE,Conferences,"The traditional English listening teaching model is centered on teachers, while students are in a passive position, and thus it cannot stimulate their learning enthusiasm. This paper presents a multimedia-based method of English listening test platform to improve the current situation. First of all, multimedia-assisted English teaching is discussed. On this basis, this paper designs the hardware of English listening test platform combining the hardware model function, the environment development and the functionality. Then the software of English listening test platform is designed with the vector expression of the position in the algorithm, and the design of multimedia listening platform based on multimedia is completed. Experiments show that the proposed design concept can realize the real-time, validity and reliability of English listening test, and provide a solid foundation for the research of the subject.",https://ieeexplore.ieee.org/document/9194858/,2020 IEEE International Conference on Artificial Intelligence and Information Systems (ICAIIS),20-22 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIS49377.2020.9194904,Design of Teaching Expert Evaluation System Based on Artificial Intelligence,IEEE,Conferences,"In order to improve the teaching evaluation expert system intelligent and real-time performance, reduce the teaching expert evaluation system output error rate, improve the accuracy of teaching expert evaluation, a teaching expert evaluation system design method is proposed based on Internet and artificial intelligence. The overall design framework for teaching expert evaluation system, the design of network teaching the expert evaluation system of artificial intelligence chaos control method using a one-way chain network, the transmission control protocol for online teaching evaluation and monitoring information recognition, to improve the real-time transmission capability assessment information teaching, to collect multimedia monitoring information into the signal conditioning module, multimedia information monitoring and scheduling in the open the application programming interface. Combined with the artificial intelligent control method to realize remote control teaching expert evaluation system, through the PIC bus will be teaching expert evaluation data acquisition to the PC machine, DSP after receiving the signal after signal processing and playback, the artificial intelligence control of teaching expert evaluation system is realized. The test results show that this method of teaching design expert system with artificial intelligence, it has good data information fusion ability.",https://ieeexplore.ieee.org/document/9194904/,2020 IEEE International Conference on Artificial Intelligence and Information Systems (ICAIIS),20-22 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCE.2007.4382196,Developing Ontology for Intelligent Home Service Framework,IEEE,Conferences,"Home services are all kind of services which are provided in home environment. Legacy Researches was focused on controlling home devices and appliances. And recent works focus on the multimedia services. In the future, devices existing in the home would infer the status and intention of users and provide intelligent home services without much intervention of users. To do this, there must be context-aware technologies [1]. This research focused on the ontology which is the core technology of the context information management [2]. Through developing ontology for the home services, we classified services into several categories and, described requirements for the service execution. And then, we adopted the ontology for the intelligent home service framework called iHSF. Ontology designed for the home services consists of the concept of users, places (semantic locations), environment around the specific places, devices and individual services and subordinate concepts of them, which all concepts have properties and relations among them [2]. Ontology for the iHSF has form of RDF (resource description framework) [3, 4, 5] and OWL (web ontology language) [6, 7] in order to be interpreted by devices. Ontology should mirror the constantly varying environment of the home dynamically. iHSF using well-designed ontology reacts to the constantly varying .context on a real time basis.",https://ieeexplore.ieee.org/document/4382196/,2007 IEEE International Symposium on Consumer Electronics,20-23 June 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SBGAMES.2018.00021,Efficient A Co-processor for Reconfigurable Gaming Devices,IEEE,Conferences,"Pathfinding algorithms are at the heart of most games, especially to fulfill increasingly demanding Artificial Intelligence and Level Design tasks. Recent smartphones and tablets are equipped with efficient Multi-Processing Systemson-Chip (MPSoC) devices, with demanding performance requirements and energy consumption constraints. While not primarily designed for gaming, such mobile machines are quickly climbing to the top of the list of preferred gaming devices, augmented at each new product iteration with stateof-the-art multimedia subsystems and co-processors. Therefore, this work aims at designing and evaluating an efficient A* pathfinding co-processor for reconfigurable gaming devices. The co-processor is designed using Xilinx High-Level Synthesis (HLS) compiler and is implemented in the programming logic of a Xilinx Ultrascale+ Field-Programmable Gate Array (FPGA) embedded with a 64-bit quad-core ARM Cortex-A53 MPSoC, dual-core Cortex-R5 real-time processors, and a Mali400 MP2 graphics processing unit. Extensive performance, circuit-area and energy consumption results shows that the coprocessor running at only 200MHz can efficiently find paths approximately four times faster than one ARM processor running at 1.2GHz for a set of pathfinding benchmarks based on artificial maps and commercial games such as StarCraft and Baldur’s Gate, paving the way for novel dedicated gaming co-processors. Moreover, the co-processor only requires about one third of the system’s total dynamic power.",https://ieeexplore.ieee.org/document/8636904/,2018 17th Brazilian Symposium on Computer Games and Digital Entertainment (SBGames),29 Oct.-1 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2009.5118409,Emulation of salamander retina with multilayer neural network,IEEE,Conferences,"We utilized a multilayer cellular neural network platform to emulate the functions of salamander retina based on the PIPE model. The template parameters were designed with analytic and embedding methods. By using a FPGA multimedia board for implementation, the emulator completely imitate the function of the PIPE model. The performance of the emulator was evaluated by average SNR and maximum error accumulated in the final image. Both tests confirm the validity of the emulator and the analysis of processing power demonstrated that the emulator can process the input image sequences in real-time with 18-bit precision.",https://ieeexplore.ieee.org/document/5118409/,2009 IEEE International Symposium on Circuits and Systems,24-27 May 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC51774.2021.00027,From teaching books to educational videos and vice versa: a cross-media content retrieval experience,IEEE,Conferences,"Due to the rapid growth of multimedia data and the diffusion of remote and mixed learning, teaching sessions are becoming more and more multi-modal. To deepen the knowledge of specific topics, learners can be interested in retrieving educational videos that complement the textual content of teaching books. However, retrieving educational videos can be particularly challenging when there is a lack of metadata information. To tackle the aforesaid issue, this paper explores the joint use of Deep Learning and Natural Language Processing techniques to retrieve cross-media educational resources (i.e., from text snippets to videos and vice versa). It applies NLP techniques to both the audio transcript of the videos and to the text snippets in the books in order to quantify the semantic relationships between pairs of educational resources of different media types. Then, it trains a Deep Learning model on top of the NLP-based features. The probabilities returned by the Deep Learning model are used to rank the candidate resources based on their relevance to a given query. The results achieved on a real collection of educational multimodal data show that the proposed approach performs better than state-of-the-art solutions. Furthermore, a preliminary attempt to apply the same approach to address a similar retrieval task (i.e., from text to image and vice versa) has shown promising results.",https://ieeexplore.ieee.org/document/9529794/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCC.2004.1358387,Functional networks training algorithm for statistical pattern recognition,IEEE,Conferences,"Pattern classification is the study of how machines can observe the environment, learn to distinguish patterns of interest from their background, and make reasonable decisions about the categories of the patterns. It is a very important in a variety of engineering and scientific disciplines such as computer vision, artificial intelligence, and medicine. New and emerging applications, such as Web searching, multimedia data retrieval, data mining, and machine learning require robust and efficient pattern classification techniques. Recently, functional network has been proposed as a generalization of the standard neural network. In This work we are interested in dealing with the statistical pattern recognition via functional networks and investigate its performance using some real-world applications. We use functional equations to approximate the neuron functions, which allow a wide class of functions to be presented. The steps of working with functional networks and the structural learning are proposed.",https://ieeexplore.ieee.org/document/1358387/,Proceedings. ISCC 2004. Ninth International Symposium on Computers And Communications (IEEE Cat. No.04TH8769),28-1 July 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CMSP.2011.119,IPv6 Based Intelligent Surveillance System,IEEE,Conferences,"Surveillance systems are very effective and important for security management, besides IP (Internet protocol)-based surveillance system is widely used now. However, as the development of the Internet, the scarcity of IP address becomes a serious problem for the massive application of IP-based surveillance system. With the help of IPv6 network, RFID (Radio Frequency Identification) and the multimedia technology, an IPv6-based intelligent surveillance system is given in the paper to overcome the problem as well as giving a new way for the inspecting work. The system with the multi-functions of video surveillance, entrance guard, localization and SMS (Short Message Services) alarm has been successfully deployed in the unattended computer room in China Uniform for real-time surveillance and it has been proved stable and accurate.",https://ieeexplore.ieee.org/document/5957487/,2011 International Conference on Multimedia and Signal Processing,14-15 May 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1995.538334,Implementation of on-line operation manuals for ISO 9000 compliance,IEEE,Conferences,Discusses intelligent hypermedia online manuals and their application in conforming to ISO 9000 quality management and assurance. Hypermedia technology can be used to document electronic files for ISO 9000 online compliance. These files can have access to real-time data from computer networks and automatically implement nonlinear searching and intelligent reasoning based on artificial intelligence technology. They are not only tutorial systems and multimedia books for training new employees but also for assisting operators to make decisions on operation behavior conforming to the quality assurance standards.,https://ieeexplore.ieee.org/document/538334/,"1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century",22-25 Oct. 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMNETSAT53002.2021.9530819,Intelligent Augmented Video Streaming Services Using Lightweight QR Code Scanner,IEEE,Conferences,"Video streaming is a multimedia service that continuously transmits the data over the Internet and presents the content on user screens without predownloading the entire video. Augmented video streaming is an advanced version of video streaming, where the video is enriched with additional embedded information in video frames. These additional data aim to provide a better user experience. Using QR code is one of the efficient approaches to incorporate information into video streams in this context. However, receiving the data in the embedded QR code is considered a challenging task owing to video quality and view angles. This paper proposes a lightweight two-stage QR code decoder for augmented video streaming using deep learning technologies. In the first stage, the position of the embedded QR code is detected using an online object detection algorithm. In the second stage, the detected region of the QR code is fed into a QR code reader to extract the embedded data. The experimental results show that the proposed decoder achieves high performances in terms of response time and decoding accuracy while being very lightweight, which is promising to be implemented in smartphones.",https://ieeexplore.ieee.org/document/9530819/,"2021 IEEE International Conference on Communication, Networks and Satellite (COMNETSAT)",17-18 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICI.2009.323,Intelligent Visual Surveillance System Based on DSP,IEEE,Conferences,"This paper introduces an intelligent visual surveillance system based on TI multimedia digital signal processor (DSP) TMS320DM642, including hardware platform, software platform and algorithm realization. As the core of this system, DSP communicates with host PC via PCI bus. User can set rules via GUI software. The video input from either analog camera or IP camera can be processed by DM642 in real time. If any event breaks the rules, alarm information will be sent to users. Finally, a shadow suppression algorithm based on HSV color information is introduced to reduce the impact of shadow on motion target detection in the sunshine background. Experimental results show that this system can work well outdoors and the CPU load is low enough to meet the requirements of future work such as recognition and tracking.",https://ieeexplore.ieee.org/document/5376266/,2009 International Conference on Artificial Intelligence and Computational Intelligence,7-8 Nov. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCIT.2012.6380963,Learning- and optimization-based channel estimation for cognitive high-speed rail broadband wireless communications,IEEE,Conferences,"In recent years, there is an ever-growing demand on high-quality broadband wireless communications (BWC) for offering high-quality multimedia information services (such as voice, Internet, and video) to passengers as well as improving the safety, security, and operational efficiency of high-speed rail (HSR) transportation. One of the most challenging issues is fast yet accurate channel estimation in HSR mobile environment, where various HSR scenarios and Doppler spread effects have to be considered. Considering the repetitive movement nature of high-speed train over a pre-determined course, a novel learning- and optimization-based channel estimation approach is proposed for cognitive HSR BWC systems in this paper. The key idea is to treat the channel estimation as a learning and optimization process, in other words, the HSR channel parameters are continuously fine-tuned while the train moves along the high-speed rail repetitively. In addition, the learning and optimization process is to be implemented offline, therefore, the computation time of the optimization algorithm (such as the genetic algorithm) is not a limiting issue for real-world implementation. The simulation results demonstrated the effectiveness and significant advantages of this cognitive approach to HSR channel estimation.",https://ieeexplore.ieee.org/document/6380963/,2012 International Symposium on Communications and Information Technologies (ISCIT),2-5 Oct. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2019.8761121,Learning-Based Cooperative Content Caching Policy for Mobile Edge Computing,IEEE,Conferences,"To address the drastic increase of multimedia traffic dominated by streaming videos, mobile edge computing (MEC) can be exploited to accelerate the development of intelligent caching at mobile network edges to reduce redundant data transmissions and improve content delivery performance. Under the MEC architecture, content providers (CPs) can access MEC servers to deploy popular content items to improve users' quality of experience. Designing an efficient caching policy is crucial for CPs due to the content dynamics, unknown spatial-temporal traffic demands and limited storage capacity. The knowledge of users' preference is important for efficient caching, but is also often unavailable in advance. Machine learning can be used to learn the users' preference based on historical demand information and decide the content items to be cached at the MEC servers. In this paper, we propose a learning based cooperative content caching policy for the MEC architecture, when the users' preference is unknown and only the historical content demands can be observed. We model the cooperative content caching problem as a multi-agent multi-armed bandit problem and propose a multiagent reinforcement learning (MARL)-based algorithm to solve the problem. Simulation experiments are conducted based on the real dataset from MovieLens and the numerical results show that the proposed MARL-based caching policy can significantly improve content cache hit rate and reduce content downloading latency in comparison with other popular caching strategies.",https://ieeexplore.ieee.org/document/8761121/,ICC 2019 - 2019 IEEE International Conference on Communications (ICC),20-24 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CGames.2014.6934142,Linguistic implementations in computer game and virtual world design,IEEE,Conferences,"Computer gaming is a rich multimedia environment that players experience across the human sensorium. Game design uses linguistic engagement to enhance and expand that experience. Linguistics offers a variety of services and benefits to computer gaming, from successful commercial localization of games to enhanced engagement, immersion and authenticity of the game play. We examine linguistic applications in computer gaming and the cross- cultural/cross-language artifacts produced.",https://ieeexplore.ieee.org/document/6934142/,"2014 Computer Games: AI, Animation, Mobile, Multimedia, Educational and Serious Games (CGAMES)",28-30 July 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICODSE.2018.8705834,Microtask Crowdsourcing Marketplace for Social Network,IEEE,Conferences,"Crowdsourcing is a powerful way to process and collect data that needs human's logic and perception. Crowdsourcing can take part in many hard to compute problems such as data entry, multimedia transcriptions and many case of artificial intelligence. While this is a powerful approach, crowdsourcing needs a relatively large marketplace to works optimum. In context of Indonesia, we hardly hear about task crowdsourcing even though there are some marketplace like poin-web.co.id. In this paper, we propose a social network for crowdsourcing marketplace to penetrate the market. People tends to waste most of their time on social network and game rather than other mobile application in case of mobile usage. The idea is to make crowdsourcing as a filler while people are using social networks like waiting for a chat or scrolling the timeline. And by applying microtask as the task, people will not have much burden on doing the task. On the other hand, he/she will get additional income. We implemented LINE as a basis of our social network marketplace. LINE does provide the most interactive way of provide message. Additionally, the user of LINE in Indonesia is growing in a fast pace. We conduct an experiment focused on worker's coverage and ease of use. By using usability testing as a basis of ease of use evaluation, we received good feedbacks as 90.9% of the users feel easier to answer through LINE and is excited to use the platform in case it goes with real money.",https://ieeexplore.ieee.org/document/8705834/,2018 5th International Conference on Data and Software Engineering (ICoDSE),7-8 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SmartNets48225.2019.9069799,Modeling and Scheduling with DAG used for tasks in ME with OVP,IEEE,Conferences,"Several techniques have been proposed to adapt multimedia and telecommunication applications to resource constraints. These techniques are mostly implemented at the application layer and make simplistic assumptions about the system resources communication and they are often unconvinced to the co-design in embedded system SoC and MPSoC. Moreover, they often assume that the data codec video characteristics and their processing needs are stationary, which is not true in practice especially in hardware for the real target FPGA. In fact, data streams in codec video are highly dynamic and may also experience concept drift, thereby requiring continuous online adaptation of the throughput and quality to each processing task in video and frame. Hence, existing solutions for multimedia and telecommunication applications are often too conservative or too aggressive, especially in video codec. To address these limitations, complexity in video test, quality of frame, precision, send video with a fast time especially in medical applications, airport and military flight problems. we propose offline algorithm scheduling which minimize the execution time (i.e., troughput and output quality of video and frame) of medical and military applications under real time and resources constraints. Our algorithm scheduling uses offline and online reinforcement learning techniques. Moreover, our scheduler is able to detect concept drifts and to smoothly adapt the scheduling strategy (DAG algorithm). Our experiments realized on a chain of tasks modeling real-life streaming application demonstrate that our scheduler is able to learn the scheduling policy and to adapt it such that it minimizes the execution time and optimize the precision, quality of frame as the video codec characteristics are dynamically changing. The strategy of scheduling algorithm is to order the tasks in processor or different processors. This approach remade tow problem in static scheduling algorithm with video and frame processing (the physiological data of a patient and allows in cases of urgent problems to trigger an alarm remotely controlled by an expert to intervene quickly in case of emergency. The key of contribution in this research are the modeling and scheduling tasks in ME blocks in video codec with new approach automatic, implementation and developpement this algorithms virtual Platform “OVP”, we work with tow codec video (H264 and H265) in ME block.",https://ieeexplore.ieee.org/document/9069799/,"2019 International Conference on Smart Applications, Communications and Networking (SmartNets)",17-19 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVSurv.2011.6157032,Multi-DSP based Intelligent Visual Surveillance System,IEEE,Conferences,"An Intelligent Visual Surveillance (IVS) system based on multi SEED-VPM642 platform is proposed in this paper, including design and realization of both the hardware and software platforms. The core chip of the platform is TI multimedia Digital Signal Processor (DSP) TMS320DM642. In the system, DSP acts as a server, waiting for client PC to get connected to the Internet. In DSP, video captured by cameras is processed by algorithms in real time, then encoded, and finally sent to PC. If any event breaks the rules set by users on PC, alarm signal will be sent through the Global System for Mobile Communications (GSM) network. Experimental results show that the system works well in detecting and tracking objects. Meanwhile, PC Graphical User Interface (GUI) supports monitoring 16 video scenes at the same time and controlling Pan/Tilt/Zoom (PTZ) camera, also allows video playing back. In addition, our system supports both Client/Server (C/S) and Browser/Server (B/S) architecture.",https://ieeexplore.ieee.org/document/6157032/,2011 Third Chinese Conference on Intelligent Visual Surveillance,1-2 Dec. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD-SAWN.2005.55,Multipath QoS routing of supporting DiffServ in mobile ad hoc networks,IEEE,Conferences,"Future mobile ad hoc networks (MANETs) are expected to be based on all-IP architecture and be capable of carrying multitudinous real-time multimedia applications such as voice, video and data. It is very necessary for MANETs to have a reliable and efficient routing and quality of service (QoS) mechanism to support diverse applications which have varying and stringent requirements for delay, jitter, bandwidth, packets loss. Providing multipath routing is beneficial to avoid traffic congestion and break in communication in MANETs where routes are disconnected frequently due to mobility. Differentiated Services (DiffServ) which have simple, efficient and scalable characteristics can be used to classify network traffic into different priority levels and apply different scheduling and queuing mechanisms to obtain QoS guarantees. In this paper, we propose a practical node-disjoint multipath QoS routing protocol of supporting DiffServ (MQRD), which provides low routing overhead and end-to-end QoS support. Simulation results show that MQRD achieves better performance in terms of packet delivery ratio and average delay.",https://ieeexplore.ieee.org/document/1434905/,"Sixth International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing and First ACIS International Workshop on Self-Assembling Wireless Network",23-25 May 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudCom2018.2018.00063,Online Density Grid Pattern Analysis to Classify Anomalies in Cloud and NFV Systems,IEEE,Conferences,"Technologies like machine-to-machine communication, autonomous driving or virtual reality applications form an increasingly diverse service landscape. This entails individual and dynamic requirements regarding scalability, availability, latency or throughput from the underlying IT infrastructure. To meet those, telecommunication and network providers started a transformation process towards virtualized technologies like network function virtualization (NFV). However, this drastically increases the infrastructure complexity to a point where more autonomous management is required. In order to meet the reliability of dedicated hardware, virtualized solutions are in demand of autonomous recovery and remediation systems. For critical network systems, actions must be selected very cautiously to not disrupt the operational process. To enable a precise handling, anomaly situations need to be accurately identified based on monitoring data streams. Therefore, we present a supervised machine learning method for an online classification of anomaly states based on similarities between anomaly type-specific density grid patterns. For evaluation, we created an extensive NFV testbed running a virtual implementation of the IP multimedia subsystem. Applying our method to classify various synthetically injected anomaly situations, the results reveal an average overall accuracy of 0.94. Further results also show that the classification model is applicable for identifying previously unknown anomaly situations. Thus, our approach provides a valuable step towards autonomous maintenance of virtualized IT infrastructures.",https://ieeexplore.ieee.org/document/8591032/,2018 IEEE International Conference on Cloud Computing Technology and Science (CloudCom),10-13 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.859984,Online intelligent system for operation support,IEEE,Conferences,"This paper presents an online real-time intelligent system for a heating plant operation support. The intelligent system is established based on INTEMOR software platform, which combines artificial intelligence (AI), computer technology and information technology, and runs in a real-time distributed environment. Many function modules are developed to perform some specific operation support tasks in the heating plant, which include the communication gateway, data processing and analysis, online process monitoring and diagnosis, online operation manual, reasoning system, knowledge base creator and multimedia interface. The structure and the main features of the system are discussed.",https://ieeexplore.ieee.org/document/859984/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2004.1333743,Ontology and taxonomy collaborated framework for meeting classification,IEEE,Conferences,"A framework for classification of meeting videos is proposed in this paper. We define our framework consisting of a four level concept hierarchy having movements, events, behavior, and genre; which is based on the meeting ontology and taxonomy. Ontology is the formal specification of domain concepts and their relationships. Taxonomy is the general categorization based on class/subclass relationships. This concept hierarchy is mapped to an implementation of finite state machines (FSM) and rule-based system (RBS) to classify the meetings. Events are detected by the FSMs based on the movements (head and hand tracks). Classification of the meetings is performed by the RBS based on the events, and behaviors of the people present in the meetings. Our framework is novel and scalable, capable of adding new meeting types with no re-training. We conducted experiments on various meeting sequences and classified meetings into voting, argument, presentation, and object passing. This framework has applications in automated video surveillance, video segmentation and retrieval (multimedia), human computer interaction, and augmented reality.",https://ieeexplore.ieee.org/document/1333743/,"Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.",26-26 Aug. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WI.2005.108,Ontology-based information integration in virtual learning environment,IEEE,Conferences,"A good virtual learning environment should deliver relevant learning materials to learners at the most appropriate time and locations to facilitate learners' acquisition of knowledge and skills. In this paper, we propose ontology-based information integration in virtual learning environment using ontology and Web services. Relevant concepts extracted from domain ontology provide ontology-based browsing space that allows users to browse and select relevant terms of interest and increases the degree of relevancy. By using Web services to integrate learning materials from heterogeneous public domain data sources, applications do not need to know the internal structure and working of public domain data sources, and reuse existing applications and recourses. We use gene ontology, PubMed eUtils and Google Web APIs to demonstrate our idea. The implementation involves techniques in image and video processing, database management, programming, and multimedia learning materials presentation.",https://ieeexplore.ieee.org/document/1517949/,The 2005 IEEE/WIC/ACM International Conference on Web Intelligence (WI'05),19-22 Sept. 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BICITS51482.2021.9509919,Optical Impairment Compensation in Fiber Communication Systems Based on Artificial Intelligence: A Comprehensive Survey,IEEE,Conferences,"The global demand for high-speed communication has increased dramatically over the past few years when data beginning to dominating of the traffic according to the Cisco Visual Networking Index (VNI). Data traffic is triple between 2014 and 2020, mainly, due to developing applications that consume bandwidth such as cloud services, HD video, high quality of real-time video transmission, virtual- augmented reality (VR-AR), online- games (video games), exchange of multimedia via smartphones and the more like. In fact, in 2020, more than a million minutes of multimedia (video) content is transiting the IP network every second according to the VNI; and the demands will exceed the capability of the current (core) internet backbone systems, in which optical communications are the main infrastructure. In this paper, the focus was on reviewing the mechanisms used for the most important and most effective techniques used to increase the capacity of optical transmission systems, namely Nonlinear Compensation (NLC) which work to reduce the nonlinear impairments that represent the main intrinsic challenges and the main capacity limitations facing the optical systems. The traditional NLC techniques were determined based on the approximate solution of the Nonlinear Schrodinger Equation (NLSE) through Digital Back Propagation (DBP), or Split- step Fourier Method (SSFM). however, their implementation demands excessive signal processing resources, and high-level accurate knowledge. A completely new approach that uses artificial intelligence (AI) algorithms to identify and solve these impairments has been studied in this paper. Traditional NLC techniques are reviewed in the first part to mitigation the nonlinearities and estimate the quality of transmission (QoT). Whereas in the second part, we review the uses of AI techniques that have been studied in applications related to monitoring performance, reduce nonlinearity, and quantify QoT. Finally, this paper presents a summary with a conclusion and outlook for development and challenges in optical fiber communication systems where AI is predictable to represent a hot major role in the near future.",https://ieeexplore.ieee.org/document/9509919/,2021 1st Babylon International Conference on Information Technology and Science (BICITS),28-29 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTCSA.2000.896404,Optimal scheduling of imprecise computation tasks in the presence of multiple faults,IEEE,Conferences,"With the advance of applications such as multimedia, image/speech processing and real-time AI, real-time computing models allowing to express the ""timeliness versus precision"" trade-off are becoming increasingly popular. In the imprecise computation model, a task is divided into a mandatory part and an optional part. The mandatory part should be completed by the deadline even under worst-case scenario; however, the optional part refines the output of a mandatory part within the limits of the available computing capacity. A non-decreasing reward function is associated with the execution of each optional part. Since the mandatory parts have hard deadlines, provisions should be taken against faults which may occur during execution. An FT-Optimal framework allows the computation of a schedule that simultaneously maximizes the total reward and tolerates transient faults of mandatory parts. We extend the framework to a set of tasks with multiple deadlines, multiple recovery blocks and precedence constraints among them. To this aim, we first obtain the exact characterization of imprecise computation schedules which can tolerate up to k faults, without missing any deadlines of mandatory parts. Then, we show how to generate FT-Optimal schedules in an efficient way. Our solution works for both linear and general concave reward functions.",https://ieeexplore.ieee.org/document/896404/,Proceedings Seventh International Conference on Real-Time Computing Systems and Applications,12-14 Dec. 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSD.2019.8893218,Parallel implementation of HEVC encoder on multicore ARM-based platform,IEEE,Conferences,"High Efficiency Video Coding (HEVC) is the new emerging standard released as a successor to H.264/AVC. It aims to improve the encoding performance by saving 50% of the bitrate with the same visual quality. This encoding performance makes it more suitable for high definition video applications and could be the next embedded video codec on the majority of multimedia devices. However, this performance is coupled with tremendous computational complexity which makes it very hard to achieve a real-time video encoding with classic embedded processor architectures. Consequently, multicore technology of programmable processors offers a very promising solution to overcome this complexity. In this context, this paper presents a parallel implementation of the HEVC encoder All-Intra (AI) configuration on Quad-core ARM-based platform running at 1.7GHz. OpenMP is used as parallel programming paradigm exploiting the Frame Level Parallelism technique. Experimental results show that parallel processing using four threads allows saving up to 73% of encoding time and speeds up the encoding process by a factor of 3.77 without any rate distortion in terms of video quality or bitrate.",https://ieeexplore.ieee.org/document/8893218/,"2019 16th International Multi-Conference on Systems, Signals & Devices (SSD)",21-24 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.411,Performance Evaluation of TriBA -- A Novel Scalable Architecture for High Performance Applications,IEEE,Conferences,"This paper evaluates the performance of TriBA - a new idea in multiprocessor architectures. TriBA is considered to be a high performance, scalable parallel computing architecture, therefore, its performance is evaluated by well-known scalability metrics speed-up and efficiency. The root of ""triplet based architecture"" is based on the concept that ""complex problems can be decomposed to three relatively independent sub-problems, which are data processing, data management and data communication"". TriBA consists of a 2D grid of small, programmable processing units, each physically connected to its three neighbors. TriBA is a real scalable architecture, featuring fractal nature for computers. TriBA is a new solution for computer architecture, which is suitable for sophisticated embedded applications with multiple concurrent processing centers. The characteristics of this architecture are its great modularity, flexibility and scalability to meet the real-time signal processing demands in future telecommunication and multimedia systems. The feasibility, effectiveness and performance of TriBA is illustrated using execution model and simulations comparing it with conventional 2D mesh architecture.",https://ieeexplore.ieee.org/document/4287948/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NOF.2017.8251223,Prediction of active UE number with Bayesian neural networks for self-organizing LTE networks,IEEE,Conferences,"Internet-empowered electronic gadgets and content rich multimedia applications have expanded exponentially in recent years. As a consequence, heterogeneous network structures introduced with Long Term Evolution (LTE) Advanced have increasingly gaining momentum in order to handle with data explosion. On the other hand, the deployment of new network equipment is resulting in increasing both capital and operating expenditures. These deployments are done under the consideration of the busy hour periods which the network experiences the highest amount of traffic. However, these periods refer to only a couple of hours over a 24-hour period. In relation to this, accurate prediction of active user equipment (UE) number is significant for efficient network operations and results in decreasing energy consumption. In this paper, we investigate a Bayesian technique to design an optimal feed-forward neural network for shortterm predictor executed at the network management entity and providing proactivity to Energy Saving, a Self-Organizing Network function. We first demonstrate prediction results of active UE number collected from real LTE network. Then, we evaluate the prediction accuracy of the Bayesian neural network as comparing with low complex naive prediction method, Holt- Winter's exponential smoothing method, a deterministic feedforward neural network without Bayesian regularization term.",https://ieeexplore.ieee.org/document/8251223/,2017 8th International Conference on the Network of the Future (NOF),22-24 Nov. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIT.2004.1357156,Proceedings. The Fourth International Conference on Computer and Information Technology,IEEE,Conferences,The following topics are dealt with: image understanding; networked multimedia; human-computer interaction; pattern recognition; virtual reality techniques; Web service; XML; e-learning; grid computing and next-generation Internet technologies; peer-to-peer networks; distributed computing; mobile computing; wireless computing; ad hoc networks; pervasive computing; shape abstract modeling; geometric modeling; rendering algorithms; information retrieval; medical information systems; database systems; data mining; artificial intelligence; software engineering; and natural language processing.,https://ieeexplore.ieee.org/document/1357156/,"The Fourth International Conference onComputer and Information Technology, 2004. CIT '04.",16-16 Sept. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICCSA.2016.7945679,QLAR: A Q-learning based adaptive routing for MANETs,IEEE,Conferences,"Mobile Ad-hoc Networks are highly reconfigurable networks of mobile nodes which communicate by wireless links. The main issues in MANETs include the mobility of the network nodes, energy limitations and bandwidth. Thus, routing protocols should explicitly consider network changes into the algorithm design. In order to support service requirements of multimedia and real-time applications, the routing protocol must provide Quality of Service (QoS) in terms of packets loss and average End-to-End Delay (ETED). This work proposes a Q-Learning based Adaptive Routing model (QLAR), developed via Reinforcement Learning (RL) techniques, which has the ability to detect the level of mobility at different points of time so that each individual node can update routing metric accordingly. The proposed protocol introduces: (i) new model, developed via Q-Learning technique, to detect the level of mobility at each node in the network; (ii) a new metric, called Qmetric, which account for the static and dynamic routing metrics, and which are combined and updated to the changing network topologies. The proposed metric and routing model in this paper are deployed on the Optimized Link State Routing (OLSR) protocol. Extensive simulations validate the effectiveness of the proposed model, through comparisons with the standard OLSR protocols.",https://ieeexplore.ieee.org/document/7945679/,2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA),29 Nov.-2 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCSET55632.2022.9767075,QoE-Aware Intelligent Handover Method for Intent-Based Software-Defined Wireless Network,IEEE,Conferences,"Intent-Based Networking (IBN) focuses on technologically independent, fast, and reliable interaction between network infrastructure management systems and users. This concept is being developed to automate and accelerate the deployment of the network lifecycle. Thus, IBN includes mechanisms for recognizing, understanding, and extending the intents that define the quality-of-service (QoS) requests. Therefore, this paper proposes an intent-based software-defined wireless network (IBSDWN) approach in which the controller intelligently decides when to initiate the handover of services. For this purpose, the controller selects the access point (AP) to which the client device should connect, based on the set quality of experience (QoE) requirements using an intent processing technique. A method for initiating handover in IBSDWN based on machine learning (ML) algorithms and an integral QoE criterion formed from real-time measurements of parameters: received signal strength indication (RSSI), throughput, packet loss, and delay is developed. Implementation of ML module in IBN architecture for monitoring system allowed to reduce the volume of signal traffic in communication channels between network equipment and controller. Also, the developed ML module made it possible to detect the degradation of QoE values and prevent situations when the user is not satisfied with the received QoS for adaptive prediction of the moment of network reconfiguration. On the basis of a simulation model, it is proved that the proposed solutions can improve the quality of experience of multimedia services to end-users.",https://ieeexplore.ieee.org/document/9767075/,"2022 IEEE 16th International Conference on Advanced Trends in Radioelectronics, Telecommunications and Computer Engineering (TCSET)",22-26 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.2008.4517901,Realtime detection of salient moving object: A multi-core solution,IEEE,Conferences,"Detection of salient moving object has great potentials in activity recognition, scene understanding, etc. However techniques to characterizing the object in fine granularity have not been well developed in real applications due to the computational intensity. The emerging multi-core technology in hardware design provides an opportunity for the compute intensive algorithms to boost speed in parallel. This paper proposed a scalable approach to detecting salient moving object which is designed inherently for parallelization. To characterize the object in fine granularity, we extract color-texture homogenous regions as the basic processing unit by image segmentation. To identify salient object, we generate probabilistic template by learning the space-time context. The parallel algorithm is implemented using OpenMP. Evaluations have been carried out on sports, news, and home video data. For the CIF size image, we get processing speed of 51.1 frames per second and near linear speed up on an eight-core machine. It indicates that the algorithm parallelization is a promising solution for practical applications in the multimedia field.",https://ieeexplore.ieee.org/document/4517901/,"2008 IEEE International Conference on Acoustics, Speech and Signal Processing",31 March-4 April 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPEC51340.2021.9421306,Research on Human Target Detection and Tracking Based on Artificial Intelligence Vision,IEEE,Conferences,"With the rapid development of multimedia technology and the continuous improvement of computer performance, digital video monitoring system has gradually replaced the traditional analog monitoring system and has been widely used in public places such as airports, stations, banks, hotels and so on. As an important part of intelligent video surveillance system, target detection and target tracking technology has become an important research direction in the field of vision. Human behavior detection and recognition based on artificial intelligence vision has developed rapidly due to the increasing demand, and has become a hot spot in the research field of computer vision and artificial intelligence. Under complex background, the detection, recognition and automatic tracking of human moving objects based on artificial intelligence vision are the key and difficult points in the field of computer vision research. In this paper, the image preprocessing, detection and extraction, matching recognition and automatic tracking of human moving targets based on artificial intelligence vision in complex environment are analyzed and studied, which lays a good foundation for the research and implementation of human target vision monitoring system.",https://ieeexplore.ieee.org/document/9421306/,"2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",14-16 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIS49377.2020.9194855,Research on the implementation of IMS boundary session control capability based on edge computing,IEEE,Conferences,"The purpose of the introduction of IMS Technology into the power exchange network is to meet the development of information operations, so that the exchange platform can carry more functions of services and services. The grid also deployed the integration of unified communication, mobile office, video conference and other multimedia services while introducing IMS Technology, which added great pressure to the IMS service provision process. On this demand, this paper proposes an IMS service distribution and control technology based on the edge computing arrangement ability. It can marginalize the business functions and service processes, reduce the repeated interaction between SBC and the core network. It can lightweight IMS service registration, startup, conversation, deletion and other service protocols, and improve the quality of IMS service with the help of the edge computing service information proxy function And efficiency.",https://ieeexplore.ieee.org/document/9194855/,2020 IEEE International Conference on Artificial Intelligence and Information Systems (ICAIIS),20-22 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOCOMW.2010.5700107,Resource adaptations for revenue optimization in cognitive mesh network using reinforcement learning,IEEE,Conferences,"Nowadays, licensed users (primary users, PUs) can provide a means for offering internet service for unlicensed users (secondary users, SUs). We explore the ability of cognitive mesh network (CMN) to offer quality of service (QoS) required by real time services, streaming multimedia and other applications. We consider the approach where PUs rent the surplus spectrum to SUs to get some reward. However, when a PU rents more spectrum to SUs, its quality of service (QoS) is degraded due to a reduction of the spectrum. This complex contradicting requirement is embedded in our reinforcement learning (RL) model that is developed and implemented as shown in this paper. Available spectrum is managed by the PU which executes the extracted control policy. In this work, we propose a novel resource management scheme in the radio environment. RL is used as a means for extracting an optimal policy that helps a PU to adapt to the changing network conditions, so that the PU's profit is maximized continuously over time. The proposed scheme integrates different requirements such as rewards for PUs, QoS for PUs and the radio environment conditions. Performance evaluation of the proposed RL solution shows that the scheme is able to adapt to different network conditions and to guarantee the required QoS for PUs. Moreover, it is shown that CMN can support additional SUs traffic while still ensuring PUs QoS and maximizing PUs profits.",https://ieeexplore.ieee.org/document/5700107/,2010 IEEE Globecom Workshops,6-10 Dec. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCUBEA.2016.7859996,Retrieval and extraction of unique patterns from compressed text data using the SVD technique on Hadoop Apache MAHOUT framework,IEEE,Conferences,"Nowadays Designing of an efficient information retrieval system for multimedia dataset is a big task to understand the trend. Searching for the repeated pattern within a particular genetic sequence has become a much required task in the data mining sectors. Due to the increasing size of text and audio data over internet, various techniques are needed to help with the finding and extraction of very specific information relevant to a user's task or finding the new trends. Matching the unique patterns and generate the rules by using different Pattern Matching, Rule Generation Algorithm. In addition to extracting questionnaires and curiosity based sentences patterns from large database some different implementation of the pattern matching algorithms is proposed. Real world data is large in size like images, speech signals holding high dimensions to represent data. Multiple dimensional data are more critical for detecting and developing the associations among terms. Dimensionality reduction is a technique used for reducing complexity and gives the most frequent item from high dimensional data. It reduces the dimensions of the original input data. Singular value decomposition this dimensionality reduction technique is used for large data reduction on Apache MAHOUT of Hadoop framework. Finally, retrieve and extract the curiosity and questionnaires based unique pattern from large data size using the Map Reduce Framework and compress the result using the SVD technique to give curiosity and questionnaires based subject.",https://ieeexplore.ieee.org/document/7859996/,2016 International Conference on Computing Communication Control and automation (ICCUBEA),12-13 Aug. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASS.2019.00022,SHAD: Privacy-Friendly Shared Activity Detection and Data Sharing,IEEE,Conferences,"Nowadays, there is a growing demand for sharing multimedia data among participants in the same activity. With existing social applications, users need to conduct friending and data sharing operations manually, which is troublesome due to changing attendees and highly diverse data content of different activities. To tackle this issue, in this work we propose a novel system SHAD to achieve privacy-friendly shared activity detection and multimedia data auto-sharing based on users' historical multimodal data. Facing noisy, incomplete and asynchronous data, as well as inaccurate recognition results of machine learning models, we design an algorithm to aggregate multimodal data relevant to the same activity and propose an activity-semantic graph to comprehensively characterize each activity by fusing knowledge of multimodal data. Based on the activity-semantic graph, the privacy-preserving shared activity detection and data sharing method is designed, which protects both raw data and semantic information of data. We implemented our system and conducted comprehensive evaluations with real-life multimodal data (including photos and motion sensor data). The results show the efficacy of our system. We can achieve 94.9% precision and 91.5% recall for shared activity detection.",https://ieeexplore.ieee.org/document/9077362/,2019 IEEE 16th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),4-7 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP40778.2020.9190670,Seg-Hashnet: Semantic Segmentation Based Unsupervised Hashing,IEEE,Conferences,"Hashing based multimedia retrieval methods have been widely studied because of the advantages of computation efficiency and low storage cost. Generally, there are two ways of doing hashing: Supervised and unsupervised hashing. Supervised hashing was known to be more efficient and popular. However, in many real applications, only unsupervised method work because there are no labels. But unsupervised hashing method still face some challenges such as how to get semantic information from unlabeled data. To address this problem, in this paper, we propose a deep unsupervised hashing method based on semantic segmentation (Seg-HashNet). Specifically, a pre-trained semantic segmentation model is used to mine informative semantic information to guide the learning process and generate highquality hash codes. Extensive experiments on two public datasets have been performed, which showed that Unsupervised Semantic Segmentation hashing is superior to the latest unsupervised hashing method in image retrieval tasks.",https://ieeexplore.ieee.org/document/9190670/,2020 IEEE International Conference on Image Processing (ICIP),25-28 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigMM.2015.26,Semi-supervised Multimodal Clustering Algorithm Integrating Label Signals for Social Event Detection,IEEE,Conferences,"Photo-sharing social media sites provide new ways for users to share their experiences and interests on the Web, which aggregate large amounts of multimedia resources associated with a wide variety of real-world events in different types and scales. In this work, we aim to tackle social event detection from these large amounts of image collections by devising a semi-supervised multimodal clustering algorithm, denoted by SSMC, which exploits label signals to guide the fusion of the multimodal features. Particularly, SSMC takes advantage of the distribution over the similarities on a small amount of labeled data to represent the images, fusing multiple heterogeneous features seamlessly. As a result, SSMC has low computational complexity in processing multimodal features for both initial and updating stages. Experiments are conducted on the Mediaeval social event detection challenge, and the results show that our approach achieves better performance compared with the baseline algorithms.",https://ieeexplore.ieee.org/document/7153853/,2015 IEEE International Conference on Multimedia Big Data,20-22 April 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCSIC54682.2021.00073,Study of the Effect of Security on Quality of Service on a WebRTC Framework for Videocalls,IEEE,Conferences,"Web Real-Time Communication (WebRTC) is a promising new standard and technology stack, providing full audio/video communications in a secured solution. Organizations implementing such technology deal with both quality of service and security demands, therefore it is mandatory to investigate the impact of QoS parameters when applying security mechanisms in multimedia services for the case of videocalls. This work presents a study of quality of service indicators such as jitter, delay between RTP (Real Time Protocol) packets, establishment and release time for three levels of security, considering the effect over signaling and media planes of the videocall. Four scenarios were implemented in two groups: the first one consisted of a LAN with a Laptop and a PC with WebRTC clients or between a Smartphone and a PC as well. The second one consisted of a DSL WAN with a Laptop and a PC with WebRTC clients or between a Smartphone and a PC as well. The three levels of security for each scenario were implemented as follows: the first one without security, the second one with TLS in signaling, and the third with both TLS in signaling and DTLS in media traffic. Codecs employed were VP8 for video and OPUS for audio on every testbed over WebRTC with EasyRTC framework. The results show, in general terms that QoS media indicators were on the recommended levels according to ITU and IETF. Establishing and liberation time were degraded. In some cases there was an improvement, as in the case of jitter due to the use of RTP (Real Time Transport) protocol for audio. We recommend the use of machine learning algorithm K-means for detecting clusters between different QoS indicator for the three levels of security just to detect with more accuracy the impact for each level.",https://ieeexplore.ieee.org/document/9644361/,2021 International Symposium on Computer Science and Intelligent Controls (ISCSIC),12-14 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICS.2005.1689220,Support System for Telecommunication Using Keywords,IEEE,Conferences,"We developed a prototype system to support telecommunications that uses keywords selected by the presenter in videoconference. The presenter holds to show keyword cards, and the listeners at remote sites can see additional information with the keywords. Although keyword captions are considered effective in video learning environments for learning foreign languages, we think they should also be available for interactive communications. Our prototype system recognizes letters and characters in a video image, and provides us with additional functions, such as language translation, 3D models, and audio reproductions. The visual data are overlaid onto the real scene with a multilayered display, using a Web browser as a presentation tool to enable us to easily author/edit multimedia data. Optical character reader (OCR) middleware was implemented into the recognition function for Japanese, and an open-source library, ARToolkit was used to detect the character markers that controlled the functions of the information presentation. The support system is designed to efficiently exploit audiovisual signals of existing videoconferencing systems",https://ieeexplore.ieee.org/document/1689220/,2005 5th International Conference on Information Communications & Signal Processing,6-9 Dec. 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IIAI-AAI.2014.4,Table of contents,IEEE,Conferences,The following topics are dealt with: data mining; Japanese WordNet synonym misplacement detection; social network; recommender system; sentiment analysis; workshop-based instruction; Japanese public libraries; machine learning methods; collaborative Web presentation support system; SMS4 ultracompact hardware implementation; wireless sensor networks; personalized public transportation recommendation system; adaptive user interface; NIS-Apriori algorithm; GetRNIA software tool; rough set-based rule generation; tree-Ga bump hunting; neural network model; weighted citation network analysis; sound proofing ventilation unit; touch interaction; mutually dependent Markov decision processes; ozone treatment; dynamic query optimization; big data; learner activity recognition; IoT-security approach; nutrition-based vegetable production; farm product cultivation; polynomial time mat learning; C-deterministic regular formal graph system; article abstract key expression extraction; English text comprehension; online social games; knowledge creation; knowledge utilization; online stock trading; customer behavior analysis; project-based collaborative learning; in-field mobile game-based learning activities; e-portfolio system design; self-regulated learning ontological model; mobile augmented reality based scaffolding platform; context-aware mobile Japanese conversation learning system; English writing error classification; image processing; outside-class learning; exercise-centric teaching materials; UML modeling; online historical document reading literacy; MMORPG-based learning environment; computer courses; undergraduate education; energy management system; higher education; decentralised auction-based bandwidth allocation; wireless networked control systems; resource scheduling algorithm; embedded cloud computing; Poisson distribution; Japanese seismic activity; suspect vehicle detection; 3D network traffic visualization; Web information retrieval; agent based disaster evacuation assist system; electroencephalogram; random number generator; multiagent simulations; multicore environment; CPU scheduler; multithreaded processes; reserve-price biddings; real-time traffic signal control; evolutionary computation; robot-assisted rehabilitation system; hybrid automata; Batik motif classification; color-texture-based feature extraction; backpropagation; multimedia storytelling; e-tourism service; Web mining; search engine; simulation-based e-learning mobile application software; library classification training system; WebQuest learning strategy; context-aware ubiquitous English learning; support vector machine; RFID tag ownership transfer protocol; cognitive linguistics; collaborative software engineering learning; write-access reduction method; NVM-DRAM hybrid memory; garbage collection; parallel indexing scheme; lazy-updating snoop cache protocol; distributed storage system; ITS application; software engineering education; ophthalmic multimodal imaging system; injected bug classification; secure live virtual machine migration; flash memory management; genetic programming; heterogeneous databases; time series similarity search; concurrency control program generation; incremental data migration; multidatabase system; software release time decision making; analytic hierarchy process; interactive genetic algorithm; biometric intelligence; talking robots; archaeological ruin analysis; GIS; optical wireless pedestrian-support systems; visual impairment; extreme programming; Japanese e-commerce Web sites; Chinese sign language animation; hearing-impaired people mammography inspection; geographical maps; electroculogram; XML element retrieval technique; image recognition; reinforcement learning; ECU formal verification; gasoline direct injection engines; earthquake disaster simulation; smart devices for autistic children; RoboCup rescue simulation; inductive logic programming; master-slave asynchronous evolutionary hybrid algorithm; VANET routing optimization; and Web image sharing services.,https://ieeexplore.ieee.org/document/6913248/,2014 IIAI 3rd International Conference on Advanced Applied Informatics,31 Aug.-4 Sept. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCEA.2010.4,Table of contents - Volume 1,IEEE,Conferences,The following deals with the following topics: algorithms; artificial intelligence; software engineering; bioinformatics; computer graphics; computer architecture; information systems; computer aided instruction; computer games; virtual reality; data security; digital simulation; computer aided design; ethical aspects; database systems; digital libraries; signal processing; image processing; logic design; e-commerce; human computer interaction; embedded systems; Internet; mobile computing; multimedia systems; natural language processing; neural networks; programming languages; robotics; control systems; theoretical computer science; and wireless sensor networks.,https://ieeexplore.ieee.org/document/5445635/,2010 Second International Conference on Computer Engineering and Applications,19-21 March 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCRD.2011.5764067,Table of contents vol. 01,IEEE,Conferences,The following topics are dealt with: computer research and development; event driven programming; artificial intelligence; expert systems; algorithm analysis; high performance computing; automated software engineering; human computer interaction; bioinformatics; scientific computing; image processing; information retrieval; compilers; interpreters; computational intelligence; computer architecture; embedded systems; computer animation; Internet; Web applications; communication/networking; knowledge data engineering; computer system implementation; logics; VLSI; mathematical software; information systems; computer based education; mathematical logic; mobile computing; computer games; multimedia applications; computer graphics; virtual reality; natural language processing; neural networks; computer modeling; parallel computing; distributed computing; computer networks; pattern recognition; computer security; computer simulation; computer vision; probability; statistics; performance evaluation; computer aided design/manufacturing; computing ethics; programming languages; problem complexity; control systems; physical sciences; engineering; discrete mathematics; reconfigurable computing systems; data communications; robotics; automation; system security; cryptography; data compression; data encryption; data mining; database systems; document processing; text processing; educational technology; digital library; technology management; digital signal processing; theoretical computer science; digital systems; logic design; ubiquitous computing; and visualizations.,https://ieeexplore.ieee.org/document/5764067/,2011 3rd International Conference on Computer Research and Development,11-13 March 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSA.2007.90,The 2007 International Conference Computational Science and its Applications - Title,IEEE,Conferences,The following topics are dealt with: computational intelligence; high performance technical computing; information systems; Web based learning; component based software engineering; software process model; computational geometry; distributed computing; digital content security; data storage devices; data storage systems; intelligent design technology; intelligent data mining; information services; information technologies; mobile multimedia networks; pattern recognition; ubiquitous computing; computer graphics; computational science; wireless sensor networks; virtual reality; optimization; mobile communication; molecular simulations.,https://ieeexplore.ieee.org/document/4301111/,2007 International Conference on Computational Science and its Applications (ICCSA 2007),26-29 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEKIM52309.2021.00100,The Creation of Multi Intelligence Music Classroom in Children's Enlightenment Stage Based on Virtual Reality Technology,IEEE,Conferences,"With the development of virtual reality technology, the real realization of virtual reality will cause great changes in human life and development. VR is the abbreviation of virtual reality, which means virtual reality in Chinese. Virtual reality is the ultimate application form of multimedia technology. It is the crystallization of the rapid development of computer hardware and software technology, sensor technology, robot technology, artificial intelligence and behavioral psychology. Because of this, the combination of VR and education classroom will become the inevitable trend of future education development. In the context of the development of traditional education, the enlightenment and influence of music education classroom on children is point like, and through the intervention and influence of VR technology, the influence of VR on music education begins to appear face like, and can maximize the mental and thinking ability of preschool children. Through the above analysis, this paper concludes the feasibility and importance of introducing VR technology into the current music education classroom, and will explore for the development of multiple intelligence education.",https://ieeexplore.ieee.org/document/9479537/,"2021 2nd International Conference on Education, Knowledge and Information Management (ICEKIM)",29-31 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.198,The Design and Optimization of H.264 Encoder Based on the Nexperia Platform,IEEE,Conferences,"The Nexperia PNX1501 media processor is a complete audio/video/graphics system on a chip that contains a high-performance 32-bit VLIW processor, capable of high quality software video, audio signal processing. Based on Philips Semiconductor's multimedia processing chip Nexperia PNX1501, designs a new H.264 encoder with high performance for real time signal processing. Chooses the rapid integral discrete cosine translation and the advanced hexagon searching algorithm, accomplishes the intelligent optimization and comprehensive debugging for codes. The results of experiments indicate that the encoder can encode all kinds of video data rapidly, and can realize the real time processing for all kinds of video signals.",https://ieeexplore.ieee.org/document/4287505/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1999.812388,The script based virtual actor control for interactive drama,IEEE,Conferences,"As multimedia services have been generalized, users wish to interact with service providers while converting services as desired rather than being provided with one-sided services. This paper discusses script-based virtual actor control, which is one kind of such application for interactive drama or interactive cinema. The entire system is in the client/server form, where the client generates scenario graphs, which are a type of scripts, after receiving various scenario information for developing stories from the server providing the services, and it generates and shows other scenes according to its conversation with and the action of the user, while traversing the generated graphs. Information on the movements of virtual actors are all stored in the form of files, and are animated through a motion generation engine whenever necessary, while traversing the generated scenario graphs.",https://ieeexplore.ieee.org/document/812388/,"IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)",12-15 Oct. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETCE48199.2020.9091776,Tools and Techniques in optimization of Network Resources,IEEE,Conferences,"Any network, whether private or public is usually governed by a number of conflicting goals such as meaningful communication between the systems in a heterogeneous routing environment. Exchange of network meta-data for forwarding and routing, distribution of network resources for optimization and ensuring the quality of service (QoS), and sharing of network resources using heterogeneous formats of files are the other parameters. The above-mentioned goals are related to security and bandwidth. The other parameters are required for hardware with software compatibility in the implementation of different protocols. Especially, communication takes place in a real-life environment (i.e. consisting of multimedia and hypermedia files) carrying out the optimization. Such optimization of resources can be achieved using certain available tools and techniques. The paper consists of two parts: the first part is a survey of the tools of the trade and the techniques based on the optimal usages of the tools. The second part shows the implementation details of one of the techniques, for example, security in a particular layer such as transport/IP layer. The experimentation is to show that vulnerable ports can be closed by killing the processes using operating system tools to make the system secure.",https://ieeexplore.ieee.org/document/9091776/,2020 3rd International Conference on Emerging Technologies in Computer Engineering: Machine Learning and Internet of Things (ICETCE),7-8 Feb. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARCV.2002.1234946,Unsupervised semantic video objects segmentation over optical-flow field,IEEE,Conferences,"An unsupervised semantic video objects segmentation system is introduced in this paper, which is a region-based non-parametric spatio-temporal approach over optical-flow field. The proposed method overcomes multiple drawbacks inherited in existing supervised pixel-based parametric schemes. The unsupervised mechanism is realized by extracting the phase of the optical-flow field and forming the phase histogram to identify the number of dominant video objects contained within the video frame. Through extensive simulations, dominant video objects are automatically detected and segmented with high accuracy. The segmented VOs have semantic meaning that matches human being's perception; thus, the proposed segmentation system should be very useful to many applications encountered in multimedia, virtual reality and computer vision.",https://ieeexplore.ieee.org/document/1234946/,"7th International Conference on Control, Automation, Robotics and Vision, 2002. ICARCV 2002.",2-5 Dec. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.2003.1562212,Video source traffic flow prediction using neural networks,IEEE,Conferences,"Prediction of traffic generated by multimedia sources can facilitate effective dynamic bandwidth allocation and implementation of quality-of-service (QoS) control strategies at the network edges. The time-series representing frame or VOP sizes of an MPEG-coded stream is extremely noisy and it has very long-range time dependencies. This paper proposes an approach to develop predictors for single-step-ahead (SS) and multi-step-ahead (MS) prediction of MPEG-coded real-time video traffic. The designed SS predictor consists of a recurrent neural network for I-VOPs and two feedforward neural networks for P- and B-VOPs, respectively. The SS prediction scheme is used for frame-by-frame prediction. A moving average of the frame or VOP sizes time-series is generated from the individual frame or VOP sizes and used for both SS and MS prediction. The designed MS predictor consists of a recurrent neural network and it is used to perform two-step-ahead and four-step-ahead prediction. Two-step and four-step-ahead prediction correspond to MS prediction horizons of 1 and 2 seconds, respectively",https://ieeexplore.ieee.org/document/1562212/,2003 46th Midwest Symposium on Circuits and Systems,27-30 Dec. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITHET.2012.6246058,Virtual industrial training: Joining innovative interfaces with plant modeling,IEEE,Conferences,"Training in industry is one of the most critical and expensive tasks to be faced by the management. Furthermore, in some cases, it is dangerous or even impossible to directly train operators on the real plants where security and safety problems may arise, making it very difficult to start training programs at low cost. For these reasons, the field of training in industry is rapidly developing using software or hardware solutions coming mainly from the following research areas: i) Human-Computer interaction, i.e., the use of complex and interactive human-machine interfaces, ii) plant simulators, i.e., software systems which are delivered with the plant itself to test and to learn complex tasks and processes, iii) Intelligent Training Systems, i.e., the availability of intelligent and personalized training systems where a virtual tutor guides users through a personalized learning path. In this paper we present the overall architecture of a system for industrial training, embedded into an Intelligent Tutoring System that can provide more effective and personalized training and learning in a context where working directly on real plants can be difficult and very expensive. In particular we present a simulator for training operators in using power plants, based on a multimedia and on interactive interface. This system is particularly suitable to be used for training in industrial electric and oil plants. Moreover, the system allows operators for collaborative problem solving. Currently the system is under delivery to an Italian Electric industry.",https://ieeexplore.ieee.org/document/6246058/,2012 International Conference on Information Technology Based Higher Education and Training (ITHET),21-23 June 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2016.0091,Visual Context Learning with Big Data Analytics,IEEE,Conferences,"Understanding contextual information composed of both text and images is very useful for multimedia information processing. However, capturing such contexts is not trivial, especially while dealing with real datasets. Existing solutions such as using ontologies (e.g., WordNet) are mainly interested in individual terms, but they do not support identifying a group of terms that describe a specific context available at runtime. Within our knowledge, there are very limited solutions regarding the integration of contextual information from both images and text. Furthermore, existing solutions are not scalable due to the computationally intensive tasks and are prone to data sparsity. In this paper, we propose a semantic framework, called VisContext that is based on a contextual model combined with images and text. The VisContext framework is based on the scalable pipeline that is composed of the primary components as follows: (i)Natural Language Processing (NLP), (ii) Feature extraction using Term Frequency-Inverse Document Frequency (TF-IDF), (iii) Feature association using unsupervised learning algorithms including K-Means clustering (KM) and Expectation-Maximization(EM) algorithms, iv) Validation of visual context models using supervised learning algorithms (Naïve Bayes, Decision Trees, Random Forests). The proposed VisContext framework has been implemented with the Spark MLlib and CoreNLP. We have evaluated the effectiveness of the framework in visual understanding with three large datasets (IAPR, Flick3k, SBU) containing more than one million images and their annotations. The results are reported in the discovery of the contextual association of terms and images, image context visualization, and image classification based on contexts.",https://ieeexplore.ieee.org/document/7836722/,2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW),12-15 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.2006.322736,Work in Progress: An Intelligent Tutoring System for Forensic Biology,IEEE,Conferences,"The Interactive Virtual Intelligent System for Scientific Inquiry in a Biology Learning Environment (INVISSIBLE) is software environment being developed as a intelligent tutoring system that provides high school biology students a virtual, hands-on, multimedia learning environment. Using interactive, intelligent software, a student is placed in goal driven scenarios that reflect the authentic experiences of a scientist engaged in using scientific inquiry methods. This paper describes the first of three planned modules, one which involves forensic science and the use of DNA evidence in combination with hairs, fibers, and other evidence in solving a crime scene problem. Core objectives of this module are to increase student learning regarding: (a) knowledge acquisition of content, concepts and principles relevant to genetics, forensic science, and evolutionary biology, (b) relevant scientific process skills and knowledge, and (c) knowledge of nature and methods of science. A demonstration of the software will be given",https://ieeexplore.ieee.org/document/4117057/,Proceedings. Frontiers in Education. 36th Annual Conference,27-31 Oct. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NDT.2009.5272059,[Copyright notice],IEEE,Conferences,The following topics are dealt with: data mining; genetic algorithm; software engineering; Web applications; information management; data management; wireless networks; databases; neural networks; ad hoc networks; real time systems; social network; security; multimedia; multi-agent system; artificial intelligence; distributed systems; parallel systems.,https://ieeexplore.ieee.org/document/5272059/,2009 First International Conference on Networked Digital Technologies,28-31 July 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOSP.2010.5655737,[Front matter],IEEE,Conferences,The following topics are dealt with: digital signal processing; adaptive signal processing; spectrum estimation and modeling; spectrum analysis; wavelet transforms; higher order spectral analysis; adaptive filtering; array signal processing; hardware implementation; speech and audio coding; and speech synthesis and recognition; video compression & streaming; computer vision & VR; multimedia & human-computer interaction; statistic learning; pattern recognition; AI & neural networks; communication signal processing; Internet; and wireless communications.,https://ieeexplore.ieee.org/document/5655737/,IEEE 10th INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING PROCEEDINGS,24-28 Oct. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2008.1,[Title page i - Volume 1],IEEE,Conferences,The following topics were dealt with: artificial intelligence; knowledge representation and reasoning; machine learning; trends in SAT and CSPs; knowledge-based systems; AI in multimedia and real-time systems; AI algorithms; collaborative software agents; vision/image processing and understanding; search and heuristics; constraint programming; information retrieval; semantic web techniques and technologies; planning and scheduling; data mining and knowledge discovery with imbalanced data; AI in databases and data mining.,https://ieeexplore.ieee.org/document/4669658/,2008 20th IEEE International Conference on Tools with Artificial Intelligence,3-5 Nov. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISIS.2010.1,[Title page i],IEEE,Conferences,The following topics are dealt with: software engineering; data mining; database; intelligent context aware system; artificial intelligence; autonomic system; biomedical-inspired system; socially inspired system; network control; mobile network; wireless network; pervasive computing; ubiquitous computing; ad hoc network; wireless sensor network; P2P system; distributed system; ontologies; semantic Web; security of data; data privacy; middleware; grid computing; Internet computing; resource management; biotechnology; human computer interface; mobile computing; resource usage optimisation; interconnection network; data visualization; virtual environment architecture; multimedia; and haptic interface.,https://ieeexplore.ieee.org/document/5447304/,"2010 International Conference on Complex, Intelligent and Software Intensive Systems",15-18 Feb. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSTE.2010.5608973,[Title page],IEEE,Conferences,The following papers are dealt with: component-based software engineering; artificial intelligence; computer animation; data mining; knowledge recovery; process engineering; distributed intelligent system; software security; formal methods; human-computer interaction; information management systems; middleware design technique; mobile computing; object-oriented technology; parallel computing; reverse engineering; software architecture; virtual reality; computer graphics; and multimedia computing.,https://ieeexplore.ieee.org/document/5608973/,2010 2nd International Conference on Software Technology and Engineering,3-5 Oct. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2854922,3D Panoramic Virtual Reality Video Quality Assessment Based on 3D Convolutional Neural Networks,IEEE,Journals,"Virtual reality (VR), a new type of simulation and interaction technology, has aroused widespread attention and research interest. It is necessary to evaluate the VR quality and provide a standard for the rapidly developing technology. To the best of our knowledge, a few researchers have built benchmark databases and designed related algorithms, which has hindered the further development of the VR technology. In this paper, a free available data set (VRQ-TJU) for VR quality assessment is proposed with subjective scores for each sample data. The validity for the designed database has been proved based on the traditional multimedia quality assessment metrics. In addition, an end-to-end 3-D convolutional neural network is introduced to predict the VR video quality without a referenced VR video. This method can extract spatiotemporal features and does not require using hand-crafted features. At the same time, a new score fusion strategy is designed based on the characteristics of the VR video projection process. Taking the pre-processed VR video patches as input, the network captures local spatiotemporal features and gets the score of every patch. Then, the new quality score fusion strategy is applied to get the final score. Such approach shows advanced performance on this database.",https://ieeexplore.ieee.org/document/8409959/,IEEE Access,2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OJCOMS.2021.3122844,A Deep Neural Network-Based Multi-Label Classifier for SLA Violation Prediction in a Latency Sensitive NFV Application,IEEE,Journals,"Recent advancements in the domain of Network Function Virtualization (NFV), and rollout of next-generation networks have necessitated the requirement for the upkeep of latency-critical application architectures in future networks and communications. While Cloud service providers recognize the evolving mission-critical requirements in latency sensitive verticals such as autonomous driving, multimedia, gaming, telecommunications, and virtual reality, there is a wide gap to bridge the Quality of Service (QoS) constraints for the end-user experience. Most latency-critical services are over-provisioned on all fronts to offer reliability, which is inefficient towards scalability in the long run. To address this, we propose a strategy to model frequent violations on the application level as a multi-output target to enable more complex decision-making in the management of virtualised communication networks. In this work, we utilize data from a real-world deployment to configure and draft a realistic set of Service Level Objectives (SLOs) for a voice based NFV application, and develop a deep neural network based multi-label classification methodology to identify and predict multiple categories of SLO breaches associated with an application state. With this, we aim to gain granular SLA and SLO violation insights, enabling us to study and mitigate their impact and inform precision in drafting proactive scaling policies. We further compare the performance against a set of multi-label compatible machine learning classifiers, and address class imbalance in a multi-label setup. We perform a comprehensive evaluation to assess the performance on example-based, label-based and ranking-based measures, and demonstrate the suitability of deep learning in such a use-case.",https://ieeexplore.ieee.org/document/9592636/,IEEE Open Journal of the Communications Society,2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2905875,A Novel Definition of Equivalent Uniform Dose Based on Volume Dose Curve,IEEE,Journals,"With the improvement of mobile device performance, the requirement of equivalent dose description in intensity-modulated radiation therapy is increasing in mobile multimedia for healthcare. The emergence of mobile cloud computing will provide cloud servers and storage for intensity-modulated radiotherapy (IMRT) mobile applications, thus realizing visualized radiotherapy in a real sense. Equivalent uniform dose (EUD) is a biomedical indicator based on the dose measure. In this paper, the dose volume histogram is used to describe the dose distribution of different tissues in target and nontarget regions. The traditional definition of EUD, such as the exponential form and the linear form, has only a few parameters in the model for fast calculation. However, there is no close relationship between this traditional definition and the dose volume histogram. In order to establish the consistency between the EUD and the dose volume histogram, this paper proposes a novel definition of EUD based on the volume dose curve, called VD-EUD. By using a unique organic volume weight curve, it is easy to calculate VD-EUD for different dose distributions. In definition, different weight curves are used to represent the biological effects of different organs. For the target area, we should be more careful about those voxels with a low dose (cold point); thus, the weight curve is monotonically decreasing. While for the nontarget area, the curve is monotonically increasing. Furthermore, we present the curves for parallel, serial, and mixed organs of nontarget areas separately, and we define the weight curve form with only two parameters. Medical doctors can adjust the curve interactively according to different patients and organs. We also propose a fluence map optimization model with the VD-EUD constraint, which means that the proposed EUD constraint will lead to a large feasible solution space. We compare the generalized EUD (gEUD) and the proposed VD-EUD by experiments, which show that the VD-EUD has a closer relationship with the dose volume histogram. If the biological survival probability is equivalent to the VD-EUD, the feasible solution space would be large, and the target areas can be covered. By establishing a personalized organic weight curve, medical doctors can have a unique VD-EUD for each patient. By using the flexible and adjustable EUD definition, we can establish the VD-EUD-based fluence map optimization model, which will lead to a larger solution space than the traditional dose volume constraint-based model. The VD-EUD is a new definition; thus, we need more clinical testing and verification.",https://ieeexplore.ieee.org/document/8673563/,IEEE Access,2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JBHI.2019.2891729,A Smartphone Application for Automated Decision Support in Cognitive Task Based Evaluation of Central Nervous System Motor Disorders,IEEE,Journals,"Background and Objective: New technology enables constant boost to the powers of mobile devices, which in the previous years have transformed from simple mobile phones to smart phones. Computational powers of these electronics enable actions that previously were possible only for computers. By the use of special applications, we may benefit from sensors and multimedia capabilities of operating systems. Therefore, a new era for devoted implementations opens, in which a smart application can take a role of computing system to estimate the symptoms of diseases by evaluating signals coming from a human body. Methods: We propose a model of an application implemented for mobile android systems, which can be used for examination of central nervous system motor disorders occurring in patients suffering from Huntington (HD), Alzheimer, or Parkinson diseases. In particular, the model tracks tremors (involuntary movements), and cognitive (memory loss or dementia) impairments using touch and visual stimulus modalities. The proposed model interprets the symptoms from human bodies that indicate one of the diseases of the nervous system. Pre-processing of collected data for feature extraction is executed on a mobile device by using core functionality and methods provided in android's application programming interface. The information is evaluated by a back-propagation neural network classifier and the result is presented to the end user. The system is able to contact medical supervision and provide an assistance from the clinic. Results: The system uses a collected dataset of 1928 records, taken from 11 HD patients and 11 healthy persons in Lithuania, to gather statistics about examinations and presents the results as medical evaluation with prediction on the state of health. The accuracy of recognition of early, prodromal symptoms for central nervous system motor disorders is 86.4% (F-measure 0.859). The app (available on Google Play) is easy to use and is efficient tool for decision support in medical examinations. Conclusions: The use of intelligent apps which can help to evaluate neurodegenerative disorders is an important enhancement to medical diagnosis. The developed smartphone app supports the doctor with additional results that are easy to compare with other examinations. This kind of examination is a nice change from classic stereotypes, especially for younger age patients, who are used to various aspects of information technology.",https://ieeexplore.ieee.org/document/8606105/,IEEE Journal of Biomedical and Health Informatics,Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/JCIN.2022.9745481,Adaptive Digital Twin for Vehicular Edge Computing and Networks,PTP,Journals,"To better support the emerging vehicular applications and multimedia services, vehicular edge computing (VEC) provides computing and caching services in proximity to vehicles, by reducing network transmission latency and alleviating network congestion. However, current VEC networks may face some implementation challenges, such as high mobility of vehicles, dynamic vehicular environment, and complex network scheduling. Digital twin, as an emerging technology, can make the virtual representation of physical networks to predict, estimate, and analyze the real-time network state. In this paper, we integrate digital twin into VEC networks to adaptively make network management and policy schedule. We first introduce the framework of VEC networks and present the key problems in a VEC network. Next, we give the concept of digital twin and propose an adaptive digital twin-enabled VEC network. In the proposed network, digital twin can enable adaptive network management via the two-closed loops between physical VEC networks and digital twins. Further, we propose a digital twin empowered VEC offloading problem with vehicle digital models and road side unit (RSU) digital models. A deep reinforcement learning (DRL)-based offloading scheme is designed to minimize the total offloading latency. Numerical results demonstrate the effectiveness of the proposed DRL-based algorithm for VEC offloading.",https://ieeexplore.ieee.org/document/9745481/,Journal of Communications and Information Networks,March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSTSP.2009.2039180,Adaptive Topologic Optimization for Large-Scale Stream Mining,IEEE,Journals,"Real-time classification and identification of specific features in high-volume data streams are critical for a plethora of applications, including large-scale multimedia analysis, processing, and retrieval. Content of interest is filtered using a collection of binary classifiers that are deployed on distributed resource-constrained infrastructure. In this paper, we focus on selecting the optimal topology (chain) of classifiers, and present algorithms for classifier ordering and configuration, to tradeoff accuracy of feature identification with filtering delay. The order selection is dependent on the data characteristics, system resource constraints as well as the performance and complexity characteristics of each classifier. We first develop centralized algorithms for joint ordering and individual classifier operating point selection. We then propose a decentralized approach and use reinforcement learning methods to design a dynamic routing based order selection strategy. We investigate different learning strategies that lead to rapid convergence, while requiring minimum coordination and message exchange.",https://ieeexplore.ieee.org/document/5447618/,IEEE Journal of Selected Topics in Signal Processing,June 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3006103,An Application of Ternary Hash Retrieval Method for Remote Sensing Images in Panoramic Video,IEEE,Journals,"With the development of multimedia technology, the application of panoramic video is emerging, and there are specific application scenarios in the field of drones. Remote sensing images have tremendous applications in data acquisition and processing in the panoramic maps used to construct the panoramic videos. Efficient remote sensing image retrieval can greatly improve the efficiency of image utilization and get a fast response in the construction of panoramic video. Thus, how to achieve fast and efficient remote sensing image retrieval becomes increasingly important. Scholars have explored a number of ways to address this problem, from traditional manual feature-based retrieval methods to deep hash retrieval methods that today can cope with large data volumes. The hash method is a way to encode high-dimensional data into a set of binary codes while maintaining similarity between images. In this paper, we propose an end-to-end ternary label depth hashing method that is capable of both feature learning and hash coding processes in one-stage. The experimental results show that the algorithm has high accuracy and good retrieval effect.",https://ieeexplore.ieee.org/document/9129722/,IEEE Access,2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TBC.2021.3077758,An Elastic System Architecture for Edge Based Low Latency Interactive Video Applications,IEEE,Journals,"5G and edge computing have brought great changes to video industry. Interactive video is becoming an emerging application form of multimedia service, which provides attraction beyond typical scenarios like cloud gaming and remote virtual reality (VR), and puts forward great challenges in resource capacity, response latency, and function flexibility to its service system. In this paper, we propose an elastic system architecture with low latency features to accommodate generic interactive video applications on near user edges. To increase system flexibility, we firstly design a dynamic Directed Acyclic Graph (dDAG) model for efficient task representation. Secondly, based on the model, we present the elastic architecture together with its scalable workflow pipeline. Thirdly, we propose a set of novel latency measurement metrics to analyze and optimize the performance of an interactive video system. Based on the proposed approaches, we disassemble a real world free-viewpoint synthesis application and benchmark its performance with the metrics. Extensive experimental results show the flexibility of our system to handle the stochastic human interactions during a video service session, with less than 5 ms additional scheduling latency introduced. End to end latency is kept within 43 ms for complex functions, and 28 ms for simpler scenarios, which satisfies the restrictions of most interactive video applications provided by an edge. Client of the architecture serves as a pure video player, which is also friendly to power limited terminals such as 5G phones. Efficiency and stability analyses of the system show superiorities over existing work, and also reveal potential optimization directions for future research.",https://ieeexplore.ieee.org/document/9431586/,IEEE Transactions on Broadcasting,Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/21.135684,An intelligent agent framework for enterprise integration,IEEE,Journals,"The authors present a framework in which human and intelligent agents (IAs) can interact to facilitate the information flow and decision making in real-world enterprises. Underlying the framework is the notion of an enterprise model that is built by dividing complex enterprise operations into a collection of elementary tasks or activities. Each such task is then modeled in cognitive terms and entrusted to an IA for execution. Tasks that require human involvement are referred to the appropriate person through their personal assistant, a special type of IA that knows how to communicate both with humans, through multimedia interfaces, and with other IAs and the shared knowledge base. The computer-aided software engineering tools supported by a library of activity models permit every individual in an enterprise to model the activities with which they are personally most familiar. The preliminary experimental results suggest that this divide-and-conquer strategy, leading to cognitive models that are buildable and maintainable by end-users, is a viable approach to real-world distributed artificial intelligence.<>",https://ieeexplore.ieee.org/document/135684/,"IEEE Transactions on Systems, Man, and Cybernetics",Nov.-Dec. 1991,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIM.2010.2082750,Audio Signature-Based Condition Monitoring of Internal Combustion Engine Using FFT and Correlation Approach,IEEE,Journals,"This work proposes a novel prototype-based engine fault classification scheme employing the audio signature of engines. In this scheme, Fourier transform and correlation methods have been used. Notably, automated audio classification has immense significance in the present times, used in both audio-based content retrieval and audio indexing in multimedia industry. Likewise, it is also becoming increasingly important in automobile industries. It has been observed that real world automobile engine audio data are contaminated with substantial noise and out fliers. Hence, it is challenging to categorize different fault types in different engines. Accordingly, the present paper discusses a methodology where a set of algorithms checks the state of an unknown engine as either healthy or faulty. Fault categorizing algorithm is based on its cross- and autocorrelation coefficient values. Appropriately, in this study, the engine amplitude-frequency values of fast Fourier transform are calculated and subdivided into bands to calculate the correlation coefficient matrix. The correlation coefficient matrix for the unknown engine is then calculated and matched with this “prototype” engine matrix to categorize it into a single or multiple fault(s). It is worth mentioning here that although a rank-based maximum close scheme is adopted for finding the unknown engine's fault, the work can be extended to any other parametric and neural network-based classification scheme. Keeping this background in mind, the present paper discusses the proposed methodology to find a prototype engine, unknown engine classification, implementation on real audio signal for single cylinder engine data, and its results.",https://ieeexplore.ieee.org/document/5618563/,IEEE Transactions on Instrumentation and Measurement,April 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3182009,Deep Reinforcement Learning Based Routing in IP Media Broadcast Networks: Feasibility and Performance,IEEE,Journals,"The media broadcast industry has evolved from Serial Digital Interface (SDI) based infrastructures to IP networks. While IP based video broadcast is well established in the data plane, the use of IP networks to transport media flows still poses challenges in terms of resource management and orchestration. Software Defined Networking (SDN) based orchestration architectures have emerged in the industry that use SDN to route the media flows of a broadcast service across the provider IP network. Several approaches to multimedia flow routing in IP based SDN networks have been proposed in the context of streaming applications over the Internet. These range from model based linear optimization solutions that have high complexity to simple shortest path based routing with either Static Link Costs (SLC) or Dynamic Link Costs (DLC). More recently model-free optimization methods such as Deep Reinforcement Learning (DRL) have been proposed for routing and Traffic Engineering (TE) of multimedia flows in SDN networks. The media broadcast scenario however has specific requirements, with services like Master Control Room (MCR) operation and live broadcasting of events, and it has been rarely addressed in the literature. In this work we propose a DRL based routing method for this scenario and compare it to SLC and DLC algorithms based on Dijkstra shortest paths. This is, to our knowledge, the first work to follow this approach in the context of media broadcast services in IP infrastructures. The algorithm is designed considering the specifications and capabilities of one of the leading SDN orchestrators in the market and considers the more common Service Level Agreement (SLA) requirements in the industry. Three different DRL algorithms are implemented and compared and we evaluate them using a real service provider network topology. The results indicate that DRL based routing is applicable in real production scenarios and that it achieves considerable performance gains when compared to the SLC and DLC shortest path algorithms commonly used today.",https://ieeexplore.ieee.org/document/9793657/,IEEE Access,2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2923552,DeepStyle: Multimodal Search Engine for Fashion and Interior Design,IEEE,Journals,"In this paper, we propose a multimodal search engine that combines visual and textual cues to retrieve items from a multimedia database aesthetically similar to the query. The goal of our engine is to enable intuitive retrieval of fashion merchandise such as clothes or furniture. Existing search engines treat textual input only as an additional source of information about the query image and do not correspond to the real-life scenario, where the user looks for “the same shirt but of denim”. Our novel method, dubbed DeepStyle, mitigates those shortcomings by using a joint neural network architecture to model contextual dependencies between features of different modalities. We prove the robustness of this approach on two different challenging datasets of fashion items and furniture where our DeepStyle engine outperforms baseline methods by more than 20% on tested datasets. Our search engine is commercially deployed and available through a Web-based application.",https://ieeexplore.ieee.org/document/8737943/,IEEE Access,2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2971785,Digital Video Source Identification Based on Container’s Structure Analysis,IEEE,Journals,"The mobile device ecosystem has dramatically evolved over the last few years, since users have openly embraced a massive use of mobile phones for different purposes: professional use, personal use, etc. Digital videos can be used to define legal responsibilities or as part of the evidence in trials. The forensic analysis of digital videos becomes very relevant to determine the origin and authenticity of a video in order to link an individual with a device, place or event. The field of forensic analysis of digital videos is constantly facing new and direct challenges. Even though the basic principles of this discipline remain unchanged, numerous issues appear every year that require new procedures and tools. Therefore, it is necessary to provide forensic analysts with techniques to identify the origin of multimedia content. In this paper, the topic of source identification in open scenarios will be discussed, since analysts do not know in advance the set of cameras to which a video belongs so they find it difficult to identify its source. This approach is similar to real-life situations since in most cases, analysts are unaware of the set of video cameras. This paper aims to create a technique that identifies the source of digital videos generated by digital devices through the use of unsupervised algorithms based on the analysis of the structure of multimedia video devices.",https://ieeexplore.ieee.org/document/8984287/,IEEE Access,2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TII.2020.3020386,Edge Computing-Enabled Deep Learning for Real-time Video Optimization in IIoT,IEEE,Journals,"Real-time multimedia applications have gained immense popularity in the industrial Internet of Things (IIoT) paradigm. Due to the impact of the complex industrial environment, the transmission of video streaming is usually unstable. In the duration of a low bandwidth transmission, existing optimization methods often reduce the original resolution of some frames in a random way to avoid the video interruption. If the key frames with some important content are selected to be transmitted with a low resolution, it will greatly reduce the effect of industrial supervision. In view of this challenge, a real-time video streaming optimization method by reducing the number of video frames transmitted in the IIoT environment is proposed. Concretely, a deep learning-based object detection algorithm is recruited to effectively select the key frames in our method. The key frames with the original resolution will be transmitted along with audio data. As some nonkey frames are selectively discarded, it is helpful for smooth network transmitting with fewer bandwidth requirements. Moreover, we employ edge servers to run the object detection algorithm, and adjust video transmission flexibly. Extensive experiments are conducted to validate the effectiveness, and dependability of our method.",https://ieeexplore.ieee.org/document/9181434/,IEEE Transactions on Industrial Informatics,April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2789918,Energy-Efficient Architecture for Wireless Sensor Networks in Healthcare Applications,IEEE,Journals,"The need to deploy wireless sensor networks (WSNs) for real-world applications, such as mobile multimedia for healthcare organizations, is increasing spectacularly. However, the energy problem remains one of the core barriers preventing an increase in investment in this technology. In this paper, we propose a new technique to resolve the problems due to limited energy sources. Using a quaternary transceiver (in the architecture on a sensor node), instead of a binary one, which will use the amplitude/phase, modulator/demodulator units to increase the number of bits transmitted per symbol. The system will reduce the consumption of energy in the transmission phase due to the increased bits transmitted per symbol. Moreover, neural network static random access memory (NN-SRAM) implementation in a clusteringbased system for energy-constrained WSNs is proposed. The scheme reduces the total amount of energy consumption in storage and transmissions during the data dissemination process. Through simulation results based on MATLAB and Spice software tools, it is shown that the neural network static random access memory implementation in a clustering-based system reduces the energy consumption of the entire system by about 76.99%.",https://ieeexplore.ieee.org/document/8247183/,IEEE Access,2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2953565,FWFS: Selecting Robust Features Towards Reliable and Stable Traffic Classifier in SDN,IEEE,Journals,"Real-time Internet traffic flow classification is important in managing network resources in accordance to Quality of Service (QoS) requirements. The centralized network's control in Software Defined Networking (SDN) provides a platform for Internet Service Provider (ISP) to perform specific actions on the classified flows through routing and scheduling. Though machine learning (ML) can be the alternative to Deep Packet Inspection (DPI) in classifying SDN traffic flows, several problems, such as classifier's accuracy, computational complexity, multi-class imbalanced data, and concept drift, need to be addressed in order to have a reliable solution. Therefore, this work has proposed a hybrid filter-wrapper feature selection (FS) algorithm, named Filter-Wrapper Feature Selection (FWFS). The algorithm selects robust features that represent minority classes and resistant to concept drift and is also computationally inexpensive by discarding irrelevant features before further processing with wrapper function. Based on the performance evaluation, the feature selection process of FWFS is computationally inexpensive; i.e. 59.6s, which produces a classifier with an overall accuracy of 98.9%. The result is better than state-of-the-art FS algorithm, Efficient Feature Optimization Approach (EFOA) which requires >400s to select features which can produced a classifier with 97.7% accuracy. In addition to the high overall accuracy, the classifier trained with features selected by FWFS has better F-measure values for each classes including minority classes; i.e. >0.8 in MULTIMEDIA and INTERACTIVE which consist only 0.15% and 0.03% instances, respectively, of the total 377,526 instances in the dataset. Furthermore, the classifier is stable and reliable for classifying new data; i.e. 98.7% accuracy for classifying new data and F-measure of more than 0.8 in every class. The classifier model will be embedded in the SDN-ISP traffic classification solution which provides insights for resource allocations and traffic scheduling in the network.",https://ieeexplore.ieee.org/document/8901139/,IEEE Access,2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3100139,Fast Shot Boundary Detection Based on Separable Moments and Support Vector Machine,IEEE,Journals,"The large number of visual applications in multimedia sharing websites and social networks contribute to the increasing amounts of multimedia data in cyberspace. Video data is a rich source of information and considered the most demanding in terms of storage space. With the huge development of digital video production, video management becomes a challenging task. Video content analysis (VCA) aims to provide big data solutions by automating the video management. To this end, shot boundary detection (SBD) is considered an essential step in VCA. It aims to partition the video sequence into shots by detecting shot transitions. High computational cost in transition detection is considered a bottleneck for real-time applications. Thus, in this paper, a balance between detection accuracy and speed for SBD is addressed by presenting a new method for fast video processing. The proposed SBD framework is based on the concept of candidate segment selection with frame active area and separable moments. First, for each frame, the active area is selected such that only the informative content is considered. This leads to a reduction in the computational cost and disturbance factors. Second, for each active area, the moments are computed using orthogonal polynomials. Then, an adaptive threshold and inequality criteria are used to eliminate most of the non-transition frames and preserve candidate segments. For further elimination, two rounds of bisection comparisons are applied. As a result, the computational cost is reduced in the subsequent stages. Finally, machine learning statistics based on the support vector machine is implemented to detect the cut transitions. The enhancement of the proposed fast video processing method over existing methods in terms of computational complexity and accuracy is verified. The average improvements in terms of frame percentage and transition accuracy percentage are 1.63% and 2.05%, respectively. Moreover, for the proposed SBD algorithm, a comparative study is performed with state-of-the-art algorithms. The comparison results confirm the superiority of the proposed algorithm in computation time with improvement of over 38%.",https://ieeexplore.ieee.org/document/9496657/,IEEE Access,2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2016.2609407,FreeScup: A Novel Platform for Assisting Sculpture Pose Design,IEEE,Journals,"Sculpture design is challenging due to its inherent difficulty in characterizing artworks quantitatively; thus, few works have been done to assist sculpture design in the past decades in the multimedia community. We have cooperated with several sculptors on analyzing styles of different artists consisting of Giacometti, Augeuste Rodin, Henry Moore, and Marino Marini from which we find pose editing plays an important role in sculpture design. Motivated by this, we present a novel platform that allows sculptors to edit virtual three-dimensional (3-D) sculptures by a free way. The proposed platform consists of three modules, namely, sculpture initialization, sculptor-sculpture mapping, and interactive pose editing. In sculpture initialization, a virtual 3-D sculpture is first incrementally reconstructed from multiview images. Then, we define Laplace operator and its corresponding spectrum to describe the geometry information of the reconstructed sculpture. During sculptor-sculpture mapping, we apply spectral analysis on the low-frequency parts of the spectrum to search for candidate editing points on the surface of the sculpture. Next, body actions of the sculptor are captured by Kinect and further mapped onto editing points as a predefined configuration set. Finally, during interactive pose editing, a real-time Kinect-driven sculpture pose editing scheme is presented, which not only preserves geometry features of the sculpture but also allows instant changes of sculpture poses. We demonstrate that our platform successfully assists sculptors on real-time pose editing by comparing its performance with those of the existing sculpture assisting methods.",https://ieeexplore.ieee.org/document/7567515/,IEEE Transactions on Multimedia,Jan. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2021.3065580,Human Memory Update Strategy: A Multi-Layer Template Update Mechanism for Remote Visual Monitoring,IEEE,Journals,"In the era of rapid development of artificial intelligence, the integration of multimedia and human-artificial intelligence has become an important research hotspot. Especially in the multimedia environment, effective remote visual monitoring has become the exploration direction of many scholars. The use of traditional correlation filtering (CF) algorithm for real-time monitoring in the context of multimedia is a practical strategy. However, most existing filtering-based visual monitoring algorithms still have the problem of insufficient robustness and effectiveness. Therefore, by considering the strategy of updating human memory, this paper proposes a multi-layer template update mechanism to achieve effective monitoring in a multimedia environment. In this strategy, the weighted template of the high-confidence matching memory is used as the confidence memory, and the unweighted template of the low-confidence matching memory is used as the cognitive memory. Through the alternate use of confidence memory, matching memory, and cognitive memory, it is ensured that the target will not be lost during the monitoring process. Experimental results show that this strategy does not affect the speed (still real-time) and improves the robustness in the multimedia background.",https://ieeexplore.ieee.org/document/9376997/,IEEE Transactions on Multimedia,2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIP.2008.924280,Image-Based Human Age Estimation by Manifold Learning and Locally Adjusted Robust Regression,IEEE,Journals,"Estimating human age automatically via facial image analysis has lots of potential real-world applications, such as human computer interaction and multimedia communication. However, it is still a challenging problem for the existing computer vision systems to automatically and effectively estimate human ages. The aging process is determined by not only the person's gene, but also many external factors, such as health, living style, living location, and weather conditions. Males and females may also age differently. The current age estimation performance is still not good enough for practical use and more effort has to be put into this research direction. In this paper, we introduce the age manifold learning scheme for extracting face aging features and design a locally adjusted robust regressor for learning and prediction of human ages. The novel approach improves the age estimation accuracy significantly over all previous methods. The merit of the proposed approaches for image-based age estimation is shown by extensive experiments on a large internal age database and the public available FG-NET database.",https://ieeexplore.ieee.org/document/4531189/,IEEE Transactions on Image Processing,July 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TASL.2011.2163512,Interactive Spoken Document Retrieval With Suggested Key Terms Ranked by a Markov Decision Process,IEEE,Journals,"Interaction with users is a powerful strategy that potentially yields better information retrieval for all types of media, including text, images, and videos. While spoken document retrieval (SDR) is a crucial technology for multimedia access in the network era, it is also more challenging than text information retrieval because of the inevitable recognition errors. It is therefore reasonable to consider interactive functionalities for SDR systems. We propose an interactive SDR approach in which given the user's query, the system returns not only the retrieval results but also a short list of key terms describing distinct topics. The user selects these key terms to expand the query if the retrieval results are not satisfactory. The entire retrieval process is organized around a hierarchy of key terms that define the allowable state transitions; this is modeled by a Markov decision process, which is popularly used in spoken dialogue systems. By reinforcement learning with simulated users, the key terms on the short list are properly ranked such that the retrieval success rate is maximized while the number of interactive steps is minimized. Significant improvements over existing approaches were observed in preliminary experiments performed on information needs provided by real users. A prototype system was also implemented.",https://ieeexplore.ieee.org/document/6018284/,"IEEE Transactions on Audio, Speech, and Language Processing",Feb. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TWC.2021.3121584,"Joint Resource, Deployment, and Caching Optimization for AR Applications in Dynamic UAV NOMA Networks",IEEE,Journals,"The cache-enabling unmanned aerial vehicle (UAV) non-orthogonal multiple access (NOMA) networks for mixture of augmented reality (AR) and normal multimedia applications are investigated, which is assisted by UAV base stations. The user association, power allocation of NOMA, deployment of UAVs and caching placement of UAVs are jointly optimized to minimize the content delivery delay. A branch and bound (BaB) based algorithm is proposed to obtain the per-slot optimization. To cope with the dynamic content requests and mobility of users in practical scenarios, the original optimization problem is transformed to a Stackelberg game. Specifically, the game is decomposed into a leader level user association sub-problem and a number of power allocation, UAV deployment and caching placement follower level sub-problems. The long-term minimization was further solved by a deep reinforcement learning (DRL) based algorithm. Simulation result shows that the content delivery delay of the proposed BaB based algorithm is much lower than benchmark algorithms, as the optimal solution in each time slot is achieved. Meanwhile, the proposed DRL based algorithm achieves a relatively low long-term content delivery delay in the dynamic environment with lower computation complexity than BaB based algorithm.",https://ieeexplore.ieee.org/document/9591297/,IEEE Transactions on Wireless Communications,May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIE.2016.2613978,Leveraging Smart Devices for Automatic Mood-Transferring in Real-Time Oil Painting,IEEE,Journals,"Smart devices are becoming a convergent platform for multimedia sensing, computing, and communication. Digital oil painting (DOP) is a revolutionary technique where a digital artist uses a digital paintbrush to simulate the traditional bristle paintbrush of a painting to get an oil paint image. Artists explore the perceiving moods of people through colors and lights in the painting. In this paper, we attempt to exploit the convergence toward the problem of automatically transferring mood of a user in DOP in terms of ambient light and color perception. We propose a mood-based oil painting technique that can be implemented and used in current smart devices in real time without using any explicit hardware. The proposed technique is optimized significantly by reducing the processing time and power consumption. We also develop a prototype on Android operating system to create mood-based oil paint images.",https://ieeexplore.ieee.org/document/7577768/,IEEE Transactions on Industrial Electronics,Feb. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSTSP.2020.3002101,Media Forensics and DeepFakes: An Overview,IEEE,Journals,"With the rapid progress in recent years, techniques that generate and manipulate multimedia content can now provide a very advanced level of realism. The boundary between real and synthetic media has become very thin. On the one hand, this opens the door to a series of exciting applications in different fields such as creative arts, advertising, film production, and video games. On the other hand, it poses enormous security threats. Software packages freely available on the web allow any individual, without special skills, to create very realistic fake images and videos. These can be used to manipulate public opinion during elections, commit fraud, discredit or blackmail people. Therefore, there is an urgent need for automated tools capable of detecting false multimedia content and avoiding the spread of dangerous false information. This review paper aims to present an analysis of the methods for visual media integrity verification, that is, the detection of manipulated images and videos. Special emphasis will be placed on the emerging phenomenon of deepfakes, fake media created through deep learning tools, and on modern data-driven forensic methods to fight them. The analysis will help highlight the limits of current forensic tools, the most relevant issues, the upcoming challenges, and suggest future directions for research.",https://ieeexplore.ieee.org/document/9115874/,IEEE Journal of Selected Topics in Signal Processing,Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSM.2020.3034482,Mobility Management With Transferable Reinforcement Learning Trajectory Prediction,IEEE,Journals,"Future mobile networks will enable the massive deployment of mobile multimedia applications anytime and anywhere. In this context, mobility management schemes, such as handover and proactive multimedia service migration, will be essential to improve network performance. In this article, we propose a proactive mobility management approach based on group user trajectory prediction. Specifically, we introduce a mobile user trajectory prediction algorithm by combining the Long-Short Term Memory networks (LSTM) with Reinforcement Learning (RL) to automate the model training procedure. We further develop a group user trajectory predictor to reduce prediction calculation overheads of users with similar movement patterns. To validate the impact of the proposed mobility management approach, we present a virtual reality (VR) service migration scheme built on the top of the proactive handover mechanism that benefits from trajectory predictions. Experiment results validate our predictor's outstanding accuracy and its impacts on enhancing handover and service migration performance to provide quality of service assurance.",https://ieeexplore.ieee.org/document/9244233/,IEEE Transactions on Network and Service Management,Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TCSS.2016.2561200,Multivariate Data Fusion-Based Learning of Video Content and Service Distribution for Cyber Physical Social Systems,IEEE,Journals,"Integration of physical processes with the computing world is driving newer challenges for networking frameworks. Cyber physical social systems (CPSSs) are another upcoming paradigm that encompasses the ever-growing interaction between the physical, social, and cyber worlds. As communication networks form the basis of these interactions, a cognitive evaluation of networks is called for. This CPSS driven network evolution was a direction motivating this paper. With the implementation of the next generation networks, traffic from real-time interactive services, such as video conferencing, is surpassing those of conventional transactional services. As such multimedia data transportation over IP networks has stringent quality constraints in terms of required bandwidth, latency, and jitter, legacy networks with no quality of service face challenges in terms of performance. We attempt to perform a multivariate analysis of video call record data collected from a wide area organizational network over a period of time. Learning-based prediction is attempted by training four classifiers: naïve Bayes, $k$ -nearest neighbor, decision tree, and support vector machine. Two independent set of experiments were conducted with oversights of bandwidth and destination prediction. Both the discrete and continuous valued predictors were involved in the training. Performance evaluation of the generated hypothesis in both the cases was conducted using tenfold cross validation. Combined analysis using the assorted combinations of attributes was conducted, and thereafter, the effect of each feature was evaluated through singular attribute portioning. This paper presents observations, which exhibit deviations from the conventional machine learning paradigms. An attempt to increase the prediction accuracy of the classifiers was made through the boosting ensemble methodology. However, miniscule addition in performance was achieved. A maximum prediction accuracy of 81% for bandwidth and 60% for destination was obtained. Reasons of low accuracy of conventionally better performing algorithm were reasoned with a mathematical comprehension. Divergence of the obtained results from the accepted patterns poses an open research problem, particularly with respect to the nature and peculiarities of the data set. The proposed learning technique can have potential applications in social, tactical, and strategic spheres.",https://ieeexplore.ieee.org/document/7476853/,IEEE Transactions on Computational Social Systems,March 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2814075,Natural Language Description of Video Streams Using Task-Specific Feature Encoding,IEEE,Journals,"In recent years, deep learning approaches have gained great attention due to their superior performance and the availability of high speed computing resources. These approaches are also extended towards the real time processing of multimedia content exploiting its spatial and temporal structure. In this paper, we propose a deep learning-based video description framework which first extracts visual features from video frames using deep convolutional neural networks (CNN) and then pass the derived representations into a long-short term memory-based language model. In order to capture accurate information for human presence, a fine-tuned multi-task CNN is presented. The proposed pipeline is end-to-end, trainable, and capable of learning dense visual features along with an accurate framework for the generation of natural language descriptions of video streams. The evaluation is done by calculating Metric for Evaluation of Translation with Explicit ORdering and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores between system generated and human annotated video descriptions for a carefully designed data set. The video descriptions generated by the traditional feature learning and proposed deep learning frameworks are also compared through the ROUGE scores.",https://ieeexplore.ieee.org/document/8319487/,IEEE Access,2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3167446,No-Reference Video Quality Assessment Using Distortion Learning and Temporal Attention,IEEE,Journals,"The rapid growth of video consumption and multimedia applications has increased the interest of the academia and industry in building tools that can evaluate perceptual video quality. Since videos might be distorted when they are captured or transmitted, it is imperative to develop reliable methods for no-reference video quality assessment (NR-VQA). To date, most NR-VQA models in prior art have been proposed for assessing a specific category of distortion, such as authentic distortions or traditional distortions. Moreover, those developed for both authentic and traditional distortions video databases have so far led to poor performances. This resulted in the reluctance of service providers to adopt multiple NR-VQA approaches, as they prefer a single algorithm capable of accurately estimating video quality in all situations. Furthermore, many existing NR-VQA methods are computationally complex and therefore impractical for various real-life applications. In this paper, we propose a novel deep learning method for NR-VQA based on multi-task learning where the distortion of individual frames in a video and the overall quality of the video are predicted by a single neural network. This enables to train the network with a greater amount and variety of data, thereby improving its performance in testing. Additionally, our method leverages temporal attention to select the frames of a video sequence which contribute the most to its perceived quality. The proposed algorithm is evaluated on five publicly-available video quality assessment (VQA) databases containing traditional and authentic distortions. Results show that our method outperforms the state-of-the-art on traditional distortion databases such as LIVE VQA and CSIQ video, while also delivering competitive performance on databases containing authentic distortions such as KoNViD-1k, LIVE-Qualcomm and CVD2014.",https://ieeexplore.ieee.org/document/9757199/,IEEE Access,2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2017.2751966,PROVID: Progressive and Multimodal Vehicle Reidentification for Large-Scale Urban Surveillance,IEEE,Journals,"Compared with person reidentification, which has attracted concentrated attention, vehicle reidentification is an important yet frontier problem in video surveillance and has been neglected by the multimedia and vision communities. Since most existing approaches mainly consider the general vehicle appearance for reidentification while overlooking the distinct vehicle identifier, such as the license plate number, they attain suboptimal performance. In this paper, we propose PROVID, a PROgressive Vehicle re-IDentification framework based on deep neural networks. In particular, our framework not only utilizes the multimodality data in large-scale video surveillance, such as visual features, license plates, camera locations, and contextual information, but also considers vehicle reidentification in two progressive procedures: coarse-to-fine search in the feature domain, and near-to-distant search in the physical space. Furthermore, to evaluate our progressive search framework and facilitate related research, we construct the VeRi dataset, which is the most comprehensive dataset from real-world surveillance videos. It not only provides large numbers of vehicles with varied labels and sufficient cross-camera recurrences but also contains license plate numbers and contextual information. Extensive experiments on the VeRi dataset demonstrate both the accuracy and efficiency of our progressive vehicle reidentification framework.",https://ieeexplore.ieee.org/document/8036238/,IEEE Transactions on Multimedia,March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMC.2020.2990374,Power-Constrained Quality Optimization for Mobile Video Chatting With Coding-Transmission Adaptation,IEEE,Journals,"Mobile video chatting has emerged as an important Internet multimedia application that greatly enriches interpersonal communications. Mobile power efficiency is crucial to the service quality and time of video chatting on battery-limited smartphones. However, the power characteristics of the video coding and data communication are highly complex due to the time-varying network conditions and dynamic mobile energy features. This incurs crucial challenges to maintaining the low power dissipation of mobile chatting application while streaming satisfactory-quality videos. To address these challenges, this paper presents a joinT cOding-tranSmission Optimization (TOSO) protocol at application layer that performs machine learning based adaptation of the video bit rate and FEC (Forward Error Correction) coding parameters. By taking advantage of analytical and empirical models characterizing the quality-power relationship, TOSO is able to maximize video quality subject to a specified upper bound of power consumption in mobile chat application. This distinguishing feature prevents the video chat from draining battery too quickly. Moreover, it allows the smartphone operating system or the mobile user to define a desired video chat duration given the remaining battery, avoiding unpleasant conversation disruption due to battery depletion. Extensive experiments based on the Linphone platform and Exata network emulator show that TOSO outperforms baseline approaches by 29.3 percent in power conservation while achieving the same video quality level.",https://ieeexplore.ieee.org/document/9078880/,IEEE Transactions on Mobile Computing,1 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3079319,Robust and Secure Digital Image Watermarking Technique Using Arnold Transform and Memristive Chaotic Oscillators,IEEE,Journals,"With the advent of technology and multimedia production, the world has witnessed a tremendous increase in digital media attacks, which duplicates, forges and tamper the data leading to the violation of copyright laws. In this paper, a robust and secure digital image watermarking is proposed, which exploits the chaotic behaviour of the non - linear oscillators realized through Memristive diodes. The proposed scheme relies on a Human Visual System (HVS) model in order to mimic the real-life scenario. To improve the robustness of the proposed approach and to further increase the security of the digital watermarked media whilst still retaining compatibility with the real-time events, Histogram of Oriented Gradients (HOG) and extreme learning machine (ELM) is implemented. Secure key generation by means of scrambling through Arnold Transform and the coefficients of Memristive Chaotic Oscillator ensures extreme security. The watermark embedding followed the pixel transformation based on discrete cosine coefficient modification, and a semi-blind watermarking extraction procedure was carried out through trained ELM models. A detailed analysis has been presented to evaluate the tradeoff between imperceptibility, security and robustness using performance metrics like PSNR, NC, SSIM, and BER. To establish a real-time implementation of the proposed architecture, the simulated results were verified using real-time chaotic signals generated from the chaotic oscillator, which dictates excellent performance against watermarking attacks and image processing tasks.",https://ieeexplore.ieee.org/document/9427566/,IEEE Access,2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2956182,Scalable Prediction of Service-Level Events in Datacenter Infrastructure Using Deep Neural Networks,IEEE,Journals,"The complexity of cloud datacenter scenarios poses new challenges in infrastructure management processes such as the impracticality of collecting specific service level events from inside the datacenter infrastructure, and the scalability issues that can appear during event monitoring when thousands of virtual machines have to be polled at a granularity of seconds. Therefore, it would be desirable to provide mechanisms for obtaining these types of events without incurring in the previously described problems. To this end, we propose a generic and scalable method based on the application of deep neural network architectures for predicting service level events using only a reduced number of generic datacenter infrastructure statistics that can be monitored in a scalable way. We demonstrate in a controlled scenario of a real datacenter and using only three variables from a physical machine that it is possible to predict events in real-time and with decent accuracy, without needing to deploy any meter in the end-user equipment. Specifically, we demonstrate this over two service-level events: i) the so-called Noisy Neighbors effect, a harmful situation that appears in physical machines due to the interferences created by the interaction of virtual machines running on them; and ii) the jitter values of a multimedia call running in a virtual machine. We set up a testbed in a real datacenter deploying physical and virtual machines, running a large amount of different experiments for 1000 hours and collecting samples at a 10 seconds granularity in a dataset of 260,000 records. Two different scenarios, in which training and testing data sets contain significant statistical differences, are deployed to demonstrate a better generalization ability of deep models in changing scenarios when compared with traditional Machine Learning techniques. A set of different deep architectures are proposed for both use cases and approximately 4,000 deep models were trained and tested. In both use cases, the best deep models show a good performance when predicting service level events, even if the inputs do not exactly follow the statistical patterns of the data used during training.",https://ieeexplore.ieee.org/document/8915840/,IEEE Access,2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSAC.2004.825962,Serving radio network controller relocation for UMTS all-IP network,IEEE,Journals,"To support real-time multimedia services in UMTS all-IP network, Third-Generation Partnership Project TR 25.936 proposed two approaches to support real-time serving radio network controller (SRNC) switching, which require packet duplication during SRNC relocation. These approaches significantly consume extra system resources. This paper proposes the fast SRNC relocation (FSR) approach that does not duplicate packets. In FSR, a packet buffering mechanism is implemented to avoid packet loss at the target RNC. We propose an analytic model to investigate the performance of FSR. The numerical results show that packet loss at the source RNC can be ignored. Furthermore, the expected number of packets buffered at the target RNC is small, which does not prolong packet delay.",https://ieeexplore.ieee.org/document/1295050/,IEEE Journal on Selected Areas in Communications,May 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TIP.2022.3158092,Supervised Adaptive Similarity Matrix Hashing,IEEE,Journals,"Compact hash codes can facilitate large-scale multimedia retrieval, significantly reducing storage and computation. Most hashing methods learn hash functions based on the data similarity matrix, which is predefined by supervised labels or a distance metric type. However, this predefined similarity matrix cannot accurately reflect the real similarity relationship among images, which results in poor retrieval performance of hashing methods, especially in multi-label datasets and zero-shot datasets that are highly dependent on similarity relationships. Toward this end, this study proposes a new supervised hashing method called supervised adaptive similarity matrix hashing (SASH) via feature-label space consistency. SASH not only learns the similarity matrix adaptively, but also extracts the label correlations by maintaining consistency between the feature and the label space. This correlation information is then used to optimize the similarity matrix. The experiments on three large normal benchmark datasets (including two multi-label datasets) and three large zero-shot benchmark datasets show that SASH has an excellent performance compared with several state-of-the-art techniques.",https://ieeexplore.ieee.org/document/9740336/,IEEE Transactions on Image Processing,2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TC.2020.3038286,Task Splitting and Load Balancing of Dynamic Real-Time Workloads for Semi-Partitioned EDF,IEEE,Journals,"Many real-time software systems, such as those commonly found in the context of multimedia, cloud computing, robotics, and real-time databases, are characterized by a dynamic workload, where applications can join and leave the system at runtime. Global schedulers can transparently support dynamic workload without requiring any off-line task-allocation phase, thus providing advantages to the system designer. Nevertheless, such schedulers exhibit poor worst-case performance when compared to semi-partitioned schedulers, which instead can achieve near-optimal schedulability performance when used in conjunction with smart task splitting and partitioning techniques, and they are also lighter in terms of run-time overhead. This article proposes an approach to efficiently schedule dynamic real-time workloads on multiprocessor systems by means of semi-partitioned scheduling. A linear-time approximation scheme for the C=D splitting algorithm under partitioned EDF scheduling is proposed. Then, a load-balancing algorithm is presented to admit new real-time workloads with a limited number of re-allocations. The article finally reports on a large-scale experimental study showing that (i) the linear-time approximation is characterized by a very limited utilization loss compared with the corresponding exact approach (that has a much higher complexity), and that (ii) the whole approach allows achieving considerable improvements with respect to global and partitioned EDF scheduling.",https://ieeexplore.ieee.org/document/9261105/,IEEE Transactions on Computers,1 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSM.2017.2666781,Topology-Aware Prediction of Virtual Network Function Resource Requirements,IEEE,Journals,"Network functions virtualization (NFV) continues to gain attention as a paradigm shift in the way telecommunications services are deployed and managed. By separating network function from traditional middleboxes, NFV is expected to lead to reduced capital expenditure and operating expenditure, and to more agile services. However, one of the main challenges to achieving these objectives is how physical resources can be efficiently, autonomously, and dynamically allocated to virtualized network function (VNF) whose resource requirements ebb and flow. In this paper, we propose a graph neural network-based algorithm which exploits VNF forwarding graph topology information to predict future resource requirements for each VNF component (VNFC). The topology information of each VNFC is derived from combining its past resource utilization as well as the modeled effect on the same from VNFCs in its neighborhood. Our proposal has been evaluated using a deployment of a virtualized IP multimedia subsystem, and real VoIP traffic traces, with results showing an average prediction accuracy of 90%, compared to 85% obtained while using traditional feed-forward neural networks. Moreover, compared to a scenario where resources are allocated manually and/or statically, our technique reduces the average number of dropped calls by at least 27% and improves call setup latency by over 29%.",https://ieeexplore.ieee.org/document/7849149/,IEEE Transactions on Network and Service Management,March 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2018.2791803,Toward Intelligent Product Retrieval for TV-to-Online (T2O) Application: A Transfer Metric Learning Approach,IEEE,Journals,"It is desired (especially for young people) to shop for the same or similar products shown in the multimedia contents (such as online TV programs). This indicates an urgent demand for improving the experience of TV-to-Online (T2O). In this paper, a transfer learning approach as well as a prototype system for effortless T2O experience is developed. In the system, a key component is high-precision product search, which is to fulfill exact matching between a query item and the database ones. The matching performance primarily relies on distance estimation, but the data characteristics cannot be well modeled and exploited by a simple Euclidean distance. This motivates us to introduce distance metric learning (DML) for improving the distance estimation. However, in traditional DML methods, the side information (such as the similar/dissimilar constraints or relevance/irrelevance judgements) in the target domain is leveraged. These methods may fail due to limited side information. Fortunately, this issue can be alleviated by utilizing transfer metric learning (TML) to exploit information from other related domains. In this paper, a novel manifold regularized heterogeneous multitask metric learning framework is proposed, in which each domain is treated equally. The proposed approach allows us to simultaneously exploit the information from other domains and the unlabeled information. Furthermore, the ranking-based loss is adopted to make our model more appropriate for search. Experiments on two challenging real-world datasets demonstrate the effectiveness of the proposed method. This TML approach is expected to impact the transformation of the emerging T2O trend in both TV and online video domains.",https://ieeexplore.ieee.org/document/8253525/,IEEE Transactions on Multimedia,Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TMM.2015.2505083,Zero-Shot Person Re-identification via Cross-View Consistency,IEEE,Journals,"Person re-identification, aiming to identify images of the same person from various cameras configured in different places, has attracted much attention in the multimedia retrieval community. In this problem, choosing a proper distance metric is a crucial aspect, and many classic methods utilize a uniform learnt metric. However, their performance is limited due to ignoring the zero-shot and fine-grained characteristics presented in real person re-identification applications. In this paper, we investigate two consistencies across two cameras, which are cross-view support consistency and cross-view projection consistency. The philosophy behind it is that, in spite of visual changes in two images of the same person under two camera views, the support sets in their respective views are highly consistent, and after being projected to the same view, their context sets are also highly consistent. Based on the above phenomena, we propose a data-driven distance metric (DDDM) method, re-exploiting the training data to adjust the metric for each query-gallery pair. Experiments conducted on three public data sets have validated the effectiveness of the proposed method, with a significant improvement over three baseline metric learning methods. In particular, on the public VIPeR dataset, the proposed method achieves an accuracy rate of 42.09% at rank-1, which outperforms the state-of-the-art methods by 4.29%.",https://ieeexplore.ieee.org/document/7346474/,IEEE Transactions on Multimedia,Feb. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IranianCEE.2017.7985285,3D simulation of tennis game using learning algorithms by application of virtual reality,IEEE,Conferences,"replicating a human-like behavior for a Non-player character (NPC) was always a challenge in sports simulations. Having a non-repetitive and intelligent behavior can dramatically increase the entertainment value of the game. To achieve this, many developers used some rule based or expert system methods which result in a controlled NPC behavior, but recently some machine learning methods are deployed in various types of games. In this paper we propose a method for an NPC to learn main skills in the tennis game. The method is based on the multi layered perceptron (MLP) network. We created a virtual 3D environment of the tennis game which can be interacted with via virtual reality devices. Then, after collecting required data from this environment, implemented a virtual opponent that learns the fundamental skills required to play a tennis game. Finally we compared results of different learning methods.",https://ieeexplore.ieee.org/document/7985285/,2017 Iranian Conference on Electrical Engineering (ICEE),2-4 May 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00024,A Live Storytelling Virtual Reality System with Programmable Cartoon-Style Emotion Embodiment,IEEE,Conferences,"Virtual reality (VR) is a promising new medium for immersive storytelling. While previous research works on VR narrative have tried to engage audiences through nice scenes and interactivity, the emerging live streaming shows the role of a presenter, especially the conveyance of emotion, for promoting audience involvement and enjoyment. In this paper, to lower the requirement of emotion embodiment, we borrow experience from cartoon animation and comics, and propose a novel cartoon-style hybrid emotion embodiment model to increase a storyteller's presence during live performance, which contains an avatar with six basic emotions and auxiliary multimodal display to enhance emotion expressing. We further design and implement a system to teleoperate the embodiment model in VR for live storytelling. In particular, 1) we design a novel visual programming tool that allows users to customize emotional effects based on the emotion embodiment model; 2) we design a novel face tracking module to map presenters' emotional states to the avatar in VR. Our lightweight web-based implementation also makes the application very easy to use. We conduct two preliminary qualitative studies to explore the potential of the hybrid model and the storytelling system, including interviews with three experts and a workshop study with local secondary school students. Results show the potential of the VR storytelling system for education.",https://ieeexplore.ieee.org/document/8942322/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC51774.2021.00083,A Multipath Routing Approach for Tile-based Virtual Reality Video Streaming Based on SDN,IEEE,Conferences,"With the increasing demand of virtual reality (VR) video applications, it is necessary to adopt corresponding strategies to deal with the challenges they bring to the network. Multipath routing is proposed to address the VR video bandwidth problem by splitting a large flow into multiple subflows and routing them separately. In addition, Software Defined Networking (SDN) is used to manage these subflows so that they are assigned to the appropriate paths. This paper presents a MCTS-based VR video multipath transmission approach (MVRMPT), which allocates better paths to the VR video tiles that have greater impacts on the user’s Quality of Experience (QoE). More specifically, the Monte Carlo tree search (MCTS) algorithm is modified to find multiple disjoint paths with the minimum delay between node pairs. Then the paths are sorted by the predicted QoE. Finally, the VR video is spatially divided into different zones, and these zones are assigned to different paths according to their impacts on the user’s QoE. The proposed algorithm is implemented in the SDN controller, and the evaluation results show that our method achieves higher QoE and network throughput.",https://ieeexplore.ieee.org/document/9529557/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8797710,A Research Framework for Virtual-Reality Neurosurgery Based on Open-Source Tools,IEEE,Conferences,"Fully immersive virtual reality (VR) has the potential to improve neurosurgical planning. However, there is a lack of research tools for this area. We present a research framework for VR neurosurgery based on open-source tools. We showcase the potential of such a framework using clinical data of two patients and research data of one subject. As first step toward practical evaluations, certified neurosurgeons positively assessed the VR visualizations and interactions using head-mounted displays. Methods and findings described in our study thus provide a foundation for research and development of versatile and user-friendly VR tools for improving neurosurgical planning and training.",https://ieeexplore.ieee.org/document/8797710/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIE53562.2021.00156,A Review on the Application of Virtual Reality Technology in Ideological and Political Teaching,IEEE,Conferences,"In recent years, the application of virtual reality technology in different fields has attracted the attention of society and academic circles. Some scholars connect virtual reality technology with ideological and political teaching, so as to further explore the new ideas of ideological and political teaching. This paper analyzes the meaning, characteristics and functions of virtual reality technology, and puts forward its current difficulties in ideological and political courses. At the same time, this paper also points out the implementation principles, approaches and strategies of virtual reality technology in the ideological and political curriculum teaching, in order to enrich the teaching methods in the ideological and political curriculum, and lay a research foundation for improving the educational quality of the ideological and political curriculum.",https://ieeexplore.ieee.org/document/9534508/,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),18-20 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCIC31437.2018.9080444,"A Smart Condition Monitoring System for HV Networks with Artificial Intelligence, Augmented Reality and Virtual Reality: Copyright Material IEEE, Paper No. PCIC-2018-37",IEEE,Conferences,"The authors present a conceptual design for a SMART asset monitoring solution for high voltage (HV) networks in the petrochemical industry. The paper discusses the potential for incorporating artificial intelligence (AI), augmented reality (AR) and virtual reality (VR) into an application of the Industrial Internet of Things (IIoT) for condition monitoring. The paper is a continuation of the work presented by the authors at the IEEE-PCIC 2017 conference in Calgary. The proposed asset management system analyses condition monitoring (CM) data and assesses the risk of failure data across complete HV networks. Knowledge of deteriorating asset condition provides the operator with an advanced, early warning of incipient mechanical and electrical faults. With knowledge of the severity and source of such faults, pinpointed preventative maintenance interventions can then made during planned maintenance outages. The complete HV network asset monitoring solution described includes permanent sensors and monitoring nodes deployed at strategic locations across the network. Processed data is passed via a local area network to local servers and then via secure data cloud transmission to a centralized monitoring server located at the CM headquarters. This central server operates a CM database that logs, displays, benchmarks and trends the condition data with comparison to a statistically-significant database of measurements. It is proposed in the IIoT solution proposed that this database will be downloadable to a smartphone/tablet for use by the field engineer. The monitoring technology will likely also incorporate a number of AI machine learning software modules for the de-noising of raw signals and the diagnosis of different types of defects within different types of HV plant items. The proposed SMART CM system includes an advanced graphical user interface (GUI) for viewing HV asset CM data along with operational and maintenance (O&M) data. The GUI will also be able to display both condition criticality and operational criticality (on a color-coded range of 0-100%) for individual HV plant items on a digitized mimic of the HV network's single-line diagram (SLD). This could also be combined with geometric positioning data of assets across the facility (including HV cable routes and lengths) to provide a fully digitized SMART network diagram for use in the IIoT asset management solution. Asset management data, combined with the application of the developing techniques of AI, AR and VR, will greatly help the user to visualize the plant items in 3-D, their position within the network, their condition and operational criticality along with all related asset management information together on one dashboard screen, downloaded onto smartphone/tablet. The paper concludes with a case study showing the development of a specification for a SMART IIoT asset condition monitoring solution suitable for a large petrochemical refining facility.",https://ieeexplore.ieee.org/document/9080444/,2018 IEEE Petroleum and Chemical Industry Technical Conference (PCIC),24-26 Sept. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT52272.2021.00120,A Teacher Training Proposal for Classroom Conflict Management through Virtual Reality,IEEE,Conferences,"This paper addresses the use of Virtual Reality (VR) in the training of Secondary Education teachers in Spain as an integral part of the new ecology of learning and proposes some premises from which to design a training program to improve teachers' communicative competence and their ability to manage conflict affecting the classroom climate. First, the paper explains the experiential and experimental potential in which VR immerses us, its ability to create challenging and personalized virtual worlds, as well as the possibility to generate instant feedback and feedforward. Finally, an example of a prototype scenario designed on this conceptual basis is provided and some ideas are discussed to introduce VR into initial teacher training.",https://ieeexplore.ieee.org/document/9499828/,2021 International Conference on Advanced Learning Technologies (ICALT),12-15 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI52376.2021.9515182,A practical method to build-up a virtual reality building emulator,IEEE,Conferences,"This paper presents a method for building-up a virtual reality (VR) application which emulates a real building. Some remarks on VR world technique are presented, together with its advantages and disadvantages. A design plan for the VR emulator (applied for the studied building) is also presented, together with all major implementation steps. In the final part, some simulation results in direct connection with the significant conclusions are emphasized.",https://ieeexplore.ieee.org/document/9515182/,"2021 13th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",1-3 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAAIC53929.2022.9792882,AI Interfaced Learning Module for Road Safety using Virtual Reality,IEEE,Conferences,"This paper deals with an innovative idea for kids to learn about road safety in an effective way of AI interfaced learning using Virtual Reality (VR). The game has five levels. At each level, kids will learn different types of traffic rules interactively. A method of supervised learning is followed by the AI module in the game to teach and guide the player. The first level teaches the kids about the pedestrian rules that should be followed by pedestrians. The second level is much more interesting where the kids will learn about the traffic signs. In the third level kids will get to know about the road safety rules that must be followed by a two-wheeler driver. In the fourth level, the kids will know about the traffic rules that must be followed by a four-wheeler driver. Finally, the kids will learn how and where to park a vehicle on the fifth level. After each level, the player must attend a quiz just to have a self-evaluation to have an idea of what they have learned. The quizzes are based on the random forest algorithm, which randomly selects questions based on the level played previously. These AI modules were integrated with a navmesh agent given in unity software. The whole game is experienced on the VR platform with AI interfacing. So, the kids can get a real-time adventure and can learn things in a virtual-practical manner. This makes the learning part easy, interesting, and fun.",https://ieeexplore.ieee.org/document/9792882/,2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC),9-11 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.1999.807223,Advances in the virtual reality interstitial brachytherapy system,IEEE,Conferences,"This paper describes improvements to the virtual reality (VR) brachytherapy system that we have previously designed and implemented. Brachytherapy is a cancer treatment modality whereby, needles are inserted into a patient to act as channels to deliver radioactive sources to the diseased tissue. The focus of this paper is on the advances that have been made on the system. More software tools have been added to the system, as well as the interfacing of a new head mounted display (HMD). The method in which the user inputs information into the system has changed from a graphical user interface to a voice user interface (VUI). Some preliminary work has been carried out on an artificial intelligence (AI) component, which will suggest optimal needle placement to the user. This early work on the Al component involves the use of Simulated Annealing to find optimal needle positions within a single 2D slice of a cancerous tumour. The new software tools have been found to be very useful. The new HMD, has been successfully interfaced to the VR system and far exceeds the usefulness of the previous device. The VUI has a high level of accuracy and greatly increases the usability of the VR system. Preliminary results from the AI component of the system suggest that further research will lead to a very useful optimization method for use in the VR brachytherapy system.",https://ieeexplore.ieee.org/document/807223/,Engineering Solutions for the Next Millennium. 1999 IEEE Canadian Conference on Electrical and Computer Engineering (Cat. No.99TH8411),9-12 May 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIHAS.1993.410558,Aiding teachers in constructing virtual-reality tutors,IEEE,Conferences,"Teachers need different tools for constructing virtual realities than do professional programmers. Teachers building tutoring environments need only and should only provide declarative and nonquantitative specification of the application, as such information is sufficient to build powerful prototypes or even products when exploited properly. The METUTOR tutor-generation system for sequential-action skills, which uses means-ends analysis on a teacher's declarative specification of a set of actions, is described. METUTOR asks the teacher to specify conditions for recommending actions, preconditions of actions, and expected and random consequences of actions. METUTOR also asks the teacher to associate pictorial and/or aural representations with facts, and to specify how and when to use them. METUTOR provides facilities for automatic resolution of interactions and conflicts between media objects. Examples from a firefighting tutor and a pilot's emergency tutor are presented.<>",https://ieeexplore.ieee.org/document/410558/,"1993 4th Annual Conference on AI, Simulation and Planning in High Autonomy Systems",20-22 Sept. 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAPAI49758.2021.9462061,An Industrial Assistance System with Manual Assembly Step Recognition in Virtual Reality,IEEE,Conferences,"In the era of Industry 4.0, worker assistance systems are becoming more and more important. In order to assist shop floor workers in manual assembly tasks, we implemented an assistance system in virtual reality. A deep neural network was trained to recognize the current work step in real-time during an assembly process, thus giving the assistance system context-awareness. We defined the problem of assembly step recognition as a multivariate time series classification using the poses of the workers head, both hands and all relevant tools and objects. With this definition, the VR environment's output can also be replaced with data from the real world. For our proof-of-concept assembly step recognition system, we created an assembly process consisting of six different work steps, five movable assembly parts and one tool. We showed that we can train an activity recognition model for assembly steps with only 10 assembly recordings. To achieve this, we used multiple data augmentation techniques and proposed a novel method of synthesizing new training data, which we call Path Joining. With only 10 training recordings, we attain a categorical classification accuracy of 81 percent and with 60 recordings we achieve an accuracy of 89 percent.",https://ieeexplore.ieee.org/document/9462061/,2021 International Conference on Applied Artificial Intelligence (ICAPAI),19-21 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BMSB49480.2020.9379584,An Innovative Algorithm for Improved Quality Multipath Delivery of Virtual Reality Content,IEEE,Conferences,"This paper describes and evaluates an Innovative Algorithm for Improved Quality Multipath Delivery of Virtual Reality Content (QM4VR) that addresses the stringent communication requirements of Virtual Reality (VR) applications. Making use of the Multipath TCP (MPTCP) built-in multipath delivery features (subflows), QM4VR explores the subflows' characteristics, evaluates their performance (e.g., delay, throughput or loss) and proposes a new management scheme to improve the Quality of Service (QOS) of the VR applications. glsqm4vr adopts a Machine Learning (ML)-based approach to evaluate the subflows' performance which is implemented in two steps: 1) a linear regression scheme to forecast the subflow's performance for a given feature; and 2) a linear classification scheme to arrange the results obtained in step 1. Based on these results QM4VR selects the most appropriate subflows for data delivery in order to achieve improvement of VR QOS levels.",https://ieeexplore.ieee.org/document/9379584/,2020 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),27-29 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PIC.2018.8706286,An Interior Decoration System via Virtual Reality and Artificial Intelligence,IEEE,Conferences,"In this paper, we proposed an interior design system. We have implemented the scene selection function and the house type drawing function to get the apartment type. After getting the basic apartment type, we have also provided some other basic decoration functions such as furniture placement, furniture conversion, material conversion, and light switch. These functions are operated by mouse clicking and keyboard control. In addition, we have added some AI modules to provide an additional assistant. Through the recognition of the picture, the texture can be trained, and the ideal texture has been obtained. The implementing environment of our design system is UE4, and the AI algorithm was written in Python and tensor flow.",https://ieeexplore.ieee.org/document/8706286/,2018 IEEE International Conference on Progress in Informatics and Computing (PIC),14-16 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1999.809795,An agent-based approach to developing intelligent virtual reality-based training systems,IEEE,Conferences,"This paper presents an agent-based approach to developing intelligent virtual reality-based training systems. The architecture of an intelligent virtual reality-based training system is modeled as five collaborative agents: training task-planning agent, simulation agent, performance evaluation agent, on-line instruction agent, and intelligent interface agent. The behavioral and knowledge models of each agent type and the mechanism of inter-agent interaction and coordination are described. A Petri net-based specification approach that gives the work formalism for effective modeling and implementation is elaborated. Finally, a computerized numeric control (CNC) milling operation virtual training prototype system is introduced to show the effectiveness of this approach.",https://ieeexplore.ieee.org/document/809795/,Proceedings 11th International Conference on Tools with Artificial Intelligence,9-11 Nov. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE51474.2020.00073,Analysis of 3D Image Reconstruction System Based on Virtual Reality Technology,IEEE,Conferences,"With the continuous development of science and technology, two-dimensional image technology can no longer meet people's needs, so three-dimensional image reconstruction technology has been studied by people. Three-dimensional image reconstruction has been applied in many fields. For example: medical, military, detection, etc. This article mainly introduces the research of 3D geological simulation system based on virtual reality technology and virtual reality technology.",https://ieeexplore.ieee.org/document/9403827/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2001.1020558,Analysis of immergence using biological signals and neural network in virtual reality environment,IEEE,Conferences,"We tried to measure the immergence objectively under a virtual reality environment using bio-signals. Multi-channeled bio-signals, such as EEG, ECG, EOG, respiration, PPG, skin temperature, skin conductance level and nausea time are acquired in VR environment. For quantification of nausea, we used an artificial neural network with preprocessed bio-signals. For real-time nausea evaluation, a new system is implemented with Matlab and Visual C++.",https://ieeexplore.ieee.org/document/1020558/,2001 Conference Proceedings of the 23rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society,25-28 Oct. 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2006.258766,Application Study of Virtual Reality Technology in Digital Oilfield,IEEE,Conferences,"The paper discusses the concept and system function of digital oilfield, and clarifies the importance of virtual reality technology in oilfields. It provides a systematic study on interactive seismic data interpretation based on virtual reality and puts forward a systematic formation plan for the interpretation platform. The experiments of both hardware and software show that the design is feasible and could meet the need of 3D seismic data interpretation",https://ieeexplore.ieee.org/document/4028301/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAAIC53929.2022.9793152,Application of Virtual Reality Technology in College Students&#x0027; Social Anxiety Intervention Platform with C# Implementations,IEEE,Conferences,"This paper studies the construction of college students&#x0027; social anxiety intervention platform based on the virtual reality technology implemented by C#. The results of an empirical study on the intervention of virtual reality technology on anxiety disorders (including specific phobias, obsessive-compulsive disorder and social anxiety) in autism spectrum disorder groups show that: the intervention effect of virtual reality technology on adult patients with specific phobias in the autism spectrum disorder group It is better than school-age children. In order to further understand the reliability and ecological validity of the test subject&#x0027;s assessment under the intervention of the virtual environment, a controlled experiment was used to analyze the experimental data, and the Cronbach reliability coefficient was used to determine the relationship between the self-awareness scale and the The reliability of the Social Anxiety Scale was analyzed, and the correlation and significance level of each scale were analyzed by S PS S software.",https://ieeexplore.ieee.org/document/9793152/,2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC),9-11 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLBDBI54094.2021.00125,Application of virtual reality technology in improving cognitive function in patients with cognitive dysfunction,IEEE,Conferences,"Research significance: traditional cognitive impairment training methods have long cycle and poor effect. The emerging virtual reality technology can be used as an intervention method for cognitive impairment to effectively improve the level of cognitive function of patients. Objective: To explore the research status of virtual reality technology in improving the cognitive ability of patients with cognitive impairment. Research methods: search the &#x201C;Journal Navigation&#x201D; for relevant literature on the application of virtual technology VR in cognitive impairment since 2016, classify it from three dimensions: patient type, etiology and intervention methods, and analyze it by using data statistics software. Results: a total of 37 studies were retrieved, mainly involving patients with mild cognitive impairment and dementia, as well as cognitive impairment groups caused by stroke and brain death. The use of virtual technology mainly includes the separate use of virtual technology, combined with conventional training methods and combined with emerging technologies. Conclusion: virtual technology has good applicability in cognitive impairment groups, and it is worth popularizing, and its intervention effect is more significant than conventional methods. VR has broad application prospects. In the future, it needs to be further explored from the aspects of diversified patient groups and integration of emerging technologies.",https://ieeexplore.ieee.org/document/9730994/,"2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",3-5 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigComp.2018.00076,Artificial Landmarks to Facilitate Spatial Learning and Recalling for Curved Visual Wall Layout in Virtual Reality,IEEE,Conferences,"Current VR (Virtual Reality) browsing interfaces provide few landmarks, particularly, less is known about the effects of 3D landmarks on facilitating spatial learning and recalling for VR browsing interfaces. In this work, the implementation of a new artificial landmark (3D visual pin) along with three previous landmarks (grid, image, and anchor) are presented with a curved visual wall layout, in which we examined the effects on facilitating learning and recalling items' locations in small and large item sets. We found that both the 3D pin and the anchor interfaces assisted multiple targets retrieving and recalling, compared to the grid interface, and more preferences of 3D pins, but no significant difference of retrieval and recall performances between the 3D pin and the anchor interfaces. The image interface had little impact on trial completion time, but significantly improved the selection accuracy compared to the grid interface. The findings provide the possibility of enabling artificial landmarks for future VR browsing interfaces.",https://ieeexplore.ieee.org/document/8367156/,2018 IEEE International Conference on Big Data and Smart Computing (BigComp),15-17 Jan. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW50115.2020.00024,Assessing Personality Traits of Team Athletes in Virtual Reality,IEEE,Conferences,"Assessment of personality traits is highly relevant in team sports in order to analyze the performance of an athlete under pressure when in competitive situations, for team-strategic decisions, to optimize command transmission, and ultimately to understand top-level performers. It further facilitates the development and application of personalized exercises, coaching to improve performance in competition, and can be considered a valuable criterion for talent scouting and development. The current state of the art method to assess personality traits in sports relies on validated questionnaires. However, these often provide non-sport-specific, subjective self-reported information and lack the ability to measure how these characteristics are reflected in context-based performance.We developed a virtual reality (VR) tool for the assessment of personality traits in team sports, in our case for soccer. An evaluation of this tool within a study with 24 subjects yielded a benchmark of its immersion through user experience and provided an objective description of athletes’ personalities based on performance indicators extracted from activity-tracking. Within the tool, we implemented two realistic virtual soccer environments to assess the motivational orientation of soccer players (i.e. action- and state-orientation) which we discerned from the gold standard questionnaire.Results show that user experience and presence of the implemented virtual environments scored significantly higher compared to benchmark measurements. Additionally, a significant difference between the two groups of action and state-oriented athletes could be observed. Measures of failure rate, pass accuracy, number of perceived opponents, and achieved bonus goals are parameters that differ significantly among the two athlete groups. These findings show that VR technology is applicable for the assessment of athletes’ motivational orientation and thus demonstrate the feasibility of virtual environments as functional game scenario-based assessment tools for athletes.",https://ieeexplore.ieee.org/document/9090622/,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),22-26 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00037,Assessing the Value of 3D Software Experience with Camera Layout in Virtual Reality,IEEE,Conferences,"Preproduction is a critical step in creating 3D animated content for film and TV. The current process is slow, costly, and creatively challenging, forcing the layout director (LD) to interpret and create 3D worlds and camera directions from 2D drawings. Virtual reality (VR) offers the potential to make the process faster, cheaper, and more accessible. We conducted a user study evaluating the effectiveness of VR as a preproduction tool, specifically focusing on prior 3D modeling experience as an independent variable. We assessed the performance of experienced 3D software participants to those with no experience. Participants were tasked with laying out a camera shot for an animated scene. Our results revealed that the experienced 3D software participants did not significantly outperform their non-experienced counterparts. Overall, our study suggests that VR may provide an effective platform for animation pre-production, ""leveling the playing field"" for users with limited 3D software experience and broadening the talent pool of potential LDs.",https://ieeexplore.ieee.org/document/8942285/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8797969,Automatic Generation of Interactive 3D Characters and Scenes for Virtual Reality from a Single-Viewpoint 360-Degree Video,IEEE,Conferences,This work addresses the problem of using real-world data captured from a single viewpoint by a low-cost 360-degree camera to create an immersive and interactive virtual reality scene. We combine different existing state-of-the-art data enhancement methods based on pre-trained deep learning models to quickly and automatically obtain 3D scenes with animated character models from a 360-degree video. We provide details on our implementation and insight on how to adapt existing methods to 360-degree inputs. We also present the results of a user study assessing the extent to which virtual agents generated by this process are perceived as present and engaging.,https://ieeexplore.ieee.org/document/8797969/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCloud.2017.51,Brain-Based Computer Interfaces in Virtual Reality,IEEE,Conferences,"Virtual Reality (VR) research is accelerating the development of inexpensive real-time Brain Computer Interface (BCI). Hardware improvements that increase the capability of Virtual Reality displays and Brain Computer wearable sensors have made possible several new software frameworks for developers to use and create applications combining BCI and VR. It also enables multiple sensory pathways for communications with a larger sized data to users' brains. The intersections of these two research paths are accelerating both fields and will drive the needs for an energy-aware infrastructure to support the wider local bandwidth demands in the mobile cloud. In this paper, we complete a survey on BCI in VR from various perspectives, including Electroencephalogram (EEG)-based BCI models, machine learning, and current active platforms. Based on our investigations, the main findings of this survey highlights three major development trends of BCI, which are entertainment, VR, and cloud computing.",https://ieeexplore.ieee.org/document/7987213/,2017 IEEE 4th International Conference on Cyber Security and Cloud Computing (CSCloud),26-28 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEARIS.2012.6231171,Configurable semi-autonomic animated animal characters in interactive virtual reality applications,IEEE,Conferences,"In many virtual reality (VR) simulation and training applications, it is desirable and even critical to have computer controlled animal characters that can behave with a high degree of realism. The realism can be measured in two aspects. One is behavioral realism, or how real the characters act and respond to the commands and environments. The other aspect is visual realism. Many efforts have been directed at animating human characters as well as other animal characters in applications such as interactive computer games. However, more research is still needed for realistic animation of animal characters due to the vast variety of animal species and different application purposes. In some of the environments for which security personnel are being trained, animals, such as dogs, are part of the training programs. So we need to include realistic behavior and visual representations of animal characters in our training applications. This study is focused on animal behavior and animation for VR applications. A simple implementation of a real-time animal animation method that is configurable is proposed to make it easy for user interaction. Synthetic animal characters are included in the system so that their behavior can be programmed and controlled digitally. A two-stage state machine is used in the system. The top-level state machine controls the animal behavior. The secondary state machine controls animation. Animation blending and procedural control are used to make the animation smooth.",https://ieeexplore.ieee.org/document/6231171/,2012 5th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS),5-5 March 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00045,Creating Virtual Reality and Augmented Reality Development in Classroom: Is it a Hype?,IEEE,Conferences,"The fast-growing number of high-performance computer processor and hand-held devices have paved the way for the development of Virtual Reality and Augmented Reality in terms of hardware and software in the education sector. The question of whether students can adopt these new technologies is not fully addressed. Answering this question thus plays an essential role for instructors and course designers. The objectives of this study are: (1) to investigate the feasibility of the Virtual Reality/Augmented Reality development for undergraduate students, and (2) to highlight some practical challenges when creating and sharing Virtual Reality and Augmented Reality applications from student's perspective. Study design for the coursework was given with detail. During a 16-week long, 63 Virtual Reality/Augmented Reality applications were created from a variety of topics and various development tools. 43 survey questions are prepared and administered to students for each phase of the projects to address technical difficulties. The exploration method was used for data analysis.",https://ieeexplore.ieee.org/document/8942272/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00042,Customizable Memory Training in Virtual Reality with Personal Memoirs,IEEE,Conferences,"Memory training methods are often not designed for all age groups, which causes older people to be failed by educational systems. However, the brain remains plastic for a lifetime. The application discussed in this paper demonstrates how approaches to learning psychology can be implemented in virtual reality to create an individualized learning environment that addresses all age groups. In addition, personal memoirs were used to address the episodic part of the brain and to increase motivation. In a small usability study, twelve participants tested the application and filled out a questionnaire. This showed that the design and usability of this application, using a virtual learning environment customized to the user as well as different perception channels, has been successful. Moreover, the wellbeing was rated positively as well.",https://ieeexplore.ieee.org/document/9644379/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BCI48061.2020.9061587,Decoding prefrontal cognitive states from electroencephalography in virtual-reality environment,IEEE,Conferences,"Rapid advances in deep learning enabled us to develop various brain-computer interface (BCI) applications. This study presents a novel BCI framework in virtual-reality environment based on an electroencephalography (EEG) decoder for learning strategy classification. We collected 9 subjects' EEG data using a 6-channel EEG-VR headset, and implemented 2D convolutional neural networks with spectrograms as input features. The proposed method achieved 82% classification accuracy, despite a small number of channels and noise artifact. The results suggest a possibility of exploiting BCI technologies for various VR applications.",https://ieeexplore.ieee.org/document/9061587/,2020 8th International Winter Conference on Brain-Computer Interface (BCI),26-28 Feb. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPEC51340.2021.9421145,Design and Implementation of Virtual Reality Classroom of Surveying and Mapping Geographic Information Technology Based on Web GIS,IEEE,Conferences,"In order to improve the accuracy of urban traffic flow information and improve the efficiency of traffic control, this paper studies the application of artificial intelligence technology in urban traffic. This paper mainly introduces the intelligent transportation data acquisition and control system based on artificial intelligence and neural network architecture, and points out the trend of the integration of neural network technology and intelligent transportation. This paper designs an intelligent transportation data fusion method based on neural network, and applies it to real-time acquisition, inductive dynamic control, optimal path planning and so on. In this paper, the application of geographic information system (GIS), various intelligent sensors and electronic tag positioning technology in intelligent transportation system (ITS) is described in detail.",https://ieeexplore.ieee.org/document/9421145/,"2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",14-16 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2009.5212340,Design and development of a distributed Virtual Reality system,IEEE,Conferences,"A Virtual Reality system with multi-interaction based on working distributed was designed to solve the application problem of VR; developed the server end, the render end and the control end of system used the graph rendering engine OGRE combined with the network engine Raknet, and an application example is presented.",https://ieeexplore.ieee.org/document/5212340/,2009 International Conference on Machine Learning and Cybernetics,12-15 July 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISAI54367.2021.00204,Design of 3D Exhibition Hall System of Art Museum Based On Virtual Reality,IEEE,Conferences,"The existing system has the problem of imperfect three-dimensional display model, resulting in excessive CPU occupation. A 3D exhibition hall system of art museum based on virtual reality is designed. The hardware part: adopt different connection modes to connect the host, bus and reader with the line interface. The software part: in this paper, a 3D exhibition hall browsing interface is built on the basis of hardware, and the works are converted into pictures and displayed in the virtual scene. We choose distance as the similarity evaluation index, and use virtual reality technology to design system module functions. Experimental results: the average CPU occupancy of the 3D exhibition hall system of the Art Museum designed in this paper and the other two systems are 28.426%, 50.139% and 50.759% respectively, which proves that the 3D exhibition hall system of the art museum integrated with virtual reality technology has better performance.",https://ieeexplore.ieee.org/document/9719102/,2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI),17-19 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCE.2018.00228,Design of Simulation Training System of Self-Propelled Gun Based on Virtual Reality,IEEE,Conferences,"In recent years, training cost of army is increasing, it is necessary to save the training expenditure and improve the quality of training. This paper establishes a simulation training system of self-propelled gun based on virtual reality, distributed simulation and artificial intelligence. It introduces the design of system software. Simulation software of self-propelled gun information terminal consists of driver control panel, fire control panel, turret control panel, servo gun and so on. This paper analyzes the design process of each module and give its realization function in the form of block diagram. After testing, the results show that the system can simulate all kinds of skill operations and tactical drill and it greatly saves the cost of training.",https://ieeexplore.ieee.org/document/8612726/,2018 5th International Conference on Information Science and Control Engineering (ICISCE),20-22 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53024.2021.9733774,Development of Online Teaching Tools for Engineering: Immersive Virtual Reality Application for Manipulation of a Vertical Milling Machine Using Unity,IEEE,Conferences,"The global COVID-19 pandemic forced educational institutions at all levels to modify and migrate their teaching methodologies entirely to an online modality. Universities with technical programs were challenged to teach students the skills and abilities obtained only through laboratory or physical workshop experiences. This project focused on developing an immersive virtual reality application as a learning methodology to interact with a vertical milling machine. Additionally, we integrated a web plugin with similar functions to expand students&#x0027; learning experience. Using the computerized design software Solidworks and Blender, we configured the assembly of a vertical milling machine to use in video game development software such as Unity. With this software, we created a virtual reality application (governed by fundamentals of educational theory) that allowed users to recognize the milling machine&#x0027;s parts and functions and simulate actual interactions. Additionally, with an accessible virtual reality kit, we added a web plugin with similar functions to achieve a greater scope of teaching. Thus, developing a virtual reality application as didactic support and training familiarized students with a vertical milling machine&#x0027;s parts and operations. This project can be implemented in different educational institutions seeking a dynamic and innovative way to teach machinery in a virtual environment.",https://ieeexplore.ieee.org/document/9733774/,2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop,15-17 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2003.1244337,Development of a virtual reality surgical platform for unicompartmental knee replacement,IEEE,Conferences,"A virtual reality (VR) surgical training platform for unicompartmental knee replacement is developed in his study. The platform is constructed with VR software employed on PC-based system and connected with a low-cost manual manipulator. The primary components in the VR include soft tissue and skeletal tissue of the knee as well as various surgical tools such asscapels, cutting jig, drill, and oscillating saw, and so on. The manipulator provides 6-DOF motion and 3-DOF force measurements. During manipulation, all measurement data is displayed via a monitoring interface on the screen. Besides, an evaluation system with different assessment method for each surgical process is built for training purpose.",https://ieeexplore.ieee.org/document/1244337/,"SMC'03 Conference Proceedings. 2003 IEEE International Conference on Systems, Man and Cybernetics. Conference Theme - System Security and Assurance (Cat. No.03CH37483)",8-8 Oct. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI.2012.6158803,Digital Sand Model using Virtual Reality Workbench,IEEE,Conferences,"Sand models as used in military are thematic 3D representations of an area of interest which combined the information of maps with a more realistic 3D bird's eye view of the terrain. Sand models have been used for military planning and war gaming for many years as a field expedient, small-scale map, for planning and training of military operations. This, however increasingly fell out of favor with improved maps, aerial and satellite photography, and later, with digital terrain simulations. The proposed system, a Digital Sand Model (DSM) using a Virtual Reality (VR) Workbench, is a digital replica of the same. It involves a leader interacting with the digital environment and other viewers share his view of the digital scene. The system uses a stereoscopic projector, a mirror and Viewing Screen setup to produce a 3D stereoscopic display. This VR system allows performing interactive 3D explorations via stereo camera setup and wearable IR LEDs. A gesture recognition system is also described which allows interaction with the system. The proposed system uses Model-View-Controller (MVC) architecture as the software design.",https://ieeexplore.ieee.org/document/6158803/,2012 International Conference on Computer Communication and Informatics,10-12 Jan. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIIoT52608.2021.9454247,"Education System for Bangladesh Using Augmented Reality, Virtual Reality and Artificial Intelligence",IEEE,Conferences,"This paper presents an innovative application for students to study and understand their coursework without any external help from a private tutor. The system uses Augmented Reality (AR) to provide hands on experience for the students. The presented system also supports Virtual Reality (VR) that enriches this process and immerses the users into a fun and productive learning experience. Moreover, the system introduces an industry first Artificial intelligence (AI) based study guide that directs students towards necessary topics and advises them on what to improve on. All the core system features are implemented and are accessible via two mediums. First, a standalone mobile phone application. Second, a dedicated web portal.",https://ieeexplore.ieee.org/document/9454247/,2021 IEEE World AI IoT Congress (AIIoT),10-13 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW50115.2020.00126,Enhancing Proxy-Based Haptics in Virtual Reality,IEEE,Conferences,"Rich haptic sensations in interactive virtual reality (VR) applications support immersive experiences. This position paper outlines my research efforts aiming to bring enhanced haptic interactions to VR users. Leveraging the highly realistic haptic feedback provided by real, physical proxy objects, I present two orthogonal research directions attempting to overcome the central drawbacks of conventional passive haptics. The first research direction leverages physical manipulations to enhance scalability through reusable yet low-complexity dynamic passive haptic proxy objects. Orthogonal to this, I explore hand redirection techniques in a second, more software-focused research direction based on virtual manipulations. In a concluding section, this position paper outlines how both approaches could be combined to further enhance haptics in VR.",https://ieeexplore.ieee.org/document/9090415/,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),22-26 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00015,Evaluating Engagement of Virtual Reality Games Based on First and Third Person Perspective Using EEG and Subjective Metrics,IEEE,Conferences,"Viewing perspective plays a significant role in gameplay experiences and especially in virtual reality (VR). However, it is still underexplored. In this paper we evaluate how different viewing perspectives relate to psychophysiological engagement, arousal, and valence metrics that are measured using brainwaves. We perform an experiment in which volunteers play a game using different perspectives and displays while their brainwaves are monitored by a commercial Electroencephalogram (EEG) headset. Using the EEG data, we calculate levels of engagement, arousal and valence using a metric from the literature, and then compare these results to answers from a subjective, self-reporting questionnaire. We then discuss how the psychophysiological metrics compare with the subjective ones in this study, verifying that their results do not always match.",https://ieeexplore.ieee.org/document/8613634/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00053,Evaluating the Effects of a Cartoon-Like Character with Emotions on Users' Behaviour within Virtual Reality Environments,IEEE,Conferences,"In this research we explore the effect of a virtual avatar that is non-human like and can express basic distinguishable emotions on users' level of engagement and interest. Virtual reality (VR) environments are able to render realistic representations. However, not all virtual environments require life-like representations of their characters-in our research a 'life-like' human character means that it resembles very closely to an actual person in real life. It is very common for games to use simple non-human characters. Cartoon-like characters can actually have a greater impact on users' affinity towards these games. The aim of this research is to examine if interactions with a cartoon-like character that has the capacity to express simple but common emotional expressions is sufficient to bring forth a change in the behavior and level of engagement of users with the character. This research seeks to find out if adding simple emotions to virtual characters is beneficial to increasing users' interest. To explore these questions, we have conducted a study with a human-like cartoon character in a VR environment that can express simple, basic human emotions based on users' input. The results of our experiment show that a cartoon-like character can benefit from displaying emotional traits or responses when interacting with humans in a VR environment.",https://ieeexplore.ieee.org/document/8613672/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2004.1398290,Event-based causality in virtual reality,IEEE,Conferences,"In this paper, we describe such an environment, in which the user can be presented in real-time with an alternative range of consequences for a given interaction with virtual world objects, thus inducing various perceptions of causality. From a systemic perspective, we can adopt a pragmatic approach, inspired from human philosophy, which considers that causal relations are established by the user in response to certain event cooccurrences. To control causal perception in VR, the system comprises the following components: a visualisation engine, an event recognition system (together with its specific event formalism that supports event modification), and a causal engine as an event modification system, which selects co-occurring events on a rule-based manner. A search process, evaluates alternative consequences using semantic and spatio contiguity information, such as comparison between candidate objects on which the action consequences should be applied. A first prototype has been fully implemented and is described together with an example real-time simulation",https://ieeexplore.ieee.org/document/1398290/,"2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)",10-13 Oct. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00038,Exploring CNN-Based Viewport Prediction for Live Virtual Reality Streaming,IEEE,Conferences,"Live virtual reality streaming (a.k.a., 360-degree video streaming) is gaining popularity recently with its rapid growth in the consumer market. However, the huge bandwidth required by delivering the 360-degree frames becomes the bottleneck, keeping this application from a wider range of deployment. Research efforts have been carried out to solve the bandwidth problem by predicting the user's viewport of interest and selectively streaming a part of the whole frame. However, currently most of the viewport prediction approaches cannot address the unique challenges in the live streaming scenario, where there is no historical user or video traces to build the prediction model. In this paper, we explore the opportunity of leveraging convolutional neural network (CNN) to predict the user's viewport in live streaming by modifying the workflow of the CNN application and the training/testing process. The evaluation results reveal that the CNN-based method could achieve a high prediction accuracy with low bandwidth usage and low timing overhead.",https://ieeexplore.ieee.org/document/8942314/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2018.8513213,Exploring the Feasibility of EMG Based Interaction for Assessing Cognitive Capacity in Virtual Reality,IEEE,Conferences,"With the growth and aging of the world population, the prevalence of cognitive diseases and disabilities like dementia and mild cognitive impairments increases. To determine the influence of such diseases, find therapeutic effects and further improve quality of life, cognitive assessment and training is required. This can be done with the application of high immersive technologies like virtual reality.In this paper we evaluate the feasibility of an electromyography (EMG) arm muscle-motion based interaction technique for controlling a VR cognitive performance diagnostic and training environment. Therefore, we compared the state-of-the-art controller input to our EMG based approach in terms of presence and user experience.Results show significant differences in terms of Novelty and Dependability. Since there are only few significant differences regarding presence and user experience, the advantage of applying a more demanding physical motion interaction approach (EMG), seems to be a promising method with the potential of having a positive effect on the cognitive training progress. This is mainly caused by the fact that the implemented gesture interaction reinforces the connection between decision making and action execution.",https://ieeexplore.ieee.org/document/8513213/,2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),18-21 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF44664.2019.9049048,Fire Frontline Monitoring by Enabling UAV-Based Virtual Reality with Adaptive Imaging Rate,IEEE,Conferences,"Recently, using drones for forest fire management has gained a lot of attention from the research community due to their advantages such as low operation and deployment cost, flexible mobility, and high-quality imaging. It also minimizes human intervention, especially in hard-to-reach areas where the use of ground-based infrastructure is troublesome. Drones can provide virtual reality to firefighters by collecting on-demand high-resolution images with adjustable zoom, focus, and perspective to improve fire control and eliminate human hazards. In this paper, we propose a novel model for fire expansion as well as a distributed algorithm for drones to relocate themselves towards the front-line of an expanding fire field. The proposed algorithm comprises a light-weight image processing for fire edge detection that is highly desirable over computational expensive deep learning methods for resource-constrained drones. The positioning algorithm includes motions tangential and normal to fire frontline to follow the fire expansion while keeping minimum pairwise distances for collision avoidance and non-overlapping imaging. We proposed an action-reward mechanism to adjust the drones’ speed and processing rate based on the fire expansion rate and the available onboard processing power. Simulations results are provided to support the efficacy of the proposed algorithm.",https://ieeexplore.ieee.org/document/9049048/,"2019 53rd Asilomar Conference on Signals, Systems, and Computers",3-6 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.1993.327406,Fuzzy logic implementation of intent amplification in virtual reality,IEEE,Conferences,"The authors continue exploration of a concept called intelligent virtual reality (IVR). One of its features is detection and amplification of user intent. A model developed from consideration of the semantics of fuzzy logic has been advanced as a formalism for a first order implementation of intent amplification aspects of IVR as a universal computer interface. Goal preservation is the proposed metric by which to test the process. The model formalism is introduced. Intent amplification, the idea of possible worlds introduced by E.H. Ruspini, and the concept of goal preservation are discussed.<>",https://ieeexplore.ieee.org/document/327406/,[Proceedings 1993] Second IEEE International Conference on Fuzzy Systems,28 March-1 April 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCET.2018.8537334,Gesture Controlled Virtual Reality Based Conferencing,IEEE,Conferences,"The technology available today for interacting with a virtual environment involves wired or wireless hand controls with limited buttons or a large setup involving a camera and/or a sensor to capture movements. The cost of such a setup is such that it makes it inaccessible to most. The project aims at providing a cost effective VR solution which can produce the same effect with precision and flexibility, which is accessible to all. The proposed idea is to provide a hardware device that provides the user with an immersive VR experience and uses hand gestures captured via a camera placed on the device to control and interact with the VR environment. As an application of the project, an interactive workspace environment would be simulated, using a single hardware component that provides the user the viewing interface as well as can be controlled by simple hand gestures eliminating the need for additional hand-held devices controls. The project will also delve into the field of supervised learning to make possible the implementation of gesture recognition.",https://ieeexplore.ieee.org/document/8537334/,2018 International Conference on Smart City and Emerging Technology (ICSCET),5-5 Jan. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1999.832699,Gesture recognition for virtual reality applications using data gloves and neural networks,IEEE,Conferences,"Explores the use of hand gestures as a means of human-computer interactions for virtual reality applications. For the application, specific hand gestures, such as ""fist"", ""index finger"" and ""victory sign"", have been defined. Most existing approaches use various camera-based recognition systems, which are rather costly and very sensitive to environmental changes. In contrast, this paper explores a data glove as the input device, which provides 18 measurement values for the angles of different finger joints. The paper compares the performance of different neural network models, such as backpropagation and radial-basis functions, which are used by the recognition system to recognize the actual gesture. Some network models achieve a recognition rate (training as well a generalization) of up to 100% over a number of test subjects. Due to its good performance, this recognition system is the first step towards virtual reality applications in which program execution is controlled by a sign language.",https://ieeexplore.ieee.org/document/832699/,IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339),10-16 July 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct51615.2020.00030,Ginput: a tool for fast hi-fi prototyping of gestural interactions in virtual reality,IEEE,Conferences,"Gestural interfaces in virtual reality (VR) expand the design space for user interaction, allowing spatial metaphors with the environment and more natural and immersive experiences. Typically, machine learning approaches recognize gestures with models that rely on a large number of samples for the training phase, which is an obstacle for rapidly prototyping gestural interactions. In this paper, we propose a solution designed for hi-fi prototyping of gestures within a virtual reality environment through a high-level Domain-Specific Language (DSL), as a subset of the natural language. The proposed DSL allows non-programmer users to intuitively describe a broad domain of poses and connect them for compound gestures. Our DSL was designed to be general enough for multiple input classes, such as body tracking, hand tracking, head movement, motion controllers, and buttons. We tested our solution for wands with VR designers and developers. Results showed that the tool gives non-programmers the ability to prototype gestures with ease and refine its recognition within a few minutes.",https://ieeexplore.ieee.org/document/9288432/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISM52913.2021.00016,Head Rotation Model for Virtual Reality System Level Simulations,IEEE,Conferences,"Virtual Reality (VR) promises immersive experiences in diverse areas such as gaming, entertainment, education, healthcare, and remote monitoring. In VR environments, users can navigate 360-degree content by moving or looking around in all directions, by rotating their heads, as in real life. A rapid head rotation can corrupt the wireless link, degrading the user experience. Due to the lack of proper head rotation models, testbeds are usually required to analyze VR systems. In this paper, we propose an open source code package that generates realistic head rotation traces. The code package is based on a simple, yet flexible, time-correlated mathematical model, which is extrapolated from a publicly available VR head rotation measurement-based dataset. We show that the probability density function of head rotation pitch and roll angles can be modeled as Gaussian distributions, while the probability density function of yaw angles can be modeled as a Gaussian mixture distribution. To introduce temporal correlation, we extrapolate the power spectral density of the angular processes, which are modeled with a bi-exponential decay. Finally, we show how the model can support and accelerate the design of future VR systems by proposing the analysis of a distributed Multiple Input Multiple Output (MIMO) system and the design of a situational awareness Machine Learning (ML) based beamforming training for millimeter wave networks.",https://ieeexplore.ieee.org/document/9666091/,2021 IEEE International Symposium on Multimedia (ISM),29 Nov.-1 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC46164.2021.9631002,Hess Screen Revised: How Eye Tracking and Virtual Reality change Strabismus Assessment,IEEE,Conferences,"Strabismus is a visual disorder characterized by eye misalignment. The extent of ocular misalignment is denoted as the deviation angle. With the advent of Virtual Reality (VR) Head-Mounted-Displays (HMD) and eye tracking technology, new possibilities measuring strabismus arise. Major research addresses the novel field of VR strabismus assessment by replicating prism cover tests while there is a paucity of research on screen tests. In this work the Hess Screen Test was implemented in VR using a HMD with eye tracking for an objective measurement of the deviation angle. In a study, the functionality was tested and compared with a 2D monitor-based test. The results showed significant differences in the measured deviation angle between the methods. This can be attributed to the type of dissociation of the eyes.",https://ieeexplore.ieee.org/document/9631002/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCONS.2018.8662900,Hospital Emergency Room Training Using Virtual Reality and Leap Motion Sensor,IEEE,Conferences,"The objective of this project is to provide a virtual experience for medical students to learn techniques and skills in Medical Intensive Care Unit that allows interaction with the equipment in the virtual environment. Virtual Reality is the innovation used to give an immersive ordeal of Medical Intensive Care Unit. Interaction with the virtual medical room is implementing using Leap Motion Controller, which is utilized to track the hand movement. All the procedures inside an Intensive care unit are carried out using bare hand interaction. Leap Motion has the capability to grab/pinch an object in the virtual world. Data with respect to the equipment and procedures will be shown utilizing Leap Motion User Interface. Unity3D is the development engine used to build up this application.",https://ieeexplore.ieee.org/document/8662900/,2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS),14-15 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00066,Immersive Virtual Reality Training for Inspecting Flagger Work zones,IEEE,Conferences,"Construction and maintenance work on roads pose safety risks to both drivers and workers. The responsible agencies regularly inspect work zones for compliance with traffic control and signage standards. The current training practice is to review documents related to temporary traffic control and reports from previous inspections, typically Power Point files with pictures. It would be beneficial if a new mechanism for training could be developed that is as effective as field visits but without the amount of time and effort required to visit multiple field sites. This study developed an immersive training module for transportation agency staff that inspect flagger operations in road construction and maintenance work zones. Human flaggers are commonly used to control traffic at work zones on two lane highways (one lane in each direction). The main objective of the proposed training is to deliver a realistic experience to trainees in an immersive virtual environment using the current traffic control protocols and standards. The module creation consisted of three steps. First, the roadway geometrics, work zone signage, traffic control devices, and the natural environment was created. Second, motion capture technology was used to replicate the actual movement of a human flagger directing traffic in a work zone. The environment and flagger avatar created in the first two steps were integrated and implemented in a simulation in the third step. The module was demonstrated to inspection staff at one state department of transportation (DOT) and revised based on their feedback. The state DOT staff were highly receptive to the use of virtual reality for training and commented on the benefits of the immersive experience that is lacking in their current training practices.",https://ieeexplore.ieee.org/document/9319124/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DCABES.2017.43,Impact and Challenges of Using of Virtual Reality & Artificial Intelligence in Businesses,IEEE,Conferences,"Virtual Reality (VR) is an emerging technology and is disrupting various parts of the businesses. Combining them with Artificial Intelligence (AI) creates a new dimension to the business and affects crucial decisions for the future continuation of the business. AI's role is more pronounced than the VR aspects as not all the business needs VR but need AI. Determining the areas where VR/AI can impact the business is one of the issues stake holders are interested in. As it is new, the integration of VR and AI opens new possibilities and will provide new range of experiences. Thus, this paper focuses on addressing few of those issues and considers a case study with the need for incorporating recent developments and the challenges that need to be addressed.",https://ieeexplore.ieee.org/document/8253059/,"2017 16th International Symposium on Distributed Computing and Applications to Business, Engineering and Science (DCABES)",13-16 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETAS51660.2020.9484306,Impact of Augmented Reality and Virtual Reality in the Transformation of Virtual Customer Relationship Management Sector,IEEE,Conferences,"The integration of virtual reality and artificial intelligence into the business environment takes human-machine interaction to an advanced level. The inception of these technologies along with deep learning approaches has transformed the business environment completely, especially customer relationship management. Additionally, augmented reality has changed the business production environment with interactive designs and machines learning approaches. Augmented reality is expected to grow by $814.7B in 2025 which will bring a huge transformation in the businesses. Though leveraging technologies have been introduced to transform customer experience, with an ever-accelerating tidal wave of advancing technology, the adoption rate of these technologies in business is not satisfactory. This is because of unawareness of the experience transformation rate of augmented reality on business. Many applications related to Artificial Intelligence and Virtual Reality experience were found but no success rate was observed because of unfamiliarity with the key features to the technology. There is a need to introduce the power of technology by engaging with it. A systematic approach to identify the key aspects of augmented reality and virtual reality for the successful adoption of business in customer relationship management sector has been used for the present study. A case study of Amazon is considered to analyze the impact of the implementation of Virtual Reality on product sales.",https://ieeexplore.ieee.org/document/9484306/,2020 IEEE 7th International Conference on Engineering Technologies and Applied Sciences (ICETAS),18-20 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI.2017.8280839,Implementation of gesture driven virtual reality for car racing game using back propagation neural network,IEEE,Conferences,"This paper explores the development an intelligent human-computer communicative interface for a computer gaming appliance Gesture tracking is selected as the most suitable interactive modality, to ensure a user friendly gaming environment. For complete capture of an entity position, both distance and angle based features have been considered. Gradient Descent Learning is adapted to derive the weights following the feed forward architecture of a Back Propagation Neural Network. The system is formulated in such a way that it provides unbiased subject independent results. A gesture driven console for Forza Motorsport 4 game is designed, and its performance is compared with contender neural networks.",https://ieeexplore.ieee.org/document/8280839/,2017 IEEE Symposium Series on Computational Intelligence (SSCI),27 Nov.-1 Dec. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00041,Improving the Approach-Avoidance Task in Virtual Reality Through Presence and Virtual Risk Situations,IEEE,Conferences,"Automatic approach biases toward alcohol- and smoking-related cues have been implicated in the development and maintenance of addictive behaviors. Studies aiming at modifying such biases with the Approach-Avoidance Task (AAT) have shown promise in changing maladaptive approach tendencies for addiction-related cues and reducing addictive behavior. However, training effects tend to be small and partly inconsistent. For this reason, we have already developed and evaluated a VR-AAT to improve efficacy of the training. Results showed that our new VR-AAT did not change approach biases, nor other cognitive biases, but it was superior in reducing daily smoking, and both groups improved in other smoking- and health-related variables across time. To further investigate working mechanisms of Approach Bias Modification (ABM), in particular crucial training ingredients, we designed and implemented an extension for our VR-AAT application. This extension takes the form of a virtual bar, which in everyday life represents a risk situation in terms of alcohol and cigarette consumption. Combined with a high degree of presence, we hope to further improve training effectiveness as well as ecological validity. This article describes the theoretical background, the concept as well as the implementation of the virtual risk situation, and presents the planned study design.",https://ieeexplore.ieee.org/document/9644300/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMC53470.2022.9753872,Intelligent Application of Virtual Reality Technology in the Design of Elderly Service Station,IEEE,Conferences,"This article is mainly based on virtual reality technology to carry out related researches on the architectural design of the retirement post in the old community, which is mainly divided into three parts. First, the traditional design and shortcomings of the old-age service station building are introduced, and then the virtual reality technology and circuit system are designed to complete the transformation of the old-age facilities service from extensive to refined. Through an in-depth analysis of the functional composition and service aspects of the elderly care service stations in the functional types and corresponding design strategies of the elderly care service stations adapted to different levels of care and individual needs are proposed, and the space design basis for the refined services of the terminal elderly care facilities is improved. 7.3 %.",https://ieeexplore.ieee.org/document/9753872/,2022 6th International Conference on Computing Methodologies and Communication (ICCMC),29-31 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON46332.2021.9454032,Intelligent Virtual Reality Tutoring Systems as a New Generation of Simulators: Requirements and Opportunities,IEEE,Conferences,"This article explores the features and requirements of intelligent virtual reality tutoring systems - an innovative type of educational software. It also explores the applicability of these systems as educational simulators and classifies them with the help of existing typologies. The authors present a detailed discussion of immersive virtual learning environments, the penetration of artificial intelligence into this domain, and the effects of this penetration. The paper seems to be the first publication to survey intelligent virtual reality tutoring systems (IVRTSs). The results are our own IVRTS that can act as an educational simulator for kinematics as well as a set of recommendations on IVRTS design and implementation.",https://ieeexplore.ieee.org/document/9454032/,2021 IEEE Global Engineering Education Conference (EDUCON),21-23 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT46573.2021.9453573,Interaction by Hand-Tracking in a Virtual Reality Environment,IEEE,Conferences,"This article describes the elaboration of a low-cost system that allows user interaction with VR environments modeled in Unity and viewed through Google Cardboard. The proposed system divides the processing in a client-server architecture in which the VR experience is executed on the client, while it sends pictures captured from the mobile device inside the Card Board to the server. This server processes the hand pose information and sends it to the client.",https://ieeexplore.ieee.org/document/9453573/,2021 22nd IEEE International Conference on Industrial Technology (ICIT),10-12 March 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2018.8441031,Interactive Distance Media Learning Collaborative Based on Virtual Reality with Solar System Subject,IEEE,Conferences,"The emergence of the Internet and various computer devices has revolutionized the learning process in various schools and colleges, both in terms of media and interaction methods. E-learning and distance learning is one of the revolutions of how education can be passed well through cyberspace media that are connected, organized, and integrated with each other. In this research will be developed distance learning media using virtual reality technology, where teachers and students can make communication and made an immersive learning process although they stayed in different places. However, they can have the same face-to-face conversation, sitting in one table, and made virtual meeting with their avatars like meet in the real world in space. As an example learning subject, this project will present solar system learning with interactive virtual reality media.",https://ieeexplore.ieee.org/document/8441031/,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",27-29 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIE53562.2021.00033,Interactive simulation teaching system of electrical engineering based on virtual reality,IEEE,Conferences,"The traditional simulation teaching system is limited by the system support software, which cannot realize good interactive teaching. To this end, an interactive simulation teaching system based on virtual reality for electrical engineering is designed. Based on the hardware consisting of ATmega328 chip as the core, sensors and surrounding circuits, the development tool OpenGL is used to draw 3D virtual teaching scenes. The system design is completed by using a double-layer hidden Markov chain to realize simultaneous interactive teaching of multiple paths. The comparison experiment shows that the teaching system has better interactivity, can effectively enhance students' learning interest, and has good user feedback.",https://ieeexplore.ieee.org/document/9534584/,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),18-20 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2018.8446260,Inverse Virtual Reality: Intelligence-Driven Mutually Mirrored World,IEEE,Conferences,"Since artificial intelligence has been integrated into virtual reality, a new branch of virtual reality, which is called inverse virtual reality (IVR), is created. A typical IVR system contains both the intelligence-driven virtual reality and the physical reality, thus constructing an intelligence-driven mutually mirrored world. We propose the concept of IVR, and describe the details about the definition, structure and implementation of a typical IVR system. The parallel living environment is proposed as a typical application of IVR, which reveals that IVR has a significant potential to extend the human living environment.",https://ieeexplore.ieee.org/document/8446260/,2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),18-22 March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8798046,Keynote Speaker: Virtual Reality for Enhancing Human Perceptional Diversity Towards an Inclusive Society,IEEE,Conferences,"We conducted research project towards an inclusive society from the viewpoint of the computational assistive technologies. This project aims to explore AI-assisted human-machine integration techniques for overcoming impairments and disabilities. By connecting assistive hardware and auditory/visual/tactile sensors and actuators with a user-adaptive and interactive learning framework, we propose and develop a proof of concept of our “xDiversity AI platform” to meet the various abilities, needs, and demands in our society. For example, one of our studies is a wheelchair for automatic driving using “AI technology” called “tele wheelchair”. Its purpose is not fully automated driving but labor saving at nursing care sites and nursing care by natural communication. These attempts to solve the challenges facing the body and sense organs with the help of AI and others. In this keynote we explain the case studies and out final goal for the social design and deployment of the assistive technologies towards an inclusive society.",https://ieeexplore.ieee.org/document/8798046/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6009874,Kinematics simulation of upper limb rehabilitant robot based on virtual reality techniques,IEEE,Conferences,"The wearable exoskeletal robot for upper extremity rehabilitation is taken as the research object. According to D-H method, an accurate three-dimensional mechanism model for the robot system is established by SolidWorks software. The virtual set was generated in Simulink/VRML to carry out dynamic simulation. The variable parameters were set based on robotic practical joint range movement. The simulation of all joints and terminal trajectory and space motion area provided theoretical basis for position control, remote control and trajectory planning, realizing the rehabilitation robot visualizations and system interaction.",https://ieeexplore.ieee.org/document/6009874/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00038,Machine Learning Concepts for Dual-Arm Robots within Virtual Reality,IEEE,Conferences,"The collaboration between humans and artificial intelligence (AI) driven robots lay the foundations for new approaches in industrial production. However, intensive research is required to develop machine learning behavior that is not only able to execute shared tasks but also acts following the expectations of the human partner. Rigid setups and restrictive safety measures deny the acquisition of adequate training samples to build general-purpose machine learning solutions for evaluation within experimental studies. Based on established research that trains AI systems within simulated environments, we present a machine learning implementation that enables the training of a dual-arm robot within a virtual reality (VR) application. Building upon preceding research, an activity diagram for a shared task for the machine learning model to learn, was conceptualized. A first approach, using vector distances, led to flawed results, whereas a revised solution based on collision boxes resulted in a stable outcome. While the implementation of the machine learning model is fixed on the activity diagram of the shared task, the presented approach is expandable as a universal platform for evaluating Human-Robot Collaboration (HRC) scenarios in VR. Future iterations of this VR sandbox application can be used to explore optimal workplace arrangements and procedures with autonomous industrial robots in a wide range of possible scenarios.",https://ieeexplore.ieee.org/document/9644376/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSPA.2018.8368706,Modeling the affective space of 360 virtual reality videos based on arousal and valence for wearable EEG-based VR emotion classification,IEEE,Conferences,"This study attempts to produce a novel database for emotional analysis which uses virtual reality (VR) contents, obtained from third party sources such as YouTube, Discovery VR, Jaunt VR, NYT VR, Veer VR and Google Cardboard, as the visual stimuli in the classification of emotion using commercial-of-the-shelf (COTS) wearable electroencephalography (EEG) headsets. While there are available sources for emotional analysis such as Dataset for Emotion Analysis using EEG, Physiological and video signals (DEAP) dataset presented by Koelstra et al. and Database for Emotional Analysis in Music (DEAM) dataset by Soleymani et al, their contents are focused on using music stimuli and music video stimuli. The database which will be presented here will consist of novel affective taggings using virtual reality content, specifically on Youtube 360 videos, as evaluated by 15 participants based on the Arousal-Valence emotion model (AVS). The feedback obtained from these evaluations will serve as the underlying dataset for the next stage of machine learning implementation, which is the targeted emotion classification of virtual reality stimuli using wearable EEG headsets.",https://ieeexplore.ieee.org/document/8368706/,2018 IEEE 14th International Colloquium on Signal Processing & Its Applications (CSPA),9-10 March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIAM54119.2021.00121,Multi-Robot Cooperative Hunting Based on Virtual Reality Technology,IEEE,Conferences,"Multi-robot cooperative rounding up is one of the important methods to test the intelligence level of multi-robot system. In order to test the intelligence of multi-robot system, a multi-robot cooperative rounding up system is established in this paper. In order to realize the rounding up system, this paper uses finite state machine to simplify the rounding up behavior of multi-robot. At the same time, in order to observe the multi-robot system more intuitively, this paper uses Vega software to realize virtual reality technology and applies it to the rounding up system established in this paper. Experiments show that the virtual reality technology based on VEGA can effectively simulate the multi-robot roundup, so that observers can observe the system more intuitively and effectively, so as to find out whether there are problems in the system.",https://ieeexplore.ieee.org/document/9724756/,2021 3rd International Conference on Artificial Intelligence and Advanced Manufacture (AIAM),23-25 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNSC.2013.6548806,Multi-agent based architecture for virtual reality intelligent simulation system of vehicles,IEEE,Conferences,"Most existed researches on traffic simulation with virtual reality are mainly concentrated on the visualization or the real-time simulation to create immersion experiences in virtual world, but little attention has been paid on system modeling and formal specification of traffic simulation. Based on multi-agent technology, Virtual Reality Intelligent Simulation System of Vehicles (VR-ISSV) is proposed to investigate the modeling method of vehicles simulation system, with the advantages of reusability, scalability and flexibility. Firstly, the framework modeling is introduced to depict the general intelligent simulation system. Then, intelligent vehicle agent and environment agent are presented for the simulation of interactions among the vehicles, the traffic situation and the environment. Finally, the application results of the proposed system are realized.",https://ieeexplore.ieee.org/document/6548806/,"2013 10th IEEE INTERNATIONAL CONFERENCE ON NETWORKING, SENSING AND CONTROL (ICNSC)",10-12 April 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.862545,Multipurpose virtual-reality-based motion simulator,IEEE,Conferences,"Public security has become an important issue everywhere. Especially, the safe manipulation and control of various machines and vehicles has gained special attention such that the authorities keep emphasizing the strict training and censoring of human operators. Currently, such training and censoring process usually relies on the actual machines, equipment, or vehicles in the real sites. This not only has high demands in space, time and cost, but also causes another public security problem. In this connection, the world-wide trend is to tackle the above dilemma by using virtual reality (VR). However, the current researches or products on VR are more matured in the software display part of VR. How to combine 3D VR display with motion platform to achieve the aforementioned training and censoring purposes is an important research issue. This paper focuses on this research issue, and the goal is to develop a multipurpose virtual-reality-based motion simulation system to meet the requirements of public security in training and censoring of human operators.",https://ieeexplore.ieee.org/document/862545/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCSE.2009.661,Organ Texture Synthesis for Virtual Reality-Based Surgical Simulators,IEEE,Conferences,"Virtual Reality-based simulators is an important tool for surgical skills training and assessment. In general, the degree of realism experienced by the trainees is determined by the visual and biomechanical fidelity of the simulator. Organ textures can greatly affect visual realism and hence the overall quality of the simulation. In this paper, we use a very simple algorithm that can efficiently synthesize a wide variety of organ textures. The algorithm uses pixel-based texture synthesize technology. For each pixel in the output image, an L-shaped neighborhood of current pixel of a specific size is considered. Each pixel from this neighborhood generates a ¿shifted¿ candidate pixel according to its original position in the input texture. The best pixel is chosen with a neighborhood most similar to the current L-shaped neighborhood in the output image. This method is fast and its implementation is straightforward. The experiment results show the algorithm is feasility and validity.",https://ieeexplore.ieee.org/document/5403478/,2009 Second International Workshop on Computer Science and Engineering,28-30 Oct. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAICT.2019.8784846,Overcoming Glossophobia Based on Virtual Reality and Heart Rate Sensors,IEEE,Conferences,"Glossophobia or commonly called speech anxiety is the fear of public speaking. It is a psychological disorder which a person is afraid to speak in public or can be interpreted as nervous. This problem is caused by the lack of preparation or training carried out to public speaking. In Addition, the training is generally lack the atmosphere or impression like speaking in public. Therefore, this system was created for helping someone in preparation before public speaking. This system simulation for practicing public speaking based on technology such as virtual reality, video 360, and arduino heart rate sensors. The results of the functionality and non-functionality of the system have been fully implemented and are running well. In addition, based on the results of the questionnaire and test of objectivity, this system has good feedback for helping someone to prepare and practice in public speaking based on virtual reality technology.",https://ieeexplore.ieee.org/document/8784846/,"2019 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",1-3 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DeSE51703.2020.9450783,Overcoming Speech Anxiety Using Virtual Reality with Voice and Heart Rate Analysis,IEEE,Conferences,"Social phobias are afflictions that millions of people suffer from. One common social phobia is speech anxiety (Glossophobia), which makes it difficult for people to talk in public or with others. To address this problem, this system was created to help people with Glossophobia practice making presentations or having personal interviews with less fear. The objective is to train them before their presentations or interviews by simulating 360° video environments with virtual reality (VR) technology. During the practice, the system analyzes the voice and heart rate of the person to discover any emotional and physical symptoms of speech anxiety using Arduino heart rate sensors, machine learning, and speech recognition techniques. The system will generate advice based on the symptoms to help make the user more confident. Additionally, after several training sessions, the system will present a report showing the progress in the user's performance. The system has been fully implemented and has demonstrated its operational effectiveness.",https://ieeexplore.ieee.org/document/9450783/,2020 13th International Conference on Developments in eSystems Engineering (DeSE),14-17 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI46879.2019.9041996,Pain Relief using Virtual Reality,IEEE,Conferences,"A serious issue in the medical field is the treatment of patients affected by chronic pain, burns or recovery treatment after an accident or surgery. The regular treatment is providing the patient with opioid pain relievers medications that in a wide variety of cases leads to addiction. To prevent the partial use of pain reducing drugs in patients that suffer from minor to medium pain and delay their use as much as possible, we propose the use of Virtual Reality technology. Current technological advances have allowed virtual reality application to leave their mark in the medical sector by the use of different training simulators and applications that help the doctors experience from simulating a variety of surgeries. In this paper we present the effects of Virtual Reality immersion on patients that suffer mild to medium pain, as well as our knowledge in the implementation of a virtual reality application that can achieve a positive effect on this serious issue. A direct benefit of successfully creating an application would be the reduction in the amount of pain the patient feels. A second positive effect would be the decrease in the consumption of pain relievers and a better response of the patient to the recovery treatment without the risk of becoming depended on opioids.",https://ieeexplore.ieee.org/document/9041996/,"2019 11th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",27-29 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2019.8848119,Realtime Adaptive Virtual Reality for Pain Reduction,IEEE,Conferences,"Recent years have seen digital game mediums taking conventional amusement, entertainment and leisure industries by storm. They have revolutionized the system to the extent that the industry cannot now even dream to do without this overwhelming reality. The same game mediums that have capitalized on intrinsic leisure aspects have simultaneously focused with equal vigor on other equally, if not more, important collateral objectives. This paper builds on this concept and discusses a work in progress currently being carried out at the University of Malta. It proposes the use of games as a means of distraction therapy for individuals undergoing painful clinical treatment procedures. The creation of an adaptive Virtual Reality (VR) game within an Artificial Intelligence framework will without doubt be of a significantly greater benefit to the community than mere entertainment applications.",https://ieeexplore.ieee.org/document/8848119/,2019 IEEE Conference on Games (CoG),20-23 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRAIS.1993.380750,Realtime collision detection for virtual reality applications,IEEE,Conferences,"Virtual reality technology aims at the expansion of the communication bandwidth by providing users with 3D immersive environments. For the true direct manipulation of the environments, fast collision detection must be provided to increase the sense of reality. A collision detection scheme for virtual reality applications is proposed. The method exploits a hierarchical object representation to facilitate the detection of colliding segments.<>",https://ieeexplore.ieee.org/document/380750/,Proceedings of IEEE Virtual Reality Annual International Symposium,18-22 Sept. 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW50115.2020.00206,ReliveReality: Enabling Socially Reliving Experiences in Virtual Reality via a Single RGB camera,IEEE,Conferences,"We present a new experience sharing method, ReliveReality, which transforms traditional photo/video memories into 3D reconstructed memories and allows users to relive past experiences through VR socially. ReliveReality utilizes deep learning-based computer vision techniques to reconstruct people in clothing, estimate multi-person 3D pose and reconstruct 3D environments with only a single RGB camera. Integrating with a networked multi-user VR environment, ReliveReality enables people to ‘enter’ into a past experience, move around and relive a memory from different perspectives in VR together. We discuss the technical implementation and implications of such techniques for privacy",https://ieeexplore.ieee.org/document/9090534/,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),22-26 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AINIT54228.2021.00111,Research on the Application of Virtual Reality Technology in Digital Media Art Creation,IEEE,Conferences,"Virtual reality technology, also known as VR technology, first appeared in the United States. After years of development, it has evolved into a brand-new computer automatic control technology. The main function of this technology is to simulate the environment and then realize human-computer interaction. In short, virtual reality technology is the use of computer software technology to simulate the real environment, combining perception systems, three-dimensional technology, sensing equipment and computer automation skills to promote the organic integration of multi-source information, and establish an interactive three-dimensional real-world and simulated entity system, Enabling users to experience the environment and perform human-computer interaction. The thesis will give examples of the successful application of virtual reality technology in the improvement of digital media interaction, hoping to provide reference for digital media art creation.",https://ieeexplore.ieee.org/document/9725004/,"2021 2nd International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)",15-17 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00120,Research on the Application of Virtual Reality Technology in English Classroom,IEEE,Conferences,"With the continuous development of computer hardware and network technology, many emerging technologies are also constantly moving towards the road of popularization from behind the scenes. In the past two years, virtual reality technology has become a hot topic in the industry, education and research fields. With the development of software and the reduction of related hardware costs, coupled with the great potential of virtual reality technology in the field of training and education, the development and design of related courses and supporting materials are imperative. This paper combines the research of some Chinese scholars on the problems in college English teaching, and combines virtual reality technology with college English teaching to form a new way of college English teaching.",https://ieeexplore.ieee.org/document/9696118/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00060,"ScienceVR: A Virtual Reality Framework for STEM Education, Simulation and Assessment",IEEE,Conferences,"This paper addresses the use of Virtual Reality (VR) in Science, Technology, Engineering, and Math (STEM) education. There are limited studies investigating the proper design and effectiveness of VR in STEM education, and current VR frameworks and applications lack explicit links to the established learning theories and assessment mechanisms to evaluate learning outcomes. We present ScienceVR, an educational virtual reality design framework, illustrated through a science laboratory prototype, to bridge some of the gaps identified in the design and development of a VR environment for learning. We established design guidelines and implemented an in-app data collection system to measure users’ learning, performance, and task completion rate. Our evaluation using ANOVA and other non-parametric methods with 36 participants in three groups: immersive VR (IVR), desktop VR(DVR), and 2D indicated improved usability and learning outcomes for the IVR group. Task completion rate in the IVR group was higher (68% compared to DVR with 50%). For memorability, the IVR condition performed better than DVR while for learnability, IVR&DVR performed significantly better than 2D. IVR group has performed better and faster with more accuracy compared to the DVR group in completing the tasks.",https://ieeexplore.ieee.org/document/9644266/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2018.8446187,Shopping in Virtual Reality,IEEE,Conferences,"In contrast to traditional retail stores, online shopping offers many advantages, such as unlimited opening hours and a stronger focus on functionality. But this is accompanied by a complex categorization, limited product visualization and immersion. Virtual Reality (VR) has the potential to create new shopping experiences that combine the advantages of e-commerce sites and conventional brick-and-mortar shops. We examined the main features of online and offline shops in terms of buying behavior and customer frequency. Furthermore, we designed and implemented an immersive WebVr online purchasing environment and aimed to retain the benefits of online shops, such as search functionality and availability, while focusing on the shopping experience and immersion. This VR shop prototype was evaluated in a case study with respect to the Virtual Reality Shopping Experience (VRSE) model. The next step is to classify, investigate and evaluate the next generation of VR shops, including product interaction and navigation techniques, as well as store and product representations.",https://ieeexplore.ieee.org/document/8446187/,2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),18-22 March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00024,SnapMove: Movement Projection Mapping in Virtual Reality,IEEE,Conferences,"We present SnapMove a technique to reproject reaching movements inside Virtual Reality. SnapMove can be used to reduce the need of large, fatiguing or difficult motions. We designed multiple reprojection techniques, linear or planar, uni-manual, bi-manual or head snap, that can be used for reaching, throwing and virtual tool manipulation. In a user study (n=21) we explore if the self-avatar follower effect can be modulated depending on the cost of the motion introduced by remapping. SnapMove was successful in re-projecting user's hand position from e.g. a lower area, to a higher avatar-hand position-a mapping which can be ideal for limiting fatigue. It was also successful in preserving avatar embodiment and gradually bring users to perform movements with higher cost energies, which have most interest for rehabilitation scenarios. We implemented applications for menu interaction, climbing, rowing, and throwing darts. Overall, SnapMove can make interactions in virtual environments easier. We discuss the potential impact of SnapMove for application in gaming, accessibility and therapy.",https://ieeexplore.ieee.org/document/9319115/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCON52037.2021.9702309,Suspicious Behavior Detection Using Man Machine Model with Integration of Virtual Reality,IEEE,Conferences,"Video or CCTV surveillance plays an important role in the security of any place whether it is residential areas, industries, public spaces like shopping malls, museums and other monuments, banks, offices, building sites, warehouses, airports, railway stations, etc. It will help in preventing theft and damage to manufactured goods and products as well as manufacturing equipment, having complete and recorded production accident data, having complete and recorded production accident data, monitoring every stage of the manufacturing process, and prevention and analysis of any type of crime. But the current systems rely too much on humans monitoring the feeds from these videos which are prone to some problems like reduced attention and fatigue during long stretches of monitoring. So, there is a need for a system where these humans are aided by the machines in the monitoring process. The system proposed and implemented in this study would help to overcome this problem by aiding the man with smart machines and neural networks. Also, the system works on video camera feed instead of static camera shots which would help in capturing the sequential information that may be missed when using static images. And along with this, a model has been proposed using neural network technology that can automatically identify the individuals exhibiting the suspicious behavior from live camera feed input. At last research has been done on whether to use virtual reality on live video or CCTV surveillance or not based on various research papers published in similar domain.",https://ieeexplore.ieee.org/document/9702309/,2021 5th International Conference on Information Systems and Computer Networks (ISCON),22-23 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2017.8166500,System using a hybrid application for virtual reality 3D drawing,IEEE,Conferences,"Smart devices have many sensors that capture the user data and use it for device control, improving the user experience, or send it through a computer network in order to be processed. The interaction between user and applications, especially in virtual reality, is made via motion and gestures with information offered by sensors such as accelerometer, compass or gyroscope. In order to reduce the complexity of developing this type of applications, it is possible to write hybrid applications that allow the programmer to code using web technologies then deploy and run the resulted applications natively on multiple operating systems. This paper presents the implementation of a virtual reality drawing hybrid application, created with the Telerik Mobile platform, which reads the compass and accelerometer data and sends the information to a server that creates a 3D representation using the Processing Development Environment.",https://ieeexplore.ieee.org/document/8166500/,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",29 June-1 July 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISE.2009.5366159,The Development of Rule-Based Virtual Reality System,IEEE,Conferences,"In this paper, we describe an approach to communicate with computers in virtual reality (VR) system, this VR system can provide an operating sequence saved in an expert database by using mouse-click event. The VR system allows supervisor to set a series of training procedures, then to test operators whether could follow operating procedures or not. Furthermore, the testing procedures are saved into a XML file which can be transformed into instructions. This means that testing sequence can be captured and transformed into operation instructions automatically, as while as, the operating instructions also can be analyzed by a data mining system. In the end, Mal-operation of processing plants that could cause an injury to people and facilities can be avoided.",https://ieeexplore.ieee.org/document/5366159/,2009 International Conference on Computational Intelligence and Software Engineering,11-13 Dec. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICE3IS54102.2021.9649738,The Experiment Design to Investigate The Effect of Selective Visual Attention Implementation on User Experience in Virtual Reality Anatomy Learning,IEEE,Conferences,"Anatomy learning in medical education has been considered a fundamental foundation. The use of cadavers in anatomy studies has been considered the gold standard. However, due to several factors, the use of cadavers in modern medical education was considered irrelevant. As a result, anatomy learning began to involve the use of technology. One of them is using a virtual reality system. Anatomy learning using the virtual reality system relies on the visual field in presenting the information. Many of the bones make up the structure of the human skeleton that we use as our dataset, which raises the problem of visual attention in presenting content in virtual reality systems. For that, we propose a selective visual attention method (a feature that can isolate objects) to improve the user experience. In this study, we develop an experimental design to investigate the effect of our proposed methods on user experience. The developed design offers credible investigative results. To ensure the accuracy of data collection and the accuracy of data analysis from the developed experimental design, we use methods that suit the research needs. The results of this study are in the form of a planned experimental design that can increase readers' confidence in the results of investigations related to user experience.",https://ieeexplore.ieee.org/document/9649738/,2021 1st International Conference on Electronic and Electrical Engineering and Intelligent System (ICE3IS),15-16 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1995.531965,The hyper hospital-virtual reality based medical system on the computer network-its concept and user-configurable virtual world creating system,IEEE,Conferences,"We have been developing a novel medical care system which is constructed on an electronic or computerized information network using virtual reality as the principal human interface. The major purpose of the hyper hospital is to restore humane interactions between patients and various medical caretakers by making a much closer contact between them in the real medical scene than that in current conventional medical practice. In the present study, we discuss the most fundamental part of the development of the virtual reality system for the medical use, that is, system software for the creation of the virtual world which can be defined and modified by users of various levels, not only by the medical caretakers, but particularly by patients.",https://ieeexplore.ieee.org/document/531965/,Proceedings 4th IEEE International Workshop on Robot and Human Communication,5-7 July 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIHAS.1993.410557,Understanding natural language for virtual reality: An information theoretic approach,IEEE,Conferences,"The approach to the solution of the natural language parsing (NLP) problem has traditionally taken a linguistic route. A more basic approach using the theory of information is exploited here. Parsing is accomplished based on the information content of the sentence. This reduces processing time, allows for the handling of ungrammatical sentences, and produces a structure that allows the computer to gain information on a continuous basis. The approach is entirely integrable with much of the important NLP work which has been done in the past. An application program which has been developed to illustrate the basic technique is also described.<>",https://ieeexplore.ieee.org/document/410557/,"1993 4th Annual Conference on AI, Simulation and Planning in High Autonomy Systems",20-22 Sept. 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR50410.2021.00036,Unscripted Retargeting: Reach Prediction for Haptic Retargeting in Virtual Reality,IEEE,Conferences,"Research is exploring novel ways of adding haptics to VR. One popular technique is haptic retargeting, where real and virtual hands are decoupled to enable the reuse of physical props. However, this technique requires the system to know the users' intended interaction target, or requires additional hardware for prediction. We explore software-based reach prediction as a means of facilitating responsive, unscripted retargeting. We trained a Long Short-Term Memory network on users' reach trajectories to predict intended targets. We achieved an accuracy of 81.1 % at approximately 65% of movement. This could enable haptic retargeting during the last 35% of movement. We discuss the implications for possible physical proxy locations.",https://ieeexplore.ieee.org/document/9417717/,2021 IEEE Virtual Reality and 3D User Interfaces (VR),27 March-1 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMCAS52219.2021.9629097,Using Machine Learning and Virtual Reality for Orthopedic Treatment and Abnormality Detection Based on Multivariate Time Series Data,IEEE,Conferences,"In this work we present a virtual reality machine-learning system for telehealth orthopedic treatment. Our system can recognize orthopedic abnormalities and the presence of pain. It is based on a widely used virtual reality system, combined with its sensors. We implemented an algorithm that can identify very accurately wrist and neck pain and can serve as a real-time remote system for rehabilitation doctors or physical therapists, as part of a virtual reality telehealth treatment program. Our algorithms synchronize the patient’s movement data with a dedicated data server. The system has an easy-to-use interface for analysis of the collected data. We achieved more than 90% success rates evaluating the presence of neck pain and wrist pain across given exercises for each of our volunteers. Our system can serve as the basis for a real-world telehealth, clinically operative machine.",https://ieeexplore.ieee.org/document/9629097/,"2021 IEEE International Conference on Microwaves, Antennas, Communications and Electronic Systems (COMCAS)",1-3 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISC246665.2019.9071662,Using Virtual Reality Environments to Predict Pedestrian Behaviour,IEEE,Conferences,"Pedestrian behaviour modelling and simulation play a fundamental role in reducing traffic risks and new policies implementation costs. However, representing human behaviour in this dynamic environment is not a trivial task and such models require an accurate representation of pedestrian behaviour. Virtual environments have been gaining notoriety as a behaviour elicitation tool, but it is still necessary to understand the validity of this technique in the context of pedestrian studies, as well as to create guidelines for its use. This work proposes a proper methodology for pedestrian behaviour elicitation using virtual reality environments in conjunction with surveys or questionnaires. The methodology focuses on gathering data about the subject, the context, and the action taken, as well as on analyzing the collected data to finally output a behavioural model. The resulting model can be used as a feedback signal to improve environment conditions for experiment iterations. A concrete implementation was built based on this methodology, serving as an example for future studies. A virtual reality traffic environment and two surveys were used as data sources for pedestrian crossing experiments. The subjects controlled a virtual avatar using an HTC Vive and were asked to traverse the distance between two points in a city. The data collected during the experiment was analyzed and used as input to a machine learning model capable of predicting pedestrian speed, taking into account their actions and perceptions. The proposed methodology allowed for the successful data gathering and its use to predict pedestrian behaviour with fairly acceptable accuracy.",https://ieeexplore.ieee.org/document/9071662/,2019 IEEE International Smart Cities Conference (ISC2),14-17 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00031,Using Visualization of Convolutional Neural Networks in Virtual Reality for Machine Learning Newcomers,IEEE,Conferences,"Software systems and components are increasingly based on machine learning methods, such as Convolutional Neural Networks (CNNs). Thus, there is a growing need for common programmers and machine learning newcomers to understand the general functioning of these algorithms. However, as neural networks are complex in nature, novel presentation means are required to enable rapid access to the functionality. For that purpose, we examine how CNNs can be visualized in Virtual Reality (VR), as it offers the opportunity to focus users on content through effects such as immersion and presence. With a first exploratory study, we confirmed that our visualization approach is both intuitive to use and conductive to learning. Moreover, users indicated an increased motivation to learning due to the unusual virtual environment. Based on our findings, we propose a follow-up study that specifically compares the benefits of a virtual visualization approach to a traditional desktop visualization.",https://ieeexplore.ieee.org/document/8942366/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSITSS47250.2019.9031021,Using virtual reality to boost the effectiveness of brain-computer interface applications,IEEE,Conferences,"Noninvasive brain-computer interface (BCI) uses data from electroencephalographic (EEG) sensors to train a model using machine learning and pattern detection algorithms to recognize certain patterns. These patterns correspond to a command to an actuator like wheels of a wheelchair, keyboard or mouse. Virtual reality is used to make a virtual environment (VE) where the users can control virtual objects. BCI generated commands can be used to control the virtual objects with a suitable interface. Unity 3D software provides the interface and the VE. The noninvasive nature of input collection makes the input prone to noise and error. The user needs considerable amount of training to learn to control the wheelchair efficiently. This training is done in a VE where the user controls a virtual wheelchair. The accuracy of wheelchair control before and after training in the VE is compared. It is observed that the user can control the physical wheelchair with better accuracy after training in the VE.",https://ieeexplore.ieee.org/document/9031021/,2019 4th International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS),20-21 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ZINC.2018.8448645,VR Job Interview Simulator: Where Virtual Reality Meets Artificial Intelligence for Education,IEEE,Conferences,"Nowadays, people have to face many challenges when going to an interview: introversion, insecurity, lack of technical or social skills. Training becomes highly recommended in order to improve interview performances. The current paper presents VR Job, an application which proposes an innovative way of training for an interview. By combining the advantages of various technologies, such as virtual reality and chatbots, our application creates an interactive way of helping software engineers train for their interviews. Emotion recognition techniques are also included, helping provide accurate feedback for the user.",https://ieeexplore.ieee.org/document/8448645/,2018 Zooming Innovation in Consumer Technologies Conference (ZINC),30-31 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW50115.2020.00229,VR2ML: A Universal Recording and Machine Learning System for Improving Virtual Reality Experiences,IEEE,Conferences,"We developed software for recording and reusing user actions and associated effects in various VR applications. In particular, we implemented two plugin systems that allow 3D motion data recorded in VR to be transferred to a machine learning module without programming. One is a system that runs on Unity and records actions and events in VR, and it is called “VRec.” The other is a system for using data recorded with VRec for machine learning, called “VR2ML.”",https://ieeexplore.ieee.org/document/9090450/,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),22-26 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCES48766.2020.9137914,VRCAuth: Continuous Authentication of Users in Virtual Reality Environment Using Head-Movement,IEEE,Conferences,"The traditional PIN (Personal Identification Number) and password-based mechanisms are challenging in terms of remembering by the user and eavesdropping in VR (Virtual Reality) environment. The easy PINs and passwords weaken user authentication. The biometric-based authentication provides the solution to this problem but does not verify the user continuously during activities because it needs the active participation of the user for every authentication. The VRCAuth, a novel continuous authentication system is implemented for the user in the VR environment. In our system, head motion is used to provide continuous authentication. VRCAuth uses different machine learning techniques to classify the head motion of an authorized and unauthorized user. VRCAuth does not require any OS-level modifications or extra hardware, and it is suitable to use with existing hardware. VRCAuth performs better by attaining the accuracy of 99 % in 0.02 seconds providing a continuous authentication to the user in VR environment.",https://ieeexplore.ieee.org/document/9137914/,2020 5th International Conference on Communication and Electronics Systems (ICCES),10-12 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00042,VRescuer: A Virtual Reality Application for Disaster Response Training,IEEE,Conferences,"With the advancement of modern technologies, Virtual Reality plays an essential role for training rescuers, particularly for disaster savers employing simulation training. By wholly immersed in the virtual environment, rescuers are capable of practicing the required skills without being threatened of their lives before experiencing the real world situation. This paper presented a work-in-progress Virtual Reality application called VRescuer to help trainees get used to various disaster circumstances. A scenario of a city was created with an ambulance rescuer and several rescuees in the scene. The intelligent ambulance rescuer was introduced as a rescuer/guider to automatically search and find the optimal paths for saving all rescuees. The trainee can interfere in the rescuing process by placing obstacles or adding more rescuees along the ways which cause the rescue agent to re-route the paths. The VRescuer was implemented in Unity3D with an Oculus Rift device, and it was assessed by twenty users to improve the proposed application.",https://ieeexplore.ieee.org/document/8942307/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2012.6180897,Validation of virtual humanoid intelligent agents in virtual reality systems,IEEE,Conferences,"One of the great benefits VR systems offer is their ability to simulate a number of virtual humans when their presence is needed in the context of some learning or training experience. Being that the real humans may not be available to play different roles and support virtual sessions, the ability of a system to generate highly believable representations of autonomous virtual humans - virtual intelligent agents - is vital in achieving specific learning and training objectives. Eliminating the elements of the system that can cause a negative learning and training transfer is a paramount in those systems. We illustrate the results of two user studies focused on validation of non-deterministic domain-specific behaviors generated by our system (example: behaviors typical for a well coordinated group of paramedics or military unit). The results and observations confirmed that when it comes to VR systems with stringent requirements and high expectations for positive learning/training transfer, we still need humans to evaluate and validate synthesized human-like agent behaviors.",https://ieeexplore.ieee.org/document/6180897/,2012 IEEE Virtual Reality Workshops (VRW),4-8 March 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00029,ViRe-in2-GP-Methodology for Virtual Reality in Major Industrial and Infrastructural Projects,IEEE,Conferences,"Even though virtual reality (VR) technologies promise to boost efficiency because their three-dimensionality, virtuality and interactivity and enable simple and objectified access to object and process information representations, the design and implementation of VR systems that support major industrial and infrastructure project planning is largely case-based, permitting virtually no systematic improvement. The ViRe-in2 -methodology presented combines a structured, iterative process for VR planning of major industrial and infrastructure projects with the integrability of established, domain-specific methods and inherent cycles of self-optimization. The complexity of the requirements in this challenging domain is already being met systematically with effectively employed VR approaches.",https://ieeexplore.ieee.org/document/9644387/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIRPHARO52252.2021.9571060,Virtual Reality Simulation of Autonomous Solar Plants Inspections with Unmanned Aerial Systems,IEEE,Conferences,"This paper presents the development of a virtual reality simulation environment for Unmanned Aerial Systems (UAS) solar plant inspection. The objective of this work is to provide a tool to test autonomous inspection and computer vision algorithms and generate realistic synthetic data for deep learning. These techniques demand realistic synthetic data, which can be made available by high-quality graphics engines, such as the ones used for game development. In this work, Unreal Engine 4 is used to host the virtual solar plant. The solar panels were modeled using Blender and Photoshop. Microsoft's AirSim plugin is used to simulate the UAS motion, together with the ArduPilot Software-In- The-Loop flight controller. The environment was evaluated through a virtual autonomous inspection of a plant with 9200 panels, where a georeferencing algorithm was used to locate the defective solar panel in a raster plant layout, based on the pixel position of the defects in the aerial images. The virtual inspection resulted on more than 1000 images and the localization of the defective panels in the layout plant using the georeferencing algorithm had an error of 0.34 meters on the North axis and 0.26 meters on the East axis, which is acceptable for large solar plants with sparse modules' arrangement.",https://ieeexplore.ieee.org/document/9571060/,2021 Aerial Robotic Systems Physically Interacting with the Environment (AIRPHARO),4-5 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8798186,Virtual Reality and Photogrammetry for Improved Reproducibility of Human-Robot Interaction Studies,IEEE,Conferences,"Collecting data in robotics, especially human-robot interactions, traditionally requires a physical robot in a prepared environment, that presents substantial scalability challenges. First, robots provide many possible points of system failure, while the availability of human participants is limited. Second, for tasks such as language learning, it is important to create environments that provide interesting' varied use cases. Traditionally, this requires prepared physical spaces for each scenario being studied. Finally, the expense associated with acquiring robots and preparing spaces places serious limitations on the reproducible quality of experiments. We therefore propose a novel mechanism for using virtual reality to simulate robotic sensor data in a series of prepared scenarios. This allows for a reproducible dataset that other labs can recreate using commodity VR hardware. We demonstrate the effectiveness of this approach with an implementation that includes a simulated physical context, a reconstruction of a human actor, and a reconstruction of a robot. This evaluation shows that even a simple “sandbox” environment allows us to simulate robot sensor data, as well as the movement (e.g., view-port) and speech of humans interacting with the robot in a prescribed scenario.",https://ieeexplore.ieee.org/document/8798186/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAAIC53929.2022.9793007,Virtual Reality in Construction Design,IEEE,Conferences,"This paper is about visualizing the 2D paper planned construction design into an immersive 3D using virtual Reality Technology. The client/customer would experience their planned construction before they start to build. The Simulation of construction is to built using Unity 3D engine and C# scripts to make the environment more realistic. By using VR headset, the constructional scenes of the building can be viewed and with the help of the hand controllers the navigation can be done. This method helps to convince the client/customer with high level of satisfaction towards their constructional design. The 3D virtual visualizing is a never before experience for the clients who dreams to build their construction more precisely. Our primary objective is to implement Virtual reality in construction sector in more efficient way as possible by replacing the traditional methods which is prone to error and still fails in some aspects of transferring the idea between builders and clients thus leading to post construction demolition by unsatisfactory reason. The Virtual reality helps in understanding, analyzing and most importantly it upgrades the level of communication between the builders and clients. This helps a lot in eradicating the losses due to unsatisfactory reasons and as it can be implemented in various phases during construction, it can be implemented in individual phase to track the progress and resolve errors in a compound interval course of the construction/architecture being built.",https://ieeexplore.ieee.org/document/9793007/,2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC),9-11 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAICT52856.2021.9532564,Virtual Reality in Halal Tourism: The Role of System Quality and Content Quality,IEEE,Conferences,"This paper aims to examine the roles of Virtual Reality (VR) system quality and VR content quality in affecting satisfaction and loyalty toward VR among Muslim tourists. The data were gathered from 282 Muslim tourists from various countries who visit tourist destinations in non-organization of Islamic Countries (OIC) countries via VR. The data were collected using Qualtrics Software and the M-Turk Survey application by generating self-administered questionnaires. Partial Least Square Modeling software was used to test the hypotheses. The results indicate that only the quality of VR content gives a direct impact on tourist loyalty. However, tourist satisfaction is influenced by the quality of both VR system and content. This study highlights the key role of VR system quality to enable delivering high content quality, providing satisfaction, and generating loyalty among Muslim tourists. It also deepens our knowledge of the role of the Muslim tourist experience in VR tourism and provides practitioners with insights to develop strategies in order to build and maintain Muslim tourist loyalty through VR.",https://ieeexplore.ieee.org/document/9532564/,"2021 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",27-28 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC51239.2020.9357281,VirtualPT: Virtual Reality based Home Care Physiotherapy Rehabilitation for Elderly,IEEE,Conferences,"This paper describes the development of Personal computer based Virtual Reality home-care Physiotherapy system aimed for rehabilitating full body function in elders. VirtualPT is a true virtual reality platform where the environment is completely replaced by a virtual reality platform based on the mental condition of the person at the time. While doing the home-based prescribed physiotherapy exercises, the key health metrics are continuously monitored and tracked by combining the immersive Virtual Reality with the wearable VirtualPT Sensor kit. Virtual Reality combined with 3D motion capture lets real time movements to be accurately translated onto the virtual reality avatar that can be viewed in a virtual environment to assist physiotherapist to add exercises to the system easily. This ultimate virtual reality Physiotherapy assistant avatar is used to provide guidance to elders at home, to demonstrate and assist elders in adhering to the prescribed exercises. As a significant aspect of social interactions, mirroring of movements has been added to focus on whether the elder is able to accurately follow the movements of avatar. Furthermore, the insightful dashboard offers the elders and physiotherapists an interactive platform through virtual reality capabilities. VirtualPT physiotherapy system is cost effective and makes recovery and more convenient to elders at home while the participatory and immersive nature of Virtual Reality offers a unique realistic quality that is not generally existing in clinical-based physiotherapy. When looking at the broader concept of VirtualPT; continuity of care, integration of services, quality of life and access are equally important criteria which add more value.",https://ieeexplore.ieee.org/document/9357281/,2020 2nd International Conference on Advancements in Computing (ICAC),10-11 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00010,Web-Based Virtual Reality Development in Classroom: From Learner's Perspectives,IEEE,Conferences,"Virtual Reality (VR) content development tools are in continuous production by both enthusiastic researchers and software development companies. Yet, learners could benefit from participating in this development, not only for learning vital programming skills, but also skills in creativity and collaboration. Web-based VR (WebVR) has emerged as a platform-independent framework that permits individuals (with little to no prior programming experience) to create immersive and interactive VR applications. Yet, the success of WebVR relies on students' technological acceptance, the intersectionality of perceived utility and ease of use. In order to determine the effectiveness of the emerging tool for learners of varied experience levels, this paper presents a case study of 38 students who were tasked with developing WebVR 'dream' houses. Results showed that students were accepting of the technology by not only learning and implementing WebVR in a short time (one month), but were also capable of demonstrating creativity and problem-solving skills with classroom supports (i.e., pre-project presentations, online discussions, exemplary projects, and TA support). Results as well as recommendations, lessons learned, and further research are addressed.",https://ieeexplore.ieee.org/document/8613629/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/iLRN47897.2020.9155208,Work-in-Progress—Utilizing Virtual Reality to Promote Active Learning in Construction Management,IEEE,Conferences,"This work in progress paper discusses a Virtual Reality system dubbed Virtual Reality Construction Site Safety Simulator (VR-C3S). Within VR-C3S, students can assess the safety of a virtually simulated construction site by utilizing measurement tools available in the simulation. VR-C3S provides a hands-on learning experience to Construction Management students and involves them in active learning. VR-C3S was deployed in Oculus Go and Windows environment. A user evaluation study was conducted to determine how the application performs when compared to traditional learning methods.",https://ieeexplore.ieee.org/document/9155208/,2020 6th International Conference of the Immersive Learning Research Network (iLRN),21-25 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRW50115.2020.00132,"[DC] Quality, Presence, and Emotions in Virtual Reality Communications",IEEE,Conferences,"This doctoral thesis looks for the identification and evaluation of the factors that allow to improve the QoE of a remote client in telepresence and virtual reality scenarios. Specifically, quality and socioemotional concepts such as social and spatial presence, empathy, and emotions of being in a completely different place, as well as communicate and interact with people who are in that place. The main goals of my research are the analysis of the methodologies to evaluate video quality and socioemotional concepts, the implementation of additional tools using ML techniques to improve the QoE, and finally, experiments in real use cases.",https://ieeexplore.ieee.org/document/9090552/,2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW),22-26 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AI-CSP52968.2021.9671151,i-DERASSA: e-learning Platform based on Augmented and Virtual Reality interaction for Education and Training,IEEE,Conferences,"With the Coronavirus 2019 disease (COVID-19) spread, causing a world pandemic, e-learning can provide an optimal education and training solution. COVID-19 has wholly disrupted the education system. Switching to e-learning could be the enabler to create a new, more effective method of educating students when correctly applied. This paper proposes an elearning platform based on 3D interaction using augmented reality (AR) and virtual reality (VR) designed to meet particular learning objectives divided into levels and subjects, which aims to facilitate the teaching process and administrative workload in schools. We provide details on the platform, VR and AR courses/exercises conception and implementation. Many concepts may be hard to explain in a classroom or visualise in a textbook, e.g., anatomical concepts, molecular structures, space phenomena, or complex abstract topics. The AR and VR technologies make it much easier to achieve, creating a rich, interactive experience that combines real and virtual worlds. VR that provides information through realistic 3D models in immersive environments can better present concepts and skills. Our main contribution is the integration of AR and VR interaction in the web; we develop courses/ exercises selected from the Algerian Ministry of Education and Teaching learning program based on 3D interaction in virtual and augmented scenes. We hope by the integration of VR and AR into education attempts to increase the level of participation and maybe even the understanding of abstract and complex concepts.",https://ieeexplore.ieee.org/document/9671151/,2021 International Conference on Artificial Intelligence for Cyber Security Systems and Privacy (AI-CSP),20-21 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVT.2020.2995877,An Adaptive Wireless Virtual Reality Framework in Future Wireless Networks: A Distributed Learning Approach,IEEE,Journals,"Wireless virtual reality (VR) is predicted to become a killer application in 5G and beyond, which provides an immersive experience and revolutionizes the way people communicate. It is well-known that rendering is the key performance bottleneck in wireless VR systems, especially for VR games. However, real-time rendering and data correlation are ignored by most researchers. In this paper, we propose an adaptive VR framework that enables high-quality wireless VR in future mmWave-enabled wireless networks with mobile edge computing (MEC), where real-time VR rendering tasks can be offloaded to MEC servers adaptively and the caching capability of MEC servers enables further performance improvement. First, we formulate the addressed problem to maximize the quality of experience (QoE) of the users, where association, adaptive offloading mode selection, and caching policy are jointly optimized. Considering the high complexity of the addressed problem, we then propose a distributed learning approach consisting of an offline training phase and an online running phase, which maintains scalability and adaptation capability. The offline phase is based on deep reinforcement learning (DRL) while the latter utilizes game theory. At last, simulation results show the superiority of the proposed algorithm over the other baseline algorithms in terms of QoE utility values, latency, and convergence time.",https://ieeexplore.ieee.org/document/9097455/,IEEE Transactions on Vehicular Technology,Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.2992283,Application of Internet of Things and Virtual Reality Technology in College Physical Education,IEEE,Journals,"College physical education is an important component of the higher education system and national health plans. Promoting the scientific and modern construction of the physical education teaching system in colleges and universities is conducive to enhancing the science and effectiveness of higher education. Aiming at the problems of single teaching methods and insufficient long-distance teaching ability in the current physical education teaching process of colleges and universities, based on virtual reality technology, this paper designs and proposes a college physical education virtual reality system consisting of the Internet of Things, cloud platform and mobile client. This system collects relevant data from the Internet of Things and interacts with the virtual reality scene in real time, rendering the scene through the cloud and experiencing virtual reality through the mobile terminal. After the requirements analysis and system framework design of the virtual reality system for physical education in colleges and universities, the system's main functions and data warehouse, software architecture design and system testing process were carried out. Through the analysis of specific trial cases and user feedback information of a college, it shows that the designed virtual reality system of college physical education has good application and promotion effect, and provides a scientific reference for deepening reform of college physical education Suggest.",https://ieeexplore.ieee.org/document/9091187/,IEEE Access,2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2021.3079427,Autonomous Endoscope Robot Positioning Using Instrument Segmentation With Virtual Reality Visualization,IEEE,Journals,"This paper presents a method for endoscope's autonomous positioning by a robotic endoscope holder for minimally invasive surgery. The method improves human-robot cooperation in robot-assisted surgery by allowing the endoscope holder to acknowledge the surgeon's view projection and navigate the camera without manual control. The real-time prediction of next desired camera location is estimated using segmented instrument's tip locations from endoscope video and surgeon's attention focus given by tracked virtual reality headset. To tackle the issue of real-time surgical instrument segmentation for more precise instrument tip localization, we propose the YOLOv3 and ResNet Combined Neural Network. The method showed an 86.6% IoU across MICCAI'17 Endovis datasets with 30 frames per second processing speed. The proposed pipeline was implemented in ROS on Ubuntu with visualization running under Windows operating system in Unity3D. The simulation demonstrates the robotic arm, endoscope, and surgical environment visualized in 3D in the virtual reality headset to provide a stable view of the endoscope and improve the surgeon's perception of the operating environment.",https://ieeexplore.ieee.org/document/9429186/,IEEE Access,2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LRA.2017.2737046,Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in Manufacturing,IEEE,Journals,"We demonstrate a low-cost telerobotic system that leverages commercial virtual reality (VR) technology and integrates it with existing robotics control infrastructure. The system runs on a commercial gaming engine using off-the-shelf VR hardware and can be deployed on multiple network architectures. The system is based on the homunculus model of mind wherein we embed the user in a VR control room. The control room allows for multiple sensor displays, and dynamic mapping between the user and robot. This dynamic mapping allows for selective engagement between the user and the robot. We compared our system with state-of-the-art automation algorithms and standard VR-based telepresence systems by performing a user study. The study showed that new users were faster and more accurate than the automation or a direct telepresence system. We also demonstrate that our system can be used for pick and place, assembly, and manufacturing tasks.",https://ieeexplore.ieee.org/document/8003431/,IEEE Robotics and Automation Letters,Jan. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVCG.2021.3067777,Combining Dynamic Passive Haptics and Haptic Retargeting for Enhanced Haptic Feedback in Virtual Reality,IEEE,Journals,"To provide immersive haptic experiences, proxy-based haptic feedback systems for virtual reality (VR) face two central challenges: (1) similarity, and (2) colocation. While to solve challenge (1), physical proxy objects need to be sufficiently similar to their virtual counterparts in terms of haptic properties, for challenge (2), proxies and virtual counterparts need to be sufficiently colocated to allow for seamless interactions. To solve these challenges, past research introduced, among others, two successful techniques: (a) Dynamic Passive Haptic Feedback (DPHF), a hardware-based technique that leverages actuated props adapting their physical state during the VR experience, and (b) Haptic Retargeting, a software-based technique leveraging hand redirection to bridge spatial offsets between real and virtual objects. Both concepts have, up to now, not ever been studied in combination. This paper proposes to combine both techniques and reports on the results of a perceptual and a psychophysical experiment situated in a proof-of-concept scenario focused on the perception of virtual weight distribution. We show that users in VR overestimate weight shifts and that, when DPHF and HR are combined, significantly greater shifts can be rendered, compared to using only a weight-shifting prop or unnoticeable hand redirection. Moreover, we find the combination of DPHF and HR to let significantly larger spatial dislocations of proxy and virtual counterpart go unnoticed by users. Our investigation is the first to show the value of combining DPHF and HR in practice, validating that their combination can better solve the challenges of similarity and colocation than the individual techniques can do alone.",https://ieeexplore.ieee.org/document/9382898/,IEEE Transactions on Visualization and Computer Graphics,May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3022644,Dynamic Visual Communication Image Framing of Graphic Design in a Virtual Reality Environment,IEEE,Journals,"This paper explores dynamic visual communication image framing for graphic design based on virtual reality algorithms; it defines corresponding feature representations by delineating layers of pixels, elements, relationships, planes, and applications; and it investigates methods for quantifying geometric features, perceptual features, and style features. The contents include extraction methods for element colors, calculation methods for layout perceptual features and color-matching perceptual features, and pairwise comparison methods for style features. By overfitting the distribution of geometric features in the data, the model can predict the probability density distribution of features such as element position and color under specific conditions to support the generation of flat images. To construct a prediction model, the sampling method of features, the model optimization method, and the data learning strategy are investigated. This thesis involves the design and implementation of a lossless/near-lossless compression system for high-frame-rate gaze camera image data, which is faced with the technical problems of high fidelity and strong real-time and reliable compression. The image single-frame lossless/near-loss-free compression ratio is generally low, and the compression ratio can be improved by using the correlation between image frames. In this paper, we study the application of lossless compression between image frames, the efficient computing structure of FPGA, and an onboard compression system.",https://ieeexplore.ieee.org/document/9187835/,IEEE Access,2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2019.2937937,Enhanced Player Interaction Using Motion Controllers for First-Person Shooting Games in Virtual Reality,IEEE,Journals,"The main purpose of virtual reality (VR) is to enhance realism and the player experience. To do this, we focus on VR interaction design methods, analyze the existing interaction solutions including both accurate and rough interaction methods, and propose a new method for creating stable and realistic player interactions in a first-person shooter (FPS) game prototype. In this research, we design and modify the existing mapping methods between physical and virtual worlds, and create interfaces such that physical devices correspond to shooting tools in virtual reality. Moreover, we propose and design prototypes of universal interactions that can be implemented in a simple and straightforward way. Proposed interactions allow the player to perform actions similar to those of real shooting, using both hands such as firing, reloading, attaching and grabbing objects. In addition, we develop a gun template with haptic feedback, and a visual collision guide that can optionally be enabled. Then, we evaluate and compare our methods with the existing solutions. We then use these in a VR FPS game prototype and conduct a user study with participants, and the resulting user study proves that the proposed method is more stable, player-friendly and realistic.",https://ieeexplore.ieee.org/document/8817959/,IEEE Access,2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JSAC.2021.3118405,"Learning-Based Prediction, Rendering and Transmission for Interactive Virtual Reality in RIS-Assisted Terahertz Networks",IEEE,Journals,"The quality of experience (QoE) requirements of wireless virtual reality (VR) can only be satisfied with high data rate, high reliability, and low VR interaction latency. This high data rate over short transmission distances may be achieved via the abundant bandwidth in the terahertz (THz) band. However, THz waves experience severe signal attenuation, which may be compensated by the reconfigurable intelligent surface (RIS) technology with programmable reflecting elements. Meanwhile, the low VR interaction latency can be achieved with the mobile edge computing (MEC) network architecture due to its computation capabilities. Motivated by these considerations, in this paper, we propose an MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by taking into account the uplink viewpoint prediction and position transmission, the MEC rendering, and the downlink transmission. We propose two methods, which are referred to as centralized online gated recurrent unit (GRU) and distributed federated averaging (FedAvg), to predict the viewpoints of the VR users. In the uplink, an algorithm that integrates online long-short term memory (LSTM) and convolutional neural networks (CNN) is deployed to predict the locations and the line-of-sight and non-line-of-sight statuses of the VR users over time. In the downlink, we develop a constrained deep reinforcement learning algorithm to select the optimal phase shifts of the RIS under latency constraints. Simulation results show that our proposed learning architecture achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and about two times improvement in QoE compared to the random phase shift selection scheme.",https://ieeexplore.ieee.org/document/9565222/,IEEE Journal on Selected Areas in Communications,Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1093/iwc/iwv010,Neuro-Fuzzy Physiological Computing to Assess Stress Levels in Virtual Reality Therapy,OUP,Journals,"This paper reports the design and assessment of a neuro-fuzzy model to support clinicians during virtual reality therapy. The implemented model is able to automatically recognize the perceived stress levels of the patients by analyzing physiological and behavioral data during treatment. The model, consisting of a self-organizing map and a fuzzy-rule-based module, was trained unobtrusively recording electrocardiogram, breath rate and activity during stress inoculation provided by the exposure to virtual environments. Twenty nurses were exposed to sessions simulating typical stressful situations experienced at their workplace. Four levels of stress severity were evaluated for each subject by gold standard clinical scales administered by trained personnel. The model's performances were discussed and compared with the main machine learning algorithms. The neuro-fuzzy model shows better performances in terms of stress level classification with 83% of mean recognition rate.RESEARCH HIGHLIGHTS Stress levels were predicted on the basis of physiological computing using a neuro-fuzzy model during virtual reality therapy. Features were extracted from ECG and respiration obtaining high accuracy and optimization of computational costs. The neuro-fuzzy model shows better performance than the more frequently adopted classifiers. This approach may enhance the use of physiological computing for stress treatment in clinical practice.",https://ieeexplore.ieee.org/document/8155481/,Interacting with Computers,Sept. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVCG.2021.3101854,OctoPocus in VR: Using a Dynamic Guide for 3D Mid-Air Gestures in Virtual Reality,IEEE,Journals,"Bau and Mackays OctoPocus dynamic guide helps novices learn, execute, and remember 2D surface gestures. We adapt OctoPocus to 3D mid-air gestures in Virtual Reality (VR) using an optimization-based recognizer, and by introducing an optional exploration mode to help visualize the spatial complexity of guides in a 3D gesture set. A replication of the original experiment protocol is used to compare OctoPocus in VR with a VR implementation of a crib-sheet. Results show that despite requiring 0.9s more reaction time than crib-sheet, OctoPocus enables participants to execute gestures 1.8s faster with 13.8 percent more accuracy during training, while remembering a comparable number of gestures. Subjective ratings support these results, 75 percent of participants found OctoPocus easier to learn and 83 percent found it more accurate. We contribute an implementation and empirical evidence demonstrating that an adaptation of the OctoPocus guide to VR is feasible and beneficial.",https://ieeexplore.ieee.org/document/9507320/,IEEE Transactions on Visualization and Computer Graphics,1 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVCG.2017.2656978,Shifty: A Weight-Shifting Dynamic Passive Haptic Proxy to Enhance Object Perception in Virtual Reality,IEEE,Journals,"We define the concept of Dynamic Passive Haptic Feedback (DPHF) for virtual reality by introducing the weight-shifting physical DPHF proxy object Shifty. This concept combines actuators known from active haptics and physical proxies known from passive haptics to construct proxies that automatically adapt their passive haptic feedback. We describe the concept behind our ungrounded weight-shifting DPHF proxy Shifty and the implementation of our prototype. We then investigate how Shifty can, by automatically changing its internal weight distribution, enhance the user's perception of virtual objects interacted with in two experiments. In a first experiment, we show that Shifty can enhance the perception of virtual objects changing in shape, especially in length and thickness. Here, Shifty was shown to increase the user's fun and perceived realism significantly, compared to an equivalent passive haptic proxy. In a second experiment, Shifty is used to pick up virtual objects of different virtual weights. The results show that Shifty enhances the perception of weight and thus the perceived realism by adapting its kinesthetic feedback to the picked-up virtual object. In the same experiment, we additionally show that specific combinations of haptic, visual and auditory feedback during the pick-up interaction help to compensate for visual-haptic mismatch perceived during the shifting process.",https://ieeexplore.ieee.org/document/7833030/,IEEE Transactions on Visualization and Computer Graphics,April 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TVCG.2022.3150486,Stereopsis Only: Validation of a Monocular Depth Cues Reduced Gamified Virtual Reality with Reaction Time Measurement,IEEE,Journals,"The visual depth perception is composed of monocular and binocular depth cues. Studies show that in absence of binocular depth cues the performance of visuomotor tasks like pointing to or grasping objects is limited. Thus, binocular depth cues are of great importance for motor control required in everyday life. However, binocular depth cues like retinal disparity (basis for stereopsis) might be influenced due to developmental disorders of the visual system. For example, amblyopia in which one eye&#x0027;s visual input is not processed leads to loss of stereopsis. The primary amblyopia treatment is occlusion of the healthy eye to force the amblyopic eye to train. However, improvements in stereopsis are poor. Therefore, binocular treatments arose that equilibrate both eyes&#x0027; visual input to enable binocular vision. However, most approaches rely on divided stimuli which do not account for loss of stereopsis. We created a Virtual Reality (VR) with reduced monocular depth cues in which a stereoscopic task is shown to both eyes simultaneously, consisting of two balls jumping towards the user. One ball appears closer to the user which must be identified. To evaluate the task performance the reaction time is measured. We validated our approach with 18 participants with stereopsis under three contrast settings including one leading to monocular vision. The number of correct responses reduces from 90&#x0025; under binocular vision to 52&#x0025; under monocular vision corresponding to random guessing. Our results indicate that it is possible to disable monocular depth cues and create a dynamic stereoscopic task inside a VR.",https://ieeexplore.ieee.org/document/9714042/,IEEE Transactions on Visualization and Computer Graphics,May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2022.3186318,Stress Level Estimation Based on Physiological Signals for Virtual Reality Applications,IEEE,Journals,"Virtual Reality (VR) video games have become popular in recent years with the emergence of a wide variety of enhanced VR hardware systems. Typically, current VR video games incorporate high-quality auditory and video feedback along with vibro-tactile cues to provide an immersive experience. However, the ongoing VR hardware and software do not consider the state of the user to determine whether the video game is generating an enjoyable experience, or if it has become a stressing or boring experience. This work provides an assessment of stress level estimation from features extracted from Electrocardiography (ECG), Electrodermal Activity (EDA), and Electromyography (EMG) signals of users while playing a VR video game with different difficulty levels. Statistical differences were found between the rest and gaming stages for several extracted features. Regardless of the fact that no significant statistical differences between the three levels of difficulty were found by analyzing the EDA and ECG features, an 83.1&#x0025; accuracy was obtained for the classification between the three levels with a KNN model. For the EMG signal, the obtained accuracy ranged between 99&#x0025; and 100&#x0025; for the distinction between a difficult level 1 and rest stage for all models. When all features from the ECG, EDA and EMG signals were used, an accuracy of 99&#x0025; was obtained for the differentiation between the three difficulty levels and a resting stage. The presented work results may serve as a reference for future work regarding feature extraction and game difficulty adaptation for user experience enhancement.",https://ieeexplore.ieee.org/document/9807299/,IEEE Access,2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TNSRE.2022.3183686,The Effects of a Virtual Reality Rehabilitation Task on Elderly Subjects: An Experimental Study Using Multimodal Data,IEEE,Journals,"Ageing populations are becoming a global issue. Against this background, the assessment and treatment of geriatric conditions have become increasingly important. This study draws on the multisensory integration of virtual reality (VR) devices in the field of rehabilitation to assess brain function in young and old people. The study is based on multimodal data generated by combining high temporal resolution electroencephalogram (EEG) and subjective scales and behavioural indicators reflecting motor abilities. The phase locking value (PLV) was chosen as an indicator of functional connectivity (FC), and six brain regions, namely LPFC, RPFC, LOL, ROL, LMC and RMC, were analysed. The results showed a significant difference in the alpha band on comparing the resting and task states in the younger group. A significant difference between the two states in the alpha and beta bands was observed when comparing task states in the younger and older groups. Meanwhile, this study affirms that advancing age significantly affects human locomotor performance and also has a correlation with cognitive level. The study proposes a novel accurate and valid assessment method that offers new possibilities for assessing and rehabilitating geriatric diseases. Thus, this method has the potential to contribute to the field of rehabilitation medicine.",https://ieeexplore.ieee.org/document/9797783/,IEEE Transactions on Neural Systems and Rehabilitation Engineering,2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3008165,Using EEG and Deep Learning to Predict Motion Sickness Under Wearing a Virtual Reality Device,IEEE,Journals,"Virtual Reality (VR) research has been widely applied in many fields. VR promises to deliver the experience that is beyond the user's imagination. One of the advantages of VR is the feeling it gives of being there. VR can provide experiences impossible in the real world, such as flying, diving in deep water, exploring outer space, or living with dinosaurs. Despite the improvements in the software and hardware, the problem of motion sickness remains. We implement a deep learning model to train and predict motion sickness. A questionnaire is a well-known method to measure motion sickness. The weakness of the questionnaire is the measurement carried out after the user experiences motion sickness symptoms. By using the deep learning and EEG, the system will learn and classify motion sickness. The system learns the user's EEG pattern when they begin to feel the sickness symptoms. The system will be trained using deep learning to identify the sickness patterns in the future. By the EEG patterns, the system can predict the sickness symptoms before it occurs. Our model outperforms traditional models in loss values, accuracy, and F-measure metrics in Roller Coaster. With other datasets, our model also performs well. Our model can achieve 82.83% accuracy from the dataset. We also found that the time steps to predict motion sickness during 5 minute periods is a suitable configuration.",https://ieeexplore.ieee.org/document/9137130/,IEEE Access,2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2020.3012207,Value of Virtual Reality Technology in Image Inspection and 3D Geometric Modeling,IEEE,Journals,"Aiming at the poor expressive ability of image statistical information during the reconstruction process of traditional 3D image reconstruction method based on virtual reality technology, resulting in low accuracy of 3D image after reconstruction, a new image detection and 3D image reconstruction based on virtual reality technology are studied method. This paper first proposed a new two-level cascade convolutional neural network structure. The first level of the network predicts target positioning based on the image-level labels of the training image, generates a bounding box of the target in the original image, and generates a cropped image. The cropped image is input to the second-level network. The cropped image may contain areas where the target is stuck in the original image. Level 2 networks only use the adhesion area as training data. Secondly, the visualization software development platform and virtual reality 3D image processing software are selected as the platform for 3D image reconstruction. After the original image is imported into the computer through data input and file analysis steps, the original image is detected. The detected image is in the virtual in the real software, the bounding box method is first used to construct the three-dimensional data field of image reconstruction, and the three-dimensional direct volume of the image is drawn according to the three-dimensional data field of image reconstruction. Preferably, the three-dimensional image reconstruction output formula is obtained through the three-dimensional image direct volume to realize the three-dimensional image reconstruction based on the virtual reality technology. The simulation results show that the method proposed in this paper can effectively detect images. The average traversal coverage of 3D image reconstruction is up to 0.979, and the reconstruction accuracy is higher than 0.97.",https://ieeexplore.ieee.org/document/9149908/,IEEE Access,2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/70.508435,Virtual-reality-based point-and-direct robotic inspection in manufacturing,IEEE,Journals,"This paper explores a flexible manufacturing paradigm in which robot grasping is interactively specified and skeletal images are efficiently used in combination to allow rapidly setting up surface flaw identification tasks in small-quantity/large-variety manufacturing. Two complementary technologies are combined to make implementation of inspection as rapid as possible. First, a novel material handling approach is described for robotic picking and placing of parts onto an inspection table using virtual tools. This allows an operator to point and give directives to set up robotic inspection tasks. Second, since specification may be approximate using this method, a fast and flexible means of identifying images of perfect and flawed parts is explored that avoids rotational or translational restrictions on workpiece placement. This is accomplished by using skeleton pixel counts as neural network inputs. The total system, including material handling and skeleton-based inspection, features flexibility during manufacturing set-up, and reduces the process time and memory requirements for workpiece inspection.",https://ieeexplore.ieee.org/document/508435/,IEEE Transactions on Robotics and Automation,Aug. 1996,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCESS.2018.2832089,Visual Appearance Modulates Prediction Error in Virtual Reality,IEEE,Journals,"Different rendering styles induce different levels of agency and user behaviors in virtual reality environments. We applied an electroencephalogram-based approach to investigate how the rendering style of the users' hands affects behavioral and cognitive responses. To this end, we introduced prediction errors due to cognitive conflicts during a 3-D object selection task by manipulating the selection distance of the target object. The results showed that, for participants with high behavioral inhibition scores, the amplitude of the negative event-related potential at approximately 50-250 ms correlated with the realism of the virtual hands. Concurring with the uncanny valley theory, these findings suggest that the more realistic the representation of the user's hand is, the more sensitive the user becomes toward subtle errors, such as tracking inaccuracies.",https://ieeexplore.ieee.org/document/8355496/,IEEE Access,2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CGIV.2006.2,3D Archaeological Reconstruction and Visualisation: An Artificial Life Model for Determining Vegetation Dispersal Patterns in Ancient Landscapes,IEEE,Conferences,"This paper describes a methodology and software engine for generating dynamic vegetation models for archaeological reconstruction and interactive visualisation, integrating the disciplines of artificial life (Alife) and virtual reality. The engine, based on the concept of emergence (a phenomenon in complex Alife systems), uses real botanical parameters, channelled through simple rules, in order to synthesise the dispersal patterns of natural vegetation communities as they grow, reproduce, and compete for resources. The foci for the development and evaluation of the Alife engine described relate to different scenarios in nature as may have existed during the Mesolithic period. Results from the study showed evidence of correlations between the artificial vegetation and their natural counterparts, demonstrating the feasibility of using such models in historical landscape reconstructions",https://ieeexplore.ieee.org/document/1663776/,"International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)",26-28 July 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESS49830.2020.9301562,3D Hand Pose Estimation from Single Depth Images with Label Distribution Learning,IEEE,Conferences,"Reliable hand pose estimation enriches the way of human-computer interaction, such as sign language recognition and virtual reality. However, the task of estimating the hand pose faces two severe challenges. To be specific, it is difficult to learn spatial information from a 2D image and regress the location of a point in 3D space. And the highly non-linear correlation between the hand feature space and the joint location makes it hard to be modeled. To deal with the above problems, we propose a deep regression network, which learns the hand feature space from the point cloud and includes a specific label distribution learning network. Due to the point cloud contains more spatial information, it is beneficial for the neural network to extract the hand spatial geometric features. Utilizing the deep network to guide label learning actively reduces the negative effects of nonlinearity. According to the experimental results, our proposed network achieves the state-of-the-art performance on MSRA dataset.",https://ieeexplore.ieee.org/document/9301562/,2020 IEEE International Conference on Embedded Software and Systems (ICESS),10-11 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671898,3D Modeling of Cities for Virtual Environments,IEEE,Conferences,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display technologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments and typically model the scene as meshes. In this paper, we develop an end-to-end workflow for creating urban scale real world accurate synthetic environments that can be visualized in virtual environments including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction. Four meshing algorithms are evaluated for representation accuracy and city-scale meshes imported into Unity for assessing the quality of the immersive experience.",https://ieeexplore.ieee.org/document/9671898/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VIMS.2002.1009367,"3D virtual ""smart home"" user interface",IEEE,Conferences,"In contrast to the rapid development of home automation equipment and 'smart home' capabilities, comparatively less attention has been paid to the development of comprehensive, comfortable and self-explaining user interfaces. This is acknowledged to be an important obstacle for the broad success of smart home ideas and products. In order to make home automation desirable and economically feasible, it is vital to improve the attractivity, intuitiveness and adaptivity of the user-home interaction taking advantage from modern information technologies such as Virtual Reality technology and downloadable hypertext documents. The user interface is augmented by the use of a virtual environment in conjunction with the physical environment adding information from different systems and sensors in the home, based on virtual environment activities. In this paper we describe a powerful graphical interface created with standard 3D programming tools to control and supervise the household. A number of features implemented in a prototype and implementation aspects will be described.",https://ieeexplore.ieee.org/document/1009367/,2002 IEEE International Symposium on Virtual and Intelligent Measurement Systems (IEEE Cat. No.02EX545),19-20 May 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SERA.2007.3,"5th ACIS International Conference on Software Engineering Research, Management & Applications-Title",IEEE,Conferences,"The following topics were dealt with: software architecture; communication systems; communication networks, and telecommunications; Web engineering; advanced Internet technology; artificial intelligence; formal methods; data mining; knowledge recovery; component-based software engineering; reliability monitoring; ubiquitous IT applications; reverse engineering; information systems engineering; management systems; mobile computing; wireless computing; parallel computing; distributed computing; healthcare engineering; computer animation; computer game development; virtual reality; and computer graphics.",https://ieeexplore.ieee.org/document/4296901/,"5th ACIS International Conference on Software Engineering Research, Management & Applications (SERA 2007)",20-22 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCC.2018.8310258,A 16Gb/s/pin 8Gb GDDR6 DRAM with bandwidth extension techniques for high-speed applications,IEEE,Conferences,"Recently the demand for high-bandwidth graphic DRAM, for game consoles and graphic cards, has dramatically increased due to the development of virtual reality, artificial intelligence, deep learning, autonomous driving cars, etc. These applications require greater data transfer speeds than pervious devices, GDDR5 [1] and GDDR5X [2], which are limited to 12Gb/s/pin. This paper introduces an 8Gb GDDR6 operating at up to 16Gb/s/pin. To exceed the prior speed limit various bandwidth extension techniques are proposed. WCK is driven with a dividing scheme to overcome speed limitations and to reduce power consumption. In addition, a dual-band architecture with different types of nibble drivers is proposed in order to cover stability of CML-to-CMOS in all frequency regions; CML nibble is used for high-speed, while CMOS nibble is used for low-speed. A DC-split scheme is implemented for duty-cycle correction and skew compensation. The bandwidth of the high-frequency divider is extended by using a proposed mode-changed flip-flop. The receiver uses a loop-unrolled one-tap decision-feedback equalizer (DFE) designed to eliminate channel inter-symbol interference (ISI). A two-stage pre-amplifier is also used for bandwidth extension. The transmitter uses a 4:1 multiplexer using a half-rate sampler, where a 1UI pulse is unnecessary to minimize the full-rate operation. To secure on-chip signal transmission characteristic, the bandwidth limitation of transistor in a DRAM process is extended by adopting an on-chip feedback EQ filter.",https://ieeexplore.ieee.org/document/8310258/,2018 IEEE International Solid - State Circuits Conference - (ISSCC),11-15 Feb. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSCC42614.2022.9731653,A 78.8fJ/b/mm 12.0Gb/s/Wire Capacitively Driven On-Chip Link Over 5.6mm with an FFE-Combined Ground-Forcing Biasing Technique for DRAM Global Bus Line in 65nm CMOS,IEEE,Conferences,"Advances in virtual reality, artificial intelligence, and big data have increased demand for high-bandwidth memory. Accordingly, pre-fetch sizes have also increased with DRAM generations, meaning an increased number of global bus lines. An increase to this number is limited as it also increases the chip size; instead, the data-rate per lane can be increased for higher throughput [1]. As the global bus lines are on-chip wires in a DRAM chip, they can be driven capacitively. Prior work [2], [3] has shown the superior efficiency of capacitive drivers, over conventional repeaters, in driving on-chip wires at the cost of a reduced voltage swing. However, as there is no well-defined DC level on the capacitively-driven wires [4], wire biasing is fraught with implementation challenges [3]. To define the DC potential on the interconnect, prior work sent signals differentially [2], [4], [5] or dissipated static power to define the DC level [3]. Unfortunately, these approaches may not be preferable for DRAM chips that require dense and energy-efficient data transfers.",https://ieeexplore.ieee.org/document/9731653/,2022 IEEE International Solid- State Circuits Conference (ISSCC),20-26 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00028,A Benchmark of Four Methods for Generating 360° Saliency Maps from Eye Tracking Data,IEEE,Conferences,"Modeling and visualization of user attention in Virtual Reality is important for many applications, such as gaze prediction, robotics, retargeting, video compression, and rendering. Several methods have been proposed to model eye tracking data as saliency maps. We benchmark the performance of four such methods for 360° images. We provide a comprehensive analysis and implementations of these methods to assist researchers and practitioners. Finally, we make recommendations based on our benchmark analyses and the ease of implementation.",https://ieeexplore.ieee.org/document/8613647/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MICRO50266.2020.00076,A Benchmarking Framework for Interactive 3D Applications in the Cloud,IEEE,Conferences,"With the growing popularity of cloud gaming and cloud virtual reality (VR), interactive 3D applications have become a major class of workloads for the cloud. However, despite their growing importance, there is limited public research on how to design cloud systems to efficiently support these applications due to the lack of an open and reliable research infrastructure, including benchmarks and performance analysis tools. The challenges of generating human-like inputs under various system/application nondeterminism and dissecting the performance of complex graphics systems make it very difficult to design such an infrastructure. In this paper, we present the design of a novel research infrastructure, Pictor, for cloud 3D applications and systems. Pictor employs AI to mimic human interactions with complex 3D applications. It can also track the processing of user inputs to provide in-depth performance measurements for the complex software and hardware stack used for cloud 3D-graphics rendering. With Pictor, we designed a benchmark suite with six interactive 3D applications. Performance analyses were conducted with these benchmarks, which show that cloud system designs, including both system software and hardware designs, are crucial to the performance of cloud 3D applications. The analyses also show that energy consumption can be reduced by at least 37% when two 3D applications share a could server. To demonstrate the effectiveness of Pictor, we also implemented two optimizations to address two performance bottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering system. These two optimizations improved the frame rate by 57.7% on average.",https://ieeexplore.ieee.org/document/9251945/,2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO),17-21 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.0916,A DVGE service system for risk assessment of dam-break in barrier lake,IET,Conferences,"This paper employs theories and technologies of the distributed virtual reality and geographic information system (GIS) to construct a DVGE (Distributed Virtual Geographic Environment) system. The proposed DVGE System provides geographically distributed users with a shared virtual space and a collaborative platform in order to implement risk assessment work. Using five-layer service system architecture efficiently integrates and shares geographically distributed resources as well as modeling procedures. Meanwhile some key technologies including distributed virtual scene modelling and implementing mechanism of collaborative workflow are discussed. Finally, a DVGE prototype system is implemented to support risk assessment and impact analysis of dam-break in Barrier Lake. The experimental results show that the scheme developed in this paper is efficient and feasible.",https://ieeexplore.ieee.org/document/6492523/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACHI.2010.29,A Game-Based 3D Simulation of Otranto in the Middle Ages,IEEE,Conferences,"In educational sense, the virtual reality shows its value when the user can actively participate in the creation and development of his knowledge. According to the MediaEvo Project and with the multiplayer educational game realized, the paper shows that the entertainment games platforms can also be used to develop platforms for multi-channel and multi-sensory cultural edutainment. Herein we present the process for collecting and processing data, the methodology and the tools used in the work and the multi-playing and artificial intelligence models implemented in the project. At the moment the MediaEvo Project is working in progress.",https://ieeexplore.ieee.org/document/5430112/,2010 Third International Conference on Advances in Computer-Human Interactions,10-15 Feb. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD51958.2021.9643557,A General Hardware and Software Co-Design Framework for Energy-Efficient Edge AI,IEEE,Conferences,"A huge number of edge applications including self-driving cars, mobile health, robotics, and augmented reality / virtual reality are enabled by deep neural networks (DNNs). Currently, much of this computation for these applications happens in the cloud, but there are several good reasons to perform the processing on local edge platforms such as smartphones: improved accessibility to different parts of the world, low latency, and data privacy. In this paper, we present a general hardware and software co-design framework for energy-efficient edge AI for both simple classification and structured output prediction tasks (e.g., 3D shapes from images). This framework relies on two key ideas. First, we design a space of DNNs of increasing complexity (coarse to fine) and perform input-specific adaptive inference by selecting a DNN of appropriate complexity depending on the hardness of input examples. Second, we execute the selected DNN on the target edge platform using a resource management policy to save energy. We also provide instantiations of our co-design framework for three qualitatively different problem settings: convolutional neural networks for image classification, graph convolutional networks for predicting 3D shapes from images, and generative adversarial networks on photo-realistic unconditional image generation. Our experiments on real-world benchmarks and mobile platforms show the effectiveness of our co-design framework in achieving significant gain in energy with little to no loss in accuracy of predictions.",https://ieeexplore.ieee.org/document/9643557/,2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD),1-4 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2012.71,A Group Oriented Intelligent Tutoring System For Rocket Engineers and Researchers,IEEE,Conferences,"An architecture of a group oriented intelligent tutoring system for rocket engineers and researchers is proposed, which combines the technologies of multi-agent and virtual reality with HLA. At the same time, a group oriented joint intention is introduced into the description of multi-agent cooperation in order to teach or guide trainers/beginners to learn right operation procedures or resolve faults. Then, an intelligent tutoring system for measuring and controlling rocket launch is implemented to verify the proposed method, which operates well at Jiuquan Satellite Launch Center (JSLC).",https://ieeexplore.ieee.org/document/6268024/,2012 IEEE 12th International Conference on Advanced Learning Technologies,4-6 July 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AINA.2010.112,A Japanese Calligraphy Trainer Based on Skill Acquisition Through Haptization,IEEE,Conferences,"We present an approach for implementing a virtual reality system targeted at learning handwritten characters. We especially aim at enabling learners to acquire important writing skills required to be a good writer in Japanese calligraphy. The proposed system provides a haptic channel allowing the learners to intuitively master an instructor's fine motor skills through the sense of touch. We utilize a commercially available haptic device called PHANTOMTM for simulating a writing brush in a virtual learning space. The system implements a function for recording and replaying the instructor's hand motions via the PAHNTOM device. The instructor's writing techniques such as brush-strokes and pen pressures he/she performs/adds when writing characters are effectively presented to the learners via the PHANTOM device. Then, they can master how to write the recorded characters with feeling the instructor's style of handwritings. We also invented a simple yet powerful 3D brush model for real-time visualization of handwritten characters without compromising the quality and reality of the characters. The users can start learning at any time and iterate training without worrying about resource consumptions such as papers and ink as much as they like. We conducted an experiment to validate the effectiveness of the proposed system for learning calligraphy in a virtual environment.",https://ieeexplore.ieee.org/document/5474852/,2010 24th IEEE International Conference on Advanced Information Networking and Applications,20-23 April 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196677,A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in Homes,IEEE,Conferences,"We describe a mobile manipulation hardware and software system capable of autonomously performing complex human-level tasks in real homes, after being taught the task with a single demonstration from a person in virtual reality. This is enabled by a highly capable mobile manipulation robot, whole-body task space hybrid position/force control, teaching of parameterized primitives linked to a robust learned dense visual embeddings representation of the scene, and a task graph of the taught behaviors. We demonstrate the robustness of the approach by presenting results for performing a variety of tasks, under different environmental conditions, in multiple real homes. Our approach achieves 85% overall success rate on three tasks that consist of an average of 45 behaviors each. The video is available at: https://youtu.be/HSyAGMGikLk.",https://ieeexplore.ieee.org/document/9196677/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SVR.2016.26,A New User-Friendly Sketch-Based Modeling Method Using Convolution Surfaces,IEEE,Conferences,"3d modeling systems are essential tools for creating3D content for virtual reality systems. They are powerfuland sophisticated computer applications with a steep learningcurve. When considering simple shape prototyping it is moreappropriate to use a software with a simpler and intuitiveuser interface. This paper proposes a new sketch-based modelingapproach relying on convolution surfaces that enablesthe user to specify 3d shapes of arbitrary topology andresolution from 2d hand-drawn sketches that are embeddedand manipulated in 3d space. To make this approach feasiblewe propose two innovations: the first one is an interactionmechanism to create, manipulate and assemble independent2d sketches embedded in 3d space relying only on simplerotations, translations and a ray casting step; the second oneis a new approach to fit the level set surfaces generated bythe convolution of skeleton primitives to the silhouette curvesspecified by the user. The proposed fitting combines a methodthat automatically computes weights for the convolutionoperation with a Stolte's blending function that controls theinfluence of each implicit functional components to the finalshape. We show via experiments and preliminary user teststhat our method permits the user to easily express his 3dconcepts by sketching and assembling 2d simple drawings intridimensional space.",https://ieeexplore.ieee.org/document/7517260/,2016 XVIII Symposium on Virtual and Augmented Reality (SVR),21-24 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCMEIM52463.2020.00019,A Portable System for ToF Camera based Human Body Detection and Pose Estimation,IEEE,Conferences,"In recent years, human motion detection and analysis has been a research hotspot in the field of machine vision. It has been widely used in sports fitness, sports rehabilitation, sports analysis and virtual reality. Most of the existing human motion detection systems are based on optical equipment using retroreflective markers, which have many defects such as high deployment cost and complex operation. To this end, this article develops a household fitness motion evaluation system based on ToF camera with a mobile phone. The system firstly uses the ToF camera to collect the 3D human body point cloud information. Then Huawei augmented reality engine is employed to track the human skeleton. As all the spatial coordinates of joints are acquired, virtual human model can be reconstructed remotely on the phone. The mobile phone, working as the edge node between the person and the cloud server, transmits the captured human motion model parameters to the cloud platform. Then a hierarchical co-occurrence neural network recognizes and scores the detected user actions online. The experimental results show that the system proposed in this paper is portable and inexpensive. Real-time capabilities and measurement accuracy can also satisfy the requirements of evaluation for physical movements in sports rehabilitation.",https://ieeexplore.ieee.org/document/9409493/,2020 3rd World Conference on Mechanical Engineering and Intelligent Manufacturing (WCMEIM),4-6 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00071,A QoE and Visual Attention Evaluation on the Influence of Spatial Audio in 360 Videos,IEEE,Conferences,"Recently, there has been growing interest from academia and industry on the application of immersive technologies across a range of domains. Once such technology, 360° video, can be captured using an omnidirectional multi-camera arrangement. These 360° videos can then be rendered via Virtual Reality (VR) Head Mounted Displays (HMD). Viewers then have the freedom to look around the scene in any direction they wish. Whereas a body of work exists that focused on modeling visual attention (VA) in VR, little research has considered the impact of the audio modality on VA in VR. It is well accepted that audio has an important role in VR experiences. High quality spatial audio offers listeners the opportunity to experience sound in all directions. One such technique, Ambisonics or 3D audio, offers a complete 360° soundscape. This paper reports the results of an empirical study that looked at understanding how (if at all) spatial audio influences visual attention in 360° videos. It also assessed the impact of spatial audio on the user’s Quality of Experience (QoE) by capturing implicit, explicit, and objective metrics. The results suggest surprisingly similar explicit QoE ratings for both the spatial and non-spatial audio environments. The implicit metrics indicate that users integrated with the spatial environment more quickly than the non-spatial environment. Users who experienced the spatial audio environment had a higher maximum mean head pose pitch value and were found to be more focused towards the sound-emitting regions in the spatial audio environment experiences.",https://ieeexplore.ieee.org/document/9319084/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEA49774.2020.9102083,A Rapid Deployment Indoor Positioning Architecture based on Image Recognition,IEEE,Conferences,"With the rapid development of information, the immediacy, interoperability, and portability of information long been essential elements in present society. Technology has also continued to develop from these elements and creating more and more information equipment. In recent years, as new technologies continue to be developed, they have been mainly divided into two major technologies, namely “deep learning” and “augmented reality. These are including face recognition, image recognition, and augmented reality (AR), virtual reality (VR) and so on which are the key development trends now whether on mobile devices or web pages. In this study, the emerging mainstream “deep learning” and “augmented reality” are used to solve the problem of Google Map that cannot perform indoor navigation. The image recognition technology in deep learning is used to achieve the positioning function and the algorithm is used to calculate the best route to your destination, then indoor navigation presented. Finally, augmented reality (AR) technology is used to set up an exclusive AR floating electronic bulletin board at a specific location to display the relevant information and content of the specific location point for users. This research has the characteristics of rapid deployment which can be launched quickly in an indoor environment.",https://ieeexplore.ieee.org/document/9102083/,2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA),16-21 April 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2012.27,A Study for Vision Based Data Glove Considering Hidden Fingertip with Self-Occlusion,IEEE,Conferences,"Data glove is widely used interface device which measures hand posture (finger joint angles) and inputs it into a computer in virtual reality field. But it dose not spread throughout home because of expensive interface. On the other hand, researches to recognize a hand posture from photo/video images are performed, which are a kind of hand posture measurement system and called Vision Based Data Glove (VBDG) in recent years. In this paper, we propose a new VBDG system. It estimates hand motion using detected fingertip positions with monocular camera and inverse kinematics. However it cannot estimate hand motion when a fingertip is undetectable with self-occlusion. So our system estimates hidden finger motion, then estimates hand motion. Our experimental results show that the proposed method can estimate it with sufficient accuracy in real-time. Since camera base system is inexpensive, it fits personal use.",https://ieeexplore.ieee.org/document/6299298/,"2012 13th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing",8-10 Aug. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC.2019.8666471,"A UAV with Autonomy, Pattern Recognition for Forest Fire Prevention, and AI for Providing Advice to Firefighters Fighting Forest Fires",IEEE,Conferences,"The design of a long endurance UAV powered by solar energy, with autonomy flying over a pre specified forest area equipped with LiDAR which includes R, G, B and Infrared or near infrared bands to take clear and detail video of every part of the forest in order to recognize legal or illegal camp fires, dry areas providing hazardous conditions, upload temperature, humidity and other ground sensor data, helping to characterize the degree of forest fire vulnerability using pattern recognition, and communicating this information to firefighters either upon request, or as alarm events. The airplane electronic hardware software uses computational photography and virtual reality to create a detail 3-D forest video real time. It communicates all this information to a ground station real time. Although the aircraft has autonomy, trained pilots in the ground station can override the autonomy and fly the aircraft. In case of forest fire the aircraft electronic hardware software system can compute the exact area affected, the fire velocity and speed, the wind direction, and provide advice to the firefighters regarding the optimal way of fighting the fire. Often time forest fires destroy the communication infrastructure, so the airplane has a router to enable firefighters to exchange, text and voice information.",https://ieeexplore.ieee.org/document/8666471/,2019 IEEE 9th Annual Computing and Communication Workshop and Conference (CCWC),7-9 Jan. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBMS49503.2020.00079,A Virtual Assistant for Cybersickness Care,IEEE,Conferences,"We present an avatar and task-oriented dialog agent for monitoring user discomfort during a virtual reality (VR) cognitive exercise and providing personalized information and advice on its relief. The goal of this approach is to provide instantaneous assistance to users for a more comfortable VR experience, thereby enabling them to spend more time on cognitive tasks. We developed an avatar in a VR environment with which users may communicate verbally, and a dialog agent in a machine-learning based conversational AI platform. We performed a technical evaluation of the natural language understanding (NLU) component by comparing 2 models (BERT and StarSpace) using a train-test split, showing a significant benefit of BERT with smaller data sets. We validated the turn prediction using a train-test split and using randomly generated conversations. Both validations showed acceptable conversation-level accuracy. We undertook a usability study at two sites, showing effectiveness at both and good acceptability at one of the two. The framework outlined can be used to develop other virtual agents for cognitive self-care. Suggested improvements include validating the avatar with integrated BERT and reducing reliance on data augmentation, offline voice interaction modules, improved UX design, clinically validating the effect of the dialog agent on user discomfort and on cognitive performance, and increasing the ubiquity of the avatar within the VR cognitive care environment.",https://ieeexplore.ieee.org/document/9182864/,2020 IEEE 33rd International Symposium on Computer-Based Medical Systems (CBMS),28-30 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2009.4913063,A bio-inspired haptic interface for tele-robotics applications,IEEE,Conferences,"This paper presents the design concept for a bio-inspired exoskeleton intended for applications in tele-robotics and virtual reality. We based the development on an attentive analysis of the human arm anatomy with the intent to synthesize a system that will be able to interface with the human limb in a natural way. Our main goal is to develop a multi contact-point haptic interface that does not restrict the arm mobility and therefore increases the operational workspace. We propose a simplified kinematic model of the human arm using a notation coming from the robotics field. To figure out the best kinematic architecture we employed real movement data, measured from a human subject, and integrated them with the kinematic model of the exoskeleton. This allows us to test the system before its construction and to formalize specific requirements. We also implemented and tested a first passive version of the shoulder joint.",https://ieeexplore.ieee.org/document/4913063/,2008 IEEE International Conference on Robotics and Biomimetics,22-25 Feb. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSST.1994.287869,A knowledge-based software reuse environment for program development,IEEE,Conferences,"The knowledge-based software reuse environment (KBSRE) for program development assists the user to familiarize himself with the domain application environment, to locate partially matched components from the reusable component library, to understand the life-cycle knowledge of a component, and to decompose a component when its subcomponents are available. The knowledge base was created by the frame based component representation method and a set of decomposition rules and sibling rules. The components are used to generate the new software system by component composition methods. This KBSRE is an open system which allows the implementor to modify the knowledge base, decomposition rules, and sibling rules with minimum efforts. Development of a virtual reality application is shown as an example.<>",https://ieeexplore.ieee.org/document/287869/,Proceedings of 26th Southeastern Symposium on System Theory,20-22 March 1994,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1997.649083,A mobile robot for service use: behaviour simulation system and intelligent control,IEEE,Conferences,"The structure of hardware and software of AI control system of a mobile robot for service use are described. Hardware of the mobile robot described include an autonomous wheel vehicle and a five degree of freedom manipulator. The software of the AI control system is based on soft computing including fuzzy control rules, fuzzy neural network and genetic algorithms. The intelligent control of cooperative motion between the autonomous vehicle and manipulator realises flexible operations such as navigation of a mobile robot in presence of static and dynamic obstacles, processes of opening door in rooms and pushing buttons of an elevator. New hierarchical structure of the AI control system includes direct human-robot communication line based on natural language and cognitive graphics, and a generator of virtual reality for simulation of artificial life conditions for the mobile service robot. Simulation and experimental results of navigation and technical operations with the manipulator mobile service robot used in office building are described.",https://ieeexplore.ieee.org/document/649083/,Proceedings of the 1997 IEEE/RSJ International Conference on Intelligent Robot and Systems. Innovative Robotics for Real-World Applications. IROS '97,11-11 Sept. 1997,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HAPTICS.2014.6775492,A novel haptic interface and control algorithm for robotic rehabilitation of stoke patients,IEEE,Conferences,"Rehabilitation robots are gradually becoming popular for stroke rehabilitation to improve motor recovery. By using a robot, the patient may perform the training more frequently on their own, but they must be motivated to do so. Therefore, this project develops a set of rehabilitation training programs with different haptic modalities on Compact Rehabilitation Robot (CR2) - a robot used to train upper and lower limbs reaching movement. The paper present the developed haptic interface, Haptic Sense with five configurable haptic modalities that include sensations of weight, wall, spring, sponge and visual amplification. A combination of several haptic modalities was implemented into virtual reality games, Water Drop - a progressive training game with up to nine levels of difficulties that requires user to move the cup to collect the water drops.",https://ieeexplore.ieee.org/document/6775492/,2014 IEEE Haptics Symposium (HAPTICS),23-26 Feb. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEARIS.2012.6231175,A scala-based actor-entity architecture for intelligent interactive simulations,IEEE,Conferences,"Simulator X is a software research platform for intelligent Real-time Interactive Systems. Based on the actor model, it supports fine grained concurrency and parallelism. The architecture uses actors to realize a distributed application state and execution model which is mapped to an object-centered global and coherent world view based on entities. The architecture pays specific attention to the minimize coupling and maximize cohesion software engineering principle. Entities are associated with specific properties. These properties are realized by so called state variables. State variable access is implicitly relayed to individual actors guarding access to them. The underlying asynchronous event mechanism is based on the actors' message passing facility to provide intra- and interprocess communication. It supports typical read/write access as well as change notification and snapshot services. An extensible world interface uses a semantic annotation layer grounded into an ontology to provide a coherent world view of the distributed world state and execution model. The world interface greatly simplifies configurability and the semantic layer provides a solid foundation for incorporation of different Artificial Intelligence components. The system is implemented in Scala using the Java virtual machine. This choice additionally fosters low-level scalability, portability, and reusability. The platform is targeted at Virtual Reality, Augmented Reality, Mixed Reality, and computer games.",https://ieeexplore.ieee.org/document/6231175/,2012 5th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS),5-5 March 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN.2008.4618263,A system scheme of 3D object reconstruction from single 2D graphics based on neural networks,IEEE,Conferences,"3D reconstruction has turned out to be a significant need in computer aided design (CAD) technology. With the rapid development of virtual reality and industry design, more and more 3D models are required in practical needs. So an artificial neural network based system scheme is presented in this paper, which can be used in 3D object reconstruction from single 2D image. BP neural network (BP NN) is applied in our system due to its non-linear mapping and self-adaptive ability. The 3D objects are represented by Open Inventor, which is a 3D software development kit based on OpenGL. Experimental results show that neural network is a promising approach for reconstruction and representation of 3D objects.",https://ieeexplore.ieee.org/document/4618263/,2008 6th IEEE International Conference on Industrial Informatics,13-16 July 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAI.2014.6918167,About the conference,IEEE,Conferences,The following topics are dealt with: intelligent systems; machine vision; health care system; neural networks; telecommunication security; software engineering; virtual reality; social computing; electronics; e-learning; and e-business.,https://ieeexplore.ieee.org/document/6918167/,2014 Science and Information Conference,27-29 Aug. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISCT.2019.8777408,Adoption of VR influencing AI on 3D objects,IEEE,Conferences,"The advent of new technologies has made it possible for new areas to be explored in the gaming industry. Virtual Reality has progressed at an exponential rate which has allowed it to be used in many industries. Virtual Reality allows users to be immersed in a highly detailed environment which allows for a more realistic experience. This paper outlines the development of a game on adoption of VR influencing AI on 3D objects, and the integration of virtual reality gaming Oculus and Leap Motion device in the gameplay, The Leap Motion is used to recognize hand gestures while Oculus will serve as the visual medium for the user. The game itself has been designed and developed on the Unity 3D gaming engine. This project endeavors to highlight the importance and diverse application of VR in various industries. The distinctive integration of Oculus, Leap and PC leads to a realistic gaming experience.",https://ieeexplore.ieee.org/document/8777408/,2019 International Conference on Information Science and Communication Technology (ICISCT),9-10 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIC.2000.882954,Advanced technologies for intelligent control research and teaching,IEEE,Conferences,"We live in an era of digital competition. That means that the world is facing rapid and impressive developments that are fueled by rapid advances in digital communications, digital satellite technologies, worldwide web, virtual reality, and computer and learning technologies. As a major area of interest, intelligent control will have to benefit from these important developments. Our paper presents some directions for integrating advanced technologies in intelligent control research and teaching.",https://ieeexplore.ieee.org/document/882954/,Proceedings of the 2000 IEEE International Symposium on Intelligent Control. Held jointly with the 8th IEEE Mediterranean Conference on Control and Automation (Cat. No.00CH37147),19-19 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOSP.2006.345664,Agent Based Intelligent Virtual Environment and Its Entertainment Application,IEEE,Conferences,"Intelligent Virtual Environment (IVE) is the integration of virtual reality and artificial intelligence (AI), i.e., it incorporates AI technology into virtual environment. Using IVE concept together with intelligent agents and neural networks, we design and implement an entertainment IVE application platform. The humanlike intelligent objects in the virtual environment are implemented with intelligent agents, which are endowed with intelligence for learning by applying artificial neural networks to agent controllers",https://ieeexplore.ieee.org/document/4129145/,2006 8th international Conference on Signal Processing,16-20 Nov. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIHAS.1993.410585,"Aggregation, disaggregation, and the challenge of crossing levels of resolution when designing and connecting models",IEEE,Conferences,"Virtual reality will sometimes require consistent simulated information at different levels of resolution. It will also require connecting models coherently that were designed independently. Basic concepts of variable-resolution modeling (VRM) are introduced, and a design approach involving hierarchical model processes and objects is described. Even in attempting to connect existing models, this approach can clarify the ideal, after which one can make integrated model adaptations rather than fixing connection problems sequentially. Experiments to improve the methods used in developing phenomenologically subtle variable-resolution models are described. How a mathematical analysis shed considerable light on a long-standing debate within the military modeling community is also described. An experiment with cross-organizational model comparisons that revealed subtle aggregation problems and may have been a prototype for what should be routine open discussion of models in circumstances where results can be reproduced or contradicted is discussed.<>",https://ieeexplore.ieee.org/document/410585/,"1993 4th Annual Conference on AI, Simulation and Planning in High Autonomy Systems",20-22 Sept. 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE53722.2022.9823801,An Exploration into Artificial intelligence based advancement in education field,IEEE,Conferences,"The improvisation of artificial intelligence inspired technologies like augment and virtual reality for implementation of blended learning in classrooms has created a buzz in the field of education. We conducted a research study to investigate the usage of virtual or augmented reality in teaching, as well as two separate theme analyses, to get a better overview of the challenges and expectations of the educators by adopting these technologies. The first analysis looked into the implementations and reported motives offered by instructors in academic literature for building virtual reality learning environments, whereas the latter looked into the issues that have been documented. According to these assessments, the majority of academics utilize virtual reality to improve students&#x0027; intrinsic motivation and build their interactions using a limited set of variables such as constructivist pedagogy, collaboration, and gamification. Similarly, the great majority of educational virtual reality deployments discovered in our research are in a small number of educational disciplines. Following that, we introduced and compared a variety of contemporary virtual reality technologies, highlighting their potential to address some of the issues raised in our studies. However, these tools are not without problems, therefore we end this study by proposing various innovative ways to potentially solve them, and prospective possibilities for future academics interested in using these developing innovations to learning.",https://ieeexplore.ieee.org/document/9823801/,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),28-29 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IHMSC.2011.90,An Implementation of Artificial Emotion Based on Fuzzy State Machine,IEEE,Conferences,"This article focuses on how to implement Artificial Emotion for virtual reality applications with Fuzzy State Machine, and revisits the OCC emotion model. We discuss the features of human emotions and FuSM. An FuSM can deal with multi-emotions simultaneously, so we can use FuSM in our implementation to simulate emotions and control character behaviors. And the results of experiments show that we can use FuSM to create artificial emotional characters with high believability.",https://ieeexplore.ieee.org/document/6038220/,2011 Third International Conference on Intelligent Human-Machine Systems and Cybernetics,26-27 Aug. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETA.2018.8572099,An Implementation of Efficient Hierarchical Access Control Method for VR/AR Platform,IEEE,Conferences,"With the growth of Virtual Reality (VR) and Augmented Reality (AR) in technologies such as artificial intelligence, wireless, 5G, big data, massive compute, industrial 4.0 and virtual stores. An improved secure mechanism which was previously still existed some shortcomings, was presented in this paper. Another new mechanism regards to achieve the decentralized environment access control is introduced to attain the needs of the non-specific internet. Furthermore, it is very important to look attentively to the administrative previleges in VR/AR clouds. The issues which gained from VR and AR could be tackled with the new mechanism. This new studies achieves a higher circumstance. Developer worker's duty can be distributed; the database system can be compatibly coordinates; moreover, the users' information security can be entirely secured.",https://ieeexplore.ieee.org/document/8572099/,2018 16th International Conference on Emerging eLearning Technologies and Applications (ICETA),15-16 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2018.8441049,An Improvement on ArUco Marker for Pose Tracking Using Kalman Filter,IEEE,Conferences,"This paper presents a robust but simple object pose tracking algorithm based on Kalman filtering. Compared to markerless pose tracking, a fiducial marker called ArUco provides a fast and accurate solution to the problem. With these advantages, this marker-based technique is ready to be used in virtual reality and operate in low-cost wearable devices. However, it still suffers from the problem of occlusion and noise. If a large part of the marker is occluded, no pose information can be acquired for the moment. Noises due to hand shaking also affect the quality of the resulting pose. This is not desirable in a real-time environment. We tackle the problems by employing a linear Kalman filter. The pose information can be estimated even if the camera view is blocked temporarily. We have performed real experiments to demonstrate the effects of the application of Kalman filter. The results are satisfactory.",https://ieeexplore.ieee.org/document/8441049/,"2018 19th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",27-29 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2018.8560557,An Industry 4.0 Cyber-Physical Framework for Micro Devices Assembly,IEEE,Conferences,"An advanced cyber manufacturing framework to support the collaborative assembly of micro devices is presented based on Industry 4.0 principles. The distributed cyber and physical components work together to plan, assemble and monitor micro assembly related tasks; micro assembly refers to the assembly of micron sized devices which cannot be manufactured by MEMS technologies. The collaborative framework proposed includes assembly planning and path planning modules, Virtual Reality based assembly simulation environments and physical assembly work cells. An ontology based approach was implemented to address semantic interoperability issues to support formation of temporary partnerships in a Virtual Enterprise context. The key to the design and implementation of this complex framework is an information centric process modeling approach which provides a data/information oriented basis for collaboration. A collaborative cyber physical test bed has been built to demonstrate feasibility of proposed framework and approach.",https://ieeexplore.ieee.org/document/8560557/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDA.2012.6416601,An approach to semi-automatic semantic annotation on Web3D scenes based on an ontology framework,IEEE,Conferences,"The recent years have witnessed a rapid development in virtual reality technology and computer aided design, which accompanied by a vast amount of 3D content springing up from various domains, has given rise to an emerging need for methods by which they can be efficiently annotated before being searched and retrieved. In this paper a new approach to semantic annotation on 3D content is proposed. In this approach, much of the manual work on annotation is replaced with a semi-automatic process based on prior gathering of domain expertise. The geometric properties of objects and their spatial relationships are first automatically extracted and associated with a general ontology, the result of which is then matched against a set of user-defined rules to create annotations represented in an application ontology. The approach is Web3D oriented so that it can take advantage of features of X3D format. It will be shown using a prototype system how this approach can succeed in accelerating the annotation process.",https://ieeexplore.ieee.org/document/6416601/,2012 12th International Conference on Intelligent Systems Design and Applications (ISDA),27-29 Nov. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HIS.2010.5600085,An intelligent Web interface to generate and update adaptive Virtual Environments,IEEE,Conferences,"In recent years, three-dimensional Virtual Environments developed with virtual reality and graphical computation technologies have evolved in the Web. Thus, users with different profiles, cultures and knowledge levels can access such environments remotely. In such a way, it is necessary to use an efficient interface to manage Web accesses and to actualize virtual environments to adapt them to each user's characteristics. This article presents an intelligent Web interface managed by a society of software agents that follows and identifies user's actions in order to propose modifications in the virtual environment, through 3D objects updating. The interface is capable to identify user's actions and to react to them in real time, without loss of performance. The objects used in the construction and adaptation of a Virtual Environment are stored into a database and are retrieved by agents, according to the specific negotiation rules defined in an ontology for the domain.",https://ieeexplore.ieee.org/document/5600085/,2010 10th International Conference on Hybrid Intelligent Systems,23-25 Aug. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASA54658.2022.9765294,"Application of AI, IOT and ML for Business Transformation of The Automotive Sector",IEEE,Conferences,"Automotive industry is essential in human lives. It is not possible to imagine a day without driving or some public transport. Today, digital technologies are making motor vehicles and the industry more intelligent. The entire value chain of automotive business is transforming. A better connect with customers is needed. All this is possible through advanced digital technologies. Automotive companies are overhauling business processes and relationships. Legacy IT systems for manufacturing, engineering, supply chain etc. are being reinvented. This transformation encompasses software, robotics, connected devices, and artificial intelligence. Artificial intelligence (AI) made the dream of self-driving cars possible. AI will soon transform every device. Tesla, Google Waymo, and Nvidia are examples of machine learning algorithms used to detect how far different objects are, from the car. Augmented reality (AR) and virtual reality (VR) analysis enables users to watch blind spots. AI enhances security by simultaneous coordination with many sensors. With AR, VR and mixed reality (MR), automotive companies have a personalized retail platform and a competitive edge. This paper studies AI applications in the automotive sector. It studies the recent developments, and applications of AI. It discusses how companies use AI for cost reduction, market strategies, sales promotion, and even funding.",https://ieeexplore.ieee.org/document/9765294/,2022 International Conference on Decision Aid Sciences and Applications (DASA),23-25 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2000.844768,Application of automatic action planning for several work cells to the German ETS-VII space robotics experiments,IEEE,Conferences,"Experiences in space robotics show, that the user normally has to cope with a huge amount of data. So, only robot and mission specialists are able to control the robot arm directly in teleoperation mode. By means of an intelligent robot control in cooperation with virtual reality methods, it is possible for non-robot specialists to generate tasks for a robot or an automation component intuitively. Furthermore, the intelligent robot control improves the safety of the entire system. The on-ground robot control and command station for the robot arm ERA onboard the satellite ETS-VII builds on a new resource-based action planning approach to manage robot manipulators and other automation components. In the case of ERA, the action planning system also takes care of the ""real"" robot onboard the satellite and the ""virtual"" robot in the simulation system. By means of the simulation system, the user can plan tasks ahead as well as analyze and visualize different strategies. The paper describes the mechanism of resource-based action planning, its application to different work cells, the practical experiences gained from the implementation for the on-ground robot control and command station for the robot arm ERA developed in the GETEX project as well as the services it provides to support VR-based man machine interfaces.",https://ieeexplore.ieee.org/document/844768/,Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065),24-28 April 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AEECA52519.2021.9574150,Application of virtual panorama technology based on multi-source information fusion in high quality development of the Yellow River,IEEE,Conferences,"Virtual panorama technology based on multi-source information fusion is a virtual reality technology based on image technology to generate realistic graphics. Panorama production software is used to generate panoramic images with a strong sense of scene and perspective effect that can be fully displayed at 720 degrees. Various kinds of voice, video, image and other hot spots are added in the virtual panorama image, and the step and front-end customized display are freely added, and multiple types of artificial intelligence graphic algorithms are applied to realize the virtual panorama platform of multi-source information fusion. The technology has been widely used in some businesses of the Yellow River, with strong practicability, large application and development space, and obvious advantages in investment benefit ratio.",https://ieeexplore.ieee.org/document/9574150/,2021 IEEE International Conference on Advances in Electrical Engineering and Computer Applications (AEECA),27-28 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NetSoft48620.2020.9165317,Applying Machine Learning to End-to-end Slice SLA Decomposition,IEEE,Conferences,"5G is set to revolutionize the network service industry with unprecedented use-cases in industrial automation, augmented reality, virtual reality and many other domains. Network slicing is a key enabler to realize this concept, and comes with various SLA requirements in terms of latency, throughput, and reliability. Network slicing is typically performed in an end-to-end (e2e) manner across multiple domains, for example, in mobile networks, a slice can span access, transport and core networks. Thus, if an SLA requirement is specified for e2e services, we need to ensure that the total SLA budget is appropriately proportioned to each participating domain in an adaptive manner. Such an SLA decomposition can be extremely useful for network service operators as they can plan accordingly for actual deployment. In this paper we design and implement an SLA decomposition planner for network slicing using supervised machine learning algorithms. Traditional optimization based approaches cannot deal with the dynamic nature of such services. We design machine learning models for SLA decomposition, based on random forest, gradient boosting and neural network. We then evaluate each class of algorithms in terms of accuracy, sample complexity, and model explainability. Our experiments reveal that, in terms of these three requirements, the gradient boosting and neural network algorithms for SLA decomposition out-perform random forest algorithms, given emulated data sets.",https://ieeexplore.ieee.org/document/9165317/,2020 6th IEEE Conference on Network Softwarization (NetSoft),29 June-3 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766497,ArSL21L: Arabic Sign Language Letter Dataset Benchmarking and an Educational Avatar for Metaverse Applications,IEEE,Conferences,"It is complicated for the PwHL (people with hearing loss) to make a relationship with social majority, which naturally demands an interactive auto computer systems that have ability to understand sign language. With a trending Metaverse applications using augmented reality (AR) and virtual reality (VR), it is easier and interesting to teach sign language remotely using an avatar that mimics the gesture of a person using AI (Artificial Intelligence)-based system. There are various proposed methods and datasets for English SL (sign language); however, it is limited for Arabic sign language. Therefore, we present our collected and annotated Arabic Sign Language Letters Dataset (ArSL21L) consisting of 14202 images of 32 letter signs with various backgrounds collected from 50 people. We benchmarked our ArSL21L dataset on state-of-the-art object detection models, i.e., 4 versions of YOLOv5. Among the models, YOLOv5l achieved the best result with COCOmAP of 0.83. Moreover, we provide comparison results of classification task between ArSL2018 dataset, the only Arabic sign language letter dataset for classification task, and our dataset by running classification task on in-house short video. The results revealed that the model trained on our dataset has a superior performance over the model trained on ArSL2018. Moreover, we have created our prototype avatar which can mimic the ArSL (Arabic Sign Language) gestures for Metaverse applications. Finally, we believe, ArSL21L and the ArSL avatar will offer an opportunity to enhance the research and educational applications for not only the PwHL, but also in general real and virtual world applications. Our ArSL21L benchmark dataset is publicly available for research use on the Mendeley.",https://ieeexplore.ieee.org/document/9766497/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC46164.2021.9629804,Augmented Reality Assisted Surgical Navigation System for Epidural Needle Intervention,IEEE,Conferences,"An augmented reality (AR)-assisted surgical navigation system was developed for epidural needle intervention. The system includes three components: a virtual reality-based surgical planning software, a patient and tool tracking system, and an AR-based surgical navigation system. A three-dimensional (3D) path plan for the epidural needle was established on the preoperative computed tomography (CT) image. The plan is then registered to the intraoperative space by 3D models of the target vertebrae using skin markers and real-time tracking information. In the procedure, the plan and tracking information are transmitted to the head-mounted display (HMD) through a wireless network such that the device directly visualizes the plan onto the back surface of the patient. The physician determines the entry point and inserts the needle into the target based on the direct visual guidance of the system. An experiment was conducted to validate the system using two torso phantoms that mimic human respiration. The experimental results demonstrated that the time and the number of X-rays required for needle insertion were significantly decreased by the proposed method (43.6&#x00B1;20.55sec, 2.9&#x00B1;1.3times) compared to those of the conventional fluoroscopy-guided approach (124.5 &#x00B1; 46.7s, 9.3&#x00B1;2.4times), whereas the average targeting errors were similar in both cases. The proposed system may potentially decrease ionizing radiation exposure not only to the patient but also to the medical team.",https://ieeexplore.ieee.org/document/9629804/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOASE.2019.8723683,Augmented Reality Electric Circuit Experiment,IEEE,Conferences,"The advancement of hardware technology, specifically in mobiles devices, has provided great computational power for running and creating sophisticated large programs such as, virtual reality, augmented reality and neural network based programs. Augmented reality (AR) is one of the finest and most exciting technologies nowadays. AR enables creating computerized objects and blending them with the real world. Which can be applied to enhance and simplify many aspects in a variety of disciplines. In result, instead of reading text and imagining experimentations it is possible to simulate the experiments through equipment and experiments as same as in real world scenarios, which can be much understandable and comprehensible. In the past decade, several institutions have adopted technological methods of teaching and learning. These instructional technologies can be used by some institutions that cannot provide enough time or proper equipment to students. This is where augmented and virtual reality are mostly favorable. In result, instead of reading text and imagining experimentations it is possible to simulate the experiments through equipment and experiments as same as in real world scenarios, which can be much understandable and comprehensible. In this paper, a model of an electric circuit is created to simulate the motion of electrons and how electric current runs through a wire and provides power for different tools. The creation of such a model involves design, modeling, simulation and animation of the model. This is done by relying on several software and frameworks such as Maya3D, Unity3D, ARKit, etc. .",https://ieeexplore.ieee.org/document/8723683/,2019 International Conference on Advanced Science and Engineering (ICOASE),2-4 April 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VIMS.2002.1009366,Behavior-based script language for anthropomorphic avatar animation in virtual environments,IEEE,Conferences,This paper reports development aspects of a behavior-based script language for the animation of anthropomorphic avatars in virtual reality environments. It discusses definition and implementation aspects of a behavior-based script language for the autonomous or interactive animation of a standard avatar.,https://ieeexplore.ieee.org/document/1009366/,2002 IEEE International Symposium on Virtual and Intelligent Measurement Systems (IEEE Cat. No.02EX545),19-20 May 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCS52396.2021.00073,Bluetooth Communications in Educational Robotics,IEEE,Conferences,"In a world in a continuous and rapid change, it is absolutely necessary for our students to keep up with the rapid progress of new technologies: Internet of Things (IoT), Robotics, Artificial Intelligence (AI), Virtual Reality (VR), Augmented Reality (AR) etc. The rapid evolution and diversification of these emerging technologies has recently led to their introduction into the educational offer of the school curriculum for the gymnasium. The discipline of Information and Communication Technology (ICT) has already been implemented, a discipline that involves both the formation of skills to use new technologies and the formation of computational thinking necessary for the efficient and intelligent use of these technologies. In order to teach and learn Physics from a STEM (Science, Technology, Engineering and Mathematics) educational perspective, we initiated optional school courses of IoT, Robotics and AI (approached through Machine Learning). These courses stimulate, at the level of students, computational thinking, creativity and innovation and lead, from an interdisciplinary perspective, to the development of emerging specializations such as Mathematics-Physics-Automation, Mathematics-Physics-Electronics, Mathematics-Physics-Informatics-Robotics etc. In this paper we presented a method of approaching, in the school educational space, the study of wireless communication technologies between smart devices, through an Educational Robotics project. The project consisted of creating a wireless controlled mobile robotic platform (robot car) via a Bluetooth module connected to an Arduino Uno board.",https://ieeexplore.ieee.org/document/9481012/,2021 23rd International Conference on Control Systems and Computer Science (CSCS),26-28 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACSSC.2017.8335668,Brain-aware wireless networks: Learning and resource management,IEEE,Conferences,"Human-centric applications such as virtual reality and immersive gaming will be central to the future wireless networks. Common features of such services include: a) their dependence on the human user's behavior and state and b) their need for more network resources compared to conventional cellular applications. To successfully deploy such applications over wireless and cellular systems, the network must be made cognizant of the human in the loop. In this paper, a concrete measure for the delay perception of human users in a wireless network is defined. Then, a new learning method called probability distribution identification is introduced to find a probabilistic model for this delay perception. A novel approach based on Lyapunov optimization is proposed for allocating radio resources to human users while considering their brain's delay perception model. Simulation results show that a brain-aware approach can yield savings of up to 37% in power compared to the system that only considers quality of service metrics.",https://ieeexplore.ieee.org/document/8335668/,"2017 51st Asilomar Conference on Signals, Systems, and Computers",29 Oct.-1 Nov. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00052,Breast3D: An Augmented Reality System for Breast CT and MRI,IEEE,Conferences,"Adoption of Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) - known collectively as Extended Reality (XR) devices has been rapidly increasing over recent years. However, the focus of XR research has shown a lack of diversity in solutions to the problems within medicine, with it being predominantly focused in augmenting surgical procedures. Whilst important, XR applied to aiding medical diagnosis and surgical planning is relatively unexplored. In this paper we present a fully functional mammographic image analysis system, Breast3D, that can reconstruct MRI and CT scan data in XR. With breast cancer Breast Imaging-Reporting and Data System (BI-RADS) risk lexicon, early detection and clinical workflow such as Multi-disciplinary team (MDT) meetings for cancer in mind, our new mammography visualization system reconstructs CT and MRI volumes in a real 3D space. Breast3D is built upon the past literature and inspired from research for diagnosis and surgical planning. In addition to visualising the models in MR using the Microsoft HoloLens, Breast3D is versatile and portable to different XR head-mounted displays such as HTC Vive. Breast3D demonstrates the early potential for XR within diagnostics of 3D mammographic modalities, an application that has been proposed but until now has not been implemented.",https://ieeexplore.ieee.org/document/9319058/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49033.2022.9700515,CAVE-VR and Unity Game Engine for Visualizing City Scale 3D Meshes,IEEE,Conferences,"Modeling and simulation of large urban regions is beneficial for a range of applications including intelligent transportation, smart cities, infrastructure planning, and training artificial intelligence for autonomous navigation systems including ground vehicles and aerial drones. Immersive environments including virtual reality (VR), augmented reality (AR), mixed reality (MR or XR) can be used to explore city scale regions for planning, design, training and operations. Virtual environments are in the midst of rapid change as innovations in display tech-nologies, graphics processors and game engine software present new opportunities for incorporating modeling and simulation into engineering workflows. Game engine software like Unity with photorealistic rendering and realistic physics have plug-in support for a variety of virtual environments. In this paper, we explore the visualization of urban scale real world accurate meshes in virtual environments, including the Microsoft HoloLens head mounted display or the CAVE VR for multi-user interaction.",https://ieeexplore.ieee.org/document/9700515/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CPEE54040.2021.9585252,CNN-based character recognition for a contextless text input system in immersive VR,IEEE,Conferences,"The paper investigates the applicability of deep neural networks and transfer learning procedure to developing a handwriting recognition system for typing contextless content in a Virtual Reality system. Inputting special characters in such systems is still a nontrivial task, even nowadays. Most algorithms for editing text focus on contextual text recognition from speech or gestures. Unfortunately, these approaches fail for text containing not only letters additionally in the context of words, phrases, and trivial punctuation characters. In the paper, the authors explore the possibility of typing more advanced structures, which for example, can be observed in source code editing. The aim of the paper is to verify if modern machine learning algorithms can facilitate text typing in a context-free environment. The authors have chosen a transfer learning procedure utilizing already trained convolutional layers from ALEXNET [1], followed by deep network classification layers. The methods and procedures implemented for the Virtual Reality system used to acquire data necessary to train the recognition model are also presented.",https://ieeexplore.ieee.org/document/9585252/,2021 22nd International Conference on Computational Problems of Electrical Engineering (CPEE),15-17 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/iLRN52045.2021.9459413,Comparison of Direct and Vicarious VR Learning Experience: A Perspective from Accessibility and Equity,IEEE,Conferences,"A common challenge for adopting virtual reality (VR) in education is that limited VR devices are often shared among a large group of students. Consequently, there are two types of VR learners: Performers who acquire virtual learning experience through direct engagement in VR and observers who acquire such experience vicariously through observation. To explore the influence of learner type on VR learning, this study conducted a quasi-experiment with 53 elementary school students to examine the difference in VR learning experiences between the performers and the observers. The study results supported the observed VR learning experience as an adequate alternative to direct VR engagement as the observers demonstrated overall comparable learning patterns in reflection, emotion, engagement, and social interaction during the post-VR debriefing, except for the behaviors of recall and interpretation. The research findings can shed light on the issues of accessibility and equity in VR-based instruction and inform the design and implementation of large-scale VR educational programs.",https://ieeexplore.ieee.org/document/9459413/,2021 7th International Conference of the Immersive Learning Research Network (iLRN),17 May-10 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC50026.2020.9450254,Complex Networks,IEEE,Conferences,"The Internet, social networks, power grids, gene regulatory networks, neuronal systems, food webs, social systems, and networks emanating from augmented and virtual reality platforms are all examples of complex networks. Collection and analysis of data from these networks is essential for their understanding. Traffic traces collected from various deployed communication networks and the Internet have been used to characterize and model network traffic, analyze network topologies, and classify network anomalies. Data mining and statistical analysis of network data have been employed to determine traffic loads, analyze patterns of users' behavior, and predict future network traffic while spectral graph theory has been applied to analyze network topologies and capture historical trends in their development. Recent machine learning techniques have proved valuable for predicting anomalous traffic behavior and for classifying anomalies in complex networks. Further applications of these tools will help improve our understanding of the underlying mechanisms that govern behavior, improve their performance, and enhance their security of social networks such as Facebook, LinkedIn, Twitter, Internet blogs, forums, and websites.",https://ieeexplore.ieee.org/document/9450254/,2020 IEEE 19th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),26-28 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRC53822.2021.00025,Conscious Machines for Autonomous Agents and Cybersecurity,IEEE,Conferences,"Although consciousness has been difficult to define, most researchers in artificial intelligence would agree that AI systems to date have not exhibited anything resembling consciousness. But is a conscious machine possible in the near future? I suggest that a new definition of consciousness may provide a basis for developing a conscious machine. The key is pattern recognition of correlated events in time, leading to the identification of a unified self-agent. Such a conscious system can create a simplified virtual environment, revise it to reflect updated sensor inputs, and partition the environment into self, other agents, and relevant objects. It can track recent time sequences of events, predict future events based on models and patterns in memory, and attribute causality to events and agents. It can make rapid decisions based on incomplete data, and can dynamically learn new responses based on appropriate measures of success and failure. The central aspect of consciousness is the generation of a dynamic narrative, a real-time model of a self-agent pursuing goals in a virtual reality. A conscious machine of this type may be implemented using an appropriate neural network linked to episodic memories. Near-term applications may include autonomous vehicles and online agents for cybersecurity.",https://ieeexplore.ieee.org/document/9743183/,2021 International Conference on Rebooting Computing (ICRC),30 Nov.-2 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIMCIS53775.2021.9699345,Constructing Smart Digital Media for Museum Education Post Pandemic Recovery: A Review and Recommendation,IEEE,Conferences,"The research conducted a literature review to find the trend of smart digital museum construction for education media that have been developed by utilizing information and communication technology and intelligent systems. A series of procedures and methods applied in the literature review process extracting information from dataset contained articles and journals in the 2015-2021 period. A physical or traditional museum must be transformed into a smart digital museum that can carry out an educational role even during the COVID-19 pandemic. The pandemic has made limitations to museum visits that impacted fewer activities and community visits for education in the museum. Museum performance for education is not optimal during a pandemic, especially in Palembang. The results of this study indicate a trend of digital smart transformation from physical or traditional museums. Applications based on mobile devices and websites are mostly developed 72.72&#x0025; of applications for education in museums. Modern museums have begun to transform into digital museums implementing intelligent machine learning based systems in the last three years and with learning application content in the form of games, augmented reality or virtual reality, and information retrieval systems. The results of the study are expected to provide information for museum education post-pandemic recovery strategic planning in Indonesia.",https://ieeexplore.ieee.org/document/9699345/,"2021 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS",28-29 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00027,Construction and Research of Virtual Forest Environment Based on Spatial Data,IEEE,Conferences,"Virtual Forest environment is a virtual geographical environment in the application of scientific forestry practices; it is a combination of forestry science and virtual geographical environment. Based on forestry spatial data and virtual reality technology, it constructs forest objects and expresses extremely complex forest phenomena. This article introduces how to create a virtual environment object of forest, forest-related technology, the virtual environments research, and pointes out the importance of creating a virtual forest environment.",https://ieeexplore.ieee.org/document/9696141/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2019.8797830,Contextual Bandit Learning-Based Viewport Prediction for 360 Video,IEEE,Conferences,"Accurately predicting where the user of a Virtual Reality (VR) application will be looking at in the near future improves the perceive quality of services, such as adaptive tile-based streaming or personalized online training. However, because of the unpredictability and dissimilarity of user behavior it is still a big challenge. In this work, we propose to use reinforcement learning, in particular contextual bandits, to solve this problem. The proposed solution tackles the prediction in two stages: (1) detection of movement; (2) prediction of direction. In order to prove its potential for VR services, the method was deployed on an adaptive tile-based VR streaming testbed, for benchmarking against a 3D trajectory extrapolation approach. Our results showed a significant improvement in terms of prediction error compared to the benchmark. This reduced prediction error also resulted in an enhancement on the perceived video quality.",https://ieeexplore.ieee.org/document/8797830/,2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR),23-27 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEDM19574.2021.9720526,"Creating the Future: Augmented Reality, the next Human-Machine Interface",IEEE,Conferences,"XR, consisting of Virtual Reality (VR) and Augmented Reality (AR) together, will be the next general computing platform, dominating our relationship with the digital world for the next 50 years, much as personal computing has dominated the last 50. XR will be the way people work, play, and connect. VR headsets will create deeply immersive new experiences and will enable the richest Metaverse experiences. Always-available AR glasses will be with us when we&#x0027;re on the go, letting us act with extremely low latency and friction, allowing us to use virtual entities of all sorts to annotate our world, share with others, and communicate, and extending our perceptions, memory, and cognition with an Artificial Intelligence (AI) assistant that truly understands our context and our personal needs. Both will be centered around people rather than technology, allowing us to connect more strongly than ever before, and providing a contextually personalized interface that is far more intuitive and natural than anything that exists today. However, VR and particularly AR are at a very early stage, and need a great deal of innovation and development, across multiple technologies, before they can offer the level of performance that would cause billions of people to make XR a part of their everyday lives. Those technologies include optics, projectors, display systems, graphics, audio, hand tracking, eye tracking, face tracking, body tracking, world mapping and reconstruction, contextual understanding, sensors, interaction, and AI. VR headsets operate within tight power, weight, and thermal budgets, and AR glasses will need to operate within extraordinarily tight budgets; for AR glasses, the constraints include light weight, very low power, and all-day comfort and all-day operation in a socially acceptable form factor,. Breakthroughs are needed in every one of the areas listed above, and in almost every case, innovative, highly customized semiconductor technologies will be needed to achieve those breakthroughs. This will notably require new materials, miniaturization of the key components, and very-large-scale 3D heterogeneous integration of circuits and systems to create new functions. In addition, specialization at the level of architecture and domain-oriented accelerators, especially with respect to Machine Learning (ML), will be essential, and algorithm optimization will need to evolve in new directions. Finally, the challenges of AR can only be solved by true co-design of hardware and software; technology development will need to be driven from end to end, based on delivering important use cases with a full-system approach. This will create unforeseen opportunities and challenges for semiconductor technologies for decades to come.",https://ieeexplore.ieee.org/document/9720526/,2021 IEEE International Electron Devices Meeting (IEDM),11-16 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9411915,Cross-Domain Semantic Segmentation of Urban Scenes via Multi-Level Feature Alignment,IEEE,Conferences,"Semantic segmentation is an essential task in plenty of real-life applications such as virtual reality, video analysis, autonomous driving, etc. Recent advancements in fundamental vision-based tasks ranging from image classification to semantic segmentation have demonstrated deep learning-based models' high capability in learning complicated representation on large datasets. Nevertheless, manually labeling semantic segmentation dataset with pixel-level annotation is extremely labor-intensive. To address this problem, we propose a novel multi-level feature alignment framework for cross-domain semantic segmentation of urban scenes by exploiting generative adversarial networks. In the proposed multi-level feature alignment method, we first translate images from one domain to another one. Then the discriminative feature representations extracted by the deep neural network are concatenated, followed by domain adversarial learning to make the intermediate feature distribution of the target domain images close to those in the source domain. With these domain adaptation techniques, models trained with images in the source domain where the labels are easy to acquire can be deployed to the target domain where the labels are scarce. Experimental evaluations on various mainstream benchmarks confirm the effectiveness as well as robustness of our approach.",https://ieeexplore.ieee.org/document/9411915/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWCMC55113.2022.9824935,Decomposition of power system inspection services for 5G cloud-edge-end collaboration,IEEE,Conferences,"With the development of the smart power grid, the requirements for intelligent, vivid, and real-time power system inspection services are getting higher. 5G, artificial intelligence, edge computing, big data, virtual reality, and other information technologies bring breakthroughs for power system inspection services evolution, but in the meanwhile, making the services more complex and draining large amounts of computing resources. In the 5G networks with collaborative computing enabled in cloud-edge-end nodes, how to reasonably decompose a complex power system inspection service into multiple task clusters becomes a prerequisite for the efficient distributed deployment of the service and its rapid execution. This paper proposes an approach for the decomposition of intelligent power system inspection services. It considers the computing resource requirements of the functional components in a power system inspection service as well as the interactions between them, perceives the computing capabilities of heterogeneous nodes, such as cloud, edge, and collaborative end terminal in 5G networks, and decomposes the service into multiple task clusters which retain the original logical structure. Simulation results show that the approach can produce suitable results, which can reduce the expected service execution time and improve the utilization of network resources.",https://ieeexplore.ieee.org/document/9824935/,2022 International Wireless Communications and Mobile Computing (IWCMC),30 May-3 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCEE.2018.8538382,Deep Neural Classifiers For Eeg-Based Emotion Recognition In Immersive Environments,IEEE,Conferences,"Emotion recognition has become a major endeavor in artificial general intelligence applications in recent years. Although significant progress has been made in emotion recognition for music, image and video stimuli, it remains largely unexplored for immersive virtual stimuli. Our main objective for this line of investigation is to enable consistently reliable emotion recognition for virtual reality stimuli using only cheap, commercial-off-the-shelf electroencephalography (EEG) headsets which have significantly less recording channels and far lower signal resolution commonly called “Wearable EEG” as opposed to medical-grade EEG headsets with the ultimate goal of applying EEG-based emotion prediction to procedurally-generated affective content such as immersive computer games and virtual learning environments through machine learning. Our prior preliminary study has found that the use of a 4-channel, 256-Hz was indeed able to perform the required emotion recognition tasks from VR stimuli albeit at classification rates of between 65-89% classification accuracy only using Support Vector Machines (SVMs) and K-Nearest Neighbor (KNN) classifiers. For this particular study, we attempt to improve the classification rates to above 95% by conducting a comprehensive investigation into the use of various deep neural-based learning architectures for this domain. By tuning the deep neural classifiers in terms of the number of hidden layers, number of hidden nodes and the nodal dropout ratio, the emotion prediction accuracy was able to be improved to over 96%. This shows the continued promise of the application of wearable EEG for emotion prediction as a cost-effective and userfriendly approach for consistent and reliable prediction deployment in virtual reality-related content and environments through deep learning approaches.",https://ieeexplore.ieee.org/document/8538382/,2018 International Conference on Smart Computing and Electronic Enterprise (ICSCEE),11-12 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLBDBI54094.2021.00110,Design and Application of Rehabilitation Psychology Practical Teaching System Based on VR Technology,IEEE,Conferences,"Research purposes: The characteristics of VR technology, such as immersion, interaction, construction and sociality, make its application in the field of education have unique advantages. How to make use of the advantages of VR technology to design a functional, purposeful and interesting practical experience teaching in the teaching of Rehabilitation Psychology is an important research direction of psychology curriculum reform. Based on the innovative teaching mode of Rehabilitation Psychology, this paper aims to improve the teaching method and content of the course by using VR virtual reality, so as to provide reference for the reform of teaching mode. Research methods: 122 college students majoring in applied psychology and 85 college students majoring in rehabilitation medicine in our university were selected as the research objects. Based on the latest research results of computer science, psychology and other related subjects, this paper analyzes and compares the teaching scheme design, implementation and results of the practical course of rehabilitation psychology with that of the traditional course of rehabilitation psychology by comprehensively using the methods of literature analysis, teaching action research, teaching experience summary and teaching reflection. Conclusion: The teaching experience system of Rehabilitation Psychology based on VR Virtual Reality is effective, so it is feasible to integrate VR Virtual Reality into the experimental teaching of psychology. This study provide a research basis for exploring innovative teaching mode and teaching reform of applied psychology specialty.",https://ieeexplore.ieee.org/document/9731082/,"2021 3rd International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",3-5 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AINIT54228.2021.00134,Design and Implementation of Virtual Animation Based on Unity3D,IEEE,Conferences,"This project uses Unity3D engine to make VR virtual reality experiment, uses UG NX to model the experiment model, uses Shadow SDK plug-in to make the glasses show binocular effects, particle system can add special effects to the experiment, it is the second rendering of the three-dimensional space Three-dimensional images are mainly used to render different experimental requirements for effects such as smoke, lire, water droplets, fallen leaves, etc., through the use of dotween plug-in in unity3D to make the experiment, the experiment is presented in VR glasses, which greatly promotes VR Teaching development.",https://ieeexplore.ieee.org/document/9725101/,"2021 2nd International Seminar on Artificial Intelligence, Networking and Information Technology (AINIT)",15-17 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLBDBI48998.2019.00034,Design and Implementation of Virtual Campus Roaming System Based on Unity3d,IEEE,Conferences,"The rapid development of VR technology laid the foundation for the emergence of virtual reality systems. The Virtual campus roaming system plays an important role in the process of college informationization. The virtual campus roaming system takes Guangzhou University as an example and it is based on Unity3D and Sketch Up modeling tool. The development of this system uses the latest VR interactive technology, C# language and Steam VR plug-in. The virtual campus roaming system is based on a traditional campus roaming system. In this system, users no longer need to use the mouse and keyboard to operate, but use HTC Vive equipment to drive virtual characters to roam in a virtual campus.",https://ieeexplore.ieee.org/document/8945526/,"2019 International Conference on Machine Learning, Big Data and Business Intelligence (MLBDBI)",8-10 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CompAuto54408.2021.00016,Design and implementation of Virtual characters&#x2019; Expression of interactive storytelling,IEEE,Conferences,"There have been giant leaps in the field of AI storytelling in the last ten years. The AI technology has provided PC gaming more opportunities to improve themselves at their own pace. While most of game virtual engine uses various modules, the sensitive information they collect raises concerns among the player community. As technology seeps into every aspect of PC gaming currently a central problem for AI-generated narrative is the difficulty in balancing the consistency and structure of the plot.This paper aims to discuss Propp&#x2019;s theory analyzed Russian folklore enabled him to develop a total of 31 ""functions"" for interactive narrative in role-playing games, deeply discuss its realization, mechanism, and the new experiences of players in cognition and aesthetic. We therefore addressed two primary research questions. Existing architectures in virtual reality game engine system have been listed out. This section reviews the core components of story generation and presentation in AI- and computer-related storytelling. The following section focuses on outlines links between the algorithms deployed in AI-storytelling systems and the formalism of ""traditional"" narrative analysis and what the similarities and distinctions can tell us about the possibility of developing and enhancing AI-produced interactive stories.",https://ieeexplore.ieee.org/document/9726818/,2021 International Conference on Computers and Automation (CompAuto),7-9 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWECAI55315.2022.00065,Design of VR Experimental System Based on Leap Motion Gesture Recognition,IEEE,Conferences,"Virtual Reality (VR) has gradually developed into a new type of means and resources, especially in the field of experimental teaching. This paper will start from the perspective of virtual simulation, based on virtual reality technology and gesture recognition technology, combined with actual electrical experiments Application scenarios, using Maya software, next-generation game engine Unreal Engine 4 (UE4) and deep learning algorithms to design and implement a set of low investment costs, flexible experiment development, high experiment safety factor and strong simulation interaction VR experimental system based on Leap Motion gesture recognition. Including the creation of the 3D model and detailed processing of the VR electrical experiment system based on Maya software, the scene construction and blueprint logic implementation of the VR electrical experiment system based on the UE4 engine, and using python programming to complete the development of the gesture recognition algorithm based on Deep Neural Networks(DNN).",https://ieeexplore.ieee.org/document/9750795/,2022 3rd International Conference on Electronic Communication and Artificial Intelligence (IWECAI),14-16 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IS.2018.8710522,Designing Human Brain Interface Model for Interactive Cognitive Learning in an Immersive System with Neurofeedback,IEEE,Conferences,"In this article we will present an interactive model linking the relationship involving the change of human behavior and learning through a specific situation, the lived experience, the emotions felt and the reasoning. An immersive system (virtual reality) with neurofeedback information's is used to quantity the changing states. The model exposes a stimulus-servoing mechanism to produce emotions that generate a reminder of virtual-life situations, which leads to the availability of knowledge that can solve a given problem. The return emotions are detected by a neurofeedback system allowing the model to change the virtual scenario by changing the intensity or type of stimuli to reproduce other emotions or increase their intensity. This model makes it possible to adapt the scenario to the user in order to allow him to evolve into a problematic situation with his own skills and expertise acquired at the learner's pace.",https://ieeexplore.ieee.org/document/8710522/,2018 International Conference on Intelligent Systems (IS),25-27 Sept. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTERCON.2019.8853573,Development of a hand pose recognition system on an embedded computer using Artificial Intelligence,IEEE,Conferences,"The recognition of hand gestures is a very interesting research topic due to the growing demand in recent years in robotics, virtual reality, autonomous driving systems, human-machine interfaces and in other new technologies. Despite several approaches for a robust recognition system, gesture recognition based on visual perception has many advantages over devices such as sensors, or electronic gloves. This paper describes the implementation of a visual-based recognition system on a embedded computer for 10 hand poses recognition. Hand detection is achieved using a tracking algorithm and classification by a light convolutional neural network. Results show an accuracy of 94.50%, a low power consumption and a near real-time response. Thereby, the proposed system could be applied in a large range of applications, from robotics to entertainment.",https://ieeexplore.ieee.org/document/8853573/,"2019 IEEE XXVI International Conference on Electronics, Electrical Engineering and Computing (INTERCON)",12-14 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCGI.2008.31,Distributed Intelligent System for Personalized Therapy of Speech Disorders,IEEE,Conferences,"The aim of this paper is to describe an intelligent system designed for assisting the personalized therapy of dyslalia for the Romanian prescholars children. This system is developed in the frame of TERAPERS project that can be included in a very important research area: informational technologies in response to society challenges - for health: early diagnosis, personalized therapy. Taking into consideration the fact that Romanian language is a phonetic one that has its own special linguistic particularities, there is a real need for the development and use of audio-video systems, which can be used in the therapy of different speech problems. The system has a high degree of originality because his objective is to treat the pronunciation disorders in Romanian language. Also the complexity of the project results from the high number of different research areas involved: artificial intelligence, virtual reality, digital signal processing, digital electronic and psychology.",https://ieeexplore.ieee.org/document/4591364/,2008 The Third International Multi-Conference on Computing in the Global Information Technology (iccgi 2008),27 July-1 Aug. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSN53354.2021.00089,Distributed Task Offloading based on Multi-Agent Deep Reinforcement Learning,IEEE,Conferences,"Recent years have witnessed the increasing popularity of mobile applications, e.g., virtual reality, unmanned driving, which are generally computation-intensive and latency-sensitive, posing a major challenge for resource-limited user equipment (UE). Mobile edge computing (MEC) has been proposed as a promising approach to alleviate the problem, by offloading mobile tasks to the edge server (ES) deployed in close proximity to UE. However, most existing task offloading algorithms are primarily based on centralized scheduling, which could suffer from the &#x2018;curse of dimensionality&#x2019; in large MEC environments. To address this issue, this paper proposes a fully distributed task offloading approach based on multi-agent deep reinforcement learning, whose critic and actor neural networks are trained under the assistance of global and local network states, respectively. In addition, we design a model parameter aggregation mechanism, along with a normalized fine-tuned reward function, to further improve the learning efficiency of the training process. Simulation results show that our proposed approach could achieve substantial performance improvements over baseline approaches.",https://ieeexplore.ieee.org/document/9751590/,"2021 17th International Conference on Mobility, Sensing and Networking (MSN)",13-15 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2018.8545831,Dynamic Facial Expression Synthesis Driven by Deformable Semantic Parts,IEEE,Conferences,"Dynamic facial expression synthesis has some wild applications in human-computer interaction and virtual reality. The popular data-driven synthesis method like generative adversarial network (GAN) has made a great progress in generating a single face image, but has not well performed for expression sequences. To solve this problem, we design a series of deformable semantic parts to represent facial geometrical movement. And we synthesize the facial appearance by the geometrical driven under the-state-of-art pix2pixHD framework. In order to maintain the person identity among image sequence, we utilize an encoder to constrain the attributes of target face. With the above efforts, our method is capable to synthesize satisfied dynamic facial expression sequences.",https://ieeexplore.ieee.org/document/8545831/,2018 24th International Conference on Pattern Recognition (ICPR),20-24 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAI52893.2021.9639679,E-Learning - the practice in industrial enterprises,IEEE,Conferences,"The dynamic development of technology in today's digital world entails changes in the business models, thus predetermining a radical turn in human resource management (HRM). We witness the constant readjustment of the functions of HRM professionals due to the creation and implementation of new technologies. Among the most currently used technologies are: E-Learning, social networks, mobile technologies, learning management systems, virtual reality, chat bots, gamification and artificial intelligence. The authors explore in this report the E-learning practices in the industrial enterprises. Based on their survey, they draft conclusions and provide methodological guidelines for overcoming the weaknesses in the process of online training of the personnel in the industrial enterprises.",https://ieeexplore.ieee.org/document/9639679/,2021 International Conference Automatics and Informatics (ICAI),30 Sept.-2 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCONS.2018.8662987,EMG Based Gesture Recognition Using Machine Learning,IEEE,Conferences,"Gesture recognition basically involves the usage of hardware equipments and software development tools where human movements are captured and Human Computer Interaction are improved. Gesture recogntion can be employed in various applications like gaming technology, virtual reality, in the domain of medicine, sign language interpretation, home automation etc. This work basically focuses on EMG based gesture recogntion taken in real time using Myoband. The Myoband is worn on the forearm and electrical impulses are measured for five gestures namely Rest, Fist, Wavein, Waveout and Fingerspread. These signals undergo a signal processing technique called Wavelet decomposition where the signals are fragmented into wavelet coefficients that are localised in time domain as well as in frequency domain. These coefficients make up the dataset and are classified using Support Vector Machines. When a user poses for a particular gesture, the model would recognises it and labels accordingly.",https://ieeexplore.ieee.org/document/8662987/,2018 Second International Conference on Intelligent Computing and Control Systems (ICICCS),14-15 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCI-CC.2017.8109760,Early diagnosis of mild cognitive impairment: A case study in approaches to inductive-logic programming,IEEE,Conferences,"Recent rapid advances in data collection routines in clinical science have led to a trend of storing patient data in a heterogeneous database. The lack of existing computing tools to enable operability to use machine learning on these heterogeneous data sources is a barrier to the healthcare sciences. Healthcare data is usually complex and highly context-dependent, and it requires modern computational tools to handle the complexity of such data. This study sought to utilize the data collected from virtual reality (VR)-based software and a leap-motion device used for learning in mild cognitive impairment (MCI) cases to enable early detection of MCI by analyzing the classification rules for errors (action slips) based on finger-action transitions when performing instrumental activities of daily living (IADL). Finger motion was recorded as a time-series database. An induction technique known as Inductive-Logic Programming (ILP), which uses logical and clausal language to represent the training data, was then used to discover a concise classification rule using logical programming. We were able to generate rules on how action transitions of the finger in the experiments were related to the pattern of micro-errors that indicate the difference of error regarding the length of the no-motion state of the finger.",https://ieeexplore.ieee.org/document/8109760/,2017 IEEE 16th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),26-28 July 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CNSM52442.2021.9615539,Efficient Orchestration of Service Chains in Fog Computing for Immersive Media,IEEE,Conferences,"Immersive media services, such as Augmented and Virtual Reality (AR/VR) are getting significant attention in recent years with the promise of bringing immersive experiences to end users. However, despite the remarkable advances in the field, AR/VR applications are mostly local and individual experiences. The main obstacle between current technology and future remote, multi-user AR/VR applications is the stringent end-to-end (E2E) latency requirement, which cannot exceed 20 ms to avoid motion sickness. Emerging AR/VR services put even more pressure on current network infrastructures, calling for considerable advancements toward fully cloud-native architectures. Cloud-based VR services, where participants can virtually interact across vast distances, remain a distant dream. Several challenges still arise concerning the deployment and management of VR services. This paper presents a Mixed-Integer Linear Programming (MILP) formulation for the efficient orchestration of VR services in fog-cloud infrastructures. The model considers Fog Computing (FC), an extension of cloud computing, and Segment Routing (SR), which leverages the source routing paradigm. The evaluation of realistic VR container-based service chains shows that deploying VR components hosted in a fog-cloud infrastructure can satisfy the 20 ms latency boundary.",https://ieeexplore.ieee.org/document/9615539/,2021 17th International Conference on Network and Service Management (CNSM),25-29 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI49370.2019.00077,"Embedded or Integrated, Autonomous Intelligent Monitoring Architecture and Framework for Training Assistance and Automation",IEEE,Conferences,"A key element to readiness lies in the ability to provide qualified trainers and role player personnel, a challenge in a budget constrained environment. The goal of this effort is to employ practical artificial intelligence (AI) and automation techniques to minimize staffing requirements for training, and to improve the quality and pace of training. This paper outlines implementation experiments and results of these techniques to interoperate multiple AI algorithm types in an architecture and framework to automate certain aspects of training systems. A core capability the ability to integrate into a variety of training systems, from embedded and virtual reality simulations to constructive wargames, via a user-friendly system of APIs.",https://ieeexplore.ieee.org/document/9070974/,2019 International Conference on Computational Science and Computational Intelligence (CSCI),5-7 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVR51878.2021.9483833,Emotion Recognition from Body Movements with AS-LSTM,IEEE,Conferences,"With the development of artificial intelligence, people's demand for emotional interaction in virtual reality experience is becoming higher and higher. When the traditional emotion recognition method is used in virtual reality emotion recognition, it has some problems, such as tedious wearing, high demand on image clarity, inaccurate emotion recognition in motion, etc. Therefore, we propose a stack LSTM network based on attention (AS-LSTM) for emotion recognition from whole body movements in VR environment. According to the importance degree, different attention values are set for the feature sequences data of each joint point in the frame sequence of human body motion by adding attention mechanism to the basis of traditional LSTM network, which will build a particular distribution of attention, focus on the key joint points affecting emotion recognition, and reduce the invalid information. Then the proposed method can improve the learning ability of network and emotion recognition accuracy. Moreover, the equipment is simple and easy to operate, which provides users a more immersive emotional interaction experience. One can observe, this method achieves higher recognition accuracy on classification of seven kinds of emotions (happy, sad, fear, anger, surprised and disgust) compared with other deep learning methods in VR. In addition, the accuracy of emotion recognition in six categories (happy, sad, fear, anger, surprise, disgust) and four categories (happy, sad, fear, and anger) is also improved.",https://ieeexplore.ieee.org/document/9483833/,2021 IEEE 7th International Conference on Virtual Reality (ICVR),20-22 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSPNET51692.2021.9419475,Emotion based Media Playback System using PPG Signal,IEEE,Conferences,"The study involved identifying human emotions and integrates the identified emotion with the music system. The idea is to develop a complete product to utilize the detected emotion in a real-time application and also to achieve more accuracy and less memory. Human emotions are identified using physiological signals such as electrocardiography, electromyography, photoplethysmography, respiration, skin temperature, etc. Obtaining photoplethysmography (PPG) from the sensor is a simple, cost-effective, and non-invasive method. PPG sensors are capable of providing accurate heart-rate (HR) by detecting the variations in the blood flow. Signals are acquired using wearable technology from a personal whereas not compromising comfort and privacy. The attributes of Heart Rate Variability are analyzed to describe emotions, namely happy, calm, unhappy (sad), and fear using Machine learning technology. We deployed this recognized emotion to automate the music system associated with its emotion. To bring this, we built an Android app to communicate with the smart wearable utilized. Totally 150 members from both genders have participated. The accuracy of 91.81% is achieved. This emotion recognition system can be used in various fields like robotics, medicine, virtual reality, and gaming, advertising, education, automotive working conditions and safety, home appliances.",https://ieeexplore.ieee.org/document/9419475/,"2021 Sixth International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET)",25-27 March 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEAMS51251.2021.00015,Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and VR Interfaces,IEEE,Conferences,"Self-adaptation approaches usually rely on closed-loop controllers that avoid human intervention from adaptation. While such fully automated approaches have proven successful in many application domains, there are situations where human involvement in the adaptation process is beneficial or even necessary. For such “human-in-the-loop” adaptive systems, two major challenges, namely transparency and controllability, have to be addressed to include the human in the self-adaptation loop. Transparency means that relevant context information about the adaptive systems and its context is represented based on a digital twin enabling the human an immersive and realistic view. Concerning controllability, the decision-making and adaptation operations should be managed in a natural and interactive way. As existing human-in-the-loop adaptation approaches do not fully cover these aspects, we investigate alternative human-in-the-loop strategies by using a combination of digital twins and virtual reality (VR) interfaces. Based on the concept of the digital twin, we represent a self-adaptive system and its respective context in a virtual environment. With the help of a VR interface, we support an immersive and realistic human involvement in the self-adaptation loop by mirroring the physical entities of the real world to the VR interface. For integrating the human in the decision-making and adaptation process, we have implemented and analyzed two different human-in-the-loop strategies in VR: a procedural control where the human can control the decision making-process and adaptations through VR interactions (human-controlled) and a declarative control where the human specifies the goal state and the configuration is delegated to an AI planner (mixed-initiative). We illustrate and evaluate our approach based on an autonomic robot system that is accessible and controlled through a VR interface.",https://ieeexplore.ieee.org/document/9462035/,2021 International Symposium on Software Engineering for Adaptive and Self-Managing Systems (SEAMS),18-24 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HAVE.2007.4371594,Entertainment Oriented Intelligent Virtual Environment with Agent and Neural Networks,IEEE,Conferences,"Intelligent virtual environment (IVE) combines virtual reality and artificial intelligence (A I), i.e. It incorporates AI technology into virtual environment, which makes the virtual environment more interactive and credible. Using IVE concept, we design and implement an entertainment IVE application platform with intelligent agents and neural networks. The humanlike intelligent objects in the virtual environment are implemented with intelligent agents, which are endowed with intelligence for self-learning and collaborating with each other by applying BP and ESP neural network to the agent controllers.",https://ieeexplore.ieee.org/document/4371594/,"2007 IEEE International Workshop on Haptic, Audio and Visual Environments and Games",12-14 Oct. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00055,Exploring the possibilities of Extended Reality in the world of firefighting,IEEE,Conferences,"Firefighting is a crucial part of the Navy’s training program, as it must ensure the safety on board. This training is dangerous, expensive and environmentally unfriendly. Therefore, the Navy is looking for a safer form of training that can enhance the current one. Extended Reality technology offers new ways of training, with the promise to alleviate issues related to training danger, costs and environmental pollution. In this work, we develop and evaluate a Virtual Reality simulator and a proof of concept of a Mixed Reality simulator, together with a firehose controller adapted to the needs of the Navy’s firefighting training program.",https://ieeexplore.ieee.org/document/9319108/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00083,Eye Tracking Data Collection Protocol for VR for Remotely Located Subjects using Blockchain and Smart Contracts,IEEE,Conferences,"Eye tracking data collection in the virtual reality context is typically carried out in laboratory settings, which usually limits the number of participants or consumes at least several months of research time. In addition, under laboratory settings, subjects may not behave naturally due to being recorded in an uncomfortable environment. In this work, we propose a proof-of-concept eye tracking data collection protocol and its implementation to collect eye tracking data from remotely located subjects, particularly for virtual reality using Ethereum blockchain and smart contracts. With the proposed protocol, data collectors can collect high quality eye tracking data from a large number of human subjects with heterogeneous socio-demographic characteristics. The quality and the amount of data can be helpful for various tasks in datadriven human-computer interaction and artificial intelligence.",https://ieeexplore.ieee.org/document/9319118/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMULT.2010.5629720,Framework and Key Technologies for the Construction of a Virtual Mine,IEEE,Conferences,"The application of virtual reality technology is a trend that holds considerable promise as demonstrated in many applications. This technology includes the means to visually integrate and interact with diverse multidimensional data in 3D and time context, extend real-time decision-making into mine maintenance, implement virtual mine-planning simulations, training simulations, and risk management. Virtual mine is the result of integrating various disciplines including mine science, information science, artificial intelligence, computer science and 3S techniques, which will radically change the traditional mine production and our lifestyles. The functions and main characters of virtual mine are analyzed. A framework for a virtual mine operation system is provided. Key techniques concerned in the implementation of virtual mine are discussed in detail. The future development of virtual mine is prospected.",https://ieeexplore.ieee.org/document/5629720/,2010 International Conference on Multimedia Technology,29-31 Oct. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAML54311.2021.00056,Gymnasium Simulation Design and Implementation Based on 3D Virtual Building,IEEE,Conferences,"Based on the three-dimensional virtual building-simulation design and implementation method of the gymnasium, when the building space is subjected to virtual reality and simulation design, based on the constructed building space coordinate system and scale, the position and parameters of the building are used to construct a mathematical model of the building space component., The mathematical model of each component is integrated to build a mathematical model of the overall building space. OpenGL virtual reality technology is used to expand the target building based on the mathematical model of the target building space. The target building is given materials and texture characteristics to obtain the ideal 3D virtual view of the building space. The 3D virtual view of the building space is 3D rendered and displayed. The vivid 3D virtual renderings of the building space are animated using animation design technology. The experimental results show that the proposed method has good point-line rendering and overall rendering effect, can obtain more realistic 3D virtual building based gym simulation design results, and has high interactivity and practicability.",https://ieeexplore.ieee.org/document/9712032/,2021 3rd International Conference on Applied Machine Learning (ICAML),23-25 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAT.2006.68,Hand Gesture Interaction for Virtual Training of SPG,IEEE,Conferences,"We develop a virtual reality based driving training system of self-propelled gun (SPG). In order to make the interface of the system more powerful and natural, hand gesture interaction need to be incorporated into the system's interface. This paper discusses the use of hand gestures for interaction with the virtual training environment. We employ static hand gestures which coupled with hand translations and rotations as the method of interacting with the virtual training environment. An 18-sensor data glove is chosen for monitoring the movements of the fingers and the wrist. The feed-forward neural network is developed for recognizing gestures for use in virtual training application of artillery self-propelled gun (SPG). We present our approach for the algorithm design and implementation, and the use of the gestures in our application. The presented hand gesture interaction method can be effectively used in our virtual reality training system of SPG to perform various manipulating tasks in a more fast, precise, and natural way",https://ieeexplore.ieee.org/document/4089336/,16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06),29 Nov.-1 Dec. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCA51647.2021.00016,Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,IEEE,Conferences,"Emerging AI-enabled applications such as augmented and virtual reality (AR/VR) leverage multiple deep neural network (DNN) models for various sub-tasks such as object detection, image segmentation, eye-tracking, speech recognition, and so on. Because of the diversity of the sub-tasks, the layers within and across the DNN models are highly heterogeneous in operation and shape. Diverse layer operations and shapes are major challenges for a fixed dataflow accelerator (FDA) that employs a fixed dataflow strategy on a single DNN accelerator substrate since each layer prefers different dataflows (computation order and parallelization) and tile sizes. Reconfigurable DNN accelerators (RDAs) have been proposed to adapt their dataflows to diverse layers to address the challenge. However, the dataflow flexibility in RDAs is enabled at the cost of expensive hardware structures (switches, interconnects, controller, etc.) and requires per-layer reconfiguration, which introduces considerable energy costs. Alternatively, this work proposes a new class of accelerators, heterogeneous dataflow accelerators (HDAs), which deploy multiple accelerator substrates (i.e., sub-accelerators), each supporting a different dataflow. HDAs enable coarser-grained dataflow flexibility than RDAs with higher energy efficiency and lower area cost comparable to FDAs. To exploit such benefits, hardware resource partitioning across sub-accelerators and layer execution schedule need to be carefully optimized. Therefore, we also present Herald, a framework for co-optimizing hardware partitioning and layer scheduling. Using Herald on a suite of AR/VR and MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which demonstrates 65.3% lower latency and 5.0% lower energy compared to the best fixed dataflow accelerators and 22.0% lower energy at the cost of 20.7% higher latency compared to a state-of-the-art reconfigurable DNN accelerator (RDA). The results suggest that HDA is an alternative class of Pareto-optimal accelerators to RDA with strength in energy, which can be a better choice than RDAs depending on the use cases.",https://ieeexplore.ieee.org/document/9407116/,2021 IEEE International Symposium on High-Performance Computer Architecture (HPCA),27 Feb.-3 March 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2005.1492796,High-level semantics representation for intelligent simulative environments,IEEE,Conferences,This article describes an integration of knowledge based techniques into simulative virtual reality (VR) applications motivated using a virtual construction task. An abstract knowledge representation layer (KRL) provides a base formalism for the integration of simulation semantics. The KRL approach is demonstrated using a generalized scene graph representation which introduces an abstract definition and implementation of geometric node interrelations.,https://ieeexplore.ieee.org/document/1492796/,"IEEE Proceedings. VR 2005. Virtual Reality, 2005.",12-16 March 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.15439/2017F253,Human machine synergies in intra-logistics: Creating a hybrid network for research and technologies,IEEE,Conferences,"The purpose of the article is to outline the futuristic vision of Industry 4.0 in intra-logistics by creating a hybrid network for research and technologies thereby providing a detailed account on the research centre, available technologies and their possibilities for collaboration. Scientific challenges in the field of Industry 4.0 and intra-logistics are identified due to the new form of interaction between humans and machines. This kind of collaboration provides new possibilities of materials handling that can be developed with the support of real-time motion data tracking and virtual reality systems. These services will be provided by a new research centre for flexible human-machine cooperation networks in Dortmund. By the use of various reference and experiment systems various real-time scenarios can be emulated including digital twin simulation concepts. Big data emerges as an important paradigm in this research project where all systems are made flexible in terms of networking for all the systems to consume the data produced and also to combine all the data to arrive at new insights using concepts from machine learning and deep learning networks. This leads to the challenge of finding a common syntax for inter-operating systems. This paper describes the design and deployment strategies of research centre with the possibilities and the design insights for a futuristic Industry 4.0 material handling facility.",https://ieeexplore.ieee.org/document/8104684/,2017 Federated Conference on Computer Science and Information Systems (FedCSIS),3-6 Sept. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETAS.2018.8629202,IOS Mobile Application for Food and Location Image Prediction using Convolutional Neural Networks,IEEE,Conferences,"Machine Learning is a popular research area in software industry alongside with big data, micro services, virtual reality, and augmented reality. With the recent developments in improving computing capacity, deep learning approaches such as Convolutional Neural Networks (CNN) has become the trendiest topic in machine learning for image recognition. In this paper, we have developed an IOS application for food image recognition using modified CNN models. In particular, we developed an IOS mobile application by converting the models to CoreML and then using them within the IOS application for food image recognition. After fine-tuning a pre-trained Google InceptionV3 model, we were able to achieve 82.03% Top-1 accuracy on the test set using a single crop per item. Using 10 crops per example and taking the most frequent predicted class(es), we were able to achieve 86.97% Top-1 Accuracy and 97.42% Top-5 Accuracy.",https://ieeexplore.ieee.org/document/8629202/,2018 IEEE 5th International Conference on Engineering Technologies and Applied Sciences (ICETAS),22-23 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ITUK50268.2020.9303205,Immersive Technologies for Development: An Analysis of Agriculture,IEEE,Conferences,"Agricultural development is key to any economic development. Immersive technology plays a catalytic role and offers smart and sustainable choices to farmers who want to improve on agricultural productivity, and to agricultural training institutes that seek to use modern technology to advance pedagogy and reduce fatalities and operating costs in the learning space, among others. Currently, there are limited descriptive literature reviews in the area of immersive technology in agriculture, hereafter referred to as AVR-Agric. This paper presents a systematic literature review (SLR), which offers a structured, methodical, and rigorous approach to the understanding of the trend of research in AVR-Agric, and the least and most researched issues. This study explores and examines the current trends in the immersive technology-based agriculture areas, and provides a credible intellectual guide for future research direction. The SLR was limited to existing applications and peer-reviewed conference and journal articles published from 2006 to 2020. The results showed that virtual reality was implemented in 41% of the papers reviewed, augmented reality was found in 53%, while only 6% considered mixed-reality applications. The study also showed that developments that incorporate IoT, blockchain, and machine-learning technologies are still at their stage of exploration and advancement.",https://ieeexplore.ieee.org/document/9303205/,2020 ITU Kaleidoscope: Industry-Driven Digital Transformation (ITU K),7-11 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2019.00123,Implementation of Mobile-Based Real-Time Heart Rate Variability Detection for Personalized Healthcare,IEEE,Conferences,"The ubiquity of wearable devices together with areas like internet of things, big data and machine learning have promoted the development of solutions for personalized healthcare that use digital sensors. However, there is a lack of an implemented framework that is technically feasible, easily scalable and that provides meaningful variables to be used in applications for translational medicine. This paper describes the implementation and early evaluation of a physiological sensing tool that collects and processes photoplethysmography data from a wearable smartwatch to calculate heart rate variability in real-time. A technical open-source framework is outlined, involving mobile devices for collection of heart rate data, feature extraction and execution of data mining or machine learning algorithms that ultimately deliver mobile health interventions tailored to the users. Eleven volunteers participated in the empirical evaluation that was carried out using an existing mobile virtual reality application for mental health and under controlled slow-paced breathing exercises. The results validated the feasibility of implementation of the proposed framework in the stages of signal acquisition and real-time calculation of heart rate variability (HRV). The analysis of data regarding packet loss, peak detection and overall system performance provided considerations to enhance the real-time calculation of HRV features. Further studies are planned to validate all the stages of the proposed framework.",https://ieeexplore.ieee.org/document/8955523/,2019 International Conference on Data Mining Workshops (ICDMW),8-11 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iSES52644.2021.00076,Implementation of Real-Time One Degree of Freedom Haptic Device Using FPGA,IEEE,Conferences,"Virtual Reality (VR) and Artificial Intelligence (AI) are growing interests in research due to rapid advancements in technology. Fast computation and optimum power requirements are predicted to be vital elements for future technology devices. Field Programmable Gate Arrays (FPGAs) can work as a key element to fulfill this requirement. Also, it&#x2019;s been a new research area a decade back to simulate touch, smell, and taste. The mathematical model for sensory perception has been developed for touch, but not for Nose (smell) and tongue (taste). The field of Haptics is related to simulating the touch. Touch is the most prominent sense to perceive the real world. A sense of touch gives the feeling of the physical properties of the object. Over the last decades, many theories have been put forward to simulate the touch artificially in computers to feel the virtual world (objects) physical properties. The evolvement of a cost-effective real-time One Degree of Freedom Haptic device with minimum hardware resources is aimed at this paper. The system is a build-up of three phases: System Hardware, System Software, and incorporating both in FPGA as a complete closed-loop solution. The feel of touch-force computation and rendering of graphics is simulated on a single FPGA board.",https://ieeexplore.ieee.org/document/9701133/,2021 IEEE International Symposium on Smart Electronic Systems (iSES),18-22 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3ICT53449.2021.9581386,Implementation of a WGAN-GP for Human Pose Transfer using a 3-channel pose representation,IEEE,Conferences,"The computational problem of Human Pose Transfer (HPT) is addressed in this paper. HPT in recent days have become an emerging research topic which can be used in fields like fashion design, media production, animation, virtual reality. Given the image of a human subject and a target pose, the goal of HPT is to generate a new image of the human subject with the novel pose. That is, the pose of the target pose is transferred to the human subject. HPT has been carried out in two stages. In stage 1, a rough estimate is generated and in stage 2, the rough estimate is refined with a generative adversarial network. The novelty of this work is the way pose information is represented. Earlier methods used computationally expensive pose representations like 3D DensePose and 18-channel pose heatmaps. This work uses a 3-channel colour image of a stick figure to represent human pose. Different body parts are encoded with different colours. The convolutional neural networks will now have to recognize colours only, and since these colours encode body parts, eventually the network will also learn about the position of the body parts.",https://ieeexplore.ieee.org/document/9581386/,"2021 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",29-30 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCT50599.2020.9351418,Improve Teaching and Learning Approach 3D Primitives with Virtual and Augmented Reality,IEEE,Conferences,"This article offers with the design computer virtual reality (VR) and augmented reality (AR) interfaces have demonstrated the possibility to improve instructing and learning, by joining physical and virtual universes and utilizing the benefits of both. This interactive virtual learning is very convenient and effective. Today, it offers great opportunities to improve the quality of education by creating new methods and techniques Traditionalist methods of substance introduction (fixed video, sound, contents) need personalization and communication. It illustrates the positive impact of these techniques in present learning scenario and studies how these techniques are being adopted by MOOC to generate interactive and more engaging content. Finally, we create internal avatar implication 3D model by using software while using in virtual reality (VR) and augmented reality (AR) interfaces.",https://ieeexplore.ieee.org/document/9351418/,2020 International Conference on Information Science and Communications Technologies (ICISCT),4-6 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICA50127.2020.9182590,Improvement of Model Simplification Algorithm Based on LOD and Implementation of WebGL,IEEE,Conferences,"With the continuous progress of computer graphics, virtual reality and other technologies, 3D models in the field of industrial production have become more and more sophisticated, and the triangular surfaces need to be rendered are more than one million, which poses a great challenge to the storage, transmission and computing power of computers. Therefore, in order to adapt to the current performance of the computer while taking into account the rendering effect of the model, the Level-of-Detail(LOD) technology has been spawned. Industrial models tend to have complex structures and need to be displayed accurately in the rendering process, and in the case of a large number of holes in the model, the common algorithm is difficult to maintain the topology of the model well. Therefore, the article uses the edge collapse algorithm. To improve it, an algorithm that uses the mean deviation to guide the simplification process of edge collapse is proposed. While ensuring sufficient triangular mesh simplification, the topology of complex industrial models is maintained.",https://ieeexplore.ieee.org/document/9182590/,2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA),27-29 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIBA50161.2020.9277200,Improving Mobile Network Performance with Mobile Edge Computing,IEEE,Conferences,"Most cloud-based applications utilize a cloud server to handle data produced by various terminal devices, including smart devices and wearable devices. The above model has big negative effect on user experience. Furthermore, the emergence of some new computing applications, including automatic drive and virtual reality, has become possible because of availability of abundant cloud resources. Nonetheless, the delay-sensitive applications have strict response time requirements. The existing computing model does not have capability to meet the above demand. Based on the above phenomenon, Mobile Edge Computing (MEC) has been suggested to bring cloud resources, such as computation and storage resources, to the edge network. Now, MEC is considered as a good method to reduce the workload on the cloud server and to reduce response time for mobile users. In this paper, the definition of the MEC is presented. Then, we discuss several typical applications of MEC. Also, we describe some key research challenges in MEC scenario. In the end, we highlight the critical points and conclude this paper.",https://ieeexplore.ieee.org/document/9277200/,"2020 IEEE International Conference on Information Technology,Big Data and Artificial Intelligence (ICIBA)",6-8 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MEES.2019.8896394,Increase in Use Values of Future Engineers through the Education Technology Trends Implementation,IEEE,Conferences,"The paper deals with the ways to increase the use values of future engineers. The forecast of the most sought after educational technologies are considered Global Education Market Expenditure was identified. An overview of innovative educational trends has been made such as Blockchain, Virtual Reality and Augmented reality, artificial intelligence and 3D printing. The state of implementation of technological trends in the Ukrainian educational process was analyzed.",https://ieeexplore.ieee.org/document/8896394/,2019 IEEE International Conference on Modern Electrical and Energy Systems (MEES),23-25 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/JCSSE53117.2021.9493841,Intelligent Clinical Training during the COVID-19 Pandemic,IEEE,Conferences,"Clinical training is one of the most challenging areas for education especially during the COVID-19 pandemic. There are limited access to apprenticeship training in the complex scenarios with corresponding difficulty training in a time-effective manner. Our work on intelligent clinical training systems provides one effective solution to this problem by introducing intelligent clinical training systems that can supplement tutoring sessions by expert clinical instructors. The Bayesian representation techniques and algorithms for generating tutoring feedback in medical problem-based learning group problem solving made important contributions to the field of Intelligent Tutoring Systems. In particular, it was one of the first systems for tutoring groups of students and the first intelligent tutoring systems for medical problem-based learning. The virtual reality simulator we developed is one of the most sophisticated dental simulators. It stands out as the first dental simulator to integrate sophisticated analysis of the surgical procedure. Particularly noteworthy is also the creative way to understand important issues such as differences in expert and novice performance, the effectiveness of virtual pre-operative practice, and the teaching effectiveness of the simulator. The systems have been implemented in undergrad pre-clinical training and postgrad pre-surgical training with strong scientific evidence of their effectiveness.",https://ieeexplore.ieee.org/document/9493841/,2021 18th International Joint Conference on Computer Science and Software Engineering (JCSSE),30 June-2 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EGUK.2002.1011272,Intelligent self-learning characters for computer games,IEEE,Conferences,"In this paper, a novel AI-based animation approach is presented to simulate intelligent self-learning characters for computer games or other interactive virtual reality applications. The complex learning behaviours of the virtual characters are modelled as an evolutionary process so that adaptive AI algorithms such as genetic algorithms have been used to simulate the learning process. The simulation method enables the characters in a computer game environment to have abilities to learn for specific assigned tasks. Its skill for completing the tasks can be developed and evolved through its experiences of performing the tasks. The paper also describes techniques for performance evaluation and optimisation for virtual characters to perform jumping tasks.",https://ieeexplore.ieee.org/document/1011272/,Proceedings 20th Eurographics UK Conference,11-13 June 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HORA55278.2022.9800084,Interaction Design for AI Systems: An oriented state-of-the-art,IEEE,Conferences,"In recent years, new web-based technologies have emerged, and mobile devices and applications access have become widespread, resulting in new paradigms in Human-Computer Interaction (HCI). These paradigms created several challenges in interaction design regarding customization, adaptability, and accessibility. Over the lifetime of a digital product, the user must adapt to updates, changes in the interface, and new functions, while having to provide attention to customization and configuration tasks in interfaces created for millions of individuals worldwide. Simultaneously, users&#x0027; conditions change, and the system must adjust to new requirements, preferences, and needs in a fast-paced digital environment. And new functions, interfaces, and technologies that meet current market directions, such as Artificial Intelligence, Augmented Reality, Virtual Reality, and even Design without physical interfaces must be developed and implemented. This research work explores the importance of Interaction Design (IxD) in the present and future Artificial Intelligence (AI) systems, highlighting the prominent authors of User-Centered Design and usability principles, guidelines, and heuristics. Moreover, researching appropriate design principles for developing a positive, effective, and safe interaction between humans and computers. This study will also deepen and orient the study of the state-of-the-art to promote the exploration of HCI in a period of enormous challenges and opportunities in AI. As a result of this research work, a table summarizes, compares, and classifies the state of the art according to Usability &#x0026; Design Principles. Outputting a matrix that aggregates the fundamental principles for designing interfaces in artificial intelligence systems, regardless of their interface. This matrix serves as a base for a framework to create a prototype, in future work, based on the guidelines suggested.",https://ieeexplore.ieee.org/document/9800084/,"2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA)",9-11 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VRAIS.1993.378267,Interactive collision detection,IEEE,Conferences,"Collision detection and response can make a virtual-reality application seem more believable. Unfortunately, existing collision-detection algorithms are too slow for interactive use. The authors present a new algorithm that is not only fast but also interruptible, allowing an application to trade quality for more speed. The algorithm uses simple four-dimensional geometry to approximate motion, and sets of spheres to approximate three-dimensional surfaces. The algorithm allows a sample application to run five to seven times faster than it runs with existing algorithms.<>",https://ieeexplore.ieee.org/document/378267/,Proceedings of 1993 IEEE Research Properties in Virtual Reality Symposium,25-26 Oct. 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIBGRA.1998.722751,Interface: a real time facial animation system,IEEE,Conferences,"The paper describes Interface, a real time facial animation system. The system uses a simple set of pre modeled facial expressions to create a wide range of emotions, mouth positions and complete head movements. The animation is done through layered groups of actions that combined gives the virtual actor complete freedom to perform different actions independently and simultaneously, without the intervention of the animator. Due to its software structure, Interface can be used as a ""render plug-in"" for an artificial intelligence program or a script based animation system. The system was successfully implemented using the languages Java and VRML, and can be executed through the Internet in PCs or workstations with a Java-enabled Web browser. Applications that can benefit from this system are interface agents, virtual reality systems and animation software, among others.",https://ieeexplore.ieee.org/document/722751/,"Proceedings SIBGRAPI'98. International Symposium on Computer Graphics, Image Processing, and Vision (Cat. No.98EX237)",20-23 Oct. 1998,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMCA.2006.133,International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce - Title,IEEE,Conferences,"The following topics are dealt with: intelligent agents and ontologies; data mining, knowledge discovery and decision making; intelligent systems; Web technologies and Web services; virtual reality and games; image processing and image understanding techniques; adaptive control and automation; modelling, prediction and control; multi-agent systems and computational intelligence; agent systems, personal assistant agents and profiling; fuzzy systems for industrial automation; control strategies; neural network applications; clustering, classification, data mining and risk analysis; dynamics systems; innovative control systems, hardware design and implementation; robotics and automation; e-business, e-commerce, innovative Web applications; Web databases; diagnosis and medical applications; learning systems; optimization, hybrid systems, genetic algorithms and evolutionary computation control applications; online learning and ERP; knowledge acquisition and classification; nanomechatronics; simulation and control; mobile network applications; information retrieval; Bayesian networks; human computer interaction; cognitive science; mobile agents; knowledge management; intelligent control; e-search and navigation; security.",https://ieeexplore.ieee.org/document/4052645/,2006 International Conference on Computational Inteligence for Modelling Control and Automation and International Conference on Intelligent Agents Web Technologies and International Commerce (CIMCA'06),28 Nov.-1 Dec. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCWC51732.2021.9376024,Joint Activity Localization and Recognition with Ultra Wideband based on Machine Learning and Compressed Sensing,IEEE,Conferences,"Joint human activity localization and recognition has broad application prospects in human-computer interaction, virtual reality, smart healthcare system, security monitoring and robotics. Ultra-wideband (UWB) is an emerging technology adopted in real-time location system (RTLS) and has shown satisfactory performance in the task of human activity localization. However, few studies have been carried out to simultaneously recognize human activities based on UWB RTLS, which limits the use of UWB RTLS in many applications. In this study, we develop a RTLS based on UWB for the joint task of activity localization and recognition. A compressed sensing-based activity recognition approach is proposed for the task of activity recognition and several machine learning methods are designed to further improve the activity localization accuracy for the task of activity localization. The experimental results show that our UWB RTLS achieves good performance in this joint task.",https://ieeexplore.ieee.org/document/9376024/,2021 IEEE 11th Annual Computing and Communication Workshop and Conference (CCWC),27-30 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMIS.2015.22,Knowledge-Based Framework for the Virtual Communication of Public Art,IEEE,Conferences,"This paper deals with the documentation and communication of contemporary public art. It analyzes the specific requirements of such a form of cultural heritage and presents a system that implements the visualization of implicit information in a web -- based virtual reality environment. The core of the system is a knowledge -- based (ontological) module, devised to represent the values, the realization procedures, and the access ways to public art items. The visualization system relies upon the ontological representation to provide a 3D layout that takes into account the relevant traits that characterize the contemporary public art. The issues of integration of the public art within the urban context is the object of a visualization component, implemented in interactive web -- based 3D graphics, that relies on the ontological knowledge concerning the public art, integrated with additional ontological descriptions of spatial geographic knowledge.",https://ieeexplore.ieee.org/document/7284936/,2015 9th International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing,8-10 July 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VSMM.2016.7863170,LARA: A location-based and augmented reality assistive system for underground utilities' networks through GNSS,IEEE,Conferences,"In everyday life, in advance to an excavation project, there is an actual prerequisite and a by regulation in many countries, to pinpoint all the underground utilities located in the project's area. In addition, it is so crucial for the clients, the public and private utilities, to have a real representation of the water, electricity, gas, sewerage, etc. underground networks. As the practice shows, there is a constant and growing demand for the accurate positioning of the underground utilities in order to facilitate both the utilities' and the construction companies avoid damages to the existing infrastructure. Such a damage may cause a loss of thousands of euros. It is true that many problems may arise when the digging machine operator starts digging the ground, but the most dangerous is the gas line explosions, fires or even electrocution in electric lines, water lines bursts, etc. This can be very dangerous for all the field workers in the project's area but also for the property owners; even for the people just passing nearby the area. At the same time, such a development is extremely costly either for the public or the private company or for both. For years now there is a tangible market request for solutions that are able to effectively handle underground utilities' geospatial data and support the missions of utilities and construction companies in the real working environment. Innovative and edge technologies such as Global Navigation Satellite Systems (GNSS), sensors, Geographic Information Systems (GIS) and geodatabases, Augmented and Virtual Reality (AR/VR) can be used for monitoring, documenting and managing the utility-based geospatial and other data. In addition to this, these technologies are able to offer an intuitive 3D augmented visualization and navigation/positioning environment that can support anyone interested in the field. LARA project, a H2020 co-funded project by the European Commission, eavesdrops these market needs and now is developing a software and hardware-based system called LARA. Understanding the real market necessities of the utilities and construction players, a group of partners, from academia and research, SMEs and end users, is implementing this multidisciplinary project. Within LARA project, the consortium partners are evolving a mobile (tablet) device to support the utility management professionals and workers on the field and under real conditions. The LARA hand-held and mobile device is incorporating state-of-the-art technologies in the domain of positioning and sensors (GNSS), AR and 3D GIS geo-databases. The vision of LARA system is to guide the utility field workers in their daily business; to help them `see' beneath the ground by rendering the complexity of the 3D models of the underground grid such as water, gas, sewerage and electricity. The notion is to integrate the novel components of existing technologies so as to develop an integrated navigation/positioning and information system which coordinates GNSS, AR, 3D GIS and geodatabases on a mobile platform for monitoring, documenting and managing utility infrastructures on-site. In practice, the first prototype is ready and the forthcoming period it will be tested in the real environment. The ability and profits from the LARA system use, will be tested in two case studies scheduled during project lifetime. The two pilot applications are scheduled in Kozani (Greece) and the United Kingdom (Birmingham) with various underground utilities. The methodology for system testing has already been defined while the first results from the pilot application (Kozani, Greece) are coming the forthcoming period.",https://ieeexplore.ieee.org/document/7863170/,2016 22nd International Conference on Virtual System & Multimedia (VSMM),17-21 Oct. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DRCN51631.2021.9477372,Latency-aware and Survivable Mapping of VNFs in 5G Network Edge Cloud,IEEE,Conferences,"Network Functions Virtualization (NFV) and Multi-access Edge Computing (MEC) play crucial roles in 5G networks for dynamically provisioning diverse communication services with heterogeneous service requirements. In particular, while NFV improves flexibility and scalability by softwarizing physical network functions as Virtual Network Functions (VNFs), MEC enables to provide delay-sensitive/time-critical services by moving computing facilities to the network edge. However, these new paradigms introduce challenges in terms of latency, availability, and resource allocation. In this paper, we first explore MEC cloud facility location selection and then latency-aware placement of VNFs in different selected locations of NFV enabled MEC cloud facilities in order to meet the ultra-low latency requirements of different applications (e.g., Tactile Internet, virtual reality, and mission-critical applications). Furthermore, we also aim to guarantee the survivability of VNFs and an edge server against failures in resource limited MEC cloud facility due to software bugs, configuration faults, etc. To this end, we formulate the problem of latency-aware and survivable mapping of VNFs in different MEC cloud facilities as an Integer Linear Programming (ILP) to minimize the overall service provisioning cost, and show that the problem is NP-hard. Owing to the high computational complexity of solving the ILP, we propose a simulated annealing based heuristic algorithm to obtain near-optimal solution in polynomial time. With extensive simulations, we show the effectiveness of our proposed solution in a real-world network topology, which performs close to the optimal solution.",https://ieeexplore.ieee.org/document/9477372/,2021 17th International Conference on the Design of Reliable Communication Networks (DRCN),19-22 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct54149.2021.00036,Learning to Perceive: Perceptual Resolution Enhancement for VR Display with Efficient Neural Network Processing,IEEE,Conferences,"Even though the Virtual Reality (VR) industry is experiencing a rapid growth with ever-expanding demands today, VR applications have yet to provide a fully immersive experience. The insufficient resolution of the VR head-mounted display (HMD) hinders the user from further immersion into the virtual world. In this work, we attempt to enhance the immersive experience by improving the perceptual resolution of VR HMDs. We employ an efficient neural-network-based approach with the proposed temporal integration loss function. By taking the temporal integration mechanism of the Human Visual System (HVS) into account, our network learns the perception process of the human eye, and temporally upsamples a sequence that in turn improves its perceived resolution. Specifically, we discuss a possible scenario where we deploy our approach on a VR system equipped with the eye-tracking technology, which could save up to 75% of the computational load. Compared with the state-of-the-art in terms of the inference time analysis and a user experiment, it shows that our approach runs around 1.89× faster and produces more favorable results.",https://ieeexplore.ieee.org/document/9585895/,2021 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),4-8 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC47757.2020.9049747,Lightweight Evolving 360 VR Adaptive Video Delivery,IEEE,Conferences,"Moving towards 4K and 360 Virtual Reality (VR) video streaming is becoming bandwidth prohibitive with users demanding a personalized streaming experience without increasing costs. Existing efforts to improve the streaming experience has centered largely around client-side adaptive bitrate (ABR) algorithms; however, these approaches continue to experience fundamental flaws resulting in subpar content delivery network (CDN) performance. In this paper, we propose a method that addresses long-tail content in CDNs through the implementation of a multipath-aware peer-to-peer mechanism to distribute a video-specific lightweight neural network model, optimize ABR algorithms through reinforcement learning, and actively manage multipath transport networks; all of which significantly increase the 360 VR video streaming experience. Experimental results revealed that our architecture was superior in comparison to many leading ABR algorithms in multipath networks with video bitrates doubling while simultaneously reducing disruptive field-of-view latency switches.",https://ieeexplore.ieee.org/document/9049747/,"2020 International Conference on Computing, Networking and Communications (ICNC)",17-20 Feb. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2019.8803544,Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter Pruning,IEEE,Conferences,"Convolutional neural networks (CNNs) have emerged as the state-of-the-art in multiple vision tasks including depth estimation. However, memory and computing power requirements remain as challenges to be tackled in these models. Monocular depth estimation has significant use in robotics and virtual reality that requires deployment on low-end devices. Training a small model from scratch results in a significant drop in accuracy and it does not benefit from pre-trained large models. Motivated by the literature of model pruning, we propose a lightweight monocular depth model obtained from a large trained model. This is achieved by removing the least important features with a novel joint end-to-end filter pruning. We propose to learn a binary mask for each filter to decide whether to drop the filter or not. These masks are trained jointly to exploit relations between filters at different layers as well as redundancy within the same layer. We show that we can achieve around 5x compression rate with small drop in accuracy on the KITTI driving dataset. We also show that masking can improve accuracy over the baseline with fewer parameters, even without enforcing compression loss.",https://ieeexplore.ieee.org/document/8803544/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00057,Live Emoji: A Live Storytelling VR System with Programmable Cartoon-Style Emotion Embodiment,IEEE,Conferences,"We introduce a novel cartoon-style hybrid emotion embodiment model for live storytelling in virtual reality (VR). It contains an avatar with six basic emotions and an auxiliary multimodal display to enhance the expression of emotions. We further design and implement a system to teleoperate the embodiment model in VR for live storytelling. Specifically, 1) we design a novel visual programming tool that allows users to customize emotional effects based on the emotion embodiment model; 2) we design a novel face tracking module to map presenters' emotional states to the avatar in VR. Our web-based implementation makes the application easy to use. This is an accompanying paper extracted from [1] for the demo session in IEEE AIVR 2019.",https://ieeexplore.ieee.org/document/8942384/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICCS53718.2022.9788210,Machine Learning in 6G: The Future of Wireless Communication,IEEE,Conferences,"The 6<sup>th</sup> Generation of wireless communication, 6G, is the future of wireless communication technology. 6G has the potential to support various pioneering applications. Post Covid-19 pandemic, the necessity of remote data-connectivity is recognized as a socio-economic requirement. Data-centric communication has been one of the key-features in 5G and the trend is expected to continue in near future. 6G proposes to support various cutting-edge applications, including remote healthcare, holographic transportation, twin body area network, unmanned vehicle, smart infrastructures and augmented and virtual reality. These applications require a superior data-rate and infrastructure beyond the current 5G facilities. Hence, currently, both industry and academia are exploring the possible technology solutions that can support the advanced communication requirements of near future. This research work intends to provide a comprehensive summary of the Key Performance Indicators (KPI) of 6G, possible 6G network architectures, use cases and enabling technologies. Additionally, an extensive summary of the Machine Learning (ML) algorithms implemented in 6G are provided to achieve various technology goals.",https://ieeexplore.ieee.org/document/9788210/,2022 6th International Conference on Intelligent Computing and Control Systems (ICICCS),25-27 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN41052.2019.8972122,Machine learning for assistance systems: pattern-based approach to online step recognition,IEEE,Conferences,"Computer-aided assistance systems are entering the world of work and production. Such systems utilize augmented- and virtual-reality for operator training and live guidance as well as mobile maintenance and support. This is particularly important in the modern production reality of ever-changing products and `lot size one' customization of production.This paper focuses on the application of machine learning approach to extend the functionality of assistance systems. Machine learning provides tools to analyse large amounts of data and extract meaningful information. The goal here is to recognize the movement of an operator which would enable automatic display of instructions relevant to them.We present the challenges facing machine learning applications in human-centered assistance systems and a framework to assess machine learning approaches feasible for this scenario. The approach is assessed on a historical data set and then deployed in a work station for live testing. The post-hoc, or historical, analysis yields promising results. The ad-hoc, or live, analysis is a complex task and the results are affected by multiple factors, most of which are introduced by the human influence.The contribution of this paper is an approach to adapt state- of-the-art machine learning to operator movement recognition with a special focus on approaches to spatial time series data pre-processing. Presented experiment results validate the approach and show that it performs well in a real-world scenario.",https://ieeexplore.ieee.org/document/8972122/,2019 IEEE 17th International Conference on Industrial Informatics (INDIN),22-25 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00017,Mirrorlabs - creating accessible Digital Twins of robotic production environment with Mixed Reality,IEEE,Conferences,How to visualize recorded production data in Virtual Reality? How to use state of the art Augmented Reality displays that can show robot data? This paper introduces an opensource ICT framework approach for combining Unity-based Mixed Reality applications with robotic production equipment using ROS Industrial. This publication gives details on the implementation and demonstrates the use as a data analysis tool in the context of scientific exchange within the area of Mixed Reality enabled human-robot co-production.,https://ieeexplore.ieee.org/document/9319071/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTer51097.2020.9325501,Mixed Reality Supermarket: A Modern Approach into Day - to - Day Grocery Shopping,IEEE,Conferences,"In the modern world where there are massive trends in development and implementation of new technologies, combination of Virtual Reality and Augmented Reality is one which has key potential in an everyday developing world. The main concept behind Virtual Reality is simply immersing the user in a virtual environment at the comfort of their own place. This is done by creating a computer-generated 3D environment with hand gestured navigation system combined with concepts of voice recognition, image processing and machine learning that explores intense human interactions. As we are in the 21st century, where technological transformations are most certainly creating blurry lines between fiction and reality, more and more people have the need to fulfill their daily requirements easily without wasting their valuable time. Buying day to day needs from a supermarket is one of the main activities that each one of us struggle to go through during the day. Targeting the above simple daily activities, we are making an effort to apply VR Technology to this area through this research and thus trying to provide a rather new technological experience for purchasing items from a supermarket. This can be beneficial to the consumers to minimize their valuable time wasted, and also, they will be able to get the real experience of shopping while getting exposure to marketing.",https://ieeexplore.ieee.org/document/9325501/,2020 20th International Conference on Advances in ICT for Emerging Regions (ICTer),4-7 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2006.1713288,Modeling and Analyzing Multi-agent Task Plans for Intelligent Virtual Training System using Petri Nets,IEEE,Conferences,"Integrated virtual reality with intelligent tutoring system, a multi-agent architecture was proposed for intelligent virtual training system (IVTS) for mine safety training. In order to make sure IVTS agent's task plans reliable and adaptive, a Petri nets-based declarative method was applied to model the virtual training task planning knowledge, which was represented as task planning knowledge Petri nets (TP-PNets), and an algorithm was implemented to construct TP-PNets. Then, hierarchy colored Petri nets (HCPN) was used to model multi-agent task planning behaviors for IVTS, and simulation and message sequence chart was used to analyze and verify the agent task planning HCPN model",https://ieeexplore.ieee.org/document/1713288/,2006 6th World Congress on Intelligent Control and Automation,21-23 June 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAT.2006.85,Modeling and Application of Virtual Machine Tool,IEEE,Conferences,"The recent years of the 21th Century are associated with the advent of virtual reality technologies for modern industry and manufacturing engineering. Virtual Machine Tool Technology is given to design, test, control and machine parts in a virtual reality environment. This paper presents the methods to model and simulate the virtual machine tools in response to change in the machining requirements. Specifically, a set of module combination rules and a modeling method of the structure of machine tools using connectivity graph are developed. By this way virtual machine tool is implemented. The developed virtual machine tool can be efficiently used for industry training and machine leaning and operating.",https://ieeexplore.ieee.org/document/4089203/,16th International Conference on Artificial Reality and Telexistence--Workshops (ICAT'06),29 Nov.-1 Dec. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VS-Games.2018.8493435,Multi-Level Game Learning Analytics for Serious Games,IEEE,Conferences,The following topics are dealt with: computer games; virtual reality; computer aided instruction; mobile computing; serious games (computing); augmented reality; computer based training; health care; data visualisation; user interfaces.,https://ieeexplore.ieee.org/document/8493435/,2018 10th International Conference on Virtual Worlds and Games for Serious Applications (VS-Games),5-7 Sept. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9596665,Multi-Stakeholder Engagement in Agile Service Platform Co-Creation,IEEE,Conferences,"In the recent decade, the concept of Smart Tourism Destination (STD) has emerged to represent both a strategic aim to develop sustainable competitive advantages for tourism destinations or wider regions, and a managerial approach and dimensions to enhance data-driven development to measure and apply smart technologies for competitive and inclusive change in a tourism ecosystem. In tourism destination development globally, the aim for sustainable digital transformation in low-productivity business sectors is one of the key drivers for smart tourism and smart destination development. More recently, the global Covid-19 pandemic has given a boost to touchless, digital and green tourism development initiatives in the EU and worldwide. To advance sustainable digital transformation with agile methods in STD development, a systemic service platform approach and agile prototyping with smart emerging technologies, (eg. with AI, IoT, Augmented & Virtual Reality, 5G and Robotics), and multi-stakeholder co-creation is needed to engage key tourism industry ecosystem representatives. This conceptual paper advocates that open-source solutions combined with comprehensive multi-stakeholder co-creation aid in prototyping a systemic digital platform solution for smart tourism destinations. The paper concludes with an illustration of a conceptual model of service platform development for smart destinations, utilizing network co-creation with quadruple-helix stakeholders for sustainable regional impacts.",https://ieeexplore.ieee.org/document/9596665/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT49239.2020.9225680,"Notice of Violation of IEEE Publication Principles: 6G Wireless Communication: Its Vision, Viability, Application, Requirement, Technologies, Encounters and Research",IEEE,Conferences,"The fast development of multiband ultrafast seamless network and super reliable data transmission system to support heavy traffic applications such as artificial intelligence, machine learning, deep learning, augmented reality, virtual reality, 3D media, Internet of Things, Enterprise Internet of Thing and the Internet of Nano-things that involves with the real time transfer of data, voice and video in terabytes per second (Tb/s), the current cellular network (5G Network is insufficient to meet the growth of usage of triple play services in fraction of time). To meet the expectation of heavy data users is a big challenge in today's generation. To handle the situation of drastic demand of data, the sixth generation of mobile technology (6G) should be deeply studied along with its potential in terms of bandwidth, low latency, channel capacity, channel modeling techniques, loss propagation models, energy spectrum efficiency, faster network connectivity and data security. In this paper the vision in terms intelligent computing and wireless massive connectivity, feasibility, requirement in terms of modifying the existing 5G network, technologies in terms of artificial intelligence, 3D networking, SM-MIMO and optical computing, challenges after deployment, research to promote good health for 6G and application of 6G in the field of industry, automation sector, health, and transport has been studied and presented.",https://ieeexplore.ieee.org/document/9225680/,"2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT)",1-3 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAR.2017.7942721,Object detection on panoramic images based on deep learning,IEEE,Conferences,"Panoramic image can be widely used in many applications, such as virtual reality, visual surveillance and autonomous vehicle, because of its large field of view. However, the inherent distortion for panorama causes object detection to be a challenging task. This paper focuses on the multi-class objects detection in panoramic images using deep learning method. The proposed system uses three fisheye cameras to efficiently create panoramas and build a large dataset. A region based convolutional neutral network (R-CNN) is implemented to train and test on an indoor panoramic image dataset. Experiments show great improvement performance on ten categories of distorted indoor objects with a mean average precision of 68.7%.",https://ieeexplore.ieee.org/document/7942721/,"2017 3rd International Conference on Control, Automation and Robotics (ICCAR)",24-26 April 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCB48548.2020.9304919,On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR Applications,IEEE,Conferences,"Augmented and virtual reality is being deployed in different fields of applications. Such applications might involve accessing or processing critical and sensitive information, which requires strict and continuous access control. Given that Head-Mounted Displays (HMD) developed for such applications commonly contains internal cameras for gaze tracking purposes, we evaluate the suitability of such setup for verifying the users through iris recognition. In this work, we first evaluate a set of iris recognition algorithms suitable for HMD devices by investigating three well-established handcrafted feature extraction approaches, and to complement it, we also present the analysis using four deep learning models. While taking into consideration the minimalistic hardware requirements of stand-alone HMD, we employ and adapt a recently developed miniature segmentation model (EyeMMS) for segmenting the iris. Further, to account for non-ideal and non-collaborative capture of iris, we define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to quantify the iris recognition performance. Motivated by the performance of iris recognition, we also propose the continuous authentication of users in a non-collaborative capture setting in HMD. Through the experiments on a publicly available OpenEDS dataset, we show that performance with EER = 5% can be achieved using deep learning methods in a general setting, along with high accuracy for continuous user authentication.",https://ieeexplore.ieee.org/document/9304919/,2020 IEEE International Joint Conference on Biometrics (IJCB),28 Sept.-1 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VIMS.2002.1009363,On the uniform posture map algorithm for an articulated body,IEEE,Conferences,"It is important to reuse existing motion capture data for reduction of the animation producing costs as well as the eflciency of the producing process. Because its motion curve has no control point, however, captured data is difficult to modify interactively. Motion transition is a useful method for reusing existing motion data It generates a seamless intermediate motion with two short motion sequences. In this paper, the Uniform Posture Map (UPM) is proposed to perform motion transitions. The UPM is organized through the quantization of various postures with an unsupervised learning algorithm; it places the output neurons with similnr postures in adjacent positions. Using this propetty, an intermediate posture of applied two postures is generated; the generating posture is used as a key-fame to make an interpolating motion. The UPM needs fewer computational costs, in comparison with other motion transition algorithms. It provides a control parameter; an animator can not only control the motion simply by adjusting this parameter, but also produce animation interactively. The UPM prevents the generating of the invalid output neurons to present unreal postures in the learning phase; thus, it makes more realistic motion curves; finally it conbibrrles to the making of more natural motions. The motion transition algorithm proposed in this paper can be applied to various fields such as real time 30 games, virtual reality applications, and web 30 applications.",https://ieeexplore.ieee.org/document/1009363/,2002 IEEE International Symposium on Virtual and Intelligent Measurement Systems (IEEE Cat. No.02EX545),19-20 May 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.190,One Solution for Accurate Collision Detection in Virtual Assembly Environment,IEEE,Conferences,"Virtual assembly system requires higher accuracy on collision detection than ordinary virtual reality application. Based on analyzing the requirement of virtual assembly on accurate collision detection, a kind of hierarchical accurate collision detection thought is proposed, which includes three levels: assembly environment level, polygon level and accurate geometry topology model level. Three ways for virtual assembly system to obtain accurate geometry topology model are given. A specific solution that synthetically uses geometry graph supporting toolkit to realize hierarchical accurate collision detection is presented. A comparison experiment manifests that above solution and approach are feasible and efficient.",https://ieeexplore.ieee.org/document/4287604/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00022,One-Man Movie: A System to Assist Actor Recording in a Virtual Studio,IEEE,Conferences,"Acting in a movie is not an easily accessible activity for regular people, not to mention creating a movie alone. Based on the existing functions such as directors, photographers, and editors in a virtual studio that have been established in previous work, this research aims to design a computer-assisted system in virtual reality with actor functions allowing people to experience acting and create their own Computer Graphics (CG) movie. Through virtual reality and wearable motion capture devices, our system allows a user to enter the virtual studio as an actor. The user starts by choosing the role they want to play and then completing the drama performance under the guidance of the system. The motion of the actor will be recorded and in sync with the animations of other characters adjusted in real-time. As such, in addition to playing a single role, the user can also choose to perform and record multiple roles that can be put together to form the film. Thus, the entire CG movie can be completed by a single user alone. We have implemented a prototype of such a system and conducted an experiment to evaluate the system. The results will be reported in this paper.",https://ieeexplore.ieee.org/document/9644295/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VS-GAMES.2016.7590354,Ontology-Based Interactive Animation/Game Generation for Chinese Shadow Play Preservation,IEEE,Conferences,"Interactive animation generation is an attractive area of research for current animation developers in the field of virtual reality. As an essential tool for solving problems in many research areas, using standardised and structured terminologies, an ontological analysis could concisely describe the core logic of complex systems at a high level and facilitate system design. We propose a semantic framework using ontological methods to model the construction of interactive animation and promote integration process in a systematic and standardised way. A usage example is presented to provide guidance in generating animation of hand gesture based interactive Chinese shadow puppetry play for the heritage preservation of the traditional art. Domain-specific ontologies for the Chinese traditional shadow play and hand- gesture-based interaction are developed as the ontological implementation of the framework. Finally, an animation of hand-gesture-based interactive shadow play is generated.",https://ieeexplore.ieee.org/document/7590354/,2016 8th International Conference on Games and Virtual Worlds for Serious Applications (VS-GAMES),7-9 Sept. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR50242.2020.00033,Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,IEEE,Conferences,"Gaze tracking is an essential component of next generation displays for virtual reality and augmented reality applications. Traditional camera-based gaze trackers used in next generation displays are known to be lacking in one or multiple of the following metrics: power consumption, cost, computational complexity, estimation accuracy, latency, and form-factor. We propose the use of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to traditional camera-based gaze tracking approaches while taking all of these metrics into consideration. We begin by developing a rendering-based simulation framework for understanding the relationship between light sources and a virtual model eyeball. Findings from this framework are used for the placement of LEDs and photodiodes. Our first prototype uses a neural network to obtain an average error rate of 2.67° at 400 Hz while demanding only 16 mW. By simplifying the implementation to using only LEDs, duplexed as light transceivers, and more minimal machine learning model, namely a light-weight supervised Gaussian process regression algorithm, we show that our second prototype is capable of an average error rate of 1.57° at 250 Hz using 800 mW.",https://ieeexplore.ieee.org/document/9284794/,2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC51774.2021.00303,"Panel Discussion: Deriving Past, Present, and Future Tech to More Intelligent and Resilient Digital Realities for a Collaborative World",IEEE,Conferences,"No abstract or record of the panel discussion was made available for publication as part of the conference proceedings. There is the philosophical question of “does history repeat itself”. One learns from the past to affect the present, which builds our future. Advances in technology are built upon previous innovations. New technologies are derived from existing technologies. The IEEE leverages past and current technologies to advance work on new and emerging technologies through serving as a catalyst for developing new innovations, products and services. IEEE Future Directions serves as an incubator for these new initiatives. One of its focus areas, Digital Reality serves to explore and enable the coming Intelligent and Resilient Digital Realities through collaboration among technologists, engineers, regulators, practitioners, and ethicists around the world. The Digital Transformation is fueled by advances in technology, such as Artificial Intelligence (AI), Machine Learning (ML), and applications using the copious amounts of continuously generated data. By leveraging these technologies and others developed such as Augmented Reality (AR), Virtual Reality (VR), and Digital Twins, the line between the physical world and the digital world will be increasingly less distinct. Applications are already quickly emerging across the broad fields of gaming, entertainment, medicine, automotive, education, manufacturing, enabling the sharing of services, and more. Emphasis is upon presenting practical applications and its implementations of interest to attendees. Subject matter expert speakers comment on current and past implementations. Of course, the speakers look ahead to the future.",https://ieeexplore.ieee.org/document/9529363/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAMI48414.2020.9108758,Patient assessment using computer games in rehabilitation,IEEE,Conferences,"This article focuses on the issue of rehabilitation of patients with motor impairment using computer games. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion the entertainment industry in the form of computer games. We are trying to create a computer game system designed for home rehabilitation of patients. The aim of this article is to devise with a subset of motor impairment assessment of patients with the intention of adapting the demands of computer games based on the physical abilities of a particular patient. We are all trying to keep this system at the level available to a regular home user.",https://ieeexplore.ieee.org/document/9108758/,2020 IEEE 18th World Symposium on Applied Machine Intelligence and Informatics (SAMI),23-25 Jan. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2004.1357653,Pedagogical embodied conversational agent,IEEE,Conferences,"A virtual reality instructor that autonomously trains a human learner in network virtual environments, respond to multi-modal input across computer networks, and applies proven pedagogical techniques during instruction has the potential to improve human learning performance anytime, anywhere, and at any pace. Building virtual instructors, however, have challenged researchers because of multidisciplinary expertise required in areas such as education, cognitive science, sociology, artificial intelligence, 3D computer graphics, linguistics, and more. This paper discusses a model for building human computer interactive virtual instructor systems using an innovative, system/software architecture.",https://ieeexplore.ieee.org/document/1357653/,"IEEE International Conference on Advanced Learning Technologies, 2004. Proceedings.",30 Aug.-1 Sept. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiMob52687.2021.9606263,Performance Modelling and Assessment for Social VR Conference Applications in 5G Radio Networks,IEEE,Conferences,"One of the most challenging applications targeted by evolving (beyond-)5G technology is virtual reality (VR). Particularly, 'Social VR' applications provide a fully immersive experience and sense of togetherness to users residing at different locations. To support such applications the network must deal with huge traffic demands, while keeping end-to-end latencies low. Moreover, the radio access network must deal with the volatility and vulnerability of mmWave radio channels, where even small movements of the users may have substantial effects on the Quality of Experience. We present an integral modelling framework for feasibility assessment and performance optimization of the radio access network for Social VR applications in indoor office scenarios. Using the presented modelling approach, we conduct an extensive simulation-based assessment to determine the performance impact of head motion, the frequency band (3.5 GHz, 26 GHz) and radio network configurations, and derive the required carrier bandwidth for a range of 'Social VR' scenarios. Insights into these issues are a prerequisite for setting up guidelines for network deployment and configuration as well as for the development of (AI/ML-based) methods for dynamic resource management to optimally support Social VR applications.",https://ieeexplore.ieee.org/document/9606263/,"2021 17th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)",11-13 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00022,Photorealistic avatars to enhance the efficacy of Selfattachment psychotherapy,IEEE,Conferences,"We have designed, developed, and tested an Immersive virtual reality (VR) platform to practice the protocols of Self-attachment psychotherapy. We made use of customized photorealistic avatars for the implementation of both the high-end version (based on Facebook's Oculus) and the low-end version (based on Google's cardboard) of our platform. Under the Selfattachment therapeutic framework, the causes of mental disorders such as chronic anxiety and depression are traced back to the individual's insecure attachment with their primary caregiver during childhood and their subsequent problems in affect regulation. The conventional approach (without VR) to Selfattachment requires that the individual uses their childhood photographs to recall their childhood memories and then imagine that the child that they were is present with them. They thus establish a compassionate relationship with their childhood self and then, using love songs and dancing, create an affectional bond with them. Their adult self subsequently role plays a good parent and interacts with their imagined childhood self to perform various developmental and re-parenting activities. The goal is to enhance their capacities for self-regulation of emotion, which can lead them into earning secure attachment. It is hypothesized that our immersive virtual reality platform - which enables the users to interact with their customized 3D photorealistic childhood avatar - offers either a better alternative or at least a complementary visual tool to the conventional imaginal approach to Self-attachment. The platform was developed in Unity 3D, a cross-platform game engine, and takes advantage of the itSeez3D Avatar SDK for generating a customized photorealistic 3D avatar head from a 2D childhood image of the user. The platform also offers facial and body animations for some of the basic emotional states such as Happy, Sad, Scared and Joyful and it allows modifications to the avatar body (height/ width) and clothing color. A study to compare the use of the avatar-based approach (VR) to Self-attachment with the conventional photo-based approach showed promising results. Almost 85% of the participants reported that their photorealistic childhood avatar in VR was more relatable than their childhood photos. Both low-end and high-end VR based approaches were unanimously reported to be more effective than the conventional imaginal approach. Participants reported that the high-end version of the VR platform was more realistic and immersive than the low-end mobile VR version.",https://ieeexplore.ieee.org/document/9319121/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2015.7301144,Platform for bio-monitoring of vital parameters in critical infrastructures operation,IEEE,Conferences,"The objective of the present scientific research was to develop a bio-monitoring platform for the supervision of personnel working in the critical infrastructure tested in a laboratory based on virtual reality simulations. The role of this platform is to collect a set of signals in order to determine the optimal physiological profile of personnel during testing in virtual reality simulations (for real situations). From biomedical perspective, in developing this platform there are several main directions of interest including: the development of software solutions for biosignal processing derived from the multiple biosensors, the analysis of hardware and software solutions for bio-information acquisitions and the identification of the most efficient solutions. Also, it has followed the development of software and hardware solutions for seamless integration of all monitoring equipment in order to develop a complex integrated system. This platform will help in developing new training and health evaluation criteria very useful for correct conclusions about predictive character regarding the responses of the personnel in the real professional situations.",https://ieeexplore.ieee.org/document/7301144/,"2015 7th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",25-27 June 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812287,Prediction of Metacarpophalangeal Joint Angles and Classification of Hand Configurations Based on Ultrasound Imaging of the Forearm,IEEE,Conferences,"With the advancement in computing and robotics, it is necessary to develop fluent and intuitive methods for inter-acting with digital systems, augmented/virtual reality (AR/VR) interfaces, and physical robotic systems. Hand movement recognition is widely used to enable such interaction. Hand configuration classification and metacarpophalangeal (MCP) joint angle detection are important for a comprehensive reconstruction of hand motion. Surface electromyography (sEMG) and other technologies have been used for the detection of hand motions. Ultrasound images of the forearm offer a way to visualize the internal physiology of the hand from a musculoskeletal perspective. Recent works have shown that these images can be classified using machine learning to predict various hand configurations. In this paper, we propose a Convolutional Neu-ral Network (CNN) based deep learning pipeline for predicting the MCP joint angles. We supplement our results by using a Support Vector Classifier (SVC) to classify the ultrasound information into several predefined hand configurations based on activities of daily living (ADL). Ultrasound data from the forearm were obtained from six subjects who were instructed to move their hands according to predefined hand configurations relevant to ADLs. Motion capture data was acquired as the ground truth for hand movements at three speeds (0.5 Hz, 1 Hz, and 2 Hz) for the index, middle, ring, and pinky fingers. We demonstrated the perfect prediction of hand configurations through SVC classification and a correspondence between the predicted MCP joint angles and the actual MCP joint angles for the fingers, with an average root mean square error of 7.35 degrees. A low latency (6.25 &#x2013; 9.10 Hz) pipeline was implemented for the prediction of both MCP joint angles and hand configuration estimation aimed for real-time implementation.",https://ieeexplore.ieee.org/document/9812287/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC47757.2020.9049692,Predictive Caching for AR/VR Experiences in a Household Scenario,IEEE,Conferences,"Augmented/virtual reality (ARNR) technologies can be deployed in a household environment for applications such as checking the weather or traffic reports, watching a summary of news, or attending classes. Since AR/VR applications are highly delay sensitive, delivering these types of reports in maximum quality could be very challenging. In this paper, we consider that users go through a series of AR/VR experience units that can be delivered at different experience quality levels. In order to maximize the quality of the experience while minimizing the cost of delivering it, we aim to predict the users' behavior in the home and the experiences they are interested in at specific moments in time. We describe a deep learning based technique to predict the users' requests from ARNR devices and optimize the local caching of experience units. We evaluate the performance of the proposed technique on two real-world datasets and compare our results with other baselines. Our results show that predicting users' requests can improve the quality of experience and decrease the cost of delivery.",https://ieeexplore.ieee.org/document/9049692/,"2020 International Conference on Computing, Networking and Communications (ICNC)",17-20 Feb. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCWD.2006.253263,Proceedings 2006 10th International Conference on Computer Supported Cooperative Work in Design,IEEE,Conferences,The following topics are dealt with: computer supported cooperative work in design; collaborative design methods and tools; coordination methods; CSCW system security; grid computing; data and information management; networked manufacturing; workflow management; agents and multi-agent systems; ontology and knowledge management; Web services and semantic Web; e-commerce and e-businesses; virtual reality and CAD; design management; e-learning,https://ieeexplore.ieee.org/document/4019299/,2006 10th International Conference on Computer Supported Cooperative Work in Design,3-5 May 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWOBI47054.2019.9114431,Proof of Concept: Using Reinforcement Learning agent as an adversary in Serious Games,IEEE,Conferences,"This article focuses on simple rehabilitation video-game called Flying with Friends. The rehabilitation industry has been experiencing a boom in recent years, coupled with the growing popularity of virtual reality technology, a drop in prices for these technologies and the expansion entertainment industry in the form of computer games. The goal of this experiment was to provide a proof that such systems are viable option when it comes to artificial intelligence systems in serious video-games, but not limited to only serious ones. The solution described in this article, in cooperation with experts, is going to be deployed in a real rehabilitation environment.",https://ieeexplore.ieee.org/document/9114431/,2019 IEEE International Work Conference on Bioinspired Intelligence (IWOBI),3-5 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UEMCON47517.2019.8992974,Quality Model for Testing Augmented Reality Applications,IEEE,Conferences,"Augmented Reality applications have the capability of merging virtual objects into physical setting, or alternatively they can wrap physical objects within a virtual scene. Augmented reality applications are similar to virtual reality applications in that aspects of the visualizations are computer generated, but augmented reality apps also must contain a view of the physical world. Augmented reality applications are being utilized in service, manufacturing, product areas, as well as gaming. Mobile devices are becoming common runtime environments for augmented reality applications and the mobile device proliferation is enabling a wave of AR applications. Due to the combined nature of digital and physical objects, as well as the environmental and contextual constraints, a traditional test plan is not sufficient. A new quality model is proposed that takes these issues into account, and an example of how machine learning can assist with aspects of the model is discussed.",https://ieeexplore.ieee.org/document/8992974/,"2019 IEEE 10th Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)",10-12 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INVENTIVE.2016.7824820,Real time facial expression recognition using RealSense camera and ANN,IEEE,Conferences,"Facial expression is an essential part in communication. It is a challenging task in computer vision as well as in pattern recognition. Facial expression recognition has applications in many fields such as HCI, video games, virtual reality, and analyzing customer satisfaction etc. Proposed system focuses on emotion recognition using facial expressions which are captured by using Intel's RealSense SR300 camera. This camera detects landmarks on the depth image of a face automatically using Software Development Kit (SDK) of RealSense camera. Geometric feature based approach is used for feature extraction. The distance between landmarks is used as features and for selecting an optimal set of features brute force method is used. Proposed system is used Multilayer Perceptron (MLP) neural network algorithm using backpropagation method for classification. The experimental dataset is captured using RealSense SR300 camera. The proposed system recognizes three facial expressions namely neutral, happy, and surprised. The recognition rate achieved is 93.33%.",https://ieeexplore.ieee.org/document/7824820/,2016 International Conference on Inventive Computation Technologies (ICICT),26-27 Aug. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863455,Real time tracking in robot teleoperation system,IEEE,Conferences,"The robot teleoperation system based on stereo vision was developed by the State Key Lab of Intelligent Technology and System of Tsinghua University. The paper presents the design frame of the whole system, and describes in detail some of the key design and implementation problems. Finally, the paper analyses the difficulty of applying this technology to virtual reality and augmented reality systems, and some suggestions are provided. The success of this system can contribute to further research on augmented reality.",https://ieeexplore.ieee.org/document/863455/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV50864.2020.00032,Real-time Motion Planning for Robotic Teleoperation Using Dynamic-goal Deep Reinforcement Learning,IEEE,Conferences,"We propose Dynamic-goal Deep Reinforcement Learning (DGDRL) method to address the problem of robot arm motion planning in telemanipulation applications. This method intuitively maps human hand motions to a robot arm in real-time, while avoiding collisions, joint limits and singularities. We further propose a novel hardware setup, based on the HTC VIVE VR system, that enables users to smoothly control the robot tool position and orientation with hand motions, while monitoring its movements in a 3D virtual reality environment. A VIVE controller captures 6D hand movements and gives them as reference trajectories to a deep neural policy network for controlling the robot’s joint movements. Our DGDRL method leverages the state-of-art Proximal Policy Optimization (PPO) algorithm for deep reinforcement learning to train the policy network with the robot joint values and reference trajectory observed at each iteration. Since training the network on a real robot is time-consuming and unsafe, we developed a simulation environment called RobotPath which provides kinematic modeling, collision analysis and a 3D VR graphical simulation of industrial robots. The deep neural network trained using RobotPath is then deployed on a physical robot (ABB IRB 120) to evaluate its performance. We show that the policies trained in the simulation environment can be successfully used for trajectory planning on a real robot. The the codes, data and video presenting our experiments are available at https://github.com/kavehkamali/ppoRobotPath.",https://ieeexplore.ieee.org/document/9108691/,2020 17th Conference on Computer and Robot Vision (CRV),13-15 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTC2021-Spring51267.2021.9448829,Reinforcement Learning Approach for Content-Aware Resource Allocation in Hybrid WiFi-VLC Networks,IEEE,Conferences,"The diversity of users' applications based on content delivery in indoor environments dramatically increased such that the conventional RF wireless networks might not be enough to support applications such as 4K video streaming and virtual reality. Visible light communication (VLC) has emerged recently as a complementary unlicensed media. In this paper, we propose a hybrid WiFi-VLC system consist of one WiFi access point (AP) and multiple VLC APs. Based on the requested data rate, users can be assigned to WiFi or VLC AP in downlink such as to maximize the fairness among all connected users. We propose a reinforcement learning algorithm that can be implemented at the WiFi AP and help with assigning the users to a specific AP based on their requested data rate. We propose a new reward function that improves the satisfaction of all connected users. Numerical simulation results show that the proposed method improved the user satisfaction based on Jain's fairness index.",https://ieeexplore.ieee.org/document/9448829/,2021 IEEE 93rd Vehicular Technology Conference (VTC2021-Spring),25-28 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDIN45582.2020.9442114,Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow Based QoE,IEEE,Conferences,"With the merit of containing full panoramic content in one camera, Virtual Reality (VR) and 360° videos have arisen in the field of industrial cloud manufacturing and training. Industrial Internet of Things (IoT), where many VR terminals needed to be online at the same time, can hardly guarantee VR's bandwidth requirement. However, by making use of users' quality of experience (QoE) awareness factors, including the relative moving speed and depth difference between the viewpoint and other content, bandwidth consumption can be reduced. In this paper, we propose Optical Flow Based VR(OFB-VR), an interactive method of VR streaming that can make use of VR users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable Difference through Optical Flow Estimation (JND-OFE) is explored to quantify users' awareness of quality distortion in 360° videos. Accordingly, a novel 360° videos QoE metric based on Peak Signal-to-Noise Ratio and JND-OFE (PSNR-OF) is proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling scheme to lessen the tiling overhead. A Reinforcement Learning (RL) method is implemented to make use of historical data to perform Adaptive BitRate (ABR). For evaluation, we take two prior VR streaming schemes, Pano and Plato, as baselines. Vast evaluations show that our system can increase the mean PSNR-OF score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that OFB-VR is a promising prototype for actual interactive industrial VR. A prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.",https://ieeexplore.ieee.org/document/9442114/,2020 IEEE 18th International Conference on Industrial Informatics (INDIN),20-23 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCASIT48058.2019.8973210,Research On Aircraft Virtual Assembly Technology Based On Gesture Recognition,IEEE,Conferences,"With the continuous development of computer, virtual simulation and human-computer interaction technology, virtual reality technology has been continuously improved and matured. Especially in recent years, with the intelligentization of mobile phones and the birth of VR glasses and helmets, the research of virtual reality technology has formed in various industries around the world. VR technology has much lower cost and difficulty in creating virtual scenes than building real world.. This article selects HTC Vive pro kit and Leap Motion as the hardware platform, and uses Unity3D and related plug-ins and Leap Motion plug-in as the software platform to develop a set of aircraft virtual equipment maintenance system based on gesture recognition. In the process of gesture recognition, the theory of machine learning was introduced to model, train and identify the gesture information collected by Leap Motion sensor. It can be equipped with the correct virtual tool in the virtual scene to complete the assembly of the corresponding components, and has a good sense of immersion.",https://ieeexplore.ieee.org/document/8973210/,2019 IEEE 1st International Conference on Civil Aviation Safety and Information Technology (ICCASIT),17-19 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEICT.2016.7879656,Research and implementation of virtual human animation based on distributed semantic environment,IEEE,Conferences,"With the development of virtual reality technology and the application of semantic Web technology, Semantic Web-based virtual reality i.e. semantic virtual environment has become a new direction in the research of virtual reality. In this paper, the semantic web technology is applied in the DVR system. The Distributed Intelligent Virtual Reality System which is based on the semantic web is designed and implemented. Based on the semantic virtual environment ontology and distributed technology, a DVR system architecture, communication framework and communication of data packets are designed. Finally, based on the semantic virtual environments and virtual human animation engine, we developed an action reasoning system of intelligent virtual human in the office scene, and achieved very good results.",https://ieeexplore.ieee.org/document/7879656/,2016 IEEE International Conference on Electronic Information and Communication Technology (ICEICT),20-22 Aug. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICIC.2008.451,Research of Plant Domain Knowledge Model Based on Ontology,IEEE,Conferences,"Simulation of plant growth is a hotspot in virtual reality area. In order to improve the level of knowledge sharing and solve the problem that the shape of plant is difficult to describe, a plant domain knowledge model based on ontology is provided in this paper. After analyzing the knowledge of domain ontology, this paper describes an ontology model of the plants domain knowledge using related botany knowledge. Plant domain ontology is divided into two parts in the model, plant knowledge and environment knowledge. This paper uses Protege as the tool for the plant domain ontology description and store the plant domain ontology in the form of OWL files. The description of the plant domain ontology can be used in the plant growth evaluation system.",https://ieeexplore.ieee.org/document/4603297/,2008 3rd International Conference on Innovative Computing Information and Control,18-20 June 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2017.8166518,Research of model scheduling strategy in large Web3D scene based on XML,IEEE,Conferences,"In this paper, we analyze the three bottlenecks that limit the large-scene in virtual reality applications and especially in the Web-based applications. We also analyze the technical characteristics and defects of the LOD strategy and then propose a novel partitioning model. Based on the proposed model and XML, we present a scheme to overcome the three bottlenecks regarding CPU, memory and network bandwidth. Our scheme will help the large-scene Web-based virtual reality applications to be implemented in practice.",https://ieeexplore.ieee.org/document/8166518/,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",29 June-1 July 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC55111.2022.9778652,Research on Autonomous Decision-Making of UCAV Based on Deep Reinforcement Learning,IEEE,Conferences,"In order to improve the intelligence level of training opponents in UCAV air combat simulation and the realism and immersion of air combat simulation in 3D space, this paper proposes a deep reinforcement learning algorithm for UCAV autonomous control based on virtual reality technology. A combination of reinforcement learning and Unity3D is used to train UCAV agents to achieve air combat tasks in 3D virtual reality space, and imitation learning is added to improve the efficiency of policy generation. Multiple perceptrons are used to simplify the agent&#x2019;s acquisition of environmental state data, and reward functions are designed by integrating UCAV angle, speed, and altitude considerations to visualize the entire 3D visualization process of reinforcement learning training UCAV agents to interact with the environment.",https://ieeexplore.ieee.org/document/9778652/,2022 3rd Information Communication Technologies Conference (ICTC),6-8 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEI48997.2019.00053,Research on Voice Interaction Technology in VR Environment,IEEE,Conferences,"Artificial intelligence has broad prospects and has a wide range of application fields. This paper studied the interactive combination of natural language processing technology and virtual reality environment in artificial intelligence, and optimized it in the software design level. This paper achieved VR and NLP environment based on unity3d engine. Finally, the input voice signal can be converted into interactive commands in VR environment, and VR environment also can be controlled by the voice.",https://ieeexplore.ieee.org/document/8991134/,2019 International Conference on Electronic Engineering and Informatics (EEI),8-10 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIE53562.2021.00145,Research on improving students’ interest in learning based on CiteSpace.5.7.R2,IEEE,Conferences,"To comprehensively understand the research situation and research trend in the field of ""learning interest"", this paper uses CiteSpace.5.7.R2 information visualization software as a research tool, selects 2400 literature data from the Web of Science core database from 1975 to 2021 and 984 literature data from the CNKI database from 1900 to 2021 for visual analysis, respectively from the aspects of national cooperation, institutional cooperation, keyword co-occurrence, co-citation network, etc. The results show that the current international research hotspot is the interest development model, the application of virtual reality technology, and so forth. The research hotspot in China is teaching reform. Finally, the paper puts forward the prospect of the research in the field of ""learning interest"" in China: strengthening the cooperation among colleges and universities; using science and technology in teaching, such as virtual reality, to improve students’ interest in learning.",https://ieeexplore.ieee.org/document/9534593/,2021 2nd International Conference on Artificial Intelligence and Education (ICAIE),18-20 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CECNet.2012.6202239,Research on the safety awareness training software for commercial vehicles drivers,IEEE,Conferences,"The feasibility for safety awareness training on commercial vehicle drivers with the application of automotive driving simulator was analyzed in this study, the virtual reality based safety awareness training methodology was also proposed. The reference and specific contents for training courses design was firstly illustrated. The explanation of main architecture planning and implementation methods was two important parts in this study. The implementation effect of this software was then presented. The results of feedbacks from the users who were in the practical application of the system indicate that this software system can improve the safety awareness in driving. The prospect for future research and application direction were finally discussed.",https://ieeexplore.ieee.org/document/6202239/,"2012 2nd International Conference on Consumer Electronics, Communications and Networks (CECNet)",21-23 April 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2004.1310053,Resolving object references in multimodal dialogues for immersive virtual environments,IEEE,Conferences,"This paper describes the underlying concepts and the technical implementation of a system for resolving multi-modal references in virtual reality (VR). In this system the temporal and semantic relations intrinsic to referential utterances are expressed as a constraint satisfaction problem, where the propositional value of each referential unit during a multimodal dialogue updates incrementally the active set of constraints. As the system is based on findings of human cognition research it also regards, e.g., constraints implicitly assumed by human communicators. The implementation takes VR related real-time and immersive conditions into account and adapts its architecture to well known scene-graph based design patterns by introducing a so-called reference resolution engine. Regarding the conceptual work as well as regarding the implementation, special care has been taken to allow further refinements and modifications to the underlying resolving processes on a high level basis.",https://ieeexplore.ieee.org/document/1310053/,IEEE Virtual Reality 2004,27-31 March 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISAECT53699.2021.9668480,Robust Speech Emotion Recognition System Through Novel ER-CNN and Spectral Features,IEEE,Conferences,"The speech is most fundamental way of communication among the humans and an important method for human computer interaction (HCI) by employing the microphone. Measurable emotion recognition from the speech signal by employing microphone is an emerging and interesting area of research in HCI such as human reboot interaction, healthcare, virtual reality, emergency call, and behavior assessment. In this paper, we proposed a novel integration of spectral features comprises of mel-spectral frequency coefficients (MFCC), root mean square energy (RMSE), and zero crossing rate (ZCR) to represent complex audio signal. For the classification purpose, we designed a novel convolutional neural network called emotion recognition neural network (ER-CNN) to classify different emotions such as angry, disgust, fear, happy, neutral, and sad. The proposed method Speech emotion recognition (SER-CNN) obtained an equal error rate (EER) of 1.34&#x0025;, an accuracy of 94.99&#x0025;, precision of 94.96&#x0025;, recall of 94.98&#x0025;, and F1-score of 94.96&#x0025;. We evaluated the performance of the proposed system SER-CNN on the standard dataset crowd-sourced emotional multimodal actors (CREMA-D). Experimental results of the proposed method and comparative analysis against the existing methods show that our method has superior performance and can reliably be used for the emotion detection.",https://ieeexplore.ieee.org/document/9668480/,2021 4th International Symposium on Advanced Electrical and Communication Technologies (ISAECT),6-8 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2013.6631400,Robust real-time visual odometry for dense RGB-D mapping,IEEE,Conferences,"This paper describes extensions to the Kintinuous [1] algorithm for spatially extended KinectFusion, incorporating the following additions: (i) the integration of multiple 6DOF camera odometry estimation methods for robust tracking; (ii) a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm; (iii) advanced fused realtime surface coloring. These extensions are validated with extensive experimental results, both quantitative and qualitative, demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features.",https://ieeexplore.ieee.org/document/6631400/,2013 IEEE International Conference on Robotics and Automation,6-10 May 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT54291.2022.9824182,STEP UP: Systematically Motivating the Children with Low Psychological Maturity Level and Disabled Children using Gamification and Human Computer Interaction,IEEE,Conferences,"Children are the future of this world. Therefore, teaching them to have a better future is very important. Also, as the adults we have to motivate them to overcome the obstacles and challenges they face throughout their lifetime. When considering about the children, there are various types of children in our society. As examples there are children with special needs and there are children who are mentally and physically stable. Children with special needs require special attention than the other children. These kinds of children with special needs have various types of development disabilities. They are children with low psychological maturity level, autism, down syndrome, genetic disorders etc. We have proposed a system to motivate these children who are with low psychological maturity level named as &#x2018;STEP-UP&#x2019;. This system is a combination of four individual modules that have the common goal to motivate these kinds of children which were implemented using gamification, Image processing, machine learning and Human Computer Interaction. One individual module is focused on disabled children, and it will motivate those children using gamification. Another two modules are focusing on the children who are with low psychological maturity level. And that will motivate those kinds of children using gamification and HCI based technologies like Virtual Reality. The other module is a protocol to secure the data sent between the system and the database. The common goal of this overall STEP-UP system is to motivate the children with low psychological maturity level and disabled children.",https://ieeexplore.ieee.org/document/9824182/,2022 IEEE 7th International conference for Convergence in Technology (I2CT),7-9 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISRITI48646.2019.9034659,Security Issues and Vulnerabilities On A Blockchain System: A Review,IEEE,Conferences,"In recent years, we have seen very significant development in software technologies. Many new and more sophisticated technologies such as Artificial Intelligence, Virtual Reality, and Internet of Things are being adopted for daily use. One of the emerging technologies is blockchain. Every day, more and more businesses and industries start implementing blockchain to their systems. Although the features of blockchain may give us a convenient and reliable services, the security of this recent innovation is still a question to be concerned about. One of them is the majority attack that has happened to the Bitcoin network involving a group of miners having more than 51% of the computing power of the entire network, and there are some other issues as well. Before blockchain could be widely adapted for practical usage, we would have to take a deeper look into the issues and threats it could have, which we address in this paper.",https://ieeexplore.ieee.org/document/9034659/,2019 International Seminar on Research of Information Technology and Intelligent Systems (ISRITI),5-6 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICRS46726.2019.9555857,Security Protocols in Internet of Things (IoT) - A Review,IEEE,Conferences,"The technical and scientific domains have undergone a major revolution in this twenty-first century. With several new spheres have been explored to make lives simpler and solve complex challenges. The area of Computer Science and Information technology has eased people lives by introduction of Artificial Intelligence, Virtual reality. One such useful domains are the Internet of Things (IoT) that has now started a recent trend in making things simpler and easy to access. This has enabled several daily use objects to be smartly connected with various computing devices, and it can also be helpful to control them by a click of a button or sending a simple command. However, every technology gets threat with increased users and to prevent these, various security protocols have been implemented in IoT.",https://ieeexplore.ieee.org/document/9555857/,2019 International Conference on Intelligent Computing and Remote Sensing (ICICRS),19-20 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEARIS.2012.6231174,Semantic modeling of Virtual Environments using MASCARET,IEEE,Conferences,"Many Virtual Reality (VR) applications, such as Virtual Learning Environments or Interactive Virtual Tours, are based on a rich semantic description of the environment and tasks that users have to perform. These applications are built upon Virtual Environments (VEs) in which artificial agents act autonomously while interacting in realtime with users. Semantic modelling of a VR environment makes it possible the knowledge-driven access from the description of VEs that simplifies the development of VR applications. It eases the development of these types of applications. Semantic modelling should provide a consistent representation of the following aspects: 1) The simulated world, its structure and the behavior of its entities, 2) Interactions and tasks, that users and agents can perform in the environment, 3) Knowledge items, that autonomous agents can use for decision-making or for communication with users. This paper presents MASCARET, a model-based approach, for the design of semantic VR environments. This approach is based on the Unified Modeling Language (UML). In this approach, UML is used to provide a knowledge-driven access to the semantic contents of the VE and not for code generation, as in classical software development process. Interests of a UML-based approach are that its metamodel covers different views of the semantic modelling: ontology, structure, behaviors, interactions, activities. It is also an extensible language that can be specialized to provide formal operational semantics. We also present how MASCARET can be used to develop content-rich interactive applications that can be deployed over various VR platforms. Finally, we discuss the benefits of such a metamodel-based approach and show how the multi-layer semantic model can be used in different VR applications, in which adaptive behaviors of artificial agents acting within complex environments have to be simulated.",https://ieeexplore.ieee.org/document/6231174/,2012 5th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS),5-5 March 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEARIS.2016.7551582,Semantics-based software techniques for maintainable multimodal input processing in real-time interactive systems,IEEE,Conferences,"Maintainability, i.e. reusability, modifiability, and modularity, is a critical non-functional quality requirement, especially for software frameworks. Its fulfilment is already challenging for low-interactive application areas. It is additionally complicated by complex system designs of Real-time Interactive Systems (RISs), required for Augmented, Mixed, and Virtual Reality, as well as computer games. If such systems incorporate AI methods, as required for the implementation of multimodal interfaces or smart environments, it is even further exacerbated. Existing approaches strive to establish software technical solutions to support the close temporal and semantic coupling required for multimodal processing and at the same time preserve a general decoupling principle between involved software modules. We present two key solutions that target the semantic coupling issue: (1) a semantics-based access scheme to principal elements of the application state and (2) the specification of effects by means of semantic function descriptions for multimodal processing. Both concepts are modeled in an OWL ontology. The applicability of our concepts is showcased by a prototypical implementation and explained by an interaction example that is applied for two application areas.",https://ieeexplore.ieee.org/document/7551582/,2016 IEEE 9th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS),20-20 March 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2016.7840720,Shooting a moving target: Motion-prediction-based transmission for 360-degree videos,IEEE,Conferences,"Enabled by the rapid development of virtual reality hardware and software, 360-degree video content has proliferated. From the network perspective, 360-degree video transmission imposes significant challenges because it consumes 4 6χ the bandwidth of a regular video with the same resolution. To address these challenges, in this paper, we propose a motion-prediction-based transmission mechanism that matches network video transmission to viewer needs. Ideally, if viewer motion is perfectly known in advance, we could reduce bandwidth consumption by 80%. Practically, however, to guarantee the quality of viewing experience, we have to address the random nature of viewer motion. Based on our experimental study of viewer motion (comprising 16 video clips and over 150 subjects), we found the viewer motion can be well predicted in 100~500ms. We propose a machine learning mechanism that predicts not only viewer motion but also prediction deviation itself. The latter is important because it provides valuable input on the amount of redundancy to be transmitted. Based on such predictions, we propose a targeted transmission mechanism that minimizes overall bandwidth consumption while providing probabilistic performance guarantees. Real-data-based evaluations show that the proposed scheme significantly reduces bandwidth consumption while minimizing performance degradation, typically a 45% bandwidth reduction with less than 0.1% failure ratio.",https://ieeexplore.ieee.org/document/7840720/,2016 IEEE International Conference on Big Data (Big Data),5-8 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2017.8022722,Simplify methodology for developing VR applications based on shared VR materials,IEEE,Conferences,"Panorama images are most recently utilized on immersive virtual reality system. This paper point up simply methodology for developing virtual reality applications based on environment panorama display system. The 2D panorama images are taken from real world environments. Panoramas including their information are shared as VR materials and can be retrieved and rendered as virtual environments on many virtual systems in different contents. Multiple connection points among images are managed for realistic feeling to navigate in the constructed virtual environment. To assess the success of methodology process, the virtual system to tour an organization is constructed using images taking from places of a building in the organization. The built system can work well as individual mobile application.",https://ieeexplore.ieee.org/document/8022722/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2011.5759457,Simulator X: A scalable and concurrent architecture for intelligent realtime interactive systems,IEEE,Conferences,"This article presents a platform for software technology research in the area of intelligent Realtime Interactive Systems. Simulator X is targeted at Virtual Reality, Augmented Reality, Mixed Reality, and computer games. It provides a foundation and testbed for a variety of different application models. The current research architecture is based on the actor model to support fine grained concurrency and parallelism. Its design follows the minimize coupling and maximize cohesion software engineering principle. A distributed world state and execution scheme is combined with an object-centered world view based on an entity model. Entities conceptually aggregate properties internally represented by state variables. An asynchronous event mechanism allows intra- and interprocess communication between the simulation actors. An extensible world interface uses an ontology-based semantic annotation layer to provide a coherent world view of the resulting distributed world state and execution scheme to application developers. The world interface greatly simplifies configurability and the semantic layer provides a solid foundation for the integration of different Artificial Intelligence components. The current architecture is implemented in Scala using the Java virtual machine. This choice additionally fosters low-level scalability, portability, and reusability.",https://ieeexplore.ieee.org/document/5759457/,2011 IEEE Virtual Reality Conference,19-23 March 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UBMK.2019.8907108,Smart Class Applications for Education,IEEE,Conferences,"We use technological developments in education and adapt technology to education. Educational computers, tablets, smart phones and other technological devices that integrate with these devices aim to increase the quality of education within the scope of computer-aided training. At the same time, with the development of the Internet, it has become much easier to access resources. The environments where the hardware and software technologies used in education are used together can be considered as intelligent classes. Video conferencing and live broadcasts used in the first smart classrooms have now turned into different applications. Intelligent classes that support learning with robotics, mobile learning, virtual reality and augmented reality applications, and different learning environments and materials are used today to improve the quality of education. The Individualized Smart Class (BAS) application proposed in this study is designed in two steps, hardware and software. It is envisaged to use laptop computers, microphones, headphones, tablets and virtual reality glasses in the classroom as hardware. At the same time the broadband of the classes will be ensured to have the internet. Even in this class, wireless network systems such as infrared, Bluetooth, Wi-Fi can be used. As a software, a class that can be included in the virtual classes with cloud architecture and can use the increased reality applications is considered. In addition, mobile applications that provide virtual reality will enrich the course materials. With this application, equality of opportunity will be provided for disadvantaged students in education and training, quality of education will be improved and it will be beneficial for the development of our country in this context.",https://ieeexplore.ieee.org/document/8907108/,2019 4th International Conference on Computer Science and Engineering (UBMK),11-15 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIRCA51532.2021.9544776,Software Implementation Architecture Design for Online Display of Animation Works Based on VR Technology,IEEE,Conferences,"The design of the online display system of virtual reality animation is at the intersection of art, design, and VR technology. VR technology is a fusion of computer artificial intelligence, simulation technology, display technology, sensor technology and other technologies. In order to solve the shortcomings of the traditional animation display system, this paper introduces VR technology and designs a software architecture that can display animation works online. First, it introduces the research status and application prospects of VR technology. Then I learned about the software architecture of the animation display system. Finally, an animation display system is designed.",https://ieeexplore.ieee.org/document/9544776/,2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA),2-4 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR55393.2022.9826285,Stereoscopic low-latency vision system via ethernet network for Humanoid Teleoperation,IEEE,Conferences,"Despite the recent rise of artificial intelligence, the automatic operation of the robot in an unstructured environment is still insufficient to perform complicated and detailed tasks and requires remote operation by a human. The most important thing to improve the efficiency of such Teleoperation is the vision system that enables the human operator to recognize the surrounding situation of the robot. While constructing the system, immersive experience for the operator was considered. Low latency and curved plane rendering are proposed to provide an immersive experience for operators. Moreover, considering the operator&#x2019;s exhaustion during the teleoperation that comes from Virtual Reality sickness, the reduction of Virtual Reality sickness is also suggested. Finally, to check the entire system&#x2019;s visual feedback latency, display-to-display latency is quantitatively measured. This paper presents an implementation of a vision system for the Teleoperation of humanoid robots through Virtual Reality.",https://ieeexplore.ieee.org/document/9826285/,2022 19th International Conference on Ubiquitous Robots (UR),4-6 July 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2006.258811,Study on Virtual Intelligent Assembly System for Machine Tools Based on Multi-Agent and Petri-Net,IEEE,Conferences,"Virtual reality is a technology, which is often regarded as a natural extension to 3D computer graphics with advanced input and output devices. It can help manufacturing enterprises attain their common goal for lowering the total manufacturing costs, producing products in shorter times and with higher quality, and deliver products on time. This paper explored the application of several advance technologies such as VRML, multi-agent and Petri-net in virtual intelligent assembly system. A proof-of-concept desktop system, namely, virtual intelligent assembly system for machine tools (VIASMT) based on multi-agent and Petri-net has been proposed and developed to display assembly process and demonstrate correlative functions of virtual machine tool. The common feature of traditional methods is that establishing a precise state equation to simulate the dynamic process of system. But, VIASMT can't be described by accurate mathematical method in fact. Relatively speaking, Petri net has stronger ability to describe the dynamic property of system perfectly, and its graphic means is more easily to be understood and accepted. Some software development kits such as visual C++ and Open Inventor are used to implement the VIASMT with the help of multi-agent and Petri-net technologies. The work flowchart is given. In this virtual intelligent assembly system, a new machine tool can be assembled easily and rapidly. In this paper, several application instances are provided",https://ieeexplore.ieee.org/document/4028028/,2006 International Conference on Machine Learning and Cybernetics,13-16 Aug. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOMWKSHPS54753.2022.9798101,Super Resolution for Augmented Reality Applications,IEEE,Conferences,"Latest developments in machine learning (ML), adversarial networks, combined with increasingly powerful IoT devices via the introduction of efficient processors, are bringing about the implementation of near real-time object detection and classification for augmented reality (AR) and virtual reality (VR) applications. This paper intends to explore new object detection and classification technologies leveraging super-resolution (SR), that have the potential to be integrated into small, mobile and low-power ARNR devices. SR in conjunction with novel object detection and classification algorithms are examined in the presented paper, with the ultimate goal of proposing a low-footprint Generative Adversarial Network (GAN)-based framework capable of receiving an LR input and outputting an SR-supported recognition model based on FRRCNN, YOLOv3 or Retina.",https://ieeexplore.ieee.org/document/9798101/,IEEE INFOCOM 2022 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),2-5 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ZINC.2018.8448765,Supporting Constructivist Learning and Teaching with the Aid of VR-based Consumer Tech: A Case Study,IEEE,Conferences,"Technology leads to massive changes in the economy, in the way we communicate and relate to each other and in the way we learn. Thus, the classical paradigms of teaching and learning and their corresponding design models can now be supported by consumer tech. This article presents an example of experiential learning, a design model which implements constructivism, using a virtual reality application for computer architecture. The experimental results related to how the students perceive it underlines the benefits and challenges brought by this type of application.",https://ieeexplore.ieee.org/document/8448765/,2018 Zooming Innovation in Consumer Technologies Conference (ZINC),30-31 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPDWinter52325.2021.00048,Technology-driven Service Innovation in University Libraries,IEEE,Conferences,"Libraries are building a digital environment and preparing for the post-corona era. They attempt to increase operational efficiency and competitiveness by applying new technologies and strive to build a more intelligent and user-friendly environment using big data, the Internet of things (IoT), artificial intelligence (AI), virtual reality (VR), 3D printing, and automated robots. This study aims to present the direction for future university libraries by analyzing technology-based service innovation cases in line with the current era, centering on university libraries",https://ieeexplore.ieee.org/document/9403519/,"2021 21st ACIS International Winter Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD-Winter)",28-30 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIE.2002.1185854,The HyperClass: education in a broadband Internet environment,IEEE,Conferences,"The 1980s saw the advent of the PC in education, the 1990s saw the coming of narrow band Internet to education, the first decade of the new Millennium seems set to see the spread of broadband Internet in education. What will it mean for education when students and teachers can access Pentium 4 computers with 214 meg bandwidth from wherever they are? The author with Lalita Rajasingham and Nobuyoshi Terashima in collaboration with colleagues around the world, have been studying this question. In particular they have been looking at the application on the Internet of a technology called HyperReality. The idea of the technology is that it provides a space where physical reality and virtual reality and human intelligence and artificial intelligence can interact in a manner that becomes increasingly seamless. The space could be a class-hence the idea of a HyperClass. The paper looks at the experimental work already carried out between Japan and New Zealand to develop such a system. It addresses the implications for education of real students, teachers and objects freely interacting with virtual students, teachers and objects and with artificial intelligence in a class. Finally it draws on the work of Tiffin and Rajasingham in designing a global virtual university to look at the possibilities for developing HyperUniversities, HyperColleges and HyperSchools which would allow the intersection of global and local dimensions in education.",https://ieeexplore.ieee.org/document/1185854/,"International Conference on Computers in Education, 2002. Proceedings.",3-6 Dec. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NIGERCON54645.2022.9803002,The Role Of Energy Harvesting In 5G Wireless Networks Connectivity,IEEE,Conferences,"There is recent focus on secure ultra-reliable low-latency communications (SURLLC), ultra-reliable low-latency communications (URLLC) and massive Machine Type Communications (mMTC) driven by 5G wireless network connectivity which include more complex and huge networks such as Augmented Reality/Virtual Reality (AR/VR), Internet of Things (IoT), Internet of everything (IoE), Artificial Intelligence (AI) and Wearable Sensors/devices etc. Due to enormous Energy utilization keen interest has been given to energy harvesting technology as a remedy to complement the lifetime of devices and networks to prevent downtime in communications because of energy challenge. Harvesting energy from the environment will potentially lessen dependency on grid or battery energy supply. The random nature of renewable energy in contrast to regular stable energy, makes energy harvesting transmission systems challenging to install. Energy sources and models, energy harvesting and usage protocols, implementation in cognitive radio, multiuser, and cellular networks, to name a few, have all been studied in recent years to address this inherent challenge from a variety of perspectives. As a result of the massive energy usage, there is an increasing awareness of energy conservation in fifth generation (5G) wireless networks. Eventually, energy harvesting technology could be a viable option for extending the life of devices and networks.",https://ieeexplore.ieee.org/document/9803002/,2022 IEEE Nigeria 4th International Conference on Disruptive Technologies for Sustainable Development (NIGERCON),5-7 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2008.4621012,The application of Computational Fluid Dynamics to tissue bleeding simulation for medical endoscopic image,IEEE,Conferences,"Technologies based on Computational Fluid Dynamics (CFD) have been developed to many new fields, such as generating scenes in computer games, computer animation and Hollywood movies. In this paper, the application of CFD to the computer simulation of tissue bleeding, which is one important part of developing a surgical training system based on virtual reality technology, is investigated. One real-time bleeding simulation model based on CFD is proposed aiming at realistic simulation of tissue bleeding. Navier-Stokes equations are applied to model blood flow as an incompressible fluid. A faster solution to the Navier-Stokes equations is employed to solve the density field and velocity field in order to develop a real-time system. In light of this solution, a tissue bleeding model with OpenGL has been implemented. The results show that this method applied to the bleeding simulation is elucidating and practical.",https://ieeexplore.ieee.org/document/4621012/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863233,The mechanism of Java based and objects-oriented simulation on WWW,IEEE,Conferences,"The paper addresses the implementation of a fundamental, easy but powerful mechanism of discrete system simulation on the World Wide Web with Java-based mobile code. With the support of Java multi-thread and object oriented programming theory, this simulation mechanism is different from any other existing simulation modeling language. It is mainly applied on the World Wide Web, and can be easily installed in any Java integrated development environment and extended to more utilized fields, furthermore, virtual reality simulation on WWW, as the further development of this system, is discussed and an M/M/2 queuing system is also given to show the prospects.",https://ieeexplore.ieee.org/document/863233/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PAHCE.2011.5871891,Three dimensional modeling applied to breast cancer,IEEE,Conferences,"This paper presents three-dimensional modeling as a tool for education in health, being the object for integration with Virtual Reality and Artificial Intelligence. The choose for the application area was the anatomical structure of the breast, especially for the importance of a clinical model for virtual training aid in the detection of breast cancer.",https://ieeexplore.ieee.org/document/5871891/,2011 Pan American Health Care Exchanges,28 March-1 April 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMRP.2019.00013,Three-Dimensional Mapping of High-Level Music Features for Music Browsing,IEEE,Conferences,"The increased availability of musical content comes with the need of novel paradigms for recommendation, browsing and retrieval from large music libraries. Most music players and streaming services propose a paradigm based on content listing of meta-data information, which provides little insight on the music content. In services with huge catalogs of songs, a more informative paradigm is needed. In this work we propose a framework for music browsing based on the navigation into a three-dimensional (3-D) space, where musical items are placed as a 3-D mapping of their high-level semantic descriptors. We conducted a survey to guide the design of the framework and the implementation choices. We rely on state-of-the-art techniques from Music Information Retrieval to automatically extract the high-level descriptors from a low-level representation of the musical signal. The framework is validated by means of a subjective evaluation from 33 users, who give positive feedbacks and highlight promising future developments especially in virtual reality field.",https://ieeexplore.ieee.org/document/8665368/,2019 International Workshop on Multilayer Music Representation and Processing (MMRP),23-24 Jan. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1259597,Three-dimension train group operation simulation system and its application in railway intelligent transportation system,IEEE,Conferences,"The paper presents a three-dimensional train group operation simulation system, which is designed depending on the existed definition of train operation Petri net with objects (TOPNO). The structure of the simulation system possesses the characteristics of owning several separated layers, which ensure the independence of any logical unit and make the system extensible. It has small scale and the advantages in flexibility, modularization and operability etc. Compared the existed traditional two-dimension train group operation simulation system, the system is obvious, flexible and can be better operated and supply more information. At the end of the paper, it is educed that this system can be applied in virtual reality, can verify the feasibility of the train operation graph and can be adopted as the experimental platform of the railway intelligent transportation system, etc.",https://ieeexplore.ieee.org/document/1259597/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR50242.2020.00068,Towards Real-Time Recognition of Users Mental Workload Using Integrated Physiological Sensors Into a VR HMD,IEEE,Conferences,"This paper describes an “all-in-one” solution for the real-time recognition of users' mental workloads in virtual reality through the customization of a commercial HMD with physiological sensors. First, we describe the hardware and software solution employed to build the system. Second, we detail the machine learning methods used for the automatic recognition of the users' mental workload, which are based on the well-known Random Forest algorithm. In order to gather data to train the system, we conducted an extensive user study with 75 participants using a VR flight simulator to induce different levels of mental workload. In contrast to previous works which label the data based on a standardized task (e.g., n-back task) or on a pre-defined task-difficulty, participants were asked about their perceived mental workload level along the experiment. With the data collected, we were able to train the system in order to classify four different levels of mental workload with an accuracy up to 65%. In addition, we discuss the role of the signal normalization procedures, the contribution of the different physiological signals on the recognition accuracy and compare the results obtained with the sensors embedded in the HMD with commercial grade systems. Preliminary results show our pipeline is able to recognize mental workload in real-time. Taken together, our results suggest that such all-in-one approach, with physiological sensors directly embedded in the HMD, is a promising path for VR applications in which the real-time or off-line estimation of Mental Workload assessment is beneficial.",https://ieeexplore.ieee.org/document/9284745/,2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIT.2016.7474875,Towards a fully automated 3D printability checker,IEEE,Conferences,"3D printing has become one of the most popular evolutionary techniques with diverse application, even as normal people's hobby. As printing target, enormous 3D virtual models from game industry and virtual reality flood the internet, shared in various online forums, such as thingiverse. But which of them can really be printed? In this paper we propose the 3D Printability Checker, which can be used to automatically answer this non-trivial question. One of the major novelties of this paper is the process of dependable software engineering we use to build this Printability Checker. Firstly, we prove that this question is decidable with a given 3D object and a list of printer profiles. Secondly, we design and implement such a checker. Finally, we show our experimental results and use them further for a machine learning approach to improve our system in an automatic way. The generic framework provides a useful basis of automatic self-improvement of the software by combining current techniques in the area of formal method, geometry modelling and machine learning.",https://ieeexplore.ieee.org/document/7474875/,2016 IEEE International Conference on Industrial Technology (ICIT),14-17 March 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2017.8123181,Towards a virtual environment for interactive analysis of cluster-based flow pattern abstraction,IEEE,Conferences,"Recent research efforts show the benefits of using machine learning and interactive visualizations in data analytics. However, there is a void in the implementation of these techniques for the analysis of large and complex 4-dimentional (4D) unsteady flows. Hence, this paper presents an initial development of a virtual environment (VE) to fill this void. The VE has a two-layer architecture with different technologies running in the back and fore grounds. Using machine learning, the background layer implements clustering algorithms for abstracting spatiotemporal patterns of the flows like flow features, spatial regions and temporal phases. The outputs of the clustering algorithms are fed to the foreground layer for interactive selection, sorting and filtering of these patterns. The operations in the foreground make up our designed techniques of flow pattern analysis that aims to provide greater comprehension of 4D flow data. Running in the foreground, the virtual reality (VR) technologies of stereoscopic rendering and haptic feedback further enhance these analysis techniques. Thus, our work introduces a novel environment for the interactive analysis of cluster-based flow pattern abstractions.",https://ieeexplore.ieee.org/document/8123181/,"2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",5-8 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTERACT.2010.5706202,Tracking method for Human Computer Interaction using Wii Remote,IEEE,Conferences,"Over the years the techniques and methods that have been used to interact with the computers have evolved significantly. From the primitive use of punch cards to the latest touch screen panels we can see the vast improvement in interaction with the system. The same applies to the projection technology for creating various geometric displays. Researchers are exploring new ways of projection and interaction technologies to create `illusion' that can reshape our perception and interaction methodologies. In this paper we implemented (Infrared) IR light source tracking system which can be used for tracking single and multiple IR light source positions and can be used effectively in optical tracking problems such as tracking head position which is all pervasive requirement in a Virtual Reality System. We present a method of mapping IR light source position and orientation to an image. We defined the interaction techniques for an image manipulations with the help of fingers such as zoom, tilt/rotate, scale etc. We can extend our tracking capabilities to implement the touch screen feature to commercial applications. The paper illustrates the hardware and software used for implementing IR light source tracking based applications.",https://ieeexplore.ieee.org/document/5706202/,INTERACT-2010,3-5 Dec. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON52537.2022.9766804,Training of Engineers: Approaches to Customization of Educational Programs,IEEE,Conferences,"Since the field of information technology (IT) is constantly and rapidly developing, the training of engineering personnel couldn&#x2019;t be behind this global digital transformation. The new digital reality focuses on the mandatory formation of competencies which are end-to-end technology-oriented: Big Data, machine learning and artificial intelligence, augmented and virtual reality, robotics, blockchain, Internet of Things, 5G technologies, quantum technologies and others. Considering the speed of the development of these technologies, concerns have arisen about the relevance of the content of educational programs of higher education, as well as the degree of flexibility of educational trajectories of engineering graduates. To ensure the relevance and flexibility, constant revision of training programs regarding the study and development of new paradigms and solutions is required. This paper calls into question how to meet the requirements of the labor market at the time of graduation, considering that the educational programs are compiled at the time of the beginning of education and in accordance with the established federal standards of higher education. The goal is to adjust the results of the educational program or even to introduce completely new results in accordance with the changes that have occurred in the industry at the time of the implementation of the program to ensure students form the most relevant and in-demand competencies. The article describes the approaches of the Institute of Computer Technologies and Information Security (ICTIS) of the Southern Federal University to the creation of flexible, interdisciplinary bachelor&#x2019;s and master&#x2019;s degree educational programs. The content of these programs directly corresponds to the current and promising demands of the labor market. The presented approaches to the customization of educational programs meet the needs of students in personal and professional development. These approaches contribute to the development of a student-centered learning system that is specific for each level of education. Project-based learning is a key tool for the formation of relevant professional competencies within bachelor&#x2019;s degree programs. The master&#x2019;s degree focuses on the formation of unique research competencies in the context of the current agenda. For this purpose, the ICTIS has opened special research programs that involve a grant system for undergraduates. What is also important is the inclusion of undergraduates in research groups led by postdocs of the Institute. The key principle uniting these approaches into a single system is the introduction of its own educational standards in engineering areas in the ICTIS. The standards of the ICTIS regulate the possibility for students to choose variable professional competencies after mastering the basic educational component. These competencies are formed annually based on the analysis of prospective labor market demands and can be included in the program even after the start of its implementation. The analysis on the results of the ICTIS educational standards implementation has shown the effectiveness of the concept of these standards in the field of computer technology and information security. The effectiveness of this concept of the educational standard allows gradually being implemented in other fields of knowledge.",https://ieeexplore.ieee.org/document/9766804/,2022 IEEE Global Engineering Education Conference (EDUCON),28-31 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/CISTI.2017.7975750,UAV simulator for grown-up people quality of life enhancement,IEEE,Conferences,This paper presents the development of a virtual reality simulator for the management of a UAV (Unmanned Aerial Vehicle) focused on improving the quality of life of grown-up people. The present research has collected characteristics of gestures and physical movements from users made by other related research in order to study the same interaction within a virtual world. Through this research a smaller number of gestures were created improving the user learning curve without affecting the usability. The following implementation uses a client-server architecture composed of 2 Raspberry Pi devices and a Smartphone acting as a server the communication between them was achieved by employing Bluetooth Low Energy technology. The immersive virtual experience is accomplished by using Unity 3D and Google VR tools that allowed the design and display of a playful virtual environment as an approach to promote physical and cognitive skills such as spatial thinking and hand-eye coordination. By performing maneuvers through an aerial circuit filled with obstacles the proposed UAV simulator encourages motor and mental activity while the user is being entertained. The result is the improvement of the user quality of life by avoiding cognitive and physical sedentarism.,https://ieeexplore.ieee.org/document/7975750/,2017 12th Iberian Conference on Information Systems and Technologies (CISTI),21-24 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00072,Ubiquitous Virtual Humans: A Multi-platform Framework for Embodied AI Agents in XR,IEEE,Conferences,"We present an architecture and framework for the development of virtual humans for a range of computing platforms, including mobile, web, Virtual Reality (VR) and Augmented Reality (AR). The framework uses a mix of in-house and commodity technologies to support audio-visual sensing, speech recognition, natural language processing, nonverbal behavior generation and realization, text-to-speech generation, and rendering. This work builds on the Virtual Human Toolkit, which has been extended to support computing platforms beyond Windows. The resulting framework maintains the modularity of the underlying architecture, allows re-use of both logic and content through cloud services, and is extensible by porting lightweight clients. We present the current state of the framework, discuss how we model and animate our characters, and offer lessons learned through several use cases, including expressive character animation in seated VR, shared space and navigation in room-scale VR, autonomous AI in mobile AR, and real-time user performance feedback based on mobile sensors in headset AR.",https://ieeexplore.ieee.org/document/8942321/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00025,Unmasking Communication Partners: A Low-Cost AI Solution for Digitally Removing Head-Mounted Displays in VR-Based Telepresence,IEEE,Conferences,"Face-to-face conversation in Virtual Reality (VR) is a challenge when participants wear head-mounted displays (HMD). A significant portion of a participant's face is hidden and facial expressions are difficult to perceive. Past research has shown that high-fidelity face reconstruction with personal avatars in VR is possible under laboratory conditions with high-cost hardware. In this paper, we propose one of the first low-cost systems for this task which uses only open source, free software and affordable hardware. Our approach is to track the user's face underneath the HMD utilizing a Convolutional Neural Network (CNN) and generate corresponding expressions with Generative Adversarial Networks (GAN) for producing RGBD images of the person's face. We use commodity hardware with low-cost extensions such as 3Dprinted mounts and miniature cameras. Our approach learns end-to-end without manual intervention, runs in real time, and can be trained and executed on an ordinary gaming computer. We report evaluation results showing that our low-cost system does not achieve the same fidelity of research prototypes using high-end hardware and closed source software, but it is capable of creating individual facial avatars with personspecific characteristics in movements and expressions.",https://ieeexplore.ieee.org/document/9319106/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VSMM.2001.969765,VIAS teaching environment,IEEE,Conferences,The technological advances and the socialization of Internet have challenged education to search for new strategies to help the learning/teaching process. Such advances demonstrate a great tendency towards distance education. This article presents the VIAS virtual reality environment as a support for teaching and learning platform via software for the Internet.,https://ieeexplore.ieee.org/document/969765/,Proceedings Seventh International Conference on Virtual Systems and Multimedia,25-27 Oct. 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00036,VRMenuDesigner: A toolkit for automatically generating and modifying VR menus,IEEE,Conferences,"With the rapid development of Virtual Reality (VR) technology, the research of User Interface (UI), especially menus, in the VR environment has attracted continued attention. However, it is very tedious for researchers to develop UI from scratch or modify existing functions and there are no easy-to-use tools for efficient development. This paper aims to present VRMenuDesigner, a novel, flexible and modular toolkit for automatically generating/modifying VR menus. This toolkit is provided as an open-source library and easy to extend to adapt to various requirements. The main contribution of this work is to organize the menus and functions with object-oriented thinking, which makes the system very understandable and extensible. VRMenuDesigner includes two key tools: Creator and Modifier for quickly generating and modifying elements. Moreover, we developed several built-in menus and discussed their usability. After a brief review and taxonomy of 3D menus, the architecture and implementation of the toolbox are introduced.",https://ieeexplore.ieee.org/document/9644298/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DTS55284.2022.9809895,VVC intra prediction decoder: Feature improvement and performance analysis,IEEE,Conferences,"Nowadays, streaming applications have been in great demand, especially due to covid-19 (teleworking, online teaching, virtual reality, etc.). In addition, artificial intelligence has become widely used especially in video processing domains, so a video with high quality improves the accuracy rate of this application. To meet these needs, the Versatile Video Coding standard (VVC) has appeared to give a high compression efficiency compared to high-efficiency video coding. This norm consists of a high complexity algorithm that offers an improvement in processing time and decreases the bit rate by 50 &#x0025; thanks to several new compression techniques. In this context, we propose the implementation of an intra prediction decoding chain of this standard on a system on chip. In this work, we highlight the VVC feature enhancements, we present the suitable method for VVC intra-prediction decoder implementation on the PYNQ-Z2, and we provide profiling in terms of decoding time and power consumption. As a future work, this study is helpful to distinguish the block that will be a candidate for a Hardware acceleration.",https://ieeexplore.ieee.org/document/9809895/,2022 IEEE International Conference on Design & Test of Integrated Micro & Nano-Systems (DTS),6-9 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CogMI52975.2021.00021,Valence/Arousal Estimation of Occluded Faces from VR Headsets,IEEE,Conferences,"Emotion recognition from facial visual signals is a challenge which has attracted enormous interest over the past two decades. Researchers are attempting to teach computers to better understand a person&#x0027;s emotional state. Providing emotion recognition can massively enrich experiences. The benefits of this research for human-computer interactions are limitless. Emotions are intricate, and so we need a representative model of the full spectrum displayed by humans. A multi-dimensional emotion representation, which includes valence (how positive an emotion) and arousal (how calming or exciting an emotion), is a good fit. Virtual Reality (VR), a fully immersive computer-generated world, has witnessed significant growth over the past years. It has a wide range of applications including in mental health, such as exposure therapy and the self-attachment technique. In this paper, we address the problem of emotion recognition when the user is immersed in VR. Understanding emotions from facial cues is in itself a demanding task. It is made even harder when a head-mounted VR headset is worn, as now an occlusion blocks the upper half of the face. We attempt to overcome this issue by introducing EmoFAN-Vr,a deep neural network architecture, to analyse facial affect in the presence of a severe occlusion from a VR headset with a high level of accuracy. We simulate an occlusion representing a VR headset and apply it to all datasets in this work. EmoFAN-Vrpredicts both discrete and continuous emotions in one step, meaning it can be used in real-time deployment. We fine-tune our network on the AffectNet dataset under VR occlusion and test it on the AFEW-VA dataset, setting a new baseline for this dataset whilst under VR occlusion.",https://ieeexplore.ieee.org/document/9750297/,2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI),13-15 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2017.8166478,"Validation of website accessibility, a case study of DIMBI project",IEEE,Conferences,"The World Wide Web network, with its ubiquitous nature, a multitude of functionalities, and a broad range of benefits seems to require proper care and maintenance in support of its continued development. This responsibility lies largely in the hands of web designers and those involved in the administration of individual pages as fundamental units of the entire virtual reality of the WWW system. It is therefore advisable for any designer to emphasize not only the high quality of the distributed content but also its proper presentation and its adjustment to the technical standards of the profession.",https://ieeexplore.ieee.org/document/8166478/,"2017 9th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",29 June-1 July 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00059,Video Demo: Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video,IEEE,Conferences,"In this demonstration, we will present a video showing depth predictions for street-level 360° panoramic footage generated using our unsupervised learning model. Panoramic depth estimation is important for a range of applications in- cluding virtual reality, 3D modeling, and autonomous robotic navigation. We have developed a convolutional neural network (CNN) model for unsupervised learning of depth and ego-motion from cylindrical panoramic video. In contrast with previous works, we focus on cylindrical panoramic projection. Unlike spherical or cube map projection, cylindrical projection is fully compatible with traditional CNN layers while still supporting a continuous 360° horizontal field of view. We find that this increased field of view improves the ego-motion prediction accuracy for street-level video input. This abstract motivates our work in unsupervised structure-from-motion estimation, describes the video demonstration, outlines our implementation, and summarizes our study conclusions.",https://ieeexplore.ieee.org/document/8942320/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIS53314.2022.9742880,Virtual Design Method of Indoor Space Based on 3D Visual Cycle Modeling,IEEE,Conferences,"Aiming at the problem that traditional virtual design of living room space has always been unsatisfactory, it cannot achieve both beautification and practicality, and proposes a virtual design method of living room space based on three-dimensional vision. According to the overall design effect of the room, on the basis of image and color processing algorithm design, based on virtual reality and visual simulation technology, carry out the development and design of distributed 3D interior design system, and use 3ds MAX to carry out the 3D construction of distributed 3D interior design. Model, realize the indoor hierarchical structure design on Multigen Creator modeling software. The test results show that the visual effect of distributed 3D interior design using this method is better, and the feature expression ability is increased by 7.6&#x0025;.",https://ieeexplore.ieee.org/document/9742880/,2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS),23-25 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCCGA.1997.626194,Virtual Stage: a scenario-based karaoke system in a virtual environment,IEEE,Conferences,"We describe our on-going work on Virtual Stage, a karaoke system which incorporates virtual reality technology. A runtime environment and authoring tools for interactive scenarios have been designed and implemented. We adopt a scenario-based representation to unfold a story in the virtual environment. A producer can make Virtual Stage applications which reflect his/her own scenario. While singing-along to the song played by Virtual Stage, the participants use their body to interact with virtual characters which behave intelligently in the virtual environment. The scenario written via authoring tools describes the interaction between the participant and virtual characters. The system analyzes video images of the participant to extract participant-related data, then the video image is composed into the rendered scene of a virtual environment.",https://ieeexplore.ieee.org/document/626194/,Proceedings The Fifth Pacific Conference on Computer Graphics and Applications,13-16 Oct. 1997,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/WAC50355.2021.9559586,Virtual Testing and Policy Deployment Framework for Autonomous Navigation of an Unmanned Ground Vehicle Using Reinforcement Learning,IEEE,Conferences,"The use of deep reinforcement learning (DRL) as a framework for training a mobile robot to perform optimal navigation in an unfamiliar environment is a suitable choice for implementing AI with real-time robotic systems. In this study, the environment and surrounding obstacles of an Ackermann-steered UGV are reconstructed into a virtual setting for training the UGV to centrally learn the optimal route (guidance actions to be taken at any given state) towards a desired goal position using Multi-Agent Virtual Exploration in Deep Q-Learning (MVEDQL) for various model configurations. The trained model policies are to be transferred to a physical vehicle and compared based on their individual effectiveness for performing autonomous waypoint navigation. Prior to incorporating the learned model with the physical UGV for testing, this paper outlines the development of a GUI application to provide an interface for remotely deploying the vehicle and a virtual reality framework reconstruction of the training environment to assist safely testing the system using the reinforcement learning model.",https://ieeexplore.ieee.org/document/9559586/,2021 World Automation Congress (WAC),1-5 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2019.00145,Virtualization for Flexibility and Network-Aware on 5G Mobile Devices,IEEE,Conferences,"The dawn of 5G is rising with emerging wireless and network technologies that promisingly transform various aspects of human lives. The 5G mobile devices are going to tightly engage with the 5G infrastructure for future applications such as virtual reality, AI-enabled applications, etc. However, we argue that the tight engagement with convenience may bring downside that is a user may not correctly control/own her/his device. In particular, the user may not have the flexibility in utilizing networks, as well as, be aware of the network selection. This paper introduces a novel networking stack for 5G mobile devices (namely 5GVir) that leverages virtualization techniques for flexibility and awareness. The 5GVir device can concurrently exploit surrounding wireless networks by using Software Defined Networking (SDN). SDN provides a rich set of flexibility feature that will meet user expectation. Moreover, 5GVir includes wireless virtualization that can relax the dependence on hardware as well as provides a wireless link for the SDN's control channel. Last but not least, the 5GVir device has an additional awareness feature aware that drives each application process to predetermined networks (i.e., applying network namespace). Our prototype shows the potential of realizing 5GVir. Moreover, the initial experiments show that the virtualization in 5GVir has negligible overhead.",https://ieeexplore.ieee.org/document/8754200/,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),15-19 Jul 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8460692,Visual-Inertial Navigation Algorithm Development Using Photorealistic Camera Simulation in the Loop,IEEE,Conferences,"The development of fast, agile micro Unmanned Aerial Vehicles (UAVs) has been limited by (i) on-board computing hardware restrictions, (ii) the lack of sophisticated vision-based perception and vision-in-the-loop control algorithms, and (iii) the absence of development environments where such systems and algorithms can be rapidly and easily designed, implemented, and validated. Here, we first present a new micro UAV platform that integrates high-rate cameras, inertial sensors, and an NVIDIA Jetson Tegra X1 system-on-chip compute module that boasts 256 GPU cores. The UAV mechanics and electronics were designed and built in house, and are described in detail. Second, we present a novel “virtual reality” development environment, in which photorealistically-rendered synthetic on-board camera images are generated in real time while the UAV is in flight. This development environment allows us to rapidly prototype computing and sensing hardware as well as perception and control algorithms, using real physics, real interoceptive sensor data (e.g., from the on-board inertial measurement unit), and synthetic exteroceptive sensor data (e.g., from synthetic cameras). Third, we demonstrate repeated agile maneuvering with closed-loop vision-based perception and control algorithms, which we have developed using this environment.",https://ieeexplore.ieee.org/document/8460692/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSEET49119.2020.9206203,Welcome from the Conference Chairs,IEEE,Conferences,"In the last 30 years, the demands placed on software engineers have increased considerably. This is because the systems they develop increase in complexity, size, and criticality as well. Novel system types like adaptive systems or cyber physical systems and new technologies, like artificial intelligence and virtual reality, change the technology landscape. Professionals creating such systems hence need to stay on the cutting edge to face the challenges of the future.",https://ieeexplore.ieee.org/document/9206203/,2020 IEEE 32nd Conference on Software Engineering Education and Training (CSEE&T),9-12 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UKSim.2012.123,[Cover art],IEEE,Conferences,The following topics are dealt with: neural networks; evolutionary computation; adaptive dynamic programming; re-enforcement learning; bio-informatics; bio-engineering; computational finance; economics; semantic mining; data mining; virtual reality; data visualization; intelligent systems; soft computing; hybrid computing; e-science; e-systems; robotics; cybernetics; manufacturing; engineering; operations research; discrete event systems; real time systems; image processing; speech processing; signal processing; industry; business; social issues; human factors; marine simulation; power systems; logistics;parallel systems; distributed systems; software architectures;Internet modelling; semantic Web; ontologies; mobile ad hoc wireless networks; Mobicast; sensor placement; target tracking; circuits; sensors and devices.,https://ieeexplore.ieee.org/document/6205540/,2012 UKSim 14th International Conference on Computer Modelling and Simulation,28-30 March 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRIS.2017.101,[Front cover],IEEE,Conferences,The following topics are dealt with: education; Web technology; hospital; ZigBee; feature extraction; mobile smart tourism; marketing system; digital privacy; cement production; virtual reality; fire protection; rescue training; sports information dissemination model; lightning damage; electromechanical equipment fault signals; generalized regression neural network; cloud security intrusion detection; urban cultural landscape heritage protection; suspended centrifuge drum; VPN isolation gateway; Internet of Things; IBMS; service-oriented architecture; electronic product information; household service robot; power management chip; control system; PLC; pumped storage power station; fuzzy clustering algorithm; high performance CQRS architecture; association rule mining algorithm; electronic manufacturing industry; micro-blog; SVM; network anomaly traffic detection algorithm; parking lot; die mold process; moving target detection; tor anonymous network; IPv6 NetStream; domain ontology; machine tool; logistics; smart factory; SPSS analysis; agricultural monitoring data; computer software development; digital watermarking algorithm; and Big Data.,https://ieeexplore.ieee.org/document/8101323/,2017 International Conference on Robots & Intelligent System (ICRIS),15-16 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAS.2009.2,[Title page iii],IEEE,Conferences,The following topics are dealt with: real-time chain-structured synchronous dataflow; memory requirement formal determination; linear singular descriptor differential system; execution-optimized paths; greedy strategy; load trend evaluation; self-managed P2P streaming; context-aware ambient assisted living application; self-adaptive distributed model; autonomic systems; wireless sensor networks; topology control; learning based method; self-recovery method; mobile data sharing; heterogeneous QoS resource manager; component-based self-healing; NGN mobility; interactive user activity; .NET Windows service agent technology; agent based Web browser; resource-definition policies; autonomic computing; autonomic system administration; automatic database performance tuning; knowledge management; adaptive reinforcement learning; VoIP services; autonomic RSS: distributed virtual reality simulations; virtual machines resources allocation; multi-lier distributed systems; network I/O extensibility; virtual keyboards; self-configuring smart homes; legged underwater vehicles; particle filters; reusable semantic components; multi-agent systems; fixed-wing unmanned aerial vehicles; fuzzy inference system; robot swarms; mobile robots; optimization architecture; autonomous unmanned helicopter landing system design; heterogeneous multi-database environments; autonomic software license management system; Web server crashes prediction; laser range finder; video quality; wireless networks; ITU-T G.1030; open IMS core; context-aware data mining methodology; supply chain finance cooperative systems; autonomous pervasive environments; distributed generic stress tool; dynamic adaptive systems; multisensory media effects and user preference.,https://ieeexplore.ieee.org/document/4976566/,2009 Fifth International Conference on Autonomic and Autonomous Systems,20-25 April 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53024.2021.9733781,The Impact of Augmented Reality on Student Learning and Emotions in Mathematics,IEEE,Conferences,"The objective of this intervention project was to evaluate the impact of learning activities strengthened with Augmented Reality (AR) in Mathematics and the generation of positive and negative emotions in students who attended the third semester of the ITESM high school, Mexico City campus, where there had been a high percentage of failed students taking the subject &#x201C;Algebraic and Transcendent Functions.&#x201D; Evaluating the academic performance and the students&#x0027; emotions before and after carrying out the activities, we obtained a p-value of 0.0014 in the non-parametric Wilcoxon test, indicating a statistically significant change in acquired learning. AR activities were implemented using different pedagogical strategies. We attained statistically significant results, increasing students&#x0027; grades by 68&#x0025;, achieving a grade point average of 93.47 out of 100 in the topics covered. After the intervention, negative emotions decreased by 84.2&#x0025;, and positive emotions increased by 85&#x0025;. We concluded that AR is an excellent technological tool to support remote mathematics classes, changing how students interact. The results also indicated that changes in pedagogical strategies are essential to the successful implementation of the remote teaching modality; also, using various stimuli changed the emotions experienced by the students.",https://ieeexplore.ieee.org/document/9733781/,2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop,15-17 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISDFS55398.2022.9800837,The Necessity of Emotion Recognition from Speech Signals for Natural and Effective Human-Robot Interaction in Society 5.0,IEEE,Conferences,"The history of humanity has reached Industry 4.0 that aims to the integration of information technologies and especially artificial intelligence with all life-sustaining mechanisms in the 21st century, and consecutively, the transformation of Society 5.0 has begun. Society 5.0 means a smart society in which humans share life with physical robots and software robots as well as smart devices based on augmented reality. Industry 4.0 contains main structures such as the internet of things, big data analytics, digital transformation, cyber-physical systems, artificial intelligence, and business processes optimization. It is impossible to consider the machines to be without emotions and emotional intelligence within the transformation of smart tools and artificial intelligence, in addition, while it is planned to give most of the commands with voice and speaking, it became more important to develop algorithms that can detect emotions. In the smart society, new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-computer (HCI) or human-robot interaction (HRI) and collaboration. In this study, speech recognition and speech emotion recognition studies in robot technology are investigated and developments are revealed.",https://ieeexplore.ieee.org/document/9800837/,2022 10th International Symposium on Digital Forensics and Security (ISDFS),6-7 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO55190.2022.9803734,The Proposed Method of Measuring How Mixed Reality Can Affect the Enhancement of the User Experience,IEEE,Conferences,"The rapid development of technology and the acceptance of augmented reality (AR) and mixed reality (MR) opens a new chapter in human behavior. AR &#x0026; MR is changing how we walk, interact with other people, and live in physical and digital worlds. The line between the two worlds is becoming increasingly blurred. With a wide range of possibilities, the potential of AR &#x0026; MR to create new value for a person lies in its ability to create an enhanced personalized user experience in different aspects of human life. This paper proposes the methodology of measuring the extent of mixed reality influence on the user experience enhancement for the museum visitors. It measures the visitor&#x2019;s emotion type and level of emotional intensity during the artwork observation. Microsoft Kinect is used as a measurement device to determine emotion type and level of emotional intensity. Microsoft HoloLens will guide a visitor through the correct sequence of steps and ensure that all parts of artworks are visited in the correct order. The high precision of measurements obtained by Kinect and HoloLens make all measurements highly objective.",https://ieeexplore.ieee.org/document/9803734/,"2022 45th Jubilee International Convention on Information, Communication and Electronic Technology (MIPRO)",23-27 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00024,The Virtual Factory: Hologram-Enabled Control and Monitoring of Industrial IoT Devices,IEEE,Conferences,"Augmented reality (AR) has been exploited in manifold fields but is yet to be used at its full potential. With the massive diffusion of smart devices, opportunities to build immersive human-computer interfaces are continually expanding. In this study, we conceptualize a virtual factory: an interactive, dynamic, holographic abstraction of the physical machines deployed in a factory. Through our prototype implementation, we conducted a user-study driven evaluation of holographic interfaces compared to traditional interfaces, highlighting its pros and cons. Our study shows that the majority of the participants found holographic manipulation more attractive and natural to interact with. However, current performance characteristics of head-mounted displays must be improved to be applied in production.",https://ieeexplore.ieee.org/document/8613643/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE51222.2021.9404738,The learning approaches using Augmented Reality in learning environments: Meta-Analysis,IEEE,Conferences,"With the emergence of Industrial Revolution 4.0, the educational settings are changing quickly. Augmented Reality (AR) is one of the upcoming technologies. AR enhances the real world by overlaying/augmenting the virtual/digital information over it. It provides the user with the ability to interact with the created virtual world in real space. The aim of this study is to classify the learning approaches implemented through AR technology. The technique used for the analysis is derived from systematic search of online literature databases like Taylor Francis, Web of Science, Springer, ScienceDirect and Scopus. The keywords used for the search include learning approaches, AR, AR in education, AR in learning and teaching and integration approaches. The findings of this research work highlights 4 categories of educational learning approaches that highlight AR. The approaches are experimental learning, game-based, interactive and collaborative learning. The research findings can be referred by other researchers and educators to identify the potential of AR in education and the learning approaches currently used with AR for their further research on how these approaches can be effectively and efficiently implemented in educational settings.",https://ieeexplore.ieee.org/document/9404738/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00064,"Thermodynamics Reloaded: Experiencing Heating, Ventilation and Air Conditioning in AR",IEEE,Conferences,"Augmented Reality (AR) has great potential for new didactic concepts in teaching. Environments, information and objects can be comprehensively and dynamically represented, supporting self-paced and holistic learning. This paper presents an implementation of a multimodal AR-application for the purpose of teaching complex features and mechanics of a ”Heating, Ventilation and Air Conditioning System” in a situated and engaging way. The application was designed and implemented by an interdisciplinary team and evaluated in a mixed-methods approach. Results show a high usability and acceptance of the application. Students recognized the benefit of the application regarding their motivation and learning gains and made suggestions for further improvements.",https://ieeexplore.ieee.org/document/9319087/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWQoS.2018.8624176,Toward Smart and Cooperative Edge Caching for 5G Networks: A Deep Learning Based Approach,IEEE,Conferences,"The emerging 5G mobile networking promises ultrahigh network bandwidth and ultra-low communication latency (<;1ms), benefiting a wide range of applications, including live video streaming, online gaming, virtual and augmented reality, and Vehicle-to-X, to name but a few. The backbone Internet, however, does not keep up, particularly in latency (>100ms), due to its store-and-forward design and the physical barrier from signal propagation speed, not to mention congestion that frequently happens. Caching is known to be effective to bridge the speed gap, which has become a critical component in the 5G deployment as well. Besides storage, 5G base stations (BSs) will also be powered with strong computing modules, offering mobile edge computing (MEC) capability. This paper explores the potentials of edge computing towards improving the cache performance, and we envision a learning-based framework that facilitates smart caching beyond simple frequency- and time-based replace strategies and cooperation among base stations. Within this framework, we develop DeepCache, a deep-learning-based solution to understand the request patterns in individual base stations and accordingly make intelligent cache decisions. Using mobile video, one of the most popular applications with high traffic demand, as a case, we further develop a cooperation strategy for nearby base stations to collectively serve user requests. Experimental results on real-world dataset show that using the collaborative DeepCache algorithm, the overall transmission delay is reduced by 14%~22%, with a backhaul data traffic saving of 15%~23%.",https://ieeexplore.ieee.org/document/8624176/,2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS),4-6 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593814,Towards Real-Time Unsupervised Monocular Depth Estimation on CPU,IEEE,Conferences,"Unsupervised depth estimation from a single image is a very attractive technique with several implications in robotic, autonomous navigation, augmented reality and so on. This topic represents a very challenging task and the advent of deep learning enabled to tackle this problem with excellent results. However, these architectures are extremely deep and complex. Thus, real-time performance can be achieved only by leveraging power-hungry GPUs that do not allow to infer depth maps in application fields characterized by low-power constraints. To tackle this issue, in this paper we propose a novel architecture capable to quickly infer an accurate depth map on a CPU, even of an embedded system, using a pyramid of features extracted from a single input image. Similarly to state-of-the-art, we train our network in an unsupervised manner casting depth estimation as an image reconstruction problem. Extensive experimental results on the KITTI dataset show that compared to the top performing approach our network has similar accuracy but a much lower complexity (about 6% of parameters) enabling to infer a depth map for a KITTI image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard CPU. Moreover, by trading accuracy for efficiency, our network allows to infer maps at about 2 Hz and 40 Hz respectively, still being more accurate than most state-of-the-art slower methods. To the best of our knowledge, it is the first method enabling such performance on CPUs paving the way for effective deployment of unsupervised monocular depth estimation even on embedded systems.",https://ieeexplore.ieee.org/document/8593814/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct51615.2020.00076,"Towards Sailing supported by Augmented Reality: Motivation, Methodology and Perspectives",IEEE,Conferences,"Sailing is a multidisciplinary activity that requires years to master. Recently this sustainable sport is becoming even harder due to the increasing number of onboard sensors, automation, artificial intelligence, and the high performances obtainable with modern vessels and sail designs. Augmented Reality technology (AR) has the potential to assist sailors of all ages and experience level and improve confidence, accessibility, situation awareness, and safety. This work presents our ongoing research and methodology for developing AR assisted sailing. We started with the problem definition followed by a state of the art using a systematic review. Secondly, we elicited the main task and variables using an online questionnaire with experts. Third, we extracted the main variables and conceptualized some visual interfaces using 3 different approaches. As final phase, we designed and implemented a user test platform using a VR headset to simulate AR in different marine scenarios. For a real deployment, we witness the lack of available AR devices, so we are developing one specific headset dedicated to this task. We also envision the possible redesign of the entire boat as a consequence of the introduction of AR technology.",https://ieeexplore.ieee.org/document/9288459/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2010.184,Toys++ AR Embodied Agents as Tools to Learn by Building,IEEE,Conferences,"This paper presents a prototype for an augmented-reality based toy. Toys++ is grounded on the concept that the actual activity of building tangible artifacts can speed up learning processes. Toys ++ aims at assembling a framework that will allow the use of existing physical components of the toy as triggers. When the toy is placed under the webcam, a pre-trained 3D feature recognition system scans the entire figure, trying to identify some specific components. If any of these elements are recognized, the system will retrieve and show educational content from selected sources (texts, videos, pictures).",https://ieeexplore.ieee.org/document/5572598/,2010 10th IEEE International Conference on Advanced Learning Technologies,5-7 July 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVRV.2018.00025,Traffic Signs Detection and Augmented Reality Based on Multithreading,IEEE,Conferences,"The detection algorithm of existing traffic signs was improved, and the implementation of traffic sign detection and recognition was described in detail. This article mainly contains the following: Through targeted feature extraction, a trained cascaded classifier is used to obtain the location of traffic signs. Combined with the previous detection results, feature extraction is performed on the specified part of the target. The results are analyzed according to the location of detection or the results of the trained support vector machine are used to classify, and the classification results are obtained to realize the recognition of traffic signs. The total number of samples used for goal training in this article has reached thousands. The test results show that the detection rate of the traffic sign under the trained scene has been more than 90%. The algorithm has been optimized by the multi-thread method and combined with augmented reality to achieve real-time feedback.",https://ieeexplore.ieee.org/document/8711259/,2018 International Conference on Virtual Reality and Visualization (ICVRV),22-24 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASID50160.2020.9271762,Use of LSTM Regression and Rotation Classification to Improve Camera Pose Localization Estimation,IEEE,Conferences,"More accurately estimating camera pose can be used to greatly improve localization in applications such as augmented reality, autonomous driving, and intelligent robots. Deep learning methods have achieved great progress to improve accuracy but still have limitations with respect to rotation, which results in angle regression errors. In this work, we combine a LSTM module with rotation classification loss to regress the camera pose. The algorithm uses a robust processing pipeline to supervise the pose estimation with dynamic, weighted, multi-losses in order to limit separate Euler angle (yaw, pitch, roll) losses, and common translation-quaternion losses. An empirical test on the 7Scenes benchmark dataset shows better results than when using common absolute pose regression methods.",https://ieeexplore.ieee.org/document/9271762/,"2020 IEEE 14th International Conference on Anti-counterfeiting, Security, and Identification (ASID)",30 Oct.-1 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDUCON46332.2021.9454149,Using Augmented Reality in programming learning: A systematic mapping study,IEEE,Conferences,"Coding skills have become the new language of communication for the tech world. At an educational level, applying the concepts and logic of programming is a complex task for the student. The investigation of this problem was carried out with the intention of knowing if there are tools that could help the student understand programming by using augmented reality technology. For this purpose, a systematic mapping study was carried out to identify, filter and classify the information through a query applied in different ways of research. As a result, 34 articles were selected and classified. The main results show that: a) programming learning is not limited in terms of the student's age; b) Augmented reality has potential advantages in programming learning; c) Due to the extensive content of the programming, applications focused on specific topics were found according to the level of studies; d) The software used in the development of AR applications, mostly uses Unity with Vuforia; and, e) Augmented reality contributes to different learning techniques and styles that improve the way information is perceived and visualized. In conclusion, augmented reality technology has proven to have positive consequences in the programming learning process, providing a starting point for the development of a tool that contributes to the programming learning based on the characteristics found and analyzed in this document.",https://ieeexplore.ieee.org/document/9454149/,2021 IEEE Global Engineering Education Conference (EDUCON),21-23 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO.2019.8756928,Utilizing Apple’s ARKit 2.0 for Augmented Reality Application Development,IEEE,Conferences,"When it comes to practical augmented reality applications, mobile platform tools are the most deserving. Thanks to the nature of mobile devices and their everyday usage, the ideal basis for this kind of content has inadvertently formed itself. Consequently, within the iOS development environment, Apple's Xcode program enables application development using the ARKit library which delivers a host of benefits. Amongst the plethora of advantages, this paper focuses on utilizing features such as the ability to measure distances between two points in space, horizontal and vertical plane detection, the ability to detect three-dimensional objects and utilize them as triggers, and the consolidated implementation of ARKit and MapKit libraries in conjunction with the Google Places API intended for displaying superimposed computer-generated content on iOS 11 and later iterations of Apple's mobile operating system.",https://ieeexplore.ieee.org/document/8756928/,"2019 42nd International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)",20-24 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR.2019.00-22,VR Props: An End-to-End Pipeline for Transporting Real Objects Into Virtual and Augmented Environments,IEEE,Conferences,"Improvements in both software and hardware, as well as an increase in consumer suitable equipment, have resulted in great advances in the fields of virtual and augmented reality. Typically, systems use controllers or hand gestures to interact with virtual objects. However, these motions are often unnatural and diminish the immersion of the experience. Moreover, these approaches offer limited tactile feedback. There does not currently exist a platform to bring an arbitrary physical object into the virtual world without additional peripherals or the use of expensive motion capture systems. Such a system could be used for immersive experiences within the entertainment industry as well as being applied to VR or AR training experiences, in the fields of health and engineering. We propose an end-to-end pipeline for creating an interactive virtual prop from rigid and non-rigid physical objects. This includes a novel method for tracking the deformations of rigid and non-rigid objects at interactive rates using a single RGBD camera. We scan our physical object and process the point cloud to produce a triangular mesh. A range of possible deformations can be obtained by using a finite element method simulation and these are reduced to a low dimensional basis using principal component analysis. Machine learning approaches, in particular neural networks, have become key tools in computer vision and have been used on a range of tasks. Moreover, there has been an increased trend in training networks on synthetic data. To this end, we use a convolutional neural network, trained on synthetic data, to track the movement and potential deformations of an object in unlabelled RGB images from a single RGBD camera. We demonstrate our results for several objects with different sizes and appearances.",https://ieeexplore.ieee.org/document/8943647/,2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),14-18 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC54203.2021.9671133,Virtual Makeover and Makeup Recommendation Based on Personal Trait Analysis,IEEE,Conferences,"The utilization of facial makeup is an important attribute in modern society as a means of self-expression, and as a method to feel more confident during social interactions. A makeover has become a necessity when attending divergent functions, and makeup used on diverse occasions varies in style. Choosing the perfect makeup that best suits a person is challenging unless they have years of expertise with cosmetics. This paper proposes a “Virtual Makeover and Makeup Recommendation System” to eliminate the need to be concerned about the appearance after applying makeup. The proposed system enables real-time makeup simulation in an Augmented Reality (AR) environment and recommends makeup styles considering skin tone, colour of clothing and hair, type of clothing and occasion to be attended with makeup. The personal traits of a user would be automatically detected and processed to generate recommendations for makeup products, namely lipstick, foundation and eyeshadow. Complications of wasting makeup products and time, and cleaning makeup can be mitigated by using a real-time makeup simulation system. Recommendations generated by the application assist the users to decide on makeup styles and provides a better user experience. The proposed system is developed with the aid of Deep Learning (DL) algorithms.",https://ieeexplore.ieee.org/document/9671133/,2021 3rd International Conference on Advancements in Computing (ICAC),9-11 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SPA.2019.8936754,Vision-based mobile application for supporting the user in the vehicle operation,IEEE,Conferences,"In this paper the automatic vision-based mobile application for supporting the vehicle user is proposed. The system, using the image taken with the typical smartphone camera, recognizes three types of information located on the vehicle body: text data, displayed dashboard icons, and components in the engine compartment. The modern programming tools and recognition algorithms, including deep neural networks, were used in the implementation regarded to the limited efficiency of mobile devices. Preliminary tests using prepared database indicate on a high recognition performance. Recognition results and instructions are presented to the user in an accessible way using techniques of the augmented reality.",https://ieeexplore.ieee.org/document/8936754/,"2019 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)",18-20 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMV53313.2021.9670978,Voice Controlled Augmented Reality For Real Estate,IEEE,Conferences,"As technology advances, augmented reality is becoming more prevalent in every business. The most frequent usage of AR is to project real things onto the user, which is usually done via an image target. In the real estate industry, no one can deny AR's capacity to improve the buying and selling experience. AR can help real estate developers expand their marketing methods and give clients a more memorable home experience. It has already been used in apps for house design and land hunting, and the industry's strike proves that Augmented Reality has a lot more to give. Everyone nowadays has a smartphone or tablet with which to access the internet, and the technology utilized in these devices is improving every day. As a result, using AR tools in everyday life and having a comfortable AR experience on mobile devices is becoming more convenient. The amount of time spent touring each site with consumers and not having appropriate resources to impress them is a common challenge that real estate developers confront. Augmented reality software is frequently the seal of approval that realtors receive in order to grow their business and overcome these obstacles. This paper proposes a method for projecting a home onto an image target and allowing the user to explore the interior of the house. Voice controllers incorporated into AR can control the interior.",https://ieeexplore.ieee.org/document/9670978/,2021 International Conference on Artificial Intelligence and Machine Vision (AIMV),24-26 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3125501.3125528,Work-in-progress: towards efficient quantized neural network inference on mobile devices,IEEE,Conferences,"From voice recognition to object detection, Deep Neural Networks (DNNs) are steadily getting better at extracting information from complex raw data. Combined with the popularity of mobile computing and the rise of the Internet-of-Things (IoT), there is enormous potential for widespread deployment of intelligent devices, but a computational challenge remains. A modern DNN can require billions of floating point operations to classify a single image, which is far too costly for energy-constrained mobile devices. Offloading DNNs to powerful servers in the cloud is only a limited solution, as it requires significant energy for data transfer and cannot address applications with low-latency requirements such as augmented reality or navigation for autonomous drones.",https://ieeexplore.ieee.org/document/8094835/,"2017 International Conference on Compilers, Architectures and Synthesis For Embedded Systems (CASES)",15-20 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2018.00152,mmCNN: A Novel Method for Large Convolutional Neural Network on Memory-Limited Devices,IEEE,Conferences,"Deep learning recently has been widely used in many interactive application fields including but not limited to object recognition, speech recognition, natural language processing and so on. At the same time more and more attractive interactive applications (face recognition and augmented reality) are available on wearable and mobile devices. However, traditional deep learning methods such as CNN cost a lot of memory resources. This challenge makes it difficult to apply the powerful deep learning method on mobile memory limited platforms. In this paper we present a novel memory management strategy called mmCNN to solve this problem. This method helps us deploy a trained large size CNN on an any memory size platform including GPU, FPGA and memory-limited mobile devices. In our experiments, we run a feed-forward CNN process in an extremely small memory size (as low as 5MB) on a GPU platform. The result shows that our method saves more than 98% memory compared to a traditional CNN algorithm and further saves more than 90% compared to the sate-of-the-art related work ""vDNN"". Our work improve the computing scalability of interaction applications and break the memory bottleneck of using deep learning method on a memory-limited devices.",https://ieeexplore.ieee.org/document/8377777/,2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC),23-27 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE.2017.7889296,Efficient object detection and classification on low power embedded systems,IEEE,Conferences,"Identifying real world 3D objects such as pedestrians, vehicles and traffic signs using 2D images is a challenging task. There are multiple approaches to tackle this problem with varying degree of detection accuracy and implementation complexity. Some approaches use “hand coded” object features such as Histogram of Oriented Gradients (HOG), Haar, Scale Invariant Feature Transform (SIFT) along with a linear classifier such as Support Vector Machine (SVM), Adaptive Boosting (AdaBoost) to detect objects. Recent developments have shown that a deep multi-layered Convolution Neural Network (CNN) classifier can learn the object features on its own and also classify at an accuracy surpassing human vision. In this paper we combine both the approaches; “object detection” is done using HOG features and AdaBoost cascade classifier and “object classification” is done using CNN to classify the type of objects being detected. The proposed method is implemented on TI's low power TDA3x SoC.",https://ieeexplore.ieee.org/document/7889296/,2017 IEEE International Conference on Consumer Electronics (ICCE),8-10 Jan. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCWAMTIP51612.2020.9317440,Elaboration on Occupancy Networks and Resizing Strategy,IEEE,Conferences,"Occupancy networks introduces the decision boundary of neural network classifier to express arbitrarily complex three-dimensional topological structure, thereby solving the problem of incompatibility between the efficiency of calculation and the validity of storage in the field of 3D reconstruction. We briefly introduced the principle of occupancy network and its single-object 3D reconstruction effect. When further applying occupancy network to real online images, we propose a feasible resizing strategy to improve the versatility of occupancy network. We believe this will help occupancy network move towards a broader scene.",https://ieeexplore.ieee.org/document/9317440/,2020 17th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),18-20 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIT.2012.42,Embedded Low Power Controller for Autonomous Landing of UAV Using Artificial Neural Network,IEEE,Conferences,"We present real-time, stereo vision based autonomous landing system for small Unmanned Aerial Vehicles (UAV) onto an unknown landing target. The paper describes the algorithms and design of FPGA based co-processor implementing Artificial Neural Network (ANN) to implement real time object tracking, 3D position estimation using Visual Odometry(VO), Horizontal displacement and Euclidean distance from landing target. This approach doesn't require any explicit marker or landing target, it estimates attitude, track safe landing area, and compute distance and horizontal displacement form landing target. Experimental results show suitability of the real-time stereo vision landing approach using FPGA for tracking, that doesn't require any explicit landing marker.",https://ieeexplore.ieee.org/document/6424321/,2012 10th International Conference on Frontiers of Information Technology,17-19 Dec. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2011.118,Embracing Uncertainty: The New Machine Learning,IEEE,Conferences,"Summary form only given. Computers are based on logic, but must increasingly deal with real-world data that is full of uncertainty and ambiguity. Modern approaches to machine learning use probability theory to quantify and compute with this uncertainty, and have led to a proliferation in the applications of machine learning, ranging from recommendation systems to web search, and from spam filters to voice recognition. Most recently, the Kinect 3D full-body motion sensor, which has become the fastest-selling consumer electronics product in history, relies crucially on machine learning. Furthermore, the advent of widespread internet connectivity, with centralised data storage and processing, coupled with recently developed algorithms for computationally efficient probabilistic inference, will create many new opportunities for machine learning over the coming years. The talk will be illustrated with tutorial examples, demonstrations, and real-world case studies.",https://ieeexplore.ieee.org/document/6032316/,2011 IEEE 35th Annual Computer Software and Applications Conference,18-22 July 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC.2009.737,Emotion Simulation in Interactive Virtual Environment for Children's Safety Education,IEEE,Conferences,"Emotion simulation is very useful for modeling believable virtual character in safety education software, in order to enhance the efficacy of emotion simulation for Web3D applications, a fuzzy productive rule-based emotion model is presented, it integrates the mechanism of emotion decay and emotion filtering, an emotion intensity is calculated by Mamdani inference model. A 3D virtual interactive virtual environment is built in Virtools, a virtual character with the emotion model can express emotion to others, the result shows that the emotion simulation can improve the friendly interface of online education software.",https://ieeexplore.ieee.org/document/5367003/,2009 Fifth International Conference on Natural Computation,14-16 Aug. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AMT.2005.1505417,Empathy for Max [human-computer interaction],IEEE,Conferences,"This paper first describes two independently conducted research strands on affective human-computer interaction: one on an emotion simulation system for an expressive 3D humanoid agent called Max, which was designed at the University of Bielefeld; the other one on a real-time system for empathic (agent) feedback that is based on human emotional states derived from physiological information, and developed at the University of Tokyo and the National Institute of Informatics. Then, the integration of both systems is suggested for the purpose of realizing a highly believable agent with empathic qualities.",https://ieeexplore.ieee.org/document/1505417/,"Proceedings of the 2005 International Conference on Active Media Technology, 2005. (AMT 2005).",19-21 May 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3DUI.2015.7131759,Encountered haptic Augmented Reality interface for remote examination,IEEE,Conferences,This paper presents an interaction system for haptic based remote palpation and in general remote examination. In particular the proposed approach combines 3D representation of the remote environment with encountered haptic feedback aiming at high transparency and natureleness of interaction. The paradigm is described as interaction design and system implementation.,https://ieeexplore.ieee.org/document/7131759/,2015 IEEE Symposium on 3D User Interfaces (3DUI),23-24 March 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341392,End-to-end Contextual Perception and Prediction with Interaction Transformer,IEEE,Conferences,"In this paper, we tackle the problem of detecting objects in 3D and forecasting their future motion in the context of self-driving. Towards this goal, we design a novel approach that explicitly takes into account the interactions between actors. To capture their spatial-temporal dependencies, we propose a recurrent neural network with a novel Transformer architecture, which we call the Interaction Transformer. Importantly, our model can be trained end-to-end, and runs in real-time. We validate our approach on two challenging real-world datasets: ATG4D and nuScenes. We show that our approach can outperform the state-of-the-art on both datasets. In particular, we significantly improve the social compliance between the estimated future trajectories, resulting in far fewer collisions between the predicted actors.",https://ieeexplore.ieee.org/document/9341392/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEEECONF53024.2021.9733762,Enhancing Comprehension of the Internal Structure of the Atom with a Virtual 3D Environment,IEEE,Conferences,"An application in a 3D environment called AR Elements was used to improve the understanding of abstract concepts and maintain high school students&#x0027; motivation in studying inorganic chemistry. Concepts such as the atom, valence electrons, and orbitals were evaluated. The application was developed with Unity and Autodesk Maya software for 3D modeling, then a didactic sequence was applied to a group of 87 high school students to evaluate academic performance. The results showed a 20&#x0025; increase in the average evaluations of these topics compared to the previous two years. Eighty-nine per cent of students surveyed at the end of this research perceived that their learning improved using this tool.",https://ieeexplore.ieee.org/document/9733762/,2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop,15-17 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCC.2019.8843619,Enhancing STEM Education using Augmented Reality and Machine Learning,IEEE,Conferences,"Learning Science, Technology, Engineering and Mathematics (STEM) in the 21st century has been evolved from the conventional textbook to the interactive platform using electronic devices. This paper presents the implementation of a mobile application system, named AUREL (Augmented Reality Learning) in enhancing the learning experience by projecting Augmented Reality (AR) objects onto 2D images. This AR visualization is used to improve the understanding of STEM subjects and increases the enthusiasm of students towards STEM subjects. In this implementation, Google's Cloud Tensor Processing Units (TPUs) are used to train specific datasets alongside Cloud Vision API to detect a wide range of objects. ML Kit for Firebase is used to host the custom TensorFlow Lite models for specific use cases for better accuracy. On the other hand, Google Cloud Platform (GCP) is used to harvest STEM data, manage STEM 3D information and data processing. Subsequently, the processed information will be displayed in AR in the mobile application using ARCore's Sceneform SDK. The application of AUREL could be extended to all science subjects so that students can learn using an interactive platform.",https://ieeexplore.ieee.org/document/8843619/,2019 7th International Conference on Smart Computing & Communications (ICSCC),28-30 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863468,Estimated force emulation for space robot using neural networks,IEEE,Conferences,"This paper introduces the telerobotic system estimated force emulation using neural networks. A delay-compensating 3D stereo-graphic simulator is implemented in SGI ONYX/4 RE/sup 2/. The estimated force emulation can protect the real robot in time from being damaged in collision. The neural network is used to learn the mapping between the contact force error and the accommodated position command to the controller of the space robot. Finally, the controller can feel the emulated force with a two-hand 6-DOF master arm using the force feedback interface.",https://ieeexplore.ieee.org/document/863468/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2007.4415203,Evolving Personality of a Genetic Robot in Ubiquitous Environment,IEEE,Conferences,"This paper discusses the personality of genetic robot and its evolving algorithm within the purview of the broader ubiquitous robot framework. Ubiquitous robot systems blends mobile robot technology (Mobot) with distributed sensor systems (Embot) and overseeing software intelligence (Sobot), for various integrated services. The Sobot is a critical question since it performs the dual purpose of overseeing intelligence as well as user interface. The Sobot is hence modelled as an artificial creature with autonomously driven behavior. The artificial creature has its own genome and in which each chromosome consists of many genes that contribute to defining its personality. This paper proposes evolving the personality of an artificial creature. A genome population is evolved such that it customized the genome satisfying a set of personality traits desired by the user. Evaluation procedure for each genome of the population is carried out in a virtual environment. Effectiveness of this scheme is demonstrated by using an artificial creature, Rity in the virtual 3D world created in a PC.",https://ieeexplore.ieee.org/document/4415203/,RO-MAN 2007 - The 16th IEEE International Symposium on Robot and Human Interactive Communication,26-29 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIBM52615.2021.9669453,Explainable Prediction of Whether The Acetabular Cup Is Placed in The &#x201C;Safe Zone&#x201D; from X-ray Images,IEEE,Conferences,"Appropriate acetabular cup placement is an important step for the prevention of postoperative dislocation in total hip arthroplasty. Existing state-of-the-art methods focus on developing statistical methods and image-matching techniques. Such developments employ regression equations or 3D planning software to evaluate the acetabular cup angles, and little attention has been given to automatic prediction with machine/deep learning. In addition, existing approaches ignore the explainability and clinical utility of models, which creates a tension between accuracy and explainability. This paper proposes an innovative hybrid framework for acetabular cup angle prediction. The proposed method is based on deep stacked network (DSN) and SHapley Additive exPlanations (SHAP) with four improvements: (1) an arc-support line segments-based unsupervised segmentation module, obtaining the region of interest (ROI) occupied by the acetabular cup component automatically and efficiently; (2) a multiscale feature extraction and attribute optimization module, obtaining comprehensive radiomic features including the conventional texture features and deep learning features; (3) appending a SHAP module to the framework to improve the explainability of models through three main steps: local explanation, global explanation, and interactive visualization; (4) our method employs DSN method to predict the acetabular cup angles, and it considers the decision curve analysis (DCA) to tackle the problem of clinical utility evaluation. By integrating the above improvements in series, the models&#x2019; performances are gradually enhanced. Experimental results show that our model achieves superior results to existing acetabular cup angle prediction approaches.",https://ieeexplore.ieee.org/document/9669453/,2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),9-12 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIEDS49339.2020.9106581,"Explorer51 – Indoor Mapping, Discovery, and Navigation for an Autonomous Mobile Robot",IEEE,Conferences,"The nexus of robotics, autonomous systems, and artificial intelligence (AI) has the potential to change the nature of human guided exploration of indoor and outdoor spaces. Such autonomous mobile robots can be incorporated into a variety of applications, ranging from logistics and maintenance, to intelligence gathering, surveillance, and reconnaissance (ISR). One such example is that of a tele-operator using the robot to generate a map of the inside of a building while discovering and tagging the objects of interest. During this process, the tele-operator can also assign an area for the robot to navigate autonomously or return to a previously marked area/object of interest. Search and rescue and ISR abilities could be immensely improved with such capabilities. The goal of this research is to prototype and demonstrate the above autonomous capabilities in a mobile ground robot called Explorer51. Objectives include: (i) enabling an operator to drive the robot non-line of sight to explore a space by incorporating a first-person view (FPV) system to stream data from the robot to the base station; (ii) implementing automatic collision avoidance to prevent the operator from running the robot into obstacles; (iii) creating and saving 2D and 3D maps of the space in real time by using a 2D laser scanner, tracking, and depth/RGB cameras; (iv) locating and tagging objects of interest as waypoints within the map; (v) autonomously navigate within the map to reach a chosen waypoint.To accomplish these goals, we are using the AION Robotics R1 Unmanned Ground Vehicle (UGV) rover as the platform for Explorer51 to demonstrate the autonomous features. The rover runs the Robot Operating System (ROS) onboard an NVIDIA Jetson TX2 board, connected to a Pixhawk controller. Sensors include a 2D scanning LiDAR, depth camera, tracking camera, and an IMU. Using existing ROS packages such as Cartographer and TEB planner, we plan to implement ROS nodes for accomplishing these tasks. We plan to extend the mapping ability of the rover using Visual Inertial Odometry (VIO) using the cameras. In addition, we will explore the implementation of additional features such as autonomous target identification, waypoint marking, collision avoidance, and iterative trajectory optimization. The project will culminate in a series of demonstrations to showcase the autonomous navigation, and tele-operation abilities of the robot. Success will be evaluated based on ease of use by the tele-operator, collision avoidance ability, autonomous waypoint navigation accuracy, and robust map creation at high driving speeds.",https://ieeexplore.ieee.org/document/9106581/,2020 Systems and Information Engineering Design Symposium (SIEDS),24-24 April 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/iLRN52045.2021.9459415,Exploring the Real-Time Touchless Hand Interaction and Intelligent Agents in Augmented Reality Learning Applications,IEEE,Conferences,"During the last decade, there has been a surge in research studies exploring the adoption of Augmented Reality (AR) in educational settings. Within these multiple research studies, AR's capability to extend the teaching and learning environment with augmented 3D learning objects with enhanced interactive capabilities have been demonstrated. This new technology has not been widely adopted in the mainstream but with the recent unprecedented circumstances of COVID-19, there has been an increasing societal willingness to adopt these technologies. AR has been a desirable technology due to its inherent touchless nature which facilitates social distancing at this time but AR applications crucially offer so much more. They can provide interactive functionality through augmentation of the teaching and learning environment within an immersive user experience including 3D interactions with learning objects, gestures, hand interaction, tangible and multi-modal interaction. This paper presents the results of a review of touchless interaction studies in educational applications and proposes the implementation of real-time touchless hand interaction within kinesthetic learning and utilization of machine learning agents. The architecture of two AR applications with real-time hand interaction and machine learning agents are demonstrated within this paper enabling engaged kinesthetic learning as an alternative learning interface.",https://ieeexplore.ieee.org/document/9459415/,2021 7th International Conference of the Immersive Learning Research Network (iLRN),17 May-10 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECTC32862.2020.00276,Extracting power supply current profile by using interposer-based low-noise probing technique for PDN design of high-density POP,IEEE,Conferences,"Firmly understanding the power supply current profile (PSCP) of various scenarios used in real use cases is essential for the simulation and design of power delivery network (PDN) of system-on-chip (SOC) to maximize processor's performance within limited cost budget for die, package, and system, because the low power hard-ware implementation of leading-edge SOC including high performance computing cores for video data processing, 3D graphics, augmented reality, artificial intelligence, and 5G data communication with battery powered portable electronic devices, whose primary concern is the low power consumption, has been concentrated on reducing the minimum allowable power supply voltage for high performance computing cores including CPU, GPU, NPU and CP. The objective of this work is presenting the method to precisely probe power supply voltage fluctuation (PSVF) of whole power domains for power supply current profile (PSCP) extraction of entire cores, for which the authors present an concrete analysis methodology, based on which a test interposer scheme targeted for probing core logic blocks at the proper position of PDN is implemented and demonstrated when in operation. The proposed low noise probing system for acquiring PSCP is constructed by a test interposer designed with rigorous PI analyses.",https://ieeexplore.ieee.org/document/9159523/,2020 IEEE 70th Electronic Components and Technology Conference (ECTC),3-30 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEDSS.2017.8073687,FPGA design of efficient kidney image classification using algebric histogram feature model and sparse deep neural network (SDNN) techniques,IEEE,Conferences,"This work proposed a sparse learning optimization based kidney image classification using Sparse Deep Neural Network (SDNN) similarity measure. The kidney images are acquired and preprocessed to improve the image quality. Then the method splits the image into number of sectional image which Crops the Image for Number of Times Using Feature Extraction Method. Feature extraction using Algebraic Histogram based Sum and difference model to extract the 2d and 3d features of kidney image. The textures features are segmented by Algebric histogram (AH) method and the features are optimized using the hardware model. Based on the sparse classification depthness measure the method identifies the region which has affected or abnormal. At the classification stage, the method computes the sparse learning based pixel similarity measure to identify the most affected region and to perform classification. The entire hardware model can be split into two important levels namely Algebric histogram based Feature extraction and sparse learning based feature classification using SDNN technique. The identification of the kidney Abnormality in the image is displayed with colour for easy identification and visibility in monitor using HDL algorithms. The design and implementation in real time on both Field Programmable Gate Array (FPGA) using Xilinx System Generator (XSG) and Matlab 2013a.",https://ieeexplore.ieee.org/document/8073687/,2017 Conference on Emerging Devices and Smart Systems (ICEDSS),3-4 March 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00058,FaceAUG: A Cross-Platform Application for Real-Time Face Augmentation in Web Browser,IEEE,Conferences,"This paper presents FaceAUG, a cross-platform application for real-time face augmentation in a web browser. Human faces are detected and tracked in real-time from the video stream of the embedded or separated webcam of the user device. Then, the application overlays different 2D or 3D augmented reality (AR) filters and effects over the region of the detected face(s) to achieve a mixed virtual and AR effect. A 2D effect can be a photo frame or a 2D face mask using an image from the local repository. A 3D effect is a 3D face model with a colored material, an image texture, or a video texture. The application uses TensorFlow.js to load the pre-trained Face Mesh model for predicting the regions and landmarks of the faces that appear in the video stream. Three.js is used to create the face geometries and render them using the material and texture selected by the user. FaceAUG can be used on any device, as long as an internal or external camera and a state-of-the-art web browser are accessible on the device. The application is implemented using front-end techniques and is therefore functional without any server-side supports at back-end. Experimental results on different platforms verified the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/9319122/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CGIV.2006.41,Facial Animation Using Emotional Model,IEEE,Conferences,"Recent 3D graphics hardware technologies have made it possible to create visually realistic 3D characters and 3D scenes in real time. And then, the video game has become one of the main applications of 3D graphics technologies. However, behaviors of Non Player Characters (NPCs) in video games are not satisfactory because they are still predefined and then very simple. To attract game players, NPCs should have more complicated behaviors like the human. Facial expression is one of the most important factors for such a humanlike NPC. In this paper, the authors propose an NPC which interacts with the human. The NPC changes its facial expression according to its emotion during the interaction. For realizing such an NPC, the authors implemented a neural network based emotional model unit [5]. By some experiments, this paper also shows that the NPC can change its facial expression according to its emotion like the human.",https://ieeexplore.ieee.org/document/1663828/,"International Conference on Computer Graphics, Imaging and Visualisation (CGIV'06)",26-28 July 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1997.633250,Facial interaction between animated 3D face robot and human beings,IEEE,Conferences,"We study the realization of a realistic human-like response of an animated 3D face robot in communicative interaction with human beings. The face robot can produce human-like facial expressions and recognize human facial expressions using facial image data obtained by a CCD camera mounted inside the left eyeball. We developed the real time machine recognition of facial expressions by using a layered neural network and achieved a high correct recognition ratio of 85% with respect to 6 typical facial expressions of 15 subjects in 55 ms. We also developed a new small-size actuator for display of facial expressions on the face robot, giving the same speed in dynamic facial expressions as in human even in the case of a high-speed expression of ""surprise"". For facial interactive communication between the face robot and human beings, we integrated these two technologies to produce the facial expression in respond to the recognition result of the human facial expression in real time. This implies a high technological potential for the animated face robot to undertake interactive communication with human when an artificial emotion being implemented.",https://ieeexplore.ieee.org/document/633250/,"1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation",12-15 Oct. 1997,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSAI48974.2019.9010197,Fall detection and recognition based on GCN and 2D Pose,IEEE,Conferences,"In this paper, based on the motion properties of human skeleton, the neural network model based on graph convolution network (GCN) and 2D pose is introduced to carry out deep learning and classification of target actions, so as to detect falls. The experimental results show that our method is superior to the existing methods in two-dimensional pose, and the accuracy of the two aspects is not very different in 3D pose. However, we use GPU acceleration for testing on ubuntu platform with an average speed of 25fps, which basically meets the requirements of real-time engineering calculation and is easier to deploy to the real environment than 3D pose.",https://ieeexplore.ieee.org/document/9010197/,2019 6th International Conference on Systems and Informatics (ICSAI),2-4 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISBI45749.2020.9098373,Fast High Dynamic Range MRI by Contrast Enhancement Networks,IEEE,Conferences,"HDR-MRI is a sophisticated non-linear image fusion technique for MRI which enhances image quality by fusing multiple contrast types into a single composite image. It offers improved outcomes in automatic segmentation and potentially in diagnostic power, but the existing technique is slow and requires accurate image co-registration in order to function reliably. In this work, a lightweight fully convolutional neural network architecture is developed with the goal of approximating HDR-MRI images in real-time. The resulting Contrast Enhancement Network (CEN) is capable of performing near-perfect ( SSIM = 0.98) 2D approximations of HDR-MRI in 10ms and full 3D approximations in 1s, running two orders of magnitude faster than the original implementation. It is also able to perform the approximation ( SSIM = 0.93) with only two of the three contrasts required to generate the original HDR-MRI image, while requiring no image co-registration.",https://ieeexplore.ieee.org/document/9098373/,2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI),3-7 April 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME51207.2021.9428210,Fast Multi-Scale Residual Fusion Network for Stereo Matching,IEEE,Conferences,"Recent deep convolution-based stereo matching methods have shown significant progress. However, most state-of-the-art models achieve high accuracy by using 3D convolutions during cost aggregation, which come with more floating-point computations and makes it difficult for deployment in real-time applications. In this paper, we propose an effective and efficient multi-scale aggregation module (without 3D convolution) to build our fast Multi-Scale Residual Fusion Network (MSRFNet). Three lightweight components are involved in our aggregation module: combination block, attention block, and residual fusion block. The combination block combines features with different receptive fields and the attention block emphasizes salient regions in various cost volumes. The residual fusion module focuses on extracting the differences between adjacent cost volumes, using progressively residual aggregation instead of simply stacking or adding. Extensive experiments on Scene Flow and KITTI benchmarks demonstrate that our method achieves competitive accuracy com-pared with state-of-the-art methods while running at 56ms.",https://ieeexplore.ieee.org/document/9428210/,2021 IEEE International Conference on Multimedia and Expo (ICME),5-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCIS.2009.305,Fast Path Searching in Real Time 3D Game,IEEE,Conferences,"This paper presents an efficient path searching system implemented in real time 3D game scene. Our method focus on the base element in 3D models - polygons and mesh, treat the scene as a 3D surface. Through the connectivity analyzes of these polygons, we convert the scene data to an abstract contour model and do heuristic search on it. A path searching task management mechanism had been carried out to balance the processing time of each frame loop. Our system could work without any factitious mark and script information in the scene and high processing had been achieved.",https://ieeexplore.ieee.org/document/5209169/,2009 WRI Global Congress on Intelligent Systems,19-21 May 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2006.341630,Fast an Robust Face Tracking for CNN chips: application to wheelchair driving,IEEE,Conferences,"An algorithm for fast and robust face tracking with the CNN universal machine is proposed in this paper. It is applied to a driving mechanism for a wheelchair with an on-chip implementation. A novel object tracking CNN visual algorithm is introduced and employed in the tracking of multiple face features. The speed and robustness of this method are achieved due to the parallelism in the visual algorithm, and the tracking of multiple face features. The tracking algorithm is designed to achieve a high frame rate and exploit the specific properties of face features. The face tracking method proposed here was implemented on a Bi-I stand-alone cellular vision system and applied to a wheelchair driving mechanism. The template operations were trained and/or fine-tuned in order to generate chip-specific robust templates. In order to improve performance in environments with varying illumination, an adaptive image capture procedure was also introduced. Our simulations with a 3D model wheelchair showed that the final algorithm is capable of performing tracking with a frame rate of 92 frames/sec, which is supposedly enough for real-time driving in most of the real life situations",https://ieeexplore.ieee.org/document/4145870/,2006 10th International Workshop on Cellular Neural Networks and Their Applications,28-30 Aug. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV51458.2022.00380,Fast-CLOCs: Fast Camera-LiDAR Object Candidates Fusion for 3D Object Detection,IEEE,Conferences,"When compared to single modality approaches, fusion-based object detection methods often require more complex models to integrate heterogeneous sensor data, and use more GPU memory and computational resources. This is particularly true for camera-LiDAR based multimodal fusion, which may require three separate deep-learning networks and/or processing pipelines that are designated for the visual data, LiDAR data, and for some form of a fusion framework. In this paper, we propose Fast Camera-LiDAR Object Candidates (Fast-CLOCs) fusion network that can run high-accuracy fusion-based 3D object detection in near real-time. Fast-CLOCs operates on the output candidates before Non-Maximum Suppression (NMS) of any 3D detector, and adds a lightweight 3D detector-cued 2D image detector (3D-Q-2D) to extract visual features from the image domain to improve 3D detections significantly. The 3D detection candidates are shared with the proposed 3D-Q-2D image detector as proposals to reduce the network complexity drastically. The superior experimental results of our Fast-CLOCs on the challenging KITTI and nuScenes datasets illustrate that our Fast-CLOCs outperforms state-of-the-art fusion-based 3D object detection approaches. We will release the code upon publication.",https://ieeexplore.ieee.org/document/9706631/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCWD54268.2022.9776227,FedRME: Federated Road Markings Extraction from Mobile LiDAR Point Clouds,IEEE,Conferences,"Road markings extraction (RME) from 3D point clouds acquired by mobile LiDAR systems has been widely used for road safety and autonomous driving. However, due to the increasing awareness of personal data protection and national informat ion security regulations, most autonomous driving companies are not willing to share their private point clouds data with the community. Therefore, such restriction of centralized training might inevitably inhibit the effectiveness of RME procedure. Federated learning (FL) is a distributed machine learning architecture that could address the aforementioned privacy-accuracy dilemma to collaboratively learn a global RME model from multiple clients without sharing raw data. In this paper, we propose a novel FedRME, a federated road markings extraction system to collaboratively learn a global RME model with multiple privacy-preserved local models from 3D mobile LiDAR point clouds. FedRME adopt the classical FedAvg model to construct a generalizable global feature embedding model without accessing local data. Moreover, to tackle data heterogeneity problem that local models vary in point clouds volumes and categories, we design a dynamic weighting mechanism to optimize the cooperative training effectiveness before server aggregation. Experimental results on three real-world mobil e LiDAR point clouds datasets with federated learning settings demonstrate that FedRME not only achieves superior performance but also reduces computation by up to 25%.The source code is available at https://github.com/WwZzz/easyFL#FedRME.",https://ieeexplore.ieee.org/document/9776227/,2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD),4-6 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECO.2019.8760001,Fiber-based Photonic-FPGA Architecture and In-Fiber Computing,IEEE,Conferences,"Hardware implementation of artificial neural networks facilitates real-time parallel processing of massive data sets. Optical neural networks offer low-volume 3D connectivity together with large bandwidth and minimal heat production in contrast to electronic implementation. In this presentation we will present a conceptual design for in-fiber optical neural network, i.e. a fiber-based realization of a photonic-FPGA. Neurons and synapses are realized in two ways: first as individual silica cores in a multi-core fiber and then within a multi-mode fiber. In the first realization optical signals are transferred transversely between cores by means of optical coupling. Pump driven amplification in erbium-doped cores mimics synaptic interactions. Simulations and experimental validation show classification and learning capabilities. Therefore, devices similar to our proposed multi-core fiber could potentially serve as building blocks for future large-scale small-volume optical artificial neural networks. In the second type of realization we propose the design of an optical artificial neural network-based imaging system that has the ability to self-study image signals from an incoherent light source in different colors. Our design consists of a multi-mode fiber realizing a stochastic neural network. We show that the signals, transmitted through the multi-mode fiber, can be used for image identification purposes and can also be reconstructed using artificial neural networks with a low number of nodes.",https://ieeexplore.ieee.org/document/8760001/,2019 8th Mediterranean Conference on Embedded Computing (MECO),10-14 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.1998.677303,Flexible path planning for real-time applications using A*-method and neural RBF-networks,IEEE,Conferences,"We developed a generally applicable concept for flexible path planning and representation in high-dimensional configuration spaces. Therefore, an AI-algorithm for fast preprocessing and a neural network were combined. Specifically, the standard A*-method was developed into an advanced A*-method (AA*-method) by creating an additional class of FREE-cells to enlarge the computed surroundings of the detected optimal path, and by constituting expansion matrices to enable flexible modeling of different cell extents and configuration spaces. Furthermore, a neural RBF-network was modified by adding an activity peak generating neuron guaranteeing updates in real-time (less than 1 ms). The output of the AA*-method, a set of classified cells, was used to train the modified RBF-network. The capabilities of this novel hybrid path planning system are demonstrated for various complex 3D- and 6D- path planning tasks.",https://ieeexplore.ieee.org/document/677303/,Proceedings. 1998 IEEE International Conference on Robotics and Automation (Cat. No.98CH36146),20-20 May 1998,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/R10-HTC.2018.8629835,Foody - Smart Restaurant Management and Ordering System,IEEE,Conferences,"Customers play a vital role in the contemporary food industry when determining the quality of the restaurant and its food. Restaurants give considerable attention to customers’ feedback about their service, since the reputation of the business depends on it. Key factors of evaluating customer satisfaction are, being able to deliver the services effectively to lessen the time of consumption, as well as maintaining a high quality of service. In most cases of selecting a prominent restaurant, customers focus on their choice of favorite food in addition to available seating and space options. Long waiting times and serving the wrong order is a common mistake that happens in every restaurant that eventually leads to customer dissatisfaction. Objectives of this online application “Foody” is to address these deficiencies and provide efficient and accurate services to the customer, by providing unique menus to each customer considering their taste. This concept is implemented as a mobile application using latest IT concepts such as Business Intelligence, Data Mining, Predictive Analysis and Artificial Intelligence. This includes graphics and 3D modeling that provide existent physical information related to food such as colors, sizes and further user can view the ingredients of the meal as well as the available tables. In addition, the app shows the real-time map to the restaurant. Current table reservation status is indicated by the color change of the table. Unique food recommendation and it’s order for each customer is generated by analyzing their social media information and the system notifies the customer the wait time by calculating it. Preparation of food and allocation is done subjectively. The expected outcome of the research is to develop a fully automated restaurant management system with the mentioned features as well as to avoid confusions between orders, provide better view of food and allow the customer to choose the menu according to their taste in a minimum time.",https://ieeexplore.ieee.org/document/8629835/,2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC),6-8 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041373,Footstep planning on uneven terrain with mixed-integer convex optimization,IEEE,Conferences,"We present a new method for planning footstep placements for a robot walking on uneven terrain with obstacles, using a mixed-integer quadratically-constrained quadratic program (MIQCQP). Our approach is unique in that it handles obstacle avoidance, kinematic reachability, and rotation of footstep placements, which typically have required non-convex constraints, in a single mixed-integer optimization that can be efficiently solved to its global optimum. Reachability is enforced through a convex inner approximation of the reachable space for the robot's feet. Rotation of the footsteps is handled by a piecewise linear approximation of sine and cosine, designed to ensure that the approximation never overestimates the robot's reachability. Obstacle avoidance is ensured by decomposing the environment into convex regions of obstacle-free configuration space and assigning each footstep to one such safe region. We demonstrate this technique in simple 2D and 3D environments and with real environments sensed by a humanoid robot. We also discuss computational performance of the algorithm, which is currently capable of planning short sequences of a few steps in under one second or longer sequences of 10-30 footsteps in tens of seconds to minutes on common laptop computer hardware. Our implementation is available within the Drake MATLAB toolbox [1].",https://ieeexplore.ieee.org/document/7041373/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WF-IoT.2019.8767297,Framework-supported mechanism of testing algorithms for assessing memory and detecting disorientation from IoT sensors,IEEE,Conferences,"Assessing memory in daily life can be useful for both early detection and tracking the symptoms of neurodegenerative disease such as Alzheimer's disease. One of these symptoms is disorientation, which can cause domestic accidents. With the avenue of Internet of Things (IoT), houses can collect big data in real-time for being processed and acquiring knowledge between both edge computing (local processing for avoiding communication overload) and the cloud computing conforming the fog computing. What sensors are needed and how to collect the information is something that requires actually preparing prototypes of this setup in a lab. As an alternative, this work proposes a cheaper method which is based on the use of realistic 3D simulations of the scenarios to consider. Using these scenarios, different algorithms for assessing memory can be compared and evaluate how well they detect disorientation of a person living in a house by means of tracking the movement of a person. The contribution of this paper is a case study where researchers can design the scenarios, including the activity of the daily living of the individuals, the array of sensors to be deployed, and the disorientation detection algorithms.",https://ieeexplore.ieee.org/document/8767297/,2019 IEEE 5th World Forum on Internet of Things (WF-IoT),15-18 April 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS48704.2020.9184578,Front Moving Object Behavior Prediction System Exploiting Deep Learning Technology for ADAS Applications,IEEE,Conferences,"This paper proposes a front pedestrian crossing and vehicle cut-in prediction system based on 3D convolution behavior prediction network. The proposed design improves the original 3D convolution network (C3D) to make behavior recognition network have the ability of object localization, which is important to detect multiple moving object behaviors. The proposed system is implemented on the embedded system in real-time, which achieves 20 frames per second when it is deployed on NVIDIA Jetson AGX Xavier and possesses over 92.8% accuracy for pedestrian crossing and 94.3% accuracy for vehicle cut-in behavior detection.",https://ieeexplore.ieee.org/document/9184578/,2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS),9-12 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCT53883.2021.9642700,Fruits Detection and Distance Estimation using RGB-D camera for Harvesting Robot,IEEE,Conferences,"In this paper, fruit detection and distance estimation from detected fruits are processed in parallel based on a low-cost and compact RGB-D camera. Fruits are detected in RGB images and surrounded by bounding boxes using modified deep-learning models, which are optimized for high accuracy based on limited computational resources. Distances from the detected fruits to the camera as well as the size of those fruits are estimated based on their respective boxes in depth images, which provided third-dimensional knowledge or 3D localization of the fruits. The proposed method is implemented on an embedded computer connected to an RGB-D camera and validated in the real environment. Experimental results show that high accuracy in detection, distance, and fruit’s size estimation has been achieved. The promising results enable the further grasping action for a harvesting robot.",https://ieeexplore.ieee.org/document/9642700/,2021 International Conference on Science & Contemporary Technologies (ICSCT),5-7 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV.2014.6835736,Fully automatic 3D facial expression recognition using local depth features,IEEE,Conferences,"Facial expressions form a significant part of our nonverbal communications and understanding them is essential for effective human computer interaction. Due to the diversity of facial geometry and expressions, automatic expression recognition is a challenging task. This paper deals with the problem of person-independent facial expression recognition from a single 3D scan. We consider only the 3D shape because facial expressions are mostly encoded in facial geometry deformations rather than textures. Unlike the majority of existing works, our method is fully automatic including the detection of landmarks. We detect the four eye corners and nose tip in real time on the depth image and its gradients using Haar-like features and AdaBoost classifier. From these five points, another 25 heuristic points are defined to extract local depth features for representing facial expressions. The depth features are projected to a lower dimensional linear subspace where feature selection is performed by maximizing their relevance and minimizing their redundancy. The selected features are then used to train a multi-class SVM for the final classification. Experiments on the benchmark BU-3DFE database show that the proposed method outperforms existing automatic techniques, and is comparable even to the approaches using manual landmarks.",https://ieeexplore.ieee.org/document/6835736/,IEEE Winter Conference on Applications of Computer Vision,24-26 March 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSAI.2017.8248355,Fully convolutional denoising autoencoder for 3D scene reconstruction from a single depth image,IEEE,Conferences,"In this work, we propose a 3D scene reconstruction algorithm based on a fully convolutional 3D denoising autoencoder neural network. The network is capable of reconstructing a full scene from a single depth image by creating a 3D representation of it and automatically filling holes and inserting hidden elements. We exploit the fact that our neural network is capable of generalizing object shapes by inferring similarities in geometry. Our fully convolutional architecture enables the network to be unconstrained by a fixed 3D shape, and so it is capable of successfully reconstructing arbitrary scene sizes. Our algorithm was evaluated on a real word dataset of tabletop scenes acquired using a Kinect and processed using KinectFusion software in order to obtain ground truth for network training and evaluation. Extensive measurements show that our deep neural network architecture outperforms the previous state of the art both in terms of precision and recall for the scene reconstruction task. The network has been broadly profiled in terms of memory footprint, number of floating point operations, inference time and power consumption in CPU, GPU and embedded devices. Its small memory footprint and its low computation requirements enable low power, memory constrained, real time always-on embedded applications such as autonomous vehicles, warehouse robots, interactive gaming controllers and drones.",https://ieeexplore.ieee.org/document/8248355/,2017 4th International Conference on Systems and Informatics (ICSAI),11-13 Nov. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2004.1374845,Fuzzy reinforcement learning for an evolving virtual servant robot,IEEE,Conferences,"This work presents our research in the application of reinforcement learning algorithms for the generation of autonomous intelligent virtual robots, that can learn and enhance their task performance in assisting humans in housekeeping. For the control system architecture of the virtual agents, two algorithms, based on Watkins' Q(/spl lambda/) learning and the zeroth-level classifier system (ZLCS), are incorporated with fuzzy inference systems(FlS). Performance of these algorithms is evaluated and compared. A 3D application of a virtual robot whose task is to interact with virtual humans and offer optimal services on everyday in-house needs is designed and implemented. The learning systems are incorporated in the decision-making process of the virtual robot servant to allow itself to understand and evaluate the fuzzy value requirements and enhance its performance.",https://ieeexplore.ieee.org/document/1374845/,RO-MAN 2004. 13th IEEE International Workshop on Robot and Human Interactive Communication (IEEE Catalog No.04TH8759),22-22 Sept. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324476,Generalized framework for the parallel semantic segmentation of multiple objects and posterior manipulation,IEEE,Conferences,"The end-to-end approach presented in this paper deals with the recognition, detection, segmentation and grasping of objects, assuming no prior knowledge of the environment nor objects. The proposed pipeline is as follows: 1) Usage of a trained Convolutional Neural Net (CNN) that recognizes up to 80 different classes of objects in real time and generates bounding boxes around them. 2) An algorithm to derive in parallel the pointclouds of said regions of interest (ROI). 3) Eight different segmentation methods to remove background data and noise from the pointclouds and obtain a precise result of the semantically segmented objects. 4) Registration of the object's pointclouds over time to generate the best possible model. 5) Utilization of an algorithm to detect an array of grasping positions and orientations based mainly on the geometry of the object's model. 6) Implementation of the system on the humanoid robot MyBot, developed in the RIT Lab at KAIST. 7) An algorithm to find the bounding box of the object's model in 3D to then create a collision object and add it to the octomap. The collision checking between robot's hand and the object is removed to allow grasping using the MoveIt libraries. 8) Selection of the best grasping pose for a certain object, plus execution of the grasping movement. 9) Retrieval of the object and moving it to a desired final position.",https://ieeexplore.ieee.org/document/8324476/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSC55427.2022.9826198,Generating Reality-Analogous Datasets for Autonomous UAV Navigation using Digital Twin Areas,IEEE,Conferences,"In order for autonomously navigating Unmanned Air Vehicles(UAVs) to be implemented in day-to-day life, proof of safe operation will be necessary for all realistic navigation scenarios. For Deep Learning powered navigation protocols, this requirement is challenging to fulfil as the performance of a network is impacted by how much the test case deviates from data that the network was trained on. Though networks can generalise to manage multiple scenarios in the same task, they require additional data representing those cases which can be costly to gather. In this work, a solution to this data acquisition problem is suggested by way of the implementation of a visually realistic, yet artificial, simulated dataset. A method is presented for the creation of a &#x201C;Digital Twin Area&#x201D; inside of a modern high fidelity game engine using 3D scanned models of physical locations, and a realistic dataset of each area is created to showcase this concept.",https://ieeexplore.ieee.org/document/9826198/,2022 33rd Irish Signals and Systems Conference (ISSC),9-10 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2004.356,Gesture Recognition Using 3D Appearance and Motion Features,IEEE,Conferences,"We present a novel 3D gesture recognition scheme that combines the 3D appearance of the hand and the motion dynamics of the gesture to classify manipulative and controlling gestures. Our method does not directly track the hand. Instead, we take an object-centered approach that efficiently computes the 3D appearance using a region-based coarse stereo matching algorithm in a volume around the hand. The motion cue is captured via differentiating the appearance feature. An unsupervised learning scheme is carried out to capture the cluster structure of these feature-volumes. Then, the image sequence of a gesture is converted to a series of symbols that indicate the cluster identities of each image pair. Two schemes (forward HMMs and neural networks) are used to model the dynamics of the gestures. We implemented a real-time system and performed numerous gesture recognition experiments to analyze the performance with different combinations of the appearance and motion features. The system achieves recognition accuracy of over 96% using both the proposed appearance and the motion cues.",https://ieeexplore.ieee.org/document/1384958/,2004 Conference on Computer Vision and Pattern Recognition Workshop,27 June-2 July 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO54168.2021.9739622,Globally Learnable Point Set Registration Between 3D CT and Multi-view 2D X-ray Images of Hip Phantom,IEEE,Conferences,"2D-3D registration is a crucial step in Image-Guided Intervention, such as spine surgery, total hip re-placement, and kinematic analysis. To find the information in common between pre-operative 3D CT images and intra-operative X-ray 2D images is vital to plan and navigate. In a nutshell, the goal is to find the movement and rotation of the 3D body&#x2019;s volume to make them reorient with the patient body in the 2D image space. Due to the loss of dimensionality and different sources of images, efficient and fast registration is challenging. To this end, we propose a novel approach to incorporate a point set Neural Network to combine the information from different views, which enjoys the robustness of the traditional method and the geometrical information extraction ability. The pre-trained Deep BlindPnP captures the global information and local connectivity, and each implementation of view-independent Deep BlindPnP in different view pairs will select top-priority pairs candidates. The transformation of different viewpoints into the same coordinate will accumulate the correspondence. Finally, a POSEST-based module will output the final 6 DoF pose. Extensive experiments on a real-world clinical dataset show the effectiveness of the proposed framework compared to the single view. The accuracy and computation speed are improved by incorporating the point set neural network.",https://ieeexplore.ieee.org/document/9739622/,2021 IEEE International Conference on Robotics and Biomimetics (ROBIO),27-31 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.00413,Gum-Net: Unsupervised Geometric Matching for Fast and Accurate 3D Subtomogram Image Alignment and Averaging,IEEE,Conferences,"We propose a Geometric unsupervised matching Net-work (Gum-Net) for finding the geometric correspondence between two images with application to 3D subtomogram alignment and averaging. Subtomogram alignment is the most important task in cryo-electron tomography (cryo-ET), a revolutionary 3D imaging technique for visualizing the molecular organization of unperturbed cellular landscapes in single cells. However, subtomogram alignment and averaging are very challenging due to severe imaging limits such as noise and missing wedge effects. We introduce an end-to-end trainable architecture with three novel modules specifically designed for preserving feature spatial information and propagating feature matching information. The training is performed in a fully unsupervised fashion to optimize a matching metric. No ground truth transformation information nor category-level or instance-level matching supervision information is needed. After systematic assessments on six real and nine simulated datasets, we demonstrate that Gum-Net reduced the alignment error by 40 to 50% and improved the averaging resolution by 10%. Gum-Net also achieved 70 to 110 times speedup in practice with GPU acceleration compared to state-of-the-art subtomogram alignment methods. Our work is the first 3D unsupervised geometric matching method for images of strong transformation variation and high noise level. The training code, trained model, and datasets are available in our open-source software AITom.",https://ieeexplore.ieee.org/document/9156695/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2006.259498,Hardware Implementation of Hierarchical Volume Subdivision-based Elastic Registration,IEEE,Conferences,"Real-time, elastic and fully automated 3D image registration is critical to the efficiency and effectiveness of many image-guided diagnostic and treatment procedures relying on multimodality image fusion or serial image comparison. True, real-time performance will make many 3D image registration-based techniques clinically viable. Hierarchical volume subdivision-based image registration techniques are inherently faster than most elastic registration techniques, e.g. free-form deformation (FFD)-based techniques, and are more amenable for achieving real-time performance through hardware acceleration. Our group has previously reported an FPGA-based architecture for accelerating FFD-based image registration. In this article we show how our existing architecture can be adapted to support hierarchical volume subdivision-based image registration. A proof-of-concept implementation of the architecture achieved speedups of 100 for elastic registration against an optimized software implementation on a 3.2 GHz Pentium III Xeon workstation. Due to inherent parallel nature of the hierarchical volume subdivision-based image registration techniques further speedup can be achieved by using several computing modules in parallel",https://ieeexplore.ieee.org/document/4462029/,2006 International Conference of the IEEE Engineering in Medicine and Biology Society,30 Aug.-3 Sept. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00070,High-quality First-person Rendering Mixed Reality Gaming System for In Home Setting,IEEE,Conferences,"With the advent of low-cost RGB-D cameras, mixed reality serious games using `live' 3D human avatars have become popular. Here, RGB-D cameras are used for capturing and transferring user' motion and texture onto the 3D human avatar in virtual environments. A system with a single camera is more suitable for such mixed reality games deployed in homes, considering the ease of setting up the system. In these mixed reality games, users can have either a third-person perspective or a first-person perspective of the virtual environments used in the games. Since first-person perspective provides a better Sense of Embodiment (SoE), in this paper, we explore the problem of providing a first-person perspective for mixed reality serious games played in homes. We propose a real time textured humanoid-avatar framework to provide a first-person perspective and address the challenges involved in setting up such a gaming system in homes. Our approach comprises: (a) SMPL humanoid model optimization for capturing user' movements continuously; (b) a real-time texture transferring and merging OpenGL pipeline to build a global texture atlas across multiple video frames. We target the proposed approach towards a serious game for amputees, called Mr.MAPP (Mixed Reality-based framework for Managing Phantom Pain), where amputee' intact limb is mirrored in real-time in the virtual environment. For this purpose, our framework also introduces a mirroring method to generate a textured phantom limb in the virtual environment. We carried out a series of visual and metrics-based studies to evaluate the effectiveness of the proposed approaches for skeletal pose fitting and texture transfer to SMPL humanoid models, as well as the mirroring and texturing missing limb (for future amputee based studies).",https://ieeexplore.ieee.org/document/9319120/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/ic:19950663,How visual salience influences natural language descriptions,IET,Conferences,"The model presented is part of a larger project called VITRA (visual translator), where we investigate aspects at the interaction of vision and language. A particular interest lies on the information flow from the analysis of visual data to language generation. We focus on how visual information can be used for grounding descriptions in the environment. In cooperation with the visual perception group of the IIFB at the Fraunhofer Institute at the University of Karlsruhe, we have shown how real world visual data in dynamic environments can be used in natural language descriptions. A model based approach is used for automatically generating 3D representations of the environment. Our current work is related to problems which occur if an agent moves through real or synthetic environments. The agent's task during its movement is to incrementally describe a route from a starting point to a destination by refering to visually obtained objects. In general, the whole complexity of AI research is involved, e.g. control laws for movement, early vision processing, high level vision, naive physics, temporal and spatial reasoning, knowledge representation, planning and language processing. In a first approach, we have implemented a software agent called MOSES who describes a path in a synthetic 3D environment. MOSES can only refer to visually obtained objects (landmarks) in the current situation. Information about the path is extracted from a map by using an incremental path finding procedure.",https://ieeexplore.ieee.org/document/478375/,"IEE Colloquium on Grounding Representations: Integration of Sensory Information in Natural Language Processing, Artificial Intelligence and Neural Networks",15-15 May 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ETFA45728.2021.9613331,Human Action Recognition as part of a Natural Machine Operation Framework,IEEE,Conferences,"The reliability of systems that use machine learning to recognize the human working in an industrial environment is of high importance for the employee safety. we present a framework which is capable of recognizing the person's natural interaction with an industrial machine. We focus on the application of human action recognition in the context of machine operation by skilled workers in industrial or commercial environments. We propose a framework that includes action recognition as part of a software component for understanding behavior. For our use case, we defined an exemplary machine operation workflow which we use to compare five different neural networks in terms of prediction accuracy and real-time capabilities. Moreover, we compare different input shapes as the resolution of input images and the size of the possible 3D-volume in order to study the robustness of the models. For our evaluation, we created our own custom dataset containing six action classes. Our analysis shows that the best model is the I3D with color images, a resolution of 112 × 112 pixels and 16 consecutive frames. The I3D also exhibited the best run-time performance for real-time applications.",https://ieeexplore.ieee.org/document/9613331/,2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA ),7-10 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APCC.2018.8633477,Human Behavior Based Learning Framework for Small Cell Placement in 5G Networks Using Hypergraph Construction,IEEE,Conferences,"In this paper, we consider the 3 dimensional (3D) small cell basestations (SBSs) placement problem for hetergeneous users with incomplete QoS requirements. We propose a learning embedded hypergraph construction algorithm to jointly predict the heterogeneous users' QoS requirements and deploy the SBSs to cover as many users as possible. Specifically, we predict and leverage users' QoS requirements with a real-world dataset, China Family Panel Studies (CFPS), and design a human behavior based learning framework (HBLF) to anticipate users' QoS requirements with users' profiles. Then, based on HBLF, we propose a hypergraph construction algorithm to deploy the SBSs. Simulation results indicate that the learning embedded hypergraph construction can use the least number of SBSs to cover all users compared with a number of algorithms, such as K-means. The HBLF can achieve a higher coverage ratio of users compared with other learning algorithms, such as decision tree. Simulation results also corroborate that the more accurately we can predict the users' QoS requirements, the more users we can satisfy their QoS requirements.",https://ieeexplore.ieee.org/document/8633477/,2018 24th Asia-Pacific Conference on Communications (APCC),12-14 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV.2007.31,Human Pose Inference from Stereo Cameras,IEEE,Conferences,"In this paper, a Bayesian mixture expert (BME) framework for the estimation of 3D human poses from two uncalibrated wide-baseline cameras is presented. The two cameras will reduce the ambiguities of the pose estimation greatly and is easy to implement. BME is learnt to conduct multimodal pose estimation regression. K-means algorithm considering Euclidean distance and maximum-value distance for the joint angle vector is used for the initial clustering in BME learning. This will give the better cluster results to separate the ambiguous poses into different experts. Also a weighted PCA is implemented in an expectation-maximization (EM) framework to learn the parameters of the BME. This can reduce the dimension of the training data more effectively compared with global PCA. The system is trained with synthesized silhouettes from motion capture data. The experimental results on synthesized and real images illustrate that our approach does not need precise camera calibration and can estimate the poses effectively",https://ieeexplore.ieee.org/document/4118766/,2007 IEEE Workshop on Applications of Computer Vision (WACV '07),21-22 Feb. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AFGR.2000.840654,Human body postures from trinocular camera images,IEEE,Conferences,"This paper proposes a new real-time method for estimating human postures in 3D from trinocular images. In this method, an upper body orientation detection and a heuristic contour analysis are performed on the human silhouettes extracted from the trinocular images so that representative points such as the top of the head can be located. The major joint positions are estimated based on a genetic algorithm-based learning procedure. 3D coordinates of the representative points and joints are then obtained from the two views by evaluating the appropriateness of the three views. The proposed method implemented on a personal computer runs in real-time. Experimental results show high estimation accuracies and the effectiveness of the view selection process.",https://ieeexplore.ieee.org/document/840654/,Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580),28-30 March 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.1999.815625,Human media interface system for the next generation plant operation,IEEE,Conferences,"An overview is given of a project on ""Development of a human interface for the next generation plant operation"" running as a subproject of the MITI-funded Human Media project. The goals of the project include developing a next generation human centered interface for plant operation. We adopted an oil refinery plant as a task domain and have developed an interface system consisting of: an Interface Agent which is responsible for plant monitoring and for direct interaction with operators; a Virtual Plant Agent responsible for 3D virtual display of the plant; a Semantic Information Presentation Agent for presentation of semantically interpreted information based on a sophisticated model of the plant; an Ontology Server for standardizing of Agents' understanding of the plant; and a Distributed Collaboration Infrastructure for enabling agents to perform collaboration. Design philosophies and a project overview are presented.",https://ieeexplore.ieee.org/document/815625/,"IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)",12-15 Oct. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIIP.2013.6707549,Hybrid 3D registration approach using RGB and depth images,IEEE,Conferences,"We propose a novel technique for registration of 3D point sets using both the RGB data as well as the depth data. The main advantage of any RGB-D sensor is the pixel wise correspondence between RGB values and depth values, which can be leveraged to register two RGB-D datasets. RGB images are used for correspondence identification and these correspondences are transferred to depth images to be used for the registration algorithm. RANSAC is used for rejection of noisy data points, which increases the registration accuracy. We also analyze and present an error threshold selection strategy for fitting 3D points. Our approach achieves faster execution, thus enabling real-time implementation of change detection and 3D mapping of the environment, etc. Multiple feature extraction methods have been tested to evaluate tradeoffs between accuracy and time.",https://ieeexplore.ieee.org/document/6707549/,2013 IEEE Second International Conference on Image Information Processing (ICIIP-2013),9-11 Dec. 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITIIT49094.2020.9071548,Hybrid Spatio-temporal Deep Learning Framework for Particulate Matter(PM2.5) Concentration Forecasting,IEEE,Conferences,"With massive urbanization, air pollution has turned out to be a life-threatening factor that requires serious control management. Proper assessment and prediction of outdoor air pollution could significantly warn people about the risk of chronic and acute respiratory diseases including asthma during outdoor exposure. IoT based air quality monitoring through sensors deployed at different locations could be an expedient solution for this. Accurate modeling of the air quality big data collected by the IoT sensors extensively helps in predicting future values for accessing the risk of outdoor exposure. This extraction of knowledge gained could greatly in reducing the deterioration of human health through early warnings and decision-making. In this paper, a hybrid deep learning-based architecture using CNN-LSTM combination is proposed to model Particulate Matter (PM2.5) of a location with the values collected from the IoT air quality sensors. The encoder-decoder based architecture models the time distributed multivariate air pollutant data of 9 locations. The 3D CNN and 1D CNN is exploited to encode both the inter-dependency (spatial autocorrelation) and intra-dependency (heterogeneity) in the spatiotemporal air pollutant data. The CNN based encoder captures all relevant spatiotemporal features for facilitating improved accuracy in predictions. The LSTM learns the temporal dependencies in the encoded air pollutant data for PM2.5 concentration forecasting. Extensive experimentations are performed on real-world IoT City Pulse Pollution dataset. The proposed model is compared with ConvLSTM in terms of root mean square error (RMSE), the mean absolute error (MAE) and the R-squared (R2) and is found to outperform ConvLSTM.",https://ieeexplore.ieee.org/document/9071548/,2020 International Conference on Innovative Trends in Information Technology (ICITIIT),13-14 Feb. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MDM48529.2020.00023,IFLoc: Indoor Height Estimation by Telco Data,IEEE,Conferences,"Understanding the fine-grained distribution of telecommunication (Telco) signals in terms of a three-dimensional (3D) space is important for Telco operators to manage, operate and optimize Telco networks. It is particularly true in nowadays urban cities with a large number of high buildings. One of the key tasks is to infer the location height of mobile devices, e.g., the floor within a high building where mobile devices are located. However, precise height estimation is challenging due to complex Telco signal propagation within an indoor 3D space, sparse cell tower deployment and scarce training samples. To tackle these issues, in this paper, we propose an indoor MR height estimation framework, namely IFLoc, via a machine learning model. IFLoc first builds a training MR database via a pre-processing step to comfortably tag raw MR samples by precisely inferred height from auxiliary data such as GPS and barometer readings. Next, IFLoc trains a regression model for height estimation by a set of developed techniques including 3D space division, post-processing techniques, feature augmentation and an improved SVR (Supported Vector Regression) model. Our evaluation on eight real datasets collected within five representative high buildings in Shanghai validates that IFLoc outperforms state-of-the-art counterparts in particularly with scarce training data.",https://ieeexplore.ieee.org/document/9162333/,2020 21st IEEE International Conference on Mobile Data Management (MDM),30 June-3 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAEAC.2018.8577729,Image segmentation based on Blob analysis and quad-tree algorithm,IEEE,Conferences,"Image segmentation is one of the most popular topic in recent research and studies, there are lots of different method to solve this problem. Some existing method works pretty well, and others not. This paper proposed to implement two improved existing method, Split and merge and Blob coloring algorithm, and compared their segmentation results in both 2D and 3D. Meanwhile, to clarify our achievements following scientific method, we have to establish our evaluation function FMI to give a score to tell what level of goodness our implementation could achieve. The first task is implementation of different region growing algorithms. We implement a general split and merge algorithm and the Blob Coloring algorithm that can work both in 2D and in 3D with different homogeneity criteria. For the split and merge algorithm, we will use quad-tree algorithm to split and merge our image based on the homogeneity. For the blob algorithm, we will check the L shape, then merge the pixels with similar homogeneity, and ignore those not. According to the experimental results, We found the split merge algorithm is generally better than the blob algorithm. Although for some case, the blob algorithm can also reach to more than 0.8 FMI score in evaluating, the overall performance is still bad. Split and merge algorithm works well on all the image cases. As we think, we decide to apply our implementation in some real images to achieve some goals. For example, we choose a group of interior design and a group of animals as 3D images. The result coming from our implementation could be used in biomedical field such as cancer detection, as well as in public management such as suspects outline detection. However, there are real problems for us to use region growing techniques to detect boundary of objects with some specific indexes because if the target has obvious difference inside, the algorithm will not treat it as a whole. However, from a visual point of view, we treat it as a complete object.",https://ieeexplore.ieee.org/document/8577729/,"2018 IEEE 3rd Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)",12-14 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAECST54258.2021.9695887,Immersive Visualization VR System of 3D Time-varying Field,IEEE,Conferences,"To meet the application need of dynamic visualization VR display of 3D time-varying field, this paper designed an immersive visualization VR system of 3D time-varying field based on the Unity 3D framework. To reduce visual confusion caused by 3D time-varying field flow line drawing and improve the quality and efficiency of visualization rendering drawing, deep learning was used to extract features from the mesoscale vortex of the 3D time-varying field. Moreover, the 3D flow line dynamic visualization drawing was implemented through the Unity Visual Effect Graph particle system.",https://ieeexplore.ieee.org/document/9695887/,2021 3rd International Academic Exchange Conference on Science and Technology Innovation (IAECST),10-12 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCCE46687.2019.9015335,Implementation of 3D Drawing to Interactive Display System,IEEE,Conferences,"Our laboratory has been developing an entertainment system called the “light crayon”. This system can draw on a screen with light pointer. In this paper, we propose a new system that can draw on 3D(three-dimensional) space with Mixed Reality technology.",https://ieeexplore.ieee.org/document/9015335/,2019 IEEE 8th Global Conference on Consumer Electronics (GCCE),15-18 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIT.2019.8834580,Implementation of Depth-HOG based Human Upper Body Detection On A Mini PC Using A Low Cost Stereo Camera,IEEE,Conferences,"In this paper, we propose a human upper body detection using a depth image that is implemented on a mini PC using a low cost stereo camera. The camera, named Minoru 3D webcam, produces depth image from its two parallel cameras. A pyramid-based region of interest (RoI) is applied on the depth-image frames to scan the possibility of human upper body existence. Simultaneously, a histogram of oriented gradient (HOG) method is performed in the ROI to extract the HOG feature. The results of the HOG feature extraction are then classified using linear support vector machine (SVM). Our system has been experimentally tested publicly in the campus environment. The detection speed obtained from a computer is 8.53 fps and the Mini PC is 4.26 fps under non-threaded programming. The result of object detection using HOG and SVM Classification method on static image reaches an average accuracy of 78.71%. Testing the system for real implementation has average error accuracy 4.60%. The results of detection for two human objects can achieve an accuracy 71.00%. Moving image testing has an average error of 4.60%.",https://ieeexplore.ieee.org/document/8834580/,2019 International Conference of Artificial Intelligence and Information Technology (ICAIIT),13-15 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSP.2018.8524377,Implementation of Robotic Vision to Perform Threaded Assembly,IEEE,Conferences,"In manufacturing of mechanical parts and assemblies, proper thread-engagement between a bolt and a nut is vital for the performance and reliability of the product. Typically, this is a precision work, requiring repetitive manual operations. In this paper, we explain how such assembly operations can be carried out by collaborative robots (co-bots) by monitoring the position and orientation of the nut and bolt using an image-sensor (camera). The focus of our discussion is the assembly-operation of bolting of a nut by the grippers of a co-bot. Slips and misalignment, leading to wrong positioning of the nut and the bolt, are identified by capturing the images of the two components in real time using Microsoft Kinect camera-sensor. 3D Reconstruction of the image captured by the camera-sensor is carried out using the Kinect Fusion application. The reconstructed image is in the form of a polygonal mesh which is further converted to 3D Point Cloud data which is less sensitive to noise. Thereafter, the Point Cloud is segmented by dividing the entire scene into many clusters in order to distinguish the objects of the scene as grippers and nut and bolt. These clusters can be used for the training of the co-bot for the proposed operation. This method of extracting object-boundaries leading to recognition of objects is a vital operation in the field of robotic vision. We provide baseline description of various machine learning techniques that can be applied to realize proper assembly of a nut and a bolt.",https://ieeexplore.ieee.org/document/8524377/,2018 International Conference on Communication and Signal Processing (ICCSP),3-5 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NSSMIC.2014.7430955,Implementation of Self-organizing Map Based Positioning Scheme on FPGA,IEEE,Conferences,"For continuous crystal based PET detector, we develop a Self-organizing Map (SOM) Neural Network Based Positioning Scheme which can achieve 2.07mm average resolution and is feasible for implementation on Field Programmable Gate Array (FPGA). In this paper, we propose the FPGA design of SOM scheme and apply it to our experiment data. Taking advantage of the pipelined and parallel structure, the implementation of this algorithm is able to process 5M events per second with system clock running at 200M Hz. The test results show that the FPGA solution has almost the equal performance with software platform. Considering the potentiality of DOI determination using SOM scheme, it is promising to realize real-time 3D position estimation in the future.",https://ieeexplore.ieee.org/document/7430955/,2014 IEEE Nuclear Science Symposium and Medical Imaging Conference (NSS/MIC),8-15 Nov. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2012.1188,Implementation of a progressive target recognition system for all the angles,IET,Conferences,"3D target recognition is a popular research field in the laser near-field detection, where the scan image of the target is obtained line by line. The arbitrary spatial position between the detector and the target, along with the interference caused by the clouds, make it difficult to realize accurate real-time identification for the detector. This paper presents a method to generate gray-scale plane images under any viewpoint by calculating the echo signals from the surface of 3D planes. Besides, considering both scan imaging and signal processing in hardware, a local feature based on improved and simplified SURF algorithm is implemented in hardware to describe the target. Then a progressive 3D target recognition system is built on the FPGA, and the experiments performed on the Modelsim simulation software show that the system can reach a high recognition rate.",https://ieeexplore.ieee.org/document/6492795/,International Conference on Automatic Control and Artificial Intelligence (ACAI 2012),3-5 March 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INAGENTSYS.2014.7005719,Implementation of multi-agent system using fuzzy logic towards shopping center simulation,IEEE,Conferences,"A computer simulation tool has a purpose to simulate real-world phenomenon using mathematical and computational calculations to run the simulations and provide a detailed report as the result of the simulation. By using a computer simulation tool, users can retrieve information relevant to the simulation area in a relatively short time. In this paper, the author presents a prototype of multi-agent simulation tool for shopping centers. The shopping centers and all its components are presented in a simulated 3D environment. The simulation tool is built by using Unity3D engine for building 3D environment and running the simulation. The shopping behavior of the agents in the simulation is performed with a fuzzy logic calculation with the basis of the agent's knowledge. This simulation tool is tested by experts for its ability to simulate and gives accurate information of the simulation. The result of the simulation shows that the implementation of the multi-agent system in shopping centers can show the general idea of people's activities in shopping centers.",https://ieeexplore.ieee.org/document/7005719/,"2014 International Conference on Intelligent Autonomous Agents, Networks and Systems",19-21 Aug. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NSSMIC.1994.474587,Improved resolution via 3D iterative reconstruction for PET volume imaging,IEEE,Conferences,"The authors have implemented iterative filtered backprojection (IFBP) and maximum likelihood by expectation maximization (ML-EM) algorithms in 3D space and applied them to phantom and real PET data. Transaxial resolution improves /spl ap/50% and axial resolution improves /spl ap/15% for IFBP at 15 iterations without a sieve compared to FBP. With a sieve, the improvements are reduced to /spl ap/6%. 3D ML-EM reconstruction shows similar resolution improvement with a much slower convergence rate compared to IFBP. The improvements in resolution from both IFBP and ML-EM are apparent in 3D FDG brain data.<>",https://ieeexplore.ieee.org/document/474587/,Proceedings of 1994 IEEE Nuclear Science Symposium - NSS'94,30 Oct.-5 Nov. 1994,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC46164.2021.9629726,Improving Accuracy and Runtime of Skeletal Tracking of Lower Limbs for Athletic Jump Mechanics Assessment,IEEE,Conferences,"Previous studies have shown that athletic jump mechanics assessments are valuable tools for identifying indicators of an individual’s anterior cruciate ligament injury risk. These assessments, such as the drop jump test, often relied on camera systems or sensors that are not always accessible nor practical for screening individuals in a sports setting. As human pose estimation deep learning models improve, we envision transitioning biometrical assessments to mobile devices. As such, here we have addressed two of the most preclusive hindrances of the current state-of-the-art models: accuracy of the lower limb joint prediction and the slow run-time of in-the-wild inference. We tackle the issue of accuracy by adding a post-processing step that is compatible with all inference methods that outputs 3D key points. Additionally, to overcome the lengthy inference rate, we propose a depth estimation method that runs in real-time and can function with any 2D human pose estimation model that outputs COCO key points. Our solution, paired with a state-of-the-art model for 3D human pose estimation, significantly increased lower-limb positional accuracy. Furthermore, when paired with our real-time joint depth estimation algorithm, it is a plausible solution for developing the first mobile device prototype for athlete jump mechanics assessments.",https://ieeexplore.ieee.org/document/9629726/,2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC),1-5 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARA51699.2021.9376453,Improving Deep Multi-modal 3D Object Detection for Autonomous Driving,IEEE,Conferences,"Object detection in real-world applications such as autonomous driving scenarios is a challenging issue since objects often occlude each other. 3D object detection has achieved high accuracy and efficiency, but detecting small object instances and occluded objects are the most challenging issues to deploy detectors in crowded scenes. Our main focus in this paper is deep multi-modal based object detector in an automated driving system with early fusion on 3D object detection utilizing both Light Detection and Ranging (LiDAR) and image data. We aim at obtaining highly accurate 3D localization and recognition of objects in the road scene and try to improve the performance. In this regard, our basic architecture follows an established two-stage architecture, Aggregate View Object Detection-Feature Pyramid Network (AVOD-FPN), one of the best among sensor fusion-based methods. AVOD-FPN has yielded promising results especially for detecting small instances. Moreover, another main challenging issue in autonomous driving is detecting the occluded objects. So we try to address this difficulty by integrating attention network into the multi-modal 3D object detector. Experiments are shown to produce state-of-the-art results on the KITTI 3D sensor fusion-based object detection benchmark.",https://ieeexplore.ieee.org/document/9376453/,"2021 7th International Conference on Automation, Robotics and Applications (ICARA)",4-6 Feb. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV48863.2021.9576020,In-Cabin vehicle synthetic data to test Deep Learning based human pose estimation models,IEEE,Conferences,"The use of vehicle in-cabin monitoring has been increasing to fulfil the specifications of European safety regulations. These regulations present several requirements for detecting driver distraction, and more complex requirements are soon to be expected in higher automation levels. Today's restraint systems provide optimal protection in standard frontal seat positions and deviations to this might cause severe airbag-induced injuries. This makes in-cabin monitoring critical to improve safety and mitigate dangerous situations in case of a crash, and especially in high levels of autonomous driving. Defining the best sensor positioning inside the vehicle's cabin is a challenge due to its constraints and limitations. The main aim of this work was to verify if simulated 3D human models integrated into a 3D modelled vehicle interior environment can be used to run Deep Learning based human pose estimation models. To perform such task, we utilized the software MakeHuman combined with Blender, to build the virtual environment and create photorealistic scenes containing selected front occupants' postures use cases and then feed into Openpose and Mask R-CNN models. The results showed that using a 2D HPE (Human Pose Estimation) network pre-trained on real data, can detect successfully photorealistic synthetic data of humans under complex scenarios. It is also shown that complex and rare postures can cause failure on 2D HPE detections, as shown in the literature review. This work helps to define the most suitable camera positions which, in combination with specific camera lenses, can deliver quality images for a robust pose detection.",https://ieeexplore.ieee.org/document/9576020/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICHMS53169.2021.9582627,Increasing the Efficacy of a Powered Ankle-Foot Prosthesis with 3D Joint Angle Tracking,IEEE,Conferences,"This research aims to utilize joint angle tracking from depth sensors to design a real-time, powered ankle-foot prosthesis assistive algorithm. These sensors are used primarily for computer vision tasks surrounding rehabilitation; however, their use in motor feedback control is limited. By extracting joint positions from a depth map, the joint angle is calculated. This, along with angular velocity collected from an external IMU, are used to train a neural network. This is deployed alongside human-in-the-loop optimization to determine how the most effective prosthetic regimen can be found most efficiently. The 3D joint angle method is evaluated with EMG data and metabolic cost from prosthetic use and is then compared to current control methods.",https://ieeexplore.ieee.org/document/9582627/,2021 IEEE 2nd International Conference on Human-Machine Systems (ICHMS),8-10 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2010.5509682,Indoor scene recognition through object detection,IEEE,Conferences,"Scene recognition is a highly valuable perceptual ability for an indoor mobile robot, however, current approaches for scene recognition present a significant drop in performance for the case of indoor scenes. We believe that this can be explained by the high appearance variability of indoor environments. This stresses the need to include high-level semantic information in the recognition process. In this work we propose a new approach for indoor scene recognition based on a generative probabilistic hierarchical model that uses common objects as an intermediate semantic representation. Under this model, we use object classifiers to associate low-level visual features to objects, and at the same time, we use contextual relations to associate objects to scenes. As a further contribution, we improve the performance of current state-of-the-art category-level object classifiers by including geometrical information obtained from a 3D range sensor that facilitates the implementation of a focus of attention mechanism within a Monte Carlo sampling scheme. We test our approach using real data, showing significant advantages with respect to previous state-of-the-art methods.",https://ieeexplore.ieee.org/document/5509682/,2010 IEEE International Conference on Robotics and Automation,3-7 May 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PACRIM.1995.519580,Industrial inspection employing a three dimensional vision system and a neural network classifier,IEEE,Conferences,"An automatic inspection system for manufactured parts employing a 3D machine vision system and associated software for part identification and dimensional inspection is described. The machine vision module collects a range image of accurate data from the part surface employing a structured light approach. In order to measure specific surface parameters, the entire part data set is decomposed into its constituent surface patches. A neural network classifier is employed to recognise each part from its range data set and also to classify a specific surface patch, on a part, from the overall set of part surface patches. The output of the neural network classifier is presented to a database of part information which is created off-line. The performance of the system has been tested by experimenting on real range data.",https://ieeexplore.ieee.org/document/519580/,"IEEE Pacific Rim Conference on Communications, Computers, and Signal Processing. Proceedings",17-19 May 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ACC45564.2020.9147268,Inferential Methods for Additive Manufacturing Feedback,IEEE,Conferences,"Adaptive manufacturing has revolutionized desktop prototyping and the production of physical models for non-load bearing or stress inducing applications. Many extrusion-based printers are available for purchase by entrepreneurial enthusiasts or businesses with manufacturing space limitations. These low-cost printers allow for quick prototyping but are not designed or intended for high quality production or high-cycle production, requiring extensive user tuning and upkeep to maintain the printer in usable condition. In a quest to apply modern deep learning and reinforcement learning based models, this work focuses on the development of control systems and infrastructure needed to resolve many of these intrinsic limitations of desktop 3D printers. A series of real-time agents were designed and deployed to actively monitor the printing of every layer and make continuous corrections in the printing parameters and G-code commands to reduce the variance in the tensile strength of homogeneous parts printed in a large batch.",https://ieeexplore.ieee.org/document/9147268/,2020 American Control Conference (ACC),1-3 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2001.934451,Initial evolvability experiments on the CAM-brain machines (CBMs),IEEE,Conferences,"Presents the results of some of the first evolvability experiments undertaken on CAM (content-addressable memory) brain machines (CBMs), using the hardware itself (not software simulations). A CBM is a specialised piece of programmable (evolvable) hardware that uses Xilinx XC6264 programmable FPGA chips to grow and evolve, at electronic speeds, 3D cellular automata (CA) based neural network circuit modules of some 1,000 neurons each. A complete run of a genetic algorithm (e.g. with 100 generations and a population size of 100) is executed in a few seconds. 64,000 of these modules can be evolved separately according to the fitness definitions of human evolutionary engineers and downloaded, one by one, into a gigabyte of RAM. Human brain architects then interconnect these modules ""by hand"" according to their artificial brain architectures. The CBM then updates the binary neural signaling of the artificial brain (with 64,000 ""hand"" interconnected modules, i.e. 75 million neurons) at a rate of 130 billion CA cell updates a second, which is fast enough for the real-time control of robots. Before such multi-module artificial brains can be constructed, it is essential that the quality of the evolution (the ""evolvability"") of the individual modules should be adequate. This paper reports on the initial evolvability results obtained on CBM hardware.",https://ieeexplore.ieee.org/document/934451/,Proceedings of the 2001 Congress on Evolutionary Computation (IEEE Cat. No.01TH8546),27-30 May 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS51556.2021.9401402,Instantaneous Stereo Depth Estimation of Real-World Stimuli with a Neuromorphic Stereo-Vision Setup,IEEE,Conferences,"The stereo-matching problem, i.e., matching corresponding features in two different views to reconstruct depth, is efficiently solved in biology. Yet, it remains the computational bottleneck for classical machine vision approaches. By exploiting the properties of event cameras, recently proposed Spiking Neural Network (SNN) architectures for stereo vision have the potential of simplifying the stereo-matching problem. Several solutions that combine event cameras with spike-based neuromorphic processors already exist. However, they are either simulated on digital hardware or tested on simplified stimuli. In this work, we use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a brain-inspired event-based stereo-matching architecture implemented on a mixed-signal neuromorphic processor with real-world data. Our experiments show that this SNN architecture, composed of coincidence detectors and disparity sensitive neurons, is able to provide a coarse estimate of the input disparity instantaneously, thereby detecting the presence of a stimulus moving in depth in real-time.",https://ieeexplore.ieee.org/document/9401402/,2021 IEEE International Symposium on Circuits and Systems (ISCAS),22-28 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2001.990508,Integrated face and gait recognition from multiple views,IEEE,Conferences,"We develop a view-normalization approach to multi-view face and gait recognition. An image-based visual hull (IBVH) is computed from a set of monocular views and used to render virtual views for tracking and recognition. We determine canonical viewpoints by examining the 3D structure, appearance (texture), and motion of the moving person. For optimal face recognition, we place virtual cameras to capture frontal face appearance; for gait recognition we place virtual cameras to capture a side-view of the person. Multiple cameras can be rendered simultaneously, and camera position is dynamically updated as the person moves through the workspace. Image sequences from each canonical view are passed to an unmodified face or gait recognition algorithm. We show that our approach provides greater recognition accuracy than is obtained using the unnormalized input sequences, and that integrated face and gait recognition provides improved performance over either modality alone. Canonical view estimation, rendering, and recognition have been efficiently implemented and can run at near real-time speeds.",https://ieeexplore.ieee.org/document/990508/,Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001,8-14 Dec. 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00054,"Integrating Biomechanical and Animation Motion Capture Methods in the Production of Participant Specific, Scaled Avatars",IEEE,Conferences,"3D motion capture of human movement in animation and biomechanics has developed in relatively separate and parallel domains. The two disciplines use different language, software, computational models and have different aims. As a result, in the life sciences, human movement is predominantly analyzed as non-visual biomechanical data. Whereas human movement visualization in animation typically lacks the accuracy outside of that required in the entertainment industry. This project draws from both disciplines to develop a novel approach in the creation of participant specific, motion capture skeletons which are retargeted onto participant specific, anatomically scaled, humanoid avatars. The customized motion capture marker placement, skeleton and character scaling used in this new approach aims to retain a high level of movement fidelity and minimize discrepancies between participant and avatar movement. This process has been used in the visualization of aesthetic movement such as dance and provides a step towards the generation of a digital double which can facilitate full body immersion into digital environments.",https://ieeexplore.ieee.org/document/8613673/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR.2009.5336478,Integration of georegistered information on a virtual globe,IEEE,Conferences,"In collaborative augmented reality (AR) missions, much georegistered information is collected and sent to a command and control center. This paper describes the concept and prototypical implementation of a mixed reality (MR) based system that integrates georegistered information from AR systems and other sources on a virtual globe. The application can be used for a command and control center to monitor the field operation where multiple AR users are engaging in a collaborative mission. Google Earth is used to demonstrate the system, which integrates georegistered icons, live video streams from field operators or surveillance cameras, 3D models, and satellite or aerial photos into one MR environment.",https://ieeexplore.ieee.org/document/5336478/,2009 8th IEEE International Symposium on Mixed and Augmented Reality,19-22 Oct. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SDS.2018.8370412,Intel Optane™ technology as differentiator for internet of everything and fog computing,IEEE,Conferences,"Traditional network and cloud solutions cannot address effectively the infrastructure and data architecture challenges introduced by the emergence of the Internet of Everything (IoE). IoE creates unprecedented volume of complex data which must be stored, transferred, processed and analyzed in real time for time sensitive applications. Indeed, most of the applications in IoE domain (autonomous driving, health monitoring, banking, industrial control systems ...) are time and quality of service sensitive and need new solutions for data integrity and data availability for successful decision. At Intel, we are working on new computer paradigm and storage technologies such as Intel® Optane™ SSDs and Intel® 3D NAND SSDs that can address some of these challenges. With Intel® Optane™, it is now possible to create a high performance and real time available Software-Defined Infrastructure (SDI) by enabling full dis-aggregate and pool of the underlying hardware resources, creating distributed memory/storage imperatives and giving research community and enterprise the performance and capabilities to benefit from revolutionary technologies such as Fog Computing and Artificial Intelligence by dynamically assigning compute, storage and network resource in real-time time sensitive workloads. In this keynote, we will discuss data storage, availability, integrity, quality of service and movement in this compute paradigm shift. We will also touch on how it impacts mobile edge, fog computing and cloud infrastructures. We will then conclude with the challenges and opportunities this new solution will bring.",https://ieeexplore.ieee.org/document/8370412/,2018 Fifth International Conference on Software Defined Systems (SDS),23-26 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WIIAT.2008.320,Intelligent Objects to Facilitate Human Participation in Virtual Institutions,IEEE,Conferences,"Our research combines electronic institutions and 3D virtual worlds for the construction of virtual institutions which are virtual worlds with normative regulation of interactions. That is, a virtual world where participants actions have to comply with predefined institutional rules. In this context, the actions a participant may perform depend on the institutional rules and the current execution state. We propose to include iObjects, intelligent objects, as entities having both visualization properties and decision mechanisms in the virtual institution. They are a new key element to improve users participation in virtual institutions. We situate them in a middleware infrastructure in order to be independent of 3D virtual world platform and to provide a general solution in which participants could be connected from different immersive environment platforms.",https://ieeexplore.ieee.org/document/4740448/,2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,9-12 Dec. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CW.2004.40,Intelligent agents in virtual worlds,IEEE,Conferences,"The realistic animation of the behavior of virtual agents emulating human beings and evolving within virtual 3D worlds (a field also known as artificial life) has received great attention during the last few years. The current challenge is to create a behavioral system for the virtual agents so that they behave as realistic as possible. Such a system should provide the agents with the most typical features of human intelligence: perception, recognition, memory, learning, reasoning, etc. Among the large number of different techniques to solve this problem, those based on artificial intelligence (AI) seem to be especially suitable for this purpose. In this paper, we apply several artificial intelligence techniques (such as neural networks, expert systems, fuzzy logic, K-means) to create a sophisticated behavioral system that allows the agents to take intelligent decisions by themselves. The paper describes the behavioral system, its main components and how these AI techniques have been effectively incorporated into the system. In order to show the good performance of our approach, an illustrative example is analyzed.",https://ieeexplore.ieee.org/document/1366155/,2004 International Conference on Cyberworlds,18-20 Nov. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2010.5642509,Intelligentized work-preparation for ship hull construction with Optimized Assembly Planning System,IEEE,Conferences,"Efficiency of work-preparation influences greatly to the delivery time of shipbuilding. Many current commercial softwares are able to generate 3D topological ship construction model. However, little study has been made on utilizing the added value of those models. This paper focuses on introducing Productivity Extension Module (PEM) as a secondary development of software based on AutoCAD platform. PEM is able to effectively generate 3D topological ship construction model with Optimized Assembly Planning System. Benchmark based on a real engineering project is provided by this paper in order to prove the efficiency of PEM. It is concluded that PEM is in high efficiency when dealing with construction modeling and making workshop assembly plan for shipbuilding.",https://ieeexplore.ieee.org/document/5642509/,"2010 IEEE International Conference on Systems, Man and Cybernetics",10-13 Oct. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSN.2017.8230363,Interaction design of gesture control for DMS multi-channel system,IEEE,Conferences,"With the development of Artificial Intelligence, gesture recognition technology has been applied in practical schemes. We built up the Dynamic Matrix Sound system (DMS system) which makes an excellent sound field reproduction effect. However, because of the weak controllability of the large-scale system, the application of the DMS system is very limited. In order to control a DMS system by gesture, the Leap Motion is adopted and the Node.js is used to connect the DMS system with client terminal. The real time 3D gesture position data, which is captured by leap motion, will be translated into DMS control instruction and transferred to the DMS system to implement the play, pause and other functions. In order to support a wider range of client hardware, the design is realized by javascript and C++, of which the data communication between them is realized through Node.js. In the design of the system, the whole process is to transfer the communication logic of websocket to the Node.js, so that the communication between the mobile devices and the back-end is more reasonable.",https://ieeexplore.ieee.org/document/8230363/,2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN),6-8 May 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCIS.2009.348,Interactive Ray Tracing for Volume Visualization and 3D Rendering Using Neural Networks,IEEE,Conferences,This article describes a novel approach to the real-time visualization and 3D rendering using neural networks. The neural network model is proposed in this paper for optimal control with control constraints. We consider 3D metamorphosis applied to volume-based representations of objects. We discuss the issues which arise in volume morphing and present a method for creating morphs. The technique of volume ray casting can be derived directly from the rending equation. It provides results of very high quality.,https://ieeexplore.ieee.org/document/5208975/,2009 WRI Global Congress on Intelligent Systems,19-21 May 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341795,Interactive Tactile Perception for Classification of Novel Object Instances,IEEE,Conferences,"In this paper, we present a novel approach for classification of unseen object instances from interactive tactile feedback. Furthermore, we demonstrate the utility of a low resolution tactile sensor array for tactile perception that can potentially close the gap between vision and physical contact for manipulation. We contrast our sensor to high-resolution camera-based tactile sensors. Our proposed approach interactively learns a one-class classification model using 3D tactile descriptors, and thus demonstrates an advantage over the existing approaches, which require pre-training on objects. We describe how we derive 3D features from the tactile sensor inputs, and exploit them for learning one-class classifiers. In addition, since our proposed method uses unsupervised learning, we do not require ground truth labels. This makes our proposed method flexible and more practical for deployment on robotic systems. We validate our proposed method on a set of household objects and results indicate good classification performance in real-world experiments.",https://ieeexplore.ieee.org/document/9341795/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2008.5035636,Interactively evolved modular neural networks for game agent control,IEEE,Conferences,"As the realism in games continues to increase, through improvements in graphics and 3D engines, more focus is placed on the behavior of the simulated agents that inhabit the simulated worlds. The agents in modern video games must become more life-like in order to seem to belong in the environments they are portrayed in. Many modern artificial intelligence approaches achieve a high level of realism but this is accomplished through significant developer time spent scripting the behaviors of the non-playable characters or NPC's. These agents will behave in a believable fashion in the scenarios they have been programmed for, but do not have the ability to adapt to new situations. In this paper we introduce a modularized, real-time evolution training technique to evolve adaptable agents with life-like behaviors. Online performance during evolution is also improved by using selection mechanisms found in temporal difference learning methods to appropriately balance the exploration and exploitation of control policies. These methods are implemented and tested using the XNA framework producing very promising results regarding efficiency of techniques, and demonstrating many potential avenues for further research.",https://ieeexplore.ieee.org/document/5035636/,2008 IEEE Symposium On Computational Intelligence and Games,15-18 Dec. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WPNC.2016.7822857,Introducing a novel marker-based geometry model in monocular vision,IEEE,Conferences,"A spherical marker-based distance capture concept using monocular vision is presented in this paper. A novel method is explored, within the concept of a virtual sphere, which shows how to improve the reading measurements of the distance of a moving object from low resolution digital images, and from a single viewpoint. The aim here is to be able to track accurately the object at a furthest possible position. A conclusion with experimental simulations carried out using 3D modeling of markers and representing the real world showing the potency of the marker's geometry on improving the accuracy of the measurements. A potential application field of the proposed method is the implementation of tracking object in mobile robots, marker-based localization, and field of topography.",https://ieeexplore.ieee.org/document/7822857/,"2016 13th Workshop on Positioning, Navigation and Communications (WPNC)",19-20 Oct. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8594049,Joint 3D Proposal Generation and Object Detection from View Aggregation,IEEE,Conferences,"We present AVOD, an Aggregate View Object Detection network for autonomous driving scenarios. The proposed neural network architecture uses LIDAR point clouds and RGB images to generate features that are shared by two subnetworks: a region proposal network (RPN) and a second stage detector network. The proposed RPN uses a novel architecture capable of performing multimodal feature fusion on high resolution feature maps to generate reliable 3D object proposals for multiple object classes in road scenes. Using these proposals, the second stage detection network performs accurate oriented 3D bounding box regression and category classification to predict the extents, orientation, and classification of objects in 3D space. Our proposed architecture is shown to produce state of the art results on the KITTI 3D object detection benchmark [1] while running in real time with a low memory footprint, making it a suitable candidate for deployment on autonomous vehicles. Code is available at: https://github.com/kujason/avod.",https://ieeexplore.ieee.org/document/8594049/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FG52635.2021.9667011,KSL-Guide: A Large-scale Korean Sign Language Dataset Including Interrogative Sentences for Guiding the Deaf and Hard-of-Hearing,IEEE,Conferences,"Many advancements in computer vision and machine learning have shown potential for significantly improving the lives of people with disabilities. In particular, recent research has demonstrated that deep neural network models could be used to bridge the gap between the deaf who use sign language and hearing people. The major impediment to advancing such models is the lack of high-quality and large-scale training data. Moreover, previously released sign language datasets include few or no interrogative sentences compared to declarative sentences. In this paper, we introduce a new publicly available large-scale Korean Sign Language (KSL) dataset-KSL-Guide-that includes both declarative sentences and comparable interrogative sentences, which are required for a model to achieve high performance in real-world interactive tasks deployed on service applications. Our dataset contains a total of 121K sign language video samples featuring sentences and words spoken by native KSL speakers with extensive annotations (e.g., gloss, translation, keypoints, and timestamps). We exploit a multi-camera system to produce 3D human pose keypoints as well as 2D keypoints from multi-view RGB. Our experiments quantitatively demonstrate that the inclusion of interrogative sentences in training for sign language recognition and translation tasks greatly improves their performance. Furthermore, we empirically show the qualitative results by developing a prototype application using our dataset, providing an interactive guide service that helps to lower the communication barrier between sign language speakers and hearing people.",https://ieeexplore.ieee.org/document/9667011/,2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021),15-18 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICALT.2003.1215101,Kana-input navigation system for kids based on the cyber assistant,IEEE,Conferences,"In Japan, it has increased the opportunity for young children to experience the personal computer in elementary schools. However, in order to use computer, many domestic barriers have confronted young children (kids) because they cannot read Kanji characters and had not learnt Roman alphabet yet. As a result, they cannot input text strings by JIS keyboard. We developed Kana-input navigation system for kids (KINVS) based on the cyber assistant system (CAS). CAS is a human-style software robot based on the 3D-CG real-time animation and voice synthesis technology. KINVS enables to input Hiragana/Katakana characters by mouse operation only (without keyboard) and CAS supports them by using speaking, facial expression, body action and sound effects. KINVS displays the 3D-stage like a classroom. In this room, blackboard, interactive parts to input Kana-characters, and CAS are placed. Mouse input method of KINVS are designed to use only single click and wheeler rotation. To input characters, kids clicks or rotates the interactive parts. KINVS reports all information by voice speaking and Kana subtitles instead of Kanji text. Furthermore, to verify the functional feature of KINVS, we measured how long kids had taken to input long text by using KINVS.",https://ieeexplore.ieee.org/document/1215101/,Proceedings 3rd IEEE International Conference on Advanced Technologies,9-11 July 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITR54349.2021.9657326,Kidland: An Augmented Reality-based approach for Smart Ordering for Toy Store,IEEE,Conferences,"Augmented reality (AR) is an iconic topic that can be applied in different domains in modern world technology. With the rapid development of technologies, eCommerce (Online Shopping) has become closer to human life. As a result, AR was started implemented with eCommerce platforms by the developers. With the busy lives and the pandemic situation, people are limited to visiting toy stores while providing a solution. An AR-based virtual toy store is proposed with 3D Toy generation for visualizing selected toys, a Virtual tour for enhancing the remote virtual shopping experience, and an Indoor navigation system visualizing the path within large scale shopping malls are new features of the proposed system. The majority of the existing eCommerce platforms are missing image search features. As a solution, &#x201C;KidLand&#x201D; has implemented an image search engine, suggesting add-on-related items and nearest branches using machine learning algorithms. An intelligent chatbot uses a reinforcement learning algorithm and Natural Language Understanding (NLU) to give possible solutions regarding the toy store. As a solution to the language literacy problem, developed a chatbot that can chat both English and Sinhala languages. &#x201C;Kidland&#x201D; was developed to provide the users the next level of shopping experience with attractive features of AR technology with marketing and use advanced technologies overcoming the issues of ordinary eCommerce platforms. In Sri Lanka, this system has been identified as a solution for the issues with ordinary shopping platforms.",https://ieeexplore.ieee.org/document/9657326/,2021 6th International Conference on Information Technology Research (ICITR),1-3 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196885,Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping,IEEE,Conferences,"We provide an open-source C++ library for real-time metric-semantic visual-inertial Simultaneous Localization And Mapping (SLAM). The library goes beyond existing visual and visual-inertial SLAM libraries (e.g., ORB-SLAM, VINS-Mono, OKVIS, ROVIO) by enabling mesh reconstruction and semantic labeling in 3D. Kimera is designed with modularity in mind and has four key components: a visual-inertial odometry (VIO) module for fast and accurate state estimation, a robust pose graph optimizer for global trajectory estimation, a lightweight 3D mesher module for fast mesh reconstruction, and a dense 3D metric-semantic reconstruction module. The modules can be run in isolation or in combination, hence Kimera can easily fall back to a state-of-the-art VIO or a full SLAM system. Kimera runs in real-time on a CPU and produces a 3D metric-semantic mesh from semantically labeled images, which can be obtained by modern deep learning methods. We hope that the flexibility, computational efficiency, robustness, and accuracy afforded by Kimera will build a solid basis for future metric-semantic SLAM and perception research, and will allow researchers across multiple areas (e.g., VIO, SLAM, 3D reconstruction, segmentation) to benchmark and prototype their own efforts without having to start from scratch.",https://ieeexplore.ieee.org/document/9196885/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ALPIT.2007.64,Kinematical Access for Realistic Behavior Generation of Artificial Fish,IEEE,Conferences,"The objects real time rendered in the 3D cyberspace can interact with each others according to the events which are happened when satisfying some conditions. But, to representing the behaviors with these interactions, too many event conditions are considered because each behavior pattern and event must be corresponded in a one-to-one ratio. It leads to problems which increase the system complexity. So, in this paper, we try to physical method based on elasticity force for representing more realistic behaviors of AI fish and present a new method can create the various behavior patterns responding to one evasion event.",https://ieeexplore.ieee.org/document/4460656/,Sixth International Conference on Advanced Language Processing and Web Information Technology (ALPIT 2007),22-24 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerCom.2014.6813937,"Kintense: A robust, accurate, real-time and evolving system for detecting aggressive actions from streaming 3D skeleton data",IEEE,Conferences,"Kintense is a robust, accurate, real-time, and evolving system for detecting aggressive actions such as hitting, kicking, pushing, and throwing from streaming 3D skeleton joint coordinates obtained from Kinect sensors. Kintense uses a combination of: (1) an array of supervised learners to recognize a predefined set of aggressive actions, (2) an unsupervised learner to discover new aggressive actions or refine existing actions, and (3) human feedback to reduce false alarms and to label potential aggressive actions. This paper describes the design and implementation of Kintense and provides empirical evidence that the system is 11% - 16% more accurate and 10% - 54% more robust to changes in distance, body orientation, speed, and person when compared to standard techniques such as dynamic time warping (DTW) and posture based gesture recognizers. We deploy Kintense in two multi-person households and demonstrate how it evolves to discover and learn unseen actions, achieves up to 90% accuracy, runs in real-time, and reduces false alarms with up to 13 times fewer user interactions than a typical system.",https://ieeexplore.ieee.org/document/6813937/,2014 IEEE International Conference on Pervasive Computing and Communications (PerCom),24-28 March 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00655,L3-Net: Towards Learning Based LiDAR Localization for Autonomous Driving,IEEE,Conferences,"We present L3-Net - a novel learning-based LiDAR localization system that achieves centimeter-level localization accuracy, comparable to prior state-of-the-art systems with hand-crafted pipelines. Rather than relying on these hand-crafted modules, we innovatively implement the use of various deep neural network structures to establish a learning-based approach. L3-Net learns local descriptors specifically optimized for matching in different real-world driving scenarios. 3D convolutions over a cost volume built in the solution space significantly boosts the localization accuracy. RNNs are demonstrated to be effective in modeling the vehicle's dynamics, yielding better temporal smoothness and accuracy. We comprehensively validate the effectiveness of our approach using freshly collected datasets. Multiple trials of repetitive data collection over the same road and areas make our dataset ideal for testing localization systems. The SunnyvaleBigLoop sequences, with a year's time interval between the collected mapping and testing data, made it quite challenging, but the low localization error of our method in these datasets demonstrates its maturity for real industrial implementation.",https://ieeexplore.ieee.org/document/8954371/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2007.4522258,Layered omnidirectional walking controller for the humanoid soccer robot,IEEE,Conferences,"This paper proposes the layered omnidirectional walking controller for the humanoid soccer robot. The gait of the robot can be parameterized using the destination posititon and the desired direction while reaching the destination. Its implementation in our RoboCup simulation team — SEU-3D is detailed in this paper. Our approach generates smooth robot trajectories without stop before changing direction or turning, and is fast enough to meet the real-time requirements. The proposed approach has been tested in the RoboCup soccer 3D server platform. The results showed that omnidirectional walking has advantages in dynamic environments.",https://ieeexplore.ieee.org/document/4522258/,2007 IEEE International Conference on Robotics and Biomimetics (ROBIO),15-18 Dec. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UR52253.2021.9494666,Learning Multi-modal Attentional Consensus in Action Recognition for Elderly-Care Robots,IEEE,Conferences,"This paper addresses a practical action recognition method for elderly-care robots. Multi-stream based models are one of the promising approaches for solving the complexity of real-world environments. While multi-modal action recognition have been actively studied, there is a lack of research on models that effectively combine features of different modalities. This paper proposes a new mid-level feature fusion method for two-stream based action recognition network. In multi-modal approaches, extracting complementary information between different modalities is an essential task. Our network model is designed to fuse features at an intermediate level of feature extraction, which leverages a whole feature map from each modality. Consensus feature map and consensus attention mechanism are proposed as effective ways to extract information from two different modalities: RGB data and motion features. We also introduce ETRI-Activity3D-LivingLab, a real-world RGB-D dataset for robots to recognize daily activities of the elderly. It is the first 3D action recognition dataset obtained in a variety of home environments where the elderly actually reside. We expect our new dataset to contribute to the practical study of action recognition with the previously released ETRI-Activity3D dataset. To prove the effectiveness of the method, extensive experiments are performed on NTU RGB+D, ETRI-Activity3D and, ETRI-Activity3D-LivingLab dataset. Our mid-level fusion method achieves competitive performance in various experimental settings, especially for domain-changing situations.",https://ieeexplore.ieee.org/document/9494666/,2021 18th International Conference on Ubiquitous Robots (UR),12-14 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICELTICS.2018.8548939,Learning Orthodontic Cephalometry through Augmented Reality: A Conceptual Machine Learning Validation Approach,IEEE,Conferences,"The orthodontic cephalometry which forms a quintessential radiographic examination provides a valuable diagnostic resource for everyday orthodontic diagnosis and treatment planning. The method of analyzing the lateral cephalometric radiograph (LCR) has been found to lack effective methods of landmark identification and objective validation. This drawback, with the use of augmented reality (AR) for 3D visualization and machine learning for objective validation, can be effectively incorporated into the learning of cephalometrics. The two technologies together can produce a learning platform by reducing subjective errors and its implications on treatment planning in addition to effectively eliminating dissonance in learning.",https://ieeexplore.ieee.org/document/8548939/,2018 International Conference on Electrical Engineering and Informatics (ICELTICs),19-20 Sept. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2018.00321,Learning Patch Reconstructability for Accelerating Multi-view Stereo,IEEE,Conferences,"We present an approach to accelerate multi-view stereo (MVS) by prioritizing computation on image patches that are likely to produce accurate 3D surface reconstructions. Our key insight is that the accuracy of the surface reconstruction from a given image patch can be predicted significantly faster than performing the actual stereo matching. The intuition is that non-specular, fronto-parallel, in-focus patches are more likely to produce accurate surface reconstructions than highly specular, slanted, blurry patches - and that these properties can be reliably predicted from the image itself. By prioritizing stereo matching on a subset of patches that are highly reconstructable and also cover the 3D surface, we are able to accelerate MVS with minimal reduction in accuracy and completeness. To predict the reconstructability score of an image patch from a single view, we train an image-to-reconstructability neural network: the I2RNet. This reconstructability score enables us to efficiently identify image patches that are likely to provide the most accurate surface estimates before performing stereo matching. We demonstrate that the I2RNet, when trained on the ScanNet dataset, generalizes to the DTU and Tanks & Temples MVS datasets. By using our I2RNet with an existing MVS implementation, we show that our method can achieve more than a 30× speed-up over the baseline with only an minimal loss in completeness.",https://ieeexplore.ieee.org/document/8578419/,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,18-23 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIE.2007.4374932,Learning Wall Following Behaviour in Robotics through Reinforcement and Image-based States,IEEE,Conferences,"In this work, a visual and reactive wall following behaviour is learned by reinforcement. With artificial vision the environment is perceived in 3D, and it is possible to avoid obstacles that are invisible to other sensors that are more common in mobile robotics. Reinforcement learning reduces the need for intervention in behaviour design, and simplifies its adjustment to the environment, the robot and the task. In order to facilitate its generalization to other behaviours and to reduce the role of the designer, we propose a regular image-based codification of states. Even though this is much more difficult, our implementation converges and is robust. Results are presented with a Pioneer 2 AT. Learning phase has been realized on the Gazebo 3D simulator and the test phase has been proved in simulated and real environments to demonstrate the correct design and robustness of our algorithms.",https://ieeexplore.ieee.org/document/4374932/,2007 IEEE International Symposium on Industrial Electronics,4-7 June 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2017.8324818,Learning complex assembly skills from kinect based human robot interaction,IEEE,Conferences,"Acquiring complex assembly skills is still a challenging task for robot programming. Because of the sensory and body structure differences, the human knowledge has to be demonstrated, recorded, converted and finally learned by the robot, in an inexplicit and indirect way. During this process, “how to demonstrate”, “how to convert” and “how to learn” are the key problems. In this paper, Kinect sensor is utilized to provide the behavior information of the human demonstrator. Through natural human robot interaction, body skeleton and joint 3D coordinates are provided in real-time, which can fully describe the human intension and task related skills. To overcome the structural and individual differences, a Cartesian level unified mapping method is proposed to convert the human motion and match the specified robot. The recorded data set are modeled using Gaussian mixture model(GMM) and Gaussian mixture regression(GMR), which can extract redundancies across multiple demonstrations and build robust models to regenerate the dynamics of the recorded movements. The proposed methodologies are implemented in the imNEU humanoid robot platform. Experimental results verify the effectiveness.",https://ieeexplore.ieee.org/document/8324818/,2017 IEEE International Conference on Robotics and Biomimetics (ROBIO),5-8 Dec. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2017.7969562,Learning of a tracker model from multi-radar data for performance prediction of air surveillance system,IEEE,Conferences,"A valid model of the air surveillance system performance is highly valued when making decisions related to the optimal control of the system. We formulate a model for a multi-radar tracker system by combining a radar performance model with a tracker performance model. A tracker as a complex software system is hard to model mathematically and physically. Our novel approach is to utilize machine learning to create a tracker model based on measurement data from which the input and target output for the model are calculated. The measured data comprises the time series of 3D coordinates of cooperative aircraft flights, the corresponding target detection recordings from multiple radars, and the related multi-radar track recordings. The collected data is used to calculate performance measures for the radars and the tracker at specific locations in the air space. We apply genetic programming to learning such rules from radar performance measures that explain tracker performance. The easily interpretable rules are intended to reveal the real behavior of the system providing comprehension for its control and further development. The learned rules allow predicting tracker performance level for the system control in all radar geometries, modes, and conditions at any location. In the experiments, we show the feasibility of our approach to learning a tracker model and compare our rule learner with two tree classifiers, another rule learner, a neural network, and an instance-based classifier using the real air surveillance data. The tracker model created by our rule learner outperforms the models by the other methods except for the neural network whose prediction performance is equal.",https://ieeexplore.ieee.org/document/7969562/,2017 IEEE Congress on Evolutionary Computation (CEC),5-8 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561636,Learning to Propagate Interaction Effects for Modeling Deformable Linear Objects Dynamics,IEEE,Conferences,"Modeling dynamics of deformable linear objects (DLOs), such as cables, hoses, sutures, and catheters, is an important and challenging problem for many robotic manipulation applications. In this paper, we propose the first method to model and learn full 3D dynamics of DLOs from data. Our approach is capable of capturing the complex twisting and bending dynamics of DLOs and allows local effects to propagate globally. To this end, we adapt the interaction network (IN) dynamics learning method for capturing the interaction between neighboring segments in a DLO and augment it with a recurrent model for propagating interaction effects along the length of a DLO. For learning twisting and bending dynamics in 3D, we also introduce a new suitable representation of DLO segments and their relationships. Unlike the original IN method, our model learns to propagate the effects of local interaction between neighboring segments to each segment in the chain within a single time step, without the need for iterated propagation steps. Evaluation of our model with synthetic and newly collected real-world data shows better accuracy and generalization in short-term and long- term predictions than the current state of the art. We further integrate our learned model in a model predictive control scheme and use it to successfully control the shape of a DLO. Our implementation is available at https://gitsvn-nt.oru.se/ammlab–public/in–bilstm.",https://ieeexplore.ieee.org/document/9561636/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC45102.2020.9294743,Learning-Based Shape Estimation with Grid Map Patches for Realtime 3D Object Detection for Automated Driving,IEEE,Conferences,"3D object detection serves as a crucial basis of visual perception, motion prediction, and planning for automated driving. To apply an algorithm for this purpose, the detection of all types of road users in real-time is an essential condition. In this paper, we propose an approach that projects the 3D points of image-based bounding box proposals into so-called grid map patches. These patches are used to estimate the exact dimensions of the 3D box with the help of a lightweight CNN. The complete proposed processing chain is parallelized and implemented on a GPU. This makes our approach the fastest stereo-based 3D object detector on the KITTI benchmark while still achieving results that are within the range of the best image-based algorithms.",https://ieeexplore.ieee.org/document/9294743/,2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC),20-23 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV.2017.15,Leveraging Tree Statistics for Extracting Anatomical Trees from 3D Medical Images,IEEE,Conferences,"Using different priors (e.g. shape and appearance) have proven critical for robust image segmentation of different types of target objects. Many existing methods for extracting trees (e.g. vascular or airway trees) from medical images have leveraged appearance priors (e.g. tubular-ness and bifurcationness) and the knowledge of the cross-sectional geometry (e.g. circles or ellipses) of the tree-forming tubes. In this work, we present the first method for 3D tree extraction from 3D medical images (e.g. CT or MRI) that, in addition to appearance and cross-sectional geometry priors, utilizes prior tree statistics collected from the training data. Our tree extraction method collects and leverages topological tree prior and geometrical statistics, including tree hierarchy, branch angle and length statistics. Our implementation takes the form of a Bayesian tree centerline tracking method combining the aforementioned tree priors with observed image data. We evaluated our method on both synthetic 3D datasets and real clinical CT chest datasets. For synthetic data, our method's key feature of incorporating tree priors resulted in at least 13% increase in correctly detected branches under different noise levels. For real clinical scans, the mean distance from ground truth centerlines to the detected centerlines by our method was improved by 12% when utilizing tree priors. Both experiments validate that, by incorporating tree statistics, our tree extraction method becomes more robust to noise and provides more accurate branch localization.",https://ieeexplore.ieee.org/document/8287685/,2017 14th Conference on Computer and Robot Vision (CRV),16-19 May 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/AEITAUTOMOTIVE50086.2020.9307387,LiDAR point-cloud processing based on projection methods: a comparison,IEEE,Conferences,"An accurate and rapid-response perception system is fundamental for autonomous vehicles to operate safely. 3D object detection methods handle point clouds given by LiDAR sensors to provide accurate depth and position information for each detection, together with its dimensions and classification. The information is then used to track vehicles and other obstacles in the surroundings of the autonomous vehicle, and also to feed control units that guarantee collision avoidance and motion planning. Nowadays, object detection systems can be divided into two main categories. The first ones are the geometric based, which retrieve the obstacles using geometric and morphological operations on the 3D points. The seconds are the deep learning-based, which process the 3D points, or an elaboration of the 3D point-cloud, with deep learning techniques to retrieve a set of obstacles. This paper presents a comparison between those two approaches, presenting one implementation of each class on a real autonomous vehicle. Accuracy of the estimates of the algorithms has been evaluated with experimental tests carried in the Monza ENI circuit. The positions of the ego vehicle and the obstacle are given by GPS sensors with real time kinematic (RTK) correction, which guarantees an accurate ground truth for the comparison. Both algorithms have been implemented on ROS and run on a consumer laptop.",https://ieeexplore.ieee.org/document/9307387/,2020 AEIT International Conference of Electrical and Electronic Technologies for Automotive (AEIT AUTOMOTIVE),18-20 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME46284.2020.9102769,Lightningnet: Fast and Accurate Semantic Segmentation for Autonomous Driving Based on 3D LIDAR Point Cloud,IEEE,Conferences,"Semantic segmentation is A series of earlier works have demonstrated that 3D LiDAR point cloud segmentation is a promising approach. However, these methods usually rely on pretrained models. And their requiring improvements in either speed or accuracy prevent them to be applied to mobile platforms. To address these problems, a smaller Convolutional Neural Network (CNN) architecture, namely, LightningNet is presented. Furthermore, we propose a lightweight pipeline with Feature Refinement Modules to boost the performance of real-time 3D LiDAR point cloud segmentation. Our model is trained on spherical images projected from LiDAR point clouds on KITTI [1] dataset. Experiments show accuracy improvements of 4.1-8.2% in small objects and 4% in terms of mIoU over the state-of-the-art of spherical-image based method with 111 fps on a single 1080Ti GPU and 14fps on a Jetson TX2, which enables deployment for real-time semantic segmentation in autonomous driving. We achieve superior performance on another large dataset called SemanticKITTI (illustrated in Figure 1) both in speed and accuracy also.",https://ieeexplore.ieee.org/document/9102769/,2020 IEEE International Conference on Multimedia and Expo (ICME),6-10 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.00608,Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled Representation,IEEE,Conferences,"We present a lightweight solution to recover 3D pose from multi-view images captured with spatially calibrated cameras. Building upon recent advances in interpretable representation learning, we exploit 3D geometry to fuse input images into a unified latent representation of pose, which is disentangled from camera view-points. This allows us to reason effectively about 3D pose across different views without using compute-intensive volumetric grids. Our architecture then conditions the learned representation on camera projection operators to produce accurate per-view 2d detections, that can be simply lifted to 3D via a differentiable Direct Linear Transform (DLT) layer. In order to do it efficiently, we propose a novel implementation of DLT that is orders of magnitude faster on GPU architectures than standard SVD-based triangulation methods. We evaluate our approach on two large-scale human pose datasets (H36M and Total Capture): our method outperforms or performs comparably to the state-of-the-art volumetric methods, while, unlike them, yielding real-time performance.",https://ieeexplore.ieee.org/document/9157707/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NSSMIC.2010.5874269,List-mode MLEM image reconstruction from 3D ML position estimates,IEEE,Conferences,"Current thick detectors used in medical imaging allow recording many attributes, such as the 3D location of interaction within the scintillation crystal and the amount of energy deposited. An efficient way of dealing with these data is by storing them in list-mode (LM). To reconstruct the data, maximum-likelihood expectation-maximization (MLEM) is efficiently applied to the list-mode data, resulting in the list-mode maximum-likelihood expectation-maximization (LMMLEM) reconstruction algorithm. In this work, we consider a PET system consisting of two thick detectors facing each other. PMT outputs are collected for each coincidence event and are used to perform 3D maximum-likelihood (ML) position estimation of location of interaction. The mathematical properties of the ML estimation allow accurate modeling of the detector blur and provide a theoretical framework for the subsequent estimation step, namely the LMM-LEM reconstruction. Indeed, a rigorous statistical model for the detector output can be obtained from calibration data and used in the calculation of the conditional probability density functions for the interaction location estimates. Our implementation of the 3D ML position estimation takes advantage of graphics processing unit (GPU) hardware and permits accurate real-time estimates of position of interaction. The LMMLEM algorithm is then applied to the list of position estimates, and the 3D radiotracer distribution is reconstructed on a voxel grid.",https://ieeexplore.ieee.org/document/5874269/,IEEE Nuclear Science Symposuim & Medical Imaging Conference,30 Oct.-6 Nov. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIOCAS.2019.8919138,Live Demonstration: An AIoT Wearable ECG Patch with Decision Tree for Arrhythmia Analysis,IEEE,Conferences,"This live demonstration presents a novel electrocardiogram (ECG) monitoring system with artificial intelligence of things (AIoT) design, which is based on decision tree (DT). The proposed system includes a front-end device and a software system. The front-end device includes a solar charging circuit, a wireless charging circuit and an analog front-end circuit. First, the solar charging block takes a dye-sensitized sorlar cell from National Chung Hsing University, which is responsible for energy harvesting under indoor illuminance. Second, the wireless charging block gives users an additional charging method to meet the demand of long-term monitoring. Third, the analog front-end block is composed of the ECG-sensing circuit, the microcontroller unit (MCU) and the Bluetooth Low Energy (BLE) module. The ECG-sensing circuit is based on single lead measurement, and it includes level shifter units, differential amplifiers and filters. The circuits are implemented by the commercial components and realized by self-designed print circuit boards (PCB). On the other hand, this paper takes ARM Cortex M4 and BLE 5.0 as the solution for data transmitting and encoding. All the above circuits are integrated into one PCB, and the prototype is designed by 3D-printer. The whole ECG Patch's size is 86.6 mm* 50 mm* 20 mm. The software system includes an application (APP) with DT algorithms, a cloud server is available to execute DT training and to provide a user interface for supporting telemedicine. This paper proposes a simplified DT model, which can be realized in APP based on iOS system. The APP classifies real-time ECG data into different arrhythmias, and the delay latency is 500 ms in average. Meanwhile, according to 4G or Wi-Fi, the collected ECG data are uploaded to the cloud server for training DT. Then, the coefficients of the pre-trained DT will be sent back to the APP for updating. The accuracy is 98.7%. By the proposed AIoT system, doctors and users can realize the task of long-term ECG monitoring, which is valuable for cardiovascular disease diagnosis. Also, doctors can assist users instantly by the web user interface, to meet the demands of telemedicine. The proposed AIoT system has been conducted human trials in National Cheng Kung University Hospital. The power consumption of the proposed front-end device is 8.25 mW, and it can be continuously used up to 32 hours with a 120 mAh lithium-ion battery. If it turns on solar charging, the device can continually operate, until the solar cell is dead.",https://ieeexplore.ieee.org/document/8919138/,2019 IEEE Biomedical Circuits and Systems Conference (BioCAS),17-19 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMSP.2017.8122226,Lossless point cloud geometry compression via binary tree partition and intra prediction,IEEE,Conferences,"Characterized by geometry and photometry attributes, point cloud has become widely applied in the real-time presentation of various 3D objects and scenes. The development of even more precise capture devices and the increasing requirements for vivid rendering inevitably induce huge point capacity, thus making the point cloud compression a demanding issue. Considering the non-uniform sampling and time-variant geometry, appropriate structural representation for point cloud is important. In this paper, we propose a lossless geometry compression algorithm for 3D point cloud which serves as the basis of future adaptive improvement. We utilize the binary tree structure for effectively partitioning unorganized points into block structure. This hierarchical representation obtains roughly the same quantity level for each leaf node. Further analysis is conducted on an intra-geometry prediction via extended Travelling Salesman Problem (TSP), achieving an impressive performance in eliminating point-wise redundancy while preserving one single reference position for each block. The residual encoding is accomplished via a shallow neural network-based lossless compression algorithm, PAQ. Simulation results confirm the lossless compression of geometry from high quality capture, achieving approximately 3.5 times efficiency gain over the state of art algorithm implemented as MPEG Point Cloud Compression (PCC) reference software.",https://ieeexplore.ieee.org/document/8122226/,2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP),16-18 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561878,MFPN-6D : Real-time One-stage Pose Estimation of Objects on RGB Images,IEEE,Conferences,"6D pose estimation of objects is an important part of robot grasping. The latest research trend on 6D pose estimation is to train a deep neural network to directly predict the 2D projection position of the 3D key points from the image, establish the corresponding relationship, and finally use Pespective-n-Point (PnP) algorithm performs pose estimation. The current challenge of pose estimation is that when the object texture-less, occluded and scene clutter, the detection accuracy will be reduced, and most of the existing algorithm models are large and cannot take the real-time requirements. In this paper, we introduce a Multi-directional Feature Pyramid Network, MFPN, which can efficiently integrate and utilize features. We combined the Cross Stage Partial Network (CSPNet) with MFPN to design a new network for 6D pose estimation, MFPN-6D. At the same time, we propose a new confidence calculation method for object pose estimation, which can fully consider spatial information and plane information. At last, we tested our method on the LINEMOD and Occluded-LINEMOD datasets. The experimental results demonstrate that our algorithm is robust to textureless materials and occlusion, while running more efficiently compared to other methods.",https://ieeexplore.ieee.org/document/9561878/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV.2009.5403083,ML-fusion based multi-model human detection and tracking for robust human-robot interfaces,IEEE,Conferences,"A novel stereo vision system for real-time human detection and tracking on a mobile service robot is presented in this paper. The system integrates the individually enhanced stereo-based human detection, HOG-based human detection, color-based tracking, and motion estimation for the robust detection and tracking of humans with large appearance and scale variations in real-world environments. A new framework of maximum likelihood based multi-model fusion is proposed to fuse these four human detection and tracking models according to the detection-track associations in 3D space, which is robust to the possible missed detections, false detections, and duplicated responses from the individual models. Multi-person tracking is implemented in a sequential near-to-far way, which well alleviates the difficulties caused by human-over-human occlusions. Extensive experimental results demonstrate the robustness of the proposed system under real-world scenarios with large variations in lighting conditions, cluttered backgrounds, human clothes and postures, and complex occlusion situations. Significant improvements in human detection and tracking have been achieved. The system has been deployed on six robot butlers to serve drinks, and showed encouraging performance in open ceremony events.",https://ieeexplore.ieee.org/document/5403083/,2009 Workshop on Applications of Computer Vision (WACV),7-8 Dec. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAT.2003.1241099,MMASS approach to localization problems,IEEE,Conferences,"The paper presents the application of the multilayered multi agent situated system (MMASS) model to localization problems. In the context of multi agent system based modelling and distributed problem solving, the MMASS model allows explicit representation of the environment structure where agents are situated, and provides spatial dependant agent behavior and interaction mechanisms. These aspects are of particular relevance in problems like localization, in which the representation of spatial features is unavoidable. In particular, examples of localization problems that are considered in the paper are: shopping center localization in extra-urban areas, guide placement in museums, and sign and poster positioning in cities. Moreover, the paper briefly presents the software system that has been developed for the three-dimensional (3D) simulation of virtual worlds inhabited by virtual agents. This tool has been exploited for 3D representation of the presented MMASS-based models of localization problems.",https://ieeexplore.ieee.org/document/1241099/,"IEEE/WIC International Conference on Intelligent Agent Technology, 2003. IAT 2003.",13-17 Oct. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV.2019.00153,MURAUER: Mapping Unlabeled Real Data for Label AUstERity,IEEE,Conferences,"Data labeling for learning 3D hand pose estimation models is a huge effort. Readily available, accurately labeled synthetic data has the potential to reduce the effort. However, to successfully exploit synthetic data, current state-of-the-art methods still require a large amount of labeled real data. In this work, we remove this requirement by learning to map from the features of real data to the features of synthetic data mainly using a large amount of synthetic and unlabeled real data. We exploit unlabeled data using two auxiliary objectives, which enforce that (i) the mapped representation is pose specific and (ii) at the same time, the distributions of real and synthetic data are aligned. While pose specifity is enforced by a self-supervisory signal requiring that the representation is predictive for the appearance from different views, distributions are aligned by an adversarial term. In this way, we can significantly improve the results of the baseline system, which does not use unlabeled data and outperform many recent approaches already with about 1% of the labeled real data. This presents a step towards faster deployment of learning based hand pose estimation, making it accessible for a larger range of applications.",https://ieeexplore.ieee.org/document/8658723/,2019 IEEE Winter Conference on Applications of Computer Vision (WACV),7-11 Jan. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCE49384.2020.9179391,Machine Learning Based Approach to Process Characterization for Smart Devices in 3D Industrial Manufacturing,IEEE,Conferences,"A key differentiator between the additive manufacturing and the traditional injection molding is the precision-manufacturing. An error-free print job significantly guarantees good part quality with minimum wastage of the material and energy. In practice, however, achieving error-free production is quite challenging and this emphasizes the need to learn what behavior of the machine leads to an erroneous job. Knowing the health of the printer or the print job helps quantify print job performance, as well as build system alerts to take reactive actions. Intending to run the model dynamically while the machine is printing, time-series based deep learning models like LSTM are most suitable. This paper presents a machine learning based anomaly detection approach to discover patterns in sensors' measurements in the streaming mode in MultiJet Fusion 3d printer developed at HP Inc. A hybrid architecture of LSTM and Auto-encoder has been proposed to learn the printer behavior generate an alarm in the event of an anomaly. The results of both LSTM and LSTM-Autoencoder models have also been discussed by taking real-life examples of 3D printing jobs.",https://ieeexplore.ieee.org/document/9179391/,"2020 International Conference on Electrical, Communication, and Computer Engineering (ICECCE)",12-13 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DELCON54057.2022.9753403,Machine Learning Based VoxelNet and LUNET architectures for Object Detection using LiDAR Cloud Points,IEEE,Conferences,"Autonomous vehicles are the epitome of technological transformation in this decade. Object detection is a primary task in an autonomous vehicle, is an important issue to evaluate to bring autonomous vehicles into reality. This paper examines 3D object detection using VoxelNet architecture and LUNet architecture from LiDAR-based cloud points. The aforementioned architecture addresses the problem of manual feature extraction, high computational costs, and memory constraints. The datasets that were utilized for training, as well as testing, have been taken out of the KITTI vehicle detection benchmarking software. Post-training and testing of the model, the results obtained have been used to evaluate the performance of this implementation. The LUNET fares better in terms of low loss whereas VoxelNet fares better with regard to low computational costs and ease of implementation.",https://ieeexplore.ieee.org/document/9753403/,2022 IEEE Delhi Section Conference (DELCON),11-13 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAP.2019.00-31,Maestro: A Memory-on-Logic Architecture for Coordinated Parallel Use of Many Systolic Arrays,IEEE,Conferences,"We present the Maestro memory-on-logic 3D-IC architecture for coordinated parallel use of a plurality of systolic arrays (SAs) in performing deep neural network (DNN) inference. Maestro reduces under-utilization common for a single large SA by allowing parallel use of many smaller SAs on DNN weight matrices of varying shapes and sizes. In order to buffer immediate results in memory blocks (MBs) and provide coordinated high-bandwidth communication between SAs and MBs in transferring weights and results Maestro employs three innovations. (1) An SA on the logic die can access its corresponding MB on the memory die in short distance using 3D-IC interconnects, (2) through an efficient switch based on H-trees, an SA can access any MB with low latency, and (3) the switch can combine partial results from SAs in an elementwise fashion before writing back to a destination MB. We describe the Maestro architecture, including a circuit and layout design, detail scheduling of the switch, analyze system performance for real-time inference applications using input with batch size equal to one, and showcase applications for deep learning inference, with ShiftNet for computer vision and recent Transformer models for natural language processing. For the same total number of systolic cells, Maestro, with multiple smaller SAs, leads to 16x and 12x latency improvements over a single large SA on ShiftNet and Transformer, respectively. Compared to a floating-point GPU implementation of ShiftNet and Transform, a baseline Maestro system with 4,096 SAs (each with 8x8 systolic cells) provides significant latency improvements of 30x and 47x, respectively.",https://ieeexplore.ieee.org/document/8825148/,"2019 IEEE 30th International Conference on Application-specific Systems, Architectures and Processors (ASAP)",15-17 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412722,Manual-Label Free 3D Detection via An Open-Source Simulator,IEEE,Conferences,"LiDAR based 3D object detectors typically need a large amount of detailed-labeled point cloud data for training, but these detailed labels are commonly expensive to acquire. In this paper, we propose a manual-label free 3D detection algorithm that leverages the CARLA simulator to generate a large amount of self-labeled training samples and introduces a novel Domain Adaptive VoxelNet (DA-VoxelNet) that can cross the distribution gap from the synthetic data to the real scenario. The self-labeled training samples are generated by a set of high quality 3D models embedded in a CARLA simulator and a proposed LiDAR-guided sampling algorithm. Then a DA-VoxelNet that integrates both a sample-level DA module and an anchor-level DA module is proposed to enable the detector trained by the synthetic data to adapt to real scenario. Experimental results show that the proposed unsupervised DA 3D detector on KITTI evaluation set can achieve 76.66% and 56.64% mAP on BEV mode and 3D mode respectively. The results reveal a promising perspective of training a LIDAR-based 3D detector without any hand-tagged label.",https://ieeexplore.ieee.org/document/9412722/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SiPS.2018.8598454,Mapping Systolic Arrays onto 3D Circuit Structures: Accelerating Convolutional Neural Network Inference,IEEE,Conferences,"In recent years, numerous designs have used systolic arrays to accelerate convolutional neural network (CNN) inference. In this work, we demonstrate that we can further speed up CNN inference and lower its power consumption by mapping systolic arrays onto 3D circuit structures as opposed to conventional 2D structures. Specifically, by operating in 3D space, a wide systolic array consisting of a number of subarrays can efficiently implement wide convolutional layers prevalent in state of the art CNNs. Additionally, by accumulating intermediate results along the third dimension, systolic arrays can process partitioned data channels in parallel with reduced data skew for lowered inference latency. We present a building block design using through-silicon vias (TSVs) for the 3D realization of systolic subarrays. We validate the 3D scheme using a 2.5D FPGA design and demonstrate that when mapped onto 3D structures wide systolic arrays can scale up in size without increasing wiring length in interconnecting subarrays. Further, by taking full advantage of 3D structures, we are able to pipeline inference across multiple layers of a CNN over a series of systolic arrays, dramatically reducing the inference time per input sample. These improvements lead to significantly reduced inference latency, which is especially important for real-time applications where it is common to process samples one at a time.",https://ieeexplore.ieee.org/document/8598454/,2018 IEEE International Workshop on Signal Processing Systems (SiPS),21-24 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW53098.2021.00141,MarkerPose: Robust Real-time Planar Target Tracking for Accurate Stereo Pose Estimation,IEEE,Conferences,"Despite the attention marker-less pose estimation has attracted in recent years, marker-based approaches still provide unbeatable accuracy under controlled environmental conditions. Thus, they are used in many fields such as robotics or biomedical applications but are primarily implemented through classical approaches, which require lots of heuristics and parameter tuning for reliable performance under different environments. In this work, we propose MarkerPose, a robust, real-time pose estimation system based on a planar target of three circles and a stereo vision system. MarkerPose is meant for high-accuracy pose estimation applications. Our method consists of two deep neural networks for marker point detection. A SuperPoint-like network for pixel-level accuracy keypoint localization and classification, and we introduce EllipSegNet, a lightweight ellipse segmentation network for sub-pixel-level accuracy keypoint detection. The marker’s pose is estimated through stereo triangulation. The target point detection is robust to low lighting and motion blur conditions. We compared MarkerPose with a detection method based on classical computer vision techniques using a robotic arm for validation. The results show our method provides better accuracy than the classical technique. Finally, we demonstrate the suitability of MarkerPose in a 3D freehand ultrasound system, which is an application where highly accurate pose estimation is required. Code is available in Python and C++ at https://github.com/jhacsonmeza/MarkerPose.",https://ieeexplore.ieee.org/document/9523117/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),19-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPEC.2014.7040963,Memory access optimized routing scheme for deep networks on a mobile coprocessor,IEEE,Conferences,"In this paper, we present a memory access optimized routing scheme for a hardware accelerated real-time implementation of deep convolutional neural networks (DCNNs) on a mobile platform. DCNNs consist of multiple layers of 3D convolutions, each comprising between tens and hundreds of filters and they generate the most expensive operations in DCNNs. Systems that run DCNNs need to pass 3D input maps to the hardware accelerators for convolutions and they face the limitation of streaming data in and out of the hardware accelerator. The bandwidth limited systems require data reuse to utilize computational resources efficiently. We propose a new routing scheme for 3D convolutions by taking advantage of the characteristic of DCNNs to fully utilize all the resources in the hardware accelerator. This routing scheme is implemented on the Xilinx Zynq-7000 All Programmable SoC. The system fully explores weight level and node level parallelization of DCNNs and achieves a peak performance 2x better than the previous routing scheme while running DCNNs.",https://ieeexplore.ieee.org/document/7040963/,2014 IEEE High Performance Extreme Computing Conference (HPEC),9-11 Sept. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2019.00988,Mesh R-CNN,IEEE,Conferences,"Rapid advances in 2D perception have led to systems that accurately detect objects in real-world images. However, these systems make predictions in 2D, ignoring the 3D structure of the world. Concurrently, advances in 3D shape prediction have mostly focused on synthetic benchmarks and isolated objects. We unify advances in these two areas. We propose a system that detects objects in real-world images and produces a triangle mesh giving the full 3D shape of each detected object. Our system, called Mesh R-CNN, augments Mask R-CNN with a mesh prediction branch that outputs meshes with varying topological structure by first predicting coarse voxel representations which are converted to meshes and refined with a graph convolution network operating over the mesh's vertices and edges. We validate our mesh prediction branch on ShapeNet, where we outperform prior work on single-image shape prediction. We then deploy our full Mesh R-CNN system on Pix3D, where we jointly detect objects and predict their 3D shapes.",https://ieeexplore.ieee.org/document/9008508/,2019 IEEE/CVF International Conference on Computer Vision (ICCV),27 Oct.-2 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.861514,Mesh construction with fast soft vector quantisation,IEEE,Conferences,"In this paper a method to accelerate soft vector quantisation (VQ), making it a quasi-real time procedure, is described. Through the local analysis of the data density a criterion to set a reasonable value of the parameters and to initialise the position of the reference vectors (hyper-box preprocessing), allows to cut about 75% of the iterations and to make the computational cost of each iteration constant, independent of the number of sampled points. Moreover, it makes soft VQ of possible implementation on parallel machines. Overall the processing time with hyper-box pre-processing can be brought down to 3%. This method, in conjunction with Delaunay tessellation, has been extensively applied to the construction of 3D triangular meshes from dense noisy data. Results on the reconstruction of 3D models of human faces are reported and discussed.",https://ieeexplore.ieee.org/document/861514/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00875,MetaSets: Meta-Learning on Point Sets for Generalizable Representations,IEEE,Conferences,"Deep learning techniques for point clouds have achieved strong performance on a range of 3D vision tasks. However, it is costly to annotate large-scale point sets, making it critical to learn generalizable representations that can transfer well across different point sets. In this paper, we study a new problem of 3D Domain Generalization (3DDG) with the goal to generalize the model to other unseen domains of point clouds without any access to them in the training process. It is a challenging problem due to the substantial geometry shift from simulated to real data, such that most existing 3D models underperform due to overfitting the complete geometries in the source domain. We propose to tackle this problem via MetaSets, which meta-learns point cloud representations from a group of classification tasks on carefully-designed transformed point sets containing specific geometry priors. The learned representations are more generalizable to various unseen domains of different geometries. We design two benchmarks for Sim-to-Real transfer of 3D point clouds. Experimental results show that MetaSets outperforms existing 3D deep learning methods by large margins.",https://ieeexplore.ieee.org/document/9577308/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEAST.2019.8802545,Method of Sea Wave Extraction and Matching from Images Based on Convolutional Neural Network,IEEE,Conferences,"Tsunami is known as one of the most destructive natural disasters. Aiming to measure Tsunami, our laboratory proposed a method based on binocular stereo vision. As we calculate the 3D coordinates of sea waves, the sea surface height in surveillance area can be measured in real time. Sea wave extraction and sea wave matching are two key steps in the above method. By conventional image processing methods, the effects of sea wave extraction and sea wave matching are not satisfactory because of uneven illuminance and different filming angles. In this paper, we propose a method of sea wave extraction and sea wave matching based on Convolutional Neural Network (CNN). Firstly, we take photos of the sea surface with two cameras. Given the raw photos, we divide them into small blocks of waves. According to the wave quantity in each block, we can build databases for sea wave extraction. And databases for sea wave matching can be built according to the matching situation of two blocks from left and right photos. Then we make use of the open-source software library Tensorflow to establish two CNN frameworks. After being trained by training databases, experiments on the test database show that the two CNN systems can separately extract and match sea waves from photos with considerable accuracy within several seconds. Besides, our network is self-adaptive to different sea wave images as well as to different camera settings.",https://ieeexplore.ieee.org/document/8802545/,"2019 5th International Conference on Engineering, Applied Sciences and Technology (ICEAST)",2-5 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSITSS.2018.8768747,Methodologies in Face Recognition for Surveillance,IEEE,Conferences,"Face recognition being the most significant part of the biometrics and video surveillance systems has to deal with many challenges like pose, illumination, distance, expression variations along with occlusion, low resolution and noise. This paper analyses the latest approaches and methodologies adapted by Face Recognition systems in overcoming the challenges with the focus on their performance on publicly available bench-mark face databases. Few of the approaches discussed in this paper are Local Binary Pattern technique, Reference Face Graph framework, Principal Component Analysis algorithm, Linear Discriminant Analysis algorithm, MLP Neural Networks, 3D Modeling, Back Propagation Neural Network, Local Pavithra S G Bhat Gradient Hexa Pattern descriptor, FPGA based architecture, Conditional Generative Adversarial Networks and Trunk-Branch Ensemble Convolutional Neural Networks. This paper provides an insight for selection of an approach for future researchers and practitioners for real world implementation.",https://ieeexplore.ieee.org/document/8768747/,2018 3rd International Conference on Computational Systems and Information Technology for Sustainable Solutions (CSITSS),20-22 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI54926.2021.00305,Metricam: Fast and Reliable Social Distancing Analysis in Online Security Cameras,IEEE,Conferences,"Distance measurements taken from 2D camera images are subject to the correct estimation of the camera&#x2019;s perspective, that is, the spatial mapping from 2D points imaged by a camera to the correspondent 3D ones in the real world. Current solutions to solve this 3D reconstruction are either dependent on the estimation of vanishing points through the detection of straight lines on targeted images or by employing sophisticated sensors and deep learning algorithms, which require expensive training on huge annotated datasets. Nevertheless, none of those approaches provide the required level of precision and accuracy for social distancing evaluation. In this paper we present Metricam, a real-time lightweight software system for security cameras that computes a 2D to 3D mapping using computational geometry and uses the DBSCAN clustering algorithm to evaluate social distancing evaluation. With Metricam, we have been able to identify several places prone to agglomeration inside the Butant&#x00E3; campus of the University of S&#x00E3;o Paulo, and provide the local authorities with valuable information to fight off the pandemic.",https://ieeexplore.ieee.org/document/9799026/,2021 International Conference on Computational Science and Computational Intelligence (CSCI),15-17 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9283113,Micro-expression Video Clip Synthesis Method based on Spatial-temporal Statistical Model and Motion Intensity Evaluation Function,IEEE,Conferences,"Micro-expression (ME) recognition is an effective method to detect lies and other subtle human emotions. Machine learning-based and deep learning-based models have achieved remarkable results recently. However, these models are vulnerable to overfitting issue due to the scarcity of ME video clips. These videos are much harder to collect and annotate than normal expression video clips, thus limiting the recognition performance improvement. To address this issue, we propose a micro-expression video clip synthesis method based on spatial-temporal statistical and motion intensity evaluation in this paper. In our proposed scheme, we establish a micro-expression spatial and temporal statistical model (MSTSM) by analyzing the dynamic characteristics of micro-expressions and deploy this model to provide the rules for micro-expressions video synthesis. In addition, we design a motion intensity evaluation function (MIEF) to ensure that the intensity of facial expression in the synthesized video clips is consistent with those in real -ME. Finally, facial video clips with MEs of new subjects can be generated by deploying the MIEF together with the widely-used 3D facial morphable model and the rules provided by the MSTSM. The experimental results have demonstrated that the accuracy of micro-expression recognition can be effectively improved by adding the synthesized video clips generated by our proposed method.",https://ieeexplore.ieee.org/document/9283113/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC54389.2021.9674237,Mitigating Multipath Interference for Time of Flight Range Sensors via Deep Convolutional Networks,IEEE,Conferences,"Time of Flight (ToF) range sensors, are a sensing technology used for 3D imaging. They provide different features such as depth image and amplitude image with a high frame rate. These data are prone to many errors, of which multipath interference (MPI) is a significant one. The state-of-the-art approaches used for multipath mitigation either need some hardware modifications or are based on costly computations, such as advantages offered by compressive sensing (CS). However, CS faces two main challenges: firstly, no data in the real world is precisely sparse on a fixed basis; secondly, in recovery algorithms with a high converging time limit CS needs to be a non-real-time application. We put forward an improved software-oriented implementation to rectify these errors in depth. This method requires neither a camera transformation nor any hardware modifications. Our method mitigates depth errors caused by MPI in ~12-15 ms, i.e., in real-time. Most of the MPI information is imbibed in the scene response received at the sensor, and input-output data has a similar structure. Based on this fact, the problem of MPI is well fitted to convolutional neural networks (CNN) to mitigate MPI. We have designed the network in such a way that it learns both, a signal’s representation and an inverse map. It approximates a convex or greedy algorithm by learning weights and biases. Justification of the proposed method on actual scenarios is presented using a hold-out validation dataset.",https://ieeexplore.ieee.org/document/9674237/,2021 7th International Conference on Computer and Communications (ICCC),10-13 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW.2016.9,Mobile Device Based Outdoor Navigation with On-Line Learning Neural Network: A Comparison with Convolutional Neural Network,IEEE,Conferences,"Outdoor navigation is challenging with its dynamic environments and huge appearance variances. Traditional autonomous navigation systems construct 3D driving scenes to recognize open and occupied voxels by using laser range scanners, which are not available on mobile devices. Existing image-based navigation methods, on the other hand, are costly in computation and thus cannot be deployed onto a mobile device. To overcome these difficulties, we present an on-line learning neural network for real-time outdoor navigation using only the computational resources available on a standard android mobile device (i.e. camera, GPS, and no cloud back-end). The network is trained to recognize the most relevant object in current navigation setting and make corresponding decisions (i.e. adjust direction, avoid obstacles, and follow GPS). The network is compared with state of the art image classifier, the Convolutional Neural Network, in various aspects (i.e. network size, number of updates, convergence speed and final performance). Comparisons show that our network requires a minimal number of updates and converges significantly faster to better performance. The network successfully navigated in regular long-duration testing in novel settings and blindfolded testing under sunny and cloudy weather conditions.",https://ieeexplore.ieee.org/document/7789499/,2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),26 June-1 July 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS.2014.35,Mobile Robot Performance in Robotics Challenges: Analyzing a Simulated Indoor Scenario and Its Translation to Real-World,IEEE,Conferences,"This paper discusses the pros and cons of using 3D simulators for testing the autonomous behavior of mobile robots in indoor environments. Major contribution of the paper is the discussion about which problems that can be faced using the simulator and those that cannot. We present the integration and calibration of a real non-commercial robot in a simulator, the characterization of the errors in sensing, navigation, and manipulation, and how these errors would impact in the real performance of the robot. The experimental support of the claims made in the paper has been developed using the gazebo simulator. RoCKIn competition rulebook defined the indoor restrictions.",https://ieeexplore.ieee.org/document/7102451/,"2014 2nd International Conference on Artificial Intelligence, Modelling and Simulation",18-20 Nov. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITECH.2017.8079942,Mobile augmented reality application for early Arabic language education-: ARabic,IEEE,Conferences,"Technology nowadays has grown rapidly and has led to the widespread use of technology in every sector that exists throughout the world whether in agriculture, business, research or education. Previous studies have shown lack of approach in Mobile Augmented Reality (MAR) based application specifically in Arabic language education. The main objective of this work is to develop a MAR application (ARabic) for teaching and learning early Arabic language and study its effectiveness. This application has been developed using ADDIE methodology which includes the analysis, design, development, implementation and testing phases. The application has two main modules; Learning and Exercise Module which combine the elements of animation, 3D objects, video, audio and graphics to keep the students interest. Based on the user testing, it shows that users are very interested in ARabic and learned the modules quickly.",https://ieeexplore.ieee.org/document/8079942/,2017 8th International Conference on Information Technology (ICIT),17-18 May 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.1998.724376,Model development of an underwater manipulator for coordinated arm-vehicle control,IEEE,Conferences,"This paper presents research on the hydrodynamic modeling of a manipulator for an autonomous underwater scientific vehicle. The focus is on improving the modeling accuracy of the in-line hydrodynamic coupling between a two-link manipulator and a small, free-floating vehicle in order to achieve better control for coordinated motion of the combined system. Loads predicted using existing models for underwater arms were determined to be off by as much as 25% when applied to a real, two-link arm in a test tank. In this new approach, an experimentally-determined model has been developed that takes into account the 3D flow effects that have previously not been included. The end result is a model that provides accurate predictions for the joint torques of a two-link arm in a form simple enough to be implemented in algorithms for precision planning and control. This project is part of a joint program between the Aerospace Robotics Laboratory at Stanford University and the Monterey Bay Aquarium Research Institute.",https://ieeexplore.ieee.org/document/724376/,IEEE Oceanic Engineering Society. OCEANS'98. Conference Proceedings (Cat. No.98CH36259),28 Sept.-1 Oct. 1998,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2016.7797065,Modeling 2D Appearance Evolution for 3D Object Categorization,IEEE,Conferences,"3D object categorization is a non-trivial task in computer vision encompassing many real-world applications. We pose the problem of categorizing 3D polygon meshes as learning appearance evolution from multi-view 2D images. Given a corpus of 3D polygon meshes, we first render the corresponding RGB and depth images from multiple viewpoints on a uniform sphere. Using rank pooling, we propose two methods to learn the appearance evolution of the 2D views. Firstly, we train view-invariant models based on a deep convolutional neural network (CNN) using the rendered RGB-D images and learn to rank the first fully connected layer activations and, therefore, capture the evolution of these extracted features. The parameters learned during this process are used as the 3D shape representations. In the second method, we learn the aggregation of the views from the outset by employing the ranking machine to the rendered RGB- D images directly, which produces aggregated 2D images which we term as ``3D shape images"". We then learn CNN models on this novel shape representation for both RGB and depth which encode salient geometrical structure of the polygon. Experiments on the ModelNet40 and ModelNet10 datasets show that the proposed method consistently outperforms existing state-of-the-art algorithms in 3D shape recognition.",https://ieeexplore.ieee.org/document/7797065/,2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA),30 Nov.-2 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB52898.2021.9657633,Modular Telepresence Robot for Distance Medical Education,IEEE,Conferences,"In special conditions such as pandemics, wars or any other problem can lead to the physical suspension of the education system including courses in medical schools. Telepresence robotics is a solution through which courses and laboratories can be held remotely where medical students can interact physically, through robots, with teaching or laboratory equipment, thus actively participating in the educational process. Although there are various variants of telepresence robots, this paper presents the study of the possibility to design and make a telepresence robot kit, low cost, containing modules easy to assemble and adapted for various teaching situations, respectively for the development of remote laboratory experiments. The proposed kit has mechanical components that can be printed on a 3D printer. The electronic components are compatible with the Arduino environment and include three modules: sensory, navigation and a communications and artificial intelligence module. The software elements are also modularly designed and allow adaptation to various study and laboratory situations. Robotic elements compatible with the Arduino environment such as arms or lifting or throwing systems can be added to the robot. The results of the experimental measurements showed that the robot has acceptable travel errors for unpretentious applications, but to increase its accuracy in laboratory conditions, the software was programmed so that, with the help of artificial intelligence, the robot can perform real-time measurements and correct automatically. This mechanism is useful in medical applications such as assisting and guiding blind patients.",https://ieeexplore.ieee.org/document/9657633/,2021 International Conference on e-Health and Bioengineering (EHB),18-19 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.00539,Monocular Real-Time Hand Shape and Motion Capture Using Multi-Modal Data,IEEE,Conferences,"We present a novel method for monocular hand shape and pose estimation at unprecedented runtime performance of 100fps and at state-of-the-art accuracy. This is enabled by a new learning based architecture designed such that it can make use of all the sources of available hand training data: image data with either 2D or 3D annotations, as well as stand-alone 3D animations without corresponding image data. It features a 3D hand joint detection module and an inverse kinematics module which regresses not only 3D joint positions but also maps them to joint rotations in a single feed-forward pass. This output makes the method more directly usable for applications in computer vision and graphics compared to only regressing 3D joint positions. We demonstrate that our architectural design leads to a significant quantitative and qualitative improvement over the state of the art on several challenging benchmarks. We will make our code publicly available for future research.",https://ieeexplore.ieee.org/document/9157654/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00478,Monocular Real-time Full Body Capture with Inter-part Correlations,IEEE,Conferences,"We present the first method for real-time full body capture that estimates shape and motion of body and hands together with a dynamic 3D face model from a single color image. Our approach uses a new neural network architecture that exploits correlations between body and hands at high computational efficiency. Unlike previous works, our approach is jointly trained on multiple datasets focusing on hand, body or face separately, without requiring data where all the parts are annotated at the same time, which is much more difficult to create at sufficient variety. The possibility of such multi-dataset training enables superior generalization ability. In contrast to earlier monocular full body methods, our approach captures more expressive 3D face geometry and color by estimating the shape, expression, albedo and illumination parameters of a statistical face model. Our method achieves competitive accuracy on public benchmarks, while being significantly faster and providing more complete face reconstructions.",https://ieeexplore.ieee.org/document/9577987/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.01099,"Monocular, One-stage, Regression of Multiple 3D People",IEEE,Conferences,"This paper focuses on the regression of multiple 3D people from a single RGB image. Existing approaches predominantly follow a multi-stage pipeline that first detects people in bounding boxes and then independently regresses their 3D body meshes. In contrast, we propose to Regress all meshes in a One-stage fashion for Multiple 3D People (termed ROMP). The approach is conceptually simple, bounding box-free, and able to learn a per-pixel representation in an end-to-end manner. Our method simultaneously predicts a Body Center heatmap and a Mesh Parameter map, which can jointly describe the 3D body mesh on the pixel level. Through a body-center-guided sampling process, the body mesh parameters of all people in the image are easily extracted from the Mesh Parameter map. Equipped with such a fine-grained representation, our one-stage framework is free of the complex multi-stage process and more robust to occlusion. Compared with state-of-the-art methods, ROMP achieves superior performance on the challenging multi-person benchmarks, including 3DPW and CMU Panoptic. Experiments on crowded/occluded datasets demonstrate the robustness under various types of occlusion. The code, released at https://github.com/Arthur151/ROMP, is the first real-time implementation of monocular multi-person 3D mesh regression.",https://ieeexplore.ieee.org/document/9710639/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2008.36,Moving Sensor Video Image Processing Enhanced with Elimination of Ego Motion by Global Registration and SIFT,IEEE,Conferences,"This field of target tracking is relatively mature when the camera is stationary but the moving sensor poses uniquely challenging problems because relative to the camera, everything in the scene appears to be moving. This report will present a robust and efficient video stabilization algorithm based on the scale invariant feature transform (SIFT) algorithm and global registration. An important comparison will be made to KLT, a widely used feature based tracker, stabilization algorithms. We will demonstrate KLT's deficiencies and SIFT's advantages when confronted with limited features and 3D structures. Finally an implementation of the overall system with real data provided by UAV's from MRLets Technologies Inc will be described and analyzed.",https://ieeexplore.ieee.org/document/4669752/,2008 20th IEEE International Conference on Tools with Artificial Intelligence,3-5 Nov. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMEW53276.2021.9455983,Multi-Modal Fusion Enhanced Model For Driver&#x2019;s Facial Expression Recognition,IEEE,Conferences,"Facial expression recognition (FER) for monitoring a driver&#x2019;s emotional state has become an increasing need for advanced driver assistant systems (ADAS). Though state-of-art results of recognition accuracy have been achieved in FER with the development of deep neural networks (DNNs) in recent years, FER in real-world is still challenging due to illumination and head pose variation. In this work, we propose a multi-modal fusion based FER model capable of recognizing facial expressions accurately regardless of the lighting conditions and head poses, using a structured-light imaging camera which provides three modalities of images - RGB, Near-infrared (NIR), and Depth Maps. The model is implemented in two phases, where the first phase extracts feature from single modalities separately using 3D ResNet while the second phase combines the multi-modal features and classifies expressions. The model is trained and tested with a novel facial expression dataset with the three image modalities, with varying lighting conditions and head poses. The experimental results show that combining different modalities improves the model performance and robustness. A recognition accuracy of over 90&#x0025; has been obtained in the usage scenario of FER for drivers.",https://ieeexplore.ieee.org/document/9455983/,2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),5-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICE54393.2021.00059,Multi-Rotor UAV Autonomous Tracking and Obstacle Avoidance Based on Improved DDPG,IEEE,Conferences,"To solve the problem of multi-rotor UAV autonomous tracking dynamic ground targets in obstacles environment, we used Markov decision process (MDP) to establish an autonomous maneuvering model of multi-rotor. Considering the obstacle avoidance requirements of UAV during the tracking process, we integrated the Long Short-Term Memory (LSTM) neural network with memory unit and time series data processing characteristics into the Deep Deterministic Policy Gradient (DDPG) algorithm framework, so that the Actor network can fully refer to the prior state information when making decisions. Finally, the performance test was implemented on the UAV 3D simulation platform based on Robot Operating System (ROS). The results show that the method proposed in this paper can enable the UAV to complete the whole process of autonomous tracking of the ground dynamic target. Compared with the traditional DDPG algorithm, the DDPG algorithm combined with LSTM has stronger accuracy and real-time performance, and can better meet the tracking and obstacle avoidance mission requirements of the multi-rotor UAV.",https://ieeexplore.ieee.org/document/9797591/,2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE),5-7 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct51615.2020.00067,Multi-feature 3D Object Tracking with Adaptively-Weighted Local Bundles,IEEE,Conferences,"3D object tracking with monocular RGB images faces many challenges in real environments. The popular color- and edge-based methods, although have been well studied, are still known to be limited in handling specific cases. We observed that the color and edge features are complementary for different cases, and thus propose to fuse them for improving tracking robustness. To optimize the combination and to cope with inconsistency between color and edge features, we propose to fuse different energy terms with respect to a set of local bundles. Each bundle represents a local region containing a set of pixel locations for computing color and edge energies, in which two energy terms are adaptively weighted to play advantages of them. Experiments show that the proposed method can improve the accuracy in challenging cases, especially in light changing and similar color condition.",https://ieeexplore.ieee.org/document/9288449/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DS-RT.2014.24,Multi-player Educational Video Game over Cloud to Stimulate Logical Reasoning of Children,IEEE,Conferences,"This research is about how to create an educational video game whose main purpose is to stimulate logical and spatial reasoning of children. For this goal, we have applied an incremental methodology in order to be able to produce a Massive Multiplayer Online Game. With the intention of incorporating operations necessary for cognitive development and simultaneously to control the mechanics of the video game, we applied Artificial Intelligence, Intelligent Agents, and modern 3D technology approaches such as Unity 3D Game Engine and Photon Cloud. Validation of our solution has been performed by running several tests in public schools. The results demonstrate that this educational software stimulates the cognitive development of children.",https://ieeexplore.ieee.org/document/6957185/,2014 IEEE/ACM 18th International Symposium on Distributed Simulation and Real Time Applications,1-3 Oct. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2015.7354094,Multi-robot 6D graph SLAM connecting decoupled local reference filters,IEEE,Conferences,"Teams of mobile robots can be deployed in search and rescue missions to explore previously unknown environments. Methods for joint localization and mapping constitute the basis for (semi-)autonomous cooperative action, in particular when navigating in GPS-denied areas. As communication losses may occur, a decentralized solution is required. With these challenges in mind, we designed a submap-based SLAM system that relies on inertial measurements and stereo-vision to create multi-robot dense 3D maps. For online pose and map estimation, we integrate the results of keyframe-based local reference filters through incremental graph SLAM. To the best of our knowledge, we are the first to combine these two methods to benefit from their particular advantages for 6D multi-robot localization and mapping: Local reference filters on each robot provide real-time, long-term stable state estimates that are required for stabilization, control and fast obstacle avoidance, whereas online graph optimization provides global multi-robot pose and map estimates needed for cooperative planning. We propose a novel graph topology for a decoupled integration of local filter estimates from multiple robots into a SLAM graph according to the filters' uncertainty estimates and independence assumptions and evaluated its benefits on two different robots in indoor, outdoor and mixed scenarios. Further, we performed two extended experiments in a multi-robot setup to evaluate the full SLAM system, including visual robot detections and submap matches as inter-robot loop closure constraints.",https://ieeexplore.ieee.org/document/7354094/,2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),28 Sept.-2 Oct. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00816,Multi-view Depth Estimation using Epipolar Spatio-Temporal Networks,IEEE,Conferences,"We present a novel method for multi-view depth estimation from a single video, which is a critical task in various applications, such as perception, reconstruction and robot navigation. Although previous learning-based methods have demonstrated compelling results, most works estimate depth maps of individual video frames independently, without taking into consideration the strong geometric and temporal coherence among the frames. Moreover, current state-of-the-art (SOTA) models mostly adopt a fully 3D convolution network for cost regularization and therefore require high computational cost, thus limiting their deployment in real-world applications. Our method achieves temporally coherent depth estimation results by using a novel Epipolar Spatio-Temporal (EST) transformer to explicitly associate geometric and temporal correlation with multiple estimated depth maps. Furthermore, to reduce the computational cost, inspired by recent Mixture-of-Experts models, we design a compact hybrid network consisting of a 2D context-aware network and a 3D matching network which learn 2D context information and 3D disparity cues separately. Extensive experiments demonstrate that our method achieves higher accuracy in depth estimation and significant speedup than the SOTA methods.",https://ieeexplore.ieee.org/document/9577311/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2018.8593899,Multisensor Online Transfer Learning for 3D LiDAR-Based Human Detection with a Mobile Robot,IEEE,Conferences,"Human detection and tracking is an essential task for service robots, where the combined use of multiple sensors has potential advantages that are yet to be fully exploited. In this paper, we introduce a framework allowing a robot to learn a new 3D LiDAR-based human classifier from other sensors over time, taking advantage of a multisensor tracking system. The main innovation is the use of different detectors for existing sensors (i.e. RGB-D camera, 2D LiDAR) to train, online, a new 3D LiDAR-based human classifier based on a new “trajectory probability”. Our framework uses this probability to check whether new detection belongs to a human trajectory, estimated by different sensors and/or detectors, and to learn a human classifier in a semi-supervised fashion. The framework has been implemented and tested on a real-world dataset collected by a mobile robot. We present experiments illustrating that our system is able to effectively learn from different sensors and from the environment, and that the performance of the 3D LiDAR-based human classification improves with the number of sensors/detectors used.",https://ieeexplore.ieee.org/document/8593899/,2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),1-5 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KIMAS.2003.1245070,Multisensor image fusion & mining: from neural systems to COTS software,IEEE,Conferences,"We summarize our methods for the fusion of multisensor imagery based on concepts derived from neural models of visual processing and pattern learning and recognition. These methods have been applied to real-time fusion of night vision sensors in the field, airborne multispectral and hyperspectral imaging systems, and space-based multiplatform multimodality sensors. The methods enable color fused 3D visualization, as well as interactive exploitation and data mining in the form of human-guided machine learning and search for targets and cultural features. Over the last year we have developed a user-friendly system integrated into a COTS exploitation environment known as ERDAS Imagine. We demonstrate fusion and interactive mining of low-light Visible/SWIR/MWIR/LWIR night imagery, and IKONOS multispectral imagery. We also demonstrate how target learning and search can be enabled over extended operating conditions by allowing training over multiple scenes. This is illustrated for detecting small boats in coastal waters using fused Visible/MWIR/LWIR imagery.",https://ieeexplore.ieee.org/document/1245070/,IEMC '03 Proceedings. Managing Technologically Driven Organizations: The Human Side of Innovation and Change (IEEE Cat. No.03CH37502),30 Sept.-4 Oct. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WARSD.2003.1295180,Multisensor image fusion and mining: from neural systems to COTS software with application to remote sensing AFE,IEEE,Conferences,"We summarize our methods for the fusion of multisensor/spectral imagery based on concepts derived from neural models of visual processing (adaptive contrast enhancement, opponent-color contrast, multi-scale contour completion, and multi-scale texture enhancement) and semi-supervised pattern learning and recognition. These methods have been applied to the problem of aided feature extraction (AFE) from remote sensing airborne multispectral and hyperspectral imaging systems, and space-based multi-platform multi-modality imaging sensors. The methods enable color fused 3D visualization, as well as interactive exploitation and data mining in the form of human-guided machine learning and search for objects, landcover, and cultural features. This technology has been evaluated on space-based imagery for the National Imagery and Mapping Agency, and real-time implementation has also been demonstrated for terrestrial fused-color night imaging. We have recently incorporated these methods into a commercial software platform (ERDAS Imagine) for imagery exploitation. We describe the approach and user interfaces, and show results for a variety of sensor systems with application to remote sensing feature extraction including EO/IR/MSI/SAR imagery from Landsat and Radarsat, multispectral Ikonos imagery, and Hyperion and HyMap hyperspectral imagery.",https://ieeexplore.ieee.org/document/1295180/,"IEEE Workshop on Advances in Techniques for Analysis of Remotely Sensed Data, 2003",27-28 Oct. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2009.64,Networking Resources for Research and Scientific Education in BW-eLabs,IEEE,Conferences,"The aim of the BW-eLabs architecture (networked virtual and remote labs in Baden-Wuumlrttemberg) is the expansion of the access to heterogeneous experimental resources (remote and virtual) for the sustainable indexing and use of raw data and experiments for research and education purposes. The creation of efficient possibilities of external access to local experimental surroundings as well as a guarantee of transparency and reproducibility of experiments form a central request. A substantial characteristic of BW-eLabs is that the corresponding data and documents are examined along their entire life cycle and embedded into the entire process chain of experimental environments. Scientific communities and the promotion of cooperation and collaboration in high-technology take centre stage in this concept. Nanotechnology serves as a pilot discipline because especially in this cost intensive area access to experimental equipment is an important prerequisite for ensuring access to professional tools for all scientific communities involved. Existing infrastructure, e.g. digital libraries, decentralized tools, and repositories, are embedded into the 3D-Plattform BW-eLabs. The BW-eLabs stands under OpenAccess-Policy and sees itself as an open network for scientific data and experimental set-ups.",https://ieeexplore.ieee.org/document/5286694/,"2009 10th ACIS International Conference on Software Engineering, Artificial Intelligences, Networking and Parallel/Distributed Computing",27-29 May 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICONIP.2002.1201979,Neural network algorithm for solving ray-tracing problem,IEEE,Conferences,"This work is dedicated to the study of neural network method for solving of ray-tracing task, which appears in 3D visualization algorithms. Physical representation of the task is the problem of finding the nearest point of the ""vision"" ray crossing with the surfaces of the scene. Application: Real time 3D visualization, rendering of the complex scenes, containing semitransparent, reflecting, diffusive objects, soft shadows and volume light sources.",https://ieeexplore.ieee.org/document/1201979/,"Proceedings of the 9th International Conference on Neural Information Processing, 2002. ICONIP '02.",18-22 Nov. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HSI49210.2020.9142672,Neural network based algorithm for hand gesture detection in a low-cost microprocessor applications,IEEE,Conferences,"In this paper the simple architecture of neural network for hand gesture classification was presented. The network classifies the previously calculated parameters of EMG signals. The main goal of this project was to develop simple solution that is not computationally complex and can be implemented on microprocessors in low-cost 3D printed prosthetic arms. As the part of conducted research the data set EMG signals corresponding to 5 different gestures was created. The accuracy of elaborated solution was 90% when applied real time on data sampled with 1kHz frequency and 75% when applied real time on data acquired and process directly on microprocessor with lower,100Hz sampling frequency.",https://ieeexplore.ieee.org/document/9142672/,2020 13th International Conference on Human System Interaction (HSI),6-8 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.1999.791257,Neurocalibration: a neural network that can tell camera calibration parameters,IEEE,Conferences,"Camera calibration is a primary crucial step in many computer vision tasks. We present a new neural approach for camera calibration. Unlike some existing neural approaches, our calibrating network can tell the perspective-projection-transformation matrix between the world 3D points and the corresponding 2D image pixels. Starting from random initial weights, the net can specify the camera model parameters satisfying the orthogonality constraints on the rotational transformation. The neurocalibration technique is shown to solve four different types of calibration problems that are found in computer vision applications. Moreover, it can be extended to the more difficult problem of calibrating cameras with automated active lenses. The validity and performance of our technique are tested with both synthetic data under different noise conditions and with real images. Experiments have shown the accuracy and the efficiency of our neurocalibration technique.",https://ieeexplore.ieee.org/document/791257/,Proceedings of the Seventh IEEE International Conference on Computer Vision,20-27 Sept. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2014.6888679,New approach on PCA-based 3D face recognition and authentication,IEEE,Conferences,"In this paper, we propose a new approach which allows us producing a new representation independent from the position and the orientation of each 3D point cloud. This approach builds, from the 3D point cloud, the models of faces which are used afterward for the recognition. This framework allows us to use statistical inferences such as the estimation of the missing parts of the face by means of the PCA on the tangent spaces of the variety of shape. For that purpose, the proposed method explores the basic of projection by comparing every point cloud input with those of the database. To reduce the cost of the exploration, we define a comparison function based on the transformed of 3D distance. Experimental results using real data show the potential of our method, we obtain a 99% rate of verification performance of the CASIA-3D dataset, which compares well with other state of the art methods.",https://ieeexplore.ieee.org/document/6888679/,"15th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",30 June-2 July 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SKIMA47702.2019.8982452,Novel Technique for Isolated Sign Language Based on Fingerspelling Recognition,IEEE,Conferences,"Sign language is used by deaf and hard hearing people to exchange information between their own community and with other people. Fingerspelling recognition method from isolate sign language has attracted research interest in computer vision and human-computer interaction based on a novel technique. The essential for real-time recognition of isolate sign language has grown with the emergence of better-capturing devices such as Kinect sensors. The purpose of this paper is to design a user independent framework for automatic recognition of American Sign Language which can recognize several one-handed dynamic isolated signs and interpreting their meaning. We built datasets as a raw data for alphabets (A-Z) or numbers (1-20) by used left-hand the 3D point (XL, YL, ZL) or switch by right-hand (XR, YR, ZR) centroid as one of contribution. The proposed approach was tested for gestures that involve left-hand or right-hand and was compared with other approach and gave better accuracy. Two machine learning methods are involved like Hidden Conditional Random Field (HCRF), and Random Decision Forest (RDF) for the classification part. The third contribution based on low lighting condition and cluttered background. In this research work is achieved for recognition accuracy over 99.7%.",https://ieeexplore.ieee.org/document/8982452/,"2019 13th International Conference on Software, Knowledge, Information Management and Applications (SKIMA)",26-28 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TAI.1994.346403,Object reconstruction using the cooperation of 3D segments and 3D facets information,IEEE,Conferences,"Given a series of trinocular images of an object, we have developed a method for building 3D Facets and 3D segments model of the object. From each triplet, a partial description of the object, called 3D View, is extracted. From the set of all extracted 3D Facets, a strategy for guiding the object reconstruction, based on a statistical method is developed. A 3D Matching Builder computes matchings between the 3D Primitives of consecutive 3D Views. Guided by the strategy, a Superstructure gathers all the matching informations given by the 3D Matching Builder in a set of equivalence classes. For each equivalence class of Superstructure, a representative is derived. The 3D Primitives model is finally computed merging informations of 3D Facet representatives and 3D Segment representatives. This method implemented in Smalltalk80, has been applied on a series of stereoscopic real images triplets; some results are provided at the end of the paper.<>",https://ieeexplore.ieee.org/document/346403/,Proceedings Sixth International Conference on Tools with Artificial Intelligence. TAI 94,6-9 Nov. 1994,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ASCC56756.2022.9828170,Object-Oriented Navigation with a Multi-layer Semantic Map,IEEE,Conferences,"In this paper, a novel architecture is proposed for object-oriented navigation of a mobile robot. The idea is to autonomous navigate and docking with respect to a given object in an indoor environment. For this purpose, the navigation scheme is designed based on a novel multi-layer semantic map. The map includes an occupancy layer for path planning, a localization layer for control and an object pose estimation layer for docking control. The two-dimensional (2D) occupancy map is generated from a 3D RTAB-Map method. The multi-layer semantic map integrates the outputs of a deep-learning model of real-time 3D object pose estimation for robot localization. Experimental results show that the proposed navigation system can plan a collision-free path for the mobile robot to reach and dock to the target object. The LiDAR-based navigation control runs at 15 Hz and the object-based localization is updated at 4 Hz on the Jetson TX2 embedded controller. In the current implementation, the mobile robot can dock to an object with an accuracy of 8 cm in position and 5 degrees in orientation. The navigation design has the potential to be applied for daily life tasks of mobile robots in an indoor environment.",https://ieeexplore.ieee.org/document/9828170/,2022 13th Asian Control Conference (ASCC),4-7 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CYBER.2017.8446065,Obstacle avoidance of aerial vehicle based on monocular vision,IEEE,Conferences,"Collision-free autonomous navigation is extremely important for quadcopter and other flying robots. The implementation of autonomic moving capabilities can contribute significantly to their promotion and usage in fields such as goods delivering, aerial photos shooting, and monitoring. In order to realize the autonomous flight without crash, the obstacle avoidance problem demands a prompt solution. Also, with the concern of cost and endurance, using only single camera to perform this task would be a better choice for low-cost flying robots. Thus, this paper focuses on achieving quadcopter's collision avoidance in unknown stable (rarely changes, such as high sky or inner room) environment only by single camera. The algorithm proposed by this paper is composed of PTAM (Parallel Tracking and Mapping), DTAM (Dense Tracking and Mapping in Real-Time) algorithm and CNN (Convolutional Neural Network). PTAM is used to create the 3D map of the environment. DTAM is used to obtain the depth map of those image frames. And the CNN is used to train and get a model used for automatically avoidance. Finally, this algorithm is proved to be valid by an experiment.",https://ieeexplore.ieee.org/document/8446065/,"2017 IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)",31 July-4 Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV51458.2022.00037,Occlusion Resistant Network for 3D Face Reconstruction,IEEE,Conferences,"3D face reconstruction from a monocular face image is a mathematically ill-posed problem. Recently, we observed a surge of interest in deep learning-based approaches to address the issue. These methods possess extreme sensitivity towards occlusions. Thus, in this paper, we present a novel context-learning-based distillation approach to tackle the occlusions in the face images. Our training pipeline focuses on distilling the knowledge from a pre-trained occlusion-sensitive deep network. The proposed model learns the context of the target occluded face image. Hence our approach uses a weak model (unsuitable for occluded face images) to train a highly robust network towards partially and fully-occluded face images. We obtain a landmark accuracy of 0.77 against 5.84 of recent state-of-the-art-method for real-life challenging facial occlusions. Also, we propose a novel end-to-end training pipeline to reconstruct 3D faces from multiple variations of the target image per identity to emphasize the significance of visible facial features during learning. For this purpose, we leverage a novel composite multi-occlusion loss function. Our multi-occlusion per identity model shows a dip in the landmark error by a large margin of 6.67 in comparison to a recent state-of-the-art method. We deploy the occluded variations of the CelebA validation dataset and AFLW2000-3D face dataset: naturally-occluded and artificially occluded, for the comparisons. We comprehensively compare our results with the other approaches concerning the accuracy of the reconstructed 3D face mesh for occluded face images.",https://ieeexplore.ieee.org/document/9706716/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IE54923.2022.9826758,On-the-Fly Spatio-Temporal Human Segmentation of 3D Point Cloud Data By Micro-Size LiDAR,IEEE,Conferences,"The technology of 3D recognition is evolving rapidly, enabling unprecedented growth of applications towards human-centric intelligent environments. On top of these applications human segmentation is a key technology towards analyzing and understanding human mobility in those environments. However, existing segmentation techniques rely on deep learning models, which are computationally intensive and data-hungry solutions. This hinders their practical deployment on edge devices in realistic environments. In this paper, we introduce a novel micro-size LiDAR device for understanding human mobility in the surrounding environment. The device is supplied with an on-device lightweight human segmentation technique for the captured 3D point cloud data using density-based clustering. The proposed technique significantly reduces the computational complexity of the clustering algorithm by leveraging the Spatiotemporal relation between consecutive frames. We implemented and evaluated the proposed technique in a real-world environment. The results show that the proposed technique obtains a human segmentation accuracy of 99% with a drastic reduction of the processing time by 66%.",https://ieeexplore.ieee.org/document/9826758/,2022 18th International Conference on Intelligent Environments (IE),20-23 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412087,One-stage Multi-task Detector for 3D Cardiac MR Imaging,IEEE,Conferences,"Fast and accurate landmark location and bounding box detection are important steps in 3D medical imaging. In this paper, we propose a novel multi-task learning framework, for real-time, simultaneous landmark location and bounding box detection in 3D space. Our method extends the famous single-shot multibox detector (SSD) from single-task learning to multitask learning and from 2D to 3D. Furthermore, we propose a post-processing approach to refine the network landmark output, by averaging the candidate landmarks. Owing to these settings, the proposed framework is fast and accurate. For 3D cardiac magnetic resonance (MR) images with size 224×224×64, our framework runs ~128 volumes per second (VPS) on GPU and achieves 6.75mm average point-to-point distance error for landmark location, which outperforms both state-of-the-art and baseline methods. We also show that segmenting the 3D image cropped with the bounding box results in both improved performance and efficiency.",https://ieeexplore.ieee.org/document/9412087/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196769,Online LiDAR-SLAM for Legged Robots with Robust Registration and Deep-Learned Loop Closure,IEEE,Conferences,"In this paper, we present a 3D factor-graph LiDAR-SLAM system which incorporates a state-of-the-art deeply learned feature-based loop closure detector to enable a legged robot to localize and map in industrial environments. Point clouds are accumulated using an inertial-kinematic state estimator before being aligned using ICP registration. To close loops we use a loop proposal mechanism which matches individual segments between clouds. We trained a descriptor offline to match these segments. The efficiency of our method comes from carefully designing the network architecture to minimize the number of parameters such that this deep learning method can be deployed in real-time using only the CPU of a legged robot, a major contribution of this work. The set of odometry and loop closure factors are updated using pose graph optimization. Finally we present an efficient risk alignment prediction method which verifies the reliability of the registrations. Experimental results at an industrial facility demonstrated the robustness and flexibility of our system, including autonomous following paths derived from the SLAM map.",https://ieeexplore.ieee.org/document/9196769/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2017.8202247,Online learning for human classification in 3D LiDAR-based tracking,IEEE,Conferences,"Human detection and tracking are essential aspects to be considered in service robotics, as the robot often shares its workspace and interacts closely with humans. This paper presents an online learning framework for human classification in 3D LiDAR scans, taking advantage of robust multi-target tracking to avoid the need for data annotation by a human expert. The system learns iteratively by retraining a classifier online with the samples collected by the robot over time. A novel aspect of our approach is that errors in training data can be corrected using the information provided by the 3D LiDAR-based tracking. In order to do this, an efficient 3D cluster detector of potential human targets has been implemented. We evaluate the framework using a new 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments analyse the real-time performance of the cluster detector and show that our online learned human classifier matches and in some cases outperforms its offline version.",https://ieeexplore.ieee.org/document/8202247/,2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24-28 Sept. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2008.4587514,Online learning of patch perspective rectification for efficient object detection,IEEE,Conferences,"For a large class of applications, there is time to train the system. In this paper, we propose a learning-based approach to patch perspective rectification, and show that it is both faster and more reliable than state-of-the-art ad hoc affine region detection methods. Our method performs in three steps. First, a classifier provides for every keypoint not only its identity, but also a first estimate of its transformation. This estimate allows carrying out, in the second step, an accurate perspective rectification using linear predictors. We show that both the classifier and the linear predictors can be trained online, which makes the approach convenient. The last step is a fast verification - made possible by the accurate perspective rectification - of the patch identity and its sub-pixel precision position estimation. We test our approach on real-time 3D object detection and tracking applications. We show that we can use the estimated perspective rectifications to determine the object pose and as a result, we need much fewer correspondences to obtain a precise pose estimation.",https://ieeexplore.ieee.org/document/4587514/,2008 IEEE Conference on Computer Vision and Pattern Recognition,23-28 June 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IHMSC.2009.38,Ontology-Based Fast Devices Configuration in 3D Configuration Software,IEEE,Conferences,"Using 3D user interface in the configuration software, the user interface can become realistic and friendly. And it can reflect the devices running status in the industrial field so realistically. Moreover, it can avoid the window effect effectively due to the screen size influence. The window information content has the enormous enhancement. But it brings some other problems such as configuration difficultly. This paper presents a ontology-based fast configuration method by attaching semantic information to the device model. The computer can understand the user intention and assist him do the configuration work. Using this method, the devices configuration becomes so simple and convenient. The users that are not 3D professionals can also carry on the devices configuration relaxed.",https://ieeexplore.ieee.org/document/5335932/,2009 International Conference on Intelligent Human-Machine Systems and Cybernetics,26-27 Aug. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOTEH48170.2020.9066274,Ontology-Driven Generation of Interactive 3D Worlds,IEEE,Conferences,"In this paper, the ontology-driven approach to automated generation of interactive 3D applications is presented. The ontologies describing both static and dynamic application properties combined with domain-specific knowledge are leveraged. The approach is evaluated in two case studies: smart city simulator and virtual robotics testbed. The generated output is JavaScript code using Three.js library. According to the achieved results, the adoption of ontologies dramatically speeds up the development time compared to manual process.",https://ieeexplore.ieee.org/document/9066274/,2020 19th International Symposium INFOTEH-JAHORINA (INFOTEH),18-20 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2004.1380385,Optimal control for real-time visualization and 3D rendering using neural networks,IEEE,Conferences,This article describes a novel approach to the real-time visualization and 3D rendering using neural networks. The present scheme is within the general framework of approximate dynamic programming where optimal/suboptimal control is achieved through learning to use pulse-coupled neural networks. The neural network model is proposed in this paper for optimal control with control constraints. The resulting 3D models can then be viewed from any angle and subsequently processed to integrately match them against 3D model data stored in a synthetic database. Our work is unique in that it supports both an online and off-tine visualization and rendering of a distributed system.,https://ieeexplore.ieee.org/document/1380385/,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),26-29 Aug. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASID50160.2020.9271765,PPMGNet: A Neural Network Algorithm for Point Cloud 3D Object Detection,IEEE,Conferences,"3D object detection based on point cloud data is an essential part of L4 automatic driving. This paper proposes a novel end-to-end trainable deep learning network structure, PPMGNet, which can quickly encode point clouds, obtain the spatial feature of point clouds, predict multiple categories, and perform 3D object detection in real time. A large number of experiments show that in terms of speed and accuracy, PPMGNet's detection performance is very suitable for direct deployment in autonomous driving applications.",https://ieeexplore.ieee.org/document/9271765/,"2020 IEEE 14th International Conference on Anti-counterfeiting, Security, and Identification (ASID)",30 Oct.-1 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CACRE52464.2021.9501390,Pallet detection and localization with RGB image and depth data using deep learning techniques,IEEE,Conferences,"This paper presents a novel approach of pallet identification and localization algorithm (PILA) based on RGB image and depth data. The algorithm is implemented in C++ for real-time running and the RGB and depth data from low-cost RGB-D camera. Deep neural network (DNN) method is applied to detect and locate the pallet in the RGB images. The pallet's point cloud data is correlated with the labeled region of interest (ROI) in the RGB images through RGB-D fusion. The pallet's front-face plane is extracted and the orientation of the pallet is obtained at the same time. The triangle centric points of pallet's front-face could be determined with extracting x and y lines at the edge by the simple geometrical rules. Experimentally, the orientation angle and centric location of the two kinds of pallets are investigated with natural pallet surface without any artificial markings. The results show that the pallet could be located with the 3D localization accuracy of 1cm and the angle resolution of 0.4 degree at the distance of 3m. The end-to-end running time is less than 700 ms from CAN-IO interface and this is a promising solution for autonomous pallet picking instrument and self-driving forklift applications.",https://ieeexplore.ieee.org/document/9501390/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/URAI.2012.6463006,Path planning through maze routing for a mobile robot with nonholonomic constraints,IEEE,Conferences,"A comprehensive technique to plan path for a mobile robot with nonholonomic constraints through maze routing technique has been presented. Our robot uses a stereo vision based approach to detect the obstacles by creating dense 3D point clouds from the stereo images. ROS packages have been implemented on the robot for specific tasks of providing: i) Linear and angular velocity commands, ii) Calibration and rectification of the stereo images for generating point clouds, iii) Simulating the URDF (Unified Robot Description Format) module in real time, with respect to the real robot and iv) For visualizing the sensor data. For efficient path planning a hybrid technique using Lee's algorithm, modified by Hadlock and Soukup's algorithm has been implemented. Different path planning results have been shown using the maze routing algorithms. Preliminary results shows that Lee's algorithm is more time consuming in comparison with other algorithms. A hybrid of Lee's with Soukup's algorithm is more efficient but unpredictable for minimal path. A hybrid of Lee's with Hadlock's algorithm is the most efficient and least time consuming.",https://ieeexplore.ieee.org/document/6463006/,2012 9th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI),26-28 Nov. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CRV52889.2021.00019,PathBench: A Benchmarking Platform for Classical and Learned Path Planning Algorithms,IEEE,Conferences,"Path planning is a key component in mobile robotics. A wide range of path planning algorithms exist, but few attempts have been made to benchmark the algorithms holistically or unify their interface. Moreover, with the recent advances in deep neural networks, there is an urgent need to facilitate the development and benchmarking of such learning-based planning algorithms. This paper presents PathBench, a platform for developing, visualizing, training, testing, and benchmarking of existing and future, classical and learned 2D and 3D path planning algorithms, while offering support for Robot Operating System (ROS). Many existing path planning algorithms are supported; e.g. A*, wavefront, rapidly-exploring random tree, value iteration networks, gated path planning networks; and integrating new algorithms is easy and clearly specified. We demonstrate the benchmarking capability of PathBench by comparing implemented classical and learned algorithms for metrics, such as path length, success rate, computational time and path deviation. These evaluations are done on built-in PathBench maps and external path planning environments from video games and real world databases. PathBench is open source 1.",https://ieeexplore.ieee.org/document/9469507/,2021 18th Conference on Robots and Vision (CRV),26-28 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoDT255437.2022.9787422,Physical Adversarial Attack Scheme on Object Detectors using 3D Adversarial Object,IEEE,Conferences,"Adversarial attacks are being frequently used these days to exploit different machine learning models including the deep neural networks (DNN) either during the training or testing stage. DNN under such attacks make the false predictions. Digital adversarial attacks are not applicable in physical world. Adversarial attack on object detection is more difficult as compared to the adversarial attack on image classification. This paper presents a physical adversarial attack on object detection using 3D adversarial objects. The proposed methodology overcome the constraint of 2D adversarial patches as they only work for certain viewpoints only. We have mapped an adversarial texture onto a mesh to create the 3D adversarial object. These objects are of various shapes and sizes. Unlike adversarial patch attacks, these adversarial objects are movable from one place to another. Moreover, application of 2D patch is limited to confined viewpoints. Experimentation results show that our 3D adversarial objects are free from such constraints and perform a successful attack on object detection. We used the ShapeNet dataset for different vehicle models. 3D objects are created using Blender 2.93 [1]. Different HDR images are incorporated to create the virtual physical environment. Moreover, we targeted the FasterRCNN and YOLO pre-trained models on the COCO dataset as our target DNN. Experimental results demonstrate that our proposed model successfully fooled these object detectors.",https://ieeexplore.ieee.org/document/9787422/,2022 2nd International Conference on Digital Futures and Transformative Technologies (ICoDT2),24-26 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412150,Planar 3D Transfer Learning for End to End Unimodal MRI Unbalanced Data Segmentation,IEEE,Conferences,"We present a novel approach of 2D to 3D transfer learning based on mapping pre-trained 2D convolutional neural network weights into planar 3D kernels. The method is validated by the proposed planar 3D res-u-net network with encoder transferred from the 2D VGG-16, which is applied for a single-stage unbalanced 3D image data segmentation. In particular, we evaluate the method on the MICCAI 2016 MS lesion segmentation challenge dataset utilizing solely fluid-attenuated inversion recovery (FLAIR) sequence without brain extraction for training and inference to simulate real medical praxis. The planar 3D res-u-net network performed the best both in sensitivity and Dice score amongst end to end methods processing raw MRI scans and achieved comparable Dice score to a state-of-the-art unimodal not end to end approach. Complete source code was released under the open-source license, and this paper complies with the Machine learning reproducibility checklist. By implementing practical transfer learning for 3D data representation, we could segment heavily unbalanced data without selective sampling and achieved more reliable results using less training data in a single modality. From a medical perspective, the unimodal approach gives an advantage in real praxis as it does not require co-registration nor additional scanning time during an examination. Although modern medical imaging methods capture high-resolution 3D anatomy scans suitable for computer-aided detection system processing, deployment of automatic systems for interpretation of radiology imaging is still rather theoretical in many medical areas. Our work aims to bridge the gap by offering a solution for partial research questions.",https://ieeexplore.ieee.org/document/9412150/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PERCOMW.2010.5470538,Player Tracker - a tool to analyze sport players using RFID,IEEE,Conferences,"A work in progress of a tool designed to help sports coaches to analyze their players using an RFID technology connected to a 3-layer software is described in this paper. A few ranging techniques available in conventional RFID systems were studied to best fit this application scenario. Together with the RFID equipments, there is a 3-layer system managing all the data. The software bottom layer is responsible for the low-level communication between the software and the hardware. The middle layer is the system engine where all the calculation is performed. Finally, the top layer is the software part responsible for showing reports and the player analyzed. The system has 2 tracking modes, one for 2D player location in the field, and a 3D mode to capture players' movement in small area sports. It has 2 report modes also, one for real-time report displaying each player's actual location in the field; and the other to present reports with statistics of the player such as distance covered, a heat map of the field showing where the player is at a time interval and number of sprints.",https://ieeexplore.ieee.org/document/5470538/,2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops),29 March-2 April 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct51615.2020.00069,"Pleistocene Crete: A narrative, interactive mixed reality exhibition that brings prehistoric wildlife back to life",IEEE,Conferences,"This paper describes a three-part interactive museum exhibition targeted at the domain of Natural History, where various technological components are utilized to deliver a compelling Mixed Reality experience for the museum's visitors. The goal is to create new educational pathways for both adults and children wishing to learn about prehistoric life on the island of Crete, while simultaneously attracting a broader audience and maintaining its engagement with the museum digital content for longer periods of time. In these experiences, holographic technology, diminished reality and multi-view 3D reconstruction are combined with fact/fantasy storytelling, utilizing cost-efficient state-of-the-art solutions.",https://ieeexplore.ieee.org/document/9288450/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIRA.2005.1554245,Plenary talk June 29; The 3rdGeneration of Robotics: Ubiquitous Robot,IEEE,Conferences,"This talk shows its possibility of implementation in real life through demonstrations using a Sobot, Rity: i) continuous interface between physical and virtual worlds ii) seamless transmission of Sobot between a PC and a Mobot, and iii) omnipresence of Sobot. Rity, developed at the Robot Intelligence Technology (RIT) Laboratory, KAIST, is a Sobot implemented as a 12 DOF artificial creature in the virtual 3D world created in a PC. It has virtual sensors to survive in the virtual world and physical sensors attached to the PC to interact with the real world. Based on sensor information it can express its emotion, and interact with human beings through a web camera in the real world. It can generate behaviors autonomously and has its own IP. This means that it can be accessed through a network at anywhere and anytime using any device. With this technique omnipresence of Sobot can be realized in a ubiquitous space. The eventual goal of this research is to integrate Sobot, Embot, and Mobot to build up a Ubibot so that ubiquitous services through it can be available in a ubiquitous era",https://ieeexplore.ieee.org/document/1554245/,2005 International Symposium on Computational Intelligence in Robotics and Automation,27-30 June 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eIT53891.2022.9813882,Point Cloud Generation with Stereo and Monocular Depth Estimation on FPGA,IEEE,Conferences,"This research proposes a system that leverages stereo vision and monocular depth estimation to form a depth map from which a 3D point cloud scene is extracted. The emergence of competitive neural networks for depth map estimation presents an opportunity for deployment on embedded systems. Both stereo and monocular depth algorithms were tested on multiple FPGA hardware platforms. To perform acceleration on configurable hardware, a Deep Learning Processing Unit (DPU) IP was used for processing AI workloads, together with a Pseudo-LiDAR reprojection for 3D reconstruction on the Arm&#x00AE; processor. The DPU performance configuration will depend on the logic resources the targeted board has. This paper highlights the performance, resource utilization, and bottlenecks that came across different tested combinations of both hardware and neural network architectures.",https://ieeexplore.ieee.org/document/9813882/,2022 IEEE International Conference on Electro Information Technology (eIT),19-21 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412691,PointDrop: Improving Object Detection from Sparse Point Clouds via Adversarial Data Augmentation,IEEE,Conferences,"Current 3D object detection methods achieve accurate and efficient results on the standard point cloud dataset. However, in real-world applications, the point cloud samples obtained in the real-time running may be much sparser due to various reasons (occlusion, low reflectivity of objects and fewer laser beams) and existing methods do not consider the limitations of their models on sparse point clouds. To improve the robustness of an object detector to sparser point clouds, we propose PointDrop, which learns to drop the features of some key points in the point clouds to generate challenging sparse samples for data augmentation. Moreover, PointDrop is able to adjust the difficulty of the generated samples based on the capacity of the detector and thus progressively improve the performance of the detector. We create two sparse point clouds datasets from the KITTI dataset to evaluate our method, and the experimental results show that PointDrop significantly improves the robustness of the detector to sparse point clouds.",https://ieeexplore.ieee.org/document/9412691/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3DUI.2014.6798857,Poster: Evaluation of a smart tablet's interface for 3D interaction,IEEE,Conferences,"The present work highlights current developments and an ongoing experimental evaluation of how a tablet PC can effectively act as an interaction device for 3D manipulation tasks. More specifically, we focused on a 3D docking task. This collaborative project was conducted within the Visionair European project. The goal was to design a versatile interface that was independent of the targeted VR system (from 2D screens to CAVE-like systems). We investigated the usability of tactile (i.e. finger-controlled) tablets that are omnipresent in our daily life, assumed to have intuitive control characteristics and easily connected to a VR system (via WIFI connections). Participants' performance was evaluated when using a tablet, as compared to a classical mouse interface. Results indicated comparable learning effects (with task repetition) for both devices, but a clear advantage of the tablet's interface, as concerns the simultaneous use of multiple degrees of freedom of motion of the manipulated object.",https://ieeexplore.ieee.org/document/6798857/,2014 IEEE Symposium on 3D User Interfaces (3DUI),29-30 March 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3DUI.2015.7131722,Practical chirocentric 3DUI platform for immersive environments,IEEE,Conferences,"Chirocentric 3D user interfaces are sometimes hailed as the “holy grail” of human-computer interaction. However, implementations of these UIs can require cumbersome devices (such as tethered wearable datagloves), be limited in terms of functionality or obscure the algorithms used for hand pose and gesture recognition. These limitations inhibit designing, deploying and formally evaluating such interfaces. To ameliorate this situation, we describe the implementation of a practical chirocentric UI platform, targeted at immersive virtual environments with infrared tracking systems. Our main contributions are two machine learning techniques for the recognition of hand gestures (trajectories of the user's hands over time) and hand poses (configurations of the user's fingers) based on marker clouds and rigid body data. We report on the preliminary use of our system for the implementation of a bimanual 3DUI for a large immersive tiled display. We conclude with plans on using our system as a platform for the design and evaluation of bimanual chirocentric UIs, based on the Framework for Interaction Fidelity Analysis (FIFA).",https://ieeexplore.ieee.org/document/7131722/,2015 IEEE Symposium on 3D User Interfaces (3DUI),23-24 March 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VS-GAMES.2009.18,Prototyping a Context-Aware Framework for Pervasive Entertainment Applications,IEEE,Conferences,"This paper presents research and development of dedicated system architecture, designed to enable its users to interact with each other as well as to access information on Points of Interest that exist in their immediate environment. This is accomplished through managing personal preferences and contextual information in a distributed manner and in real-time. The advantage of this system is that it uses mobile devices, heterogeneous sensors and a selection of user interface paradigms to produce a socio-technical framework to enhance the perception of the environment and promote intuitive interactions. Representation of the real-world objects, their spatial relations and other captured features are visualised on scalable interfaces, ranging from 2D to 3D models and from photorealism to stylised clues and symbols. The framework is fit for use in unknown environments and therefore suitable for ubiquitous operation. The presented prototype is multifaceted and capable of supporting peer-to-peer exchange of information in a pervasive fashion, usable in various contexts. The modalities of these interactions are explored and laid out particularly in the context of entertainment.",https://ieeexplore.ieee.org/document/5116557/,2009 Conference in Games and Virtual Worlds for Serious Applications,23-24 March 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM48880.2022.9796802,RC6D: An RFID and CV Fusion System for Real-time 6D Object Pose Estimation,IEEE,Conferences,"This paper studies the problem of 6D pose estimation, which is practically important in various application scenarios such as robotic-based object grasping, obstacle avoidance in autonomous driving scene, and object integration in mixed reality. However, existing methods suffer from at least one of the five major limitations: dependence on object identification, complex deployment, difficulty in data collection, low accuracy, and incomplete estimation. To overcome the above limitations, this paper proposes an RC6D system, which is the first to estimate 6D poses by fusing RFID and Computer Vision (CV) data with multi-modal deep learning techniques. In RC6D, we first detect 2D keypoints through a deep learning approach. We then propose a novel RFID-CV fusion neural network to predict the depth of the scene, and use the estimated depth information to expand the 2D keypoints to 3D keypoints. Finally, we model the coordinate correspondences between the detected 2D-3D keypoints, which is applied to estimate the 6D pose of the target object. When implementing RC6D, we mainly address the following three technical challenges. (i) To predict 6D poses without using the CAD model, we propose a network architecture for monocular depth estimation. (ii) To train the neural network for 6D pose estimation without time-consuming 6D labeling, we use an unsupervised learning algorithm based on 2D-3D point pair matching. (iii) To detect the subject of the object without identification, we leverage optical flow to restrict the object and RFID to directly obtain its information. The experimental results show that the localization error of RC6D is less than 10 cm with a probability higher than 90.64% and its orientation estimation error is less than 10&#x00B0; with a probability higher than 79.63%. Hence, the proposed RC6D system performs much better than the state-of-the-art related solutions.",https://ieeexplore.ieee.org/document/9796802/,IEEE INFOCOM 2022 - IEEE Conference on Computer Communications,2-5 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DigitalHeritage.2013.6744821,REVEAL: One future for heritage documentation,IEEE,Conferences,"It can be frustrating to ensure that cultural heritage work, from archaeological excavations to historic surveys, is documented fully and that the evidence is recorded properly and thoroughly. Automated computer-based documentation and research tools would seem to offer many benefits. They can be more accurate and cost effective, saving time and ensuring that all finds and their contexts are appropriately and fully recorded. And if designed well, new digital field data acquisition systems can enable new types of hypothesis testing, new insight into the past, and new visualizations that in turn can lead to a paradigm shift in how heritage sites are managed and information disseminated. There have been many computer-based data collection systems for heritage management; many databases, many digital archives, and many digital publication options. REVEAL is special. REVEAL (Reconstruction and Exploratory Visualization: Engineering meets ArchaeoLogy), is the product of a US National Science Foundation collaboration among the Institute for the Visualization of History, Brown University's Division of Engineering, Laboratory for Man/Machine Systems, and the University of North Carolina, Charlotte's Department of Electrical and Computer Engineering. The project uses computer-vision, pattern-recognition, and machine-learning research to augment applications for archaeology and the humanities. REVEAL is a single piece of (free and open-source) software that coordinates all data types (e.g., photos, drawings, 3D models, and tabular information) with semi-automated tools for documenting sites, trenches and objects, recording excavation and site-evaluation progress, researching and analyzing the collected evidence, and creating 3D models and virtual worlds. Search and retrieval, building interactive visualizations, and testing hypo",https://ieeexplore.ieee.org/document/6744821/,2013 Digital Heritage International Congress (DigitalHeritage),28 Oct.-1 Nov. 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2012.57,Random Forests Based View Generation for Multiview TV,IEEE,Conferences,"The appearance of multiview display systems in the consumer market is not far from reality. With technical knowledge in this field constantly improving, production of multiview content is the only other key factor that will determine the successful adoption of this technology. Multiview content can be generated from two or three views and their associated depth maps. Estimating a high quality depth map is challenging. Moreover transmission of depth map information requires extra bandwidth. In this study, we propose an effective algorithm, which utilizes a 3D visual attention model, multiple monocular depth cues and a fraction of depth information for estimating the whole depth map of the scene using the Random Forests (RF) machine learning algorithm. Having the estimated depth maps and stereo videos, other views may be synthesized. Performance evaluations have shown that the proposed method estimates high quality depth maps for stereo sequences from limited depth information. Implementation of our proposed technique in the future multiview pipeline eliminates the need for estimating and transmitting the whole depth map for all the views, producing high quality multiview content while reducing the required bandwidth.",https://ieeexplore.ieee.org/document/6495069/,2012 IEEE 24th International Conference on Tools with Artificial Intelligence,7-9 Nov. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CADGRAPHICS.2015.23,Random-Accessible Volume Data Compression with Regression Function,IEEE,Conferences,"Typical volumetric data sets are 3D regular grids captured from computerized tomography (CT) or magnetic resonance (MR) scanners. There has been a constant tension between volume data resolution and memory usage. In this paper, we introduce an effective 3D compression scheme for volume data, which exploits the power of regression function. A regression function represents a non-linear mapping from positions of voxels to scalar values, and is modeled as a multi-layer feed-forward neural network. Our approach is feature preserving and resolution efficient, and supports real-time random access at decoding stage, via a simple GPU pixel shader. We applied our technique on multiple medical and scientific data sets, demonstrating that it is well suited for encoding and decoding volumetric data including quality and compression ratios.",https://ieeexplore.ieee.org/document/7450408/,2015 14th International Conference on Computer-Aided Design and Computer Graphics (CAD/Graphics),26-28 Aug. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561277,Rapid Pose Label Generation through Sparse Representation of Unknown Objects,IEEE,Conferences,"Deep Convolutional Neural Networks (CNNs) have been successfully deployed on robots for 6-DoF object pose estimation through visual perception. However, obtaining labeled data on a scale required for the supervised training of CNNs is a difficult task - exacerbated if the object is novel and a 3D model is unavailable. To this end, this work presents an approach for rapidly generating real-world, pose-annotated RGB-D data for unknown objects. Our method not only circumvents the need for a prior 3D object model (textured or otherwise) but also bypasses complicated setups of fiducial markers, turntables, and sensors. With the help of a human user, we first source minimalistic labelings of an ordered set of arbitrarily chosen keypoints over a set of RGB-D videos. Then, by solving an optimization problem, we combine these labels under a world frame to recover a sparse, keypoint-based representation of the object. The sparse representation leads to the development of a dense model and the pose labels for each image frame in the set of scenes. We show that the sparse model can also be efficiently used for scaling to a large number of new scenes. We demonstrate the practicality of the generated labeled dataset by training a CNN based 6-DoF object pose estimator.",https://ieeexplore.ieee.org/document/9561277/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA49215.2020.9233821,Real Time Three-Dimensional image processing with Application to Automated Cell Surgery Visual Servoing,IEEE,Conferences,"The locations of the target cell, its internal structures and surgical tools are essential to achieve visual servoing control for automated cell surgery. Real time image feedback allows the controller to adjust promptly for location variation due to motion of the cell when in contact with surgery tools. This paper uses Z-stack images of a 2-blastomere mouse embryo cell to develop a novel real time three-dimensional (3D) image processing algorithm. The proposed algorithm computes the centroid of each embryo blastomere and surgical tool tip in 3D image-plane coordinate. 3D Canny edge detector processes the embryo to produce a segmented 3D model. By converting the 3D model into a point cloud, the centroid of each blastomere is then estimated using an unsupervised machine learning technique, k-means clustering. For the surgical tool, 2D Canny edge detector is used in this case to compress computation time. The surgical tool tip location is selected as the furthest point away from the image edge where surgical tool appears. With 6.1μm computation variation and 2.4Hz update frequency, the proposed algorithm is suitable to perform automated cell surgery using visual servoing, especially Image-Based Visual Servoing (IBVS), with the obtained image-plane locations in the 3D image. The proposed algorithm has also shown theoretical potential to be implemented into other embryo development stages.",https://ieeexplore.ieee.org/document/9233821/,2020 IEEE International Conference on Mechatronics and Automation (ICMA),13-16 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSDE53843.2021.9718388,"Real world objects augmentation in virtual 3D environment RealSense SDK, Deep Learning and Game Engine",IEEE,Conferences,"We propose and demonstrate an approach for real world objects augmentation in a virtual 3D environment using deep neural networks and game engine. The proposed system consists of three basic parts: acquisition, processing and visualization with VR. The processing unit comprises translation of 2D to 3D images of real world objects and augmentation of the real world object in a virtual environment with unity. Through deep learning and RealSense SDK, skeleton data is achieved which helps augment the real world 3D object into a virtual environment in unity. The proposed system enables real-time augmentation of the real world objects in a virtual 3D environment created in unity game engine. Therefore, the system allows users developing mixed reality applications for real life problem solving.",https://ieeexplore.ieee.org/document/9718388/,2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE),8-10 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NAECON46414.2019.9057988,Real-Time 3-D Segmentation on An Autonomous Embedded System: using Point Cloud and Camera,IEEE,Conferences,"Present day autonomous vehicle relies on several sensor technologies for it's autonomous functionality. The sensors based on their type and mounted-location on the vehicle, can be categorized as: line of sight and non-line of sight sensors and are responsible for the different level of autonomy. These line of sight sensors are used for the execution of actions related to localization, object detection and the complete environment understanding. The surrounding or environment understanding for an autonomous vehicle can be achieved by segmentation. Several traditional and deep learning related techniques providing semantic segmentation for an input from camera is already available, however with the advancement in the computing processor, the progression is on developing the deep learning application replacing traditional methods. This paper presents an approach to combine the input of camera and lidar for semantic segmentation purpose. The proposed model for outdoor scene segmentation is based on the frustum pointnet, and ResNet which utilizes the 3d point cloud and camera input for the 3d bounding box prediction across the moving and non-moving object and thus finally recognizing and understanding the scenario at the point-cloud or pixel level. For real time application the model is deployed on the RTMaps framework with Bluebox (an embedded platform for autonomous vehicle). The proposed architecture is trained with the CITYScpaes and the KITTI dataset.",https://ieeexplore.ieee.org/document/9057988/,2019 IEEE National Aerospace and Electronics Conference (NAECON),15-19 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWCMC55113.2022.9825089,Real-Time Application for Recognition and Visualization of Arabic Words with Vowels based DL and AR,IEEE,Conferences,"Text is difficult to read in some cases due to text orientation, writing style, very light colors, etc. Visually impaired or visually impaired people have difficulty reading text in all of these situations. The architecture proposed in this work is intended to detect and identify Arabic characters with vowels in natural environments. This architecture can help visually impaired or blind people to read the text correctly. It allows users to read the text in a better and more immersive way by combining augmented reality with digital material. The approach uses both deep learning and more specifically the VGG 19 model and augmented reality to improve the efficiency, clarity, and accuracy of text reading. For text detection and identification we use the VGG 19 model, and for text visualization, we use augmented reality. The implementation technique presented in this research for an augmented reality interactive virtual assistant system is for users to use their smartphone&#x0027;s camera to receive enhanced text information via a text image and a three-dimensional image to understand the displayed text. It offers an interesting way to understand their environment. The use of augmented reality to better display recognized text in 3D is a fantastic feature. User research studies are conducted to assess usability and user satisfaction.",https://ieeexplore.ieee.org/document/9825089/,2022 International Wireless Communications and Mobile Computing (IWCMC),30 May-3 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICComm.2018.8484777,Real-Time Autonomous System of Navigation Using a Stereoscopic Camera,IEEE,Conferences,"Artificial intelligence is in a continuous development and automation of vehicles is a discussed subject. The navigation of any vehicle is based on obstacle avoidance, but it still challenges the field of automation because of the unpredictable environment. Usually, an algorithm for obstacle avoidance is based on several types of sensors. This article will present an evaluation of two algorithms, one using the mean estimation method and one using the threshold estimation method, both based on only one imaging sensor, which simplify the complexity of the system. The algorithms are based on 3D reconstruction of the environment and making the disparity map which is a matrix that contains the depth information of each pixel from an image. Both algorithms have been implemented in C++, using as device DUO MLX stereo camera on an embedded system with ARM processing unit (DUO VPC).",https://ieeexplore.ieee.org/document/8484777/,2018 International Conference on Communications (COMM),14-16 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISAI54367.2021.00190,Real-Time Facial Expression Driving based on 3D Facial Feature Point,IEEE,Conferences,"Facial expression driving can make the expression of virtual characters more real and natural. It is widely used in movies, games and some social software. The existing real-time facial expression driving algorithms have the limitation of physical hardware, prefabricated model or long-time training. In order to get rid of the limitation of existing algorithms, this paper proposes a real-time expression driving algorithm based on 3D facial feature points with RGBD data as input. In the face capture, we use ICP algorithm to get the rigid data of the face and deformation transmission algorithm to capture the non-rigid data of the face. Moreover, the whole face process only takes 0.4ms to complete. Because the calculation of the algorithm is based on 3D feature points of the face, there is no need to prefabricate a specific face model and a lot of time training. Our algorithm can not only use real faces to drive virtual faces, but also use virtual characters to drive virtual faces.",https://ieeexplore.ieee.org/document/9718961/,2021 International Conference on Computer Information Science and Artificial Intelligence (CISAI),17-19 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794220,Real-Time Joint Semantic Segmentation and Depth Estimation Using Asymmetric Annotations,IEEE,Conferences,"Deployment of deep learning models in robotics as sensory information extractors can be a daunting task to handle, even using generic GPU cards. Here, we address three of its most prominent hurdles, namely, i) the adaptation of a single model to perform multiple tasks at once (in this work, we consider depth estimation and semantic segmentation crucial for acquiring geometric and semantic understanding of the scene), while ii) doing it in real-time, and iii) using asymmetric datasets with uneven numbers of annotations per each modality. To overcome the first two issues, we adapt a recently proposed real-time semantic segmentation network, making changes to further reduce the number of floating point operations. To approach the third issue, we embrace a simple solution based on hard knowledge distillation under the assumption of having access to a powerful `teacher' network. We showcase how our system can be easily extended to handle more tasks, and more datasets, all at once, performing depth estimation and segmentation both indoors and outdoors with a single model. Quantitatively, we achieve results equivalent to (or better than) current state-of-the-art approaches with one forward pass costing just 13ms and 6.5 GFLOPs on 640×480 inputs. This efficiency allows us to directly incorporate the raw predictions of our network into the SemanticFusion framework [1] for dense 3D semantic reconstruction of the scene.",https://ieeexplore.ieee.org/document/8794220/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2018.8351244,Real-Time Road Segmentation Using LiDAR Data Processing on an FPGA,IEEE,Conferences,"This paper presents the FPGA design of a convolutional neural network (CNN) based road segmentation algorithm for real-time processing of LiDAR data. For autonomous vehicles, it is important to perform road segmentation and obstacle detection such that the drivable region can be identified for path planning. Traditional road segmentation algorithms are mainly based on image data from cameras, which is subjected to the light condition as well as the quality of lane markings. LiDAR sensor can obtain the precise 3D geometry information of the vehicle surroundings. However, it is a computational challenge to process a large amount of LiDAR data at real-time. In this work, a convolutional neural network model is proposed and trained to perform semantic segmentation using the LiDAR sensor data. Furthermore, an efficient hardware design is implemented on the FPGA that can process each LiDAR scan in 16.9 ms, which is much faster than the previous works. Evaluated using KITTI road benchmarks, the proposed solution achieves high accuracy of road segmentation.",https://ieeexplore.ieee.org/document/8351244/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CHASE.2016.72,Real-Time Tidal Volume Estimation Using Iso-surface Reconstruction,IEEE,Conferences,"Breathing volume measurement has long been an important physiological indication widely used for the diagnosis and treatment of pulmonary diseases. However, most of existing breathing volume monitoring techniques require either physical contact with the patient or are prohibitively expensive. In this paper we present an automated and inexpensive non-contact, vision-based method for monitoring an individual's tidal volume, which is extracted from a three-dimensional (3D) chest surface reconstruction from a single depth camera. In particular, formulating the respiration monitoring process as a 3D space-time volumetric representation, we introduce a real-time surface reconstruction algorithm to generate omni-direction deformation states of a patient's chest while breathing, which reflects the change in tidal volume over time. These deformation states are then used to estimate breathing volume through a per-patient correlation metric acquired through a Bayesian-network learning process. Through prototyping and implementation, our results indicate that we have achieved 92.2% to 94.19% accuracy in the tidal volume estimations through the experimentation based on the proposed vision-based method.",https://ieeexplore.ieee.org/document/7545835/,"2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)",27-29 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICARM52023.2021.9536118,Real-time Monocular 3D People Localization and Tracking on Embedded System,IEEE,Conferences,"Localizing people in 3D space, rather than in original 2D image plane, provides a more comprehensive understanding of the scene and brings up more potential applications. However, inferring 3D locations usually requires stereo camera or additional sensors since deriving depth information from single image is regarded as an ill-posed problem. With recent progress in deep learning methods, depth estimation neural network can provide convincing depth map by a single RGB image. This work develops a people localization and tracking method based on a monocular camera. Specifically, an efficient self-supervised monocular depth estimation method is adopted to generate pseudo depth map. Afterwards, 2D object detection results are adopted for finding accurate people location. Finally, a filter based tracking method is adopted to fuse temporal information and improve the accuracy. Aiming to provide a real time solution for people tracking on embedded system, our methods are deployed and tested on a NVIDIA Jetson Xavier NX develop kit. The proposed efficient localization and tracking method is validated by a group of field tests. The overall performance reaches 12 fps with an acceptable accuracy compared to ground truth.",https://ieeexplore.ieee.org/document/9536118/,2021 6th IEEE International Conference on Advanced Robotics and Mechatronics (ICARM),3-5 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAR46387.2019.8981549,Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models,IEEE,Conferences,"This paper presents methods for performing realtime semantic SLAM aimed at autonomous navigation and control of a humanoid robot in a manufacturing scenario. A novel multi-keyframe approach is proposed that simultaneously minimizes a semantic cost based on class-level features in addition to common photometric and geometric costs. The approach is shown to robustly construct a 3D map with associated class labels relevant to robotic tasks. Alternatively to existing approaches, the segmentation of these semantic classes have been learnt using RGB-D sensor data aligned with an industrial CAD manufacturing model to obtain noisy pixel-wise labels. This dataset confronts the proposed approach in a complicated real-world setting and provides insight into the practical use case scenarios. The semantic segmentation network was fine tuned for the given use case and was trained in a semi-supervised manner using noisy labels. The developed software is real-time and integrated with ROS to obtain a complete semantic reconstruction for the control and navigation of the HRP4 robot. Experiments in-situ at the Airbus manufacturing site in Saint-Nazaire validate the proposed approach.",https://ieeexplore.ieee.org/document/8981549/,2019 19th International Conference on Advanced Robotics (ICAR),2-6 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEA.2016.7603570,Real-time facial expression recognition for affective computing based on Kinect,IEEE,Conferences,"Affective computing endows computers with the ability to observe and understand, thus generates a variety of emotional characteristics. Facial expression has continuous signal which is direct expression of emotion. Affective computing via facial expressions has aroused great interest in intelligent interactive and pattern recognition recently. RGB camera is sensitive to surroundings, especially to illumination conditions. 2D images are not robust to human faces which are 3D objects. In this paper, we propose a method that uses noisy depth data produced by the low resolution sensor for robust face recognition. We capture color and depth information by Kinect sensor, extract Facial Feature Points (FFPs) feature vector by Face Tracking SDK, and recognize facial expression by Random Forest (RF) algorithm. This method enables facial expression recognition implementation real-time in intelligent interactive.",https://ieeexplore.ieee.org/document/7603570/,2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA),5-7 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1996.549131,Real-time image restoration with an artificial neural network,IEEE,Conferences,"We present a neural network that can be applied to image correction in a preprocessing unit. Blur, geometric distortion and unequal brightness distribution are typical for many scanning techniques and can lead to difficulties during further processing of an image. These and other effects of image degradation, the space-variant can be considered simultaneously by this approach. In order to calibrate the correcting system the weights of a neural network are trained. Using suitable training patterns and an appropriate optimization criterion for the degraded images, the dimensioned network represents a space-variant filter with a behavior similar to the well-known Wiener filter. The restoration result can be easily altered by the scheme of the learning data generation. Theoretical considerations and examples for 1D, 2D and 3D implementations in both software and hardware are given.",https://ieeexplore.ieee.org/document/549131/,Proceedings of International Conference on Neural Networks (ICNN'96),3-6 June 1996,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VR.2001.913773,Real-time input of 3D pose and gestures of a user's hand and its applications for HCI,IEEE,Conferences,"Introduces a method for tracking a user's hand in 3D and recognizing the hand's gesture in real time without the use of any invasive devices attached to the hand. Our method uses multiple cameras for determining the position and orientation of a user's hand moving freely in a 3D space. In addition, the method identifies pre-determined gestures in a fast and robust manner by using a neural network which has been properly trained beforehand. This paper also describes results of user study of our proposed method and several types of applications, including 3D object handling for a desktop system and a 3D walkthrough for a large immersive display system.",https://ieeexplore.ieee.org/document/913773/,Proceedings IEEE Virtual Reality 2001,13-17 March 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2008.4650431,Real-time intensity-based rigid 2d-3d medical image registration using RapidMind Multi-core Development Platform,IEEE,Conferences,"In this paper, we present an efficient intensity-based rigid 2D-3D image registration method. We implement the algorithm using the RapidMind Multi-core Development Platform1 to exploit the highly parallel multi-core architecture of graphics processing units (GPUs). We use a ray casting algorithm to generate the digitally reconstructed radiographs (DRRs) on GPUs and efficiently reduce the complexity of DRR construction. The registration optimization problem is solved by the Gauss-Newton method. To fully exploit the multi-core parallelism, we implement almost the entire registration process in parallel by RapidMind. We also discuss the RapidMind implementation of the major computation steps. Numerical results are presented to demonstrate the efficiency of our method.",https://ieeexplore.ieee.org/document/4650431/,2008 30th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,20-25 Aug. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICUAS.2016.7502588,Real-time unmanned aerial vehicle 3D environment exploration in a mixed reality environment,IEEE,Conferences,"This paper presents a novel human robot interaction system that can be used for real-time 3D environment exploration with an unmanned aerial vehicle (UAV). The method creates a mixed reality environment, in which a user can interactively control a UAV and visualize the exploration data in real-time. The method uses a combination of affordable sensors, and transforms the control and viewing space from the UAV to the controller's perspective. Different hardware and software configurations are studied so that the system can be adjusted to meet different needs and environments. A prototype system is presented and test results are discussed.",https://ieeexplore.ieee.org/document/7502588/,2016 International Conference on Unmanned Aircraft Systems (ICUAS),7-10 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMCCE51767.2020.00092,Real-time virtual UR5 robot imitation of human motion based on 3D camera,IEEE,Conferences,"Human robot interaction is playing significant roles in many industries, in which humans are allowed to corporate with robots or control robots to complete some tasks. Achieving realtime control of robots by demonstrating human arm motion is one of the effective methods to improve the quality of human robot interaction. In this paper, a new method to accomplish human skeleton key points extraction is proposed and a system for controlling a virtual UR5 robot (which is modeling in Unity 3D) is developed to imitate human motions. A sequential architecture composed of convolutional networks is used to identify the 2D coordinates; the depth information from Kinect v2 sensor is used to obtain the actual 3D coordinates and the result of the proposed method is used for joint angles calculation. The appropriate mapping relationships between human arm and UR5 robots are built by transforming the arm angel to UR5's degrees of freedom. The performance of this method is measured by the degree of real-time simulation of human hand trajectory. Further, the comparison between the method in this paper and other method is implemented. The experimental results indicate that the developed system and proposed method are reliable and efficient to accomplish human motion imitation.",https://ieeexplore.ieee.org/document/9421649/,"2020 5th International Conference on Mechanical, Control and Computer Engineering (ICMCCE)",25-27 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.1996.568818,Realistic 3D mouth animation using a minimal number of parameters,IEEE,Conferences,"Our work on real-time animation enhances the realism of computer-generated faces by improving mouth modeling and allowing protrusion: 3 points on the corner of the lips are detected and the mouth model is completely controlled by the horizontal height and vertical width (h, w) of the mouth. Our approach parametrically models lip motion and mouth protrusion using parameters (h, w) and a textured realistic 3D face model. The basic idea for our approach, given parameters (h, w) is to compute the opening angle at the corner of the mouth as well as the protrusion coefficients using a radial basis function neural network, and to obtain the lip shape using a piecewise spline interpolation. Once the accurate lip shape is computed, the motion is propagated to outer contours of the mouth area based on heuristics of mouth motion and assuming a symmetry of the face. The CG rendering and implementation of our mouth animation algorithm achieves interesting results in enhancing visual realism with minimal computational expense by more accurately modeling the lip shape.",https://ieeexplore.ieee.org/document/568818/,Proceedings 5th IEEE International Workshop on Robot and Human Communication. RO-MAN'96 TSUKUBA,11-14 Nov. 1996,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2019.8917330,Realtime 3D Object Detection for Automated Driving Using Stereo Vision and Semantic Information,IEEE,Conferences,"We propose a 3D object detection and pose estimation method for automated driving using stereo images. In contrast to existing stereo-based approaches, we focus not only on cars, but on all types of road users and can ensure real-time capability through GPU implementation of the entire processing chain. These are essential conditions to exploit an algorithm for highly automated driving. Semantic information is provided by a deep convolutional neural network and used together with disparity and geometric constraints to recover accurate 3D bounding boxes. Experiments on the challenging KITTI 3D object detection benchmark show results that are within the range of the best image-based algorithms, while the runtime is only about a fifth. This makes our algorithm the first real-time image-based approach on KITTI.",https://ieeexplore.ieee.org/document/8917330/,2019 IEEE Intelligent Transportation Systems Conference (ITSC),27-30 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEIB53692.2021.9686433,Recognition and Alarm System with Deep Learning for Correctly Wearing a Face Mask,IEEE,Conferences,"We have designed and implemented a detection and alarm contact tracing mask recognition access control system to wear a face mask correctly to improve Taiwan CDC 1922 SMS contact tracing system, which is difficult to be indeed executed on university campuses and lacks efficiency. Our system involved deep learning, the Internet of Things, 3D printing technologies to overcome the shortcomings of the current real link system. We optimized the deep learning model recognition accuracy and solved the imbalance problem of the dataset on this task. Thus, the model recognition achieved 88.89% in mAP@0.50 and 75.02% in Avg. IOU.",https://ieeexplore.ieee.org/document/9686433/,"2021 International Conference on Electronic Communications, Internet of Things and Big Data (ICEIB)",10-12 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AFGR.2004.1301594,Recognition of isolated complex mono- and bi-manual 3D hand gestures,IEEE,Conferences,"In this paper, we address the problem of the recognition of isolated complex mono- and bi-manual hand gestures. In the proposed system, hand gestures are represented by the 3D trajectories of blobs. The blobs are obtained by tracking colored body parts in real-time using the EM algorithm. In most of the studies on hand gestures, only small vocabularies have been used. In this paper, we study the results obtained on a more complex database of mono- and bi-manual gestures. These results are obtained by using a state-of-the-art sequence processing algorithm, namely hidden Markov models (HMMs), implemented within the framework of an open source machine learning library.",https://ieeexplore.ieee.org/document/1301594/,"Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.",19-19 May 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2019.8803189,Recognizing Chinese Texts with 3D Convolutional Neural Network,IEEE,Conferences,"In this paper, we propose a deep learning system to localize and recognize Chinese texts in scenes with signage and road marks through 3D convolutional neural network. The proposed system adopts YOLO for detecting target location and exploits 3D convolutional neural network for recognizing the contents. The proposed design outperforms the existing designs based on LSTM and achieves real-time processing performance, which is feasible to be implemented on embedded platforms. The proposed system reaches over 90% accuracy in recognizing Chinese texts on bird's-eye viewing road marks in a self-driving vehicle equipped with a fisheye camera. In addition, this system can achieve 20 fps execution speed with NVIDIA DIGITS DevBox with 1080Ti GPU, which is fast enough for autonomous driving applications.",https://ieeexplore.ieee.org/document/8803189/,2019 IEEE International Conference on Image Processing (ICIP),22-25 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPL.2018.00056,Reconfigurable Acceleration of 3D-CNNs for Human Action Recognition with Block Floating-Point Representation,IEEE,Conferences,"Human action recognition (HAR) has been widely employed in various applications such as autonomous cars and intelligent video surveillance. Among the algorithms proposed for HAR, the 3D-CNNs algorithm can achieve the best accuracy. However, its algorithmic complexity imposes a substantial overhead over the speed of these networks, which limits their deployment in real-life applications. This paper proposes a novel customizable architecture for 3D-CNNs based on block floating-point (BFP) arithmetic, where the utilization of BFP significantly reduces the bitwidth while eliminating the need to retrain the network. Optimizations such as locality exploration and block alignment with 3D blocking are performed to improve performance and accuracy. An analytical model and tool are developed to predict the optimized parameters for hardware customization based on user constraints such as FPGA resources or accuracy requirement. Experiments show that without retraining, a 15-bit mantissa design using single-precision accumulation on a Xilinx ZC706 device can be 8.2 times faster than an Intel i7-950 processor at 3.07 GHz with only 0.4% accuracy loss.",https://ieeexplore.ieee.org/document/8533510/,2018 28th International Conference on Field Programmable Logic and Applications (FPL),27-31 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.1994.413671,Reconstruction of visual surfaces from sparse data using parametric triangular approximants,IEEE,Conferences,"This paper presents the application of a recently proposed geometrical modelling technique to computer vision in order to reconstruct smooth surfaces from arbitrary triangulations of scattered 3D points. These points are considered to be noisy as a result of a sensory acquisition process. The reconstruction problem is transformed into one of surface approximation over arbitrary triangular meshes. The reconstructed surface is composed of a collection of triangular patches that join with C/sup 0/ or G/sup 1/ geometric continuity and that can be computed independently. Since these patches are parametric functionals, arbitrary topologies of any genus can be represented. This method is efficient and easily parallelizable. Affine invariance, locality and other valuable properties are analysed and some examples are finally shown to illustrate the behaviour of this technique in reconstructing real complex objects.<>",https://ieeexplore.ieee.org/document/413671/,Proceedings of 1st International Conference on Image Processing,13-16 Nov. 1994,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2017.8022704,Recovering camera motion from points and lines in stereo images: A recursive model-less approach using trifocal tensors,IEEE,Conferences,"Estimating the 3-D motion of a moving camera from images is a common task in robotics and augmented reality. Most existing marker-less approaches make use of either points or lines. Taking the advantages of both kinds of features in an unknown environment is more attractive due to their availability and differences in characteristics. A novel model-less method is presented in this paper to tackle the 3-D motion tracking problem. Two Bayesian filters, one for point measurements while another for line measurements, are embedded in the Interacting Probabilistic Switching (IPS) framework. They compensate for the weaknesses in one another by utilizing both kinds of features in the stereo images. The proposed method is able to obtain the 3D motion given as little as two line or two point correspondences in consecutive images with the use of multiple trifocal tensors. Our method outperformed two recent methods in terms of accuracy and the problem of drifting was very little in real scenarios.",https://ieeexplore.ieee.org/document/8022704/,"2017 18th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",26-28 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FG47880.2020.00049,ReenactNet: Real-time Full Head Reenactment,IEEE,Conferences,"Video-to-video synthesis is a challenging problem aiming at learning a translation function between a sequence of semantic maps and a photo-realistic video depicting the characteristics of a driving video. We propose a head-to-head system of our own implementation capable of fully transferring the human head 3D pose, facial expressions and eye gaze from a source to a target actor, while preserving the identity of the target actor. Our system produces high-fidelity, temporally-smooth and photo-realistic synthetic videos faithfully transferring the human time-varying head attributes from the source to the target actor. Our proposed implementation: 1) works in real time (~20 fps), 2) runs on a commodity laptop with a webcam as the only input, 3) is interactive, allowing the participant to drive a target person, e.g. a celebrity, politician, etc, instantly by varying their expressions, head pose, and eye gaze, and visualising the synthesised video concurrently.",https://ieeexplore.ieee.org/document/9320178/,2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020),16-20 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC49870.2020.9289086,Reinforcement Learning based Real Time Aerial BS Positioning for Dense Urban 5G Mobile Network,IEEE,Conferences,"Due to the recent surge in mobile users and traffic, next-generation mobile communication aims to increase network capacity through large bandwidth and small cell deployment. To respond to this situation, there have been lots of researches on aerial base stations (BSs) using unmanned aerial vehicles (UAVs) for the mobile user. Aerial BS has the advantage of providing flexible communication range by avoiding obstacles such as buildings in urban areas through 3D positioning. However, finding an optimal point for aerial BS considering the variance of user's requirements, movement, and obstacles in real time is difficult problem to solve. Therefore, it is necessary to find an approximate optimal point for applying to the real-time flight path control of aerial BS. This paper aims to find optimal behavior in real time interacting with a given environment, through reinforcement learning. We propose the algorithm based on Q-learning with a new concept called Coarse Search to reduce convergence speed. We evaluate the performance of the algorithm by comparing it with that of other heuristic algorithms.",https://ieeexplore.ieee.org/document/9289086/,2020 International Conference on Information and Communication Technology Convergence (ICTC),21-23 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ITUK53220.2021.9662100,Reinforcement Learning for Scheduling and Mimo beam Selection using Caviar Simulations,IEEE,Conferences,"This paper describes a framework for research on Reinforcement Learning (RL) applied to scheduling and MIMO beam selection. This framework consists of asking the RL agent to schedule a user and then choose the index of a beamforming codebook to serve it. A key aspect of this problem is that the simulation of the communication system and the artificial intelligence engine is based on a virtual world created with AirSim and the Unreal Engine. These components enable the so-called CAVIAR methodology, which leads to highly realistic 3D scenarios. This paper describes the communication and RL modeling adopted in the framework and also presents statistics concerning the implemented RL environment, such as data traffic, as well as results for three baseline systems.",https://ieeexplore.ieee.org/document/9662100/,2021 ITU Kaleidoscope: Connecting Physical and Virtual Worlds (ITU K),6-10 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2018.8500494,Rendering Physically Correct Raindrops on Windshields for Robustness Verification of Camera-based Object Recognition,IEEE,Conferences,"Recent developments in the field of autonomous cars indicate the appearance of those vehicles on the streets of every city in the near future. This urban driving requires zero error tolerance. In order to guarantee safety requirements self-driving cars and the used software have to pass exhaustive tests under as many different conditions as possible. The more versatile the considered influences and the more thorough the tests made under those influences, the safer the car will drive under real conditions. Unfortunately, it is very time and resource intensive to record the same test set of images over and over again, every time producing, or hoping for, specific conditions; especially when using real test vehicles. This is where environment simulation comes into play. This research investigates the simulation of environmental influences which may affect the sensors used in autonomous vehicles, in particular how raindrops resting on a windshield affect cameras as they may occlude large parts of the field of view. We propose a novel method to render these raindrops using Continuous Nearest Neighbor search leveraging the benefits of R-trees. The 3D scene in front of the camera, which is generated from stereo images, reflects physically correct in these drops. This leads to near photo-realistic simulated results. The derived images may be used to extend the training data sets used for machine learning without being forced to capture new real pictures.",https://ieeexplore.ieee.org/document/8500494/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00797,RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation,IEEE,Conferences,"This paper addresses the problem of 3D human pose estimation from single images. While for a long time human skeletons were parameterized and fitted to the observation by satisfying a reprojection error, nowadays researchers directly use neural networks to infer the 3D pose from the observations. However, most of these approaches ignore the fact that a reprojection constraint has to be satisfied and are sensitive to overfitting. We tackle the overfitting problem by ignoring 2D to 3D correspondences. This efficiently avoids a simple memorization of the training data and allows for a weakly supervised training. One part of the proposed reprojection network (RepNet) learns a mapping from a distribution of 2D poses to a distribution of 3D poses using an adversarial training approach. Another part of the network estimates the camera. This allows for the definition of a network layer that performs the reprojection of the estimated 3D pose back to 2D which results in a reprojection loss function. Our experiments show that RepNet generalizes well to unknown data and outperforms state-of-the-art methods when applied to unseen data. Moreover, our implementation runs in real-time on a standard desktop PC.",https://ieeexplore.ieee.org/document/8953653/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS52187.2021.9522338,Research and Application of Intelligent Route Planning Method based on 3D Model,IEEE,Conferences,"After the emergency rescue personnel enter the site of a major natural disaster or work safety accident, the site environment will change with the development of the disaster, and the rescue or evacuation route needs to be replanned in time according to the real-time situation. In view of this kind of situation, this paper combines cost distance with greedy algorithm, and uses artificial intelligence technology to identify and analyze buildings, roads, vehicles, pedestrians, trees and other elements in the region, and proposes an intelligent route planning method based on 3D model. This method can provide an effective and feasible intelligent route planning scheme for the rescue and decision commanders, greatly improve the efficiency of the rescue work in the path selection, and further ensure the safety of the rescue workers in the process of moving or evacuation.",https://ieeexplore.ieee.org/document/9522338/,2021 IEEE 12th International Conference on Software Engineering and Service Science (ICSESS),20-22 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSN.2016.7586606,Research of 3D face recognition algorithm based on deep learning stacked denoising autoencoder theory,IEEE,Conferences,"This electronic Due to the fact that the 3D face depth data have more information, the 3D face recognition is attracting more and more attention in the machine learning area. Firstly, this paper selects 30 feature points from the 113 feature points of Candide-3 face model to characterize face, which improves the efficiency of recognition algorithm obviously without affecting the recognition accuracy. With the significant advantage of the characterization of essential features by learning a deep nonlinear network, this paper presents a stacked denoising autoencoder algorithm model based on deep learning which improves neural networks model. This algorithm conducts the unsupervised preliminary training of face depth data and the supervised training to fine-tuning the network which is better than neural network's random initialization. The experiment indicates that compared with real face data, the reconstruction face model has a small matching error by using SDAE algorithm and it achieves an excellent face recognition effect.",https://ieeexplore.ieee.org/document/7586606/,2016 8th IEEE International Conference on Communication Software and Networks (ICCSN),4-6 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.2007.4352483,Research on 3D Modeling for Head MRI Image Based on Immune Sphere-Shaped Support Vector Machine,IEEE,Conferences,"In head MRI image sequences, the boundary of each encephalic tissue is highly complicated and irregular. It is a real challenge to traditional 3D modeling algorithms. Support Vector Machine (SVM) based on statistical learning theory has solid theoretical foundation. Sphere-Shaped SVM (SSSVM) was originally developed for solving some special classification problems. In this paper, it is extended to image 3D modeling which tries to find the smallest hypersphere enclosing target data in high dimensional space by kernel function. However, selecting parameter is a complicated problem which directly affects modeling accuracy. Immune Algorithm (IA), mainly applied to optimization, is used to search optimal parameter for SSSVM. So, Immune SSSVM (ISSSVM) is proposed to construct the 3D models for encephalic tissues. As our experiment demonstrates, the models are constructed and reach satisfactory modeling accuracies. Theory and experiment indicate ISSSVM exhibits its great potential in image 3D modeling.",https://ieeexplore.ieee.org/document/4352483/,2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society,22-26 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3DV53792.2021.00094,Residual Geometric Feature Transform Network for 3D Surface Super-Resolution,IEEE,Conferences,"In 3D reconstruction, how to recover high-resolution 3D surface details from the existing low-resolution 3D surface is still a challenging problem. Due to the unstructured and irregular characteristics of 3D data, it is usually difficult to obtain extremely dense 3D surface and capture detailed local features. To tackle this problem, this article introduces an effective deep convolutional network, namely RGFTNet, to perform 3D surface super-resolution in 2D normal domain. To restore dense surface details and learn sharp geometry structures simultaneously, a shape prior acquisition method is designed to achieve the high-quality shape normal from the input low-resolution one. Subsequently, the extracted shape normal as the shape prior is incorporated into a deep convolutional network through the Geometric Feature Transform (GFT) layer. Experimental results show the superiority of the proposed RGFTNet over several recent advances on both the computer-generated and the real-world data.",https://ieeexplore.ieee.org/document/9665935/,2021 International Conference on 3D Vision (3DV),1-3 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560893,Robot Navigation in Constrained Pedestrian Environments using Reinforcement Learning,IEEE,Conferences,"Navigating fluently around pedestrians is a necessary capability for mobile robots deployed in human environments, such as buildings and homes. While research on social navigation has focused mainly on the scalability with the number of pedestrians in open spaces, typical indoor environments present the additional challenge of constrained spaces such as corridors and doorways that limit maneuverability and influence patterns of pedestrian interaction. We present an approach based on reinforcement learning (RL) to learn policies capable of dynamic adaptation to the presence of moving pedestrians while navigating between desired locations in constrained environments. The policy network receives guidance from a motion planner that provides waypoints to follow a globally planned trajectory, whereas RL handles the local interactions. We explore a compositional principle for multi-layout training and find that policies trained in a small set of geometrically simple layouts successfully generalize to more complex unseen layouts that exhibit composition of the structural elements available during training. Going beyond walls-world like domains, we show transfer of the learned policy to unseen 3D reconstructions of two real environments. These results support the applicability of the compositional principle to navigation in real-world buildings and indicate promising usage of multi-agent simulation within reconstructed environments for tasks that involve interaction. https://ai.stanford.edu/∼cdarpino/socialnavconstrained/",https://ieeexplore.ieee.org/document/9560893/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636467,Robust Feedback Motion Policy Design Using Reinforcement Learning on a 3D Digit Bipedal Robot,IEEE,Conferences,"In this paper, a hierarchical and robust framework for learning bipedal locomotion is presented and successfully implemented on the 3D biped robot Digit built by Agility Robotics. We propose a cascade-structure controller that combines the learning process with intuitive feedback regulations. This design allows the framework to realize robust and stable walking with a reduced-dimensional state and action spaces of the policy, significantly simplifying the design and increasing the sampling efficiency of the learning method. The inclusion of feedback regulation into the framework improves the robustness of the learned walking gait and ensures the success of the sim-to-real transfer of the proposed controller with minimal tuning. We specifically present a learning pipeline that considers hardware-feasible initial poses of the robot within the learning process to ensure the initial state of the learning is replicated as close as possible to the initial state of the robot in hardware experiments. Finally, we demonstrate the feasibility of our method by successfully transferring the learned policy in simulation to the Digit robot hardware, realizing sustained walking gaits under external force disturbances and challenging terrains not incurred during the training process. To the best of our knowledge, this is the first time a learning-based policy is transferred successfully to the Digit robot in hardware experiments.",https://ieeexplore.ieee.org/document/9636467/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00844,Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments,IEEE,Conferences,"Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.",https://ieeexplore.ieee.org/document/9577932/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9562105,Robust Place Recognition using an Imaging Lidar,IEEE,Conferences,"We propose a methodology for robust, real-time place recognition using an imaging lidar, which yields image-quality high-resolution 3D point clouds. Utilizing the intensity readings of an imaging lidar, we project the point cloud and obtain an intensity image. ORB feature descriptors are extracted from the image and encoded into a bag-of-words vector. The vector, used to identify the point cloud, is inserted into a database that is maintained by DBoW for fast place recognition queries. The returned candidate is further validated by matching visual feature descriptors. To reject matching outliers, we apply PnP, which minimizes the reprojection error of visual features’ positions in Euclidean space with their correspondences in 2D image space, using RANSAC. Combining the advantages from both camera and lidar-based place recognition approaches, our method is truly rotation-invariant, and can tackle reverse revisiting and upside down revisiting. The proposed method is evaluated on datasets gathered from a variety of platforms over different scales and environments. Our implementation and datasets are available at https://git.io/image-lidar.",https://ieeexplore.ieee.org/document/9562105/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.01204,Robustness of 3D Deep Learning in an Adversarial Setting,IEEE,Conferences,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space.",https://ieeexplore.ieee.org/document/8953599/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412909,SAILenv: Learning in Virtual Visual Environments Made Simple,IEEE,Conferences,"Recently, researchers in Machine Learning algorithms, Computer Vision scientists, engineers and others, showed a growing interest in 3D simulators as a mean to artificially create experimental settings that are very close to those in the real world. However, most of the existing platforms to interface algorithms with 3D environments are often designed to setup navigation-related experiments, to study physical interactions, or to handle ad-hoc cases that are not thought to be customized, sometimes lacking a strong photorealistic appearance and an easy-to-use software interface. In this paper, we present a novel platform, SAILenv, that is specifically designed to be simple and customizable, and that allows researchers to experiment visual recognition in virtual 3D scenes. A few lines of code are needed to interface every algorithm with the virtual world, and non-3D-graphics experts can easily customize the 3D environment itself, exploiting a collection of photorealistic objects. Our framework yields pixel-level semantic and instance labeling, depth, and, to the best of our knowledge, it is the only one that provides motion-related information directly inherited from the 3D engine. The client-server communication operates at a low level, avoiding the overhead of HTTP-based data exchanges. We perform experiments using a state-of-the-art object detector trained on real-world images, showing that it is able to recognize the photorealistic 3D objects of our environment. The computational burden of the optical flow compares favourably with the estimation performed using modern GPU-based convolutional networks or more classic implementations. We believe that the scientific community will benefit from the easiness and high-quality of our framework to evaluate newly proposed algorithms in their own customized realistic conditions.",https://ieeexplore.ieee.org/document/9412909/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.01163,SegGCN: Efficient 3D Point Cloud Segmentation With Fuzzy Spherical Kernel,IEEE,Conferences,"Fuzzy clustering is known to perform well in real-world applications. Inspired by this observation, we incorporate a fuzzy mechanism into discrete convolutional kernels for 3D point clouds as our first major contribution. The proposed fuzzy kernel is defined over a spherical volume that uses discrete bins. Discrete volumetric division can normally make a kernel vulnerable to boundary effects during learning as well as point density during inference. However, the proposed kernel remains robust to boundary conditions and point density due to the fuzzy mechanism. Our second major contribution comes as the proposal of an efficient graph convolutional network, SegGCN for segmenting point clouds. The proposed network exploits ResNet like blocks in the encoder and 1 × 1 convolutions in the decoder. SegGCN capitalizes on the separable convolution operation of the proposed fuzzy kernel for efficiency. We establish the effectiveness of the SegGCN with the proposed kernel on the challenging S3DIS and ScanNet real-world datasets. Our experiments demonstrate that the proposed network can segment over one million points per second with highly competitive performance.",https://ieeexplore.ieee.org/document/9157177/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8793744,Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data,IEEE,Conferences,"The ability to segment unknown objects in depth images has potential to enhance robot skills in grasping and object tracking. Recent computer vision research has demonstrated that Mask R-CNN can be trained to segment specific categories of objects in RGB images when massive hand-labeled datasets are available. As generating these datasets is time-consuming, we instead train with synthetic depth images. Many robots now use depth sensors, and recent results suggest training on synthetic depth data can transfer successfully to the real world. We present a method for automated dataset generation and rapidly generate a synthetic training dataset of 50,000 depth images and 320,000 object masks using simulated heaps of 3D CAD models. We train a variant of Mask R-CNN with domain randomization on the generated dataset to perform category-agnostic instance segmentation without any hand-labeled data and we evaluate the trained network, which we refer to as Synthetic Depth (SD) Mask R-CNN, on a set of real, high-resolution depth images of challenging, densely-cluttered bins containing objects with highly-varied geometry. SD Mask R-CNN outperforms point cloud clustering baselines by an absolute 15% in Average Precision and 20% in Average Recall on COCO benchmarks, and achieves performance levels similar to a Mask R-CNN trained on a massive, hand-labeled RGB dataset and fine-tuned on real images from the experimental setup. We deploy the model in an instance-specific grasping pipeline to demonstrate its usefulness in a robotics application. Code, the synthetic training dataset, and supplementary material are available at https://bit.ly/2letCuE.",https://ieeexplore.ieee.org/document/8793744/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00016,Self-Organizing Maps for Intuitive Gesture-Based Geometric Modelling in Augmented Reality,IEEE,Conferences,"Modelling three-dimensional virtual objects in the context of architectural, product and game design requires elaborate skill in handling the respective CAD software and is often tedious. We explore the potentials of Kohonen networks, also called self-organizing maps (SOM) as a concept for intuitive 3D modelling aided through mixed reality. We effectively provide a computational ""clay"" that can be pulled, pushed and shaped by picking and placing control objects with an augmented reality headset. Our approach benefits from combining state of the art CAD software with GPU computation and mixed reality hardware as well as the introduction of custom SOM network topologies and arbitrary data dimensionality. The approach is demonstrated in three case studies.",https://ieeexplore.ieee.org/document/8613635/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct.2019.00-39,Setforge - Synthetic RGB-D Training Data Generation to Support CNN-Based Pose Estimation for Augmented Reality,IEEE,Conferences,"The objective of this contribution is to introduce setforge, a set of software tools for synthetic data generation for convolutional neural network (CNN) training. Our focus is on CNNs for 6-degree-of-freedom pose estimation using RGB-D data. To determine the pose of physical objects in 6-degree-of-freedom is an essential task for many augmented reality applications. The recent years have shown the advent of trainable methods such as CNNs and others. However, those approaches require training data. The tools, this paper introduces, allow one to generate training data from 3D models. They come with plenty of features for random data generation and augmentation, adapting colors, hue, and noise. We contribute these tools as open-source software available on Github. A prototype CNN demonstrates how one can utilize it. An augmented reality demo application also shows its real-time pose estimation performance.",https://ieeexplore.ieee.org/document/8951897/,2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),10-18 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2015.7280771,Si elegans: Hardware architecture and communications protocol,IEEE,Conferences,"The hardware layer of the Si elegans EU FP7 project is a massively parallel architecture designed to accurately emulate the C. elegans nematode in biological real-time. The C. elegans nematode is one of the simplest and well characterized Biological Nervous Systems (BNS) yet many questions related to basic functions such as movement and learning remain unanswered. The hardware layer includes a Hardware Neural Network (HNN) composed of 302 FPGAs (one per neuron), a Hardware Muscle Network (HMN) composed of 27 FPGAs (one per 5 muscles) and one Interface Manager FPGA, which is physically connected through 2 Local Area Networks (LANs) and through an innovative 3D optical connectome. Neuron structures (gap junctions and synapses) and muscles are modelled in the design environment of the software layer and their simulation data (spikes, variable values and parameters) generate data packets sent across the Local Area Networks (LAN). Furthermore, a software layer gives the user a set of design tools giving the required flexibility and high level hardware abstraction to design custom neuronal models. In this paper the authors present an overview of the hardware layer, connections infrastructure and communication protocol.",https://ieeexplore.ieee.org/document/7280771/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.01165,Sim-Real Joint Reinforcement Transfer for 3D Indoor Navigation,IEEE,Conferences,"There has been an increasing interest in 3D indoor navigation, where a robot in an environment moves to a target according to an instruction. To deploy a robot for navigation in the physical world, lots of training data is required to learn an effective policy. It is quite labour intensive to obtain sufficient real environment data for training robots while synthetic data is much easier to construct by render-ing. Though it is promising to utilize the synthetic environments to facilitate navigation training in the real world, real environment are heterogeneous from synthetic environment in two aspects. First, the visual representation of the two environments have significant variances. Second, the houseplans of these two environments are quite different. There-fore two types of information,i.e. visual representation and policy behavior, need to be adapted in the reinforce mentmodel. The learning procedure of visual representation and that of policy behavior are presumably reciprocal. We pro-pose to jointly adapt visual representation and policy behavior to leverage the mutual impacts of environment and policy. Specifically, our method employs an adversarial feature adaptation model for visual representation transfer anda policy mimic strategy for policy behavior imitation. Experiment shows that our method outperforms the baseline by 19.47% without any additional human annotations.",https://ieeexplore.ieee.org/document/8953924/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LARS-SBR-WRE48964.2019.00060,Sim-to-Real in Reinforcement Learning for Everyone,IEEE,Conferences,"In reinforcement learning (RL), it remains a challenge to have a robotic agent perform a task in the real world for which it was trained in simulation. In this paper, we present our work training a low-cost robotic arm in simulation to move towards a predefined target in space, represented by a red ball in an RGB image, and transferring the capability to the real arm. We exercised the entire end-to-end flow including the 3D modeling of the arm, training of a state-of-the-art RL policy in simulation with multiple actors in a distributed fashion, domain randomization in order to close the sim-to-real gap, and finally the execution of the trained model in the real robot. We also implemented a mechanism to edit the image captured from the camera before sending it to the model for inference, which allowed us to automate reward computation in the physical world. Our work highlights important challenges of training RL agents and moving them to the real world, validating important aspects shown by other works as well as detailing steps not explained by some of them (e.g. how to compute the reward in the real world). The conducted experiments show the improvements observed as the techniques were added to the final solution.",https://ieeexplore.ieee.org/document/9018558/,"2019 Latin American Robotics Symposium (LARS), 2019 Brazilian Symposium on Robotics (SBR) and 2019 Workshop on Robotics in Education (WRE)",23-25 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WoWMoM51794.2021.00053,"Similarity Measures for Location-Dependent MMIMO, 5G Base Stations On/Off Switching Using Radio Environment Map",IEEE,Conferences,"The Massive Multiple-Input Multiple-Output (MMIMO) technique together with Heterogeneous Network (Het-Net) deployment enables high throughput of 5G and beyond networks. However, a high number of antennas and a high number of Base Stations (BSs) can result in significant power consumption. Previous studies have shown that the energy efficiency (EE) of such a network can be effectively increased by turning off some BSs depending on User Equipments (UEs) positions. Such mapping is obtained by using Reinforcement Learning. Its results are stored in a so-called Radio Environment Map (REM). However, in a real network, the number of UEs' positions patterns would go to infinity. This paper aims to determine how to match the current set of UEs' positions to the most similar pattern, i.e., providing the same optimal active BSs set, saved in REM. We compare several state-of-the-art distance metrics using a computer simulator: an accurate 3D-Ray-Tracing model of the radio channel and an advanced system-level simulator of MMIMO Het-Net. The results have shown that the so-called Sum of Minimums Distance provides the best matching between REM data and UEs' positions, enabling up to 56% EE improvement over the scenario without EE optimization.",https://ieeexplore.ieee.org/document/9469498/,"2021 IEEE 22nd International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM)",7-11 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2000.859462,"Simulating the evolution of 2D pattern recognition on the CAM-Brain Machine, an evolvable hardware tool for building a 75 million neuron artificial brain",IEEE,Conferences,"This paper presents some simulation results of the evolution of 2D visual pattern recognizers to be implemented very shortly on real hardware, namely the ""CAM-Brain Machine"" (CBM), an FPGA based piece of evolvable hardware which implements a genetic algorithm (GA) to evolve a 3D cellular automata (CA) based neural network circuit module, of approximately 1,000 neurons, in about a second, i.e. a complete run of a GA, with tens of thousands of circuit growths and performance evaluations. Up to 65,000 of these modules, each of which is evolved with a humanly specified function, can be downloaded into a large RAM space, and interconnected according to humanly specified artificial brain architectures. This RAM, containing an artificial brain with up to 75 million neurons, is then updated by the CBM at a rate of 130 billion CA cells per second. Such speeds will enable real time control of robots and hopefully the birth of a new research field that we call ""brain building"". The first such artificial brain, to be built at STARLAB in 2000 and beyond, will be used to control the behaviors of a life sized kitten robot called ""Robokitty"". This kitten robot will need 2D pattern recognizers in the visual section of its artificial brain. This paper presents simulation results on the evolvability and generalization properties of such recognizers.",https://ieeexplore.ieee.org/document/859462/,Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium,27-27 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC49862.2020.9338925,Simulation Modeling for Inertial Work-pattern of UAV Optoelectronic Gimbals,IEEE,Conferences,"At present, the simulation of photoelectric gimbals is only semi physical simulation, which can not meet the training requirements of full digital simulation. Based on working principle of the optoelectronic gimbals, a two-freedom-degrees optoelectronic gimbals model composed of inner ring, outer ring and bracket was built. Based on direction cosine theory and quasi Newton numerical iterative technique, for the actual equipment of the optoelectronic gimbals, the inertial work-pattern simulating & modeling algorithm (IWPSMA) were proposed in this projection, which to keep the axis stable in the inertial space. The model and IWPSMA were simulated, with the UAV moving in a circle, the azimuth changes periodically in 360°, and the elevation angle change nonlinearly in 0°-35°. The turntable model and algorithm are loaded into the 3D simulation scene. The experimental results show that the fidelity of the model and the real equipment are high, the boresight axis of the model keeps relatively stable in the inertial space, the computer resource occupation rate is low, and the 3D picture is smooth and stable.",https://ieeexplore.ieee.org/document/9338925/,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),11-13 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISBI48211.2021.9433984,Simulation of Astrocytic Calcium Dynamics in Lattice Light Sheet Microscopy Images,IEEE,Conferences,"Astrocytes regulate neuronal information processing through a variety of spatio-temporal calcium signals. Recent advances in calcium imaging have started to shine light on astrocytic activity, but the complexity and size of the recorded data strongly call for more advanced computational analysis tools. Their development is currently hindered by the lack of reliable, labeled annotations that are essential for the evaluation of algorithms and the training of learning-based methods. To solve this labeling problem, we have designed a generator of 2D/3D lattice light sheet microscopy (LLSM) sequences which realistically depict the calcium dynamics of astrocytes. By closely modeling calcium kinetics in real astrocytic ramifications, the generated datasets open the door for the deployment of convolutional neural networks in LLSM.",https://ieeexplore.ieee.org/document/9433984/,2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI),13-16 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVW.2017.84,SkiMap++: Real-Time Mapping and Object Recognition for Robotics,IEEE,Conferences,"We introduce SkiMap++, an extension to the recently proposed SkiMap mapping framework for robot navigation [1]. The extension deals with enriching the map with semantic information concerning the presence in the environment of certain objects that may be usefully recognized by the robot, e.g. for the sake of grasping them. More precisely, the map can accommodate information about the spatial locations of certain 3D object features, as determined by matching the visual features extracted from the incoming frames through a random forest learned off-line from a set of object models. Thereby, evidence about the presence of object features is gathered from multiple vantage points alongside with the standard geometric mapping task, so to enable recognizing the objects and estimating their 6 DOF poses. As a result, SkiMap++ can reconstruct the geometry of large scale environments as well as localize some relevant objects therein (Fig.1) in real-time on CPU. As an additional contribution, we present an RGB-D dataset featuring ground-truth camera and object poses, which may be deployed by researchers interested in pursuing SLAM alongside with object recognition, a topic often referred to as Semantic SLAM1.",https://ieeexplore.ieee.org/document/8265293/,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),22-29 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP40776.2020.9054674,Slicenet: Slice-Wise 3D Shapes Reconstruction from Single Image,IEEE,Conferences,"3D object reconstruction from a single image is a highly ill-posed problem, requiring strong prior knowledge of 3D shapes. Deep learning methods are popular for this task. Especially, most works utilized 3D deconvolution to generate 3D shapes. However, the resolution of results is limited by the high resource consumption of 3D deconvolution. In this paper, we propose SliceNet, sequentially generating 2D slices of 3D shapes with shared 2D deconvolution parameters. To capture relations between slices, the RNN is also introduced. Our model has three main advantages: First, the introduction of RNN allows the CNN to focus more on local geometry details,improving the results' fine-grained plausibility. Second, replacing 3D deconvolution with 2D deconvolution reducs much consumption of memory, enabling higher resolution of final results. Third, an slice-aware attention mechanism is designed to provide dynamic information for each slice's generation, which helps modeling the difference between multiple slices, making the learning process easier. Experiments on both synthesized data and real data illustrate the effectiveness of our method.",https://ieeexplore.ieee.org/document/9054674/,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",4-8 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASS50613.2020.00079,Slimmer: Accelerating 3D Semantic Segmentation for Mobile Augmented Reality,IEEE,Conferences,"Three-Dimensional (3D) semantic segmentation is an essential building block for interactive Augmented Reality (AR). However, existing Deep Neural Network (DNN) models for segmenting 3D objects are not only computation-intensive but also memory heavy, hindering their deployment on resource-constrained mobile devices. We present the design, implementation and evaluation of Slimmer, a generic and model-independent framework for accelerating 3D semantic segmentation and facilitating its real-time applications on mobile devices. In contrast to the current practice that directly feeds a point cloud to DNN models, Slimmer is motivated by our observation that these models remain high accuracy even if we remove a fraction of points from the input, which can significantly reduce the inference time and memory usage of these models. Our design of Slimmer faces two key challenges. First, the simplification method of point clouds should be lightweight. Otherwise, the reduced inference time may be canceled out by the incurred overhead of input-data simplification. Second, Slimmer still needs to accurately segment the removed points from the input to create a complete segmentation of the original input, again, using a lightweight method. Our extensive performance evaluation demonstrates that, by addressing these two challenges, Slimmer can dramatically reduce the resource utilization of a representative DNN model for 3D semantic segmentation. For example, if we can tolerate 1% accuracy loss, the reduction could be ~20% for inference time and ~9% for memory usage. The reduction increases to around ~27% for inference time and ~15% for memory usage when we can tolerate 2% accuracy loss.",https://ieeexplore.ieee.org/document/9356048/,2020 IEEE 17th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),10-13 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR-Adjunct51615.2020.00038,"Small Marker Tracking with Low-Cost, Unsynchronized, Movable Consumer Cameras For Augmented Reality Surgical Training",IEEE,Conferences,"Surgeons improve their skills through repetition of training tasks in order to operate on living patients, ideally receiving timely, useful, and objective performance feedback. However, objective performance measurement is currently difficult without 3D visualization, with effective surgical training apparatus being extremely expensive or limited in accessibility. This is problematic for medical students, especially in situations such as the COVID-19 pandemic in which they are needed by the community but have few ways of practicing without lab access. In this work, we propose and prototype a system for augmented reality (AR) visualization of laparoscopic training tasks using cheap and widely-compatible borescopes, which can track small objects typical of surgical training. We use forward kinematics for calibration and multi-threading to attempt synchronization in order to increase compatibility with consumer applications, resulting in an effective AR simulation with low-cost devices and consumer software, while also providing dynamic camera and marker tracking. We test the system with a typical peg transfer task on the HoloLens 1 and MagicLeap One.",https://ieeexplore.ieee.org/document/9288414/,2020 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct),9-13 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/cp.2018.0016,Smart IoT and Soft AI,IET,Conferences,"Soft artificial intelligence (AI) is defined as non-sentient AI designed to perform close to human level in one specific domain. This is in contrast to “Artificial General Intelligence” (AGI) which solves the problem for human level intelligence across all domains. Soft AI is a reality now in the new generation of smart Internet of Things devices like Amazon's Alexa, Apple's Siri or Microsoft's Cortana, giving rise to concerns about privacy and how the technology is being used. This research is based around an experiment in “AI as a service” where fifteen chatbot agents using Google's “Dialogflow” are deployed around the Queen Elizabeth Olympic Park in London for the general public to interact with. The physical devices are 3D printed representations of creatures living in the park, designed to fit with the park's biodiversity remit. Park visitors interact with the creatures via their mobile phones, engaging in a conversation where the creature offers to tell them a memory in exchange for one of their own, while warning them that anything they say might be repeated to others. The scope of the work presented here is as follows. After explaining the details of the deployment and three month study, the conversational data collected from visitors is then analysed. Following a review of the current literature, techniques for working with the unstructured natural language data are developed, leading to recommendations for the design of future conversational “chatbot” agents. The results show distinct patterns of conversation, from simple and direct “verb plus noun” commands to complex sentence structure. How users interact with the agents, given that they are conversing with a mechanism, is discussed and contrasted with the memories that they have agreed to share. The conclusion drawn from this work is that, while the current generation of devices only listen for commands from users, there is a danger that smart IoT devices in the future can be used as active information probes unless properly understood and regulated. We finish with observations on privacy and security based on our experiences here.",https://ieeexplore.ieee.org/document/8379703/,Living in the Internet of Things: Cybersecurity of the IoT - 2018,28-29 March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SaCoNeT.2018.8585616,Smart Navigation of Mobile Robot Using Neural Network Controller,IEEE,Conferences,"The field of autonomous navigation of mobile robot is advancing so fast especially with the development of machine learning algorithms. This study aims to introduce a neural network controller that controls the trajectory and the obstacle avoidance of a non-holonomic mobile robot.We train the robot in environment containing multiple obstacles with different places. This paper includes both a kinematic and a dynamic study of a mobile robot. Different training schemes have been studied that tackle the learning objectives differently. The trained controller is producing the Pulse Width Modulation (PWM) signals that could be implemented in a microprocessor and validated by simulations. Unlike some other recent approaches, this work was validated by a 3D simulation which is similar to the real model.",https://ieeexplore.ieee.org/document/8585616/,2018 International Conference on Smart Communications in Network Technologies (SaCoNeT),27-31 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VLSID.2018.26,Special Session: Design of Energy-Efficient and Reliable VLSI Systems: A Data-Driven Perspective,IEEE,Conferences,"Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. The amount of data generated and collected across computing platforms every day is not only enormous, but growing at an exponential rate. Advanced data analytics and machine learning techniques have become increasingly essential to analyze and extract meaning from such “Big Data”. These techniques can be very useful to detect patterns and trends to improve the operational behavior of computing systems, but they also introduce a number of outstanding challenges: (1) How can we design and deploy data analytics mechanisms to improve energy-efficiency and reliability in IoT and mobile devices, without introducing significant software overheads? (2) How to leverage emerging technologies (e.g.,3D integration) to design energy-efficient and reliable manycore systems for big data computing? (3) How to use machine learning and data mining techniques for effective design space exploration of computing systems, and enable adaptive control to improve energy-efficiency? (4) How can data analytics detect anomalies and increase robustness in the network backbone of emerging large-scale networking systems? To address these outstanding challenges, out-of-the-box approaches need to be explored. In this special session, we will discuss these outstanding problems and describe far-reaching solutions applicable across the interconnected ecosystem of IoT and mobile devices, manycore chips, datacenters, and networks. The special session brings together speakers with unique insights on applying data analytics and machine learning to real-world problems to achieve the most sought after features on multi-scale computing platforms, viz. intelligent data mining, energyefficiency, and robustness. By integrating data analytics and machine learning algorithms, statistical modeling, embedded hardware and software design, and cloud computing content, this session will engage a broad section of Embedded and VLSI Design conference attendees. This special session is targeted towards university researchers/professors, students, industry professionals, and embedded/VLSI system designers. This session will attract newcomers who want to learn how to apply data analytics to solve problems in computing systems, as well as experienced researchers looking for exciting new directions in embedded systems, VLSI design, EDA algorithms, and multi-scale computing.",https://ieeexplore.ieee.org/document/8326889/,2018 31st International Conference on VLSI Design and 2018 17th International Conference on Embedded Systems (VLSID),6-10 Jan. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2018.8462926,SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D LiDAR Point Cloud,IEEE,Conferences,"We address semantic segmentation of road-objects from 3D LiDAR point clouds. In particular, we wish to detect and categorize instances of interest, such as cars, pedestrians and cyclists. We formulate this problem as a point-wise classification problem, and propose an end-to-end pipeline called SqueezeSeg based on convolutional neural networks (CNN): the CNN takes a transformed LiDAR point cloud as input and directly outputs a point-wise label map, which is then refined by a conditional random field (CRF) implemented as a recurrent layer. Instance-level labels are then obtained by conventional clustering algorithms. Our CNN model is trained on LiDAR point clouds from the KITTI [1] dataset, and our point-wise segmentation labels are derived from 3D bounding boxes from KITTI. To obtain extra training data, we built a LiDAR simulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize large amounts of realistic training data. Our experiments show that SqueezeSeg achieves high accuracy with astonishingly fast and stable runtime (8.7±0.5 ms per frame), highly desirable for autonomous driving. Furthermore, additionally training on synthesized data boosts validation accuracy on real-world data. Our source code is open-source released1. The paper is accompanied by a video2 containing a high level introduction and demonstrations of this work.",https://ieeexplore.ieee.org/document/8462926/,2018 IEEE International Conference on Robotics and Automation (ICRA),21-25 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2002.1176744,Stereo camera system and its application in networked virtual environment,IEEE,Conferences,"With the appearance of different kinds of networked virtual environment systems, the technology of realistic video avatar has become a hotspot of research. How to create realistic video avatar with less hardware investment and limited computation is one of the purposes of these researches. We introduce a method that combines a single stereo camera system and a 3D head model to create a realistic video avatar. To create the video avatar's most important part-the face, we use a 2D image and a depth map, which is generated by the stereo camera system. The other parts of of the video avatar are created by mapping the head's texture to the 3D head model.",https://ieeexplore.ieee.org/document/1176744/,Proceedings. International Conference on Machine Learning and Cybernetics,4-5 Nov. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00038,StickyPillars: Robust and Efficient Feature Matching on Point Clouds using Graph Neural Networks,IEEE,Conferences,"Robust point cloud registration in real-time is an important prerequisite for many mapping and localization algorithms. Traditional methods like ICP tend to fail without good initialization, insufficient overlap or in the presence of dynamic objects. Modern deep learning based registration approaches present much better results, but suffer from a heavy runtime. We overcome these drawbacks by introducing StickyPillars, a fast, accurate and extremely robust deep middle-end 3D feature matching method on point clouds. It uses graph neural networks and performs context aggregation on sparse 3D key-points with the aid of transformer based multi-head self and cross-attention. The network output is used as the cost for an optimal transport problem whose solution yields the final matching probabilities. The system does not rely on hand crafted feature descriptors or heuristic matching strategies. We present state-of-art art accuracy results on the registration problem demonstrated on the KITTI dataset while being four times faster then leading deep methods. Furthermore, we integrate our matching system into a LiDAR odometry pipeline yielding most accurate results on the KITTI odometry dataset. Finally, we demonstrate robustness on KITTI odometry. Our method remains stable in accuracy where state-of-the-art procedures fail on frame drops and higher speeds.",https://ieeexplore.ieee.org/document/9578620/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCA.2019.00016,String Figure: A Scalable and Elastic Memory Network Architecture,IEEE,Conferences,"Demand for server memory capacity and performance is rapidly increasing due to expanding working set sizes of modern applications, such as big data analytics, inmemory computing, deep learning, and server virtualization. One promising techniques to tackle this requirements is memory networking, whereby a server memory system consists of multiple 3D die-stacked memory nodes interconnected by a high-speed network. However, current memory network designs face substantial scalability and flexibility challenges. This includes (1) maintaining high throughput and low latency in large-scale memory networks at low hardware cost, (2) efficiently interconnecting an arbitrary number of memory nodes, and (3) supporting flexible memory network scale expansion and reduction without major modification of the memory network design or physical implementation. To address the challenges, we propose String Figure1, a highthroughput, elastic, and scalable memory network architecture. String Figure consists of (1) an algorithm to generate random topologies that achieve high network throughput and nearoptimal path lengths in large-scale memory networks, (2) a hybrid routing protocol that employs a mix of computation and look up tables to reduce the overhead of both in routing, (3) a set of network reconfiguration mechanisms that allow both static and dynamic network expansion and reduction. Our experiments using RTL simulation demonstrate that String Figure can interconnect over one thousand memory nodes with a shortest path length within five hops across various traffic patterns and real workloads.",https://ieeexplore.ieee.org/document/8675231/,2019 IEEE International Symposium on High Performance Computer Architecture (HPCA),16-20 Feb. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2009.5206699,Structured output-associative regression,IEEE,Conferences,"Structured outputs such as multidimensional vectors or graphs are frequently encountered in real world pattern recognition applications such as computer vision, natural language processing or computational biology. This motivates the learning of functional dependencies between spaces with complex, interdependent inputs and outputs, as arising e.g. from images and their corresponding 3d scene representations. In this spirit, we propose a new structured learning method-Structured Output-Associative Regression (SOAR)-that models not only the input-dependency but also the self-dependency of outputs, in order to provide an output re-correlation mechanism that complements the (more standard) input-based regressive prediction. The model is simple but powerful, and, in principle, applicable in conjunction with any existing regression algorithms. SOAR can be kernelized to deal with non-linear problems and learning is efficient via primal/dual formulations not unlike ones used for kernel ridge regression or support vector regression. We demonstrate that the method outperforms weighted nearest neighbor and regression methods for the reconstruction of images of handwritten digits and for 3D human pose estimation from video in the HumanEva benchmark.",https://ieeexplore.ieee.org/document/5206699/,2009 IEEE Conference on Computer Vision and Pattern Recognition,20-25 June 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ART.2002.1107002,Student projects using ARToolKit,IEEE,Conferences,"This paper describes three student projects developed at the University of Applied Science at Hagenberg (media technology and design) using ARToolKit. All projects were implemented by students during their semester projects or during their master thesis. MusicAR is the first application presented in this paper. It describes the master thesis of a student and shows a nice music learning program for children. The second application is ASR (augmented sound reality), realized by two students during their semester project. ASR allows the user to place 3D sound sources and gives the user an acoustic 3D impression of the sound sources. Finally, two students implemented ARoom, a furnishing program using AR.",https://ieeexplore.ieee.org/document/1107002/,"The First IEEE International Workshop Agumented Reality Toolkit,",29-29 Sept. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2006.384813,Supervised Learning of Motion Style for Real-time Synthesis of 3D Character Animations,IEEE,Conferences,"In this paper, we present a supervised learning framework to learn a probabilistic mapping from values of a low-dimensional style variable, which defines the characteristics of a certain kind of 3D human motion such as walking or boxing, to high-dimensional vectors defining 3D poses. All possible values of the style variable span an Euclidean space called style space. The supervised learning framework guarantees that each dimension of style space corresponds to a certain aspect of the motion characteristics, such as body height and pace length, so the user can precisely define a 3D pose by locating a point in the style space. Moreover, every curve in the Euclidean style space corresponds to a smooth motion sequence. We developed a graphical user interface program, with which, users simply points mouse cursor in the style space to define a 3D pose and drags mouse cursor to synthesis 3D animations in real-time.",https://ieeexplore.ieee.org/document/4274578/,"2006 IEEE International Conference on Systems, Man and Cybernetics",8-11 Oct. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AEMCSE50948.2020.00064,Surgical Workflow Recognition Using Two-Stream Mixed Convolution Network,IEEE,Conferences,"Surgical workflow recognition is the prerequisite for automatic indexing of surgical video databases and optimization of real-time operating scheduling, which is an important part of the modern operating room (OR). In this paper, we propose a surgical phase recognition method based on a two-stream mixed convolutional network (TsMCNet) to automatically recognize surgical workflow. TsMCNet optimizes the visual and temporal features learned from surgical videos by integrating 2D and 3D convolutional networks (CNNs) to form a spatio-temporal complementary architecture. Specifically, temporal branch (3D CNN) is responsible for learning the spatio-temporal features among adjacent frames, whereas the parallel visual branch (2D CNN) is focused on capturing the deep visual features of each frame. Extensive experiments on a public surgical video dataset (MICCAI 2016 Workflow Challenge) demonstrated outstanding performance of our proposed method, exceeding that of state-of-the-art methods (e.g., 86.2% accuracy and 83.0% F1 score).",https://ieeexplore.ieee.org/document/9131322/,"2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)",24-26 April 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QoMEX48832.2020.9123135,Synthetic Thermal Image Generation for Human-Machine Interaction in Vehicles,IEEE,Conferences,"Thermal infrared imaging holds promise for human-machine interaction in vehicles owing to superior performance in low-light and low-visibility conditions, and the potential for monitoring human psycho-physiological state. However, the shortage of large-scale 2D thermal image datasets and public benchmarks has hindered progress of deep-learning-based solutions. To tackle this problem, we develop a pipeline for creating a synthetic thermal image dataset. Firstly, 3D models of human heads are generated from uncalibrated TIR images (without additional visible or depth images) using photogrammetry techniques. A synthetic dataset of 100k images of 640×480 resolution are then generated by rendering each of the five 3D models for a range of head poses, camera positions and backgrounds using commercial animation software. The effectiveness of the approach is evaluated using a number of deep learning algorithms that may enable human-machine interaction such as head pose estimation and face detection. The neural networks are trained on the new synthetic thermal dataset, before fine tuning on real world data where possible.",https://ieeexplore.ieee.org/document/9123135/,2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX),26-28 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IoTaIS53735.2021.9628587,System configuration of Human-in-the-loop Simulation for Level 3 Autonomous Vehicle using IPG CarMaker,IEEE,Conferences,"The increasingly automated vehicles (AV) have increased the complexity of the testing methods and number of driven miles required to demonstrate the vehicle system’s reliability. Most modern autonomous driving systems also used deep neural networks which requires a large amount of data to develop. Physical driving alone to collect driving data and test system’s safety is no longer suitable for development of AV as this is costly, time consuming and could harm the road users if the safety system failed. This paper proposed human-in-the-loop simulation testing for evaluation of an autonomous vehicle using a 3D virtual vehicle driving platform that can be used for safety assessment of autonomous vehicle. The aim of this study is to establish human-computer interaction platform that can be used as safety testing for Level 3 autonomous vehicle whereby an emergency takeover is required during critical driving conditions. The proposed platform make use of IPG CarMaker to provide 3D virtual environment with accurate vehicle dynamics model, sensor model and environment model. We are able to interface the IPG CarMaker with Simulink and successfully developed a Simulink model that can interface a steering and pedal driving hardware with the virtual vehicle in the simulation. We can also collect driving data and simulation data from the IPG CarMaker as well as accessing the variable in the IPG CarMaker in real-time using Python. The recorded data can be used to train and fine-tune autonomous system based on machine learning.",https://ieeexplore.ieee.org/document/9628587/,2021 IEEE International Conference on Internet of Things and Intelligence Systems (IoTaIS),23-24 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2019.00718,TSM: Temporal Shift Module for Efficient Video Understanding,IEEE,Conferences,"The explosive growth in video streaming gives rise to challenges on performing video understanding at high accuracy and low computation cost. Conventional 2D CNNs are computationally cheap but cannot capture temporal relationships; 3D CNN based methods can achieve good performance but are computationally intensive, making it expensive to deploy. In this paper, we propose a generic and effective Temporal Shift Module (TSM) that enjoys both high efficiency and high performance. Specifically, it can achieve the performance of 3D CNN but maintain 2D CNN&#x2019;s complexity. TSM shifts part of the channels along the temporal dimension; thus facilitate information exchanged among neighboring frames. It can be inserted into 2D CNNs to achieve temporal modeling at zero computation and zero parameters. We also extended TSM to online setting, which enables real-time low-latency online video recognition and video object detection. TSM is accurate and efficient: it ranks the first place on the Something-Something leaderboard upon publication; on Jetson Nano and Galaxy Note8, it achieves a low latency of 13ms and 35ms for online video recognition. The code is available at: https://github. com/mit-han-lab/temporal-shift-module.",https://ieeexplore.ieee.org/document/9008827/,2019 IEEE/CVF International Conference on Computer Vision (ICCV),27 Oct.-2 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC48633.2019.8996630,Target Recognition and 3D Pose Estimation Based on Prior Knowledge and Convolutional Neural Network for Robots,IEEE,Conferences,"In the competition of RoboMaster, the robot needs to trigger the target, the “Energy Mechanism” which consists of nine different dynamic flame numbers, in a nine square area by shooting projectiles. Therefore, 3D target detection should be implemented including target recognition and 3D pose estimation in real-time. As the targets are dynamic flame numbers and quite small in the whole image, it increases the difficulty to detect. The robot should achieve to shoot the target in multi-angle and multi-scale to adjust the competition. To address these issues, we propose a fast and accurate method to detect all nine numbers and estimate each 3D pose based on prior knowledge and convolutional neural network only by a monocular camera. The geometric constraints around the target are employed as prior knowledge when estimating the target pose. Then, we utilize the relative position information to detect the region of each dynamic number in the image, which is recognized by a convolutional neural network trained by flame numbers. Experiments in the actual environment show that our method can achieve the detection of each dynamic number in real-time and high accuracy. The runtime is 29ms on average (about 11ms in detection and 18ms in recognition) and the recognition accuracy is about 94.69%. And our method wins the first place in the technical challenge of 2018 RoboMaster competition.",https://ieeexplore.ieee.org/document/8996630/,2019 Chinese Automation Congress (CAC),22-24 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.1999.812762,Task-model based human robot cooperation using vision,IEEE,Conferences,"In order to assist a human, the robot must recognize human motion in real time by vision, and must plan and execute the needed assistance motion based on the task purpose and the context. In this research, we tried to solve such problems. We defined the abstract task model, analyzed the human demonstration by using events and an event stack, and automatically generated the task models needed in the assistance by the robot. The robot planned and executed the appropriate assistance motions based on the task: models according to the human motions in the cooperation with the human. We implemented a 3D object recognition system and a human grasp recognition system by using trinocular stereo color cameras and a real time range finder. The effectiveness of these methods was tested through an experiment in which the human and the robotic hand assembled toy parts in cooperation.",https://ieeexplore.ieee.org/document/812762/,Proceedings 1999 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human and Environment Friendly Robots with High Intelligence and Emotional Quotients (Cat. No.99CH36289),17-21 Oct. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2016.637,TenSR: Multi-dimensional Tensor Sparse Representation,IEEE,Conferences,"The conventional sparse model relies on data representation in the form of vectors. It represents the vector-valued or vectorized one dimensional (1D) version of an signal as a highly sparse linear combination of basis atoms from a large dictionary. The 1D modeling, though simple, ignores the inherent structure and breaks the local correlation inside multidimensional (MD) signals. It also dramatically increases the demand of memory as well as computational resources especially when dealing with high dimensional signals. In this paper, we propose a new sparse model TenSR based on tensor for MD data representation along with the corresponding MD sparse coding and MD dictionary learning algorithms. The proposed TenSR model is able to well approximate the structure in each mode inherent in MD signals with a series of adaptive separable structure dictionaries via dictionary learning. The proposed MD sparse coding algorithm by proximal method further reduces the computational cost significantly. Experimental results with real world MD signals, i.e. 3D Multi-spectral images, show the proposed TenSR greatly reduces both the computational and memory costs with competitive performance in comparison with the state-of-the-art sparse representation methods. We believe our proposed TenSR model is a promising way to empower the sparse representation especially for large scale high order signals.",https://ieeexplore.ieee.org/document/7781006/,2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),27-30 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2018.8423008,Tensor-Generative Adversarial Network with Two-Dimensional Sparse Coding: Application to Real-Time Indoor Localization,IEEE,Conferences,"Localization technology is important for the development of indoor location-based services (LBS). Global Positioning System (GPS) becomes invalid in indoor environments due to the non-line-of-sight issue, so it is urgent to develop a real-time high-accuracy localization approach for smartphones. However, accurate localization is challenging due to issues such as real-time response requirements, limited fingerprint samples and mobile device storage. To address these problems, we propose a novel deep learning architecture: Tensor-Generative Adversarial Network (TGAN). We first introduce a transform-based 3D tensor to model fingerprint samples. Instead of those passive methods that construct a fingerprint database as a prior, our model applies artificial neural network with deep learning to train network classifiers and then gives out estimations. Then we propose a novel tensorbased super-resolution scheme using the generative adversarial network (GAN) that adopts sparse coding as the generator network and a residual learning network as the discriminator. Further, we analyze the performance of TGAN and implement a trace-based localization experiment, which achieves better performance. Compared to existing methods for smartphones indoor positioning, that are energy- consuming and high demands on devices, TGAN can give out an improved solution in localization accuracy, response time and implementation complexity.",https://ieeexplore.ieee.org/document/8423008/,2018 IEEE International Conference on Communications (ICC),20-24 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCWAMTIP47768.2019.9067677,Terrain Edge Stitching Based On Least Squares Generative Adversarial Networks,IEEE,Conferences,"Virtual terrain is widely used in computer graphics and game, etc. This paper proposes an end-to-end deep learning approach for a 3D unit terrain generation from a simple sketch of terrain features using Least Squares Generative Adversarial Networks (LSGANs). Then edge regeneration strategy is used to seamlessly stitches the edges of multiple unit terrains. Unlike the existing works, edge regeneration strategy regenerates the connection area between multiple unit terrains, to expand the terrain size and shape while the existing terrain unchanged. LSGANs trained by using real-world terrains and their sketched counterparts. Experimental results show that the proposed approach produces a vast variety of complex terrains in high level of realism without fixed shapes and sizes only for a minimum sketch cost.",https://ieeexplore.ieee.org/document/9067677/,2019 16th International Computer Conference on Wavelet Active Media Technology and Information Processing,14-15 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2006.385090,The SOMN-HMM Model and Its Application to Automatic Synthesis of 3D Character Animations,IEEE,Conferences,"Learning HMM from motion capture data for automatic 3D character animation synthesis is becoming a hot spot in research areas of computer graphics and machine learning. To ensure realistic synthesis, the model must be learned to fit the real distribution of human motion. Usually the fitness is measured by likelihood. In this paper, we present a new HMM learning algorithm, which incorporates stochastic optimization technique within the expectation-maximization (EM) learning framework. This algorithm is less prone to be trapped in local optimal and converges faster than traditional Baum-Welch learning algorithm. We apply the new algorithm to learning 3D motion under control of a style variable, which encodes the mood or personality of the performer. Given new style value, motions with corresponding style can be generated from the learned model.",https://ieeexplore.ieee.org/document/4274699/,"2006 IEEE International Conference on Systems, Man and Cybernetics",8-11 Oct. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASAP.1992.218560,The Sarnoff Engine: a massively parallel computer for high definition system simulation,IEEE,Conferences,"This paper describes the Sarnoff Engine, a 1.6 TeraFLOP, ral-time, high definition, video and image processing computer. It is a second generation, scalable, linear array, multi-user, MIMD architecture focused on the applications of real-time high definition video and image processing (data compression encoders and decoders), 3D/4D data visualization, and neural network development.<>",https://ieeexplore.ieee.org/document/218560/,[1992] Proceedings of the International Conference on Application Specific Array Processors,4-7 Aug. 1992,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICISYS.2009.5358028,The adaptive learning mechanism design for game agents' real-time behavior control,IEEE,Conferences,"In this paper, we present an approach of adaptive learning mechanism for game agents' real-time behavior control. This approach mainly focuses on how to generate game agent's adaptability in real-time. It is possible to apply our approach in complicated game character interactions by following the framework discussed in this paper. We consider the layered architecture, the behavior pattern and the adaptive mechanism design to be the three key points of our approach. We provide a brief example of how to apply adaptive learning in game agents' behavior processing. From this example, we demonstrate that the planning and learning process is fast enough to have 3D model rendered in time.",https://ieeexplore.ieee.org/document/5358028/,2009 IEEE International Conference on Intelligent Computing and Intelligent Systems,20-22 Nov. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ChiCC.2014.6896010,The design and optimization method of near space intelligent target generator,IEEE,Conferences,"This paper presents the design and optimization method of near space intelligent target generator to simulate the physical characteristics of the near space vehicle. Combined with High Level Architecture distributed simulation technology, a common, repeatable and verified platform for the near space vehicle has been provided. This method used 3D modeling software Creator and 3D visual rendering software Vega, two-dimensional map and three-dimensional vision were constructed to form a simulation environment, which enhanced the authenticity of the simulation. Based on particle swarm optimization, the intelligent path planning study of near space vehicle was conducted in this environment to make up for the inadequate intelligence of traditional target generators.",https://ieeexplore.ieee.org/document/6896010/,Proceedings of the 33rd Chinese Control Conference,28-30 July 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIS.2002.1048107,The real world virtual models,IEEE,Conferences,The principles and technology of the creation of the real world virtual models are examined in the paper. It is done on the example of open cast mining (OCM) modeling. The most advanced and quickly developed 3D-models and their stereoscopic presentation are emphasized. The implementation is done within the context of object oriented design in the automated system of the OCM survey support.,https://ieeexplore.ieee.org/document/1048107/,Proceedings 2002 IEEE International Conference on Artificial Intelligence Systems (ICAIS 2002),5-10 Sept. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCBDA.2019.8725696,Thermal-Stress Analysis of 3D-IC Based on Artificial Neural Network,IEEE,Conferences,"Three-Dimensional Integrated Circuits (3D-IC) is a new processing technology emerging in the field of microelectronics in recent years. Through Silicon Via (TSV) technology is used to connect different wafers vertically. Due to the increasing number of transistors in 3D-IC, its heat dissipation and thermal-stress mechanical problems are increasing. These problems can cause performance degradation of components even fail. When the chip is working, the working conditions of various components are constantly changing. The thermal stress of each point is also changing. It is very difficult to get real-time data, so the dynamic thermal stress management technology will be severely limited. The paper mainly proposes a method for quickly predicting the maximum thermal stress at various points in the work of TSV-based 3D-IC chips. Firstly, the finite element simulation software COMSOL is used to establish the overall chip finite element model of TSV based 3D-IC. TSV temperature and chip thermal stress were calculated to find the maximum thermal stress near each TSV and obtain the temperature distribution within a certain range of TSV. Then, an RBF neural network model is built through sample training based on the derived temperature and thermal stress data. RBF neural network model is used to provide some information support for dynamic thermal stress management technology of working chip. Through the extraction of data characteristic information, the thermal stress around TSV is predicted accurately, which provides an accurate and efficient solution for dynamic thermal stress management when the chip works.",https://ieeexplore.ieee.org/document/8725696/,2019 IEEE 4th International Conference on Cloud Computing and Big Data Analysis (ICCCBDA),12-15 April 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iThings/GreenCom/CPSCom/SmartData.2019.00153,Three-Dimensional Simulation for Training Autonomous Vehicles in Smart City Environments,IEEE,Conferences,"This paper proposes a photorealistic 3D city simulation method for training autonomous vehicles. The proposed method incorporates human simulation, animal simulation, vehicle simulation, and traffic light simulation. To generate natural actions for humans and animals, a motivation-based approach is first applied; then the Q-Network is used to select optimal goals depending on the motivations, and action plans are made based on a hierarchical task network. For vehicles, affinity propagation, data augmentation, and convolutional neural network are employed to generate driver driving data for realistic vehicle movement simulation. A traffic light system is also implemented based on rules derived from real-life observation. The results of experiments in which a virtual city was created demonstrate that the proposed method can simulate city environments naturally. The proposed method can be applied to various smart city applications, such as autonomous vehicle training systems.",https://ieeexplore.ieee.org/document/8875271/,"2019 International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)",14-17 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EMBC.2016.7591533,Three-dimensional modeling of physiological tremor for hand-held surgical robotic instruments,IEEE,Conferences,"Hand-held robotic instruments are developed to compensate physiological tremor in real-time while augmenting the required precision and dexterity into normal microsurgical work-flow. The hardware (sensors and actuators) and software (causal linear filters) employed for tremor identification and filtering introduces time-varying unknown phase-delay that adversely affects the device performance. The current techniques that focus on three-dimensions (3D) tip position control involves modeling and canceling the tremor in 3-axes (x, y, and z axes) separately. Our analysis with the tremor data recorded from surgeons and novice subjects show that there exists significant correlation in tremor motion across the dimensions. Motivated by this, a new multi-dimensional modeling approach based on extreme learning machines (ELM) is proposed in this paper to correct the phase delay and to accurately model tremulous motion in three dimensions simultaneously. A study is conducted with tremor data recorded from the microsurgeons to analyze the suitability of proposed approach.",https://ieeexplore.ieee.org/document/7591533/,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC),16-20 Aug. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IBCAST.2019.8667269,Time and Memory Efficient 3D Point Cloud Classification,IEEE,Conferences,"Recent advancement in real-time 3D scanning has upraised the need to maximally crunch the 3D data in minimum possible time. PointNet is a modern end-to-end deep learning architecture which directly utilizes point cloud without transforming it to regular formats for object- classification, part segmentation and scene semantic parsing. In this paper, we have proposed a novel set of hyper-parameters for PointNet which significantly reduced the size of network while achieving the same accuracy in 3D object classification as of the original network. This can enable the PointNet to be deployed on less resourceful hardware. The modified PointNet utilizes 16.5 Million less parameters (weights) and 5.2 Million lesser memory units. Moreover, through ample experiments, we demonstrate that our proposed configuration is highly efficient in terms of training and testing time.",https://ieeexplore.ieee.org/document/8667269/,2019 16th International Bhurban Conference on Applied Sciences and Technology (IBCAST),8-12 Jan. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMCS.2012.6320173,Toward the construction of a virtual ecosystem by evolving virtual creature's behaviours,IEEE,Conferences,"In this paper a virtual ecosystem environment with basic physical law and energy concept has been proposed, this ecosystem is populated with 3D virtual creatures that are living in this environment in order to forage food. Artificial behaviours are developed to control virtual creatures. A genetic algorithm with an artificial neural network were implemented together to guarantee some of these behaviours like searching food. Foods are presented in different locations in the virtual ecosystem. The evolutionary process uses the physical properties of the virtual creatures and an external fitness function that will conduct to the expected behaviours. The experiment evolving locomoting virtual creatures show that these virtual creatures try to obtain at least one of the food sources presented in its trajectory. Our best-evolved creatures are able to reach multiple food sources during the simulation time.",https://ieeexplore.ieee.org/document/6320173/,2012 International Conference on Multimedia Computing and Systems,10-12 May 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IST55454.2022.9827665,Towards 3D printed modular unmanned aerial vehicle development: The landing safety paradigm,IEEE,Conferences,"Unmanned aerial vehicles (UAVs) are at the fore-front of this century&#x0027;s technological shift, becoming ubiquitous in research and market areas. Similarly, nowadays, 3D printing is a fast-emerging, widely used technology that allows individ-uals to design prototypes that fulfil their needs. This paper presents an autonomous UAV designed and implemented to be fully modular and 3D printable. Furthermore, suitable areas for landing are recognized using a lightweight deep learning architecture while a Gazebo model for simulation purposes is also given to the research community. Finally, its fly and surface recognition processes are evaluated exhaustively in real-world and simulation scenarios.",https://ieeexplore.ieee.org/document/9827665/,2022 IEEE International Conference on Imaging Systems and Techniques (IST),21-23 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2016.7797090,Towards Large-Scale 3D Face Recognition,IEEE,Conferences,"3D face recognition holds great promise in achieving robustness to pose, expressions and occlusions. However, 3D face recognition algorithms are still far behind their 2D counterparts due to the lack of large-scale datasets. We present a model based algorithm for 3D face recognition and test its performance by combining two large public datasets of 3D faces. We propose a Fully Convolutional Deep Network (FCDN) to initialize our algorithm. Reliable seed points are then extracted from each 3D face by evolving level set curves with a single curvature dependent adaptive speed function. We then establish dense correspondence between the faces in the training set by matching the surface around the seed points on a template face to the ones on the target faces. A morphable model is then fitted to probe faces and face recognition is performed by matching the parameters of the probe and gallery faces. Our algorithm achieves state of the art landmark localization results. Face recognition results on the combined FRGCv2 and Bosphorus datasets show that our method is effective in recognizing query faces with real world variations in pose and expression, and with occlusion and missing data despite a huge gallery. Comparing results of individual and combined datasets show that the recognition accuracy drops when the size of the gallery increases.",https://ieeexplore.ieee.org/document/7797090/,2016 International Conference on Digital Image Computing: Techniques and Applications (DICTA),30 Nov.-2 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2017.7966054,Towards real-time robot simulation on uneven terrain using neural networks,IEEE,Conferences,"Simulation is a valuable tool for robotics research and development, and various simulation packages have been proposed. However, we are aware of no freely-available packages which implement the required fidelity to accurately model earth-moving robots that manipulate the terrain itself. The software which does exist for this is difficult if not impossible to run in real-time while achieving the desired accuracy. This paper proposes a simulation system in which a neural network is trained using data generated in a 3D high-fidelity, non-real-time simulator. The resulting neural network is used to accurately predict the motion of a robot in a 2D simulator, while also taking into consideration a height-field representing a 3D terrain. Using a trained neural network to drive the new simulation provides considerable speedup over the high-fidelity 3D simulation, allowing behaviour to be simulated in real-time while still capturing the physics of the agents and the environment.",https://ieeexplore.ieee.org/document/7966054/,2017 International Joint Conference on Neural Networks (IJCNN),14-19 May 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SECON.2017.7925321,Towards real-time segmentation of 3D point cloud data into local planar regions,IEEE,Conferences,"This article describes an algorithm for efficient segmentation of point cloud data into local planar surface regions. This is a problem of generic interest to researchers in the computer graphics, computer vision, artificial intelligence and robotics community where it plays an important role in applications such as object recognition, mapping, navigation and conversion from point clouds representations to 3D surface models. Prior work on the subject is either computationally burdensome, precluding real time applications such as robotic navigation and mapping, prone to error for noisy measurements commonly found at long range or requires availability of coregistered color imagery. The approach we describe consists of 3 steps: (1) detect a set of candidate planar surfaces, (2) cluster the planar surfaces merging redundant plane models, and (3) segment the point clouds by imposing a Markov Random Field (MRF) on the data and planar models and computing the Maximum A-Posteriori (MAP) of the segmentation labels using Bayesian Belief Propagation (BBP). In contrast to prior work which relies on color information for geometric segmentation, our implementation performs detection, clustering and estimation using only geometric data. Novelty is found in the fast clustering technique and new MRF clique potentials that are heretofore unexplored in the literature. The clustering procedure removes redundant detections of planes in the scene prior to segmentation using BBP optimization of the MRF to improve performance. The MRF clique potentials dynamically change to encourage distinct labels across depth discontinuities. These modifications provide improved segmentations for geometry-only depth images while simultaneously controlling the computational cost. Algorithm parameters are tunable to enable researchers to strike a compromise between segmentation detail and computational performance. Experimental results apply the algorithm to depth images from the NYU depth dataset which indicate that the algorithm can accurately extract large planar surfaces from depth sensor data.",https://ieeexplore.ieee.org/document/7925321/,SoutheastCon 2017,30 March-2 April 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2010.5578869,Towards the Object Semantic Hierarchy,IEEE,Conferences,"An intelligent agent, embedded in the physical world, will receive a high-dimensional ongoing stream of low-level sensory input. In order to understand and manipulate the world, the agent must be capable of learning high-level concepts. Object is one such concept. We are developing the Object Semantic Hierarchy (OSH), which consists of multiple representations with different ontologies. The OSH factors the problems of object perception so that intermediate states of knowledge about an object have natural representations, with relatively easy transitions from less structured to more structured representations. Each layer in the hierarchy builds an explanation of the sensory input stream, in terms of a stochastic model consisting of a deterministic model and an unexplained “noise” term. Each layer is constructed by identifying new invariants from the previous layer. In the final model, the scene is explained in terms of constant background and object models, and low-dimensional pose trajectories of the observer and the foreground objects. The object representations in the OSH range from 2D views, to 2D planar components with 3D poses, to structured 3D models of objects. This paper describes the framework of the Object Semantic Hierarchy, and presents the current implementation and experimental results.",https://ieeexplore.ieee.org/document/5578869/,2010 IEEE 9th International Conference on Development and Learning,18-21 Aug. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROMAN.2017.8172430,"Towards the use of consumer-grade electromyographic armbands for interactive, artistic robotics performances",IEEE,Conferences,"In recent years, gesture-based interfaces have been explored in order to control robots in non-traditional ways. These require the use of systems that are able to track human body movements in 3D space. Deploying Mo-cap or camera systems to perform this tracking tend to be costly, intrusive, or require a clear line of sight, making them ill-adapted for artistic performances. In this paper, we explore the use of consumer-grade armbands (Myo armband) which capture orientation information (via an inertial measurement unit) and muscle activity (via electromyography) to ultimately guide a robotic device during live performances. To compensate for the drop in information quality, our approach rely heavily on machine learning and leverage the multimodality of the sensors. In order to speed-up classification, dimensionality reduction was performed automatically via a method based on Random Forests (RF). Online classification results achieved 88% accuracy over nine movements created by a dancer during a live performance, demonstrating the viability of our approach. The nine movements are then grouped into three semantically-meaningful moods by the dancer for the purpose of an artistic performance achieving 94% accuracy in real-time. We believe that our technique opens the door to aesthetically-pleasing sequences of body motions as gestural interface, instead of traditional static arm poses.",https://ieeexplore.ieee.org/document/8172430/,2017 26th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN),28 Aug.-1 Sept. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BSN.2012.24,Transition Detection and Activity Classification from Wearable Sensors Using Singular Spectrum Analysis,IEEE,Conferences,This paper proposes the use of singular spectrum analysis (SSA) to segment and classify human activities in real time by using an ear-worn Activity Recognition (e-AR) sensor. A similarity measure is calculated using SSA to construct a 3D feature vector from the 3 axes of e-AR signal. An algorithm based on the concept of clustering and buffering is then implemented in order to detect activity transition in real time as subjects perform their daily activities. An incremental subspace learning algorithm based on SSA is also proposed for activity classification. The proposed algorithm is applied to a group of five subjects performing daily activities and the results have shown the effectiveness of the method for transition detection and activity classification.,https://ieeexplore.ieee.org/document/6200556/,2012 Ninth International Conference on Wearable and Implantable Body Sensor Networks,9-12 May 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2014.6856427,Traversability analysis using terrain mapping and online-trained Terrain type classifier,IEEE,Conferences,"Path estimation is a big challenge for autonomous vehicle navigation, especially in unknown, dynamic environments, when road characteristics change often. 3D terrain information (e.g. stereo cameras) can provide useful hints about the traversability cost of certain regions. However, when the terrain tends to be flat and uniform, it is difficult to identify a better path using 3D map solely. In this scenario the use of a priori knowledge on the expected road's visual characteristics can support detection, but it has the drawback of being not robust to environmental changes. This paper presents a path detection method that mixes together 3D mapping and visual classification, trying to learn, in real time, the actual road characteristics. An on-line learning of visual characteristics is implemented to feedback a terrain classifier, so that the road characteristics are updated as the vehicle moves. The feedback data are taken from a 3D traversability cost map, which provides some hints on traversable and non-traversable regions. After several re-training cycles the algorithm converges on a better separation of the path and non-path regions. The fusion of both 3D traversability cost and visual characteristics of the terrain yields a better estimation when compared with either of these methods solely.",https://ieeexplore.ieee.org/document/6856427/,2014 IEEE Intelligent Vehicles Symposium Proceedings,8-11 June 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMBS.1997.757650,Two dimensional interleaved differential k-space sampling for fluoroscopic triggering of contrast-enhanced three dimensional MR angiography,IEEE,Conferences,"The purpose of this work is to demonstrate how 2D differential k-space sampling can be implemented with interleaving of the view (phase encoding) order, providing smoother temporal behavior and no spatial resolution penalty when compared to standard sequential k-space ordering. Implementation in a clinical setting provides a method for real-time bolus tracking and timing for contrast-enhanced 3D MR angiography.",https://ieeexplore.ieee.org/document/757650/,Proceedings of the 19th Annual International Conference of the IEEE Engineering in Medicine and Biology Society. 'Magnificent Milestones and Emerging Opportunities in Medical Engineering' (Cat. No.97CH36136),30 Oct.-2 Nov. 1997,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1997.613994,Two-stage neural network for volume segmentation of medical images,IEEE,Conferences,"We present a system to segment and label CT/MRI brain slices using feature extraction and unsupervised clustering. In this technique, each voxel is assigned a feature pattern consisting of a scaled family of differential geometrical invariant features. The invariant feature pattern is then assigned to a specific region using a two-stage neural network system. The first stage is a self-organizing principal components analysis (SOPCA) network that is used to project the feature vector onto its leading principal axes found by using principal components analysis. This step provides an effective basis for feature extraction. The second stage consists of a self-organizing feature map (SOFM) which will automatically cluster the input vector into different regions. The optimum number of regions (clusters) is obtained by a model fitting approach. Finally, a 3D connected component labeling algorithm is applied to ensure region connectivity. Implementation and performance of this technique are presented. Compared to other approaches, the new system is more accurate in extracting 3D anatomical structures of the brain, and can be adapted to real-time imaging scenarios.",https://ieeexplore.ieee.org/document/613994/,Proceedings of International Conference on Neural Networks (ICNN'97),12-12 June 1997,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043987,UAV Path Planning Based on Biological Excitation Neural Network and Visual Odometer,IEEE,Conferences,"Unmanned aerial vehicle(UAV) have been widely used in military and civil fields due to their compact structure, flexible mobility, low cost and other advantages. With the development of artificial intelligence in recent years, more intelligent and advanced algorithms have appeared, in which machine vision, as an important branch in the field of artificial intelligence, has also been greatly developed. The limitation of space, load, endurance and computing capacity hinders the application of intelligent algorithms on UAV. In the paper a semi-autonomous control platform of the quadrotor UAV was developed and the upper and lower dual control core architecture is implemented. Based on the hardware platform, the improved visual inertia odometer (VIO) and the biological excitation neural network are used to improve the flight performance and the ability of autonomy. To solve the problem of the synchronization for VIO, a cubic spline interpolation function was employed. A biological excitation neural network was extended to solve UAV on-line path planning. It provides an on-board path planning approach for UAV in the 3D world considering the dynamic obstacles. Finally, the feasibility and stability of the designed system were verified by flight experiments.",https://ieeexplore.ieee.org/document/9043987/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME.2012.186,Unsupervised Conversion of 3D Models for Interactive Metaverses,IEEE,Conferences,"A virtual-world environment becomes a truly engaging platform when users have the ability to insert 3D content into the world. However, arbitrary 3D content is often not optimized for real-time rendering, limiting the ability of clients to display large scenes consisting of hundreds or thousands of objects. We present the design and implementation of an automatic, unsupervised conversion process that transforms 3D content into a format suitable for real-time rendering while minimizing loss of quality. The resulting progressive format includes a base mesh, allowing clients to quickly display the model, and a progressive portion for streaming additional detail as desired. Sirikata, an open virtual world platform, has processed over 700 models using this method.",https://ieeexplore.ieee.org/document/6298517/,2012 IEEE International Conference on Multimedia and Expo,9-13 July 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412661,Unsupervised Domain Adaptation for Object Detection in Cultural Sites,IEEE,Conferences,"The ability to detect objects in cultural sites from the egocentric point of view of the user can enable interesting applications for both the visitors and the manager of the site. Unfortunately, current object detection algorithms have to be trained on large amounts of labeled data, the collection of which is costly and time-consuming. While synthetic data generated from the 3D model of the cultural site can be used to train object detection algorithms, a significant drop in performance is generally observed when such algorithms are deployed to work with real images. In this paper, we consider the problem of unsupervised domain adaptation for object detection in cultural sites. Specifically, we assume the availability of synthetic labeled images and real unlabeled images for training. To study the problem, we propose a dataset containing 75244 synthetic and 2190 real images with annotations for 16 different artworks. We hence investigate different domain adaptation techniques based on image-to-image translation and feature alignment. Our analysis points out that such techniques can be useful to address the domain adaptation issue, while there is still plenty of space for improvement on the proposed dataset. We release the dataset at our web page to encourage research on this challenging topic: https://iplab.dmi.unict.it/EGO-CH-OBJ-ADAPT/.",https://ieeexplore.ieee.org/document/9412661/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.00874,Unsupervised Domain Adaptive 3D Detection with Multi-Level Consistency,IEEE,Conferences,"Deep learning-based 3D object detection has achieved unprecedented success with the advent of large-scale autonomous driving datasets. However, drastic performance degradation remains a critical challenge for cross-domain deployment. In addition, existing 3D domain adaptive detection methods often assume prior access to the target domain annotations, which is rarely feasible in the real world. To address this challenge, we study a more realistic setting, unsupervised 3D domain adaptive detection, which only utilizes source domain annotations. 1) We first comprehensively investigate the major underlying factors of the domain gap in 3D detection. Our key insight is that geometric mismatch is the key factor of domain shift. 2) Then, we propose a novel and unified framework, Multi-Level Consistency Network (MLC-Net), which employs a teacher-student paradigm to generate adaptive and reliable pseudo-targets. MLC-Net exploits point-, instance- and neural statistics-level consistency to facilitate cross-domain transfer. Extensive experiments demonstrate that MLC-Net out-performs existing state-of-the-art methods (including those using additional target domain information) on standard benchmarks. Notably, our approach is detector-agnostic, which achieves consistent gains on both single- and two-stage 3D detectors. Code will be released.",https://ieeexplore.ieee.org/document/9710381/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622166,Urban Dynamic Logistics Pattern Mining with 3D Convolutional Neural Network,IEEE,Conferences,"With wide applications of various types of sensors and web-based software, massive location and time related logistics data is available. The massive spatial and temporal data significantly enhances visibility of modem logistics activities, however, integrating continuous time information to explore spatio-temporal pattern is an outstanding challenge. In this research, we develop a data mining method using image-based method to convert massive spatio-temporal information into heatmap based image sequence where spatial data can be embedded into image frames while image sequence represent time domain. By developing architecture of deep learning neural network with three-dimensional (3D) kernels for different convolution layers, 3D convolutional computation can be applied to images with certain temporal depth to extract dynamic spatio-temporal patterns. The method was validated with real taxi data of New York City. Experimental results demonstrated that dynamic logistics patterns could be identified with historical spatio-temporal information. Comparison of clusters generated by different learning methods showed that the proposed method could produce more accurate spatial-temporal pattern.",https://ieeexplore.ieee.org/document/8622166/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV48863.2021.9575140,Urban Traffic Surveillance (UTS): A fully probabilistic 3D tracking approach based on 2D detections,IEEE,Conferences,"Urban Traffic Surveillance (UTS) is a surveillance system based on a monocular and calibrated video camera that detects vehicles in an urban traffic scenario with dense traffic on multiple lanes and vehicles performing sharp turning maneuvers. UTS then tracks the vehicles using a 3D bounding box representation and a physically reasonable 3D motion model relying on an unscented Kalman filter based approach. Since UTS recovers positions, shape and motion information in a three-dimensional world coordinate system, it can be employed to recognize diverse traffic violations or to supply intelligent vehicles with valuable traffic information. We build on YOLOv3 as a detector yielding 2D bounding boxes and class labels for each vehicle. A 2D detector renders our system much more independent to different camera perspectives as a variety of labeled training data is available. This allows for a good generalization while also being more hardware efficient. The task of 3D tracking based on 2D detections is supported by integrating class specific prior knowledge about the vehicle shape. We quantitatively evaluate UTS using self generated synthetic data and ground truth from the CARLA simulator, due to the non-existence of datasets with an urban vehicle surveillance setting and labeled 3D bounding boxes. Additionally, we give a qualitative impression of how UTS performs on real-world data. Our implementation is capable of operating in real time on a reasonably modern workstation. To the best of our knowledge, UTS is to date the only 3D vehicle tracking system in a surveillance scenario (static camera observing moving targets).",https://ieeexplore.ieee.org/document/9575140/,2021 IEEE Intelligent Vehicles Symposium (IV),11-17 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2010.78,Using an Emotional Intelligent Agent to Support Customers' Searches Interactively in e-Marketplaces,IEEE,Conferences,"Emotions play a key role in any purchasing process even more when it is done through web portals. In this paper, we present a fuzzy-logic based system aimed to generate emotions for a virtual seller or avatar. The 3D avatar is integrated within the e-Zoco e-commerce portal and its behaviour depends on the product search results. Moreover, the mood of the avatar not only depend on the products that the portal can offer, but also the recommendations that can be used to guide the customer when beginning a new search. The system provides advice to the clients in order to assist them in the specification of constraints and thus obtain more interesting products in a shorter time.",https://ieeexplore.ieee.org/document/5671435/,2010 22nd IEEE International Conference on Tools with Artificial Intelligence,27-29 Oct. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/YAC53711.2021.9486615,Vehicle Behavior Recognition using Multi-Stream 3D Convolutional Neural Network,IEEE,Conferences,"Vehicle behavior recognition take an essential part in the community of intelligent driving assistance systems. Recent approaches with 3D convolutional neural network (3DCNN) achieve reasonable recognition performance in laboratory settings. However, due to the complexity of network, the model is large and the reasoning is slow, so it is difficult to be applied in practice. In order to better handle the trade-off between size, precision and reasoning speed,a lightweight multi-stream 3DCNN model is proposed in this paper, which achieves fast reasoning speed and small model size while maintaining high precision. The 3DCNN model consists of three parts. Firstly, the module of SELayer-3DCNN is developed to extract appearance information from the RGB image sequence. The motion and edge information are also extracted from the optical flow sequence and edge image sequence, respectively. The edge information is applied to enhance the optical flow features. Secondly, a novel channel attention fusion strategy is proposed to improve the feature fusion and network ability. Finally, a 3D-RFB module is proposed to enhance the receptive field of the convolutional kernel. Furthermore, this paper presents a dataset of vehicle behavior. The advantages of the proposed method are verified by ablation experiments and comparative experiments while the real-time characteristics are maintained.",https://ieeexplore.ieee.org/document/9486615/,2021 36th Youth Academic Annual Conference of Chinese Association of Automation (YAC),28-30 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO.2018.8665155,Vehicle Pose and Shape Estimation Through Multiple Monocular Vision,IEEE,Conferences,"In this paper, we present a method to estimate a vehicle's pose and shape from off-board multi-view images. These images are taken from monocular cameras with small overlaps. We utilize state-of-the-art Convolutional Neural Networks (CNNs) to extract vehicles' semantic keypoints and introduce a Cross Projection Optimization (CPO) method to estimate the 3D pose. During the iterative CPO process, an adaptive shape adjustment method named Hierarchical Wireframe Constraint (HWC) is implemented to estimate the shape. Our approach is evaluated under both simulated and real-world scenes for performance verification. It's shown that our algorithm outperforms other existing monocular and stereo methods for vehicles' pose and shape estimation. This approach provides a new and robust solution for off-board visual vehicle localization and tracking, which can be applied to massive surveillance camera networks for intelligent transportation.",https://ieeexplore.ieee.org/document/8665155/,2018 IEEE International Conference on Robotics and Biomimetics (ROBIO),12-15 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341569,Velocity Regulation of 3D Bipedal Walking Robots with Uncertain Dynamics Through Adaptive Neural Network Controller,IEEE,Conferences,"This paper presents a neural-network based adaptive feedback control structure to regulate the velocity of 3D bipedal robots under dynamics uncertainties. Existing Hybrid Zero Dynamics (HZD)-based controllers regulate velocity through the implementation of heuristic regulators that do not consider model and environmental uncertainties, which may significantly affect the tracking performance of the controllers. In this paper, we address the uncertainties in the robot dynamics from the perspective of the reduced dimensional representation of virtual constraints and propose the integration of an adaptive neural network-based controller to regulate the robot velocity in the presence of model parameter uncertainties. The proposed approach yields improved tracking performance under dynamics uncertainties. The shallow adaptive neural network used in this paper does not require training a priori and has the potential to be implemented on the real-time robotic controller. A comparative simulation study of a 3D Cassie robot is presented to illustrate the performance of the proposed approach under various scenarios.",https://ieeexplore.ieee.org/document/9341569/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2016.7860433,ViZDoom: A Doom-based AI research platform for visual reinforcement learning,IEEE,Conferences,"The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real-world tasks since they involve non-realistic 2D environments and the third-person perspective. Here, we propose a novel test-bed platform for reinforcement learning research from raw visual information which employs the first-person perspective in a semi-realistic 3D world. The software, called ViZDoom, is based on the classical first-person shooter video game, Doom. It allows developing bots that play the game using the screen buffer. ViZDoom is lightweight, fast, and highly customizable via a convenient mechanism of user scenarios. In the experimental part, we test the environment by trying to learn bots for two scenarios: a basic move-and-shoot task and a more complex maze-navigation problem. Using convolutional deep neural networks with Q-learning and experience replay, for both scenarios, we were able to train competent bots, which exhibit human-like behaviors. The results confirm the utility of ViZDoom as an AI research platform and imply that visual reinforcement learning in 3D realistic first-person perspective environments is feasible.",https://ieeexplore.ieee.org/document/7860433/,2016 IEEE Conference on Computational Intelligence and Games (CIG),20-23 Sept. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACOMP53746.2021.00015,Vietnamese vehicles speed detection with video-based and deep learning for real-time traffic flow analysis system,IEEE,Conferences,"In this paper, we have developed a system to leverage traffic surveillance cameras to detect vehicle speed. In this system, we use a detection-based tracking paradigm for multiple object tracking then speed is estimated. First, YOLOv4 with transfer learning is applied for vehicle detection, a comparative analysis is carried out to choose trackers that work well with YOLOv4 in this task. Finally tracked vehicles’ traveled distance is back-projected to the 3D world by Haversine method for speed estimation. In order to deploy to edge device, we take the advantage of tensorRT framework and ONXX technology to optimize models and modify model format as well as accelerate inferencing. For the suitability to the Vietnamese traffic scenario, feature extraction models of tracking task and detection task were fine-tuned. The system is then optimized and modified to detect common Vietnamese vehicles’ speed in normal conditions and low light intensity conditions with the video stream fed directly from the preinstalled traffic surveillance camera. The whole system proceeds AI inference and processing with the help of Nvidia Jetson NX Xavier. All modules are packed into a small box which helps simplify the integration to available traffic cameras. This would release the need for radar and sensors which are usually extremely expensive and need a lot of calibration and maintenance.",https://ieeexplore.ieee.org/document/9668251/,2021 15th International Conference on Advanced Computing and Applications (ACOMP),24-26 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAC54203.2021.9671198,Virtual Dressing Room: Smart Approach to Select and Buy Clothes,IEEE,Conferences,"The clothing industry portrays a major part of a respective country`s economy. Due to the predilection for clothing items of the people have led to the increasing of physical and online clothing stores in all around the world. Most of the people are used to go to the physical shopping and purchase their desired clothing items. But, as a consequence of the current pandemic situation, most of the people are unable to step out from their homes. This application is intended to cater an opportunity to the customers, who are not able to reach the physical clothing stores due to a pandemic situation and mobility difficulties. In addition, this application diminishes the time wastage, clothing size mismatches and the lesser user satisfaction ratio inside a physical clothing store. A customized 3D model has featured in the application to cater the virtual fitting experience to the customer. And the AI chatbot assistant in the application interacts with the user while catering virtual assistance for a better cloth selection process. In addition to that, this application has concentrated on the clothing shop by providing a future sales prediction component utilizing the K-Nearest Neighbors algorithm to provide an aid to their business commitments.",https://ieeexplore.ieee.org/document/9671198/,2021 3rd International Conference on Advancements in Computing (ICAC),9-11 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/OCEANS.1995.528724,Virtual world visualization for an autonomous underwater vehicle,IEEE,Conferences,"A critical bottleneck exists in autonomous underwater vehicle (AUV) design and development. It is tremendously difficult to observe, communicate with and test underwater robots, because they operate in a remote and hazardous environment where physical dynamics and sensing modalities are counterintuitive. An underwater virtual world can comprehensively model all necessary functional characteristics of the real world in real time. This virtual world is designed from the perspective of the robot, enabling realistic AUV evaluation and testing in the laboratory. 3D real-time graphics are our window into the virtual world, enabling multiple observers to visualize complex interactions. A networked architecture enables multiple world components to operate collectively in real time, and also permits world-wide observation and collaboration with other scientists interested in the robot and virtual world.",https://ieeexplore.ieee.org/document/528724/,'Challenges of Our Changing Global Environment'. Conference Proceedings. OCEANS '95 MTS/IEEE,9-12 Oct. 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIE50891.2020.00083,Virtualized circuit welding and simulation experiment system based on Unity3D,IEEE,Conferences,"Circuit welding and simulation experiment is a compulsory course for electronic information students in college. To make students more intuitively learn the principles of circuits and exercise their practical ability, we use Unity3D and 3DMax software to develop an immersive 3D virtual experiment system, and give the system's overall structure design and some key technologies. The system serves as an auxiliary teaching and learning tool, simulating a real experimental environment, and providing necessary conditions for students* autonomous experiments.",https://ieeexplore.ieee.org/document/9262558/,2020 International Conference on Artificial Intelligence and Education (ICAIE),26-28 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigDIA51454.2020.00020,Vision and Language Navigation using Multi-head Attention Mechanism,IEEE,Conferences,"In the wake of the developments of deep learning, more and more research focus on the intersection fields of natural language processing and machine vision, vision and language navigation (VLN) is one of them. The VLN task needs an embodied agent to carry out the natural language instruction and navigate inside a real 3D environment with the help of visual information, planning a trajectory from start point to goal location. In this paper, inspired by the previous works, we introduce a multi-head attention module with a parallel attention computing method which apply multi-head attention mechanism on visual and textual input to enhance the performance of the model. Specifically, first we design a multihead attention module with trainable parameters, which can extract associated attention from the textual and visual information, extracted attention variables are used to promote the agent aware of which part of the sentence or image is more important. Second, in order to help the agent perceives more useful information, we perform a parallel computing method on extracted attention and input, i.e. visual and textual input feature, then use a layer normalization to combine them. The results of experiments indicate our proposed module enables the model to obtain better performance and surpasses the baseline model. The success rate of our model is 51% and oracle success rate is 62% with low navigation error.",https://ieeexplore.ieee.org/document/9384528/,2020 6th International Conference on Big Data and Information Analytics (BigDIA),4-6 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2019.8794123,Visual Guidance and Automatic Control for Robotic Personalized Stent Graft Manufacturing,IEEE,Conferences,"Personalized stent graft is designed to treat Abdominal Aortic Aneurysms (AAA). Due to the individual difference in arterial structures, stent graft has to be custom made for each AAA patient. Robotic platforms for autonomous personalized stent graft manufacturing have been proposed in recently which rely upon stereo vision systems for coordinating multiple robots for fabricating customized stent grafts. This paper proposes a novel hybrid vision system for real-time visual-sevoing for personalized stent-graft manufacturing. To coordinate the robotic arms, this system is based on projecting a dynamic stereo microscope coordinate system onto a static wide angle view stereo webcam coordinate system. The multiple stereo camera configuration enables accurate localization of the needle in 3D during the sewing process. The scale-invariant feature transform (SIFT) method and color filtering are implemented for stereo matching and feature identifications for object localization. To maintain the clear view of the sewing process, a visual-servoing system is developed for guiding the stereo microscopes for tracking the needle movements. The deep deterministic policy gradient (DDPG) reinforcement learning algorithm is developed for real-time intelligent robotic control. Experimental results have shown that the robotic arm can learn to reach the desired targets autonomously.",https://ieeexplore.ieee.org/document/8794123/,2019 International Conference on Robotics and Automation (ICRA),20-24 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASCON51689.2021.9563527,Visual Perception Stack for Autonomous Vehicle Using Semantic Segmentation and Object Detection,IEEE,Conferences,"An autonomous vehicle requires a reliable system for high vehicle precision and relative estimation of its state for the safety of humans during the autonomous movement of the vehicle in an environment dominated by human drivers. Such systems have a complex environment involving multiple sensors (e.g. Vision modules, Global Navigation Satellite System (GNSS), LIDAR, RADAR). Through this paper, environment perception stack for self-driving cars is proposed to improve the intelligence for decision making and improve the safety measures. Semantic image segmentation, based on Fully convolutional Network architecture is implemented and the output received from the model is then used for implementing 3D space estimation and lane estimation. Considering the real-time cooperation required between the autonomous vehicles and other vehicles in the frame, a 2D object detector is implemented on the stack to detect different classes of objects and their relative distances are calculated. The proposed system is then implemented on the CARLA simulation software and generated outcomes are further discussed in the paper.",https://ieeexplore.ieee.org/document/9563527/,2021 IEEE Madras Section Conference (MASCON),27-28 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2016.7860428,Voluntary behavior on cortical learning algorithm based agents,IEEE,Conferences,"Operating autonomous agents inside a 3D workspace is a challenging problem domain in real-time for dynamic environments since it involves online interaction with ever-changing decision constraints. This study proposes a neuroscience inspired architecture to simulate autonomous agents with interaction capabilities inside a 3D virtual world. The environment stimulates the operating agents based on their place and course of action. They are expected to form a life cycle composed of behavior chunks inside this environment and continuously optimize it around the stimulated reward. The architecture is composed of specialized units that run Cortical Learning Algorithm (CLA) which models functional properties of layers II and III as in six layer theory of neocortex. This work focuses on extending it with functional properties of layers IV, V and basal ganglia to obtain voluntary behavior that is suitable for an autonomous agent. Through experimental scenarios, the architecture is observed and evaluated in order to obtain an apparent learning process. The communication between layers and internal connectivity of embedded CLA units are able to capture sequential and causal relations from the environment and the first evaluation of the implementation has high potential for future directions.",https://ieeexplore.ieee.org/document/7860428/,2016 IEEE Conference on Computational Intelligence and Games (CIG),20-23 Sept. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITME.2016.0198,Web3D Online Virtual Education System for Historical Battle Teaching,IEEE,Conferences,"In this paper, we establish a set of online web3D education system for history teaching, especially for historical battle scenario. Our system involves a wide range of technologies, including web3D, 3D model lightweighting and AI technology. We present a new 3D scene management strategy, called Voxel of interesting(VOI) scene management strategy. With experiment and contrast, we find a lot of students who used our system become more active and happy in history class.",https://ieeexplore.ieee.org/document/7976612/,2016 8th International Conference on Information Technology in Medicine and Education (ITME),23-25 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2007.4409082,What Can Casual Walkers Tell Us About A 3D Scene?,IEEE,Conferences,"An approach for incremental learning of a 3D scene from a single static video camera is presented in this paper. In particular, we exploit the presence of casual people walking in the scene to infer relative depth, learn shadows, and segment the critical ground structure. Considering that this type of video data is so ubiquitous, this work provides an important step towards 3D scene analysis from single cameras in readily available ordinary videos and movies. On-line 3D scene learning, as presented here, is very important for applications such as scene analysis, foreground refinement, tracking, biometrics, automated camera collaboration, activity analysis, identification, and real-time computer-graphics applications. The main contributions of this work are then two-fold. First, we use the people in the scene to continuously learn and update the 3D scene parameters using an incremental robust (L1) error minimization. Secondly, models of shadows in the scene are learned using a statistical framework. A symbiotic relationship between the shadow model and the estimated scene geometry is exploited towards incremental mutual improvement. We illustrate the effectiveness of the proposed framework with applications in foreground refinement, automatic segmentation as well as relative depth mapping of the floor/ground, and estimation of 3D trajectories of people in the scene.",https://ieeexplore.ieee.org/document/4409082/,2007 IEEE 11th International Conference on Computer Vision,14-21 Oct. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FG.2019.8756520,When Computers Decode your Social Intention,IEEE,Conferences,"In this demo session, we will propose our framework that is based on our paper [1] . In real time, we proposed to analyze the trajectories of the human arm to predict social intention (personal or social intention). The trajectories of different 3D markers acquired by Mocap system are defined in shape spaces of open curves, thus analyze in a Riemannian manifold. The results obtained in the experiments on a new dataset show an average recognition of about 68% for the proposed method, which is comparable with the average score produced by human evaluation. The experimental results show also that the classification rate could be used to improve social communication between human and virtual agents. To the best of our knowledge, this is the first demo in real time, which uses computer vision techniques to analyze the effect of social intention on motor action for improving the social communication between human and avatar. The main goal is to categorize the user intention among two classes denote {personal, social}. This experimentation contains 3 parts: a) data acquisitions and a learning step; b) classification; c) Kinematic analysis of the evolution of subjects to interact with the avatar. To successfully drive our study, all the using scripts are writing under Matlab and C/C++. Then the using equipments are: 1) Qualisys motion capture camera (qualisys system). The qualisys system is delivered with a desk computer with 8 GB, a processor Intel core i7-4770k (8 CPUs) at 3.5 GHz. The frequency of those cameras can varies from 100 to 500 Hz. A black glove equipped with infrared reflective markers, all those equipments are also provided by qualisys system. 2) A Matlab software (version R2014a) installed on a desk computer (qualisys system); the Qualisys system provide a specific driver that allow to couple all the Matlab scripts with their system. Thus, it is possible to command all the cameras directly from Matlab for real time analysis, see Fig. 1 .",https://ieeexplore.ieee.org/document/8756520/,2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019),14-18 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR.2009.5336494,Wide area localization on mobile phones,IEEE,Conferences,"We present a fast and memory efficient method for localizing a mobile user's 6DOF pose from a single camera image. Our approach registers a view with respect to a sparse 3D point reconstruction. The 3D point dataset is partitioned into pieces based on visibility constraints and occlusion culling, making it scalable and efficient to handle. Starting with a coarse guess, our system only considers features that can be seen from the user's position. Our method is resource efficient, usually requiring only a few megabytes of memory, thereby making it feasible to run on low-end devices such as mobile phones. At the same time it is fast enough to give instant results on this device class.",https://ieeexplore.ieee.org/document/5336494/,2009 8th IEEE International Symposium on Mixed and Augmented Reality,19-22 Oct. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9413066,Yolo+FPN: 2D and 3D Fused Object Detection With an RGB-D Camera,IEEE,Conferences,"In this paper we propose a new deep neural network system, called Yolo+FPN, which fuses both 2D and 3D object detection algorithms to achieve better real-time object detection results and faster inference speed, to be used on real robots. Finding an optimized fusion strategy to efficiently combine 3D object detection with 2D detection information is useful and challenging for both indoor and outdoor robots. In order to satisfy real-time requirements, a trade-off between accuracy and efficiency is needed. We not only have improved training and test accuracies and lower mean losses on the KITTI object detection benchmark comparing with our baseline method, but also achieve competitive average precision on 3D detection of all classes in three levels of difficulty comparing with other state-of-the-art methods. Also, we implemented Yolo+FPN system using an RGB-D camera, and compared the speed of object detection using different GPUs. For the real implementation of both indoor and outdoor scenes, we focus on person detection, which is the most challenging and important among the three classes.",https://ieeexplore.ieee.org/document/9413066/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops53856.2022.9767292,You Work We Care: Sitting Posture Assessment Based on Point Cloud Data,IEEE,Conferences,"The technology of 3D recognition is evolving rapidly, enabling novel applications towards human-centric intelligent environments. On top of these applications, tracking human sitting posture is essential for realizing human comfort and healthy environments. However, existing techniques rely on cameras or chair-attached sensors, which are privacy-invading or non-common technology in every environment. This paper introduces a ubiquitous portable technology for tracking sitting posture with a plug-and-play concept. Specifically, at the core of the proposed system, we leverage our proprietary LiDAR device to scan the human&#x2019;s sitting posture and render it in a privacy-keeping point cloud representation.The capture point cloud samples are leveraged to train an efficient deep neural network for enabling accurate recognition of the sitting posture. The proposed network significantly reduces the computational complexity of the model by learning special features that simplify the classification task. We implemented and evaluated the proposed system on nine different human postures in a real-world environment. The results show that it obtains an accuracy of 87% with a drastically reduced processing time.",https://ieeexplore.ieee.org/document/9767292/,2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops),21-25 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWACI.2011.6159663,[Front and back cover],IEEE,Conferences,The following topics are dealt with: advanced computational intelligence; rough set; genetic algorithm; distributed computing; flexible job-shop scheduling; system-of-systems vulnerability analysis; V-BLAST sphere decoding; spanning tree problem; object representation model; real time path planning; ant colony optimization; multi-objective evolutionary algorithm; RBF neural network; particle swarm optimization; multi-color extraction method; adaptive NN tracking control; gravitational chaotic search algorithm; approximate optimal control tracking; multi-target tracking approach; generalize disjunctive paraconsistent data model; discriminative item mining; weighted passive nearest neighbor algorithm; structure-encoding differential evolution algorithm; real-time data stream clustering; event-driven control program automatic verification; threshold signature scheme; data model driven architecture; improved post-nonlinear independent component analysis; pruning algorithm; remote sensing image classification; fuzzy matrices; grey-box neural network; training ANFIS system; delay BAM neural network stability; tuning method; ensemble learning balancing; Kalman filtering; hybrid learning model; multi-focus image fusion; object-based image retrieval; language grounding model; decision fusion; functional network analysis; image interpolation; image deblurring method; discrete-time dynamic system stability; feature selection methods; Lasso logistic regression; vehicle scheduling; cooperative air-defense system; bifurcation analysis; close-loop time-delayed filter system; Newton iteration formula; 3D object recognition; parameter identification; intelligent displacement back-analysis method; single machine total weighted tardiness scheduling problem; dimensionality reduction method; temporal Bayesian network; online leasing problem; network supported intelligent cooperative diagnosis; Hopf bifurcation analysis; multi-mode human-machine interface; GA-fuzzy automatic generation controller; tele-operation robot system; hybrid clonal selection algorithm; modified LEACH protocol; fuzzy Lyapunov synthesis; and underwater vehicle.,https://ieeexplore.ieee.org/document/6159663/,The Fourth International Workshop on Advanced Computational Intelligence,19-21 Oct. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00058,richYoga: An Interactive Yoga Recognition System Based on Rich Skeletal Joints,IEEE,Conferences,"In this demo paper, a yoga action recognition system is proposed. Based on the detected 3D positions of the rich skeletal joints provided by the face API, pose API, and hands API from MediaPipe, 543 skeletal joints are used train an LSTM neural network model to classify four yoga actions, including Half Wind Blown, Warrior II, Triangle, and Default. According to the generated yoga action classifier with 85% accuracy, a VR yoga tutor is implemented in the Unity environment to guide a user.",https://ieeexplore.ieee.org/document/9644307/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.01421,ELF-VC: Efficient Learned Flexible-Rate Video Coding,IEEE,Conferences,"While learned video codecs have demonstrated great promise, they have yet to achieve sufficient efficiency for practical deployment. In this work, we propose several novel ideas for learned video compression which allow for improved performance for the low-latency mode (I- and P-frames only) along with a considerable increase in computational efficiency. In this setting, for natural videos our approach compares favorably across the entire R-D curve under metrics PSNR, MS-SSIM and VMAF against all mainstream video standards (H.264, H.265, AV1) and all ML codecs. At the same time, our approach runs at least 5x faster and has fewer parameters than all ML codecs which report these figures.Our contributions include a flexible-rate framework allowing a single model to cover a large and dense range of bitrates, at a negligible increase in computation and parameter count; an efficient backbone optimized for ML-based codecs; and a novel in-loop flow prediction scheme which leverages prior information towards more efficient compression.We benchmark our method, which we call ELF-VC (Efficient, Learned and Flexible Video Coding) on popular video test sets UVG and MCL-JCV under metrics PSNR, MS-SSIM and VMAF. For example, on UVG under PSNR, it reduces the BD-rate by 44% against H.264, 26% against H.265, 15% against AV1, and 35% against the current best ML codec. At the same time, on an NVIDIA Titan V GPU our approach encodes/decodes VGA at 49/91 FPS, HD 720 at 19/35 FPS, and HD 1080 at 10/18 FPS.",https://ieeexplore.ieee.org/document/9710729/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS.2018.8624056,EMG-based hand gesture control system for robotics,IEEE,Conferences,"In this paper, a Electromyogram (EMG) based hand gesture control system is developed. A wearable human machine interface (HMI) device is designed for an in-home assistance service robot. An EMG-based control system utilizes MyoWave muscle sensor to acquire and amplify EMG signal. A microcontroller system is used to an artificial neural network (ANN) to classify the EMG signal. Based on different hand movements, commands are sent through WiFi to control the motor in a service robot. The on-board Camera system mounted the robot can capture video real-time. In addition, a web server is implemented to provide live video feedback for robot navigation and user instructions.",https://ieeexplore.ieee.org/document/8624056/,2018 IEEE 61st International Midwest Symposium on Circuits and Systems (MWSCAS),5-8 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2013.142,EPG Content Recommendation in Large Scale: A Case Study on Interactive TV Platform,IEEE,Conferences,"Recommender systems in TV applications mostly focusing on the recommendation of video-on-demand (VOD) content, though the major part of users' content consumption is realized on linear channel programs, termed EPG content. In this case study we present how we tackled the EPG recommendation task, which exhibits several differences compared to the VOD scenario, including the lack of explicit user feedbacks, the magnitude of cold start problem, as well as data cleaning and feature selection necessary to be applied on raw consumption data. We provide both offline and online model validation. First we showcase the typical approach in machine learning by evaluating models against recall in an offline setting. Then, we investigate in depth the real-world results of the recommendation app using the pre-trained models, and analyze how personalized recommendation influence users watching behavior. The experimentation results are based on our recommender system deployed at a Canadian IPTV service provider using Microsoft Media room middleware.",https://ieeexplore.ieee.org/document/6786127/,2013 12th International Conference on Machine Learning and Applications,4-7 Dec. 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ANDESCON50619.2020.9272089,Ecuadorian traffic sign detection through color information and a convolutional neural network,IEEE,Conferences,"This article develops an algorithm for the detection and recognition of regulatory and warning traffic signs of Ecuador. The method consists of the following steps, i) video stabilization to reduce vertical oscilation, ii) implementation of a new method based on color segmentation and a convolutional neural network. Another important contribution is a new database of regulatory and prevention traffic signs of Ecuador. This dataset is with more than 37500 images of Ecuadorian road signs divided in 52 classes; these images were taken from different angles and weather conditions. The final version of this proposal runs at 22 frames per second, and it was tested in real driving conditions, in several roads and highways of Ecuador, during the day.",https://ieeexplore.ieee.org/document/9272089/,2020 IEEE ANDESCON,13-16 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IECON.2018.8591087,Edge Assisted Efficient Data Annotation for Realtime Video Big Data,IEEE,Conferences,"There is a lack of efficient video data labeling mechanism for real-time applications. Most of the labeling solutions are designed for big data offline video analytics, however emerging real-time applications with video analytics and the data-centric global trend require real-time video analytics with real-time labeling/annotation. In this paper, we present a solution that provides a per frame custom metadata that can be easily encoded and decoded and overlaid with the video frame for labeling the relevant objects/scenes. The presented solution is implemented and tested in a pilot and is leveraging edge computing capabilities to minimize the cost of using the cloud (in terms of latency and additional network resources).",https://ieeexplore.ieee.org/document/8591087/,IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society,21-23 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEC50012.2020.00017,Edge Compression: An Integrated Framework for Compressive Imaging Processing on CAVs,IEEE,Conferences,"Machine vision is the key to the successful deployment of many Advanced Driver Assistant System (ADAS) / Automated Driving System (ADS) functions, which require accurate high-resolution video processing in a real-time manner. Conventional approaches are either to reduce the frame rate or reduce the related frame size of the conventional camera videos, which lead to undesired consequences such as losing informative high-speed information and/or small objects in the video frames.Unlike conventional cameras, Compressive Imaging (CI) cameras are the promising implications of Compressive Sensing, which is an emerging field with the revelation that the optical domain compressed signal (a small number of linear projections of the original video image data) contains sufficient high-speed information for reconstruction and processing. Yet, CI cameras usually need complicated algorithms to retrieve the desired signal, leading to the corresponding high energy consumption. In this paper, we take a step further to the real applications of CI cameras in connected and autonomous vehicles (CAVs), with the primary goal of accelerating accurate video analysis and decreasing energy consumption. We propose a novel Vehicle Edge Server-Cloud closed-loop framework called Edge Compression for CI processing on CAVs. Our comprehensive experiments with four public datasets demonstrate that the detection accuracy of the compressed video images (named measurements) generated by the CI camera is close to the accuracy on reconstructed videos and comparable to the true value, which paves the way of applying CI in CAVs. Finally, six important observations with supporting evidence and analysis are presented to provide practical implications for researchers and domain experts. The code to reproduce our results is available at https://www.thecarlab.oryoutcomes/software.",https://ieeexplore.ieee.org/document/9355642/,2020 IEEE/ACM Symposium on Edge Computing (SEC),12-14 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICACT51234.2021.9370761,Edge Computing System applying Integrated Object Recognition based on Deep Learning,IEEE,Conferences,"With the development of personal broadcasting such as YouTube, the demand for editing the video he filmed himself is steadily increasing. Traditional media video editing solutions help edit the results of the filming. However, there is a problem that requires a lot of editing time, as people usually edit videos that are from tens to hundreds of times longer than the final result of a personal broadcasting after filming. To overcome this problem, this paper automatically classifies images with specific scenes in the whole media image editing process, and secondly proposes automatic media editing solution technology in which people intervene. In particular, personal broadcasting focuses on the use of images that include characters, specific objects, and cue sign gestures among the entire. While the existing deep learning techniques such as faces, objects and gestures are advanced, integrated recognition technologies that simultaneously deal with special requirements for editing videos are still in the early stages of research. In this paper, the automatic composite recognition technology for editing video based on deep learning is proposed. The proposed technology was implemented with python and tensorflow software based on edge computing equipment. Using actual youtube videos, it took 0.1 second to process five-person recognition, 63-food recognition, or cue sign recognition using clapping or V poses at the same time. The recognized results are divided into timestamps of the entire movie, recognition results, and locations of objects on the screen, and are output to the json file. In addition, this solution was developed on an edge computing in order to increase real-time reliability. We expect to provide automatic video editing based on perceived json results as well as shorter editing times based on this implementation.",https://ieeexplore.ieee.org/document/9370761/,2021 23rd International Conference on Advanced Communication Technology (ICACT),7-10 Feb. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISC253183.2021.9562877,Edge Computing-Enabled Crowd Density Estimation based on Lightweight Convolutional Neural Network,IEEE,Conferences,"In public areas, crowd stampedes and incidents generate huge negative impacts on public security. Accurate and efficient crowd density estimation is critical to monitor crowd status for developing evacuation strategies. The existing crowd density estimation methods are established based on complex deep-learning algorithms which are usually more accurate, but, on the other side, they require much more computational resources. Consequently, cloud-computing is the only option for deploying crowd density estimation algorithms, which needs tremendous resources for real-time video data transmission and the malfunction and delay of internet service may cause wrong and delayed estimation results. Edge computing is a novel concept of accomplishing computing tasks only relying on the computational resources of edge devices. Estimating crowd density on edge devices rather than conveying images to cloud server for further analysis has several advantages, including 1) reducing network bandwidth pressure from remote transmission; 2) avoiding risks of leaking privacy on images; and 3) improving computation efficient. This study designs an edge computing-enabled crowd density estimation model based on the residual bottleneck block and dilated convolution. The experiments are designed and conducted on public crowd data sets to verify accuracy, storage cost and computation efficiency of our model. According to the experimental results, the proposed model achieves a considerable improvement in operational efficiency, while keep the accuracy at the same level with the complex deep-learning algorithms. Furthermore, the proposed model is implemented on a real edge device to detect real-world crowd density in a Beijing subway station.",https://ieeexplore.ieee.org/document/9562877/,2021 IEEE International Smart Cities Conference (ISC2),7-10 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNC.2019.8885803,Edge-assisted Adaptive Video Streaming with Deep Learning in Mobile Edge Networks,IEEE,Conferences,"Most HTTP Adaptive Streaming (HAS) video content delivery solutions today are governed by purely client-based logics. For lack of coordination among clients and awareness of the dynamic Radio Access Network (RAN) conditions, these approaches may lead to suboptimal user experience and underutilization of network resources. Recently, Mobile Edge Computing (MEC) has been studied as a new networking paradigm to play a part in the adaptive video streaming process with lower end-to-end latency and better network awareness. In this paper, we present an edge-assisted adaptive video streaming scheme with deep Q-learning techniques and bandwidth sharing policies, which performs video adaptation according to the real-time radio and network information collected at the network edge. The adaptation scheme is implemented as an edge application hosted on our own deployed MEC server in a real LTE network testbed for experimental evaluation against three popular client-based solutions, namely Buffer-Based Adaptation (BBA), Rate-Based Adaptation (RBA) and adaptation performed by dash.js with its default adaptation logic. To the best of our knowledge, it is one of the few deep Q-learning adaptive video streaming solutions that have been deployed in the MEC framework at network edge in practice. Experiment results show that our proposed scheme outperforms the other three client-based solutions with higher Quality of Experience (QoE) and fairness in the evaluated network environments.",https://ieeexplore.ieee.org/document/8885803/,2019 IEEE Wireless Communications and Networking Conference (WCNC),15-18 April 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM42981.2021.9488741,Edge-assisted Online On-device Object Detection for Real-time Video Analytics,IEEE,Conferences,"Real-time on-device object detection for video analytics fails to meet the accuracy requirement due to limited resources of mobile devices while offloading object detection inference to edges is time-consuming due to the transference of video data over edge networks. Based on the system with both on-device object tracking and edge-assisted analysis, we formulate a non-linear time-coupled program over time, maximizing the overall accuracy of object detection by deciding the frequency of edge-assisted inference, under the consideration of both dynamic edge networks and the constrained detection latency. We then design a learning-based online algorithm to adjust the threshold for triggering edge-assisted inference on the fly in terms of the object tracking results, which essentially controls the deviation of on-device tracking between two consecutive frames in the video, by only taking previously observable inputs. We rigorously prove that our approach only incurs sub-linear dynamic regret for the optimality objective. At last, we implement our proposed online schema, and extensive testbed results with real-world traces confirm the empirical superiority over alternative algorithms, in terms of up to 36% improvement on detection accuracy with ensured detection latency.",https://ieeexplore.ieee.org/document/9488741/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCGrid51090.2021.00071,Edge4Emotion: An Edge Computing based Multi-source Emotion Recognition Platform for Human-Centric Software Engineering,IEEE,Conferences,"Human emotion recognition has been widely used and intensively studied in many areas such as e-Commerce, online education, healthcare, human-computer interaction and recently human-centric software engineering (HCSE). HCSE investigates human factors in the entire software development lifecycle, and human emotion can be used in many scenarios such as requirement gathering and usability testing. However, even though existing studies have already shown the advantages of emotion recognition with multi-source data such as text, audio and video, the current research and practice in HCSE are primarily based on single-source data. In addition, emotion recognition in HCSE faces several challenges such as multiple participants, changing environments and real-time requirement. To tackle these challenges, this paper proposes Edge4Emotion, a novel edge computing-based multisource human emotion recognition platform for HCSE. Edge4Emotion takes the advantage of the edge computing paradigm to efficiently support the collection of multi-source data such as audio, video and physiological data from various IoT devices, and emotion recognition with both single- and multisource models. As an on-going project, this paper focuses on the platform design and the preliminary evaluation of the platform with representative emotion recognition applications. The platform will be further extended to include more multi-source learning models and serve as an open-source platform for the development and evaluation of multi-source emotion recognition models for HCSE.",https://ieeexplore.ieee.org/document/9499694/,"2021 IEEE/ACM 21st International Symposium on Cluster, Cloud and Internet Computing (CCGrid)",10-13 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INOCON50539.2020.9298211,Efficient Min-Cost Real Time Action Recognition using Pose Estimates,IEEE,Conferences,"Human Activity Recognition (HAR) is an interesting area of research in the field of computer vision and has been implemented using various techniques. In this approach, we try to identify the various kinds of action being performed by a user and then classify them to provide the results. Current architecture which include RNNs and 3D CNNs are computationally expensive and cannot be trained with the small datasets currently available. Multi person action recognition has been performed in order to understand the positions and action of people present in the video frame. The size of the video frame can be adjusted as a hyper-parameter depending on the hardware resources available. OpenPose has been used to calculate pose estimate using CNN to produce heap-maps, one of these heap maps provide skeleton features which are basically joint features. The features are then extracted and a classification algorithm can be applied to classify the action.",https://ieeexplore.ieee.org/document/9298211/,2020 IEEE International Conference for Innovation in Technology (INOCON),6-8 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA51294.2020.00030,Efficient and Compact Convolutional Neural Network Architectures for Non-temporal Real-time Fire Detection,IEEE,Conferences,"Automatic visual fire detection is used to complement traditional fire detection sensor systems (smoke/heat). In this work, we investigate different Convolutional Neural Network (CNN) architectures and their variants for the non-temporal real-time bounds detection of fire pixel regions in video (or still) imagery. Two reduced complexity compact CNN architectures (NasNet-A-OnFire and ShuffleNetV2-OnFire) are proposed through experimental analysis to optimise the computational efficiency for this task. The results improve upon the current state-of-the-art solution for fire detection, achieving an accuracy of 95% for full-frame binary classification and 97% for superpixel localisation. We notably achieve a classification speed up by a factor of 2.3× for binary classification and 1.3× for superpixel localisation, with runtime of 40 fps and 18 fps respectively, outperforming prior work in the field presenting an efficient, robust and real-time solution for fire region detection. Subsequent implementation on low-powered devices (Nvidia Xavier-NX, achieving 49 fps for full-frame classification via ShuffleNetV2-OnFire) demonstrates our architectures are suitable for various real-world deployment applications.",https://ieeexplore.ieee.org/document/9356284/,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),14-17 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2008.4761328,Efficient background modeling through incremental Support Vector Data Description,IEEE,Conferences,"Background modeling is an essential and important part of many high-level video processing applications. Recently, the Support Vector Data Description (SVDD) has been introduced for novelty detection when only one class of data is available, i.e. background pixels. This paper proposes a method to efficiently train an SVDD and compares the performance of this training algorithm with the traditional SVDD training techniques. We compare the performance of our method with traditional SVDD and other classification algorithms on various data sets including real video sequences.",https://ieeexplore.ieee.org/document/4761328/,2008 19th International Conference on Pattern Recognition,8-11 Dec. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MILCOM.1999.821401,Efficient military intelligence transmission system for high quality video over mobile channels,IEEE,Conferences,"A problem faced in the military intelligence transmission systems is the protection of highly sensitive real-time video information against multipath fading environments. The technique proposed is the implementation of M-level QAM in association with a novel fade estimation and compensation technique. A half-pilot symbol assisted modulation scheme, HP-SAM, a Reed-Solomon error correction coding as well as a non-uniform partitioning of the signal constellation are used. They result in considerable improvements in the bit and frame error rate performance. It is essential that the video is encoded with high compression rate. In this paper, we also proposed a highly efficient interframe coding technique that uses transmission via separate channels, each having differing degrees of error protection. The main focus of this paper is on the transmission aspects of the scheme and the performance enhancement they can provide.",https://ieeexplore.ieee.org/document/821401/,MILCOM 1999. IEEE Military Communications. Conference Proceedings (Cat. No.99CH36341),31 Oct.-3 Nov. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2004.1384539,Efficient motion estimation using two-phase algorithm,IEEE,Conferences,"This work introduces a fast block-based motion estimation algorithm named two-phase motion estimation algorithm (TPMEA). The idea of the algorithm is to divide motion estimation procedure into two-phase: in the first phase, the sum of pixels in a line (or a column) is used as the 1D vector; In the second phase, a filtering threshold is deduced from the 1D vector matching, as Minkowski inequality has shown, blocks cannot match well if their corresponding 1D vector do not match well. Hence, the expensive 2D block matching need only be performed in a limited scope, which saves a lot of time. Moreover, an efficient implementation to calculate 1D vector is presented to speed up encoder further. From the experiment conducted in the test model of H.264(version:JM61d), the algorithm presented in This work can save approximately 30% of the time compared with the exhaustive search algorithm and achieve the same image quality. It can satisfy the demand for high performance and real-time video encoding.",https://ieeexplore.ieee.org/document/1384539/,Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826),26-29 Aug. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/W-FiCloud.2018.00026,Embedded Fatigue Detection Using Convolutional Neural Networks with Mobile Integration,IEEE,Conferences,"Fatigued or drowsy drivers pose a significant risk of causing life-threatening accidents. Yet, many sleep-deprived drivers are behind the wheels exposing lives to danger. In this paper, we propose a low-cost and real-time embedded system for fatigue detection using convolutional neural networks (CNN). Our system starts by spatially processing the video signal using a real-time face detection algorithm to establish a region of interest and reduce computations. The video signal comes from a camera module mounted on the car dashboard connected to an embedded Linux board set to monitor the driver's eyes. Detected faces are then passed to an optimized fatigue recognition CNN binary classifier to detect the event of fatigued or normal driving. When temporally persistent fatigue is detected, alerts are sent to the driver's smart phone, and to possibly others, for prevention measures to be taken before accidents happen. Our testing shows that the system can robustly detect fatigue and can effectively be deployed to address the problem.",https://ieeexplore.ieee.org/document/8488186/,2018 6th International Conference on Future Internet of Things and Cloud Workshops (FiCloudW),6-8 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISSE49702.2020.9121051,Embedded Image Analysis System Based on B-ANN,IEEE,Conferences,"An increasingly common requirement is related to image analysis and automatic extraction of events “in situ” - on the place where image is acquired - without the need to send the image remotely for analysis. There are integrated systems in the video cameras that allow the analysis of images especially in applications for perimeter monitoring and there is research done for different analysis systems that can be integrated near the camera. However, these existing solutions either do not allow high levels of analysis or, if they allow, involve the use of expensive and difficult to implement technologies. In this paper we present a solution, based on B-ANN that allows image analysis, classification and extraction of events of different types. The proposed solution can be implemented on a FPGA circuit for commercial use and allows, through the digital and analog interfaces, the connection to different types of cameras.",https://ieeexplore.ieee.org/document/9121051/,2020 43rd International Spring Seminar on Electronics Technology (ISSE),14-15 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RCAR47638.2019.9043931,Embedded UAV Real-Time Visual Object Detection and Tracking,IEEE,Conferences,"The use of camera-equipped Unmanned Aerial Vehicles (UAVs, or “drones”) for a wide range of aerial video capturing applications, including media production, surveillance, search and rescue operations, etc., has exploded in recent years. Technological progress has led to commercially available UAVs with a degree of cognitive autonomy and perceptual capabilities, such as automated, on-line detection and tracking of target objects upon the captured footage. However, the limited computational hardware, the possibly high camera-to-target distance and the fact that both the UAV/camera and the target(s) are moving, makes it challenging to achieve both high accuracy and stable real-time performance. In this paper, the current state-of-the-art on real-time object detection/tracking is overviewed. Additionally, a relevant, modular implementation suitable for on-drone execution (running on top of the popular Robot Operating System) is presented and empirically evaluated on a number of relevant datasets. The results indicate that a sophisticated, neural network-based detection and tracking system can be deployed at real-time even on embedded devices.",https://ieeexplore.ieee.org/document/9043931/,2019 IEEE International Conference on Real-time Computing and Robotics (RCAR),4-9 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MIPR49039.2020.00055,End-Edge-Cloud Collaborative System: A Video Big Data Processing and Analysis Architecture,IEEE,Conferences,"In the current surveillance system, video streams are firstly captured and compressed at the cameras, and then transmitted to the backend severs or cloud for big data analysis. It is impractical to aggregate all video streams from hundreds of thousands of cameras for big data analysis. Transcoding the videos to low-bitrate ones is the conventional solution to solve the aggregation bottleneck. However, it is recognized that transcoding will inevitably affect visual feature extraction, consequently degrading the subsequent analysis performance. To address these challenges, we thus propose a new video big data analysis framework, called end-edge-cloud collaborative system. Under the end-edge-cloud collaborative framework, a camera can output two streams simultaneously, including a compressed video stream for viewing and data storage, and a compact feature stream extracted from the original video signals for visual analysis. Video stream and feature stream are synchronized by unified identification. We identify three key technologies to enable the end-edge-cloud collaborative system, including analysis-friendly video coding, visual feature compact descriptor, and user-defined neural network and parameter updating. By real-time feeding only the feature streams into the cloud center, these cameras thus form a large-scale brain-like vision system for the smart city. A prototype has been implemented to demonstrate its feasibility. Experiment results show that our system can achieve high efficient video compression and guarantee the analysis performance. Furthermore, our system makes the big data analysis feasible which only need aggregate low bit-rate compressed feature stream.",https://ieeexplore.ieee.org/document/9175568/,2020 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),6-8 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIST52614.2021.9440586,End-to-end Network Embedding Unsupervised Key Frame Extraction for Video-based Person Re-identification,IEEE,Conferences,"At present, regarding the task of video-based person re-identification, the input sequences have subtle differences and large redundancies because there are not enough effective interventions in the extraction of frame sequences. Although some studies have mentioned that key frame should be extracted first, they have not jointed the key frame extraction and the person re-identification. Consequently, it is difficult to evaluate whether the extracted key frames are effective for person re-identification. In this paper, we introduce an End-to-end Network Embedding Unsupervised Key Frame Extraction (EKEN) to address the above problems. First, we design a key frame extraction module and train it using pseudo labels generated by hierarchical clustering to extract key frames. Second, we embed the key frame extraction module into the person re-identification task. The results of the key frame extraction and the pedestrian re-recognition are fed back to each other in time. The instant feedback promotes the synchronization optimization of these two modules. The mAP achieved by our method in the MARS dataset is improved by 0.7%, 2.9%, 2.1% and 2.3% over the methods based on Random, Evenly, Cluster and Frame difference, respectively. Particularly, our method is more fit for the real-world application comparing to existing methods.",https://ieeexplore.ieee.org/document/9440586/,2021 11th International Conference on Information Science and Technology (ICIST),21-23 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2016.7860448,Enhancements for real-time Monte-Carlo Tree Search in General Video Game Playing,IEEE,Conferences,"General Video Game Playing (GVGP) is a field of Artificial Intelligence where agents play a variety of real-time video games that are unknown in advance. This limits the use of domain-specific heuristics. Monte-Carlo Tree Search (MCTS) is a search technique for game playing that does not rely on domain-specific knowledge. This paper discusses eight enhancements for MCTS in GVGP; Progressive History, N-Gram Selection Technique, Tree Reuse, Breadth-First Tree Initialization, Loss Avoidance, Novelty-Based Pruning, Knowledge-Based Evaluations, and Deterministic Game Detection. Some of these are known from existing literature, and are either extended or introduced in the context of GVGP, and some are novel enhancements for MCTS. Most enhancements are shown to provide statistically significant increases in win percentages when applied individually. When combined, they increase the average win percentage over sixty different games from 31.0% to 48.4% in comparison to a vanilla MCTS implementation, approaching a level that is competitive with the best agents of the GVG-AI competition in 2015.",https://ieeexplore.ieee.org/document/7860448/,2016 IEEE Conference on Computational Intelligence and Games (CIG),20-23 Sept. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIPR50011.2020.9425341,Enhancing Network-edge Connectivity and Computation Security in Drone Video Analytics,IEEE,Conferences,"Unmanned Aerial Vehicle (UAV) systems with high-resolution video cameras are used for many operations such as aerial imaging, search and rescue, and precision agriculture. Multi-drone systems operating in Flying Ad Hoc Networks (FANETS) are inherently insecure and require efficient security schemes to defend against cyber-attacks such as e.g., Man-in-the-middle, Replay and Denial of Service attacks. In this paper, we propose a cloud-based, end-to-end security framework viz., ""DroneNet-Sec"" that provides secure network-edge connectivity, and computation security for drone video analytics to defend against common attack vectors in UAV systems. The DroneNet-Sec features a dynamic security scheme that uses machine learning to detect anomaly events and adopts countermeasures for computation security of containerized video analytics tasks. The security scheme comprises of a custom secure packet designed with MAVLink protocol for ensuring data privacy and integrity, without high degradation of the performance in a real-time FANET deployment. We evaluate DroneNet-Sec in a hybrid testbed that synergies simulation and emulation via an open-source network simulator (NS-3) and a research platform for mobile wireless networks (POWDER). Our performance evaluation experiments in our holistic hybrid-testbed show that DroneNet-Sec successfully detects learned anomaly events and effectively protects containerized tasks execution as well as communication in drones video analytics in a light-weight manner.",https://ieeexplore.ieee.org/document/9425341/,2020 IEEE Applied Imagery Pattern Recognition Workshop (AIPR),13-15 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CADCG.2009.5246908,Entertaining video warping,IEEE,Conferences,"While various techniques of image deformation have been developed and extensively applied in animation and morphing, there are few works to extend these techniques to handle videos, especially real-time warping of a meaningful moving part in the video like human face. An efficient online algorithm is proposed in this paper to implement real-time face warping for video sequence. We employ AdaBoost to detect the sixteen human facial feature points and implement a fast face warping frame by frame while maintaining both temporal and spatial continuity of the warped video. In order to reduce the shaking of the detected facial feature points due to the noise in the video, we develop a novel model called Frame Buffer. All these procedures are designed efficient to guarantee the real-time performance of our system (15 fps). In addition, many other types of warping functions are also compatible with our framework. It is shown that our algorithm can be applied in real-time special effect editing in video as well as other entertainment applications.",https://ieeexplore.ieee.org/document/5246908/,2009 11th IEEE International Conference on Computer-Aided Design and Computer Graphics,19-21 Aug. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTCSA50079.2020.9203738,Error Vulnerabilities and Fault Recovery in Deep-Learning Frameworks for Hardware Accelerators,IEEE,Conferences,"Hardware accelerators such as GP-GPUs, Tensor Cores, and Deep-Learning Accelerators (DLA) are increasingly being used in real-time settings such as autonomous vehicles (AVs). In such deployments, any software errors and process failures in hardware systems can lead to critical faults in AVs. Therefore, assessing and mitigating hardware accelerator faults are critical requirements for safety-critical systems. Past work on this subject focused on simulated and injected software and hardware faults to understand and analyze the behavior of the software stack and the entire system. However, programming errors and process failures caused when using software frameworks must also be considered. In this paper, we present experiments which show that widely used deep-learning frameworks are vulnerable to programming mistakes and errors. We first focus on memory-related programming errors caused by applications using deep-learning frameworks that facilitate high-performance inferencing. We next find that a reset to recover from any fault imposes significant time penalties in reloading a pre-trained deep neural network model. To reduce these fault recovery times, we propose fault recovery mechanisms that checkpoint and resume the network based on the inference stage when an error is detected. Finally, we substantiate the practical feasibility of our approach and evaluate the improvement in recovery times<sup>1</sup><sup>1</sup>A demo video clip demonstrating our recovery algorithm has been uploaded to Youtube: https://www.youtube.com/watch?v=xwUYdJdA5oM.. We use a case-study with real-world applications on an Nvidia GeForce GTX 1070 GPU and an Nvidia Xavier embedded platform, which is commonly used by multiple automotive OEMs.",https://ieeexplore.ieee.org/document/9203738/,2020 IEEE 26th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),19-21 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETA54173.2021.9726680,Estimating the height of a person from a video sequence,IEEE,Conferences,"The research described in this paper focuses on a development of experimental solution, which estimates human height from a video sequence. The main requirements include portability, low hardware requirements, and accuracy. The paper includes analysis of the algorithms employed for image processing, human detection and methods for estimating height of human. The experimental software solution is implemented in Python programming language. The SSD algorithm was selected to detect the person in the image and the output was later on utilized to calculate the real human height. A calibration object in a form of ArUco marker and its known dimensions became the foundation for calculating the real human height. The experimental testing confirmed the accuracy of the developed solution to be sufficient for the real-world application. The most accurate height, with just a deviation of 0.001 meters, was estimated at a distance of 5 and 8 meters. The approach presented in this paper is also to be used as a complement by newly proposed unmanned aerial vehicle security system.",https://ieeexplore.ieee.org/document/9726680/,2021 19th International Conference on Emerging eLearning Technologies and Applications (ICETA),11-12 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC.2017.8243084,Evaluation of classroom teaching quality based on video processing technology,IEEE,Conferences,"High quality teaching has always been the pursuit of universities, but the automatic and real time evaluation on the quality of classroom teaching has not been achieved. To solve this problem, a real-time processing of classroom video through face detection technology and a software system to provide a basis for judging the quality of teaching is proposed in this paper. The main application of machine learning and deep learning is to build the face detection algorithm, and the application of C# and Hikvision SDK are for the second development building software interface and nestification of software systems.",https://ieeexplore.ieee.org/document/8243084/,2017 Chinese Automation Congress (CAC),20-22 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2018.8477698,Evolving Controllers for Mario AI Using Grammar-based Genetic Programming,IEEE,Conferences,"Video games mimic real-world situations and they can be used as a benchmark to evaluate computational methods in solving different types of problems. Also, machine learning methods are used nowadays to improve the quality of non-player characters in order (i) to create human like behaviors, and (ii) to increase the hardness of the games. Genetic Programming (GP) has presented good results when evolving programs in general. One of the main advantage of GP is the availability of the source-code of its solutions, helping researchers to understand the decision-making process. Also, a formal grammar can be used in order to facilitate the generation of programs in more complex languages (such as Java, C, and Python). Here, we propose the use of Grammar-based Genetic Programming (GGP) to evolve controllers for Mario AI, a popular platform to test video game controllers which simulates the Nintendo's Super Mario Bros. Also, as GP provides the source-code of the solutions, we present and analyze the best program obtained. Finally, GGP is compared to other techniques from the literature and the results show that GGP find good controllers, specially with respect to the scores obtained on higher difficulty levels.",https://ieeexplore.ieee.org/document/8477698/,2018 IEEE Congress on Evolutionary Computation (CEC),8-13 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SBGAMES.2014.17,Evolving Swarm Intelligence for Task Allocation in a Real Time Strategy Game,IEEE,Conferences,"Real time strategy games are complex scenarioswhere multiple agents must be coordinated in a dynamic,partially observable environment. In this work, we model thecoordination of these agents as a task allocation problem, in which specific tasks are given to the agents that are more suited to execute them. We employ a task allocation algorithm based on swarm intelligence and adjust its parameters using a genetic algorithm. To evaluate this approach, we implement this coordination mechanism in the AI of a popular video game: StarCraft: BroodWar. Experiment results show that the genetic algorithm enhances performance of the task allocation algorithm. Besides, performance of the proposed approach in matches against StarCraft's native AI is comparable to that of a tournament-level software-controlled player for StarCraft.",https://ieeexplore.ieee.org/document/7000037/,2014 Brazilian Symposium on Computer Games and Digital Entertainment,12-14 Nov. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IBCAST51254.2021.9393316,Exploiting Spatiotemporal Features for Action Recognition,IEEE,Conferences,"Action recognition from videos is important due to its numerous applications in real life scenarios. Most commonly action recognition is used for surveillance but it is also used in other applications such as entertainment and gaming. Moreover, due to high demand of automation and easy availability of higher processing power, researchers are aiming for better ways to accurately classify human actions using videos. Different methods have been proposed in literature in order to identify actions. However, most of these methods suffer from limitations such as implementation complexity and inability to recognize complex actions. In this paper, we propose a novel action recognition technique which is easy to implement and is robust to complex actions. While there are different methods to capture motion, trajectories based methods have proven to be an excellent way to represent motion in videos. Inspired by that, we sampled dense points in different frames of the video to track down their displacement information to encode the trajectories. We used dense trajectories along with oriented gradients and motion boundary information as features. We use codebook generated using Gaussian mixture model and Fisher vector to encode our features. Finally, after reducing the dimensionality of our data, we use linear Support Vector Machine to identify the actions. The experimental evaluations performed on a recent benchmark action recognition dataset showed that our method is able to achieve encouraging results compared to other competing methods.",https://ieeexplore.ieee.org/document/9393316/,2021 International Bhurban Conference on Applied Sciences and Technologies (IBCAST),12-16 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS52415.2021.9466088,Expression Classification For User Experience Testing Using Convolutional Neural Network,IEEE,Conferences,"In testing an application such as a video game, user response or user experience in playing a video game was very important for the developer itself. The utilization of the convolutional neural network (CNN) machine learning classification for facial expression feedback in terms of gameplay satisfaction would greatly help developers in finalizing their products. In this researched, the expressions that could've have been classified were angry, fear, sad, happy, neutral, disgusted, and surprised. This researched was done using cnn and the facial expression recognition 2013 (fer2013) dataset. In the proposed system, cnn was applied in the extraction of characteristics, classification of images, and recommendations. Before classifying expressions, the training model needed to have been processed first. Testing was carried throughout the following stages, namely the process of determining the dataset used, training the model, testing process, and performance test. The test results with a data ratio of 90% data training and 10% data test resulted in a training model with a final accuracy value of 64.26%, while the real time performance testing with the best result was obtained from a test scheme with distance of 60 cm with a moderate light intensity of 14 lux that gets 100% accuracy. Compared to the other researched in this area, the system implemented facial expression classification in real time and give a recap of expressions classified during the real time classification in the form of graph and pie chart.",https://ieeexplore.ieee.org/document/9466088/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCT46805.2019.8947168,FPGA Implementation of Quantized Convolutional Neural Networks,IEEE,Conferences,"Convolutional neural networks (CNN) are the most commonly used techniques in computer vision tasks. Image processing methods based on CNN are widely used, especially in areas such as face recognition, target detection, and speech recognition. A large number of CNN computing require the use of dedicated hardware, such as graphics processing units. Since this hardware is not portable in real life, there is an urgent need to apply neural networks to FPGAs. High-level synthesis (HLS) provides a good programming environment for developers, making the programming of FPGA more efficient. In this paper, we will introduce a quantized convolution neural network (QCNN) accelerator architecture based on HLS, which utilizes the parameters that are quantized during training and general processing elements during inference to improve performance. QCNN has fewer parameter operations, so it is advantageous to on-chip storage. The QCNN Accelerator uses a fast algorithm to implement batch normalization, which can greatly reduce hardware consumption while maintaining accuracy. We implemented the proposed architecture on the Nexys Video FPGA platform. The clock frequency is 100 MHz and the peak performance of QCNN reaches 22 GOP/S. Finally, the design and implementation of QCNN accelerator system based on mobile video are introduced. We connect the FPGA with the OV5640 camera to solve the problem of image classification in real-time video transmission.",https://ieeexplore.ieee.org/document/8947168/,2019 IEEE 19th International Conference on Communication Technology (ICCT),16-19 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2015.7301150,FPGA autonomous logic analyzer using innovative BERC filter optimization,IEEE,Conferences,"In this work is presented a new hardware implementation of a high speed logic analyzer inside FPGA (Field Programmable Gate Array) chips that is fully autonomous by directly driving a VGA compatible computer monitor for multiple signals display. It can be used as a very low cost and real time testing instrument for both external hardware and internal FPGA designs. The implementation is optimized at schematic level by proposing an innovative solution called BERC (Begin-End-Reset-Cell) as hardware digital filter that eliminates any magnitude or equality comparators usually inferred from hardware description languages to decode the counters signals. This new technique is much more efficient because it reduces the combinatorial resources to a minimum, and it can be generalized for all kind of video interfaces.",https://ieeexplore.ieee.org/document/7301150/,"2015 7th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",25-27 June 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DELTA.2002.994637,FPGA implementation of a neural network for a real-time hand tracking system,IEEE,Conferences,"The advantage of parallel computing of artificial neural networks can be combined with the potentials of VLSI circuits in order to design a real time detection and tracking system applied to video images. Based on these facts, a real-time localization and tracking algorithm has been developed for detecting human hands in video images. Due to the real time aspect, a single-pixel-based classification is aspired, so that a continuous data stream can be processed. Consequently, no storage of full images or parts of them is necessary. The classification, whether a pixel belongs to a hand or to the background, is done by analyzing the RGB-values of a single pixel by means of an artificial neural network. To obtain the full processing speed of this neural network a hardware solution is realized in a Field Programmable Gate Array (FPGA).",https://ieeexplore.ieee.org/document/994637/,"Proceedings First IEEE International Workshop on Electronic Design, Test and Applications '2002",29-31 Jan. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSD.2012.64,FPGA-based Neural Network for Nonuniformity Correction on Infrared Focal Plane Arrays,IEEE,Conferences,"Despite recent technological advances which improve their performance and reduce their cost, Focal Plane Arrays for infrared imagers suffer from spatial nonuniformity that renders their output unusable unless a suitable correction method is applied. This paper describes an embedded hardware implementation of Scribner's algorithm for online nonuniformity correction. Our implementation on a Xilinx Spartan XC3S1200E FPGA achieves a throughput of more than 130 frames per second on 320x240-pixel IR video, which greatly exceeds real-time requirements. The power consumption of our system is 329mW, which is two orders of magnitude smaller than a software implementation of the algorithm on a traditional processor, and can be greatly reduced with a custom-VLSI implementation of the architecture.",https://ieeexplore.ieee.org/document/6386892/,2012 15th Euromicro Conference on Digital System Design,5-8 Sept. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI50826.2021.9402670,Face Mask Detection Using Deep Learning,IEEE,Conferences,"In order to prevent the spread of CORONA virus, everyone must wear a mask during the pandemic. In these tough times of COVID-19 it is necessary to build a model that detects people with and without mask in real-time as it works as a simple precautionary measure to prevent the spread of virus. If deployed correctly, this machine learning technique helps in simplifying the work of frontline warriors and saving their lives. A basic Convolutional Neural Network (CNN) model is built using TensorFlow, Keras, Scikit-learn and OpenCV to make the algorithm as accurate as possible. Javascript API helps in accessing webcam for real-time face mask detection. Since Google Colab runs on web browser it can't access local hardware like a camera without APIs. The proposed work contains three stages: (i) pre-processing, (ii) Training a CNN and (iii) Real-time classification. The first part is the Pre-processing section, which can be divided into “Grayscale Conversion” of RGB image, “image resizing and normalization” to avoid false predictions. Then the proposed CNN, classifies faces with and without masks as the output layer of proposed CNN architecture contains two neurons with Softmax activation to classify the same. Categorical cross-entropy is employed as loss function. The proposed model has Validation accuracy of 96%. If anyone in the video stream is not wearing a protective mask a Red coloured rectangle is drawn around the face with a dialog entitled as NO MASK and a Green coloured rectangle is drawn around the face of a person wearing MASK.",https://ieeexplore.ieee.org/document/9402670/,2021 International Conference on Computer Communication and Informatics (ICCCI),27-29 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE53109.2021.9752237,Face Mask Detection for Preventing the Spread of Covid-19 using Knowledge Distillation,IEEE,Conferences,"The coronavirus pandemic (COVID-19) has unfolded hastily throughout the entire world. This pandemic disease can spread through droplets and can be airborne. Hence, the use of face masks in public places is crucial to stop its spread. The present study aims to develop a system that can identify masked or non-masked faces; whether it is a normal mask, transparent mask, or a face alike mask. The face mask detection system is developed with the help of Convolutional Neural Networks (CNN). The model compression technique of Knowledge Distillation has been used to make the machine lesser computation and memory intensive so that it is simple to install the model on a few embedded gadgets and cell computing platforms. Using the model compression technique and GPU systems will help boom the calculation velocity of the model and drop the storage space required for calculations. The experimental outcomes show that the developed detector is capable to classify diverse types of masks. Also, it can classify video images in real-time. Using the Knowledge Distillation on the baseline model can improve the testing accuracy from 88.79% to 90.13%. The proposed unique system can be implemented to assist in the prevention of COVID-19 spread and detect various mask types.",https://ieeexplore.ieee.org/document/9752237/,2021 International Conference on Computational Performance Evaluation (ComPE),1-3 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMCON53756.2021.9623222,Face Mask Detection: A Real-Time Android Application Based on Deep Learning Modeling,IEEE,Conferences,"The accelerated spread of the COVID-19 (coronavirus) disease has put stress on healthcare systems. Some safety measures are provided, such as keeping social distance and wearing a mask, which can help curb transmission and save lives. This paper aims to detect whether a person is wearing a mask or not with video surveillance to enforce health and safety regulations in real-time. We propose a solution for face mask detection using two deep learning models, the MobileNetV2 and the Modified Convolutional Neural Network (MCNN). The trained models are converted to TensorFlow Lite to deploy an Android Application. Our models can achieve up to 99&#x0025; accuracy. In this paper, an analysis of the number of individuals not wearing masks is provided by capturing the face and storing it on a mobile-backend-as-a-service. Our application can be adopted to increase health measures in real-time and control the spread of COVID-19.",https://ieeexplore.ieee.org/document/9623222/,"2021 IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",27-30 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICT50521.2020.00032,Face Recognition Techniques using Statistical and Artificial Neural Network: A Comparative Study,IEEE,Conferences,"Face recognition is the process of identifying a person by their facial characteristics from a digital image or a video frame. Face recognition has extensive applications and there will be a massive development in future technologies. The main contribution of this research is to perform a comparative study between different statistical-based face recognition techniques, namely: Eigen-faces, Fisher-faces, and Local Binary Patterns Histograms (LBPH) to measure their effectiveness and efficiency using real-database images. These recognizers still used on top of commercial face recognition products. Additionally, this research is comprehensively comparing 17 face-recognition techniques adopted in research and industry that use artificial-neural network, criticize and categories them into an understandable category. Also, this research provides some directions and suggestions to overcome the direct and indirect issues for face recognition. It has found that there is no existing recognition method that the community of face recognition has agreed on and solves all the issues that face the recognition, such as different pose variation, illumination, blurry and low-resolution images. This study is important to the recognition communities, software companies, and government security officials. It has a direct impact on drawing clear path for new face recognition propositions. This study is one of the studies with respect to the size of its reviewed approaches and techniques.",https://ieeexplore.ieee.org/document/9092128/,2020 3rd International Conference on Information and Computer Technologies (ICICT),9-12 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FarEastCon50210.2020.9271423,Face Video Tracking Using Deep Neural Networks,IEEE,Conferences,"The computational method of the face recognition in a continuous video stream using deep neural networks is offered, that is effective in terms of accuracy and high-speed performance. Adapted architectures of deep neural networks are proposed: the modified MobileNet v2 neural network and the original dual network for identifying a person by face image. The original databases for neural networks are developed. Natural experiments are made, and the characteristics of accuracy and speed of problem solutions were evaluated. To solve the problems video tracking of persons in real time the software modules of the intelligent access control system in Python are developed.",https://ieeexplore.ieee.org/document/9271423/,2020 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon),6-9 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME.2005.1521517,Face and Eye Rectification in Video Conference Using Artificial Neural Network,IEEE,Conferences,"The lack of eye contact in video conference degrades the user's experience. This problem has been known and studied for many years. There are hardware-based solutions to the eye gazing problem. However, these specialized systems are not generally accessible. This paper suggests a software approach that rectifies the face and the eyes in video conference, only utilizing one camera. In the setup phase, the view point, the head poses are calculated using 100 frames each from the front view and the above view, with the aids of face detection and eye detection algorithm. The weights of the artificial neural networks (ANN) are then trained. Once the setup is done, for each frame in real time, we apply ANN on the features found with the aid of face detection. The ANN outputs are feed to an image warping algorithm to rectify the face. Rectification of the eyes is done using image warping, based on an eye model. This is done in near real time (13 frames per second, for the resolution of 320times240). The result is not genuine but is better. However, the rectified face jerks when the user's head moves or turns rapidly. The author shall need to investigate more on this issue",https://ieeexplore.ieee.org/document/1521517/,2005 IEEE International Conference on Multimedia and Expo,6-6 July 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSP51882.2021.9408772,Face matching system in multi-pose changing scene,IEEE,Conferences,"Due to the emergence of deep learning, face recognition has made remarkable achievements. But in the traditional video surveillance scene, because the monitoring object is not controlled and the face posture changes frequently, the actual face image may be a negative face image, which greatly reduces the accuracy of face recognition. And the traditional monitoring system does not have an automatic warning function, which wastes a lot of human resources. This paper focuses on the realization of a real-time and efficient face matching system in multi-pose changing scenes. The system mainly uses the MTCNN algorithm for face detection and alignment and uses a generative confrontation network to make face positive to eliminate the influence of posture change. The feature extraction network uses lightweight MobileFaceNet as the backbone network for feature extraction, and the feature comparison is carried out by cosine similarity. Experiments show that the multi-pose face matching system can greatly improve the speed of feature extraction, and has good accuracy, real-time, and robustness.",https://ieeexplore.ieee.org/document/9408772/,2021 6th International Conference on Intelligent Computing and Signal Processing (ICSP),9-11 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2004.1334117,Face pose estimation and its application in video shot selection,IEEE,Conferences,"In this paper, a face pose estimation method and its application in video shot selection for face image preprocessing is introduced. The pose estimator is learned by a boosting regression algorithm called SquareLev.R that learns poses from simple Haar-type features. It consists of two tree structured subsystems for the left-right angle and up-down angle respectively. As a specific application in video based face recognition, the best shot selection problem is discussed, which results in a real-time system that can automatically select the most frontal face from a video sequence.",https://ieeexplore.ieee.org/document/1334117/,"Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.",26-26 Aug. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIS53314.2022.9742863,Facemask Detection to Prevent COVID-19 Using YOLOv4 Deep Learning Model,IEEE,Conferences,"The on-going global Covid-19 pandemic has impacted everyone&#x2019;s life. World Health organization (WHO) and Governments all over world have found that social distancing and donning a mask in public places has been instrumental in reducing the rate of COVID-19 transmission. Stepping out of homes in a face mask is a social obligation and a law mandate that is often violated by people and hence a face mask detection model that is accessible and efficient will aid in curbing the spread of disease. Detecting and identifying a face mask on an individual in real time can be a daunting and challenging task but using deep learning and computer vision, establish tech-based solutions that can help combat COVID-19 pandemic. In this paper, YOLOv4 deep learning model is designed and applied deep transfer learning approach to create a face mask detector which can be used in real time. GPU used was Google Collab to run the simulations and to draw inferences. Proposed implementation considered three types of data as input such as image dataset, video dataset and real time data for face mask detection. Performance parameters are tabulated and obtained mean average precision of 0.86, F1 score 0.77 for image dataset, 90 &#x0025; accuracy for video dataset. And real time face mask detector with accuracy of 95&#x0025;, it is successfully able to identify a person with and without facemask and report if they are wearing a face mask or not.",https://ieeexplore.ieee.org/document/9742863/,2022 Second International Conference on Artificial Intelligence and Smart Energy (ICAIS),23-25 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISP-BMEI.2017.8302004,Fall detection for elderly person care using convolutional neural networks,IEEE,Conferences,"Falls are one of the major leading causes of mortality for elderly people living alone at home, which can lead to severe injuries. Fall detection is the most important health care issue for the elderly. In computer vision domain, significant breakthrough technologies such as deep learning have been obtained for over five years. Deep learning belongs to computational methods that allow an algorithm to program itself by learning from training data. Convolutional neural networks (CNNs), a specific type of deep learning, have set the state-of-the-art image classification performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in recent years. In this paper, we present the use of convolutional neural networks for fall detection in video surveillance environment. CNN is directly applied to each frame image in the video to learn human shape deformation features that describe different postures of the human and determine if a fall occurs. Experimental results show that our proposed approach runs in real-time and achieves average accuracy of 99.98% for 10-fold cross-validation for fall detection. It is shown that the implemented CNN-based fall detection approach can be a promising solution for detecting falls.",https://ieeexplore.ieee.org/document/8302004/,"2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",14-16 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBAIE52039.2021.9390068,Fall detection system based on real-time pose estimation and SVM,IEEE,Conferences,"With the rapid growth of the elderly population, fall detection has become a key issue in the medical and health field. Accurately detecting fall behavior in surveillance video and timely feedback can effectively reduce the injury and even death of the elderly due to falls. For the complex scenes in surveillance video and the interference of multiple similar human behaviors, this paper proposes a method based on pose estimation and the auxiliary detection method based on yoloV5. First, extract video frames from different falling video sequences to form a data set; then, input the training sample set into the improved network for training until the network converges; finally, test the category of the target in the video according to the optimized network model and locate the target. Experimental results show that the improved algorithm can effectively detect falls or Activities of Daily Living (ADL) events in each frame of the image and give real-time feedback. The detection of falling behavior in the video further verifies the feasibility and efficiency of the recognition method based on our deep learning methods.",https://ieeexplore.ieee.org/document/9390068/,"2021 IEEE 2nd International Conference on Big Data, Artificial Intelligence and Internet of Things Engineering (ICBAIE)",26-28 March 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APSIPA.2015.7415439,Fast CU partition strategy for HEVC intra-frame coding using learning approach via random forests,IEEE,Conferences,"HEVC (High Efficiency Video Coding) achieves cutting edge encoding efficiency and outperforms previous standards, such as the H.264/AVC. One of the key contributions to the improvement is the intra-frame coding that employs abundant coding unit (CU) sizes. However finding the optimal CU size is computationally expensive. To alleviate the intra encoding complexity and facilitate the real-time implementation, we use a machine learning technique: the random forests, for training. Based on off-line training, we propose using the forest classifier to skip or terminate the current CU depth level. In addition, neighboring CU size decisions are utilized to determine the current depth range. Experimental results show that our proposed algorithm can achieve 48.31% time reduction, with 0.80% increase in the Bjantegaard delta bitrate (BD-rate), which are state-of-the-art results compared with all algorithms in the literature.",https://ieeexplore.ieee.org/document/7415439/,2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA),16-19 Dec. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2011.6116531,Fast FPGA-based architecture for pedestrian detection based on covariance matrices,IEEE,Conferences,"Pedestrian detection is a crucial task in several video surveillance and automotive scenarios, but only a few detection systems are designed to be realized on an embedded architecture, allowing to increase the processing speed which is one of the key requirements in real applications. In this paper, we propose a novel SoC (System on Chip) architecture for fast pedestrian detection in video. Our implementation is based on a linear SVM (Support Vector Machine) classification frame- work, learned on a set of overlapped image patches. Each patch is described by a covariance matrix of a set of image features. Exploiting the inner parallelism of the FPGA (Field Programmable Gate Array) boards, we dramatically accelerate the covariance matrices computation that plays a crucial role in the framework. In the experiments, we show the effectiveness and the efficiency of our pedestrian detection system, reaching a detection speed of 132 fps at VGA resolution.",https://ieeexplore.ieee.org/document/6116531/,2011 18th IEEE International Conference on Image Processing,11-14 Sept. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI44817.2019.9002922,Fast Face Recognition Model without Pruning,IEEE,Conferences,"This paper proposes a real-time high accuracy face recognition model which can be deployed on performance-constrained embedded platforms or mobile terminals without pruning. In the proposed model, we optimize the network structure by balancing the memory access cost (MAC) and Floating point Operations (FLOPs) and propose a new factor to measure the efficiency of learning parameters. Based on these two metrics, we designed two structures called Simple SqueezeFaceNet (SSN) and Channel-Split Network (CSN) to achieve a good balance between the high precision and high FPS (Frames per Second). After being trained with the refined MS1M-refine-v2 dataset, our architectures achieve 0.992 (or 0.990) accuracy on the Labeled Faces in the Wild (LFW) with 155 (or 180) FPS. This indicates that our models can also be applied to real-time multi-face recognition in video.",https://ieeexplore.ieee.org/document/9002922/,2019 IEEE Symposium Series on Computational Intelligence (SSCI),6-9 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITOEC.2018.8740558,Fast Inter Mode Decision Algorithms for x265,IEEE,Conferences,"The latest High-Efficiency Video Coding (HEVC) standard achieves nearly 50% bit rates reduction for similar quality relative to H.264/Advanced Video Coding(AVC) . However, its complexity is enormously increased ,which becomes one of the most challenges for its deployment in real time applications. The only solution to decrease the coding complexity is to set up different settings by adjusting various coding parameters. Among them, low complexity settings are suitable for industrial applications and conducive to the popularization of HEVC. Traditional fast mode decision algorithms mainly aim at decreasing coding complexity for high complexity settings. In this paper, we propose a fast mode decision method for HEVC with low complexity settings according to machine learning. A decision tree is constructed to decide whether to check 2N×2N mode or the SKIP/MERGE mode by exploiting relevant information from spatiotemporal adjacent Coding Units(CUs). Further mode skipping is performed based on the result of the first step. Experiments show that the proposed scheme can only increase by 1.42% Bjotegaard Delta Bit rate(BDBR) with an average time reduction of 22.45% for HEVC with low complexity settings.",https://ieeexplore.ieee.org/document/8740558/,2018 IEEE 4th Information Technology and Mechatronics Engineering Conference (ITOEC),14-16 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ51579.2020.9290654,Fast Portrait Segmentation of the Head and Upper Body,IEEE,Conferences,"Portrait segmentation is the process whereby the head and upper body of a person is separated from the background of an image or video stream. This is difficult to achieve accurately, although good results have been obtained with deep learning methods which cope well with occlusion, pose and illumination changes. These are however, either slow or require a powerful system to operate in real-time. We present a new method of portrait segmentation called FaceSeg which uses fast DBSCAN clustering combined with smart face tracking that can replicate the benefits and accuracy of deep learning methods at a much faster speed. In a direct comparison using a standard testing suite, our method achieved a segmentation speed of 150 fps for a 640x480 video stream with median accuracy and F1 scores of 99.96% and 99.93% respectively on simple backgrounds, with 98.81% and 98.13% on complex backgrounds. The state-of-art deep learning based FastPortrait / Mobile Neural Network method achieved 15 fps with 99.95% accuracy and 99.91% F1 score on simple backgrounds, and 99.01% accuracy and 98.43 F1 score on complex backgrounds. An efficacy-boosted implementation for FaceSeg can achieve 75 fps with 99.23% accuracy and 98.79% F1 score on complex backgrounds.",https://ieeexplore.ieee.org/document/9290654/,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),25-27 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ATC52653.2021.9598204,Fast and Accurate Fall Detection and Warning System Using Image Processing Technology,IEEE,Conferences,"Accidental falls can cause serious injuries, which can lead to serious medical problems, especially for construction and factory workers. This paper proposes a study on a fall detection system based on computer vision. This system is applied to help detect people falling in harsh working environment such as dust, loud noise, few people working. From the recorded video streams, the data is processed to recognize a person falling, lying motionless. Algorithms for tracking people are implemented on a compact, easy-to-install embedded system. Experimental results show that the system ensures safety and can provide emergency assistance to people who have fallen within the view of the camera.",https://ieeexplore.ieee.org/document/9598204/,2021 International Conference on Advanced Technologies for Communications (ATC),14-16 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACVW54805.2022.00063,Fast and Lightweight Online Person Search for Large-Scale Surveillance Systems,IEEE,Conferences,"The demand for methods for video analysis in the field of surveillance technology is rapidly growing due to the increasing amount of surveillance footage available. Intelligent methods for surveillance software offer numerous possibilities to support police investigations and crime prevention. This includes the integration of video processing pipelines for tasks such as detection of graffiti, suspicious luggage, or intruders. Another important surveillance task is the semi-automated search for specific persons-of-interest within a camera network. In this work, we identify the major obstacles for the development of person search systems as the real-time processing capability on affordable hardware and the performance gap of person detection and re-identification methods on unseen target domain data. In addition, we demonstrate the current potential of intelligent online person search by developing a real-world, large-scale surveillance system. An extensive evaluation is provided for person detection, tracking, and re-identification components on affordable hardware setups, for which the whole system achieves real-time processing up to 76 FPS.",https://ieeexplore.ieee.org/document/9707564/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW),4-8 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2005.1527745,Fast and robust face detection in video,IEEE,Conferences,"In this paper, a fast and robust face detection method in video is introduced. To speed up, we integrate motion energy into cascade-structured classifier to reject most of the candidate windows. Motion energy representing moving extent of candidate regions can be computed efficiently with integral motion image and thus accelerates the evaluating procedure greatly. By dividing state of system into three and processing input images according to current state with different modes, we treat with special situations such as still face existence, continuously failing to detect faces and input without evident changes to get a robust system suitable for real application. Without depending on any supposed motion model, the system is out of limitation of moving patterns including speed and direction. The approach is implemented to detect faces in real situation and the speed is about 6-22 ms with a detection ratio of 99%.",https://ieeexplore.ieee.org/document/1527745/,2005 International Conference on Machine Learning and Cybernetics,18-21 Aug. 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC54389.2021.9674399,Fire Detection Based on Infrared Filter and Deep Learing,IEEE,Conferences,"The use of video detection to detect fire has become an important research topic, which helps reduce personal and property losses. Detecting early fire helps prevent disaster, and the key factors for detecting fire are high accuracy, low false alarm rate, and fast response time. This paper uses infrared detection to improve the accuracy of fire detection, and at the same time uses convolutional neural networks to reduce the false alarm rate of detection. The detection performance of this model is better than some existing fire detection algorithms, and the algorithm model is deployed in the equipment, the function of real-time fire detection is realized.",https://ieeexplore.ieee.org/document/9674399/,2021 7th International Conference on Computer and Communications (ICCC),10-13 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESC48915.2020.9155625,Fire and Gun Violence based Anomaly Detection System Using Deep Neural Networks,IEEE,Conferences,"Real-time object detection to improve surveillance methods is one of the sought-after applications of Convolutional Neural Networks (CNNs). This research work has approached the detection of fire and handguns in areas monitored by cameras. Home fires, industrial explosions, and wildfires are a huge problem that cause adverse effects on the environment. Gun violence and mass shootings are also on the rise in certain parts of the world. Such incidents are time-sensitive and can cause a huge loss to life and property. Hence, the proposed work has built a deep learning model based on the YOLOv3 algorithm that processes a video frame-by-frame to detect such anomalies in real-time and generate an alert for the concerned authorities. The final model has a validation loss of 0.2864, with a detection rate of 45 frames per second and has been benchmarked on datasets like IMFDB, UGR, and FireNet with accuracies of 89.3%, 82.6% and 86.5% respectively. Experimental result satisfies the goal of the proposed model and also shows a fast detection rate that can be deployed indoor as well as outdoors.",https://ieeexplore.ieee.org/document/9155625/,2020 International Conference on Electronics and Sustainable Communication Systems (ICESC),2-4 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EIConRus.2019.8656778,Fixed-Point Convolutional Neural Network for Real-Time Video Processing in FPGA,IEEE,Conferences,"Modern mobile neural networks with a reduced number of weights and parameters do a good job with image classification tasks, but even they may be too complex to be implemented in an FPGA for video processing tasks. The article proposes neural network architecture for the practical task of recognizing images from a camera, which has several advantages in terms of speed. This is achieved by reducing the number of weights, moving from a floating-point to a fixed-point arithmetic, and due to a number of hardware-level optimizations associated with storing weights in blocks, a shift register, and an adjustable number of convolutional blocks that work in parallel. The article also proposed methods for adapting the existing data set for solving a different task. As the experiments showed, the proposed neural network copes well with real-time video processing even on the cheap FPGAs.",https://ieeexplore.ieee.org/document/8656778/,2019 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering (EIConRus),28-31 Jan. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCS47774.2020.00169,Flow control in SDN-Edge-Cloud cooperation system with machine learning,IEEE,Conferences,"Real-time prediction of communications (or road) traffic by using cloud computing and sensor data collected by Internet-of-Things (IoT) devices would be very useful application of big-data analytics. However, upstream data flow from IoT devices to the cloud server could be problematic, even in fifth generation (5G) networks, because networks have mainly been designed for downstream data flows like for video delivery. This paper proposes a framework in which a software defined network (SDN), edge server, and cloud server cooperate with each other to control the upstream flow to maintain the accuracy of the real-time predictions under the condition of a limited network bandwidth. The framework consists of a system model, methods of prediction and determining the importance of data using machine learning, and a mathematical optimization. Our key idea is that the SDN controller optimizes data flows in the SDN on the basis of feature importance scores, which indicate the importance of the data in terms of the prediction accuracy. The feature importance scores are extracted from the prediction model by a machine-learning feature selection method that has traditionally been used to suppress effects of noise or irrelevant input variables. Our framework is examined in a simulation study using a real dataset consisting of mobile traffic logs. The results validate the framework; it maintains prediction accuracy under the constraint of limited available network bandwidth. Potential applications are also discussed.",https://ieeexplore.ieee.org/document/9355689/,2020 IEEE 40th International Conference on Distributed Computing Systems (ICDCS),29 Nov.-1 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MSN53354.2021.00046,FlowFormers: Transformer-based Models for Real-time Network Flow Classification,IEEE,Conferences,"Internet Service Providers (ISPs) often perform network traffic classification (NTC) to dimension network bandwidth, forecast future demand, assure the quality of experience to users, and protect against network attacks. With the rapid growth in data rates and traffic encryption, classification has to increasingly rely on stochastic behavioral patterns inferred using deep learning (DL) techniques. The two key challenges arising pertain to (a) high-speed and fine-grained feature extraction, and (b) efficient learning of behavioural traffic patterns by DL models. To overcome these challenges, we propose a novel network behaviour representation called FlowPrint that extracts per-flow time-series byte and packet-length patterns, agnostic to packet content. FlowPrint extraction is real-time, fine-grained, and amenable for implementation at Terabit speeds in modern P4-programmable switches. We then develop FlowFormers, which use attention-based Transformer encoders to enhance FlowPrint representation and thereby outperform conventional DL models on NTC tasks such as application type and provider classification. Lastly, we implement and evaluate FlowPrint and FlowFormers on live university network traffic, and achieve a 95&#x0025; f1-score to classify popular application types within the first 10 seconds, going up to 97&#x0025; within the first 30 seconds and achieve a 95+&#x0025; f1-score to identify providers within video and conferencing traffic flows.",https://ieeexplore.ieee.org/document/9751578/,"2021 17th International Conference on Mobility, Sensing and Networking (MSN)",13-15 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYSCON.2019.8836965,Fog Computing for Real-Time Accident Identification and Related Congestion Control,IEEE,Conferences,"This paper focuses on developing (i) a benchmark application for Real-Time traffic incidence identification and related traffic management, using Real-Time congestion-aware navigation of smart vehicles (Edge nodes) with video feeds, (ii) an image database for Deep Learning used for recognition and classification of traffic incidences such as accidents and congestions, (iii) the System Level Software (or Middleware) required for Distributed Computing in such a heterogeneous Real-Time constrained system with Rapid Mobility - today's Internet-of-Everything (IoE), and (iv) a hardware prototype of the distributed computing and storage infrastructure. The video bandwidth requirement of 10-100 GigaBytes of data per minute per vehicular camera makes it a Big Data problem. With millions of smart vehicles projected to be deployed within the next 5 years, BigData from a single vehicle, multiplied with the large number of vehicles, presents a Big-Squared-Data computing space which will easily overwhelm any Cloud infrastructure with its Real-Time or near Real-Time demands. Hence the need for a Fog tier between the Edge nodes and the Cloud to bring distributed computation (servers) and storage closer to the Edge nodes. Such a Fog consists of multiple Fog instances, each one of which services cells or Virtual Clusters of Edge nodes. Results show that Fog-Cloud computing framework outperforms a Cloud-only platform by 79.7% reduction in total latency or response time.",https://ieeexplore.ieee.org/document/8836965/,2019 IEEE International Systems Conference (SysCon),8-11 April 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TELECOM50385.2020.9299566,Forest Monitoring System for Early Fire Detection Based on Convolutional Neural Network and UAV imagery,IEEE,Conferences,"Forest fires are one of the main reasons for environmental degradation. In their early stages, the fires are hard to discover, so a faster and more accurate detection method can help minimize the amount of damage they can inflict. In this paper, we present an approach for autonomous early fire detection, which is based on a system with high degree of reliability and with no need of service or human interaction. To provide the autonomous capabilities to the proposed system, we have developed an object detection method, based on a convolutional neural network, which is presented in the main part of the paper. In order to have a better field of view over the observed area, instead of traditional lookout towers and satellite based monitoring, we use live video feed from an unmanned aerial vehicle (UAV), which patrols over the risky area. To make better predictions on the fire probability, we use not only the optical camera of the UAV, but also an on-board thermal camera. With the help of the software platform Node-RED, we have developed a web-based platform, which can present the acquired data in real-time and can notify the interested parties. The workflow for the development of the web-platform is also described in this paper.",https://ieeexplore.ieee.org/document/9299566/,2020 28th National Conference with International Participation (TELECOM),29-30 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00434,ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis,IEEE,Conferences,"The rapid progress of photorealistic synthesis techniques have reached at a critical point where the boundary between real and manipulated images starts to blur. Thus, benchmarking and advancing digital forgery analysis have become a pressing issue. However, existing face forgery datasets either have limited diversity or only support coarse-grained analysis.To counter this emerging threat, we construct the ForgeryNet dataset, an extremely large face forgery dataset with unified annotations in image- and video-level data across four tasks: 1) Image Forgery Classification, including two-way (real/fake), three-way (real/fake with identity-replaced forgery approaches/fake with identity-remained forgery approaches), and n-way (real and 15 respective forgery approaches) classification. 2) Spatial Forgery Localization, which segments the manipulated area of fake images compared to their corresponding real images. 3) Video Forgery Classification, which re-defines the video-level forgery classification with manipulated frames in random positions. This task is important because attackers in real world are free to manipulate any target frame. and 4) Temporal Forgery Localization, to localize the temporal segments which are manipulated. ForgeryNet is by far the largest publicly available deep face forgery dataset in terms of data-scale (2.9 million images, 221,247 videos), manipulations (7 image-level approaches, 8 video-level approaches), perturbations (36 independent and more mixed perturbations) and annotations (6.3 million classification labels, 2.9 million manipulated area annotations and 221,247 temporal forgery segment labels). We perform extensive benchmarking and studies of existing face forensics methods and obtain several valuable observations. We hope that the scale, quality, and variety of our ForgeryNet dataset will foster further research and innovation in the area of face forgery classification, as well as spatial and temporal forgery localization etc.",https://ieeexplore.ieee.org/document/9577546/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PacificVis.2018.00029,FraudVis: Understanding Unsupervised Fraud Detection Algorithms,IEEE,Conferences,"Discovering fraud user behaviors is vital to keeping online websites healthy. Fraudsters usually exhibit grouping behaviors, and researchers have effectively leveraged this behavior to design unsupervised algorithms to detect fraud user groups. In this work, we propose a visualization system, FraudVis, to visually analyze the unsupervised fraud detection algorithms from temporal, intra-group correlation, inter-group correlation, feature selection, and the individual user perspectives. FraudVis helps domain experts better understand the algorithm output and the detected fraud behaviors. Meanwhile, FraudVis also helps algorithm experts to fine-tune the algorithm design through the visual comparison. By using the visualization system, we solve two real-world cases of fraud detection, one for a social video website and another for an e-commerce website. The results on both cases demonstrate the effectiveness of FraudVis in understanding unsupervised fraud detection algorithms.",https://ieeexplore.ieee.org/document/8365989/,2018 IEEE Pacific Visualization Symposium (PacificVis),10-13 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISBI.2017.7950722,Frequency divergence image: A novel method for action recognition,IEEE,Conferences,"Action recognition systems have the potential to support clinicians, coaches and physical therapists in identifying important adopted movement patterns which could aid injury detection potential or inform rehabilitation strategies. Currently, motion capture systems, structured light pattern and time-of-flight sensors have utilization limitations that place constraints on their use outside of the laboratory setting. For this reason, we propose a system for human action recognition from video. The method presented in this work has utility with patient populations, such as Parkinson's disease, Alzheimer's disease, multiple sclerosis and dementia, outside of laboratory setting to detect the degree of which, and progression of, gait pathology. We developed a novel vision algorithm for template matching-the characterization of the motion in a video sequence. The method, titled Frequency Divergence Image, is a paradigm shift in template matching methods. Template matching methods measure macro-motion, whereas the proposed method detects micro-motion that differs from the flow of the action. We show that micro-cues improve prediction performance of human action on a real-world data set. We demonstrate a 9.15% improvement in classification accuracy over the original Motion History Image formulation when used with a convolutional neural network. Future work will focus on the deployment of the system to identify gait pathology from various patient populations.",https://ieeexplore.ieee.org/document/7950722/,2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI 2017),18-21 April 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RECONFIG.2018.8641706,Full-HD Accelerated and Embedded Feature Detection Video System with 63fps using ORB for FREAK,IEEE,Conferences,"Detecting features using pattern recognition algorithms is a fast method to recognize objects without needing large image databases. Feature detection can be used in e.g. machine learning or Simultaneous Localization and Mapping (SLAM) algorithms. For several application areas, real-time requirements and power constraints are important. FPGAs are predestined for these requirements, due to the possibility of clock-accurate computations. This work proposes a hardware accelerator of the Oriented FAST and Rotated BRIEF (ORB) feature detector implemented as a pipelined design on an FPGA SoC. The accelerator is integrated into an HDMI input/output video processing system. The final system runs with a frequency of 148.5 MHz and processes a 1080p video stream with 63 frames per second (fps). Furthermore, we propose to replace the Binary Robust Independent Elementary Features (BRIEF) with the Fast Retina Keypoint (FREAK) descriptor to improve the performance and repeatability of the algorithm.",https://ieeexplore.ieee.org/document/8641706/,2018 International Conference on ReConFigurable Computing and FPGAs (ReConFig),3-5 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIS.2015.7166585,Fully pipelined VLSI architecture of a real-time block-based object detector for intelligent video surveillance systems,IEEE,Conferences,"This paper presents a VLSI architecture of real-time object detection design for the intelligent video surveillance systems. In order to achieve the targets of high performance, low cost, and high accuracy, an efficient block-based background subtraction (BBS) algorithm had been created for VLSI implementation. It included a background model, a luminance generator, and a block difference model. The warning signals in the intelligent video surveillance systems will be triggered once more than a threshold number of blocks are marked as object discovered blocks. The VLSI architecture of this work contained 4.6-K gate counts and consumed 2.05 mW when it operated at 100 MHz processing rate by using a TSMC 0.18 um CMOS process. Compared with previous low-complexity designs, this work had the benefits of lower cost, higher performance and higher accuracy.",https://ieeexplore.ieee.org/document/7166585/,2015 IEEE/ACIS 14th International Conference on Computer and Information Science (ICIS),28 June-1 July 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIEA.2012.6496648,GPSO versus neural network in facial emotion detection,IEEE,Conferences,"Recently, we have proposed the Guided Particle Swarm Optimization (GPSO) algorithm as a novel approach in facial emotion recognition. GPSO was a modification to the Particle Swarm Optimization (PSO) algorithm, which is widely recognized as an efficient optimization algorithm with applicability in many areas. While the results we obtained from the real-time system that we developed based on the said algorithm were very good, the question that still remained was, how does this method compare with the more conventional classification approaches, such as neural network? With the aim of answering this question, we have now re-implemented our emotion recognition system using the Back Propagation Neural Network (BPNN). The BPNN used has 3 layers, consisting of the input layer of 20 neurons representing the x and y coordinates of same 10 Facial Points (FPs) used in our previous experiments; the output layer has 7 neurons representing the six basic emotions plus Neutral and a hidden layer of 20 neurons. The same data (video clips) of 20 subjects used in previous experiments were used, randomly partitioning the data in the ratio of 60-40 to train and test the network respectively. The results show that while the BPNN has its own merits in terms of speed of detection, the GPSO method performed better in accuracy of detection for all but one of the six basic emotions.",https://ieeexplore.ieee.org/document/6496648/,2012 IEEE Symposium on Industrial Electronics and Applications,23-26 Sept. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2008.4620899,GPU based video stylization,IEEE,Conferences,"In this paper, we present a GPU based video stylization framework that can artistically stylize video stream in real time. In this framework, firstly, we use a separable implementation of bilateral filter as an adaptive and iterative smoothing operation that selectively simplifies image color, leading to an abstracted look. Secondly, we perform a soft color quantization step on the abstracted video. A significant advantage of the soft color quantization implementation is preserving temporal coherence and reducing computation time as well. Successively, some optional approaches are designed to generate different artistic styles. We evaluate the effectiveness of our stylization framework with the experiment results.",https://ieeexplore.ieee.org/document/4620899/,2008 International Conference on Machine Learning and Cybernetics,12-15 July 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VCIP.2014.7051560,GPU-aided real-time image/video super resolution based on error feedback,IEEE,Conferences,"Super resolution is a process to generate high-resolution images from their low-resolution versions. In many applications such as super-HD (4K) TV, super resolution has to be performed in real time. In this paper we propose a real-time image/video super-resolution algorithm, which achieves good performance at low computational cost via off-line learning of interpolation errors in different pixel contexts. The proposed algorithm consists of three stages: fast edge-guided interpolation to generate an initial HR estimation, GPU-aided de-convolution, and error feedback compensation. All three stages can be implemented with GPU to support real-time applications. Experiments demonstrate the competitive performance of the new real-time super-resolution algorithm in both PSNR and visual quality.",https://ieeexplore.ieee.org/document/7051560/,2014 IEEE Visual Communications and Image Processing Conference,7-10 Dec. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAI.2017.8252178,GWVT: A GPU maritime vessel tracker based on the wisard weightless neural network,IEEE,Conferences,"Maritime surveillance systems increase the security of ports and ships. The video tracking is an important and challenging component of a surveillance system. Difficulties can arise due to weather conditions, target trajectory and appearance, occlusions, lighting conditions and noise. The tracker locates the vessel frame by frame in real time. This paper proposes the GPU WiSARD Vessel Tracker GWVT, a maritime vessel tracker that uses the WiSARD weightless neural network and implemented on a GPU. The GPU parallel processing feature allows the tracking algorithm to be executed very fast. CUDA easy the implementation of the parallel WiSARD tracker because the network discriminators fit well in the thread and block layout. The implementation of a WiSARD based vessel tracker on a GPU is innovative on literature. The GWVT realizes the vessel tracking at only 1 millisecond allowing other tracking techniques to be executed in parallel to rise the performance. Discounting the kernel function call time from GWVT average tracking time, GWVT becomes 13,95 faster than the CPU tracker version.",https://ieeexplore.ieee.org/document/8252178/,2017 Computing Conference,18-20 July 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-Companion55297.2022.9793764,Gallery D.C.: Auto-created GUI Component Gallery for Design Search and Knowledge Discovery,IEEE,Conferences,"GUI design is an integral part of software development. The process of designing a mobile application typically starts with the ideation and inspiration search from existing designs. However, existing information-retrieval based, and database-query based methods cannot e&#xFB03;ciently gain inspirations in three requirements: design practicality, design granularity and design knowledge discovery. In this paper we propose a web application, called Gallery D.C. that aims to facilitate the process of user interface design through real world GUI component search. Gallery D.C. indexes GUI compo-nent designs using reverse engineering and deep learning based computer vision techniques on millions of real world applications. To perform an advanced design search and knowledge discovery, our approach extracts information about size, color, component type, and text information to help designers explore multi-faceted design space and distill higher-order of design knowledge. Gallery D.C. is well received via an informal evaluation with 7 professional designers.Web Link: http://mui-collection.herokuapp.com/.Demo Video Link: https://youtu.be/zVmsz_wY5OQ.",https://ieeexplore.ieee.org/document/9793764/,2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),22-24 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MIPR.2018.00023,Game-Aware and SDN-Assisted Bandwidth Allocation for Data Center Networks,IEEE,Conferences,"Cloud computing has recently emerged as a promising paradigm for end-users and service providers. The application of the cloud-computing model to different applications offers many attractive advantages, such as scalability, ubiquity, reliability, and cost reduction to users and providers. By applying this model, the major computational parts of underlying applications are performed in data centers. Hence, effectively assigning the resources (e.g. memory, bandwidth) to applications plays a key role in providing a high Quality of Experience (QoE) to end-users. In the case of delay sensitive applications like video streaming and online gaming, the efficient resource allocation becomes more crucial. In this paper, we propose a game traffic friendly bandwidth utilization scheme using the Software Defined Networking (SDN) paradigm to solve the bandwidth allocation problem in cloud computing data center networks. Our proposed method makes use of machine learning techniques to classify the incoming traffic flows in real-time while ensuring game flows are prioritized over others. Our simulation results for a realistic network topology indicate good performance in terms of network traffic classification accuracy, and improvements of at least 9% in average utility (QoE), up to 30% increase in fairness (according to the Jain’s fairness index), and on average an 8% reduction in delay experienced by users compared to a representative conventional method: Equal Cost Multi-path (ECMP).",https://ieeexplore.ieee.org/document/8396979/,2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),10-12 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PERCOMW.2019.8730846,Gaze Estimation Using Residual Neural Network,IEEE,Conferences,"Eye gaze tracking has become an prominent research topic in human-computer interaction and computer vision. It is due to its application in numerous fields, such as the market research, medical, neuroscience and psychology. Eye gaze tracking is implemented by estimating gaze (gaze estimation) for each individual frame in offline or real-time video captured. Therefore, in order to produce the secure the accurate tracking, especially in the emerging use in medical and community, innovation on the gaze estimation posts a challenge in research field. In this paper, we explored the use of the deep learning model, Residual Neural Network (ResNet-18), to predict the eye gaze on mobile device. The model is trained using the large-scale eye tracking public dataset called GazeCapture. We aim to innovate by incorporating methods/techniques of removing the blinking data, applying image histogram normalisation, head pose, and face grid features. As a result, we achieved 3.05cm average error, which is better performance than iTracker (4.11cm average error), the recent gaze tracking deep-learning model using AlexNet architecture. Upon observation, adaptive normalisation of the images was found to produce better results compared to histogram normalisation. Additionally, we found that head pose information was useful contribution to the proposed deep-learning network, while face grid information does not help to reduce test error.",https://ieeexplore.ieee.org/document/8730846/,2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),11-15 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CoG52621.2021.9619141,Generalization of Agent Behavior through Explicit Representation of Context,IEEE,Conferences,"In order to deploy autonomous agents in digital interactive environments, they must be able to act robustly in unseen situations. The standard machine learning approach is to include as much variation as possible into training these agents. The agents can then interpolate within their training, but they cannot extrapolate much beyond it. This paper proposes a principled approach where a context module is coevolved with a skill module in the game. The context module recognizes the temporal variation in the game and modulates the outputs of the skill module so that the action decisions can be made robustly even in previously unseen situations. The approach is evaluated in the Flappy Bird and LunarLander video games, as well as in the CARLA autonomous driving simulation. The Context+Skill approach leads to significantly more robust behavior in environments that require extrapolation beyond training. Such a principled generalization ability is essential in deploying autonomous agents in real-world tasks, and can serve as a foundation for continual adaptation as well.",https://ieeexplore.ieee.org/document/9619141/,2021 IEEE Conference on Games (CoG),17-20 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iThings-GreenCom-CPSCom-SmartData-Cybermatics53846.2021.00040,Generating High-quality Movie Tags from Social Reviews: A Learning-driven Approach,IEEE,Conferences,"As a kind of keywords to describe video contents, tags are extremely beneficial for viewers to locate videos when they search the site. Additionally, tags also help video platform operators to better organize and recommend videos to platform users. Previous approaches are mainly based on manually tagging or tag propagation via video content analysis, which is time-consuming and resource-consuming. Especially, given the high volume of fresh movies generated per year, it is difficult to accurately tag all the movies manually. Moreover, video content analysis is also not easy considering typical features (e.g., long duration, complicated scenarios) of commercial movies. In this paper, we propose an automatic tagging algorithm called TagRec that exploits crowdsourced user reviews to generate accurate movie tags. We observe that user reviews contain rich information about movies (e.g., quality, actors) which can be learned to generate high-quality movie tags. Inspired by the above observation, we choose to transform the movie video tagging problem into a tag recommendation problem, in which tags are recommended to different movies by extracting knowledge from crowdsourced movie reviews. We take latent topics, tag co-occurrence probability and tag semantics into account, and formulate the problem as a recommendation optimization problem. We evaluate the performance of our pro-posed TagRec algorithm with a large-scale real-world dataset. Extensive experiments demonstrate that TagRec achieves 7.1 &#x0025; and 9.6&#x0025; improvement compared with other state-of-the-art methods in terms of Hit Ratio and Normalized Discounted Cumulative Gain respectively.",https://ieeexplore.ieee.org/document/9694110/,"2021 IEEE International Conferences on Internet of Things (iThings) and IEEE Green Computing & Communications (GreenCom) and IEEE Cyber, Physical & Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics)",6-8 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO48877.2020.9197977,Generating Image Captions based on Deep Learning and Natural language Processing,IEEE,Conferences,"This model enables an individual to input an image and output a description for the same. The research paper makes use of the functionalities of Deep Learning and NLP (Natural Language Processing). Image Caption Generation is an important task as it allows us automate the task of generating captions for any image. This functionality enables us to easily organize files without paying heed to the task of captioning. It is also important for making dynamic web pages. This paper is for people who are visually impaired or suffer from short sightedness. So, rather than looking at an image with trouble they can easily read the caption generated by this model in a larger format. It can also be used to give description of a video in real time on later implementation for a video.",https://ieeexplore.ieee.org/document/9197977/,"2020 8th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",4-5 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNN53494.2021.9580274,Generation of synthetic images of infants for training and comparison of analysis algorithms,IEEE,Conferences,"This article is devoted to generating synthetic datasets with computer models of infants with normal and abnormal general movement activity. Our solution is a software for creating data sets similar to real ones for use in video analysis algorithms. One of the important issues in video content analysis is comparison of different algorithms. Image and video dataset are availabel for many tasks like object classification, pose recognition, etc. This allows comparing different algorithms on the same data by various metrics like accuracy or time of work. However, there is no free, public, and large enough dataset for babies at the given instant. The reason for this is both in principle that videos of babies are less common, and because videos of babies are harder to look at from a legal point of view.",https://ieeexplore.ieee.org/document/9580274/,2021 Third International Conference Neurotechnologies and Neurointerfaces (CNN),13-15 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2019.8790171,Genetic Algorithm for Topology Optimization of an Artificial Neural Network Applied to Aircraft Turbojet Engine Identification,IEEE,Conferences,"Artificial neural networks (ANN) has attracted attention of the academic community by the current progress that this technique has provided in speech recognition and digital media such as as image, video, audio, and signal processing. Some fields, as industrial process control and product development can be highly benefited by the development of techniques based on the proven potentialities of ANN models, allowing more accurate simulation, better adaptation to changing environments, and greater robustness in model-based fault diagnosis. Along with the advance of ANNs, there is a trend of open-source softwares use for soft computing which facilitates the access of the interested readers to implement their own codes and to explore other applications. Historically evolutionary algorithms such as the Genetic Algorithm (GA) have been implemented to evolve the architectures to search for solutions, in order to solve this fundamental issue that is still an open problem in the general case. Therefore, the present paper investigates the application of ANN to model the nonlinear aircraft turbojet engine through black-box approach. For that purpose it was used real-world measurements of aircraft engine's fuel and rotation as input and output, respectively. In order to facilitate the design, the ANN was optimized aiming to determine the best topology according to the one-step-ahead and free-run simulation. The results obtained encourage the use of automatically generated ANN architectures for dynamic system modeling.",https://ieeexplore.ieee.org/document/8790171/,2019 IEEE Congress on Evolutionary Computation (CEC),10-13 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BIGCOM.2018.00037,Geo-Edge: Geographical Resource Allocation on Edge Caches for Video-on-Demand Streaming,IEEE,Conferences,"Geographical information has shown great potential in optimizing the resource allocation for Video-on-Demand (VoD) systems, e.g., the VoD service provider can allocate more bandwidth/computing resources to certain regions where more user requests are generated. Recently, the deployment of edge caches close to client users has become a cost-effective solution to the large-scale VoD system, as popular video content can be placed in edge caches so as to reduce the response latency of user requests, and save the bandwidth consumption on CDN edge servers. In this paper, we propose an edge-cache-assisted VoD system, named Geo-edge. The system develops a machine learning approach-a modified LSTM (Long Short Term Memory) network-to predict the amount and distribution of requests to each video, and proactively place appropriate video resources in the buffer of edge caches. We conduct emulations over the traces of real-world video sessions in ten different autonomous systems in the backbone network of China. Results show that the length of the path between source and clients could decrease by 29% with the help of edge caches, while Geo-Edge can significantly save the expenditure of CDN bandwidth under two typical charging policies of Internet service providers (i.e., a reduction of 45% when charged by overall throughput, or 35% when charged by peak bandwidth consumption).",https://ieeexplore.ieee.org/document/8488647/,2018 4th International Conference on Big Data Computing and Communications (BIGCOM),7-9 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9196608,Gershgorin Loss Stabilizes the Recurrent Neural Network Compartment of an End-to-end Robot Learning Scheme,IEEE,Conferences,"Traditional robotic control suits require profound task-specific knowledge for designing, building and testing control software. The rise of Deep Learning has enabled end-to-end solutions to be learned entirely from data, requiring minimal knowledge about the application area. We design a learning scheme to train end-to-end linear dynamical systems (LDS)s by gradient descent in imitation learning robotic domains. We introduce a new regularization loss component together with a learning algorithm that improves the stability of the learned autonomous system, by forcing the eigenvalues of the internal state updates of an LDS to be negative reals. We evaluate our approach on a series of real-life and simulated robotic experiments, in comparison to linear and nonlinear Recurrent Neural Network (RNN) architectures. Our results show that our stabilizing method significantly improves test performance of LDS, enabling such linear models to match the performance of contemporary nonlinear RNN architectures. A video of the obstacle avoidance performance of our method on a mobile robot, in unseen environments, compared to other methods can be viewed at https://youtu.be/mhEsCoNao5E.",https://ieeexplore.ieee.org/document/9196608/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRIS.2016.8066077,Gesture based robotic arm control for meal time care using a wearable sensory jacket,IEEE,Conferences,"This work presents the development of a wireless, low cost, wearable sensor jacket for the purpose of controlling a robot arm by mimicking the motion and behaviour of a humans arm. The intended use of our system is to provide remote daily nursing care by using our system from a distant place such as a nursing home or hospital, to control a stationed robotic arm placed in an elderly or patient's home. The final system is comprised of a wearable jacket which is embedded with IMU and flex sensors to detect and track the wearers arm movements and behaviour. The system is capable of detecting up to 5 degrees of freedom of the human arm and replicate these motions using a 6 DOF robot arm. The usability, accuracy and precision of our jacket system is evaluated through a user study and the results demonstrated that our system was more accurate and easier to use for operators than a conventional robotic arm joystick controller. In a water bottle transfer task our developed wearable jacket system demonstrated an average error distance of 29.36mm from the target point, while the results using the conventional joystick demonstrated an average error distance of 37.48mm. Furthermore subjects using our system were able to complete the transfer task in an average time of 44.1s per trial which was more efficient than the joystick method in which subjects averaged 55.55s per trial. Finally, we report a feasibility study with the jacket and a subject to demonstrate the capability of this system of giving a patient water to drink. The feasibility experiment showed an 86.66% success rate in giving a patient water via video stream teleoperation control.",https://ieeexplore.ieee.org/document/8066077/,2016 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS),17-20 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITR.2018.8736158,Google Map and Camera Based Fuzzified Adaptive Networked Traffic Light Handling Model,IEEE,Conferences,"Rising traffic congestion has turned into a certain issue as the number of vehicles on roads are increasing. This research study was conducted to develop `Google Map and Camera Based Fuzzified Adaptive Networked Traffic Light Handling Model'. The main road with six major junctions was selected as the target route for the project. During this study, we were able to plan a limit and control traffic congestion utilizing two neural networks which process together to provide an efficient, productive and optimized solution based on real-time situations. Real-time video streams and Google Map traffic layer were used as primary input sources to the system. The Main algorithm was used to reduce traffic at a specific point whereas secondary algorithm was used to produce optimum decisions for the overall network. As a further advancement, REST endpoint was implemented to get the best route considering all the accessible data. With the aid of the previously mentioned techniques, an optimal traffic management model was developed.",https://ieeexplore.ieee.org/document/8736158/,2018 3rd International Conference on Information Technology Research (ICITR),5-7 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP43922.2022.9747298,Graph Convolution for Re-Ranking in Person Re-Identification,IEEE,Conferences,"Nowadays, deep learning is widely applied to extract features for similarity computation in person re-identification (re-ID). However, the difference between the training data and testing data makes the performance of learned feature degraded during testing. Hence, re-ranking is proposed to mitigate this issue and various algorithms have been developed. However, most of existing re-ranking methods focus on replacing the Euclidean distance with sophisticated distance metrics, which are not friendly to downstream tasks and hard to be used for fast retrieval of massive data in real applications. In this work, we propose a graph-based re-ranking method to improve learned features while still keeping Euclidean distance as the similarity metric. Inspired by graph convolution networks, we develop an operator to propagate features over an appropriate graph. Since graph is the essential key for the propagation, two important criteria are considered for designing the graph, and different graphs are explored accordingly. Furthermore, a simple yet effective method is proposed to generate a profile vector for each tracklet in videos, which helps extend our method to video re-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501, Duke, and MARS, demonstrate the effectiveness of our proposed approach.",https://ieeexplore.ieee.org/document/9747298/,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",23-27 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3ICT.2019.8910276,Ground Operations Management using a Data Governance Dashboard,IEEE,Conferences,"An incident involving the use of chemical, biological, radiological, and nuclear (CBRN) materials might represent a significant challenge for crime scene investigators. The paper presents a full system architecture to assess the hazardous situations resulting from CBRN materials. This issue is crucial since there were a number of incidents that occurred in which forensics people could not reach the location either due to being unreachable or due to harmful emissions. The proposed solution integrates various inputs including data from sensors, video streaming, geo-data along with using Artificial Intelligence (AI) for good decision-making and data analysis. A geo-dashboard was also designed to demonstrate, in real-time, the collected data from several angles and according to various queries. It also monitors the performance in real-time. The topic is not new however the novelty of the proposed solution is the integration of multiple sources of data, applying deep neural nets and projecting the data and data analytics in real-time on a dashboard that displays the analysis and data from different perspectives considering the viewpoint of the individuals who will use that system. The paper also presents how the ROCSAFE multidisciplinary research project addresses the identified scenario. The project combines topics from robotics, sensor technology, analytical and situation awareness software, transforming data into knowledgeable insights to support the decision-making process.",https://ieeexplore.ieee.org/document/8910276/,"2019 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",22-23 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APSIPAASC47483.2019.9023182,Gun Detection in Surveillance Videos using Deep Neural Networks,IEEE,Conferences,"The ongoing epidemic of gun violence worldwide has compelled various agencies, businesses and consumers to deploy closed-circuit television (CCTV) surveillance cameras in attempt to combat this epidemic. An active-based CCTV system extends this platform to autonomously detect potential firearms within a video surveillance perspective. However, autonomously detecting a firearm across varying CCTV camera angles, depth and illumination represents an arduous task which has seen limited success using existing deep neural networks models. This challenge is in part due to the lack of available contextual hand gun information from CCTV images, which remains unresolved. As such, this paper introduces a novel large scale dataset of hand guns which were captured using a CCTV camera. This dataset serves to substantially improve the state-of-the-art in representation learning of hand guns within a surveillance perspective. The proposed dataset consist of 250 recorded CCTV videos with a total of 5500 images. Each annotated CCTV image realistically captures the presence of a hand gun under 1) varying outdoor and indoor conditions, and 2) different resolutions representing variable scales and depth of a gun relative to a cameras sensor. The proposed dataset is used to train a single-stage object detector using a multi-level feature pyramid network (i.e. M2Det). The trained network is then validated using images from the UCF crime video dataset which contains real-world gun violence. Experimental results indicate that the proposed dataset increases the average precision of gun detection at different scales by as much as 18% when compared to existing approaches in firearms detection.",https://ieeexplore.ieee.org/document/9023182/,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),18-21 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489438,Half-precision Floating Point on Spiking Neural Networks Simulations in FPGA,IEEE,Conferences,"The use of half-precision floating-point numbers (hFP) in simulations of spiking neural networks (SNN) was investigated. The hFP format is used successfully in computer graphics and video games for storage and data transfer. The IEEE 754-2008 standard settles that arithmetic operations must occur at least on single-precision floating-point format (sFP). This means that it is necessary to convert hFP to sFP for arithmetical operations and reconvert the results to hFP before storing it. The influence of successive conversions when simulating SNN is the main concern of this article. Three methods were used to evaluate the impact of hFP on SNNs: (i) F-I curve, (ii) subthreshold regime, and (iii) the time for the next spike. We have tested the leaky integrate-and-fire and the Izhikevich's neuron model; both presented similar results. The data show that SNNs simulated with sFP present equivalent results when compared to the ones simulated with hFP with identical topology. Such results are important because hFP requires half of the memory space, simpler buses, and lower bandwidth for transferring data. We may infer they require lower clock frequency consequently lower power consumption. These are essential factors for real-time simulation of SNN on embedded electronics. The sFP to hFP conversion circuits, and vice versa, may be implemented using few logical blocks in a field-programmable gate arrays (FPGA) with no relevant Iatency. We conclude that data in the hFP format are suitable for SNNs synthesized in FPGAs, even though such implementations require conversion circuits.",https://ieeexplore.ieee.org/document/8489438/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDIS.2018.00035,Hand Gesture Controlled Drones: An Open Source Library,IEEE,Conferences,"Drones are conventionally controlled using joysticks, remote controllers, mobile applications, and embedded computers. A few significant issues with these approaches are that drone control is limited by the range of electromagnetic radiation and susceptible to interference noise. In this study we propose the use of hand gestures as a method to control drones. We investigate the use of computer vision methods to develop an intuitive way of agent-less communication between a drone and its operator. Computer vision-based methods rely on the ability of a drone's camera to capture surrounding images and use pattern recognition to translate images to meaningful and/or actionable information. The proposed framework involves a few key parts toward an ultimate action to be taken. They are: image segregation from the video streams of front camera, creating a robust and reliable image recognition based on segregated images, and finally conversion of classified gestures into actionable drone movement, such as takeoff, landing, hovering and so forth. A set of five gestures are studied in this work. Haar feature-based AdaBoost classifier[1] is employed for gesture recognition. We also envisage safety of the operator and drone's action calculating the distance based on computer vision for this task. A series of experiments are conducted to measure gesture recognition accuracies considering the major scene variabilities, illumination, background, and distance. Classification accuracies show that well-lit, clear background, and within 3 ft gestures are recognized correctly over 90%. Limitations of current framework and feasible solutions for better gesture recognition are discussed, too. The software library we developed, and hand gesture datasets are open-sourced at project website.",https://ieeexplore.ieee.org/document/8367759/,2018 1st International Conference on Data Intelligence and Security (ICDIS),8-10 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00097,Handwriting Image Recognition Based on a GAN Model,IEEE,Conferences,"Big data is a new driver of world economic and societal changes. However, our research is based on using the GAN to solve the real application Deepfakes, which are created by artificial intelligence programmed to replace one person&#x2019;s likeness with another in recorded video, photo, pr audio. To tackle this issue, this paper proposed a GAN model based handwriting image recognition which can be generated new celebrities after showing it pictures of many real celebrities. Typically, Deepfakes are made using a neural network-based architecture, the most capable of which utilizes generative adversarial networks (GANs). This involves placing two neural networks in contest with one another: the first new data from the same statistical distribution as the training set, and the second attempts to discriminate data produced by the first from data in the original training set. The experimental results demonstrated that using the GAN model to solve Deepfakes can give individuals new tools for self-expression and integration in the online world.",https://ieeexplore.ieee.org/document/9696103/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISVLSI.2016.101,Hardware Design Automation of Convolutional Neural Networks,IEEE,Conferences,"Convolutional Neural Networks (CNNs) are a variation of feed-forward Neural Networks inspired by the biological process in the visual cortex of animals. The interest in this supervised learning algorithm has rapidly grown in many fields like image and video recognition and natural language processing. Nowadays they have become the state of the art in various applications like mobile robot vision, video surveillance and Big Data analytics. The specific computation pattern of CNNs results to be highly suitable for hardware acceleration, in fact different types of accelerators have been proposed based on GPU, Field Programmable Gate Array (FPGA) and ASIC. In particular, in the embedded systems context, due to real time and power consumption challenges, it is crucial to find the right tradeoff between performance, energy efficiency, fast development round and cost. This work proposes a framework meant as a tool for the user to accelerate and simplify the design and the implementation of CNNs on FPGAs by leveraging High Level Synthesis, still providing a certain level of customization of the hardware design.",https://ieeexplore.ieee.org/document/7560201/,2016 IEEE Computer Society Annual Symposium on VLSI (ISVLSI),11-13 July 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/SPA.2018.8563404,Hardware implementation of the Gaussian Mixture Model foreground object segmentation algorithm working with ultra-high resolution video stream in real-time,IEEE,Conferences,The following topics are dealt with: medical image processing; feature extraction; acoustic signal processing; image segmentation; object detection; biomedical MRI; learning (artificial intelligence); diseases; video signal processing; image coding.,https://ieeexplore.ieee.org/document/8563404/,"2018 Signal Processing: Algorithms, Architectures, Arrangements, and Applications (SPA)",19-21 Sept. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR.2018.00039,Head-Mounted FOV Simulator for User Testing of Maritime Object Detection Tasks,IEEE,Conferences,"Search and Rescue (SAR) activities involve missions to rescue people that are lost or in danger, in environments including sea, land, mountainous terrain or deserts. Today, still-and video-camera technologies e.g. mounted on drones, are increasingly being used to capture views that may not easily be accessible. To conduct user tests of different image presentations, we implement a head-mounted 360° FOV simulator to gain new insights into how to improve object detection by operators. We describe the implemented simulator and give a brief overview of the demonstrator to be presented at the conference.",https://ieeexplore.ieee.org/document/8613658/,2018 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),10-12 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CloudNet.2018.8549558,Hi-Clust: Unsupervised Analysis of Cloud Latency Measurements Through Hierarchical Clustering,IEEE,Conferences,"Latency is nowadays one of the most relevant network and service performance metrics reflecting end-user experience. With the wide adoption and deployment of delay-sensitive applications in the Cloud (e.g., gaming, interactive video conferencing, corporate services, etc.), monitoring and analysis of Cloud service latency is becoming increasingly relevant for Cloud service providers, tenants and even users. Traditional network monitoring approaches based on time-series analysis and thresholding are capable of raising alarms when anomalous events arise, but are not applicable to detect correlations among multiple monitored dimensions, necessary to provide an adequate interpretation of an anomaly. In this paper we present Hi-Clust, an unsupervised-based approach for analyzing and interpreting anomalies in multi-dimensional network data, through the application of hierarchical clustering techniques. While Hi-Clust is applicable to the analysis of different types of nested or hierarchically structured data, we particularly focus on the analysis of Cloud service latency, using active measurements collected from geographically distributed vantage points. We implement and benchmark multiple density-based clustering approaches for Hi-Clust over four weeks of real multidimensional Cloud service latency measurements. Using the most robust underlying clustering algorithm from the benchmark, we show how to automatically extract and interpret anomalous Cloud service behavior with Hi-Clust. In addition, we show the advantages of Hi-Clust over traditional threshold-based approaches for detecting and interpreting anomalous behavior, through practical examples over the collected measurements.",https://ieeexplore.ieee.org/document/8549558/,2018 IEEE 7th International Conference on Cloud Networking (CloudNet),22-24 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EVENT.2001.938872,Hierarchical unsupervised learning of facial expression categories,IEEE,Conferences,"We consider the problem of unsupervised classification of temporal sequences of facial expressions in video. This problem arises in the design of an adaptive visual agent, which must be capable of identifying appropriate classes of visual events without supervision to effectively complete its tasks. We present a multilevel dynamic Bayesian network that learns the high-level dynamics of facial expressions simultaneously, with models of the expressions themselves. We show how the parameters of the model can be learned in a scalable and efficient way. We present preliminary results using real video data and a class of simulated dynamic event models. The results show that our model correctly classifies the input data comparably to a standard event classification approach, while also learning the high-level model parameters.",https://ieeexplore.ieee.org/document/938872/,Proceedings IEEE Workshop on Detection and Recognition of Events in Video,8-8 July 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCES.2017.8275360,High performance computing model for action localization in video,IEEE,Conferences,"Last decade, the field of High Performance Computing (HPC) has been rapidly developing. This advances in computational power and hardware resources motivated real-time video application. One of the most challenging application is “action detection” inside video. Beside its tremendous complication, it requires an intensive computation power for video analysis. This work extends our previous work in this problem by addressing both multi-core CPUs and Graphical Processing Unit (GPU) for implementing the video action localization system (previous work). In this paper, multiple (HPC) architectures are implemented to speedup real-time video streaming analytics. The experiments are conducted on the previously implemented recognition system which exploits Deep Learning models to localize an action inside a video. The results illustrates that CUDA implementation accomplished 8x speedup while CPU accomplished 5.2x speedup. The experiments were done on Sports-IM dataset with 487 action.",https://ieeexplore.ieee.org/document/8275360/,2017 12th International Conference on Computer Engineering and Systems (ICCES),19-20 Dec. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCCC.1996.493610,High quality desktop video conferencing using agent based systems,IEEE,Conferences,"This paper describes a new model for building high quality desktop video conferencing systems. The model has a distributed agent based architecture as the underlying framework, where agents cooperate and coordinate efficiently to maintain good quality of service parameters during the video conference session. The model is self-adaptive and permits the system to adjust itself in real time to optimally respond to a set of rapidly changing conditions and parameters. It employs graceful degradation to enable the conference to continue, possibly at a lower pace and perhaps with worse performance, even if some resources (hardware or software) are disabled. Basic learning mechanisms allow the computational unit, an agent, to improve in time as more knowledge about the environment is acquired. A set of performance enhancers are used to compensate hardware overloads and maintain good system throughput when external factors are affecting the environment. The performance enhancers have quality evaluators, a set of internal monitors that analyze the system behavior and activate/deactivate the compensation loop.",https://ieeexplore.ieee.org/document/493610/,Conference Proceedings of the 1996 IEEE Fifteenth Annual International Phoenix Conference on Computers and Communications,27-29 March 1996,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAD45719.2019.8942082,High-performance Hardware Architecture for Tensor Singular Value Decomposition: Invited Paper,IEEE,Conferences,"Tensor provides a brief and natural representation for large-scale multidimensional data by way of appropriate low-rank approximations, thus we can discover significant latent structures of complex data and generalize data representation. To date, tensor has gained tremendous success in various science and technology fields, especially in machine learning and big data applications. However, tensor computation, especially tensor decomposition, is usually expensive due to the inherent large-size characteristic of tensors, and hence would potentially hinder their future wide deployment. In this paper, we develop a hardware architecture to accelerate tensor singular value decomposition (t-SVD), which is a new tensor decomposition technique that has been successfully applied to high-dimensional data classification and video recovery. Specifically, design consideration of each key computing unit is analyzed and discussed. Then, the proposed t-SVD hardware architecture is implemented and synthesized using CMOS 28nm technology. Comparison with real-world CPU-based implementations shows that the proposed hardware accelerator is expected to provide average 14× speedup on various t-SVD workloads.",https://ieeexplore.ieee.org/document/8942082/,2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),4-7 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT51525.2021.9579613,Highway Drivers Drowsiness Detection System Model with R-Pi and CNN technique,IEEE,Conferences,"In order to drive the state of art technology for driver safety, the demand for Artificial Intelligence and Machine learning precision approaches are popularly used in the modern vehicles. This paper presents a driver drowsiness detection framework which can be placed in the vehicle as a standalone design unit. In this organised deep learning CNN frame work, a comprehensive analytics strategy is used to detect drowsiness in drivers. There are two distinct phases used in this experimental framework namely image data acquisition followed by identification of region of interest (ROI) in phase 1. In the next phase deep learning algorithm is used to classify and predict the outcome. The object detection training is using the Haar Cascade Classifier which facilitates to obtain an accurate model. The model is trained by exploiting the open source tools namely Keras along with Tensorflow as backend. The hardware design includes Raspberry pi model 4, pi camera module V 1.3 and a buzzer. The video capturing, labeling of open or closed eyes and the score are obtained as a real time status with the buzzer output to alert the driver during drowsiness state. Training phase and Raspberry hardware design phase are deployed parallelly. The standalone hardware module is uploaded with the complete training AI software. Real time image acquisition by the design platform and predicting the driver drowsiness detection, depicts an accuracy of 95% to 96%. The activation function ReLU used during the prediction state in CNN improves the computational efficiency of the design model.",https://ieeexplore.ieee.org/document/9579613/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QoMEX48832.2020.9123142,How Deep is Your Encoder: An Analysis of Features Descriptors for an Autoencoder-Based Audio-Visual Quality Metric,IEEE,Conferences,The development of audio-visual quality assessment models poses a number of challenges in order to obtain accurate predictions. One of these challenges is the modelling of the complex interaction that audio and visual stimuli have and how this interaction is interpreted by human users. The No-Reference Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with this problem from a machine learning perspective. The metric receives two sets of audio and video features descriptors and produces a low-dimensional set of features used to predict the audio-visual quality. A basic implementation of NAViDAd was able to produce accurate predictions tested with a range of different audio-visual databases. The current work performs an ablation study on the base architecture of the metric. Several modules are removed or re-trained using different configurations to have a better understanding of the metric functionality. The results presented in this study provided important feedback that allows us to understand the real capacity of the metric's architecture and eventually develop a much better audio-visual quality metric.,https://ieeexplore.ieee.org/document/9123142/,2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX),26-28 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AISP53593.2022.9760596,Human Activity Recognition with Privacy Preserving using Deep Learning Algorithms,IEEE,Conferences,"Human activity recognition is an extensively researched topic in the field of computer vision. Recognizing human activities without revealing a person&#x2019;s identity is one such use case. To solve this, we propose a practical method for human activity recognition (HAR) while maintaining anonymity. It captures and distributes data from a variety of sources while respecting the privacy of the individuals concerned. At the core of our approach is (DBN-RGMAA) based on deep neural networks, which are not only more accurate but can also be deployed in real-time video surveillance systems. Hence, this work presents a deep learning-based scheme for privacy-preserving human activities. Initially, for extracting the features from raw video data, a Deep Belief Network (DBN) is used. To increase the HAR identification rate, Hybrid Deep Fuzzy Hashing Algorithm (HDFHA) is employed to capture dependencies between two actions. Finally, the privacy model enhances the privacy of humans while permitting a highly accurate approach towards action recognition by the Recursive Genetic Micro-Aggregation Approach (RGMAA). The implementation is executed and the performances are evaluated by Accuracy, Precision, Recall, and F1 Score. A dataset named HMDB51 is used for empirical study. Our experiments using the Python data science platform reveal that the OPA-PPAR outperforms existing methods.",https://ieeexplore.ieee.org/document/9760596/,2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP),12-14 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IAICT50021.2020.9172031,Human Target Search and Detection using Autonomous UAV and Deep learning,IEEE,Conferences,"An Unmanned Aerial Vehicle (UAV) is an airborne system or pilotless aircraft which is remotely controlled by a human operator on ground or by an onboard computer such that the vehicle moves autonomously. The range of applications in which UAVs are used is very large. This paper describes the application of developing an autonomous surveillance system using an UAV to identify a given target and/or objects of interest in the terrain over which it flies. Such a system can be used in rescue operations, especially in remote areas where physical access is difficult. It can also be used for military operations, farming or any field where surveillance of a given land area is required. The UAV developed in this work is capable of object detection. A mounted camera is used to give visual feedback, and an onboard processing unit runs image recognition software to identify the target in real time. Optimal algorithms are used to search and find the target from the given search area. After recognition of the target, the UAV can either be used to hold its position so as to have a video feed of the target, or return to its base station once the coordinates have been estimated using GPS modules or relay the GPS location to the base station. This paper describes the implementation of the hardware and software components that lead to the realization of the UAV and the application of object detection. The details of a new search algorithm and an example of object detection is presented . The work presented in this paper is the first part in the attempt to develop a cluster of UAVs meant to work in collaboration to be deployed for search and rescue operations.",https://ieeexplore.ieee.org/document/9172031/,"2020 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT)",7-8 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZ-IEEE.2014.6891833,Human behavioural analysis with self-organizing map for ambient assisted living,IEEE,Conferences,"This paper presents a system for automatically classifying the resting location of a moving object in an indoor environment. The system uses an unsupervised neural network (Self Organising Feature Map) fully implemented on a low-cost, low-power automated home-based surveillance system, capable of monitoring activity level of elders living alone independently. The proposed system runs on an embedded platform with a specialised ceiling-mounted video sensor for intelligent activity monitoring. The system has the ability to learn resting locations, to measure overall activity levels and to detect specific events such as potential falls. First order motion information, including first order moving average smoothing, is generated from the 2D image coordinates (trajectories). A novel edge-based object detection algorithm capable of running at a reasonable speed on the embedded platform has been developed. The classification is dynamic and achieved in real-time. The dynamic classifier is achieved using a SOFM and a probabilistic model. Experimental results show less than 20% classification error, showing the robustness of our approach over others in literature with minimal power consumption. The head location of the subject is also estimated by a novel approach capable of running on any resource limited platform with power constraints.",https://ieeexplore.ieee.org/document/6891833/,2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),6-11 July 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCIT.2004.1412881,Human moving detection by frequency analysis using video signal,IEEE,Conferences,"In a motion detection area, technology such as pattern recognition through pattern matching and/or a neural network is often used. Although the algorithm ""motion vector detection"" for checking the motion in MPEG has been exploited, it is only for detecting the highest correlation block of a local area between adjacent frames and does not detect a motion of the same objective portion exactly. Hence it has a little lack of reliability for the motion detection. In this research the difference between the continuation frames of a video picture is utilized. The DCT of the difference can be implemented in a very short time, the alteration of the amplitude of each frequency domain is classified as a pattern in real time, and finally the state of the motion is estimated. In this way the weak point of the conventional method is also avoidable.",https://ieeexplore.ieee.org/document/1412881/,"IEEE International Symposium on Communications and Information Technology, 2004. ISCIT 2004.",26-29 Oct. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIP.2010.41,Hybrid Particle Swarm Optimized Wavelet Network for Video OCR,IEEE,Conferences,"A hybrid particle swarm optimization (PSO)-based wavelet neural network (WNN) for Video OCR is presented in this paper. Video OCR is an important task towards enabling automatic content-based retrieval of digital video databases. However, since text is often displayed against a complex background, its detection and extraction is a challenging problem. In this paper, wavelet transformation is done on the different current and the wavelet coefficients are obtained. The wavelet coefficients are given as inputs to the wavelet neural network trained by particle swarm optimization (PSO-WNN). The final network output of real text regions is different from those non-text regions. The experimental results demonstrate the effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/5669006/,2010 Third International Symposium on Information Processing,15-17 Oct. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ.2008.4762065,Hybrid face detection system with robust face and non-face discriminability,IEEE,Conferences,"While the face detection algorithm proposed by Viola and Jones has enough detection speed in still or video images and is applicable in practical applications, it is not reliable yet. That is, the number of false positives increases as background complexity increases. And sometimes it reports false positives, even for simple backgrounds. This problem can get the automatic face recognition systems (AFRSs) into trouble. In this paper, a hybrid system for face detection is introduced concerning itpsilas applicability in real world applications. Our approach is based on Viola and Jonespsilas work and uses Radial Basis Neural Network (RBFNN). The main characteristic of our approach is its robust face and non-face discriminability. Due to the extensive experiments, it is shown that the proposed system has decreased about 90% of false positives reported by the Viola and Jones algorithm.",https://ieeexplore.ieee.org/document/4762065/,2008 23rd International Conference Image and Vision Computing New Zealand,26-28 Nov. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EEBDA53927.2022.9744938,Ideal Construction of Chatbot Based on Intelligent Depression Detection Techniques,IEEE,Conferences,"Depression detection usually describes the task of recognizing the depression level or tendency from certain articles, posts, or even consult. Such a technique might be widely used in the diagnosis when the psychologists or consulters are present. However, when such a type of diagnosis or service comes online, there would be many types of challenges, such as detection through some amounts of words, detection through pictures or video streaming of patients, and in some cases, how depression could be solved without the participation of a real doctor. This article generally discussed some common methods to solve these challenges and some construction designs that might help depressed patients. Currently, depression detection is still an innovative field to be explored, and many techniques do not seem to be mature. According the investigation on the techniques related to the construction of depression chatbots, we recognized that some tasks-based function can be realized, it would be quite difficult for such chatbot to really understand feelings and emotions, and consequently the chatbot will have many difficulties to produce neither therapies for the patients nor a meaningful conversation to offer assistance. In the future, more advanced methods will come out which can capture more details of the mental movements of the patients, and one day it will no longer be impossible for a computer to understand humans&#x0027; subtle internal.",https://ieeexplore.ieee.org/document/9744938/,"2022 IEEE International Conference on Electrical Engineering, Big Data and Algorithms (EEBDA)",25-27 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECO.2019.8760100,Image Enhancement by Jetson TX2 Embedded AI Computing Device,IEEE,Conferences,We show how library function used for image enhancement can be tailored and optimized to use the full capacity of NVIDIA Jetson TX2 embedded AI computing device. We use parallel processing on CPU and GPU devices to achieve real-time video enhancement. We also outlined further improvement of image enhancement process by machine learning implementation.,https://ieeexplore.ieee.org/document/8760100/,2019 8th Mediterranean Conference on Embedded Computing (MECO),10-14 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636439,Image-Based Joint State Estimation Pipeline for Sensorless Manipulators,IEEE,Conferences,"Motion planning is a largely solved problem for robot arms with joint state feedback, but remains an area of research for sensorless manipulators such as toy robot arms and heavy equipment such as excavators and cranes. A promising approach to this problem is deep learning, which employs a pre-trained convolutional neural network to identify manipulator links and estimate joint states from a monocular camera video feed. Whereas manual labeling of training image sets is tedious and non-transferable, a simulation environment can automatically generate labeled training image sets of any size. The issue is the gap between simulated and real-world images. This paper solves this problem by implementing a Generative Adversarial Network. The complete joint state estimation pipeline is implemented and tested in hardware experiments to validate our proposed approach.",https://ieeexplore.ieee.org/document/9636439/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCWAMTIP53232.2021.9674087,Immersive 4D Intelligent Interactive Platform Based on Deep Learning,IEEE,Conferences,"With the advent of the Internet era, the cost of realizing virtual characters has been greatly reduced. The high cost and low efficiency exhibited by traditional virtual technology can&#x0027;t meet social needs. People are seeking fast, convenient and accurate virtual character reproduction technology. Through researching the characteristics of the characters appearance, language habits, voice tone and so on, the research direction of this paper is to simulate and reshape voice and images, construct a cloud platform-based on user-side and client-side, and integrate deep learning, natural language processing, digital twins and other technologies to an immersive 4D intelligent interactive platform. The platform under in the form of application software provides integrated services of intelligent voice interaction and virtual character interaction. In the industrial diagnosis mode, the transition from traditional video retention and voice retention to a new intelligent voice recognition and simulation mode is realized.",https://ieeexplore.ieee.org/document/9674087/,2021 18th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP),17-19 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2016.7511226,Implementation and evaluation of adaptive video streaming based on Markov decision process,IEEE,Conferences,"In HTTP-based adaptive streaming systems, media server simply stores video content segmented into a series of small chunks coded in different qualities and sizes. The decision for next chunk's quality level to achieve a high quality viewing experience is left to the client which is a challenging task, especially in mobile environment due to unexpected changes in network bandwidth. Using computer simulations, previous work has demonstrated that Markov decision process (MDP) is very effective for such decision making and that it can reduce video freezing or re-buffering events drastically compared to other methods of adaptation. However, to date there has been no practical implementation and evaluation of MDP-based DASH players. In this work, we extend a publicly available DASH player recently released by DASH industry forum to realise a real DASH player that implements MDP-based video adaptation. We implement two alternative MDP optimisation algorithms, value iteration and Q learning and evaluate their performances in real driving conditions under 300 minutes of video streaming. Our results show that value iteration and Q learning reduce video freezing by a factor of 8 and 11, respectively, compared to the default decision making algorithm implemented in the public DASH player.",https://ieeexplore.ieee.org/document/7511226/,2016 IEEE International Conference on Communications (ICC),22-27 May 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSAI53272.2021.9609705,Implementation of Face Recognition Method for Attendance in Class,IEEE,Conferences,"Face recognition has become one of the key aspects of computer vision. In this paper, an automatic face recognition system is proposed based on appearance-based features that focus on the entire face image rather than local facial features. The first step in a face recognition system is face detection. Viola-Jones face detection method that capable of processing images extremely while achieving high detection rates is used. It is a computer application for automatically identifying a person from a still image or video frame. In this paper, we proposed an automated attendance management system. In this research, our effort is to develop a system that allows easy attendance marking using real-time face recognition. The system is based on the Viola-Jones algorithm, this research provides a more efficient and secure Attendance for students at Binus University.",https://ieeexplore.ieee.org/document/9609705/,2021 1st International Conference on Computer Science and Artificial Intelligence (ICCSAI),28-28 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PAAP.2011.29,Implementation of G.729A on Embedded SIMD Processor,IEEE,Conferences,"This paper addresses a real-time implementation of multi-channel, high quality G.729A speech codec based on an embedded SIMD processor, which is used in a SIP Video Phone. A series of strategies are designed for the special characteristics of the processor and the G.729A, including the memory management and SIMD decomposing. The profile shows that the dramatic improvement is achieved. Less than 20% CPU load ensures the video codec's smooth output of the video phone.",https://ieeexplore.ieee.org/document/6128499/,"2011 Fourth International Symposium on Parallel Architectures, Algorithms and Programming",9-11 Dec. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC.2012.6386793,Implementation of WSN application service using people detection algorithm,IEEE,Conferences,"In this paper, we implements intrusion detection system for video surveillance which works well in wireless sensor networks (WSN) environment. Our system has been implemented using open geospatial consortium standard, and operated in the android platform of mobile environment. We also have been implemented intruder detection by machine learning technique. As a result, our system can achieve real-time and robust people classification in diverse scenes.",https://ieeexplore.ieee.org/document/6386793/,2012 International Conference on ICT Convergence (ICTC),15-17 Oct. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCT52121.2021.9616625,Implementation of YOLOv4 Algorithm for Multiple Object Detection in Image and Video Dataset using Deep Learning and Artificial Intelligence for Urban Traffic Video Surveillance Application,IEEE,Conferences,"Artificial intelligence and machine learning have great ability to solve real-time problems related to various fields and have many applications. One such subfield is deep learning, deep learning has many architectures like DNN, CNN and RNN. One of the major applications of deep learning is object detection. YOLO a SOTA object detector considered to be a smart convolutional neural network having capability of detecting objects in the image, classifying them accordingly and localizing the object perfectly with annotations. YOLOv4 is employed in this work for multiple object detection in image and video for traffic surveillance applications trained using a custom dataset created with Indian road traffic images. 9 different classes namely Car, Bus, Van, Truck, Two-wheeler, Auto, Person, Bicycle and Mini truck are considered in creating custom dataset. Proposed custom model trained for 500 epochs with a custom image dataset has achieved a training mAP of 92% and a training loss of 0.001. Achieved an accuracy of 99.28%, sensitivity of 94.44% and MCC of 96.78% on unseen or test dataset. Proposed model predicted all the classes with an average precision of 98.32%. This proves that our model is robust and proficient in detecting road traffic classes and has scope in developing traffic surveillance systems.",https://ieeexplore.ieee.org/document/9616625/,"2021 Fourth International Conference on Electrical, Computer and Communication Technologies (ICECCT)",15-17 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2015.7372944,Implementation of an artificial neural network in recognizing in-flight quadrotor images,IEEE,Conferences,"This paper shows an implementation of a feedforward artificial neural network capable of recognizing images of the CrazyFlie 2.0 quadrotor during flight. The network is to be used in a real-time quadrotor swarming application and has to be able to successfully differentiate pictures that show a quadrotor in flight versus pictures that do not. The network was trained using a standard backpropagation algorithm and images taken from a video of the said quadrotor in flight. These images were divided into three groups: a training set and validation set for the training stage, and a testing set for verification of the trained neural network. The results showed that the neural network was able to correctly identify the images in the testing phase 100 percent of the time while achieving a 94 percent accuracy for the images in the testing set.",https://ieeexplore.ieee.org/document/7372944/,TENCON 2015 - 2015 IEEE Region 10 Conference,1-4 Nov. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONIELECOMP.2014.6808580,Implementation of an embedded system on a TS7800 board for robot control,IEEE,Conferences,"Growing Functional Modules (GFM) learning based controllers need to be experimented on real robots. In 2009, looking to develop a flexible and generic embedded interface for such robots, we decided to use a TS-7800 single board computer (SBC) with a Debian Linux operating system. Despite the many advantages of this board, implementing the embedded system has been a complex task. This paper describes the implementation of protocols through the TS-7800 different ports (RS232, TCP/IP, USB, analog and digital pins) as well as the connection of external boards (TS-ADC24, TS-DIO64, SSC-32 and LCD display). This implementation was required to connect a large range of actuators, sensors and other peripherals. Furthermore, the architecture of the embedded system is exposed in detail, including topics such as the XML configuration file that specifies the peripherals connected to the SBC, the concept of virtual sensors, the implementation of parallelism and the embedded system interface launcher. Technical aspects such as the optimization of video capture and processing are detailed because their execution required specific compilers versions, EABI emulation and extra libraries (openCV libjpg and libpngand libv4l). The final embedded system was implemented in a humanoid robot and connected to the GFM controller in charge of developing its equilibrium subsystem.",https://ieeexplore.ieee.org/document/6808580/,"2014 International Conference on Electronics, Communications and Computers (CONIELECOMP)",26-28 Feb. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIIBMS52876.2021.9651639,Implementation of kitchen food safety regulations detection system based on deep learning,IEEE,Conferences,"Food safety has always been the focus of attention. In order to ensure the food hygiene of operational restaurants, the kitchen of restaurants is required to equip with monitoring to prevent safety and hygiene problems caused by non-standard operations. The traditional monitoring method is to view the surveillance video by manpower, which is extremely labor-consuming and time-consuming, with low accuracy and poor operability. Aiming at this problem, this paper proposes a system with a deep learning method to automatically identify the violation of kitchen staff. The system will use the camera in the kitchen scene to detect and identify a variety of non-standard behaviors affecting food safety in the collected video stream, including not wearing chef&#x0027;s hat and smoking, so as to realize intelligent supervision of kitchen food safety and achieve good results.",https://ieeexplore.ieee.org/document/9651639/,2021 6th International Conference on Intelligent Informatics and Biomedical Sciences (ICIIBMS),25-27 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IranianCEE.2017.7985298,Implementation real-time gender recognition based on facial features using a hybrid neural network Imperialist Competitive Algorithm,IEEE,Conferences,"In this paper, we have proposed a robust approach for developing an automatic system for gender classifying from a facial image at video files or image files. In the proposed algorithm, feed-forward artificial neural network manages the classification problem by use of Imperialism Competitive Algorithm (ICA) to achieve the best weights of the neural network instead of old gradient based methods, like back propagation. In this paper we first extract some facial segments from given videos by Viola Jones algorithm. Then feature selection through Non-Dominated Sorting Genetic Algorithm-II (NSGA-II) is done and finally a hybrid Artificial Neural Network (ANN) with ICA (ANN-ICA) performs classification. Experimental results show that combining the feature extraction techniques with the ANN-ICA for classification, the performance of gender classification improves significantly and reached a recognition rate of 94.3%.",https://ieeexplore.ieee.org/document/7985298/,2017 Iranian Conference on Electrical Engineering (ICEE),2-4 May 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE.2009.5012345,Improved machine learning techniques for low complexity MPEG-2 to H.264 transcoding using optimized codecs,IEEE,Conferences,"This paper discusses techniques for efficiently implementing a Mpeg-2 to H.264 video transcoder. The transcoding results reported in the literature are based on a reference implementation and may not reflect the true performance gains obtained in real world systems. We have developed low complexity transcoding algorithms and have implemented these solutions using highly optimized encoder and decoder implementations available from Intel. The transcoding algorithms are based on exploiting the mode decision knowledge inherent in the decoded MPEG-2 data. Machine learning techniques are used to make accurate and low-complexity H.264 MB encoding mode decisions. The results show that the proposed transcoder reduces the complexity by 50% without a significant loss in PSNR. This performance improvement in production quality transcoders, and demonstrates the practicality of machine learning based video transcoding algorithms.",https://ieeexplore.ieee.org/document/5012345/,2009 Digest of Technical Papers International Conference on Consumer Electronics,10-14 Jan. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM41043.2020.9155384,Improving Quality of Experience by Adaptive Video Streaming with Super-Resolution,IEEE,Conferences,"Given high-speed mobile Internet access today, audiences are expecting much higher video quality than before. Video service providers have deployed dynamic video bitrate adaptation services to fulfill such user demands. However, legacy video bitrate adaptation techniques are highly dependent on the estimation of dynamic bandwidth, and fail to integrate the video quality enhancement techniques, or consider the heterogeneous computing capabilities of client devices, leading to low quality of experience (QoE) for users. In this paper, we present a super-resolution based adaptive video streaming (SRAVS) framework, which applies a Reinforcement Learning (RL) model for integrating the video super-resolution (VSR) technique with the video streaming strategy. The VSR technique allows clients to download low bitrate video segments, reconstruct and enhance them to high-quality video segments while making the system less dependent on estimating dynamic bandwidth. The RL model investigates both the playback statistics and the distinguishing features related to the client-side computing capabilities. Trace-driven emulations over real-world videos and bandwidth traces verify that SRAVS can significantly improve the QoE for users compared to the state-of-the-art video streaming strategies with or without involving VSR techniques.",https://ieeexplore.ieee.org/document/9155384/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/QoMEX48832.2020.9123107,Inclusion of End User Playback-Related Interactions in YouTube Video Data Collection and ML-Based Performance Model Training,IEEE,Conferences,"Solutions relying on machine learning (ML) models that address the challenge of in-network QoE estimation for HTTP adaptive video streaming often neglect user behavior and its impact on performance estimation. End user playback-related interactions impact network traffic characteristics, thus having a (predominantly negative) impact on the performance of models that estimate Key Performance Indicators (KPIs) from encrypted traffic. The biggest challenge in incorporating user interactions when training and testing ML models lies in the wide range of different potential interactions, multiple interaction occurrences, various combinations of different interactions, and different time points of execution spanning across a video streaming session. With the aim of training models applicable for deployment in real networks, but also in an effort to optimize the overall process of model training, we systematically investigate the relationship between classification accuracy of models trained on data with and without certain user interactions. Our results for YouTube videos, played using the native YouTube app on a mobile device under emulated broadband network conditions, show that the impact of interactions on model performance highly depends on the target KPI being classified. In certain cases, the model training process may be simplified by reducing the need to consider a wide range of interaction scenarios.",https://ieeexplore.ieee.org/document/9123107/,2020 Twelfth International Conference on Quality of Multimedia Experience (QoMEX),26-28 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASC52595.2021.9594425,Indoor Autonomous Powerline Inspection Model,IEEE,Conferences,"This paper presents the development of an autonomous system that leverages the Quanser Qdrone to perform above-ground indoor autonomous powerline inspections. The powerline infrastructure is exposed to various extreme weather conditions that create an operational concern for utility companies. Frequent inspections ensure the safe operation of a power transmission grid. There are mainly two methods of examination, ground and air [1]. The ground inspections are often slow and challenging due to the rough terrain, utility pole height, and inaccessible remote areas. The aerial inspections are accomplished by deploying helicopters that are expensive to operate, maintain, and repair. As an alternative, Unmanned Aerial Vehicles (UAVs) are being widely adopted for both surveillance and analysis throughout the energy and utility industries. UAVs are being used for inspections of utility towers as well as powerlines as they are energy efficient, user friendly, and convenient. Drone video capturing allows for safer, faster, and more cost-effective solutions to powerline and utility tower inspections as the user does not have to leave the ground aside from repairs. The objective of this project was to develop an autonomous UAV system to detect and track powerlines and utility poles to perform fault inspections of their electrical and material components. The proposed algorithm used a state flow machine paired with an image recognition neural network to make decisions for searching, identifying, and flying along utility poles and powerlines. The proposed system was implemented using MathWorks MATLAB and Simulink with Quarc, a third-party toolbox designed by Quanser, enabling real-time applications with the QDrone. The project yielded an algorithm that would autonomously fly the Quanser QDrone through a scan of the local area, leverage a neural network, PowerNet, to locate an initial tower within the work area, follow attached powerlines if there are any, and locate the secondary tower. Once the inspection was completed, the QDrone would return to the home point and land.",https://ieeexplore.ieee.org/document/9594425/,2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),3-7 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRCE50905.2020.9199256,Industrial Implementation and Performance Evaluation of LSD-SLAM and Map Filtering Algorithms for Obstacles Avoidance in a Cooperative Fleet of Unmanned Aerial Vehicles,IEEE,Conferences,"In this paper we present an industrial implementation and performance evaluation of the problem of obstacles detection by drones using autonomous navigation systems. The software module that has been developed as well as the tests conducted are part of a large industrial R&D Vitrociset project called SWARM: an AI-Enabled Command and Control (C&C) system, able to execute and review ISR missions for mini/micro cooperative fleets of heterogeneous UAVs. The presented software module, that is currently under test, has been developed to recognize obstacles and drive correctly the drones, using images acquired by low cost RGB video cameras, whose features of lightness and reduced size allow them to be installed on mini/micro UAVs. Moreover, this setup does not require special calibration and preconfiguration processes like the ones necessary for example using stereo video camera systems. The real-time recognition of obstacles in the surrounding environment has been obtained and evaluated through the implementation, performance evaluation and tests of the LSD-SLAM and map filtering algorithms; the core of the study has been realized starting from the integration of these algorithms with a simulated drone in a synthetic environment. The areas of interest have been identified through the filtering of a computer generated map: the module was then integrated into the SWARM project platform, allowing the control of a single drone's movement and making it ready for use in a cooperative fleet environment.",https://ieeexplore.ieee.org/document/9199256/,2020 3rd International Conference on Intelligent Robotic and Control Engineering (IRCE),10-12 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWCMC48107.2020.9148208,Inferring Quality of Experience for Adaptive Video Streaming over HTTPS and QUIC,IEEE,Conferences,"Nowadays, Internet traffic encryption is rapidly increasing due to privacy and security concerns. This is because of the massive usage of end-to-end security protocols over Internet such as HTTPS and QUIC. The encryption trend will continue to rapidly increase in the future, and this trend concerns video streaming applications as well. Network providers face a serious challenge in managing their networks due to such widespread deployment of end-to-end security protocols. These operators need to have a clear visibility into traffic on their networks to monitor and manage both quality of experience (QoE)-and-service (QoS) impairments in popular video streaming services, in the most effective and efficient manner. Moreover, so many factors that influence QoE need to be taken care of to get an acceptable user experience. Most of the existing solutions use the deep packet inspection to infer these factors from the encrypted traffic. However, these solutions are inefficient, most of the time, leading to low QoE inference accuracy. To bridge this gap, we propose a machine-learning based solution that leverages a random forest classifier for a better QoE inference accuracy. The proposed solution uses network-and-transport layer information to infer QoE factors such as startup delay and stall events. It helps the network providers to react quickly and in real time for any impairments in the QoE of the encrypted video traffic. We evaluate our solution using an HTTP adaptive streaming service (YouTube) that uses HTTPS and QUIC protocols. Our experimental results show that our solution achieves up to 91.1% classification accuracy for HTTPS and up to 87.3% for QUIC.",https://ieeexplore.ieee.org/document/9148208/,2020 International Wireless Communications and Mobile Computing (IWCMC),15-19 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BMEiCON53485.2021.9745207,Injury Identification Using Video Magnification,IEEE,Conferences,"Despite the rapid technological advancements and developments that are achieved today, correct injuries diagnose is still a regular occurring issue. There are many methods to diagnose and determine injuries, but these methods are expensive and time consuming. In this work, a portable smartphone-based video magnification (VM) technique and machine learning algorithm Haar Cascade are used to detect injuries. The main objective of this work is to develop a worldwide accessible application that detects injuries in real-time manner using video magnification of the blood&#x2019;s colour circulated through the injured body part. The blood flow rate is used because since injuries directly cause an increase in blood flow rate. The proposed system was successfully implemented with accuracy of 95.07&#x0025;.",https://ieeexplore.ieee.org/document/9745207/,2021 13th Biomedical Engineering International Conference (BMEiCON),19-21 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOMWKSHPS47286.2019.9093759,Intelli-Eye: An UAV Tracking System with Optimized Machine Learning Tasks Offloading,IEEE,Conferences,"The unmanned aerial vehicles (UAVs) have been extensively used in providing intelligence such as target tracking. In our field experiments, a pre-trained deep neural network (DNN) is deployed on the UAV to identify a target from the captured video frames and enable UAV to keep tracking. However, tracking in real time by the DNN requires a lot of computational resources. This motivates us to consider offloading this type of machine learning (ML) tasks to a mobile edge computing (MEC) server. Specifically, we propose a novel hierarchical ML tasks distribution framework for the UAV tracking system, where the UAV is embedded with lower layers of the pre-trained convolutional neural network (CNN) model due to its limited computing capability, while the MEC server with rich computing resources will handle the higher layers of the CNN model. An optimization problem is formulated to minimize the CNN inference delay while taking into account the communications delay, computing time, and ML error. Insights are provided to understand the tradeoff between communications and ML computing in offloading decisions. Numerical results demonstrate the effectiveness of the proposed ML tasks distribution framework with the optimized offloading strategy.",https://ieeexplore.ieee.org/document/9093759/,IEEE INFOCOM 2019 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),29 April-2 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00351,Intelligent Black Ice Detection and Alert System Using Thermal Imaging Camera and Drone,IEEE,Conferences,"Traffic accidents caused by black ice are frequent in winter. This paper proposed to use drone embedded with thermal imaging camera to collect road information, and conduct video analysis and segmentation based on CNN (Convolutional Neural Networks). The resulting black ice position and area are used to analyze formation time and predict vanishing time, then real-time alert and navigation information updates could be realized. The study helps to reduce traffic accidents caused by black ice and provide more security for the lives and property of travelers.",https://ieeexplore.ieee.org/document/9781074/,"2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)",20-22 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEA-STEM53614.2021.9668057,Intelligent Traffic Light System Using Image Processing,IEEE,Conferences,"Nowadays, there are more cars used on the road. Traffic congestion problems can cause the economy and the environment both directly and indirectly problems, also part of the problem of air pollution. The traffic light management system in the current situation has used a fixed waiting time, which inability to be flexible according to the traffic at different times such as in rush hour and other. It is not efficient enough to manage traffic with fixed waiting time. The organizers came up with the idea of developing an intelligent traffic light system with flexibility according to the number of cars in real-time by reducing waiting time. The paper was designed and developed by implementing the intelligent traffic light system using image processing technology to process the appropriate waiting time from each image frame. Lazarus and OpenGL were used to program based on Pascal language. The software has been developed for receiving traffic video at the intersection to process car image segmentation of each frame and to calculate the distance of the length of the car in each route in addition. It is also possible to calculate the appropriate time for green-light and red-light duration and corresponding to the length of the waiting vehicles in each route at the intersection. This investigated software can be used to reduce the waiting time at the traffic light intersection by 45.35%. In addition, the intelligent traffic light system is also a social development towards a smart city. The project has created the learning environment and computational thinking for society through the process of STEM Education with using IoT and Artificial Intelligence.",https://ieeexplore.ieee.org/document/9668057/,2021 2nd SEA-STEM International Conference (SEA-STEM),24-25 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2012.6358977,Intelligent VVoIP implementation in UEC cloud computing,IEEE,Conferences,"A high-performance real-time video/voice over IP (VV oIP) applications have been implemented using an Ubuntu Enterprise Cloud (UEC) and it is denoted UEC-VVOIP. It really outperforms the previous VVoIP via P2P connection (called SCTP-IHU) because user does not need to know what is a real IP and browsing the web with TCP-Based RMTP protocol achieves a pretty good friendly user interface. Therefore, this scheme reduces computation load and power consumption dramatically at user side. We have employed back-propagation neural network (BPNN) together with particle swarm optimization (PSO) to appropriately tune seamless handoff and analyze network traffic over time. As a result it takes about 1.597 sec for the seamless handoff between base stations under mobile wireless network. In access control the rapid facial recognition and fingerprint identification via cloud computing has been done successfully within 2.18 seconds to identify the subject. In conclusion the performance of both measures is better than the alternative VVoIP using Hadoop platform we've done before.",https://ieeexplore.ieee.org/document/6358977/,2012 International Conference on Machine Learning and Cybernetics,15-17 July 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2014.6856583,Intelligent driving diagnosis based on a fuzzy logic approach in a real environment implementation,IEEE,Conferences,"This paper considers the problem of diagnosing people's driving skills under real driving conditions using GPS data and video records. For this real environment implementation, a brand new intelligent driving diagnosis system based on fuzzy logic was developed. This system seeks to propose an abstraction of expert driving criteria for driving assessment. The analysis takes into account GPS signals such as: position, velocity, accelerations and vehicle yaw angle; because of its relation with drivers' maneuvers. In that sense, this work presents in the first place, the proposed scheme for the intelligent driving diagnosis agent in terms of its own characteristics properties, which explain important considerations about how an intelligent agent must be conceived. Secondly, it attempts to explain the scheme for the implementation of the intelligent driving diagnosis agent based on its fuzzy logic algorithm, which takes into account the analysis of real-time telemetry signals and proposed set of driving diagnosis rules for the intelligent driving diagnosis, based on a quantitative abstraction of some traffic laws and some secure driving techniques. Experimental testing has been performed in driving conditions. All tested drivers performed the driving task on real streets. The testing results show that our intelligent driving diagnosis system allows quantitative qualifications of driving performance with a high degree of reliability.",https://ieeexplore.ieee.org/document/6856583/,2014 IEEE Intelligent Vehicles Symposium Proceedings,8-11 June 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/icp.2022.0411,Intelligent recognition of insulator video stream based on embedded edge computing,IET,Conferences,"Traditional intelligent recognition algorithm models of insulators are large and mostly completed by back-end servers, with large data transmission volume, high power consumption and poor real-time performance at the front and rear ends. In this paper, a miniature SSD convolutional neural network model is designed to enable it to be deployed in embedded edge computing devices and to recognize video images of insulators. Firstly, the low-latitude Core convolution group subnetwork is used to replace the high-latitude VGG network to extract the shallow features of the insulator, and then the auxiliary feature extraction subnetwork is used to predict the target, so as to reduce the number of model parameters. The model weight files generated by the training were transplanted to the edge computing platform of Jetson nano equipped with vision sensor. OpenCV was used to extract frames from the video images captured by vision sensor every 1 ms and real-time recognition performance was compared under different video resolutions. Experimental results show that the scheme reduces data transmission and realizes low delay recognition. The average recognition frame rate of insulator video images is 16.8 FPS and 19.7 FPS respectively at 1280*720 and 640*480 resolutions. The recognition process is relatively smooth and the recognition accuracy can reach 91.53%. The energy consumption of the whole identification process is less than 5W, which is an order of magnitude lower than that of traditional servers.",https://ieeexplore.ieee.org/document/9800234/,22nd International Symposium on High Voltage Engineering (ISH 2021),21-26 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2014.6958002,Intelligent surveillance system with see-through technology,IEEE,Conferences,"This paper proposes a new intelligent surveillance system for parking lot management in underground environments. The feature based image stitching and image blending techniques are used to let video streams captured by multiple cameras be fused into single output whose blind area could be easily seen through. Such system allows that anyone can immediately understand what is going on in the whole monitored area with a simple glance and without prior geometrical knowledge of the place because the blind area is translucent. The image stitching processes consists of four main parts:(1) feature correspondence detection and fundamental matrix estimation, (2) outlier filtering scheme based on the epipolar geometry and RANSAC, (3) computing the homography matrices between each pair of images, (4) projectively warping the images with their corresponding homography matrices, and conducting image fusion with the non-overlapping parts of the warped images. Finally, the process of translucent blending is applied to eliminate the blind areas inside the stitched image, and then the corresponding cameras behind the occluding pillar provide pixels for translucentizing. We have implemented the preliminary system with six surveillance cameras at underground parking lot environments, and experiment results of real world video sequences have been performed to verify the proposed design.",https://ieeexplore.ieee.org/document/6958002/,17th International IEEE Conference on Intelligent Transportation Systems (ITSC),8-11 Oct. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI52525.2021.00200,Interactive Explainable Case-Based Reasoning for Behavior Modelling in Videogames,IEEE,Conferences,"Creation of believable characters is one of the most challenging problems in the video game industry. Although there are different authoring tools available for designers and programmers to create the behavior of non-player characters, it remains a complex and error prone process that requires a high level of technical knowledge. Considering how powerful Learning from Demonstration is for building intelligent agents that are able to replicate human player behaviors, we are currently using an online case-based reasoning agent that learns how to imitate real players of Ms. Pac-Man using an interactive approach in which both the human player and the computational agent take turns controlling the character. In this paper, we describe the new explanation subsystem and how it helps game designers to understand the agent’s learning process. We also present an evaluation of the system performed by five professional video game designers.",https://ieeexplore.ieee.org/document/9643387/,2021 IEEE 33rd International Conference on Tools with Artificial Intelligence (ICTAI),1-3 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONF-SPML54095.2021.00073,Intra Fast Mode Decision Algorithm and Hardware Design for AVS2,IEEE,Conferences,"AVS2 is a kind of video coding standard proposed by China, it adopts 33 intra prediction modes to improve coding performance, while the computational complexity has increased dramatically. In order to reduce the complexity of AVS2 intra mode decision and make the AVS2 hardware encoder meet real-time requirements, this paper proposes the AVS2 intra fast mode decision algorithm and hardware design. The experimental results show that the intra mode decision algorithm and hardware design proposed in this paper can meet the throughput requirement of 1920x1080@60fps at a clock frequency of 300MHz. By using Xilinx FPGA XC7K325T 900 on the Vivado HLS platform for synthesis, only 10&#x0025; of the LUT, 5&#x0025; of FF, 5&#x0025; of BRAM, and 6&#x0025; of DSP in FPGA resources are consumed to meet the design requirements.",https://ieeexplore.ieee.org/document/9706829/,2021 International Conference on Signal Processing and Machine Learning (CONF-SPML),14-14 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/STFSSD.2009.29,Intra Media Synchronization for Actual Feeling Video Model,IEEE,Conferences,"The video clips only have simple information that these are just recorded data. There is a need to make the video clips actual sensation and look real to satisfy users for resolve the needs. We suggest new video model, called AFVM (Actual Feeling Video Model), that is based on ontology concept. The purpose of this study is to provide AFVM and new format of space where containing divided videos for actual image expression.",https://ieeexplore.ieee.org/document/4804590/,2009 Software Technologies for Future Dependable Distributed Systems,17-17 March 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.15439/2020F168,Introducing LogDL – Log Description Language for Insights from Complex Data,IEEE,Conferences,"We propose a new logic-based language called Log Description Language (LogDL), designed to be a medium for the knowledge discovery workflows over complex data sets. It makes it possible to operate with the original data along with machine-learning-driven insights expressed as facts and rules, regarded as so-called descriptive logs characterizing the observed processes in real or virtual environments. LogDL is inspired by the research at the border of AI and games, precisely by Game Description Language (GDL) that was developed for General Game Playing (GGP). We emphasize that such formal frameworks for analyzing the gameplay data are a good prerequisite for the case of real, “not digital” processes. We also refer to Fogs of War (FoW) – our upcoming project related to AI in video games with limited information – whereby LogDL will be used as well.",https://ieeexplore.ieee.org/document/9222961/,2020 15th Conference on Computer Science and Information Systems (FedCSIS),6-9 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI50826.2021.9402701,IoT Based Two Way Safety Enabled Intelligent Stove with Age Verification Using Machine Learning,IEEE,Conferences,"Smart embedded systems have become a core component in the latest technologies, and IoT based smart embedded system is the trendiest field in the research area. In our research, we are proposing an IoT based smart stove. Any accident might occur at any time from a stove. So we are designing a two-way safety enabled stove with a child lock system and gas leakage detection feature. The intelligent stove will try to ensure safety and will detect age from real-time video streaming. Our main focus is a child would not be able to turn the stove on. As well as, the stove can entitle safety via gas detection alarm. We are using a Raspberry Pi and Gas Detection Module with a buzzer for the hardware implementation. Also, we are applying a Machine Learning object detection algorithm (Haar Cascade) and a deep learning architecture (CNN) for the system execution. Since our stove is IoT-based, the stove is ensuring safety remotely as well as manually which will try to prevent accidental occurrences.",https://ieeexplore.ieee.org/document/9402701/,2021 International Conference on Computer Communication and Informatics (ICCCI),27-29 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSCE52189.2021.9530926,IoT-Based Flash Flood Detection and Alert Using TensorFlow,IEEE,Conferences,"It is important to have a real-time flash flood detection system to inform the public for them to take appropriate action. The current method of authorities using mainstream media such as newspaper, radio, TV, or public announcement is too slow to provide the local population ahead starts to prepare for coming flash flood. Several other early flood warning systems have been proposed but the system is already outdated and did not alert the user in real-time. Therefore, this paper proposes an IoT-based flash flood detection and alert using TensorFlow. The flash flood is detected by using machine learning technique and an alert will be sent to the user using Telegram. The detection did not rely on a conventional water sensor to detect floods, instead, it uses a video camera to monitor the water level. Moreover, the system was implemented in low-powered raspberry pi which can be deployed to many floods prone areas. Based on the test result, the system can differentiate between normal and flash flood water levels and alert users via Telegram Channel. The test results also show that using TensorFlow Lite with SSD-MobileNet-v2-Quantized model in IoT environment has the highest performance.",https://ieeexplore.ieee.org/document/9530926/,"2021 11th IEEE International Conference on Control System, Computing and Engineering (ICCSCE)",27-28 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIIC51459.2021.9415228,IoT-Based Vibration Sensor Data Collection and Emergency Detection Classification using Long Short Term Memory (LSTM),IEEE,Conferences,"In this paper, we used a vibration sensor known as G-Link 200 to collect real time vibration data. The sensor is connected through the internet gateway and Long Short Term Memory (LSTM) used for the classification of sensor data. The classification allows for detecting normal and anomaly activity situation which allows for triggering emergency situation. This is implemented in smart homes where privacy is an issue of concern. Example of such places are toilets, bedrooms and dressing rooms. It can also be applied to smart factory where detecting excessive or abnormal vibration is of critical importance to factory operation. The system eliminates the discomfort for video surveillance to the user. The data collected is also useful for the research community in similar research areas of sensor data enhancement. MATLAB R2019b was used to develop the LSTM. The result showed that the accuracy of the LSTM is 97.39% which outperformed other machine learning algorithm and is reliable for emergency classification.",https://ieeexplore.ieee.org/document/9415228/,2021 International Conference on Artificial Intelligence in Information and Communication (ICAIIC),13-16 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC.2019.8651791,Kerman: A Hybrid Lightweight Tracking Algorithm to Enable Smart Surveillance as an Edge Service,IEEE,Conferences,"Edge computing pushes the cloud computing boundaries beyond uncertain network resource by leveraging computational processes close to the source and target of data. Time-sensitive and data-intensive video surveillance applications benefit from on-site or near-site data mining. In recent years, many smart video surveillance approaches are proposed for object detection and tracking by using Artificial Intelligence (AI) and Machine Learning (ML) algorithms. However, it is still hard to migrate those computing and data-intensive tasks from Cloud to Edge due to the high computational requirement. In this paper, we envision to achieve intelligent surveillance as an edge service by proposing a hybrid lightweight tracking algorithm named Kerman (Kernelized Kalman filter). Kerman is a decision tree based hybrid Kernelized Correlation Filter (KCF) algorithm proposed for human object tracking, which is coupled with a lightweight Convolutional Neural Network (L-CNN) for high performance. The proposed Kerman algorithm has been implemented on a couple of single board computers (SBC) as edge devices and validated using real-world surveillance video streams. The experimental results are promising that the Kerman algorithm is able to track the object of interest with a decent accuracy at a resource consumption affordable by edge devices.",https://ieeexplore.ieee.org/document/8651791/,2019 16th IEEE Annual Consumer Communications & Networking Conference (CCNC),11-14 Jan. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TEST.2016.7805817,Keynote address Wednesday: Hardware inference accelerators for machine learning,IEEE,Conferences,"Machine learning (ML) technologies have revolutionized the ways in which we interact with large-scale, imperfect, real-world data. As a result, there is rising interest in opportunities to implement ML efficiently in custom hardware. We have designed hardware for one broad class of ML techniques: Inference on Probabilistic Graphical Models (PGMs). In these graphs, labels on nodes encode what we know and “how much” we believe it; edges encode belief relationships among labels; statistical inference answers questions such as “if we observe some of the labels in the graph, what are most likely labels on the remainder?” These problems are interesting because they can be very large (e.g., every pixel in an image is one graph node) and because we need answers very fast (e.g., at video frame rates). Inference done as iterative Belief Propagation (BP) can be efficiently implemented in hardware, and we demonstrate several examples from current FPGA prototypes. We have the first configurable, scalable parallel architecture capable of running a range of standard vision benchmarks, with speedups up to 40X over conventional software. We also show that BP hardware can be made remarkably tolerant to the low-level statistical upsets expected in end-of-Moore's-Law nanoscale silicon and post-silicon circuit fabrics, and summarize some effective resilience mechanisms in our prototypes.",https://ieeexplore.ieee.org/document/7805817/,2016 IEEE International Test Conference (ITC),15-17 Nov. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FMEC49853.2020.9144828,Keynote speech 3: Big data Computing and Machine Learning for Intelligent Transportation and Connected Vehicles,IEEE,Conferences,"We are developing machine learning algorithms and software to fuse real-time feeds from video cameras and traffic sensor data to generate real-time detection, classification, and space-time trajectories of individual vehicles and pedestrians. This information is then transmitted to a cloud-based system and then synthesized to create a real-time city-wide traffic palette. I will discuss our research on: Smart intersections: Space-time trajectories are used to understand and improve the safety and efficiency of the intersection. Using conflict points of the vehicle-pedestrian trajectories, we identify potential collisions, or &#x201C;near-misses,&#x201D; and how they are related to the state of the signal cycle (transition from green to yellow, from yellow to red, etc.) and the presence of other vehicles and pedestrians. &#x2022; Smart system: We are developing efficient signal re-timing for different corridors by time of day and day of the week to reflect the changes in network demand. We are also developing machine learning techniques for real-time detection of incidents and accidents on arterial networks. &#x2022; Smart interactions with connected and autonomous vehicles: We have developed signalized intersection control strategies and sensor fusion algorithms for jointly optimizing vehicle trajectories and signal control for a mixture of autonomous vehicles and traditional vehicles at every intersection",https://ieeexplore.ieee.org/document/9144828/,2020 Fifth International Conference on Fog and Mobile Edge Computing (FMEC),20-23 April 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CACRE52464.2021.9501372,Kinect-based intelligent monitoring and warning of students' sitting posture,IEEE,Conferences,"This paper proposes a Kinect-based software design for intelligent detection and warning of students' sitting posture. The plan includes three modules: video acquisition, preprocessing, and sitting posture recognition. Two methods, template matching method based on bone data and feature learning method based on Convolutional neural network (CNN), are used to realize sitting posture recognition. This design first uses Kinect to complete video capture, and secondly, preprocesses the real-time image frames in the video stream to extract the region of interest. Calculate the spatial position of the main joint points of the upper body of the human body again, and convert them into different spatial coordinate systems to locate the target. Finally, the best result of the two methods is used as the recognition result. When an incorrect posture is judged, a corresponding prompt or warning is issued to the supervised person. The experiment was carried out to realize the real-time detection and judgment of the student's sitting posture and timely notification of bad posture, which verified the software design scheme's feasibility.",https://ieeexplore.ieee.org/document/9501372/,"2021 6th International Conference on Automation, Control and Robotics Engineering (CACRE)",15-17 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCVW.2017.56,LBP-Flow and Hybrid Encoding for Real-Time Water and Fire Classification,IEEE,Conferences,"The analysis of dynamic scenes in video is a very useful task especially for the detection and monitoring of natural hazards such as floods and fires. In this work, we focus on the challenging problem of real-world dynamic scene understanding, where videos contain dynamic textures that have been recorded in the ""wild"". These videos feature large illumination variations, complex motion, occlusions, camera motion, as well as significant intra-class differences, as the motion patterns of dynamic textures of the same category may be subject to large variations in real world recordings. We address these issues by introducing a novel dynamic texture descriptor, the ""Local Binary Pattern-flow"" (LBP-flow), which is shown to be able to accurately classify dynamic scenes whose complex motion patterns are difficult to separate using existing local descriptors, or which cannot be modelled by statistical techniques. LBP-flow builds upon existing Local Binary Pattern (LBP) descriptors by providing a low-cost representation of both appearance and optical flow textures, to increase its representation capabilities. The descriptor statistics are encoded with the Fisher vector, an informative mid-level descriptor, while a neural network follows to reduce the dimensionality and increase the discriminability of the encoded descriptor. The proposed algorithm leads to a highly accurate spatio-temporal descriptor which achieves a very low computational cost, enabling the deployment of our descriptor in real world surveillance and security applications. Experiments on challenging benchmark datasets demonstrate that it achieves recognition accuracy results that surpass State-of-the-Art dynamic texture descriptors.",https://ieeexplore.ieee.org/document/8265266/,2017 IEEE International Conference on Computer Vision Workshops (ICCVW),22-29 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAIBD.2018.8396183,LBPH based improved face recognition at low resolution,IEEE,Conferences,"Automatic individual face recognition is the most challenging query from the past decade in computer vision. However, the law enforcement agencies are inadequate to identify and recognize any person through the video monitoring cameras further efficiently; the blur conditions, illumination, resolution, and lighting are still the major problems in face recognition. Our proposed system operates better at the minimum low resolution of 35px to identify the human face in various angles, side poses and tracking the face during human motion. We have designed the dataset (LR500) for training and classification. This paper employs the Local Binary Patterns Histogram (LBPH) algorithm architecture to address the human face recognition in real time at the low level of resolution.",https://ieeexplore.ieee.org/document/8396183/,2018 International Conference on Artificial Intelligence and Big Data (ICAIBD),26-28 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3326285.3329051,LEAP: Learning-Based Smart Edge with Caching and Prefetching for Adaptive Video Streaming,IEEE,Conferences,"Dynamic Adaptive Streaming over HTTP (DASH) has emerged as a popular approach for video transmission, which brings a potential benefit for the Quality of Experience (QoE) because of its segment-based flexibility. However, the Internet can only provide no guaranteed delivery. The high dynamic of the available bandwidth may cause bitrate switching or video rebuffering, thus inevitably damaging the QoE. Besides, the frequently requested popular videos are transmitted for multiple times and contribute to most of the bandwidth consumption, which causes massive transmission redundancy. Therefore, we propose a Learning-based Edge with cAching and Prefetching (LEAP) to improve the online user QoE of adaptive video streaming. LEAP introduces caching into the edge to reduce the redundant video transmission and employs prefetching to fight against network jitters. Taking the state information of users into account, LEAP intelligently makes the most beneficial decisions of caching and prefetching by a QoE-oriented deep neural network model. To demonstrate the performance of our scheme, we deploy the implemented prototype of LEAP in both the simulated scenario and the real Internet. Compared with all selected schemes, LEAP at least raises average bitrate by 34.4&#x0025; and reduces video rebuffering by 42.7&#x0025;, which leads to at least 15.9&#x0025; improvement in the user QoE in the simulated scenario. The results in the real Internet scenario further confirm the superiority of LEAP.",https://ieeexplore.ieee.org/document/9068647/,2019 IEEE/ACM 27th International Symposium on Quality of Service (IWQoS),24-25 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCES45898.2019.9002256,Landslide detection system using computer vision approach and Raspberry Pi,IEEE,Conferences,"Landslide is a natural hazard, which badly affects people's safety and property. Continuous monitoring of such a disaster may lead to reducing of losses of human lives. With this aim, in this paper, we have proposed a surveillance system for real time landslide monitoring using computer vision algorithms. Here we have used a video camera to acquire live view of landslide site and a small computer board `raspberry Pi' to run the algorithm we will introduce here. When landslide is detected SMS text messages are transmitted using a GSM modem. Due to video camera and Raspberry Pi, this method is inexpensive yet efficient and also requires low power, therefore this method can be used in any region. We have used median filtering to remove noise present in the detection algorithm. Here we have also proposed a new implementation method for median filtering using a linked list and parallel processing which is very time efficient.",https://ieeexplore.ieee.org/document/9002256/,2019 International Conference on Communication and Electronics Systems (ICCES),17-19 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8622221,Large Scale Open Source Video Recommender Tool Using Metadata Surrogates,IEEE,Conferences,"Video and multi-media sharing is a significant activity on social media platforms. Learning patterns of activities using raw video data is computationally intensive and impractical, and manual inspection is not scalable and prohibitively expensive. An alternate strategy is to learn information about video content using far less compute intensive metadata surrogates. This paper describes a video recommender tool implemented in GovCloud using a novel approach of using lightweight video metadata to learn and classify video content. In contrast to popular video recommender systems that use consumption models for classification, the new approach used in our tool is based solely on the video metadata along with domain expertise used to truth a relatively small subset of relevant video content. The tool is very user-friendly and captures practical knowledge of the user resulting in good learning model. The architecture and implementation specifics of the tool is outlined in this paper. The classifier performance using metadata from tens of thousands of real postings exceeds 90% for both recall and ROC metrics. This tool has shown promise in providing a console for aggregating social media videos for analysts to train the system consistent with the context and task at hand.",https://ieeexplore.ieee.org/document/8622221/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData47090.2019.9005642,Large Scale Time Series Analysis for Infrastructure Reliability,IEEE,Conferences,"Performance, reliability and efficiency of infrastructure systems are instrumental to high quality user experience at Facebook. Extensive time series logging has been implemented to monitor the health of various infrastructure systems. CPU usage of internal services (e.g., large number of machine learning models built by different teams), network traffic between internet service providers and Facebook data centers, video streaming KPI of live broadcasts generated by daily active users, just to name a few. Many methods and tools are available to analyze the logs at single time series level, for instance, time series forecasting and (contextual) anomaly detection. Relatively little is done to address an emerging use case - identifying similar/outlier instances from a large collection of time series. In this poster, we demonstrate the design and implementation of a generic framework to accomplish such needs. We discuss two real-world implementations of the approach: (i) auto-scaling the inference capacity for machine learning (ML) models, and (ii) detect video ingestion quality outliers across Facebook live broadcasts. Specifically, we fill the gaps in existing tools (and/or external, off-the-shelf alternatives) by proposing a method which is interpretable, generalizable and scalable.",https://ieeexplore.ieee.org/document/9005642/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS45743.2020.9341460,Latent Replay for Real-Time Continual Learning,IEEE,Conferences,"Training deep neural networks at the edge on light computational devices, embedded systems and robotic platforms is nowadays very challenging. Continual learning techniques, where complex models are incrementally trained on small batches of new data, can make the learning problem tractable even for CPU-only embedded devices enabling remarkable levels of adaptiveness and autonomy. However, a number of practical problems need to be solved: catastrophic forgetting before anything else. In this paper we introduce an original technique named ""Latent Replay"" where, instead of storing a portion of past data in the input space, we store activations volumes at some intermediate layer. This can significantly reduce the computation and storage required by native rehearsal. To keep the representation stable and the stored activations valid we propose to slow-down learning at all the layers below the latent replay one, leaving the layers above free to learn at full pace. In our experiments we show that Latent Replay, combined with existing continual learning techniques, achieves state-of-the-art performance on complex video benchmarks such as CORe50 NICv2 (with nearly 400 small and highly non-i.i.d. batches) and OpenLORIS. Finally, we demonstrate the feasibility of nearly real-time continual learning on the edge through the deployment of the proposed technique on a smartphone device.",https://ieeexplore.ieee.org/document/9341460/,2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),24 Oct.-24 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMI.2002.1166960,Layered representations for human activity recognition,IEEE,Conferences,"We present the use of layered probabilistic representations using hidden Markov models for performing sensing, learning, and inference at multiple levels of temporal granularity We describe the use of representation in a system that diagnoses states of a user's activity based on real-time streams of evidence from video, acoustic, and computer interactions. We review the representation, present an implementation, and report on experiments with the layered representation in an office-awareness application.",https://ieeexplore.ieee.org/document/1166960/,Proceedings. Fourth IEEE International Conference on Multimodal Interfaces,16-16 Oct. 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE49875.2021.9637402,Learning Autonomous Driving in Tangible Practice: Development and On-Road Applications of a 1/10-Scale Autonomous Vehicle,IEEE,Conferences,"This Innovative Practice Work-In-Progress Paper presents a case of learning autonomous driving in tangible practice. As technology sustainably enhances the quality of life, intelligent systems continue to contribute solutions to some of the biggest challenges faced by humans. Autonomous vehicles offer humans the opportunity to increase transportation safety by reducing human errors on the road, preventing accidents, improving human productivity by reducing commuting time, and possibly mitigating air pollution. There is a critical shortage of educational and training programs in autonomous vehicles due to the high cost of full-size vehicles, computing and sensor equipment, and big lab space needed. To address this problem, we develop a 1/10-scale autonomous vehicle powered by pre-collision detection, lane tracking, and road sign recognition systems. The pre-collision system is built using ultrasonic sensors, and the Proportional-Integral-Derivative (PID) control is implemented to manipulate the vehicle's safety response. The Open-Source Computer Vision Library (OpenCV) is exploited to detect and process real-time on-road streaming video to enable lane-tracking and road sign recognition. AI techniques are utilized for the model training. Preliminary results of this work are presented and analyzed. We also discuss the future directions of this study.",https://ieeexplore.ieee.org/document/9637402/,2021 IEEE Frontiers in Education Conference (FIE),13-16 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636547,Learning Contact-Rich Assembly Skills Using Residual Admittance Policy,IEEE,Conferences,"Contact-rich assembly tasks may result in large and unpredictable forces and torques when the locations of the contacting parts are uncertain. The ability to correct the trajectory in response to haptic feedback and accomplish the task despite location uncertainties is an important skill. We hypothesize that this skill would facilitate generalization and support direct transfer from simulations to real world. To reduce sample complexity, we propose to learn a residual admittance policy (RAP). RAP is learned to correct the movements generated by a baseline policy in the framework of dynamic movement primitives. Given the reference trajectories generated by the baseline policy, the action space of RAP is limited to the admittance parameters. Using deep reinforcement learning, a deep neural network is trained to map task specifications to proper admittance parameters. We demonstrate that RAP handles uncertainties in board location, generalizes well over space, size and shape, and facilitates quick transfer learning. Most impressively, we demonstrate that the policy learned in simulations achieves similar robustness to uncertainties, generalization and performance when deployed on an industrial robot (UR5e) without further training. See accompanying video for demonstrations.",https://ieeexplore.ieee.org/document/9636547/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2017.196,Learning Dynamic Siamese Network for Visual Object Tracking,IEEE,Conferences,"How to effectively learn temporal variation of target appearance, to exclude the interference of cluttered background, while maintaining real-time response, is an essential problem of visual object tracking. Recently, Siamese networks have shown great potentials of matching based trackers in achieving balanced accuracy and beyond realtime speed. However, they still have a big gap to classification & updating based trackers in tolerating the temporal changes of objects and imaging conditions. In this paper, we propose dynamic Siamese network, via a fast transformation learning model that enables effective online learning of target appearance variation and background suppression from previous frames. We then present elementwise multi-layer fusion to adaptively integrate the network outputs using multi-level deep features. Unlike state-of-theart trackers, our approach allows the usage of any feasible generally- or particularly-trained features, such as SiamFC and VGG. More importantly, the proposed dynamic Siamese network can be jointly trained as a whole directly on the labeled video sequences, thus can take full advantage of the rich spatial temporal information of moving objects. As a result, our approach achieves state-of-the-art performance on OTB-2013 and VOT-2015 benchmarks, while exhibits superiorly balanced accuracy and real-time response over state-of-the-art competitors.",https://ieeexplore.ieee.org/document/8237458/,2017 IEEE International Conference on Computer Vision (ICCV),22-29 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR52153.2021.00026,Learning Environment based on an Interactive Projection Table for Children,IEEE,Conferences,"This paper proposes a tabletop learning environment for children. Some physics-based phenomena such as reflection and refraction of light are difficult for children to understand. Our interactive projection significantly help children understand behaviors of light through watching simulated light particles and interacting with them by moving real objects on the table. Our system consists a PC, video projector, and depth sensor. Depth-based object detection is performed with each of captured frames at 60 FPS. In the prototype of our implemented system for learning behaviors of light, simulated light particles reacts in realtime with actual objects moved by a user hand, which would greatly help people including children understand behaviors of light such as propagation and reflection. Our future work includes to conduct a series of user experiments in local museums to evaluate usability of the proposed system. Development of other learning applications are also of high importance.",https://ieeexplore.ieee.org/document/9644370/,2021 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),15-17 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME46284.2020.9102816,Learning To See Faces In The Dark,IEEE,Conferences,"Recovering details from dark images has received increasing attention due to its potential in applications such as video surveillance. We propose the first approach to detect and enhance human faces in extreme low-light images. Our method consists of two stages: a novel Face Location Network (FLNet) to locate the face, followed by a Face Enhancement Network (FE-Net) that uses concatenated sub-modules to progressively recover the face from coarse to fine grained details. Specifically, our enhancement modules exploit the semantic priors of facial landmarks to facilitate face recovery. Extensive experiments show our method is quantitatively and qualitatively superior to the state-of-the-art in terms of enhancement quality and face recognition. We have also collected a real-world dataset to support relevant research. All code and data will be shared for reproducing our experiments.",https://ieeexplore.ieee.org/document/9102816/,2020 IEEE International Conference on Multimedia and Expo (ICME),6-10 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COASE.2018.8560597,Learning Traffic Behaviors by Extracting Vehicle Trajectories from Online Video Streams,IEEE,Conferences,"To collect extensive data on realistic driving behavior, we propose a framework using online public traffic cam video streams. We implement the Traffic Camera Pipeline (TCP), a system that leverages recent advances in deep learning for object detection to extract trajectories from the video stream to corresponding locations in a bird's eye view traffic simulator. We benchmarked several deep learning detectors for the task of vehicle detection: SSD-VGGNet, SSD-InceptionNet, and SSD-MobileNet, and Faster-RCNN; we found that SSD-VGGNet had the highest precision and quality of bounding boxes. We captured four hours of video streams and used it to train generative models describing both the starting and ending positions of vehicles as well as the trajectories of points traversed. We find that the negative log-likelihood of held-out real data is more likely to occur under the learned models compared to baseline models used in a traffic intersection simulator. The extracted dataset of 234 annotated minute-long videos containing 2618 labeled vehicle trajectories and 8980 additional unannotated minute-long videos is available at https://berkeleyautomation.github.io/Traffic_Camera_Pipeline/.",https://ieeexplore.ieee.org/document/8560597/,2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),20-24 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2015.7298860,Learning a non-linear knowledge transfer model for cross-view action recognition,IEEE,Conferences,"This paper concerns action recognition from unseen and unknown views. We propose unsupervised learning of a non-linear model that transfers knowledge from multiple views to a canonical view. The proposed Non-linear Knowledge Transfer Model (NKTM) is a deep network, with weight decay and sparsity constraints, which finds a shared high-level virtual path from videos captured from different unknown viewpoints to the same canonical view. The strength of our technique is that we learn a single NKTM for all actions and all camera viewing directions. Thus, NKTM does not require action labels during learning and knowledge of the camera viewpoints during training or testing. NKTM is learned once only from dense trajectories of synthetic points fitted to mocap data and then applied to real video data. Trajectories are coded with a general codebook learned from the same mocap data. NKTM is scalable to new action classes and training data as it does not require re-learning. Experiments on the IXMAS and N-UCLA datasets show that NKTM outperforms existing state-of-the-art methods for cross-view action recognition.",https://ieeexplore.ieee.org/document/7298860/,2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),7-12 June 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2018.00663,Learning to Extract a Video Sequence from a Single Motion-Blurred Image,IEEE,Conferences,"We present a method to extract a video sequence from a single motion-blurred image. Motion-blurred images are the result of an averaging process, where instant frames are accumulated over time during the exposure of the sensor. Unfortunately, reversing this process is nontrivial. Firstly, averaging destroys the temporal ordering of the frames. Secondly, the recovery of a single frame is a blind deconvolution task, which is highly ill-posed. We present a deep learning scheme that gradually reconstructs a temporal ordering by sequentially extracting pairs of frames. Our main contribution is to introduce loss functions invariant to the temporal order. This lets a neural network choose during training what frame to output among the possible combinations. We also address the ill-posedness of deblurring by designing a network with a large receptive field and implemented via resampling to achieve a higher computational efficiency. Our proposed method can successfully retrieve sharp image sequences from a single motion blurred image and can generalize well on synthetic and real datasets captured with different cameras.",https://ieeexplore.ieee.org/document/8578761/,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,18-23 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00437,Learning to Film From Professional Human Motion Videos,IEEE,Conferences,"We investigate the problem of 6 degrees of freedom (DOF) camera planning for filming professional human motion videos using a camera drone. Existing methods either plan motions for only a pan-tilt-zoom (PTZ) camera, or adopt ad-hoc solutions without carefully considering the impact of video contents and previous camera motions on the future camera motions. As a result, they can hardly achieve satisfactory results in our drone cinematography task. In this study, we propose a learning-based framework which incorporates the video contents and previous camera motions to predict the future camera motions that enable the capture of professional videos. Specifically, the inputs of our framework are video contents which are represented using subject-related feature based on 2D skeleton and scene-related features extracted from background RGB images, and camera motions which are represented using optical flows. The correlation between the inputs and output future camera motions are learned via a sequence-to-sequence convolutional long short-term memory (Seq2Seq ConvLSTM) network from a large set of video clips. We deploy our approach to a real drone cinematography system by first predicting the future camera motions, and then converting them to the drone's control commands via an odometer. Our experimental results on extensive datasets and showcases exhibit significant improvements in our approach over conventional baselines and our approach can successfully mimic the footage of a professional cameraman.",https://ieeexplore.ieee.org/document/8953663/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV51971.2022.9827228,Learning to Predict Collision Risk from Simulated Video Data,IEEE,Conferences,"We propose an image-based collision risk prediction model and a training strategy that allows training on simulated video data and successfully generalizes to real data. By doing so, we solve the data scarcity problem of collecting and labeling real (near) collisions, which are exceptionally rare events. Domain generalization from simulated to real data is taken into account by design by decoupling the learning strategy, and using task-specific, domain-resilient intermediate representations. Specifically, we use optical flow and vehicle bounding boxes, since they are instinctively related to the task of collision risk prediction and because their simulated-to-real domain gap is significantly lower than that of camera video data, i.e., they are more domain resilient. To demonstrate our approach, we present RiskNet, a novel neural network for image-based collision risk prediction, which classifies individual frames of a video sequence of a front-facing camera as safe or unsafe. Additionally, we present two novel datasets: the simulated Prescan dataset (which we intend to make publicly available) for training and the YouTube Driving Incidents Database (YDID) for real-world testing. The performance of RiskNet, trained solely on simulated data and tested on the real-world YDID, is comparable to that of a human driver, both in accuracy (91.8&#x0025; vs. 93.6&#x0025;) and F1-score (0.92 vs 0.94).",https://ieeexplore.ieee.org/document/9827228/,2022 IEEE Intelligent Vehicles Symposium (IV),4-9 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CASE48305.2020.9216788,Learning to Predict Friction and Classify Contact States by Tactile Sensor,IEEE,Conferences,"In real-world grasping tasks, unstable grasping will lead to sliding between the object and the robotic nd-effectors. When the target object owns special properties (e.g. glasses, etc.), sliding may cause damage to both the object and the grasping environment, so it is necessary to predict and classify whether sliding occurs at the moment of grasping. Aiming at predicting sliding, this paper proposes a sliding prediction solution for time series tactile data obtained by crude sensor Xela and designs a novel neural network: the force motion tracking network. It predicts the variation trend of the frictional force at the moment of contact, and then concatenate the collected and predicted friction force data as the input of the LSTM network to classify the contact state (whether there is slippage). In this paper, 660 sets of tactile time series data are collected, and we process high dimensional tactile time series data into video data. This data processing method can be applied to other similar tactile sensors. Meanwhile, we also verify the proposed model, the mean square error of our force tracking network is much smaller than ConvLSTM, and the prediction accuracy of our network can reach to 93.5%,which is higher than other methods.",https://ieeexplore.ieee.org/document/9216788/,2020 IEEE 16th International Conference on Automation Science and Engineering (CASE),20-21 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2018.8351530,Learning-based Congestion Control for Internet Video Communication over Wireless Networks,IEEE,Conferences,"With the deployment of real-time video applications and wireless networks, the real-time congestion control becomes a hot topic. Most existing congestion control algorithms are not designed for low-latency real-time flows, or perform poorly in the face of highly variable channel capacities. In this paper, we proposed a novel Learning-based Congestion Control (LCC) for real-time video communication over wireless networks. The key idea of LCC is employing Kernel Density Estimation for one-way delay and sending rate to capture the underlying information about channel state. Then LCC bases on the estimated probability density and Bayesian theorem to quickly adapt sending rate to the changing channel. We implemented LCC in WebRTC framework and extensive experiments were carried out. Compared with the native WebRTC congestion control (GCC), experimental results show that LCC achieves higher channel utilization, even more than 4.2× throughput in lossy links. LCC is also much better at adapting to the variable channel than GCC. Besides, LCC performs well in delay constraint and intra-protocol fairness.",https://ieeexplore.ieee.org/document/8351530/,2018 IEEE International Symposium on Circuits and Systems (ISCAS),27-30 May 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812363,Learning-based Ellipse Detection for Robotic Grasps of Cylinders and Ellipsoids,IEEE,Conferences,"In our daily life, there are many objects represented by cylindrical shapes and ellipsoids. The tops of these objects are formed by elliptic shape primitives. Thus, it is available for a robot to manipulate these objects by ellipse detection. In this work, we propose a novel approach to generating ground truth for training the model based on domain randomization. Using synthetic data generated in this manner, we build an end-to-end deep neural network with a detection backbone and then, combine multiple branches archived from the backbone for sharing the multiple-scale features; further, after employing active rotation filters, the features pass through the region proposal net to form the prediction branches of the box, orientation regression, and object classification; finally, these branches are fused to do ellipse detection, allowing robotic manipulations of cylinders and ellipsoids. To demonstrate the capabilities of the proposed detector, we show the comparison results with the state-of-the-art detector on synthetic and public datasets. The proposed model for ellipse detection and data generation pipeline based on domain randomization in a simulation are evaluated by a series of robotic manipulations implemented in real application scenarios. The results illustrate a high success rate on real-world grasp attempts despite having only been trained on a synthetic dataset. (A video of some robotic experiments is available on YouTube: https://youtu.be/Ueg1XSI2S98).",https://ieeexplore.ieee.org/document/9812363/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INCET49848.2020.9154075,Licence Plate Identification and Recognition for Non-Helmeted Motorcyclists using Light-weight Convolution Neural Network,IEEE,Conferences,"Motorcycle accidents have been rapidly increasing in many countries. The helmet is the main safety equipment of motorcyclists, but many drivers do not use it. Helmets are essential for the safety of a motorcycle rider. Hence, detecting and extracting licence plate of the motorcycle in which riders have not wear helmet becomes a crucial task. Many methods have been proposed to detect and extract the licence plate; however, due to poor video quality and non-uniform illumination, licence plate detection becomes a difficult task. Recently, due to the advancement in graphical processing units (GPUs) and larger datasets, deep learning based models have obtained remarkable performance in the object detection task. One such model is single shot detection (SSD) which classify and detect real-time objects precisely. In this paper, we propose an end-to-end approach for detecting and extracting a licence plate of the motorcycle. Here, we use a MobileNet based SSD model to detect License plates as MobileNet i.e., a light-weight CNN model which is more suitable for mobile and embedded vision applications to obtain fast operation. We also prepare a dataset of Indian motorcycle licence plates which consists of 1524 images to train and validate the SSD model. From experiments, we found that the detection module detects the Indian motorcycle licence plate accurately. Once the License plates are detected, the detected licence plate is extracted and the characters of the extracted licence plate are recognized through optical character recognition (OCR) module.",https://ieeexplore.ieee.org/document/9154075/,2020 International Conference for Emerging Technology (INCET),5-7 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC.2016.7795970,License plate recognition based on temporal redundancy,IEEE,Conferences,"Recognition of vehicle license plates is an important task in several real applications. Most approaches first detect a vehicle, locate the license plate and then recognize its characters. However, the focus relies on performing these tasks using only a single frame of each vehicle in the video. Therefore, such approaches might have their recognition rates reduced due to noise present in that particular frame. Instead of selecting a single frame to perform the recognition, we propose a novel real-time approach to automatically detect the vehicle and identify (locate/recognize) its license plate based on temporal redundancy information. To achieve further improvements, we also propose two post-processing techniques by querying a license plate database. The experimental results, performed in a dataset composed of 300 on-track vehicles acquired on an urban road, demonstrate that it is possible to improve the vehicle recognition rate in 15:3 percentage points using our proposal temporal redundancy approach. Additional 7:8 percentage points are achieved by querying registered license plates on a database by the vehicle appearance, leading to a final recognition rate of 89:6%. Furthermore, the technique is able to process 34 frames per second, which characterizes it as a real-time approach.",https://ieeexplore.ieee.org/document/7795970/,2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC),1-4 Nov. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NITC.2018.8550067,Lighthouse - Smart Virtual Learning Platform,IEEE,Conferences,"This paper presents the design, development and performance evaluation of a new virtual learning platform. The novel features in this platform include real-time transcribing of live lectures conducted in English and the ability to search recorded lectures based on keywords mentioned during the lectures. This is in addition to real-time streaming and archiving of nine channels of information namely; audio, subtitles, video, slide synchronization, code editor, whiteboard, chat, polls and emotion. The system provides a number of novel features to ease the lecturer's workload. These include one touch recording, real-time polling, real-time emotion detection of students, and cloud based lecture editor allowing editing of each of the nine channels. From an operational perspective, the system requires no additional or special hardware other than a laptop, and it uses cloud based streaming and storage, eliminating the need for a dedicated infrastructure.",https://ieeexplore.ieee.org/document/8550067/,2018 National Information Technology Conference (NITC),2-4 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SEARIS44442.2018.9180230,Lightweight Visualization and User Logging for Mobile 360-degree Videos,IEEE,Conferences,"360-degree videos are getting more popular also in mobile domain. As the amount of viewers grow, it is beneficial to track what they are doing. We have 360-degree videos with object detection metadata that we want to visualize on the video. At the same time, we are interested in how the users act when watching the videos with added information. Logging the device orientation is one way to do that.We present a study about a lightweight method to visualize information on top of 360-degree videos while logging the users. The proposed visualization technique is generic and can be used for example to visualize video content related metadata or logging results on top of 360-degree video. We evaluated the work by making a proof of concept and performance analysis, which shows that FPS starts to decrease after around 2000 simultaneous visualization objects. A comparison with other existing visualization solutions suggests that our approach is lightweight.",https://ieeexplore.ieee.org/document/9180230/,2018 IEEE 11th Workshop on Software Engineering and Architectures for Real-time Interactive Systems (SEARIS),19-19 March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICComm.2018.8484765,Little-Big Deep Neural Networks for Embedded Video Surveillance,IEEE,Conferences,"Embedded systems are under continuous development of innovative technological trends, such as adoption of smart devices which are becoming capable of running complex video analytics tasks locally. Lately, deep neural networks have been successfully applied in the field of computer vision achieving state-of-the-art results. These techniques are not yet suitable for resource limited deployments due to high memory footprint and computational cost, factors that affect the inference time. To tackle these constraints, we propose a person re-identification architecture based on the DarkNET Deep Learning Neural Network architecture for person detection and segmentation, which is combined with SIFT algorithm for feature extraction and SVM for the classification task. The algorithm is implemented on a low processing embedded hardware system, namely Raspberry PI. The experiments were conducted on the proposed SPOTTER dataset. The results are conclusive to continue further investigation of applying specialized algorithms for real-time applications which can run on resource limited embedded systems.",https://ieeexplore.ieee.org/document/8484765/,2018 International Conference on Communications (COMM),14-16 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SPC.2018.8704126,Localized Object Information from Detected Objects Based on Deep Learning in Video Scene,IEEE,Conferences,"The ultimate goal of computer vision research is to understand a scene semantically from an image or a video. Real-time object detection received significant attention over the past few years. Many challenges remain, especially in the focus of extraction of localized object information for scene representation. In order to have an accurate, intelligent and fast real-time object detection, the implementation of accurate localized information in the machine is inevitable. This research will focus on developing the object localization extractor that can extract the localized object information from the scene for further scene prediction and inference. In particular, (i) our localized extractor can encode significantly high-level features information; (ii) this rich localized information will be used for scene representation and understanding.",https://ieeexplore.ieee.org/document/8704126/,"2018 IEEE Conference on Systems, Process and Control (ICSPC)",14-15 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP43922.2022.9747094,Low-Latency Human-Computer Auditory Interface Based on Real-Time Vision Analysis,IEEE,Conferences,"This paper proposes a visuo-auditory substitution method to assist visually impaired people in scene understanding. Our approach focuses on person localisation in the user&#x2019;s vicinity in order to ease urban walking. Since a real-time and low-latency is required in this context for user&#x2019;s security, we propose an embedded system. The processing is based on a lightweight convolutional neural network to perform an efficient 2D person localisation. This measurement is enhanced with the corresponding person depth information, and is then transcribed into a stereophonic signal via a head-related transfer function. A GPU-based implementation is presented that enables a real-time processing to be reached at 23 frames/s on a 640x480 video stream. We show with an experiment that this method allows for a real-time accurate audio-based localization.",https://ieeexplore.ieee.org/document/9747094/,"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",23-27 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSN.2017.8230292,Low-resolution face recognition via convolutional neural network,IEEE,Conferences,"Face images captured by real-world video surveillance applications usually have low resolution. This leads to poor performance or even failure of most face recognition algorithms. As a consequence, identifying the face of the query in low resolution, based on the high-resolution image gallery, proves to be a huge challenge. To address this problem, a novel multi-resolution convolutional neural network (MRCNN) model is proposed in order to study the consistent feature representation from high-resolution and low-resolution face images. First, the corresponding labeled multi-resolution face images are utilized to train the MRCNN model. After this process, the trained model is used as the feature extractor in order to obtain features for the targets in the gallery and query images, respectively. Finally, the nearest neighbor method is applied as the classifier for the purpose of final identification. The experimental results from the two publicly available databases demonstrate the superiority of the proposed MRCNN.",https://ieeexplore.ieee.org/document/8230292/,2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN),6-8 May 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSE.2014.6926425,MKL-SVM-based human detection for autonomous navigation of a robot,IEEE,Conferences,"This paper presents a classifier trained by a multiple kernel-learning support vector machine (MKL-SVM) to detect a human in sequential images from a video stream. The developed method consists of two aspects: multiple features consisting of HOG features and HOF features suitable for moving objects, and combined nonlinear kernels for SVM. For the purpose of real time application in autonomous navigation, the SimpleMKL algorithm is implemented into the proposed MKL-SVM classifier. It is able to converge rapidly with comparable efficiency through a weighted 2-norm regularization formulation with an additional constraint on the weights. The classifier is compared with the state-of-the-art linear SVM using a dataset called TUD-Brussels, which is available on line. The results show that the proposed classifier outperforms the Linear SVM with respect to accuracy.",https://ieeexplore.ieee.org/document/6926425/,2014 9th International Conference on Computer Science & Education,22-24 Aug. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME52107.2021.00061,MLCatchUp: Automated Update of Deprecated Machine-Learning APIs in Python,IEEE,Conferences,"Machine learning (ML) libraries are gaining vast popularity, especially in the Python programming language. Using the latest version of such libraries is recommended to ensure the best performance and security. When migrating to the latest version of a machine learning library, usages of deprecated APIs need to be updated, which is a time-consuming process. In this paper, we propose MLCatchUp, an automated API usage update tool for deprecated APIs of popular ML libraries written in Python. MLCatchUp automatically infers the required transformation to migrate usages of deprecated API through the differences between the deprecated and updated API signatures. MLCatchUp offers a readable transformation rule in the form of a domain specific language (DSL). We evaluate MLCatchUp using a dataset of 267 real-world Python code containing 551 usages of 68 distinct deprecated APIs, where MLCatchUp achieves 90.7% accuracy. A video demonstration of MLCatchUp is available at https://youtu.be/5NjOPNt5iaA.",https://ieeexplore.ieee.org/document/9609153/,2021 IEEE International Conference on Software Maintenance and Evolution (ICSME),27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MIPR.2018.00018,MMH: Multi-Modal Hash for Instant Mobile Video Search,IEEE,Conferences,"Mobile devices have been an indispensable part of human life, which enable people to search and browse what they want on the move. Mobile video search, as one of the most important services for users, still faces great challenges under mobile internet scenario, such as the limitation of computation ability, memory, and bandwidth. Therefore, this paper proposes a multi-modal hash based framework for instant mobile video search. In particular, we adopt a efficient deep convolutional neural network, MobileNet, with the hash layer to learn discriminative and compact visual features from videos. Moreover, we also consider hand-crafted local visual descriptor and audio fingerprint to build a multi-modal hash representation of videos. With the multi-modal hash code, two types of hash indexes are built on the server to achieve efficient video search. At last, the multi-modal hash codes are extracted on the mobile devices and transferred in a three- step progressive procedure during the online search stage. The experiments on the real-world dataset show that the proposed framework not only achieves the state-of-the-art accuracy but also obtains excellent efficiency.",https://ieeexplore.ieee.org/document/8396974/,2018 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR),10-12 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GLOCOM.1995.501974,MPEG2 video and audio codec board set for a personal computer,IEEE,Conferences,"This paper describes MPEG2 video and audio coder and decoder boards and a set of dedicated codec chips which are the key components of these boards. MPEG2 is the international standard for moving picture coding algorithms and is expected to be applied to various fields because of its high image quality. The design policy has been decided based on intensive investigations of potential applications and popularity of video communication. From this view point, it is very efficient and important to implement an MPEG2 codec system on a personal computer (PC) platform. Since a short delay is particularly important for real-time applications such as teleconferencing, SP@ML has been adopted. In this case, the number of coding operations is reduced, but our simulation results show that the picture quality is almost the same as that of MP@ML. Non-real-time operation is also made possible so that application to interactive editing of video sequences can be achieved. Moreover, a considerable amount of processing is reduced by a hierarchical search algorithm. A nonlinear pre-filter is used to remove the noise included in the input video signal. Our prototype boards have been designed as PC/AT extension boards. The typical frame size is 720/spl times/480 pixels, and its rate is 30 frame per second. The maximum bit rate of the compressed data is 15 Mbit/sec.",https://ieeexplore.ieee.org/document/501974/,Proceedings of GLOBECOM '95,14-16 Nov. 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCAI53970.2022.9752517,MSLS: An Intelligent Face Expression Identification System using Modified Sparse Learning Scheme,IEEE,Conferences,"Now-a-days, image processing needs and related fields are growing in drastic manner as well as the technology enhance itself periodically. In this Face expression identification is a crucial need and attain more importance. For example, interpersonal communication, computerized education, video and pictures search, intelligent surroundings and safety detection systems might all benefit from the ability to recognize natural feelings in real human faces. Face-recognition software has always been assessed on experimentally controlled data, in which it is not reflective of the real-world environment. Facial expressions may be recognized using a novel interactive technique. Face emotions identification system is a long-term organization registration challenge for adaptive face expression detection. The following are the major advantages of this procedure: In the first place, a relevant constructs growth model describes the movement patterns of the unique face characteristics of different manifestations in terms of the disciplines; in the second place, a sparsity multi-group object recognition technique builds a pertinent spatial face expression map for each interpretation, in which it is able to explain the massive changes in the facial features of the entire population and repress the partiality due to massive inter-subject facial variability&#x0027;s; in the third place, a feature extraction approach uses both temporal and spatial image exposure information to help identification of a sparsely represented object.",https://ieeexplore.ieee.org/document/9752517/,"2022 International Conference on Advances in Computing, Communication and Applied Informatics (ACCAI)",28-29 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2018.00161,Machine Cognition of Violence in Videos Using Novel Outlier-Resistant VLAD,IEEE,Conferences,"Understanding highly accurate and real-time violent actions from surveillance videos is a demanding challenge. Our primary contribution of this work is divided into two parts. Firstly, we propose a computationally efficient Bag-of-Words (BoW) pipeline along with improved accuracy of violent videos classification. The novel pipeline's feature extraction stage is implemented with densely sampled Histogram of Oriented Gradients (HOG) and Histogram of Optical Flow (HOF) descriptors rather than Space-Time Interest Point (STIP) based extraction. Secondly, in encoding stage, we propose Outlier-Resistant VLAD (OR-VLAD), a novel higher order statistics-based feature encoding, to improve the original VLAD performance. In classification, efficient Linear Support Vector Machine (LSVM) is employed. The performance of the proposed pipeline is evaluated with three popular violent action datasets. On comparison, our pipeline achieved near perfect classification accuracies over three standard video datasets, outperforming most state-of-the-art approaches and having very low number of vocabulary size compared to previous BoW Models.",https://ieeexplore.ieee.org/document/8614186/,2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA),17-20 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCS.2018.8456261,Machine Learning Based Choice of Characteristics for the One-Shot Determination of the HEVC Intra Coding Tree,IEEE,Conferences,"In the last few years, the Internet of Things (IoT) has become a reality. Forthcoming applications are likely to boost mobile video demand to an unprecedented level. A large number of systems are likely to integrate the latest MPEG video standard High Efficiency Video Coding (HEVC) in the long run and will particularly require energy efficiency. In this context, constraining the computational complexity of embedded HEVC encoders is a challenging task, especially in the case of software encoders. The most energy consuming part of a software intra encoder is the determination of the coding tree partitioning, i.e. the size of pixel blocks. This determination usually requires an iterative process that leads to repeating some encoding tasks. State-of-the-art studies have focused on predicting, from “easily” computed characteristics, an efficient coding tree. They have proposed and evaluated independently many characteristics for one-shot quad-tree prediction. In this paper, we present a fair comparison of these characteristics using a Machine Learning approach and a real-time HEVC encoder. Both computational complexity and information gain are considered, showing that characteristics are far from equivalent in terms of coding tree prediction performance.",https://ieeexplore.ieee.org/document/8456261/,2018 Picture Coding Symposium (PCS),24-27 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IWCMC.2017.7986488,Machine learning based QoE prediction in SDN networks,IEEE,Conferences,"Our work in this paper presents a prediction of quality of experience based on full reference parametric (SSIM, VQM) and application metrics (resolution, bit rate, frame rate) in SDN networks. First, we used DCR (Degradation Category Rating) as subjective method to build the training model and validation, this method is based on not only the quality of received video but also the original video but all subjective methods are too expensive, don't take place in real time and takes much time for example our method takes three hours to determine the average MOS (Mean Opinion Score). That's why we proposed novel method based on machine learning algorithms to obtain the quality of experience in an objective manner. Previous researches in this field help us to use four algorithms: Decision Tree (DT), Neural Network, K nearest neighbors KNN and Random Forest RF thanks to their efficiency. We have used two metrics recommended by VQEG group to assess the best algorithm: Pearson correlation coefficient r and Root-Mean-Square-Error RMSE. The last part of the paper describes environment based on: Weka to analyze ML algorithms, MSU tool to calculate SSIM and VQM and Mininet for the SDN simulation.",https://ieeexplore.ieee.org/document/7986488/,2017 13th International Wireless Communications and Mobile Computing Conference (IWCMC),26-30 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS.2017.8204282,Machine learning based pattern recognition and classification framework development,IEEE,Conferences,"In this paper we describe implementation of several step pattern recognition framework. Pattern recognition is the main aspect for different important areas such as video surveillance, biometrics, interactive game applications, human computer interaction and access control systems. These systems require fast real time detection and recognition with high recognition rate. In this paper we propose implementation of the pattern recognition system. In order to increase recognition rate of the system we apply image preprocessing and neural networks.",https://ieeexplore.ieee.org/document/8204282/,"2017 17th International Conference on Control, Automation and Systems (ICCAS)",18-21 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOTEH.2018.8345519,Machine learning-based QoE prediction for video streaming over LTE network,IEEE,Conferences,"This paper presents a machine-learning software solution that performs a multi-dimensional prediction of QoE (Quality of Experience) based on network-related SIFs (System Influence Factors) as input data. The proposed solution is verified through experimental study based on video streaming emulation over LTE (Long Term Evolution) which allows the measurement of network-related SIF (i.e., delay, jitter, loss), and subjective assessment of MOS (Mean Opinion Score). Obtained results show good performance of proposed MOS predictor in terms of mean prediction error and thereby can serve as an encouragement to implement such solution in all-IP (Internet Protocol) real environment.",https://ieeexplore.ieee.org/document/8345519/,2018 17th International Symposium INFOTEH-JAHORINA (INFOTEH),21-23 March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2001.974483,Maintaining credible dialogs in a VideoBot system with special audio techniques,IEEE,Conferences,"An approach to using actual audio and video clips of real humans to construct artificial, conversational agents and bots is presented. This approach differs from other schemes focusing on believable, emotional, intelligent agents and bots in that it begins with real human subjects but constructs artificial behaviors and interactions with human users as opposed to beginning with artificial characters and trying to construct real interactions. The approach presents several production challenges during the filming, postproduction, and scripting phases of bot creation that make it difficult for human users to sustain suspension of disbelief during interaction with the bot. Various approaches to solving these problems are presented and described.",https://ieeexplore.ieee.org/document/974483/,Proceedings 13th IEEE International Conference on Tools with Artificial Intelligence. ICTAI 2001,7-9 Nov. 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISC47916.2020.9171212,Anomaly Detection in Videos for Video Surveillance Applications using Neural Networks,IEEE,Conferences,"Security is always a main concern in every domain, due to a rise in crime rate in the crowded event or suspicious lonely areas. Abnormal detection and monitoring have major applications of computer vision to tackle various problems. Due to growing demand in the protection of safety, security and personal properties, the needs and deployment of video surveillance systems can recognize and interpret the scene and anomaly events play a vital role in intelligence monitoring. Anomaly detection is a technique used to distinguish various patterns and identify unusual patterns with a minimal period, this pattern is called outliers. Surveillance videos can capture a variety of realistic anomalies. Anomaly detection in video surveillance involves breaking down the whole process into three layers, which are video labelers, image processing, and activity detection. Hence, anomaly detection in videos for video surveillance application gives assured results in regards to real-time scenarios. In this paper, we anomaly was detected in images and videos with an accuracy of 98.5 %.",https://ieeexplore.ieee.org/document/9171212/,2020 Fourth International Conference on Inventive Systems and Control (ICISC),8-10 Jan. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RASSE53195.2021.9686840,Anomaly detection using LSTM neural networks: an application to VoIP traffic,IEEE,Conferences,"Voice communications nowadays are largely dominated by VoIP, which substituted the Plain Old Telephone Network over the years. Telephone exchanges are thus software-based rather than electromechanical, and a whole set of new methods for analyzing anomalies has been introduced. This paper addresses the implementation of an AI application, based on the LSTM model, whose aim is to learn to predict the VoIP traffic shape given what happened in the past, to compare it with the normal or expected traffic, and eventually state whether the system is going to fall into an anomalous situation or not. In the paper we use data from a real working system instead of synthetic data or pre-built datasets. We consider a system in a real production environment, and we deployed the model we developed on the system itself, comparing its forecasts with the actual VoIP traffic flow. We verified that the forecasts and the actual traffic do not significantly differ in the general case. In one case, however, our model contributed to find a configuration problem in the system, allowing to correct it before it could generate further issues. These results confirm the usefulness of the model in detecting malfunctions or anomalies in VoIP based systems. Further applications of the proposed methodology include video conferencing and other IP-based communication systems.",https://ieeexplore.ieee.org/document/9686840/,2021 IEEE International Conference on Recent Advances in Systems Science and Engineering (RASSE),12-14 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GECOST52368.2021.9538768,Anticipation of Parking Vacancy During Peak/Non-peak Hours using Convolutional Neural Network – YOLOv3 in University Campus,IEEE,Conferences,"Searching for a publicly available parking space has become a nightmare to many drivers. With the constant development of global urbanization, human population has increased drastically in the past decades. Searching for a publicly available parking space in a highly populated area can be daunting and time consuming. No matter how much time is spent to find a vacant parking space, it always causes traffic congestion in the area. To alleviate these problems, it is of utmost importance to have a system that can detect and display the vacant parking spaces in real-time. This paper has conducted a study of anticipation of parking vacancy using convolutional neural network called YOLOv3 in a university campus. Image data is gathered from the video capture of the university’s campus open space parking lot. The YOLOv3 algorithm is used to train and predict whether the space is vacant or occupied. Results showed that YOLOv3 has been able to correctly predict the vacant space. The result of the rendering video will then be transformed into an image and is sent to the students via a Telegram group.",https://ieeexplore.ieee.org/document/9538768/,"2021 International Conference on Green Energy, Computing and Sustainable Technology (GECOST)",7-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2018.8490400,Anxious Learning in Real-Time Heuristic Search,IEEE,Conferences,"Real-time heuristic search methods are used when planning time available per agent's move is severely limited (e.g., while pathfinding in a video game). Such agents interleave planning and plan execution. As the agent has to move before a complete plan is computed, it is prone to be misguided by inaccuracies in its heuristic. To get out of heuristic depressions, such agents update their heuristic over time. The usual update process requires multiple state revisits which can make the agent appear irrational to the player. To alleviate such map ""scrubbing"" we propose a new learning mechanism inspired by the psychological notion of anxiety. Our agent maintains a level of anxiety which increases due to state revisits and decays naturally over time. Agent's anxiety causes it to update the heuristic more aggressively thus filling heuristic depressions quicker. Such anxiety-accelerated learning can be used on top of other real-time heuristic search techniques. Empirical evaluation on video-game pathfinding benchmarks demonstrates benefits for the average solution quality when the new mechanism is used by itself or in combination with expendable state marking.",https://ieeexplore.ieee.org/document/8490400/,2018 IEEE Conference on Computational Intelligence and Games (CIG),14-17 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECO55406.2022.9797123,Application Experiment with the Standard Linux Services for Asymmetric Multiprocessing on Heterogeneous System on a Chips,IEEE,Conferences,"Modern System on a Chips (SoC) integrate a diverse set of execution units to process the ever-increasing workloads of new embedded applications, i.e., they are heterogeneous SoCs implementing asymmetric multiprocessing. They include general-purpose application processor cores that run an embedded operating system (OS), such as Linux, and they add microcontrollers to execute real-time tasks such as input-output operations, Digital Signal Processors (DSP) for handling stream data such as video or sound, and Graphics Processing Units capable of General-Purpose Compute (GPGPU). These additional execution units, sometimes also called accelerators, are integrated into the software environment utilizing some specialized OS services and applications, generally with some software framework sitting between the OS and the execution units, hiding all the hardware details about the integration of these execution units on the hardware level. GPGPUs are typically handled by specialized frameworks such as CUDA (Compute Unified Device Architecture) or OpenCL (Open Computing Language) fully defining how the execution units are programmed and managed. However, the other types of additional execution units have different, much more diverse HW coupling to the application processors running the OS (such as shared memory, pipes, etc.), and their programming and execution management models can be also fundamentally different. In this paper, we present our experiments with the standard RemoteProc and RPMsg software frameworks of Linux for asymmetric multiprocessing. We did our evaluation on the TI Sitara AM335x and AM57x SoCs platforms utilizing the Beaglebone Black and AI single board computer boards and we also present some initial performance results on the AM335x platform demonstrating the applicability and limits of these technologies.",https://ieeexplore.ieee.org/document/9797123/,2022 11th Mediterranean Conference on Embedded Computing (MECO),7-10 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00102,Application of Active Learning in Face Liveness detection,IEEE,Conferences,"Face anti-fraud attack detection is mainly to help the face recognition system determine whether the collected face is the user's face or a printed photo, recorded video, 3D mask and other forgeries, so it is also called liveness detection. This technology is essential for the security of mobile phone unlocking, access control, and face payment. So how to automatically and efficiently distinguish the authenticity of images and resist spoofing attacks to ensure system security has become an urgent problem in face recognition technology. This article aims to study the application of active learning in face detection. Through the third-party library of python video processing, from six (three real faces, three fake faces) videos, three kinds of face images of different races and different ages are intercepted. Then through the CNN network, pool-based active learning was constructed, and it was run on the colab platform. By comparing the results obtained by selecting an example with a smaller LMU value and randomly selecting an example, the superiority of the LMU algorithm was shown, and after 350 epochs on the self-made data set, the accuracy rate is close to 90%, and as the training continues, the accuracy rate will be further improved.",https://ieeexplore.ieee.org/document/9696139/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2010.5580634,Application of Directdraw in real-time data acquisition system,IEEE,Conferences,"In many data acquisition systems which based on the Windows operating system often require to display interface to achieve real-time smooth display the current curve data, in no affecting high-priority-level working conditions such as data acquisition, storage and so on. Using the traditional GDI function under Windows will cause the page jitter phenomenon. This paper introduces the brief principle of DirectDraw that is the game development tools, use the DirectDraw method and software “video page” switching technology, and achieve a smooth real-time displaying the current curve data. This method has been successfully applied in the sleep physiology parameter acquisition analysis system, displaying computer images have advantages of fast processing and no flickering.",https://ieeexplore.ieee.org/document/5580634/,2010 International Conference on Machine Learning and Cybernetics,11-14 July 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.1996.566482,Application of reversible discrete-time cellular neural networks to image copyright labelling,IEEE,Conferences,"In this paper, we proposed a discrete-time cellular neural network (DTCNN) structure for the labelling of digital images. First, we present the concept and the structure of reversible DTCNN, which can be used to generate 2D binary random image sequences. Then both the original image and the copyright label, which is often another binary image, are used to generate a binary random key image. The key image is then used to scramble the original image. Due to the reversibility of a reversible DTCNN, the same reversible DTCNN is used to recover the copyright label from a labelled image. Due to the high speed of a DTCNN chip, our method can be used to label image sequences, e.g., video sequences, in real time. Computer simulation results are presented.",https://ieeexplore.ieee.org/document/566482/,1996 Fourth IEEE International Workshop on Cellular Neural Networks and their Applications Proceedings (CNNA-96),24-26 June 1996,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAPAI55158.2022.9801568,Applying Transfer Learning to Traffic Surveillance Videos for Accident Detection,IEEE,Conferences,"Automated traffic video surveillance is a crucial research domain in computer vision due to the need to enable highway safety. It is very important to detect road accidents from traffic surveillance videos in an automated manner to take necessary actions and save the lives of people and properties. Motivated by the same, this paper proposes a method to detect road accidents from traffic surveillance videos in an automated manner. Specifically, we use an object-centric accident detection model using the YOLOv2 architecture based on the transfer learning technique. The YOLOv2 model is a homogeneous convolutional architecture that makes it faster to predict bounding boxes. This work includes a brief description of the YOLOv2 architecture and how we fine-tune a 32-layer variant pre-trained on the VOC dataset to our custom accident dataset. Our experiments using a real-world anomaly detection dataset show significant results in terms of mean average precision. Moreover, our model works in real-time, achieving 60 FPS on an NVIDIA Tesla K80 GPU and ~16.67 FPS on a standard laptop with a 4GB GT GPU. Our implementation can thus provide a near real-time accident localization with 76% mAP on the road accident dataset.",https://ieeexplore.ieee.org/document/9801568/,2022 International Conference on Applied Artificial Intelligence (ICAPAI),5-5 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCNT51525.2021.9579722,Architecture Design of AI and IoT based System to Identify COVID-19 Protocol Violators in Public Places,IEEE,Conferences,"The World Health Organization (WHO) describes COVID-19 as a pandemic that is causing a worldwide health disaster. Wearing a face mask in public places is the most effective method to curb the spread of the virus. The Internet of Things is emerging as one of the most significant innovations and playing a vital role during the pandemic. Affordable remote health monitoring devices help doctors to track quarantined patients. Our government is trying its best to control the spread of the virus. Citizens who do not follow the protocols serve as the reason for these widespread infections. Our work proposes a system to identify protocol violators in real-time. Our system consists of a face mask detection module, a social distance monitoring module, and a non-contact temperature monitoring module. We intend to deploy our proposed approach in public places such as airports, schools, and hospitals. These modules depend on video feeds from general security cameras and IoT sensor nodes deployed around public areas. Health care and security officials can subscribe to real-time data feeds to track public behaviour.",https://ieeexplore.ieee.org/document/9579722/,2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT),6-8 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISVDAT.2014.6881081,Architectures and algorithms for image and video processing using FPGA-based platform,IEEE,Conferences,"The work illustrates the use of platform-based design to achieve efficiently-configured hardware-software system solution that can meet the conflicting demands of high performance, low power and quick turnaround time for embedded system development. It presents embedded system design techniques using field-programmable gate arrays (FPGAs) for image and video processing application. Here, by identifying, building and integrating all necessary hardware and software components, an embedded implementation of a kernel-based mean shift (KBMS) object tracking algorithm has been proposed [1]. To fulfill the specific needs of hardware/software implementation Virtex-5 FXT FPGA device (which has an embedded PowerPC processor) available on Xilinx ML-507 platform has been used [2]. The work begins with the development of required configurations of Xilinx ML-507 FPGA platform for developing architectures and algorithms in the integrated hardware software environment. This requires configurations of FPGA platform peripherals using application programmer interface (API) and other required hardware building blocks. This configuration is necessary for accessing the image pixels by the FPGA and for testing of various architectural blocks designed subsequently. It has been used for capturing real-time 640×480VGA resolution video frame at 60 frames per second in the double data rate (DDR2) synchronous dynamic random access memory (SDRAM). Fig. 1 shows a top-level view of the complete system. On the configured platform, we have proposed and developed various architectural building blocks that are mostly generic in nature and can be widely used in many practical image and video processing systems. These image processing hardware units invariably need many complex arithmetic operations. These complex operations such as division, square root etc. are realized through fixed-point binary logarithmic and antilogarithmic units. In the proposed architectures, most of the operations are performed using 32-bit fixed-point format. As the architectures are based on logarithmic number system (LNS), they have the advantages of consuming minimal logic resources and are able to process. large datasets by realizing time-critical processes in the available block RAMs (BRAMs) and DSP slices. Architecture for global image thresholding operation has been proposed that results in a resource-efficient FPGA implementation of the computation of between class variance (BCV) for realizing the Otsu's image thresholding algorithm [3]. The compute-intensive BCV requires the computation of normalized cumulative histogram and normalized cumulative intensity area. We have next proposed an improved label-equivalence based connected component labeling algorithm [3] that works on the binary images obtained from the image thresholding unit and identifies an object on a video frame. The proposed algorithm improves upon the Stefano-Bulgarelli (SB) algorithm by modifying its equivalence handling procedure, and removes the partial merging problem associated with the SB algorithm. The improved algorithm is implemented on the embedded PowerPC processor. Finally, all the hardware building blocks and algorithms described so far are utilized for an embedded implementation of the KBMS algorithm. The required application-specific architectural building blocks have been proposed for its embedded realization on Xilinx ML-507 platform. To understand issues related to the embedded realization of the KBMS algorithm, a MATLAB/C implementation is created. Subsequently, hardware architectures have been proposed for the time-critical parts, namely, the computations of weighted local histogram, kernel-smoothed local histogram (KSLH), Bhattacharyya coefficient based local similarity measure, center of gravity (COG) and new mean shift location. The embedded design utilizes soft IPs that include, joint test action group (JTAG) controller, BRAMs controller, multi-port memory controller (MPMC), processor local bus (PLB), inter-integrated circuit (I2C) controller and the universal asynchronous receiver-transmitter (UART) controller. The hard IPs includes the PowerPC 440 processor, BRAMs, digital clock manager (DCM) and DSP48E slices. Device utilization summary for implementing the KBMS algorithm shows that it utilizes 7.8% FPGA slices, 8.1% BRAMs and 35.9% DSP48E slices",https://ieeexplore.ieee.org/document/6881081/,18th International Symposium on VLSI Design and Test,16-18 July 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA49188.2021.9610783,Array computing based system for visual servoing of neuroprosthesis of upper limbs,IEEE,Conferences,"A hardware-software system implemented for visual servoing of prosthetic arms. The prosthetic system comprises glass worn eye-tracker and video camera and a prosthesis - mounted stereo camera system. The proposed light-weight architecture has to be implemented as a wearable device. We propose a hybrid solution which contains object selection and recognition, in glass camera view, object matching to the prosthesis-mounted camera to compute depth map for object reaching by the prosthetic arm. The object selection and object matching both use the Scale Invariant Feature Transform (SIFT) algorithm. The SIFT algorithm is time-consuming, so the FPGA acceleration is proposed for the light-weight device. The proposed implementation is compatible with real-time servoing of the prosthetic arm.",https://ieeexplore.ieee.org/document/9610783/,2021 17th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA),29 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/eIT53891.2022.9813944,Artificial Intelligence System for Emotion Recognition and Text Analytics,IEEE,Conferences,"Harnessing the power of emotional intelligence by analyzing a person&#x2019;s behavioral and linguistic skills can help humans improve their approach to social interactions. In this paper, we propose an artificial intelligence-based stand-alone system that will allow us to classify and analyze facial expressions in real-time and perform sentiment analysis by examining the body of the text (extracted from audio) to understand the opinion expressed by it. This helps us provide a deeper understanding of how humans really feel at a given time. The proposed system uses a deep neural network (DNN) for classifying eight basic emotions based on features extracted from facial expressions and uses pretrained sentiment analysis tools to quantify text (extracted from audio) based on polarity. The system is implemented by extracting the audio and visual cues from real-time scenarios and using these extracted cues to perform facial expression recognition and text sentiment analysis. The integrated system extracts video and audio simultaneously with a frame rate of 4-5 fps. The facial emotion detection system successfully detects facial expressions of faces detected in real-time video with an accuracy of about 86.75&#x0025;. The speech extracted is converted to text, cleaned, and processed to determine if the attitude of the speaker in a given situation is positive, negative, or neutral.",https://ieeexplore.ieee.org/document/9813944/,2022 IEEE International Conference on Electro Information Technology (eIT),19-21 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.5594/M001816,Artificial Intelligence for the automation of robotic cameras in live sports,SMPTE,Conferences,"The demand to cover lower tier sports games is rising rapidly. Unfortunately, it is economically not viable to deploy the traditional production methods, crews and equipment that we know for higher tier sports events. Production approaches requiring much less people are needed. For the last few years, high quality single man operated equipment combining replay video mixing, graphical insertions, … are being introduced. However, one still needs cameramen to operate the different cameras around the sports field. — In this paper we present the concept of a fully automated system creating multiple camera feeds via a series of robotic cameras. These robotic cameras are not human operated but piloted by an Artificial Intelligence based system. This system analyses the complete sports scene for relevant items to capture and uses that information to individually steer the different robotic cameras to generate the desired different camera views. More concretely, the AI backed system points, in real-time, the different robotic cameras to the relevant parts of the game and applies the appropriate zoom factor corresponding to the desired camera views. We will show first results of a prototype system applied to a soccer game. — To further push the limits of automating the production, the automatic camera operation is complemented with an automatic selection of the most interesting camera angle out of the available camera feeds. This automatic direction can either be used to create a fully automatic system or to assist the single operator in charge of a complete multi-camera production. First experiments indicate that the quality of the camera selection comes close to the judgement of real directors. — To achieve human operator quality, both for the camera selection and for the automatic steering of robotic cameras, we designed an Artificial Intelligence based approach, combining several techniques, including a combination of various machine learning components to mimic the behavior of human operators.",https://ieeexplore.ieee.org/document/8609875/,SMPTE 2018,22-25 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAPS52117.2021.9730707,Artificial intelligence enabled smart glove for visually impaired,IEEE,Conferences,"Object detection is a technique to tag objects present in the frame of an image, video sequence, and real-time video. In recent years, the world has been reshaped around deep learning algorithms. This paper makes the user aware of the obstacles present in his environment. There are two fundamental parts in this paper: the software part and the hardware part. The state-of-the-art You Look Only Once (YOLO) algorithm was applied in the present work for object identification. The overall analysis shows that this algorithm produces accurate results for real-time object detection and can be considered faster object identification.",https://ieeexplore.ieee.org/document/9730707/,"2021 International Conference on Control, Automation, Power and Signal Processing (CAPS)",10-12 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSCCC51823.2021.9478176,Assessment and Evaluation of cancer CT images using deep learning Techniques,IEEE,Conferences,"Image detection and extraction play's a significant role for automation and image analysis process. In the field of medical domain, the extraction of features from real-time images of cancer CT has becoming challenging. The incorporation of machine learning in medical data could explore the salient features that could be used for exploring patterns which then aids in decision making process.Motivation The realm of determining the accuracy of the observed medical image serves as an important factor in accessing image quality and recognition. The quality of image acquisition and analysis lies at quality factors such as range, sharpness, color, fare of lens, artifacts, and tone reproduction. The mechanism of automatic prediction is being useful for a number of practical applications, but if there is a systematic means of accessing the sections of poor quality of subsequent images in medical informatics it can be helpful for assessment and evaluation to determine flags and noise thereby rendering quality-based fusion on all observed images.Objective The objective is to validate the accuracy of cancer CT images using machine learning techniques. The assessment is made in accordance with the deployment of Convolution Neural Network with the enhancement in the image quality features.Methodology The process of assessment and evaluation involves training the dataset with proposed quality metrics. Once trained then it is modeled using CNN with ELM and its training parameters. Once modeled then tested accordingly with automatic prediction with quality factors and detecting sections of poor fusion focusing on interpretation and evaluation.Results and Conclusion The analyses of lung Computerized Tomography (CT) has various interventional variations which can significant degrade the level of accuracy. The NIST FRTV rate has FNMRs of lesser that 3% at the level of 0.01% FMR for higher learning algorithms. When assuming at the level of 0.1% the level of magnitude moves to a higher level of variations. Video frames if received will also have some of the variations in higher level of magnitude. In order to improve the quality metrics, we proposed a new model for predicting the level of cancer using Deep Convolution Extreme learning machine.",https://ieeexplore.ieee.org/document/9478176/,2021 2nd International Conference on Secure Cyber Computing and Communications (ICSCCC),21-23 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData50022.2020.9378267,Assessment of Data Augmentation Techniques for Firearm Detection in Surveillance Videos,IEEE,Conferences,"In this paper, we propose a Faster R-CNN based model for detection of firearms in surveillance and Closed-Circuit Television (CCTV) video. Due to the lack of a publicly available benchmark surveillance video database containing firearms, we created our own firearm database composed of only real surveillance footage from scenes with people holding firearms. Portable firearms such as rifles and pistols, often referred to as small arms, have a high degree of variation in surveillance footage in terms of shape, illumination, scale, pose, and occlusion. To deploy a fast and accurate firearm detection system for real world scenarios, it is important to detect all types of common small arms. To accomplish this, we collected images for our database that contain both handguns (pistols and revolvers) and long guns (rifles and shotguns) to capture as much variation of commonly used small arms as possible. To train our firearm detector we first, assess 18 common geometric and photometric data augmentation techniques in order to identify which ones improve detection performance. Next, we identify deep learning based small arm detection techniques that improve detection performance. We reuse these techniques in another more comprehensive augmentation assessment in order to identify the most efficient combination in terms of detection performance. Finally, we optimize the hyper-parameters of all the tested firearm detection models and apply cross-validation. Our proposed model can accurately detect firearms in video frames taken from real surveillance footage close to real-time, yielding precision and recall scores of 93.9% and 96.4% for handguns, and 95.2% and 94.6% for long guns.",https://ieeexplore.ieee.org/document/9378267/,2020 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISCE50729.2020.00081,Attendance System Based on Dynamic Face Recognition,IEEE,Conferences,"A video-based attendance system is designed by using the method of real-time face recognition. The system supports multi-user attendance and face liveness detection at the same time. The system can automatically collect face data, which will be saved in the database as well as attendance results. The face detection part of the system is based on MTCNN (Multitask Convolutional Neural Network) algorithm, and the face recognition part is based on FaceNet algorithm. The algorithm implementation is based on TensorFlow framework, and the face liveness detection part is based on ERT (Ensemble of Regression Tree) algorithm, which can judge whether the user blinks. The attendance system is written in Python language, and the user interface is designed by Qt library. The experimental results show that the system achieves a good performance in real-time face recognition. The false accept rate and false rejection rate of face recognition are within 2%, and the recognition rate can be stable at 20 FPS.",https://ieeexplore.ieee.org/document/9258833/,"2020 International Conference on Communications, Information System and Computer Engineering (CISCE)",3-5 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO51393.2021.9596399,Attendance System Implementation Using Real Time Face Recognition,IEEE,Conferences,"In this paper, we propose Facial Recognition using Supervised Learning technique to implement an attendance system using a comparative study between different methods of face detection and face recognition. The first step that is required is face detection, i.e., detecting faces in an image, video or real time coverage consisting of different types of objects (distinguishing faces from non-faces). Once the face is detected, feature extraction, i.e., determining the uniqueness of the face by takin out the features is performed. Eventually, training and classification of the facial databases is done and is tested using various classification techniques based on the comparative performance and efficiency for recognizing faces. We attempt to use this facial recognition system on a data set of faces of institute/university students for which the attendance system will be implemented. Attentiveness Monitoring feature is added which monitors facial features such as eye blinks, yawn and eye movements, to determine the relative attentiveness among students. Consolidated attendance is easily available to view and download.",https://ieeexplore.ieee.org/document/9596399/,"2021 9th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",3-4 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTKE.2010.5692918,Audience analysis solution using soft computing methodologies,IEEE,Conferences,"In this paper, we propose a pedestrian analysis solution helpful for adaptive content delivery and interest measurement for outdoor advertisement displays. The proposed system has built-in camera on the top panel of such displays which capture the real time viewers' frames. The captured frames have been analyzed for detection of faces using Viola-Jones algorithm. The detected faces have been processed with various image processing operations and age, gender, and race (ethnicity) parameters have been estimated using machine learning approaches. The current set of experiments has been generated using Classification and Regression Trees (CART) with Adaboost, Support vector machines (SVMs), Scalable boosting, and neural networks. Dimensionality reduction techniques and other optimizations have been adapted to make the system run efficiently on embedded devices. Computational experiments generated over thousands of real images and live video streams are encouraging and we plan to deploy the solution on large scale advertisement panels.",https://ieeexplore.ieee.org/document/5692918/,2010 Eighth International Conference on ICT and Knowledge Engineering,24-25 Nov. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMCET.2016.7777429,Augmented Reality application: A new implementation chain,IEEE,Conferences,"Augmented Reality (AR) is a live view of a real-world environment. With advanced AR technology, artificial information about the environment and its objects can be overlaid on the real world. This paper presents a complete augmented reality process for a video sequence captured by a moving camera. The main goal is to construct a full chain composed of 4 blocks that correspond to the main steps of augmented reality process: feature detection, feature extraction, feature matching and image registration. Our work proposes an improved technique for image augmentation, starting from feature detection and ending by image registration. We used the well-known techniques (e.g. SIFT, SURF, etc.) for features detection and extraction in order to compare their performance. Furthermore, we added a features learning step (using SVM, KNN and SRC) to improve the image registration process. The final full chain uses the best method in each block. This best combination is then applied on all video frames taken by the camera. Thereafter, we obtain a video showing the augmented object instead of the real one.",https://ieeexplore.ieee.org/document/7777429/,2016 IEEE International Multidisciplinary Conference on Engineering Technology (IMCET),2-4 Nov. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDMW.2010.27,Augmenting Chinese Online Video Recommendations by Using Virtual Ratings Predicted by Review Sentiment Classification,IEEE,Conferences,"In this paper we aim to resolve the recommendation problem by using the virtual ratings in online environments when user rating information is not available. As a matter of fact, in most of current websites especially the Chinese video-sharing ones, the traditional pure rating based collaborative filtering recommender methods are not fully qualified due to the sparsity of rating data. Motivated by our prior work on the investigation of user reviews that broadly appear in such sites, we hence propose a new recommender algorithm by fusing a self-supervised emoticon-integrated sentiment classification approach, by which the missing User-Item Rating Matrix can be substituted by the virtual ratings which are predicted by decomposing user reviews as given to the items. To test the algorithm's practical value, we have first identified the self-supervised sentiment classification's higher performance by comparing it with a supervised approach. Moreover, we conducted a statistic evaluation method to show the effectiveness of our recommender system on improving Chinese online video recommendations' accuracy.",https://ieeexplore.ieee.org/document/5693423/,2010 IEEE International Conference on Data Mining Workshops,13-13 Dec. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData.2018.8621906,AutoHighlight : Automatic Highlights Detection and Segmentation in Soccer Matches,IEEE,Conferences,"More and more football fans prefer to watch football highlights because it takes too much time to watch the entire football game. At the same time, many TV stations and sports companies need to store and retrieve fragments of different events in football matches. The traditional way is to analyze and cut the football match manually. This method is time consuming and the process largely depends on personal choice, which results in unsecured accuracy. Therefore, it’s necessary to develop a fast and accurate system for automatic summarization and analysis of football matches video. Automatic event detection and segmentation in football matches is about detecting specific event and extracting the related moments for football viewers and companies. This paper presents a deep learning based event detection and segmentation system for emphasizing specific events during football matches. The proposed system use live text of football matches as auxiliary information to segment the important event segmentation. We firstly use the deep learning model to tag each live text. Second, we merge successive live texts with the same label. Then we go to the football match to segment corresponding segmentation to the merged live text. Finally, if necessary, we run pure video based event detection model to these segmentation for finer grained segmentation. Experiments on real football matches show encouraging results. The proposed system greatly improves the accuracy and speed of segmentation.",https://ieeexplore.ieee.org/document/8621906/,2018 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMARTGENCON51891.2021.9645897,AutoRoto: Automated Rotoscoping with Refined Deep Masks,IEEE,Conferences,"The legendary movie Jurassic Park (1993) would have been nearly impossible without Computer Generated Imaginary(CGI). The movie's ground-breaking visual effects was able to smoothly integrate virtual characters with real-life footage. This rapidly increased the use of visual effects in modern movies. Creating high-end visual effects is a complex workflow where each shot is subjected to artistic operations. The key step in this workflow is the process called digital rotoscoping. It is the process of manually outlining and cutting out the foreground characters in a raw video frame via software. This is a time-consuming process and the average number of frames a professional artist can accomplished per day is 15. Due to this, it has become a temporal bottleneck of the VFX pipeline.In order to address the above mentioned issue, an automated rotoscoping approach with instance segmentation is presented in this paper. The novel method is efficient, accurate and will redeem the time of rotoscoping artists. Our model was able to extract foregrounds of multiple instances of classes and to the best of our knowledge our rotoscoping tool is the most generalized to extract foregrounds.",https://ieeexplore.ieee.org/document/9645897/,"2021 International Conference on Smart Generation Computing, Communication and Networking (SMART GENCON)",29-30 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WSAI49636.2020.9143279,Automated Analysis of Seizure Behavior in Video: Methods and Challenges,IEEE,Conferences,"Automated analysis of seizure behavior in video using intelligent video analytics technology has significant applications in healthcare industry, since it can provide accurate and quantitative measurement of human seizure behavior for assisting diagnosis. This paper presents a brief survey on intelligent video analytics for automated seizure behavior analysis, including both conventional motion analysis based approaches and the state-of-the-art machine learning based approaches. Furthermore, a new automated video analytics framework is proposed in this paper, by exploiting the machine learning approach to build a seizure motion model and performing automatic detection of seizure events in the surveillance video in real time. This paper also discusses the preliminary experimental results and deployment of the proposed framework, as well as the future research challenges in this area.",https://ieeexplore.ieee.org/document/9143279/,2020 2nd World Symposium on Artificial Intelligence (WSAI),27-29 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ic-ETITE47903.2020.203,Automated Detection Of Driving Pathway Using Image Processing,IEEE,Conferences,"Image data is one of the most popular real world input data that can be used for variety of applications ranging from robotics and computer vision to security systems. In combination with other methods such as neural network, Artificial neural network and image processing techniques, manipulation of image data can lead to applications such as detection of objects, tracking, identification and vision based robotics and so on. Advanced Driver Assistance System (ADAS) also use image for camera based driver assistance systems.The report covers a hardware model system that tests the software work of detection of traffic signs and path for it own ADAS systems. Different problems were tackled, including the choice of OS, and additional hardware components needed to tackle. The choice of programming languages, equipment, OS and methods were based on simplicity and practicality. Artificial neural network in combination with Open CV libraries were used for stop sign, traffic light and path road detection. The hardware model consisted of RC Car attached to raspberry pi board with a mounted pi camera for video streaming and an arduino controller attached to a radio transmitter for controlling through Open CV running in windows PC.",https://ieeexplore.ieee.org/document/9077722/,2020 International Conference on Emerging Trends in Information Technology and Engineering (ic-ETITE),24-25 Feb. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/3ICT53449.2021.9581593,Automated Face Detection and Control System Using Computer Vision based Video Analytics to Avoid the Spreading of COVID-19,IEEE,Conferences,"The classification of COVID-19 as global pandemic has led researchers and scientists to design solutions in order to reduce the fast spreading of the virus. This paper presents a novel detection and control system that utilizes Computer Vision based video analytics to help in reducing the speed of the spreading of the virus by recognizing people and detecting masks. The system uses the body temperature and other user biometrics to give access to a particular environement. The proposed system is able to identify a person who wants to access an environment and tracks his movement. The system can also control the door of the main entrance, the elevator, or any access zone, and generate audio notifications to alert user(s) to put their mask(s). The implementation results show that the proposed system has the advantages of a high sensitivity of 98.8% for front faces and 90.3% for turned faces, and ensure a safe environment while preserving the benefits of being modular and low cost.",https://ieeexplore.ieee.org/document/9581593/,"2021 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies (3ICT)",29-30 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOEI51242.2021.9452878,Automated Face Recognition by Smart Security System Using AI & ML Algorithms,IEEE,Conferences,"Face recognition has a very important role in various applications, from security, surveillance to authentication. For safety most of the household is having CC cameras such that they could recognize the persons from it. These CCTV are allocated for having safety and to know who visited their houses. In few highly secured places where allowance to any unknown intruder is strictly prohibited. Thereby, this paper deals with a system which could recognize the face of the intruder through surveillance camera using ML and AI based algorithms. The design specified is successfully implemented using HOG feature extraction and SVM classification algorithms and it classifies the faces for a video stream given as input. The major objective entitled to this paper is to recognize the faces of people from the video by HOG feature extractor and classify them using SVM and train the machine to tell who is the person working for the organization and who are the intruder.",https://ieeexplore.ieee.org/document/9452878/,2021 5th International Conference on Trends in Electronics and Informatics (ICOEI),3-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECON53876.2022.9752417,Automated Gate Control System using Face Mask Detection,IEEE,Conferences,"This research paper gives a brief idea of controlling entrance gates of different areas like metro stations, railway stations, airports, corporate offices, restaurants, hotels and home with the face mask detection technology. In this, the camera will capture the real time video of a person using Artificial Intelligence[15],whosoever is entering the gate, processes the video and detects if the concerned person is wearing the mask properly or not. If the person is wearing a mask then the gate will open, if not then the gate will remain closed until the mask has been worn properly. The main motivation for this project comes from the current situation in the world where Covid-19 is spreading at a pace which is being difficult to control. This upcoming technology prototype can fuel in new ideas into different projects which are already ongoing to battle the pandemic. Also, the scope of this technology is not just limited to the face mask detection and has a wider and a more complex use-case in the real world.",https://ieeexplore.ieee.org/document/9752417/,2022 International Mobile and Embedded Technology Conference (MECON),10-11 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTIS.2019.8883683,Automated Traffic Volume Analytics at Road Intersections Using Computer Vision Techniques,IEEE,Conferences,"As the hardware cost decreases, more and more cameras have been deployed and used to monitor traffics. Widely deployed cameras enable a wide range of computer vision-based applications for traffic analytics. In this work, we propose a computer vision-based intelligent system which can analysis traffic at road interactions. Our system leverages existing traffic monitoring cameras and applies computer vision techniques to provide detailed traffic analysis results. To achieve these goals, we develop a deep learning object detection model based on Single Shot MultiBox Detector (SSD). Our approach is able to detect and track vehicles, pedestrians, traffic signs and other related-objects on the road. We build a robust model to analysis the detected objects, our system could estimate traffic volume and further infer traffic congestion, traffic rule violation and so on. To evaluate the proposed model, we use video data collected from multiple cameras at several intersections in real-world environments. The evaluation results show that our system is able to provide reliable and accurate traffic analysis results in real time. In addition, we also tested our system under different light conditions, and results show that our system could achieve similar accuracies under different light conditions.",https://ieeexplore.ieee.org/document/8883683/,2019 5th International Conference on Transportation Information and Safety (ICTIS),14-17 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MILCOM.2008.4753375,Automated discovery of information services in heterogeneous distributed networks,IEEE,Conferences,"The Global Information Grid (GIG) will be comprised of collections of different service capability domains (SCDs). Each SCD offers a set of information services, such as voice over IP (VoIP), video delivery, and information translation, and is managed as a separate system. The GIG information services will include several types of communications services (e.g., VoIP and streaming video), translation services (e.g., document translation and data translation) and information services (e.g., content discovery and domain name service). These different services may be described using various methods, including specification documents and lookup tables, and will have associated service level agreements (SLAs). As SCDs become richer in their service offerings and more dynamic in their service availability, the discovery of end-to-end services meeting end-user needs becomes extremely challenging. Currently manual methods are employed to map end-user needs to the end-to-end service combinations that GIG deployments can support. Such manual methods are inefficient and error prone. Automation of end-to-end service discovery within the GIG is highly desirable, but is an exceedingly complex task. Current efforts to automate service discovery, for example, the service location protocol or the service oriented architecture, provide service discovery through registration and strict service type definitions. These require coordination across all SCDs for all possible information technology (IT) service offerings. Ideally, individual SCDs would describe their services through individual service description documents. Then mapping of end-user service requests to appropriate collections of SCD services could be performed automatically. This requires the development of semantic reasoning and ontology for service descriptions and service capability matching. This paper describes our approach for automated discovery of information services on GIG-like network deployments.",https://ieeexplore.ieee.org/document/4753375/,MILCOM 2008 - 2008 IEEE Military Communications Conference,16-19 Nov. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASEW52652.2021.00024,Automated game testing using computer vision methods,IEEE,Conferences,"Video game development is a growing industry nowadays with high revenues. However, even if there are many resources invested in the software development process, many games still contain bugs or performance issues that affect the user experience. This paper presents ideas on how computer vision methods can be used to automate the process of game testing. The goal is to replace the parts of the testing process that require human users (testers) with machines as much as possible, in order to reduce costs and perform more tests in less time by scaling with hardware resources. The focus is on solving existing real-world problems that have emerged from several discussions with industry partners. We base our methods on previous work in this area using intelligent agents playing video games and deep learning methods that interpret feedback from their actions based on visual output. The paper proposes several methods and a set of open-source tools, independent of the operating system or deployment platform, to evaluate the efficiency of the presented methods.",https://ieeexplore.ieee.org/document/9680292/,2021 36th IEEE/ACM International Conference on Automated Software Engineering Workshops (ASEW),15-19 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMEW46912.2020.9106051,Automatic Key Moment Extraction and Highlights Generation Based on Comprehensive Soccer Video Understanding,IEEE,Conferences,"The massive growth of sports video has resulted in a need for automatic generation of sports highlights that are comparable in quality to the hand-edited highlights produced by editors. Many methods have been applied to this task and have achieved some positive results. Unlike previous works ignoring the multi-modality information, we propose a novel system that leverages visual and audio information derived from the soccer video. The proposed system involves three crucial tasks which can be jointly used to produce highlights automatically, i.e. play-back detection, soccer event recognition and commentator emotion classification. We introduce a new dataset of 460 soccer games totaling 700 hours with a benchmark for three tasks. Making use of recent progress in deep learning, we further provide strong baselines on three tasks. The experiments on the proposed dataset demonstrate state-of-the-art performance on each independent task. The real-world deployment shows that this system can be useful for soccer games to find and extract soccer video highlights.",https://ieeexplore.ieee.org/document/9106051/,2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),6-10 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IRIA53009.2021.9588707,Automatic License Plate Recognition System Using SSD,IEEE,Conferences,"Automatic License Plate Recognition (ALPR) is a very widely used system in applications such as parking management, theft detection, traffic control and management etc. Most of the existing ALPR systems fail to showcase acceptable performance on real time images/video scenes. This work proposes and demonstrates implementation of a deep learning-based approach to locate license plates of four wheeler vehicles thereby enabling optical character recognition (OCR) to recognize the characters and numbers on the located plates in real time. The proposed system is decomposed into three sub-blocks viz. Vehicle image/video acquisition, License plate localization and OCR. A simple setup using a reasonable resolution webcam has been designed to capture images/videos of vehicles at some entry point. We propose to utilize Single Shot Detector (SSD) based Mobilenet V1 architecture to localize the license plates. The hyper parameters of this architecture are selected with rigorous experimentation so as to avoid over-fitting. We have compared performance of two OCRs viz. Tesseract OCR, Easy OCR and found the superiority of Easy OCR since it utilizes deep learning approach for character recognition. NVIDIA Jetson Nano and Raspberry Pi 3B hardware platforms have been used to implement the entire system. The parameters of these three sub-blocks have been optimized to yield real time performance of ALPR with acceptable accuracy. The proposed and implemented system on Jetson Nano allows processing of videos for ALPR having accuracy more than 95%.",https://ieeexplore.ieee.org/document/9588707/,2021 International Symposium of Asian Control Association on Intelligent Robotics and Industrial Automation (IRIA),20-22 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SYNASC51798.2020.00043,Automatic Real-Time Road Crack Identification System,IEEE,Conferences,"Crack identification is a common problem that requires human involvement and manual identification. Our work is focused on detecting street surface cracks using Computer Vision algorithms. The problem at hand has been divided in three steps: (i) the first step transforms a given 3D video in 2D individual frames, (ii) the second step processes these frames in order to identify the relevant part of the street using Support Vector Machine and Vanishing Point Detection and (iii) in the third step the detection itself has been implemented using three methods: Convolutional Neural Network, U-Net and a Local Binary Pattern. In this paper we present our methods, experiments and results for detecting cracks on surfaces like streets and sidewalks.",https://ieeexplore.ieee.org/document/9357098/,2020 22nd International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC),1-4 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIMSA.2003.1227225,Automatic detection of anomalous behavioural events for advanced real-time video surveillance,IEEE,Conferences,"This paper introduces the architecture and the implementation details, of an automatic real-time video surveillance system, capable of autonomously detecting anomalous behavioural events. Today video surveillance systems show two main drawbacks, namely: they are not flexible in adapting to different operative scenarios (they only work in a well known and structured world); they generally need the assistance of a human operator in order to recognize and to tag specific visual events. This paper describes an innovative approach for overcoming the previous limitations. In particular, the proposed system is capable of automatically adapting to different scenarios without any human intervention (but the placement of the TV sensors), and uses robust self-learning techniques to automatically learn the ""typical"" behaviour of the targets in each specific operative environment. Starting from this learned knowledge, the system can give alerts in an automatic way, by detecting the ""anomalous trajectories"" of visual targets in the controlled scene. To obtain robust self-learning capabilities, an improved version of the Altruistic Vector Quantization algorithm (AVQ) is proposed, capable of automatically describing the trajectories of moving objects in complex, not structured, outdoor environments. The modified AVQ is capable of autonomously calculating the number of trajectory prototypes, and improves the representativeness of the prototypes themselves. The final prototypes characterize the ""typical"" visual behavior of the targets in the controlled scene. Anomalous behavior is detected if visual trajectories deviate from the ""typical"" learned prototypes. The system has been implemented by means of standard PCs and TV cameras, and has been tested in many real outdoor contexts in different conditions (night and day). It is capable of running in real-time (15 fps for each camera) on general purpose HW. After a preliminary period (about 40 minutes in order to grant a significative interval of time to learn all the ""typical"" visual trajectories), the system is capable of giving automatic alerts about events that do not conform to typical behaviors. After changing the field of view, if enough learning time is granted (usually the previously mentioned 40 minutes), the system relearns the new scenario without any human intervention (no thresholds or other settings) and accurately detects anomalous events.",https://ieeexplore.ieee.org/document/1227225/,"The 3rd International Workshop on Scientific Use of Submarine Cables and Related Technologies, 2003.",31-31 July 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISIMP.2004.1434017,Automatic e-movie creation of 3D animation and video retrieval,IEEE,Conferences,"This paper describes a software system called EMM (electronic moviemaker) designed to visualize the user's input screenplay words by a sound motion picture with the effects of real images, 3D animation, or their composition. A virtual director achieves the user's intentions by a knowledge-based (KB) approach through setting a scene, determining the corresponding shot types and shot sequence, and planning virtual camerawork dependent on the cinematic expertise stored in a KB domain, where real images are extracted from digital video by applying advanced content-based retrieval techniques. Animation generation is automated by interpreting textual screenplay into the TVML language, and the resultant movie is showed on a TVML player.",https://ieeexplore.ieee.org/document/1434017/,"Proceedings of 2004 International Symposium on Intelligent Multimedia, Video and Speech Processing, 2004.",20-22 Oct. 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RATFG.2001.938907,Automatic learning of appearance face models,IEEE,Conferences,"This paper describes a robust algorithm for automatically learning an appearance subspace of objects performing rigid motion through an image sequence, given a manual initialization of the regions of support (masks) in the first frame. The learning process is posed as a continuous optimization problem and it is solved with a mixture of stochastic and deterministic techniques achieving sub-pixel accuracy. Additionally, we learn the dynamics of the motion and appearance parameters for scene characterization and point out the benefits of working with modular eigenspaces. Preliminary results of automatic learning a modular eigenface model with applications to real time video conferencing, human computer interaction and actor animation are reported.",https://ieeexplore.ieee.org/document/938907/,"Proceedings IEEE ICCV Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems",13-13 July 2001,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FUZZY.1999.793110,Automatic text extraction in digital videos using FFT and neural network,IEEE,Conferences,"Popular methods for extracting a text region in video images are in general based on analysis of a whole image such as merge and split method, and comparison of two frames. Thus, they take long computing time due to the use of a whole image. Therefore, this paper suggests the faster method of extracting a text region without processing a whole image. The proposed method uses line sampling methods, FFT and neural networks in order to extract texts in real time. In general, text areas are found in the higher frequency domain, thus, can be characterized using FFT. The candidate text areas can be thus found by applying the higher frequency characteristics to neural network. Therefore, the final text area is extracted by verifying the candidate areas. Experimental results show a perfect candidate extraction rate and about 92% text extraction rate. The strength of the proposed algorithm is its simplicity, real-time processing by not processing the entire image, and fast skipping of the images that do not contain a text.",https://ieeexplore.ieee.org/document/793110/,FUZZ-IEEE'99. 1999 IEEE International Fuzzy Systems. Conference Proceedings (Cat. No.99CH36315),22-25 Aug. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NITC.2017.8285657,Automatic video descriptor for human action recognition,IEEE,Conferences,"Assistive software such as screen readers are unable to describe images or videos for visually impaired people. Although recent research have found ways to describe an image automatically, describing the content of a video is still an ongoing issue. Visually impaired people find it difficult to understand video content without an indication of sound. The current solution of video description is only provided through digital television and for selected programs and movies. As an initiative to describe video content for visually impaired people, the solution acts as a video player which automatically understands the ongoing human action on screen, associates textual descriptions and narrates it to the blind user. The human actions in the video should be recognized in real time, hence fast, reliable feature extraction and classification methods must be adopted. A feature set is extracted for each frame and is obtained from the projection histograms of the foreground mask. The number of moving pixels for each row and column of the frame is used to identify the instant position of a person. Support Vector Machine (SVM) is used to classify extracted features of each frame. The final classification is given by analyzing frames in segments. The classified actions will be converted from text to speech.",https://ieeexplore.ieee.org/document/8285657/,2017 National Information Technology Conference (NITC),14-15 Sept. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VTCSpring.2019.8746507,Autonomous Driving without a Burden: View from Outside with Elevated LiDAR,IEEE,Conferences,"The current autonomous driving architecture places a heavy burden in signal processing for the graphics processing units (GPUs) in the car. This directly translates into battery drain and lower energy efficiency, crucial factors in electric vehicles. This is due to the high bit rate of the captured video and other sensing inputs, mainly due to Light Detection and Ranging (LiDAR) sensor at the top of the car which is an essential feature in autonomous vehicles. LiDAR is needed to obtain a high precision map for the vehicle AI to make relevant decisions. However, this is still a quite restricted view from the car. This is the same even in the case of cars without a LiDAR such as Tesla. The existing LiDARs and the cameras have limited horizontal and vertical fields of visions. In all cases it can be argued that precision is lower, given the smaller map generated. This also results in the accumulation of a large amount of data in the order of several TBs in a day, the storage of which becomes challenging. If we are to reduce the effort for the processing units inside the car, we need to uplink the data to edge or an appropriately placed cloud. However, the required data rates in the order of several Gbps are difficult to be met even with the advent of 5G. Therefore, we propose to have a coordinated set of LiDAR's outside at an elevation which can provide an integrated view with a much larger field of vision (FoV) to a centralized decision making body which then sends the required control actions to the vehicles with a lower bit rate in the downlink and with the required latency. The calculations we have based on industry standard equipment from several manufacturers show that this is not just a concept but a feasible system which can be implemented.The proposed system can play a supportive role with existing autonomous vehicle architecture and it is easily applicable in an urban area.",https://ieeexplore.ieee.org/document/8746507/,2019 IEEE 89th Vehicular Technology Conference (VTC2019-Spring),28 April-1 May 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCAE.2010.5451523,Autonomous agent oriented traffic control system,IEEE,Conferences,"Emerging trends in software development has been changed due to the huge amount of data, growth of internet, mobile, dynamic and smart applications. Most of such applications consist of small, intelligent, flexible and distributed components known as agents. Number of agent methodologies has been presented but few of these are evaluated and verified. Due to the invention of agent technology, the way to analyze, design and build the systems has been changed. Agents take input from the multiple sources and have real time response. Vehicle traffic management especially in large cities is rapidly becoming one of the major challenges due to heavy growth in population and vehicles. Our research proposed a solution for traffic control and management system using intelligent/ autonomous agents technology. These agents have the ability to observe, act and learn from their experience. Our system uses the knowledge of flow of previous signal to predict the incoming flow for the next signal. The proposed architecture involves the video analysis and exploration using some machine learning techniques to estimate and guess the flow of traffic.",https://ieeexplore.ieee.org/document/5451523/,2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE),26-28 Feb. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONIITI51147.2020.9240296,Autonomous virtual vehicles with FNN-GA and Q-learning in a video game environment,IEEE,Conferences,"Driving autonomous virtual vehicles have been developed by the use of several artificial intelligence (AI) algorithms. However, it is possible to improve their results and find other applications through the use of new algorithms or combinations of them. Taking into account this, in this work, firstly Feed-forward Neural Network (FNN) and Genetic Algorithm (GA) were selected, and then, these algorithms were combined to have the new FNN-GA algorithm. In order to evaluate its performance, the most popular circuits, such as ovals and tracks with different types of curves, were selected from auto racing, which were implemented in the Unity 2D video game engine. After, as a way to compare the performance of the ""FNN-GA"" algorithm, the Q-learning algorithm was selected, and the agents to learn with them were placed on said tracks. As a result, after executing 100 generations of the agents, per algorithm, it is found that taking into account the better use of CPU and RAM resources, Q-learning is the best, on the contrary, taking into account the quantity of in learning generations, FNN-GA is the best since it learns in fewer generations. On the other hand, both algorithms learn easier on ovals than on circuits with different types of curves.",https://ieeexplore.ieee.org/document/9240296/,2020 Congreso Internacional de Innovación y Tendencias en Ingeniería (CONIITI),30 Sept.-2 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.162,Background Subtraction Enhancement using Segmentation,IEEE,Conferences,"The capability of extracting moving objects from a video sequence captured using a static camera is a typical first step in visual surveillance. This procedure is called a background subtraction (BGS), and it uses the temporal distribution of pixel values over the sequence of frames. Pixel based BGS can be improved by considering the spatial coherence around each pixel, and in this paper we present a method to enhance existing BGS methods using spatial information from image segmentation.",https://ieeexplore.ieee.org/document/4617428/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2007.4379761,Background Subtraction using Incremental Subspace Learning,IEEE,Conferences,"Background modeling and subtraction is a basic component of many computer vision and video analysis applications. It has a critical impact on the performance of object tracking and activity analysis. In this paper, we propose an effective and adaptive background modeling and subtraction approach that can deal with dynamic scenes. The proposed approach uses a subspace learning method to model the background and the subspace is updated on-line with a sequential Karhunen-Loeve algorithm. A linear prediction model is also used to make the detection more robust. Experimental results show that the proposed approach is able to model the background and detect moving objects under various type of background scenarios with close to real-time performance.",https://ieeexplore.ieee.org/document/4379761/,2007 IEEE International Conference on Image Processing,16 Sept.-19 Oct. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49033.2022.9700636,Balancing Latency and Accuracy on Deep Video Analytics at the Edge,IEEE,Conferences,"Real-time deep video analytic at the edge is an enabling technology for emerging applications, such as vulnerable road user detection for autonomous driving, which requires highly accurate results of model inference within a low latency. In this paper, we investigate the accuracy-latency trade-off in the design and implementation of real-time deep video analytic at the edge. Without loss of generality, we select the widely used YOLO-based object detection and WebRTC-based video streaming for case study. Here, the latency consists of both networking latency caused by video streaming and the processing latency for video encoding/decoding and model inference. We conduct extensive measurements to figure out how the dynamically changing settings of video streaming affect the achieved latency, the quality of video, and further the accuracy of model inference. Based on the findings, we propose a mechanism for adapting video streaming settings (i.e. bitrate, resolution) online to optimize the accuracy of video analytic within latency constraints. The mechanism has proved, through a simulated setup, to be efficient in searching the optimal settings.",https://ieeexplore.ieee.org/document/9700636/,2022 IEEE 19th Annual Consumer Communications & Networking Conference (CCNC),8-11 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIRCA51532.2021.9544607,Behavioural Analysis For Prospects In Crowd Emotion Sensing: A Survey,IEEE,Conferences,"Crowd behavioural analysis is an interesting and emerging domain in research, with incomplete set of activities, tasks and lack of intermediate cub-processes which are mandated for productive analysis. Since the domain is untapped to a major extent, the research carried out in the domain needs proper stages of operations. A proper taxonomy will direct the futuristic domains in the right track of processes and organization of intermediate tasks. This review paper intends to document the list of stages and processes, data collection, pipelining the sub-tasks, pre-emptive identification of supposed problems during the later stages in detection of crowd emotions and behavioural analysis. Deep learning techniques have been widely deployed to investigate the models of crowd analysis, anomaly detection, and look for meaningful insights and patterns from datasets. The Different models are investigated thoroughly for their respective understanding about the emotional aspects considered in the studies. Emotional characteristics when powered with crowd behavioural analysis and real world entities will deliver a promising solution for crime detections, anomaly detection and ensure a safer environment for nations. Video surveillance tools, datasets from crime datasets and various other factors contributed to the previous research works, models are now being designed to incorporate the best features of these models into one and thus achieve one fruitful model for continuous video analytics.",https://ieeexplore.ieee.org/document/9544607/,2021 Third International Conference on Inventive Research in Computing Applications (ICIRCA),2-4 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CoG47356.2020.9231696,Binary GPU-Planning for Thousands of NPCs,IEEE,Conferences,"Game Artificial Intelligence Engines of commercial games run on CPUs and not on GPUs. With more and more powerful GPUs and Cloud gaming, our vision is that GPUs will become the dedicated Game AI hardware, just as it now provides computing power for Game Physics (e.g. nVidia PhysX). In particular, we believe GPUs can run Game AI Planning, which computes plans in order to control the behaviors of Non-Player Characters (NPCs) in video-games. Our objective is an efficient online GPU-based Game AI Planning component with Cloud gaming as a target. We here report on our most recent implementation which can control thousands of NPCs each frame with only one GTX 1080 on the AI server, pushing thousands of plans to the client (PC or console).",https://ieeexplore.ieee.org/document/9231696/,2020 IEEE Conference on Games (CoG),24-27 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT51068.2021.9417957,Bird.s Eye: Analysis of Video Surveillance Data using Deep Learning,IEEE,Conferences,"Video surveillance data is widely used as a measure for security in various places like residential areas, shopping malls and so on. This video data is recorded 24x7 and therefore the task to find a particular object or event becomes tedious. Precious manual hours are wasted by security officials trying to skim through the video data trying to find the object they are searching for. This paper aims to reduce the manual effort required in this process by automating and focusing on the object the user is searching for. We have implemented multi-class object detection using YOLO algorithm and deep neural networks. The application will detect commonplace objects, classify them accordingly and extract the features of these objects. The user will be able to query the video data using the name of the object and the features. The paper is expected to detect objects of interest and extract basic features with reasonable accuracy and processing times through which it can be deployed in real-world surveillance applications.",https://ieeexplore.ieee.org/document/9417957/,2021 6th International Conference for Convergence in Technology (I2CT),2-4 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SBESC49506.2019.9046091,Brazilian Mercosur License Plate Detection: a Deep Learning Approach Relying on Synthetic Imagery,IEEE,Conferences,"Automated license plate recognition (ALPR) technology is a powerful technology enabling more efficient and effective law enforcement, security, payment collection, and research. A common license plate standard was adopted by the member states of the Mercosur trading bloc (Argentina, Brazil, Paraguay and Uruguay) and consequently requires an upgrade to the ALPR software used by law enforcement and industry. Due to the scarcity of real license plate images, training state-of-the-art supervised detectors is unfeasible unless data augmentation techniques and synthetic training data are used. This paper presents an accurate and efficient automated Mercosur license plate detector using a Convolutional Neural Network (CNN) trained exclusively with synthetic imagery. In order to obtain the synthetic training data, Mercosur license plates were faithfully reproduced. Digital image processing techniques were employed to reduce the domain gap and a CNN with basic image manipulation was used to embed the artificial licensed plates in to realistic contexts. The trained model was then validated on real images captured from a parking lot and a publicly available traffic monitoring video stream. The results of experiments suggest detection accuracy of about 95% and an average running time of 40 milliseconds.",https://ieeexplore.ieee.org/document/9046091/,2019 IX Brazilian Symposium on Computing Systems Engineering (SBESC),19-22 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM42981.2021.9488823,Bringing Fairness to Actor-Critic Reinforcement Learning for Network Utility Optimization,IEEE,Conferences,"Fairness is a crucial design objective in virtually all network optimization problems, where limited system resources are shared by multiple agents. Recently, reinforcement learning has been successfully applied to autonomous online decision making in many network design and optimization problems. However, most of them try to maximize the long-term (discounted) reward of all agents, without taking fairness into account. In this paper, we propose a family of algorithms that bring fairness to actor-critic reinforcement learning for optimizing general fairness utility functions. In particular, we present a novel method for adjusting the rewards in standard reinforcement learning by a multiplicative weight depending on both the shape of fairness utility and some statistics of past rewards. It is shown that for proper choice of the adjusted rewards, a policy gradient update converges to at least a stationary point of general αfairness utility optimization. It inspires the design of fairness optimization algorithms in actor-critic reinforcement learning. Evaluations show that the proposed algorithm can be easily deployed in real-world network optimization problems, such as wireless scheduling and video QoE optimization, and can significantly improve the fairness utility value over previous heuristics and learning algorithms.",https://ieeexplore.ieee.org/document/9488823/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2006.281655,Brush Footprint Acquisition and Preliminary Analysis for Chinese Calligraphy using a Robot Drawing Platform,IEEE,Conferences,"A robot drawing platform supporting four degrees of freedom (x, y, z and z-rotation) of a brush-pen motion for studying Chinese painting and calligraphy has been operational in our laboratory. This paper describes the real-time capturing and data analysis of the brush footprint using the new hardware and software capabilities in the platform. They include a transparent drawing plate and an underneath camera system, together with projective rectification and video segmentation algorithms. Preliminary result of the footprint analysis and nonparametric modeling, and their applications to well-known Chinese calligraphy are demonstrated",https://ieeexplore.ieee.org/document/4059247/,2006 IEEE/RSJ International Conference on Intelligent Robots and Systems,9-15 Oct. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635991,BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models,IEEE,Conferences,"Tracking the 6D pose of objects in video sequences is important for robot manipulation. Most prior efforts, however, often assume that the target object's CAD model, at least at a category-level, is available for offline training or during online template matching. This work proposes BundleTrack, a general framework for 6D pose tracking of novel objects, which does not depend upon 3D models, either at the instance or category-level. It leverages the complementary attributes of recent advances in deep learning for segmentation and robust feature extraction, as well as memory-augmented pose graph optimization for spatiotemporal consistency. This enables long-term, low-drift tracking under various challenging scenarios, including significant occlusions and object motions. Comprehensive experiments given two public benchmarks demonstrate that the proposed approach significantly outperforms state-of-art, category-level 6D tracking or dynamic SLAM methods. When compared against state-of-art methods that rely on an object instance CAD model, comparable performance is achieved, despite the proposed method’s reduced information requirements. An efficient implementation in CUDA provides a real-time performance of 10Hz for the entire framework. Code is available at: https://github.com/wenbowen123/BundleTrack",https://ieeexplore.ieee.org/document/9635991/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAFS.2018.8913367,CIS: An Automated Criminal Identification System,IEEE,Conferences,"The identification of criminals and terrorists is a primary task for police, military and security forces. The terrorist activities and crime rate had increased abnormally. Combating them is a challenging task for all security departments. Presently, these departments are using latest technologies. But they have not enough efficient and accuracy as they expected This research study is based on the analysis of faces, emotions, Ages and genders to identify the suspects. Face recognition, emotion, age and gender identifications are implemented using deep learning based CNN approaches. Suits identification is based on LeNet architecture. In the implementation phase for the classification purpose, Keras deep learning library is used, which is implemented on top of Tensorflow. IMDb is the dataset used for the whole training purpose. Training is performed using in AWS cloud which is more powerful and capable way of training instead of using local machines. Real-time Video and images are taken for the experiment. Results of the training and predictions are discussed below in brief.",https://ieeexplore.ieee.org/document/8913367/,2018 IEEE International Conference on Information and Automation for Sustainability (ICIAfS),21-22 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITAMEE50454.2020.9398474,CNN Transfer Learning of Shrimp Detection for Underwater Vision System,IEEE,Conferences,"In deep learning, convolutional neural network (CNN) mostly apply common overland images instead of underwater images classifiers. Even though there are few classifiers that have been introduced in marine and aquaculture application, there is still limited sources of the underwater images such as shrimp images. Generally, most conventional management systems in shrimp aquaculture implemented manual techniques that highly depend on human to observe shrimp conditions. One of the major problems of shrimp aquaculture is the challenge of recognizing underwater images, despite the characteristic atmosphere such as the murky and turbid water conditions. Many models of image classification have been introduced in order to solve the issue of early detection in shrimp and ponds problems. However, there are several limitations of the proposed methods such as semi-intelligence or fully wired systems. Therefore, an intelligence computational method and wireless system or internet of things (IoT)-based system is crucial in making sure a precision aquaculture farming. This study conducted a transfer learning model for CNN real time shrimp recognition. This study aims to accurately assess the performance of the developed CNN model by evaluating shrimp images based on intersection over union (IoU) between annotation and proposed models. The result shows the proposed model can successfully detect the shrimps with more than 95% accuracy. As a conclusion, the proposed model is able to detect the real time video recognition of underwater shrimp in ponds and is applicable in wireless farming.",https://ieeexplore.ieee.org/document/9398474/,"2020 1st International Conference on Information Technology, Advanced Mechanical and Electrical Engineering (ICITAMEE)",13-14 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2008.4588664,CNN implementation of a moving object segmentation approach for real-time video surveillance,IEEE,Conferences,In this paper we propose a CNN implementation of an algorithm for moving object segmentation intended for video surveillance applications. The approach is based on the comparison between the current frame and a background dynamically constructed from previous frames. The proposal includes capabilities to detect changes in the illumination conditions as well as to alert against abandoned objects in the control area. The algorithm is composed by simple convolutions and morphological operations as well as simple arithmetic and logic operations which allow the implementation on current focal-plane cellular processor arrays.,https://ieeexplore.ieee.org/document/4588664/,2008 11th International Workshop on Cellular Neural Networks and Their Applications,14-16 July 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2004.1328687,CNN-based real-time video detection of plasma instability in nuclear fusion applications,IEEE,Conferences,"In this paper a real-time detection of plasma instabilities, called MARFEs, is performed through a real-time image processing on plasma video sequences. These sequences are recorded by a vision system based on a CCD camera installed at Frascati Tokamak Upgrade (FTU). The strategy used to perform the task is based on a new family of nonlinear analog processors, digitally programmable, implemented into the so-called cellular neural network universal machine (CNN-UM). The detection system allows to carry out safer nuclear fusion experiments, preventing the plant from excessive mechanical and thermal stress which occurs during plasma instability phenomena (i.e. disruptions). Experimental results, obtained on the FTU machine, are fully satisfactory.",https://ieeexplore.ieee.org/document/1328687/,2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512),23-26 May 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PerComWorkshops48775.2020.9156225,COSMOS Smart Intersection: Edge Compute and Communications for Bird's Eye Object Tracking,IEEE,Conferences,"Smart-city intersections will play a crucial role in automated traffic management and improvement in pedestrian safety in cities of the future. They will (i) aggregate data from in-vehicle and infrastructure sensors; (ii) process the data by taking advantage of low-latency high-bandwidth communications, edge-cloud computing, and AI-based detection and tracking of objects; and (iii) provide intelligent feedback and input to control systems. The Cloud Enhanced Open Software Defined Mobile Wireless Testbed for City-Scale Deployment (COSMOS) enables research on technologies supporting smart cities. In this paper, we provide results of experiments using bird's eye cameras to detect and track vehicles and pedestrians from the COSMOS pilot site. We assess the capabilities for real-time computation, and detection and tracking accuracy - by evaluating and customizing a selection of video pre-processing and deep-learning algorithms. Distinct issues that are associated with the difference in scale for bird's eye view of pedestrians vs. cars are explored and addressed: the best multiple-object tracking accuracies (MOTA) for cars are around 73.2, and around 2.8 for pedestrians. The real-time goal of 30 frames-per-second - i.e., a total of 33.3 ms of latency for object detection for vehicles will be reachable once the processing time is improved roughly by a factor of three.",https://ieeexplore.ieee.org/document/9156225/,2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops),23-27 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MWSCAS48704.2020.9184456,CU Size Decision for Low Complexity HEVC Intra Coding based on Deep Reinforcement Learning,IEEE,Conferences,"High efficiency video coding (HEVC) uses a quadtree-based structure for coding unit (CU) splitting to effectively encode various video sequences with different visual characteristics. However, this new structure results in a dramatically increased complexity that makes real-time HEVC encoding very challenging. In this paper, we propose a novel CU size decision method based on deep reinforcement learning and active feature acquisition to reduce HEVC intra coding computational complexity and encoding time. The proposed method carries out early splitting and early splitting termination by considering the encoder and CU as an agent-environment system. More specifically, through early splitting, the proposed method precludes the need for rate-distortion optimization at the current level. In addition, through early splitting termination, it disposes of the lower level computations. The proposed method provides a very fast encoder with a small quality penalty. Experimental results show that it achieves a 51.3% encoding time reduction on average with a small quality loss of 0.041 dB for the BD-PSNR, when we compare our method to the HEVC test model.",https://ieeexplore.ieee.org/document/9184456/,2020 IEEE 63rd International Midwest Symposium on Circuits and Systems (MWSCAS),9-12 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS40897.2019.8967592,Can a Robot Become a Movie Director? Learning Artistic Principles for Aerial Cinematography,IEEE,Conferences,"Aerial filming is constantly gaining importance due to the recent advances in drone technology. It invites many intriguing, unsolved problems at the intersection of aesthetical and scientific challenges. In this work, we propose a deep reinforcement learning agent which supervises motion planning of a filming drone by making desirable shot mode selections based on aesthetical values of video shots. Unlike most of the current state-of-the-art approaches that require explicit guidance by a human expert, our drone learns how to make favorable viewpoint selections by experience. We propose a learning scheme that exploits aesthetical features of retrospective shots in order to extract a desirable policy for better prospective shots. We train our agent in realistic AirSim simulations using both a hand-crafted reward function as well as reward from direct human input. We then deploy the same agent on a real DJI M210 drone in order to test the generalization capability of our approach to real world conditions. To evaluate the success of our approach in the end, we conduct a comprehensive user study in which participants rate the shot quality of our methods. Videos of the system in action can be seen at https://youtu.be/qmVw6mfyEmw.",https://ieeexplore.ieee.org/document/8967592/,2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),3-8 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2008.60,Classification of Facial Expression Using SVM for Emotion Care Service System,IEEE,Conferences,"This paper presents a real-time approach to classify facial expression from a sequence of input images to provide emotion care service in developing a wellbeing life care system. The facial expression recognition from video images is useful to handle with sequential changes of facial expression. However, it needs more cost in training images and constructing database rather than using a still image. In this paper, we present automatic technique which infers emotions by recognizing facial expression from input video in real time. To classify the facial expression the feature displacements traced by the optical flow are used for input parameters to a support vector machine (SVM). The classification result of facial expression from input video will be used for providing personal emotion-care service depending on the emotional state.",https://ieeexplore.ieee.org/document/4617340/,"2008 Ninth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing",6-8 Aug. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2018.8489558,Classification of Mice Head Orientation Using Support Vector Machine and Histogram of Oriented Gradients Features,IEEE,Conferences,"The development of computational tools allows the implementation of new technologies, promoting an advance in Neuroscience research and raising the limits of experimental design. The tool proposed in this study is initially described with the acquisition of the images, then performing the animal tracking, extraction of characteristics, and finally the classification stage. Support Vector Machines are known as one of the most powerful supervised learning algorithms widely used for multiclass image classification. In this work, we propose the classification of mice head orientation using a support vector machine trained with Histogram of Oriented Gradients descriptors, extracted from behavioral video records. We analyze a dataset composed of 1800 images, using 80% of the images for training and 20% for tests. The classifier admits 8 outputs, represented by the trigonometric circle divided into 45 degree parts, allowing the classification of mice head orientation in 8 possible directions. The results show that the mean accuracy reached by the classifier is 99.44%, with confounding errors only in neighboring classes. The computational development was performed using the C++ programming language and the Open Source Computer Vision Library. In this way, considering the accuracy achieved by the classifier and its well-known low computational cost after training, it is possible with this tool to build from simple behavioral experiments of social interaction to the synchronization of the real-time response of the animal's head orientation with light stimuli needed in real-time optogenetic experiments.",https://ieeexplore.ieee.org/document/8489558/,2018 International Joint Conference on Neural Networks (IJCNN),8-13 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP42928.2021.9506200,Classification of RIGID and Non-Rigid Transformations with Autoencoder Representations,IEEE,Conferences,"Feature matching in transformed images is critical to many fields of computer science, from autonomous robots to video analysis. However, most widely used feature matching algorithms vary in their ability to track features depending on whether rigid or non-rigid image transformations occur. This makes it critical, especially in real-time calculations, to be able to identify what kind of transformation is taking place quickly in order to deploy the best feature matching algorithm for that type of transformation. The proposed research uses a combined autoencoder and neural network classification model to classify rigid or non-rigid transformations in order to improve feature matching on the image pairs. This system is the first to perform this kind of analysis with representation learning and opens new ways to improving feature matching performance. We show that using this method improves the amount of feature matches found between correctly identified image pairs.",https://ieeexplore.ieee.org/document/9506200/,2021 IEEE International Conference on Image Processing (ICIP),19-22 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMC.2018.8488080,Classification of physical exercise intensity by using facial expression analysis,IEEE,Conferences,"Facial expression analysis has a wide area of applications including health, psychology, sports etc. In this study, we explored different methods of automatic classification of exercise intensities using facial image processing of a subject performing exercise on a cycloergometer during an incremental standardized protocol. The method can be implemented in real time using facial video analysis. The experiments were done with images extracted from a 12 min HD video collected in laboratorial normalized settings (TechSport from the University of Trás-os-Montes e Alto Douro) with a static camera (90° angle with face and camera). The time slot for video to extract images for a particular class of exercise intensity is correspondence to the incremental heart rate. The facial expression recognition has been performed mainly in two steps: facial landmark detection and classification using the facial landmarks. Luxand application was used to detect 70 landmarks were detect using the adaptation of code available in Luxand application and we applied machine learning classification algorithms including discriminant analysis, KNN and SVM to classify the exercise intensities from the facial images. KNN algorithms presents up to 100% accuracy in classification into 2 and 3 classes. The distances between a lowermost landmark of the faces, which is indicated in landmark number 11 in the Luxand application, and the 26 landmarks around mouth were calculated and considered as features vector to train and test the classifier. Separate experiments were done for classification into two, three, and four classes and the accuracy of each algorithm was analyzed. From the overall results, classification into two and three classes was easy and resulted in very good classification performance whereas the classification with four classes had poor classification performance in each algorithm. Preliminary results suggest that distinguishing more levels of exertion, might require additional feature variables.",https://ieeexplore.ieee.org/document/8488080/,2018 Second International Conference on Computing Methodologies and Communication (ICCMC),15-16 Feb. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE46568.2020.9042997,Cliff-sensor-based Low-level Obstacle Detection for a Wheeled Robot in an Indoor Environment,IEEE,Conferences,"A ramp and uneven ground formed by a low-level obstacle – whose height is too low from the ground – often stalls a robot’s navigation in an indoor environment. Few-centimeter differences between a low-level obstacle and a low-level non-obstacle are very difficult to be precisely measured in a constant distance at a mobile robot. In this paper, a wheeled mobile robot thus makes physical contact onto a low-level object, in order to measure such subtle differences. We use one or more cliff sensors typically in place for a mobile robot in order to avoid a drop-off. A wheeled robot climbs over a low-level object, classifies an obstacle vs. a non-obstacle using a cliff sensor’s timeseries data, and rapidly backs up before getting stuck onto an obstacle. While adopting a simplified deep-learning architecture, we suggest a rapid and accurate obstacle detection technique in real-time. We implemented our technique on an embedded robot platform of LG Hom-Bot. The supplementary video on the physical robot experiment can be accessed at https://youtu.be/yK57S857_II.",https://ieeexplore.ieee.org/document/9042997/,2020 IEEE International Conference on Consumer Electronics (ICCE),4-6 Jan. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9561475,CloudAAE: Learning 6D Object Pose Regression with On-line Data Synthesis on Point Clouds,IEEE,Conferences,"It is often desired to train 6D pose estimation systems on synthetic data because manual annotation is expensive. However, due to the large domain gap between the synthetic and real images, synthesizing color images is expensive. In contrast, this domain gap is considerably smaller and easier to fill for depth information. In this work, we present a system that regresses 6D object pose from depth information represented by point clouds, and a lightweight data synthesis pipeline that creates synthetic point cloud segments for training. We use an augmented autoencoder (AAE) for learning a latent code that encodes 6D object pose information for pose regression. The data synthesis pipeline only requires texture-less 3D object models and desired viewpoints, and it is cheap in terms of both time and hardware storage. Our data synthesis process is up to three orders of magnitude faster than commonly applied approaches that render RGB image data. We show the effectiveness of our system on the LineMOD, LineMOD Occlusion, and YCB Video datasets. The implementation of our system is available at: https://github.com/GeeeG/CloudAAE.",https://ieeexplore.ieee.org/document/9561475/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDSE50459.2020.9310145,Cockpit Display Graphics Symbol Detection for Software Verification Using Deep Learning,IEEE,Conferences,"In Software Development Life-cycle, Verification and Validation plays a very important role, especially in the case of Safety-Critical Industries like Aerospace. Display dashboard consists of multiple static and dynamic objects having affine transformation, graphics overlap, shadows and less inter symbol discriminative features compared to natural images. Manual Software graphics verification is an error-prone and time-consuming activity. In this paper, we propose a novel software graphics verification pipeline to verify graphics symbols and alphanumeric objects as per Software requirements. To the best of our knowledge, our proposed approach is the first study on deep learning-based graphics symbol detection from complex synthetic background which requires high model accuracy. We experiment using Single-shot Multibox Detector (SSD) and You Only Look Once (YOLO v2) to detect different Graphical symbols from display simulator real-time captured video frames. These detected objects are further classified based on their nature. Objects containing alphanumeric digits can be recognized using Optical Character Recognition and dynamic symbols are detected using object detection to infer other properties. Finally, all the extracted properties can be compared with test expectations to verify their correctness. The result shows superior accuracy of the SSD algorithm over other state-of-the-art object detection algorithms for detecting real-time graphics symbols.",https://ieeexplore.ieee.org/document/9310145/,2020 International Conference on Data Science and Engineering (ICDSE),3-5 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM42981.2021.9488910,Cocktail Edge Caching: Ride Dynamic Trends of Content Popularity with Ensemble Learning,IEEE,Conferences,"Edge caching will play a critical role in facilitating the emerging content-rich applications. However, it faces many new challenges, in particular, the highly dynamic content popularity and the heterogeneous caching configurations. In this paper, we propose Cocktail Edge Caching, that tackles the dynamic popularity and heterogeneity through ensemble learning. Instead of trying to find a single dominating caching policy for all the caching scenarios, we employ an ensemble of constituent caching policies and adaptively select the best-performing policy to control the cache. Towards this goal, we first show through formal analysis and experiments that different variations of the LFU and LRU polices have complementary performance in different caching scenarios. We further develop a novel caching algorithm that enhances LFU/LRU with deep recurrent neural network (LSTM) based time-series analysis. Finally, we develop a deep reinforcement learning agent that adaptively combines base caching policies according to their virtual hit ratios on parallel virtual caches. Through extensive experiments driven by real content requests from two large video streaming platforms, we demonstrate that CEC not only consistently outperforms all single policies, but also improves the robustness of them. CEC can be well generalized to different caching scenarios with low computation overheads for deployment.",https://ieeexplore.ieee.org/document/9488910/,IEEE INFOCOM 2021 - IEEE Conference on Computer Communications,10-13 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.1995.480079,Codebook adaptation algorithm for a scene adaptive video coder,IEEE,Conferences,"Proposes a codebook adaptation algorithm for very low bit rate, real-time video coding. Although adaptive codebook design has been studied in the past, its implementation at very low coding rates suitable for the MPEG4 standard remains significantly challenging. The coder uses a standard motion compensated predictor with DCT quantization. It is unique in that it uses a hybrid scalar/vector quantizer to code predictor residuals. Bits are dynamically allocated to minimize distortion in the current frame, and scalar quantized blocks are used to adapt the VQ codebook. A codebook adaptation algorithm is described which uses an ""equidistortion principle"" and a competitive learning algorithm to continuously adapt the codewords. This training algorithm results in an increased use of the more efficient vector quantizer and improved video quality.",https://ieeexplore.ieee.org/document/480079/,"1995 International Conference on Acoustics, Speech, and Signal Processing",9-12 May 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.2015.7249303,Cognitive prediction of end-to-end bandwidth utilisation in a non-QoS video conference,IEEE,Conferences,"The most sought after bandwidth killer application on networks has been video conference. For a complex network, specifically based within an organization, implementing quality of service (QoS) is administratively not always feasible as the priorities are regulated. Predicting future network traffic in a non-QoS implemented network by using information about the source, destination and application can give preparatory time to make the network ready for unstable and random demands. The paper uses machine learning techniques to predict the bandwidth utilization of an end-to-end video conference session. Experimental results in this paper show that these features work well in detecting the bandwidth utilization. These experiments were done on a corpus of 24,000 video conference connections. The cognition is based on experimenting on features such as time of call, source, destination, call type, expected duration and cause codes. The result is based on combination of all these features which gave an accuracy of more than 78% on real traffic using two of the common classifiers - k-nearest neighbors and tree based classifier. Support vector machine (SVM) and Naive Bayes gave lower learning accuracy. Prediction results were also obtained by varying the combination of features to detect the predominating features in the cognition. It has been established that the bandwidth at which the connection is established is not entirely dependent on the source and destination but the other features also play a role in deciding the bandwidth of the connection. The prediction accuracy further increases if video calls are allowed only at discrete pre-designated bandwidth levels.",https://ieeexplore.ieee.org/document/7249303/,2015 IEEE International Conference on Communications (ICC),8-12 June 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2018.8679004,Comparative Analysis of E-Learning Platforms on The Market,IEEE,Conferences,"The E-Learning concept came gradually in the public eye, having an ascendant exponential evolution, directly dependent on the technologies and methodologies involved. Its history is not a long one, stretched over two decades, the term being introduced in 1999 by Jay Cross. At the end of the 1990s, the term LMS, Learning Management System, was introduced, and e-Learning applications are widely used, both in education and in the private environment, as a support for continuous personal development. The emergence and extensive use of applications or program packages (eg Microsoft Office), dedicated dynamic content websites (eg Youtube) as well as access to electronic resources (eg Email, chat) led to the construction of communities within the context of online learning, facilitating communication, access and transfer of resources. After 2010, the E-Learning implications in the education system are recognized and heavily exploited. Access to huge treasures and resources, unlimited by geographical boundaries, a diverse range of ways to customize teaching and assessment techniques, an independent competitive market, ongoing development, and quality deliverables tailored to needs or purposes are just a few of the features of the current E-Learning domain. Educational programs attach great importance to both content, appearance and organization, and to the interaction between the teacher and the learner. The learner is provided with a variety of media resources, from slides, books and bibliographic materials grouped in one place to the real-time, to video-demonstrative teaching content in real time complete the learning experience and give it a practical note. This paper aims to present the characteristics that define an E-Learning platform and how they are assimilated by the educational systems, as well as to perform a comparative analysis of the existing E-learning platforms on the market, referring to the statistics on the E-learning industry in the world. The conclusions of the paper will attempt to outline some perspectives in this field.",https://ieeexplore.ieee.org/document/8679004/,"2018 10th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",28-30 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCES45898.2019.9002551,Comparative Analysis of Texture Features and Deep Learning Method for Real-time Indoor Object Recognition,IEEE,Conferences,"Object recognition and classification are considered as major tasks in the field of computer vision. They are well suited for applications such as a real-time system for people counting, object recognition system for people with visual impairments, surveillance systems, etc. The deployment of computer vision, machine learning, and deep learning algorithms enable to recognize the objects from an image or video frame. This paper proposes a real-time system for indoor object recognition. Moreover, the proposed work mainly focuses on analyzing the performance of various texture features, machine learning classifiers and deep learning methodologies to recognize the objects in indoor areas. The proposed methodology is validated in a publically available indoor object dataset “MCindoor20000”. The dataset consists of three categories of objects including doors, stairs, and sign. Our developed deep learning model using transfer learning approach yielded 100 % accuracy and texture features such as LPQ and BSIF have yielded an accuracy of more than 98% with SVM and KNN classifiers.",https://ieeexplore.ieee.org/document/9002551/,2019 International Conference on Communication and Electronics Systems (ICCES),17-19 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETET.2010.106,Comparative Implementation of Colour Analysis Based Methods for Face Detection Algorithm,IEEE,Conferences,"Human Face detection plays important role in Video surveillance, face Recognition, Human computer Interface, Face search, Face data management etc. Face Region Usually forms inconsequential part of images. In this paper basically we are trying for face detection Using various color analysis base methods & compare that analysis on various parameters such as different background conditions, positions Scale & orientation in images from several photo collection & database. The proposed method better than other skin colour segmentation method such as Face detection in colour images using Adboost algorithm & High-speed Skin colour Segmentation for Real Time Human Tracking.",https://ieeexplore.ieee.org/document/5698315/,2010 3rd International Conference on Emerging Trends in Engineering and Technology,19-21 Nov. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMCEC46724.2019.8984067,Comparative Study on Obstacle Detection and Avoidance System by Using Real-Time Image Processing and Artificial Intelligence in Autonomous Wheelchair Application,IEEE,Conferences,"The research objective of this project conducts a comparative performance study between real time image processing and Artificial Intelligence object recognition in the application of obstacle avoidance system for an autonomous wheelchair. The proposed obstacle detection system is achieved through the application of camera sensor with the implementation of Artificial Intelligence techniques in image processing. A pre-trained Convolutional Neural Network model known as MobileNet SSD, and Deep Neural Network (DNN) module in OpenCV library (for live video streams) are utilized in developing the object recognition algorithm. An Arduino and a pair of DC brushed motors together with Smart Drive 40 motor drivers made up the motion control system that acts correspondingly to the inputs obtained from the obstacle detection system(s). Comparative researches on the performance of both obstacle detection algorithms were carried out experimentally. The considered performance is the success rate in obstacle avoidance and this performance was analyzed based on varying light intensity, motor speed, and wall detection. The results obtained indicate that Artificial Intelligence based obstacle detection algorithm performed better than real time image processing with vary light intensity environment. However, it has a relatively low performance with varying motor speed and wall detection when compared to the Image Processing based obstacle detection system.",https://ieeexplore.ieee.org/document/8984067/,"2019 IEEE 3rd Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)",11-13 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SITIS.2018.00022,Comparing CNNs and JPEG for Real-Time Multi-view Streaming in Tele-Immersive Scenarios,IEEE,Conferences,"Deep learning-based codecs for lossy image compression have recently managed to surpass traditional codecs like JPEG and JPEG 2000 in terms of rate-distortion trade-off. However, they generally utilize architectures with large numbers of stacked layers, often making their inference execution prohibitively slow for time-sensitive applications. In this work, we assess the suitability of such compression techniques in real-time video streaming, and, more specifically, next-generation interactive tele-presence applications, which impose stringent latency requirements. To that end, we compare a recently published work on image compression based on convolutional neural networks which achieves state-of-the-art compression ratio using a relatively lightweight architecture, against a CPU and a GPU implementation of JPEG, measuring compression ratios and timings. With these results, we run a simulation of a tele-immersion pipeline for various networking conditions and examine the performance of the compared codecs, calculating framerates and latencies for different codec/network combinations.",https://ieeexplore.ieee.org/document/8706250/,2018 14th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS),26-29 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ColCACI.2019.8781989,Comparing SVM and SSD for classification of vehicles and pedestrians for edge computing,IEEE,Conferences,"In the context of video processing, transmission to a remote server is not always possible nor suitable. Video processing on the edge could offer a solution. However, lower processing capacities constraint the number of techniques available for devices, in this work we report the performance of two techniques for classification from video on a minicomputer. The implementation of a real-time vehicle counting and classification system is evaluated through Support Vector Machine (SVM) and the Single Shot Detector Framework (SSD) in a minicomputer. The obtained results show that with a video resolution of 1280×720 pixels and using SVM, precision and recognition rates of 86 % and 94 % are obtained respectively, while with SSD 93 % and 67 % rates are reached with times of processing higher than SVM.",https://ieeexplore.ieee.org/document/8781989/,2019 IEEE Colombian Conference on Applications in Computational Intelligence (ColCACI),5-7 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDICON52576.2021.9691485,Computer Vision-based Social Distancing Surveillance with Automated Camera Calibration for Large-scale Deployment,IEEE,Conferences,"Social distancing has been suggested as one of the effective measures to break the chain of viral transmission in the ongoing COVID-19 pandemic. We herein describe a computer vision-based AI-assisted solution to aid compliance with social distancing norms. The solution consists of modules to detect and track people, and to identify distance violations. It provides the flexibility to choose between a tool-based mode requiring user input or a fully automated mode of camera calibration (devised in-house), making the latter suitable for large-scale deployments. We also outline a strategy to estimate the number of video feeds which can be supported in parallel for scalability. Finally, we discuss different metrics to assess the risk associated with social distancing violations, including the use of “violation clusters”, and how we can differentiate between transient or persistent violations. Our proposed solution performs satisfactorily under different test scenarios, processes video feed at real-time speed, as well as addresses data privacy regulations by blurring faces of detected people, making it ideal for deployments.",https://ieeexplore.ieee.org/document/9691485/,2021 IEEE 18th India Council International Conference (INDICON),19-21 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAwST.2013.6765413,Construction of intelligent intrusion detection system based on KINECT,IEEE,Conferences,"Surveillance cameras mainly capture the video footage of a person. It is not capable of identifying suspicious intruding people when it is dark. Person identification using gait features is an area investigated for long. But extracting gait features from video data is computationally complex, and real-time identification is difficult. In this work, we use KINECT developed by Microsoft to track people's mobility features and obtain the characteristic features to identify (classify) the person. KINECT can deliver many features useful to identify a person, including simple static features like height, hand-span, etc. to complex dynamic features like hand movement, leg movement, etc. In this work, we used such features for person identification and thereby intrusion detection. Actual data was collected from subjects and classified using Artificial Neural Network (ANN). A very low False-Acceptance-Rate (FAR) and False-Rejection-Rate (FRR) could be achieved.",https://ieeexplore.ieee.org/document/6765413/,2013 International Joint Conference on Awareness Science and Technology & Ubi-Media Computing (iCAST 2013 & UMEDIA 2013),2-4 Nov. 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR42600.2020.00075,Context-Aware and Scale-Insensitive Temporal Repetition Counting,IEEE,Conferences,"Temporal repetition counting aims to estimate the number of cycles of a given repetitive action. Existing deep learning methods assume repetitive actions are performed in a fixed time-scale, which is invalid for the complex repetitive actions in real life. In this paper, we tailor a context-aware and scale-insensitive framework, to tackle the challenges in repetition counting caused by the unknown and diverse cycle-lengths. Our approach combines two key insights: (1) Cycle lengths from different actions are unpredictable that require large-scale searching, but, once a coarse cycle length is determined, the variety between repetitions can be overcome by regression. (2) Determining the cycle length cannot only rely on a short fragment of video but a contextual understanding. The first point is implemented by a coarse-to-fine cycle refinement method. It avoids the heavy computation of exhaustively searching all the cycle lengths in the video, and, instead, it propagates the coarse prediction for further refinement in a hierarchical manner. We secondly propose a bidirectional cycle length estimation method for a context-aware prediction. It is a regression network that takes two consecutive coarse cycles as input, and predicts the locations of the previous and next repetitive cycles. To benefit the training and evaluation of temporal repetition counting area, we construct a new and largest benchmark, which contains 526 videos with diverse repetitive actions. Extensive experiments show that the proposed network trained on a single dataset outperforms state-of-the-art methods on several benchmarks, indicating that the proposed framework is general enough to capture repetition patterns across domains. Code and data are available in https://github.com/Xiaodomgdomg/Deep-Temporal-Repetition-Counting.",https://ieeexplore.ieee.org/document/9157068/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),13-19 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2009.81,Context-Based Appearance Descriptor for 3D Human Pose Estimation from Monocular Images,IEEE,Conferences,"In this paper we propose a novel appearance descriptor for 3D human pose estimation from monocular images using a learning-based technique. Our image-descriptor is based on the intermediate local appearance descriptors that we design to encapsulate local appearance context and to be resilient to noise. We encode the image by the histogram of such local appearance context descriptors computed in an image to obtain the final image-descriptor for pose estimation. We name the final image-descriptor the histogram of local appearance context (HLAC). We then use relevance vector machine (RVM) regression to learn the direct mapping between the proposed HLAC image-descriptor space and the 3D pose space. Given a test image, we first compute the HLAC descriptor and then input it to the trained regressor to obtain the final output pose in real time. We compared our approach with other methods using a synchronized video and 3D motion dataset. We compared our proposed HLAC image-descriptor with the Histogram of shape context and histogram of SIFT like descriptors. The evaluation results show that HLAC descriptor outperforms both of them in the context of 3D Human pose estimation.",https://ieeexplore.ieee.org/document/5384910/,2009 Digital Image Computing: Techniques and Applications,1-3 Dec. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAS.2019.8771601,Context-Preserving Filter Reorganization for VDSR-Based Super-resolution,IEEE,Conferences,"This paper presents a hardware design to process a CNN for single image super-resolution (SISR). Very deep convolutional network for image super-resolution (VDSR) is a promising algorithm for SISR but it is too complex to be implemented in hardware for commercial products. The proposed design aims to implement VDSR with relatively small hardware resources while minimizing a degradation of image quality. To this end, 1D reorganization of a convolution filter is proposed to reduce the number of multipliers. In addition, the 1D vertical filter is changed to reduce the internal SRAM to store the input feature map. For the implementation with a reasonable hardware cost, the numbers of layers and channels per layer, as well as the parameter resolution, are decreased without a significant reduction of image quality which is observed from simulation results. The 1D reorganization reduces the number of multiplies to 55.6% whereas the size reduction of 1D vertical filter halves the buffer size. As a result, the proposed design processes a full-HD video in real time with 8,143.5k gates and 333.1kB SRAM while the image quality is degraded by 1.06dB when compared with VDSR.",https://ieeexplore.ieee.org/document/8771601/,2019 IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS),18-20 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISORC49007.2020.00033,Context-based learning for autonomous vehicles,IEEE,Conferences,"This research seeks to prove that Deep Recurrent Q-Network (DRQN) approaches are great options for the control of autonomous vehicles. DRQN algorithms are widely used in video game competitions, but not many studies are available for their use in autonomous vehicles. In this paper, we present a context-based learning approach using DRQN for driverless vehicles. Our experiments demonstrate the effectiveness of using the DRQN algorithm over others.",https://ieeexplore.ieee.org/document/9112997/,2020 IEEE 23rd International Symposium on Real-Time Distributed Computing (ISORC),19-21 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SLT.2012.6424212,Context-dependent Deep Neural Networks for audio indexing of real-life data,IEEE,Conferences,"We apply Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, to the real-life problem of audio indexing of data across various sources. Recently, we had shown that on the Switchboard benchmark on speaker-independent transcription of phone calls, CD-DNN-HMMs with 7 hidden layers reduce the word error rate by as much as one-third, compared to discriminatively trained Gaussian-mixture HMMs, and by one-fourth if the GMM-HMM also uses fMPE features. This paper takes CD-DNN-HMM based recognition into a real-life deployment for audio indexing. We find that for our best speaker-independent CD-DNN-HMM, with 32k senones trained on 2000h of data, the one-fourth reduction does carry over to inhomogeneous field data (video podcasts and talks). Compared to a speaker-adaptive GMM system, the relative improvement is 18%, at very similar end-to-end runtime. In system building, we find that DNNs can benefit from a larger number of senones than the GMM-HMM; and that DNN likelihood evaluation is a sizeable runtime factor even in our wide-beam context of generating rich lattices: Cutting the model size by 60% reduces runtime by one-third at a 5% relative WER loss.",https://ieeexplore.ieee.org/document/6424212/,2012 IEEE Spoken Language Technology Workshop (SLT),2-5 Dec. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICCS48265.2020.9121163,Convolutional Neural Network based Automated Attendance System by using Facial Recognition Domain,IEEE,Conferences,"This project aims to recognize faces in an image, video, or via live camera using a deep learning-based Convolutional Neural Network model that is fast as well as accurate. Face recognition is a process of identifying faces in an image and has practical applications in a variety of domains, including information security, biometrics, access control, law enforcement, smart cards, and surveillance system. Deep Learning uses numerous layers to discover interpretations of data at different extraction levels. It has improved the landscape for performing research in facial recognition. The state-of-the-art implementation has been bettered by the introduction of deep learning in face recognition and has stimulated success in practical applications. Convolutional neural networks, a kind of deep neural network model has been proven to achieve success in the face recognition domain. For real-time systems, sampling must be done before using CNNs. On the other hand, complete images (all the pixel values) are passed as the input to Convolutional Neural Networks. The following steps: feature selection, feature extracti on, and training are performed in each step. This might lead to the assumption, where convolutional neural network implementation has a chance to get complicated and time-consuming.",https://ieeexplore.ieee.org/document/9121163/,2020 4th International Conference on Intelligent Computing and Control Systems (ICICCS),13-15 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW.2018.00228,Convolutional Neural Networks Based Ball Detection in Tennis Games,IEEE,Conferences,"In recent years sport video research has gained a steady interest among the scientific community. The large amount of video data available from broadcast transmissions and from dedicated camera setups, and the need of extracting meaningful information from data, pose significant research challenges. Hence, computer vision and machine learning are essential for enabling automated or semi-automated processing of big data in sports. Although sports are diverse enough to present unique challenges on their own, most of them share the need to identify active entities such as ball or players. In this paper, an innovative deep learning approach to the identification of the ball in tennis context is presented. The work exploits the potential of a convolutional neural network classifier to decide whether a ball is being observed in a single frame, overcoming the typical issues that can occur dealing with classical approaches on long video sequences (e.g. illumination changes and flickering issues). Experiments on real data confirm the validity of the proposed approach that achieves 98.77% accuracy and suggest its implementation and integration at a larger scale in more complex vision systems.",https://ieeexplore.ieee.org/document/8575391/,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),18-22 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCONSpring.2016.7519418,Convolutional neural network for vehicle detection in low resolution traffic videos,IEEE,Conferences,"Recent works on Convolutional Neural Network (CNN) in object detection and identification show its superior performance over other systems. It is being used on several machine vision tasks such as in face detection, OCR and traffic monitoring. These systems, however, use high resolution images which contain significant pattern information as compared to the typical cameras, such as for traffic monitoring, which are low resolution, thus, suffer low SNR. This work investigates the performance of CNN in detection and classification of vehicles using low quality traffic cameras. Results show an average accuracy equal to 94.72% is achieved by the system. An average of 51.28 ms execution time for a 2GHz CPU and 22.59 ms execution time for NVIDIA Fermi GPU are achieved making the system applicable to be implemented in real-time using 4-input traffic video with 6 fps.",https://ieeexplore.ieee.org/document/7519418/,2016 IEEE Region 10 Symposium (TENSYMP),9-11 May 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2011.5995481,Correspondence driven adaptation for human profile recognition,IEEE,Conferences,"Visual recognition systems for videos using statistical learning models often show degraded performance when being deployed to a real-world environment, primarily due to the fact that training data can hardly cover sufficient variations in reality. To alleviate this issue, we propose to utilize the object correspondences in successive frames as weak supervision to adapt visual recognition models, which is particularly suitable for human profile recognition. Specifically, we substantialize this new strategy on an advanced convolutional neural network (CNN) based system to estimate human gender, age, and race. We enforce the system to output consistent and stable results on face images from the same trajectories in videos by using incremental stochastic training. Our baseline system already achieves competitive performance on gender and age estimation as compared to the state-of-the-art algorithms on the FG-NET database. Further, on two new video datasets containing about 900 persons, the proposed supervision of correspondences improves the estimation accuracy by a large margin over the baseline.",https://ieeexplore.ieee.org/document/5995481/,CVPR 2011,20-25 June 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTSS46320.2019.00041,Cost-Aware Edge Resource Probing for Infrastructure-Free Edge Computing: From Optimal Stopping to Layered Learning,IEEE,Conferences,"To meet the stringent requirement of artificial intelligence applications, such as face recognition and video streaming analytics, a resource-constrained device can offload its task to nearby resource-rich devices in edge computing. Resource awareness, as a prime prerequisite for offloading decision-making, is critical for achieving efficient collaborative computation performance. In this paper, we consider cost-aware edge resource probing (CERP) framework design for infrastructure-free edge computing wherein a task device self-organizes its resource probing for informed computation offloading. We first propose a multi-stage optimal stopping formulation for the problem, and derive the optimal probing strategy which reveals a nice multi-threshold structure. Accordingly, we then devise a data-driven layered learning mechanism for more practical and complicated application environments. Layered learning enables the task device to adaptively learn the optimal probing sequence and decision thresholds at runtime, aiming at deriving a good balance between the gain of choosing the best edge device and the accumulated cost of deep resource probing. We further conduct thorough performance evaluation of the proposed CERP schemes using both extensive numerical simulations and realistic system prototype implementation, which demonstrate the superior performance of CERP in the diverse application scenarios.",https://ieeexplore.ieee.org/document/9052123/,2019 IEEE Real-Time Systems Symposium (RTSS),3-6 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1993.714133,Coupled neural networks for real-time road and obstacle recognition by intelligent road vehicles,IEEE,Conferences,"Individual neural networks have been developed for specific recognition tasks within a system to be used by an intelligent road vehicle for recognizing traffic situations in real time. A set of networks, designed to cooperate in recognizing the own lane and obstacles on the lane is described. A video camera is used as a sensor. Separate neural networks are employed for different levels of processing and knowledge representation. The algorithms have been implemented on a PC. Results of real-time tests are reported.",https://ieeexplore.ieee.org/document/714133/,"Proceedings of 1993 International Conference on Neural Networks (IJCNN-93-Nagoya, Japan)",25-29 Oct. 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2019.00127,Coverage-Guided Fuzzing for Feedforward Neural Networks,IEEE,Conferences,"Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc.",https://ieeexplore.ieee.org/document/8952279/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASIANCON51346.2021.9544683,Covid-19 Safety Protocol Monitoring System using AI,IEEE,Conferences,"The entire globe is going through a severe health crisis due to the rapid spread of Covid-19 Disease, which in turn creating a deep impact on the human lives and their day-to-day livelihood. Only hope of preventing it from further spread of disease is by following all the precautionary measures provided by the WHO. One of the most important safety measures is to wear face mask and maintain social distancing. Hence, we have proposed to develop a system can monitor whether a person is wearing a facemask correctly/not in real time. This will help to reduce the rapid spread of the disease in public places and various other organizations. We have proposed a solution that uses Artificial Intelligence and has a capability to detect the violation of wearing a face mask in real-time using Image Recognition and Video Techniques. The main intention of the proposed work is to provide the front-end software (Web or mobile application) for administrators to monitor the violation. The system will capture the violated instance and store the data. Our system will also include a User registration form where the users have to register their information along with their face images. This will be an input to the face recognition model, which will train and alert the user whenever violation occurs.",https://ieeexplore.ieee.org/document/9544683/,2021 Asian Conference on Innovation in Technology (ASIANCON),27-29 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSIP49896.2020.9339378,Critical Analysis of Edge Computing,IEEE,Conferences,"The edge computing is facing many problems and efforts are being made to overcome those challenges. Hybrid Mobile Edge Computing is introduced for the mobile devices to overcome the limited battery issues and performance constraints in the devices. There were some difficulties in the integrated development environment of the edge computing, and to overcome those problems, a container-based method is introduced which improves the performance of the coding environment in EC, and also it facilitates in-place debugging. An EC architecture is presented that provides local data processing, management, and quick reaction for the Virtual IoT Devices. With EC's support, a hybrid computing framework is built and an intelligent resource scheduling strategy to fulfill the real-time requirements in smart manufacturing; which showed satisfactory results. A Multi-source Transmission Protocol is presented to counter problems such as the low video streaming and high bandwidth usage. An ECD device is presented for the displaying of results in real-time, with short-time response and also to overcome the network limits. A decentralized resource management technique is introduced along with a technical framework for the latency-sensitive applications' deployment on the edge devices while protecting the privacy of devices. A Dynamic Edge-Fabric Environment is presented; this platform can automatically learn on the basis of past performance of the available resources using machine learning techniques and it decides which task should be best executed where, based on real-time system status and task requirements; the results have proven that it can improve overall performance for the selection of resources.",https://ieeexplore.ieee.org/document/9339378/,2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP),23-25 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISWC53511.2021.00017,Cross-Platform Performance Evaluation of Stateful Serverless Workflows,IEEE,Conferences,"Serverless computing, with its inherent event-driven design along with instantaneous scalability due to cloud-provider managed infrastructure, is starting to become a de-facto model for deploying latency critical user-interactive services. However, as much as they are suitable for event-driven services, their stateless nature is a major impediment for deploying long-running stateful applications. While commercial cloud providers offer a variety of solutions that club serverless functions along with intermediate storage to maintain application state, they are still far from optimized for deploying stateful applications at scale. More specifically, factors such as storage latency and scalability, network bandwidth, and deployment costs play a crucial role in determining whether current serverless applications are suitable for stateful workloads. In this paper, we evaluate the two widely-used stateful server-less offerings, Azure Durable functions and AWS Step functions, to quantify their effectiveness for implementing complex stateful workflows. We conduct a detailed measurement-driven characterization study with two real-world use cases, machine learning pipelines (inference and training) and video analytics, in order to characterize the different performance latency and cost tradeoffs. We observe from our experiments that AWS is suitable for workloads with higher degree of parallelism, while Azure durable entities offer a simplified framework that enables quicker application development. Overall, AWS is 89% more expensive than Azure for machine learning training application while Azure is 2× faster than AWS for the machine learning inference application. Our results indicate that Azure durable is extremely inefficient in implementing parallel processing. Furthermore, we summarize the key findings from our characterization, which we believe to be insightful for any cloud tenant who has the problem of choosing an appropriate cloud vendor and offering, when deploving stateful workloads on serverless platforms,",https://ieeexplore.ieee.org/document/9668304/,2021 IEEE International Symposium on Workload Characterization (IISWC),7-9 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBMI50038.2021.9461921,Crowd Violence Detection from Video Footage,IEEE,Conferences,"Surveillance systems currently deploy a variety of devices that can capture visual content (such as CCTV, body-worn cameras, and smartphone cameras), thus rendering the monitoring of video footage obtained from multiple such devices a complex task. This becomes especially challenging when monitoring social events that involve large crowds, particularly when there is a risk of crowd violence. This paper presents and demonstrates a crowd violence detection system that can process, analyze, and alert potential stakeholders, when violence-related content is identified in crowd-based video footage. Based on deep neural networks, the proposed end-to-end framework utilizes a 3D Convolutional Neural Network (CNN) to deal with the (near) real-time analysis of video streams and video files for crowd violence detection. The framework is trained, evaluated, and demonstrated using the Violent Flows dataset, a dataset related to crowd violence that is widely used for research. The presented framework is provided as a standalone application for desktop environments and can analyze both video streams and video files.",https://ieeexplore.ieee.org/document/9461921/,2021 International Conference on Content-Based Multimedia Indexing (CBMI),28-30 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2011.6084087,Cyber-physical systems based modeling of adaptation intelligence in network systems,IEEE,Conferences,"The paper uses the cyber-physical systems (CPS) framework to model the intelligent adaptation behaviors in complex network systems. The CPS framework is anchored on intelligent physical worlds (IPW) around which complex adaptation behaviors are built. An IPW is an embodiment of control software functions wrapped around the raw physical processes (such as routers, links, hosts, and protocols), performing the core communication activities while adapting its behavior to the changing network conditions and user inputs. The IPW exhibits an intelligent behavior over a limited operating region of the network system, which is in contrast with the traditional models where the physical world is basically dumb. To perform over a wider range of network operating conditions, the IPW interacts with an intelligent computational world (ICW) to patch itself with suitable control parameters and rules & procedures relevant in those changed conditions. The modular decomposition of a network application into IPW and ICW lowers the design complexity of network systems, and simplifies the system verification/testing. The paper illuminates our CPS-based approach with a case study of adaptive video transport over a bandwidth-limited network.",https://ieeexplore.ieee.org/document/6084087/,"2011 IEEE International Conference on Systems, Man, and Cybernetics",9-12 Oct. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMSP48831.2020.9287080,DEMI: Deep Video Quality Estimation Model using Perceptual Video Quality Dimensions,IEEE,Conferences,"Existing works in the field of quality assessment focus separately on gaming and non-gaming content. Along with the traditional modeling approaches, deep learning based approaches have been used to develop quality models, due to their high prediction accuracy. In this paper, we present a deep learning based quality estimation model considering both gaming and non-gaming videos. The model is developed in three phases. First, a convolutional neural network (CNN) is trained based on an objective metric which allows the CNN to learn video artifacts such as blurriness and blockiness. Next, the model is fine-tuned based on a small image quality dataset using blockiness and blurriness ratings. Finally, a Random Forest is used to pool frame-level predictions and temporal information of videos in order to predict the overall video quality. The light-weight, low complexity nature of the model makes it suitable for real-time applications considering both gaming and non-gaming content while achieving similar performance to existing state-of-the-art model NDNetGaming. The model implementation for testing is available on GitHub1.",https://ieeexplore.ieee.org/document/9287080/,2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP),21-24 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITME53901.2021.00029,DF-VLAD: Deepfake Video Detection based on Feature Aggregation,IEEE,Conferences,"With the rapid development of Deepfake technology, face video forgery can produce highly deceptive video content and bring serious security threats. The detection of this kind of fake video is more urgent and challenging. Most of the existing detection methods regard this problem as a common binary classification problem, and using a simple average or maximum as the prediction of video results can easily lead to missed detection or false detection. While the video-based detection work such as LSTM, in Deepfake detection, too much focus on timing modeling will affect the performance of Deepfake video detection to a certain extent. Based on this, this paper proposes a VLAD-based aggregation module DF-VLAD, which advances the aggregation of multiple frames from the output layer to the feature layer, which on the one hand makes the aggregation more flexible, on the other hand, it also uses the objective function of forgery detection to directly guide the learning of frame-level depth representation; On the other hand, this paper deals with this problem as a special fine-grained classification problem, because the difference between fake face and real face is very subtle. It is found that the existing face forgery methods such as Face2Face and NeuralTextures leave some common artifacts in the spatial domain. Different forgery methods produce different artifacts, while natural faces have more similar features. To make the model pay more attention to artifacts, a forgery trace capture model based on the fusion of self-attention mechanism and channel attention mechanism is proposed in this paper. Like other fine-grained classification methods, note intentions are used to guide the network to pay attention to key parts of the face. Experimental results on different public data sets show that the proposed method achieves the latest performance.",https://ieeexplore.ieee.org/document/9750563/,2021 11th International Conference on Information Technology in Medicine and Education (ITME),19-21 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WAMICON53991.2022.9786207,DNN-based Denial of Quality of Service Attack on Software-defined Hybrid Edge-Cloud Systems,IEEE,Conferences,"In order to satisfy diverse quality-of-service (QoS) requirements of complex real-time video applications, civilian and tactical use cases are employing software-defined hybrid edgecloud systems. One of the primary QoS requirements of such applications is ultra-low end-to-end latency for video applications that necessitates rapid frame transfer between end-devices and edge servers using software-defined networking (SDN). Failing to guarantee such strict requirements leads to quality degradation of video applications and subsequently mission failure. In this paper, we show how a collaborative group of attackers can exploit SDN&#x2019;s control communications to launch Denial of Quality of Service (DQoS) attack that artificially increases end-to-end latency of video frames and yet evades detection. In particular, we show how Deep Neural Network (DNN) model training on all or partial network state information can help predict network packet drop rates with reasonable accuracy. We also show how such predictions can help design an attack model that can inflict just the right amount of added latency to the end-to-end video processing that is enough to cause considerable QoS degradation but not too much to raise suspicion. We use a realistic edgecloud testbed on GENI platform for training data collection and demonstration of high model accuracy and attack success rate.",https://ieeexplore.ieee.org/document/9786207/,2022 IEEE 22nd Annual Wireless and Microwave Technology Conference (WAMICON),27-28 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3240765.3240801,DNNBuilder: an Automated Tool for Building High-Performance DNN Hardware Accelerators for FPGAs,IEEE,Conferences,"Building a high-performance FPGA accelerator for Deep Neural Networks (DNNs) often requires RTL programming, hardware verification, and precise resource allocation, all of which can be time-consuming and challenging to perform even for seasoned FPGA developers. To bridge the gap between fast DNN construction in software (e.g., Caffe, TensorFlow) and slow hardware implementation, we propose DNNBuilder for building high-performance DNN hardware accelerators on FPGAs automatically. Novel techniques are developed to meet the throughput and latency requirements for both cloud- and edge-devices. A number of novel techniques including high-quality RTL neural network components, a fine-grained layer-based pipeline architecture, and a column-based cache scheme are developed to boost throughput, reduce latency, and save FPGA on-chip memory. To address the limited resource challenge, we design an automatic design space exploration tool to generate optimized parallelism guidelines by considering external memory access bandwidth, data reuse behaviors, FPGA resource availability, and DNN complexity. DNNBuilder is demonstrated on four DNNs (Alexnet, ZF, VGG16, and YOLO) on two FPGAs (XC7Z045 and KU115) corresponding to the edge- and cloud-computing, respectively. The fine-grained layer-based pipeline architecture and the column-based cache scheme contribute to 7.7x and 43x reduction of the latency and BRAM utilization compared to conventional designs. We achieve the best performance (up to 5.15x faster) and efficiency (up to 5.88x more efficient) compared to published FPGA-based classification-oriented DNN accelerators for both edge and cloud computing cases. We reach 4218 GOPS for running object detection DNN which is the highest throughput reported to the best of our knowledge. DNNBuilder can provide millisecond-scale real-time performance for processing HD video input and deliver higher efficiency (up to 4.35x) than the GPU-based solutions.",https://ieeexplore.ieee.org/document/8587697/,2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),5-8 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS45731.2020.9181225,DSCR-Net: A Diffractive Sensing and Complex-Valued Reconstruction Network for Compressive Sensing,IEEE,Conferences,"Recently, deep learning based compressive sensing (CS) methods show superior reconstruction performance. However, these approaches are restricted for compressive video sampling in practical design of imaging devices. In this paper, we propose a novel deep network-based CS framework for efficient optical implementation. The proposed framework, dubbed DSCR-Net, consists of a diffractive sensing network employing light diffraction for efficient sampling and a complex-valued neural network for reconstruction, respectively. The diffractive sensing network can achieve real-time sampling at the speed of light. Furthermore, complex-valued neural network is developed to facilitate the reconstruction quality from the complex-valued measurements by jointly considering their real and imaginary parts. Extensive experiments demonstrate that DSCR-Net outperforms the state-of-the-art CS methods in the low sampling-rate region with a potential of immediate implementation of imaging device.",https://ieeexplore.ieee.org/document/9181225/,2020 IEEE International Symposium on Circuits and Systems (ISCAS),12-14 Oct 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMV53313.2021.9670992,Dangerous Object Detection for Visually Impaired People using Computer Vision,IEEE,Conferences,"In this contemporary world, Artificial Intelligence and Machine Learning are one of the leading technologies creating an impact in the world by mimicking human behaviour to solve a particular problem. Hence, these technologies are widely used to aid different obstacles encountered by humans. One such problem widely faced by the mankind is visual impairment. According to World Health Organization, approximately 285 million people suffer with vision impairment. Therefore, applications of machine learning and computer vision can be applied to guide the people with such problems. This paper presents the idea of using object detection to aid the visually impaired people. In this paper, an experiment has been proposed which uses a custom-built image dataset of various dangerous objects. The objects have been categorized into 5 broad categories: Sharp objects, Danger signs, Broken glass, Manhole and Fires. A number of different algorithms have been trained on this custom image dataset containing the menacing objects and their performances have been evaluated. The evaluation indicators for the models are the validation error in terms mean Average Precision (mAP) and the processing time for each model. The models have also been tested in real world scenario by evaluating on a custom video to gauge their performance in terms of accuracy in detection of different objects as well as their ease in deployment by suggesting their frame rate handling capacity. The results are discussed and the most robust and balanced model is suggested at the end of the paper.",https://ieeexplore.ieee.org/document/9670992/,2021 International Conference on Artificial Intelligence and Machine Vision (AIMV),24-26 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR50618.2020.00020,DatAR: An Immersive Literature Exploration Environment for Neuroscientists,IEEE,Conferences,"Maintaining an overview of publications in the neuroscientific field is challenging, especially with an eye to finding relations at scale; for example, between brain regions and diseases. This is true for well-studied as well as nascent relationships. To support neuroscientists in this challenge, we developed an Immersive Analytics (IA) prototype for the analysis of relationships in large collections of scientific papers. In our video demonstration we showcase the system's design and capabilities using a walkthrough and mock user scenario. This companion paper relates our prototype to previous IA work and offers implementation details.",https://ieeexplore.ieee.org/document/9319103/,2020 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),14-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCN.2019.8847129,Data-Centric Video for Mixed Reality,IEEE,Conferences,"Network video streaming abstractions tend to replicate the paradigms of hardwired video dating back to analog broadcast. With IP video distribution becoming increasingly realistic for a variety of low-latency applications, this paper looks ahead to a data-centric architecture for video that can provide a superset of features from existing abstractions, to support how video is increasingly being used: for non-linear retrieval, variable speed and spatially selective playback, machine analysis, and other new approaches. As a case study, the paper describes the use of the Named Data Networking (NDN) network architecture within an experimental theatrical work being developed at UCLA. The work, a new play, Entropy Bound, uses NDN to enable a hybrid design paradigm for real-time video that combines properties of streams, buses, and stores. This approach unifies real-time live and historical playback, and is used to support edge-assisted machine learning. The paper introduces the play and its requirements (as well as the NDN components applied and developed), discusses key design patterns enabled and explored and their influence on the application architecture, and describes what was learned through practical implementation in a realworld production setting. The paper intends to inform future experimentation with real-time media over information-centric networking and elaborate on the benefits and challenges of using NDN in practice for mixed reality applications today.",https://ieeexplore.ieee.org/document/8847129/,2019 28th International Conference on Computer Communication and Networks (ICCCN),29 July-1 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECS46596.2019.8964645,Data-Driven Video Grasping Classification for Low-Power Embedded System,IEEE,Conferences,"Video-based hand grasp analysis can support both robotics and prosthetics. Indeed, computational aspects represent a major issue, as hand grasp analysis is expected to support grasping systems that are hosted on low-power embedded systems. This paper proposes a framework for video-based grasping classification that is designed for implementation on resource-constrained devices. The framework adopts a fully data-driven strategy and relies on deep learning to deal with advanced analysis of video signals. Nonetheless, the overall design takes advantage of CNN architectures that can cope with the constraints imposed by embedded systems. The experimental session involved a real-world dataset containing daily life activities collected using egocentric perspective. In addition, the complete inference system is implemented on a NVIDIA Jetson-TX2 obtaining real time performances. The results confirm that the proposed system can suitably balance the trade off between accuracy and computational costs.",https://ieeexplore.ieee.org/document/8964645/,"2019 26th IEEE International Conference on Electronics, Circuits and Systems (ICECS)",27-29 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2018.00614,DeLS-3D: Deep Localization and Segmentation with a 3D Semantic Map,IEEE,Conferences,"For applications such as augmented reality, autonomous driving, self-localization/camera pose estimation and scene parsing are crucial technologies. In this paper, we propose a unified framework to tackle these two problems simultaneously. The uniqueness of our design is a sensor fusion scheme which integrates camera videos, motion sensors (GPS/IMU), and a 3D semantic map in order to achieve robustness and efficiency of the system. Specifically, we first have an initial coarse camera pose obtained from consumer-grade GPS/IMU, based on which a label map can be rendered from the 3D semantic map. Then, the rendered label map and the RGB image are jointly fed into a pose CNN, yielding a corrected camera pose. In addition, to incorporate temporal information, a multi-layer recurrent neural network (RNN) is further deployed improve the pose accuracy. Finally, based on the pose from RNN, we render a new label map, which is fed together with the RGB image into a segment CNN which produces perpixel semantic label. In order to validate our approach, we build a dataset with registered 3D point clouds and video camera images. Both the point clouds and the images are semantically-labeled. Each video frame has ground truth pose from highly accurate motion sensors. We show that practically, pose estimation solely relying on images like PoseNet [25] may fail due to street view confusion, and it is important to fuse multiple sensors. Finally, various ablation studies are performed, which demonstrate the effectiveness of the proposed system. In particular, we show that scene parsing and pose estimation are mutually beneficial to achieve a more robust and accurate system.",https://ieeexplore.ieee.org/document/8578712/,2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,18-23 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVEE54186.2021.9649670,Deep Learning Application of Automated Facemask Classification and Physical-Distancing Detection,IEEE,Conferences,"COVID-19 adversity has stopped the global economy and most of the countries in the entire world are still in locked down even though there is now a vaccine available to fight the COVID-19 diseases. Many nations including the Philippines proclaimed national emergencies to combat the further spread and to reduce the infections brought about by the COVID-19 virus. People are now getting vaccinated with the COVID-19 yet everyone is still in doubt when this catastrophe will end because the virus has evolved in many variants which is more infectious and deadly compared with the original virus. To fight this disease, health authorities implements wearing of facemask, avoiding crowds, cleaning and washing of hands often and staying at least 1-meter apart. This study aims to detect and classify facemask according to the types of facemask they are wearing and detects the physical distancing observe by the person in the area. Deep Learning is a family of Artificial Intelligence and Machine Learning that emulates the human brain in processing information and creates patterns to be use for decision-making. Dataset uses 4,000 images of the person wearing a facemask and not wearing a facemask and the type of facemask they are wearing including the surgical mask, N95 mask, and cloth mask. A real-time video is used to analyze the observation of physical distancing. The images were trained using the MobileNet model and recorded an accuracy rate of 94.50% during training. The trained model has effectively classified persons wearing a cloth mask, surgical mask, N95 mask, person not wearing a facemask, and detected the persons observing the physical distancing protocol. The study can be implemented in real-time to prevent the spread of COVID-19 by identifying the persons wearing the mask, classify the type of mask they are wearing, and detecting the physical distancing protocols.",https://ieeexplore.ieee.org/document/9649670/,2021 Fourth International Conference on Vocational Education and Electrical Engineering (ICVEE),2-3 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CICN49253.2020.9242584,Deep Learning Applied to Capacity Control in Commercial Establishments in Times of COVID-19,IEEE,Conferences,"This research paper was developed to implement an intelligent solution for This research paper was developed to implement an intelligent solution for the control of the capacity of commercial establishments in times of COVID-19 using Yolo, which is a Convolutional Neural Network and a Deep Learning algorithm. For the application of this solution, a COCO dataset was used that is used in the implementation of Yolov4. A computer module was developed for the analysis of the flow of people, using Python 3.7, which mainly consists of an algorithm that determines the path and direction (movement) of a person, and this is evaluated in a limit o threshold that acts as the entrance and exit door of the main establishment; that is, it determines whether a person leaves or enters according to their route and direction. The results indicate that it is possible to implement this solution as an additional monitoring module for use as capacity control and with this offer a complete alternative to the owners of commercial establishments. In this way, it seeks to control the maximum capacity allowed due to the pandemic generated by the Sars-Cov.2 virus. The tests were conducted using an AMD Ryzen 7 3750H processor and an NVIDIA GTX 1660 TI video card. The possibility of determining whether the number of people who entered less than the number of people who left exceeds the maximum allowed by the pandemic on 50% of the real capacity.",https://ieeexplore.ieee.org/document/9242584/,2020 12th International Conference on Computational Intelligence and Communication Networks (CICN),25-26 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSASE51777.2022.9759644,Deep Learning Approach for Real-time Video Streaming Traffic Classification,IEEE,Conferences,"Video streaming services such as Amazon Prime Video, Netflix and YouTube continue to be of enormous demand in everyday peoples&#x0027; lives. This enticed research into new mechanisms to provide a clear image of network usage and ensure better Quality of Service (QoS) for these applications. This paper proposes an accurate video streaming traffic classification model based on deep learning (DL). We first collected a set of video traffic data from a real network. Then, data was pre-processed to select the desired features for video traffic classification. Based on the performance evaluation, the model produces an overall accuracy of 99.3&#x0025; when classifying video streaming traffic using a multi-layer feedforward neural network. This paper also evaluates the DL approach&#x0027;s effectiveness compared to the Gaussian Naive Bayes algorithm (GNB), one of the most well-known machine learning techniques used in Internet traffic classification. The model is promising to be applied in a real-time scenario as it showed its ability to predict new unseen data with 98.4 &#x0025; overall accuracy.",https://ieeexplore.ieee.org/document/9759644/,2022 International Conference on Computer Science and Software Engineering (CSASE),15-17 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAICTA53211.2021.9640274,Deep Learning Based Spatiotemporal Human Action Recognition and Localization System,IEEE,Conferences,"Spatiotemporal human action localization system is a field in computer vision and is of interest for real-world applications implemented in smart surveillance cameras, such as to improve public security, monitor patients' activities, or even detect any early symptoms of certain diseases. The system presented in this thesis followed the YOWO machine learning architecture reference, which was proposed by Köpüklü etc. (2019). YOWO extracts both spatial and temporal information. Bounding box regression and action classification can be done end-to-end. This aims to generate output faster compare to other state-of-the-art approaches. The implementation of this system is trained and tested with J-HMDB and NTU RGB+D datasets. Using certain specifications of machine defined, the system is just able to process video at 0.75seconds per frame with an accepted accuracy value. However, the system succeeds in increasing the human action localization accuracy from the YOWO reference with an accuracy of 41.6% to 43.84%.The result of the experiments shows that the modified architecture is able to improve the accuracy of YOWO. However, it slows down the frame rate of video processing.",https://ieeexplore.ieee.org/document/9640274/,"2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)",29-30 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ51579.2020.9290640,Deep Learning Methods for Human Behavior Recognition,IEEE,Conferences,"In this paper, we investigate the problem of human behavior recognition by using the state-of-the-art deep learning methods. In order to achieve sufficient recognition accuracy, both spatial and temporal information was acquired to implement the recognition in this project. We propose a novel YOLOv4 + LSTM network, which yields promising results for real-time recognition. For the purpose of comparisons, we implement Selective Kernel Network (SKNet) with attention mechanism. The key contributions of this paper are: (1) YOLOv4 + LSTM network is implemented to achieve 97.87% accuracy based on our own dataset by using spatiotemporal information from pre-recorded video footages. (2) The SKNet with attention model that earns the best accuracy of human behaviour recognition at the rate up to 98.7% based on multiple public datasets.",https://ieeexplore.ieee.org/document/9290640/,2020 35th International Conference on Image and Vision Computing New Zealand (IVCNZ),25-27 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8852256,Deep Learning and One-class SVM based Anomalous Crowd Detection,IEEE,Conferences,"Anomalous event detection in videos is an important and challenging task. This paper proposes a deep representation approach to the problem, which extracts and represents features in an unsupervised way. This algorithm can detect anomalous activity like standing statically and loitering among a crowd of people. Our proposed framework is a two-channel scheme by using feature channels extracted from the appearance and foreground of the original video. Two hybrid deep learning architectures SDAE-DBN-PSVM (a four-layer Stacked Denoising Auto-encoder with three-layer Deep Belief Nets and Plane-based one class SVM) are implemented for these two channels to learn the high-level feature representation automatically and produce two anomaly scores. Finally, a fusion scheme is proposed for combining anomaly scores and detecting anomalous events. Experimental results on a large real-world dataset (MCG) and two benchmark datasets (UCSD and Subway) demonstrate the effectiveness of this approach. Furthermore, quantitative analyses of the effects of the amount of training data and the illumination conditions of the video on the accuracy of anomaly detection are presented.",https://ieeexplore.ieee.org/document/8852256/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00138,Deep Learning based Customer Count/Flow Monitoring System for Social Distancing,IEEE,Conferences,"Despite the COVID-19 vaccination drives, use of preventative measures such as masks and social distancing are still deemed essential. This paper presents an application that will allow businesses/enterprises to monitor the flow of customers by detecting people as objects, counting the number of people, tracking the safe distance between them to maintain the two-meter distance norm. The proposed solution is set up to generate an alarm when the customers reach the allowed limit as per shop dimensions or overcrowding is detected. For the implementation, YOLOv4 and YOLOv3-Tiny were used for the task of object detection and transfer learning is used to set up weights. The models were evaluated using MSCOCO API with 100 image instances per class. The results of the YOLOv4 model are also compared with YOLOv3-Tiny in terms of calculating mean, average precision (AP), frames per second (FPS), and identification of groups (crowd). Experimental results (on several video clips from a shopping center CCTV) show that the YOLOv3-Tiny maintains real-time performance even on modest hardware. It is further demonstrated that if a high-end GPU is available, the overall detection of objects and cluster identification is much more accurate and clearer using YOLOv4.",https://ieeexplore.ieee.org/document/9730405/,"2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech)",25-28 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMR48346.2021.9661529,Deep Learning for Needle Detection in a Cannulation Simulator,IEEE,Conferences,"Cannulation for hemodialysis is the act of inserting a needle into a surgically created vascular access (e.g., an arteriovenous fistula) for the purpose of dialysis. The main risk associated with cannulation is infiltration, the puncture of the wall of the vascular access after entry, which can cause medical complications. Simulator-based training allows clinicians to gain cannulation experience without putting patients at risk. In this paper, we propose to use deep-learning-based techniques for detecting, based on video, whether the needle tip is in or has infiltrated the simulated fistula. Three categories of deep neural networks are investigated in this work: modified pre-trained models based on VGG-16 and ResNet-50, light convolutional neural networks (light CNNs), and convolutional recurrent neural networks (CRNNs). CRNNs consist of convolutional layers and a long short-term memory (LSTM) layer. A data set of cannulation experiments was collected and analyzed. The results show that both the light CNN (test accuracy: 0.983) and the CRNN (test accuracy: 0.983) achieve better performance than the pre-trained baseline models (test accuracy 0.968 for modified VGG-16 and 0.971 for modified ResNet-50). The CRNN was implemented in real time on commodity hardware for use in the cannulation simulator, and the performance was verified. Deep-learning video analysis is a viable method for detecting needle state in a low cost cannulation simulator. Our data sets and code are released at https://github.com/axin233/DL_for_Needle_Detection_Cannulation.",https://ieeexplore.ieee.org/document/9661529/,2021 International Symposium on Medical Robotics (ISMR),17-19 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC48688.2020.00-84,Deep Learning for Visual Segmentation: A Review,IEEE,Conferences,"Big data-driven deep learning methods have been widely used in image or video segmentation. The main challenge is that a large amount of labeled data is required in training deep learning models, which is important in real-world applications. To the best of our knowledge, there exist few researches in the deep learning-based visual segmentation. To this end, this paper summarizes the algorithms and current situation of image or video segmentation technologies based on deep learning and point out the future trends. The characteristics of segmentation that based on semi-supervised or unsupervised learning, all of the recent novel methods are summarized in this paper. The principle, advantages and disadvantages of each algorithms are also compared and analyzed.",https://ieeexplore.ieee.org/document/9202594/,"2020 IEEE 44th Annual Computers, Software, and Applications Conference (COMPSAC)",13-17 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE.2018.8326285,Deep learning in low-power stereo vision accelerator for automotive,IEEE,Conferences,"Various forms of Convolutional Neural Network (CNN) architectures are used as Deep Learning (DL) tools for learning the similarity measure on video patches in order to run the stereo matching algorithm - the most computationally intensive stage of the pipeline for the stereo vision function used in designing an autonomous car. We propose a hybrid system implementation of the algorithm for real-time, low-power and high-temperature environment. The accelerator part of our system is a programmable many-core system with a Map-Reduce Architecture. The paper describes and evaluates the proposed accelerator for different versions of the stereo matching algorithm.",https://ieeexplore.ieee.org/document/8326285/,2018 IEEE International Conference on Consumer Electronics (ICCE),12-14 Jan. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RTCSA.2018.00010,DeepCounter: Using Deep Learning to Count Garbage Bags,IEEE,Conferences,"This paper proposes DeepCounter, an automotive sensing system where deep learning based image processing technology is used to automatically count the number of collected garbage bags from the video taken by a camera mounted on the rear of a garbage truck in order to sense a fine-grain spatio-temporal distribution on the amount of disposed garbage in cities that is envisioned to be helpful to develop novel applications related to garbage collection there. A prototype system is implemented on a GPU-integrated signal-board computer. A detection-tracking-counting (DTC) algorithm is developed and implemented based on the single shot multibox detector (SSD), a well-known real-time object detection algorithm. Experimental evaluation validates the feasibility of the proposed approach using video of realistic garbage collection in Fujisawa city, Japan.",https://ieeexplore.ieee.org/document/8607228/,2018 IEEE 24th International Conference on Embedded and Real-Time Computing Systems and Applications (RTCSA),28-31 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SSCI47803.2020.9308428,DeepFake Detection on Publicly Available Datasets using Modified AlexNet,IEEE,Conferences,"Deep learning has been applied successfully in many areas, including computer vision, natural language processing, cyber physical systems and big data analytics. Recently, a synthesis of deep leaning techniques has been deployed to create fake images and videos that are not easily distinguishable from the real ones; this technology is known as DeepFakes. In this paper, we looked at various DeepFakes related datasets and created a model in order to identify whether a frame of a video is fake or real. This is important as videos can be easily manipulated in a way that can spread misinformation, and that can cause major problems in the world today, especially if the videos have political implications. In order to create a model, we utilize a modified AlexNet constructed of an arrangement of 6 layers: convolution2d, max pooling, dense, flatten, activation and dropout layers. UADFV, FaceForensics++, and Celeb-DF are the 3 datasets used in this research. There are many publicly available datasets, however, we found the UADFV, FaceForensics++ and Celeb-DF to be the most convenient in how the data was formatted. All data for each dataset was organized into videos of varying classes. While the UADFV dataset is straightforward and only has 2 classes: real and fake, the FaceForensics++ dataset looks at the various kinds of video manipulations and has 5 different classes. Our model was able to achieve a 9S.73% accuracy when identifying whether a video is real or fake on the UADFV dataset, a slightly adjusted version was able to accomplish an 87.49% accuracy with the FaceForenisics++dataset, and reached 98.85% accuracy on the Celeb-df dataset.",https://ieeexplore.ieee.org/document/9308428/,2020 IEEE Symposium Series on Computational Intelligence (SSCI),1-4 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3240765.3240791,DeepFense: Online Accelerated Defense Against Adversarial Deep Learning,IEEE,Conferences,"Recent advances in adversarial Deep Learning (DL) have opened up a largely unexplored surface for malicious attacks jeopardizing the integrity of autonomous DL systems. With the wide-spread usage of DL in critical and time-sensitive applications, including unmanned vehicles, drones, and video surveillance systems, online detection of malicious inputs is of utmost importance. We propose DeepFense, the first end-to-end automated framework that simultaneously enables efficient and safe execution of DL models. DeepFense formalizes the goal of thwarting adversarial attacks as an optimization problem that minimizes the rarely observed regions in the latent feature space spanned by a DL network. To solve the aforementioned minimization problem, a set of complementary but disjoint modular redundancies are trained to validate the legitimacy of the input samples in parallel with the victim DL model. DeepFense leverages hardware/software/algorithm co-design and customized acceleration to achieve just-in-time performance in resource-constrained settings. The proposed countermeasure is unsupervised, meaning that no adversarial sample is leveraged to train modular redundancies. We further provide an accompanying API to reduce the non-recurring engineering cost and ensure automated adaptation to various platforms. Extensive evaluations on FPGAs and GPUs demonstrate up to two orders of magnitude performance improvement while enabling online adversarial sample detection.",https://ieeexplore.ieee.org/document/8587702/,2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD),5-8 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASE.2019.00126,DeepMutation++: A Mutation Testing Framework for Deep Learning Systems,IEEE,Conferences,"Deep neural networks (DNNs) are increasingly expanding their real-world applications across domains, e.g., image processing, speech recognition and natural language processing. However, there is still limited tool support for DNN testing in terms of test data quality and model robustness. In this paper, we introduce a mutation testing-based tool for DNNs, DeepMutation++, which facilitates the DNN quality evaluation, supporting both feed-forward neural networks (FNNs) and stateful recurrent neural networks (RNNs). It not only enables to statically analyze the robustness of a DNN model against the input as a whole, but also allows to identify the vulnerable segments of a sequential input (e.g. audio input) by runtime analysis. It is worth noting that DeepMutation++ specially features the support of RNNs mutation testing. The tool demo video can be found on the project website https://sites.google.com/view/deepmutationpp.",https://ieeexplore.ieee.org/document/8952248/,2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE),11-15 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA40945.2020.9197465,DeepRacer: Autonomous Racing Platform for Experimentation with Sim2Real Reinforcement Learning,IEEE,Conferences,"DeepRacer is a platform for end-to-end experimentation with RL and can be used to systematically investigate the key challenges in developing intelligent control systems. Using the platform, we demonstrate how a 1/18th scale car can learn to drive autonomously using RL with a monocular camera. It is trained in simulation with no additional tuning in the physical world and demonstrates: 1) formulation and solution of a robust reinforcement learning algorithm, 2) narrowing the reality gap through joint perception and dynamics, 3) distributed on-demand compute architecture for training optimal policies, and 4) a robust evaluation method to identify when to stop training. It is the first successful large-scale deployment of deep reinforcement learning on a robotic control agent that uses only raw camera images as observations and a model-free learning method to perform robust path planning. We open source our code and video demo on GitHub<sup>2</sup>.",https://ieeexplore.ieee.org/document/9197465/,2020 IEEE International Conference on Robotics and Automation (ICRA),31 May-31 Aug. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCCC.2018.8711025,DeepSDN: Connecting the Dots Towards Self-driving Networks,IEEE,Conferences,"The cloud data centers are going through an unprecedented growth from past few years. In an era of real-time video streaming, on-demand gaming, door-step e-commerce services, and highly inter-connected social networks, cost-effective service models, adaptive resources provisioning and upfront applications availability contribute significantly towards such a stellar growth. However, there are many challenges that must be addressed in a systematic manner to meet the requirements of increasingly demanding current and upcoming applications of the cloud computing paradigm. Optimum resources management, instant response time, interoperability among a diverse set of emerging technologies and innovative applications are a few of these challenges. On the other hand, the recent trend in softwarization of networks, particularly enabled by network function virtualization (NFV) and software-defined networking (SDN) principles, provides immense opportunities to better utilize the network resources by programmable abstractions with an efficient control and management techniques. Furthermore, machine learning based solutions are gaining prominence in resource optimization problems and autonomous systems. Therefore, in this paper, we strive to connect the dots by state-of-the-art methodologies in networking and machine learning domains and utilize these developments to grapple with the challenges of the cloud-based systems. We propose DeepSDN, an SDN-based solution that harnesses existing machine learning techniques to move a step closer towards self-driving networks. The comparative results obtained from an experimental testbed corroborates effectiveness of our approach and suggest a way forward towards autonomous network management.",https://ieeexplore.ieee.org/document/8711025/,2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC),17-19 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITES53477.2021.9637096,Deepfake Detection Based on Incompatibility Between Multiple Modes,IEEE,Conferences,"We propose a multi-modal detection for deepfake videos, called the Incompatibility Between Multiple Modes (IBMM) detection. The detection algorithm can detect whether the video is real or fake, and may be embedded in the monitoring equipment in the future. The model adopts EfficientNet and simple 3D-CNN, and it identifies deepfake videos through three modes. In the facial motion mode and lip motion mode, we use the EfficientNet for feature learning. This network uses a series of fixed scaling coefficients to scale the dimensions of the network uniformly and achieves good results in learning image features. In the audio mode, we adopt 3D-CNN network to train the hot coding diagram of audio data. Besides, for a single mode, we use the cross-entropy loss to calculate the irrationality of the mode. For different modes, the contrastive loss is used to calculate the incongruity between the modes, such as incompatibility between lips and voice. Experimental results show that, compared with other existing fake detection methods, the method presented in this paper has higher accuracy (95.87%) on DFDC datasets. And compared with the existing methods, the accuracy increases by 5.21%.",https://ieeexplore.ieee.org/document/9637096/,2021 International Conference on Intelligent Technology and Embedded Systems (ICITES),31 Oct.-2 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSAIEE54046.2021.9543264,Deepfake Video Detection by Combining Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN),IEEE,Conferences,"Nowadays, people are facing an emerging problem called deepfake videos. These videos were created using deep learning technology. Some are created just for fun, while others are trying to manipulate your opinions, cause threats to your privacy, reputation, and so on. Sometimes, deepfake videos created using the latest algorithms can be hard to distinguish with the naked eye. That's why we need better algorithms to detect deepfake. The system we are going to present is based on a combination of CNN and RNN, as research shows that using CNN and RNN combined achieve better results. We are going to use a pre-trained CNN model called Resnext50. Using this, we save the time of training the model from scratch. The proposed system uses Resnext pretrained model for Feature Extraction and these extracted features are used to train the Long short-term memory (LSTM). Using CNN and RNN combined, we capture the inter frames as well as intra frames features which will be used to detect if the video is real or fake. We evaluated our method using a large collection of deepfake videos gathered from a variety of distribution sources. We demonstrate how our system may obtain competitive results while utilizing a simplistic architecture.",https://ieeexplore.ieee.org/document/9543264/,"2021 IEEE International Conference on Computer Science, Artificial Intelligence and Electronic Engineering (CSAIEE)",20-22 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFCOMW.2018.8406989,Demo abstract: Context-aware video streaming with Q-learning for MEC-enabled cellular networks,IEEE,Conferences,"With the proliferation of video streaming service, how to enhance user Quality of Experience (QoE) against the uneven video quality in highly dynamic networks has become a growing concern. Compared with the common solutions in which video streaming adaptation algorithms are implemented on the client-side, we proposed an MEC-side (Mobile Edge Computing) context-aware Q-leaning based video streaming approach to overcome client-driven solutions' weakness of competing behavior and difficulty in enforcing management policies. A working prototype testbed is built by deploying our video streaming platform on an MEC server (close to commercial eNBs) at a real 4G LTE network. Experiment results show our proposed approach adapts better to network variation.",https://ieeexplore.ieee.org/document/8406989/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS),15-19 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASIP.2016.7853831,Demo: HELICoiD tool demonstrator for real-time brain cancer detection,IEEE,Conferences,"In this paper, a demonstrator of three different elements of the EU FET HELICoiD project is introduced. The goal of this demonstration is to show how the combination of hyperspectral imaging and machine learning can be a potential solution to precise real-time detection of tumor tissues during surgical operations. The HELICoiD setup consists of two hyperspectral cameras, a scanning unit, an illumination system, a data processing system and an EMB01 accelerator platform, which hosts an MPPA-256 manycore chip. All the components are mounted fulfilling restrictions from surgical environments, as shown in the accompanying video recorded at the operating room. An in-vivo human brain hyperspectral image data base, obtained at the University Hospital Doctor Negrin in Las Palmas de Gran Canaria, has been employed as input to different supervised classification algorithms (SVM, RF, NN) and to a spatial-spectral filtering stage (SVM-KNN). The resulting classification maps are shown in this demo. In addition, the implementation of the SVM-KNN classification algorithm on the MPPA EMB01 platform is demonstrated in the live demo.",https://ieeexplore.ieee.org/document/7853831/,2016 Conference on Design and Architectures for Signal and Image Processing (DASIP),12-14 Oct. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2012.6331471,Demonstration of the Second Generation Real-Time Cellular Neural Network Processor: RTCNNP-v2,IEEE,Conferences,"This proceeding is compiled from our previous works, where architecture of the Second-Generation Real-Time Cellular Neural Network (CNN) Processor (RTCNNP-v2) was proposed. The system is designed for applications where high-resolution and high-speed is desired. The structure is fully-pipelined and the processing is real-time. Proposed structure is coded in VHDL and realized on two FPGA devices: one high-end and one low-budget. The system is the only reported CNN implementation supporting real-time Full-HD video image processing, to date.",https://ieeexplore.ieee.org/document/6331471/,2012 13th International Workshop on Cellular Nanoscale Networks and their Applications,29-31 Aug. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2019.00346,DenseFusion: 6D Object Pose Estimation by Iterative Dense Fusion,IEEE,Conferences,"A key technical challenge in performing 6D object pose estimation from RGB-D image is to fully leverage the two complementary data sources. Prior works either extract information from the RGB image and depth separately or use costly post-processing steps, limiting their performances in highly cluttered scenes and real-time applications. In this work, we present DenseFusion, a generic framework for estimating 6D pose of a set of known objects from RGB-D images. DenseFusion is a heterogeneous architecture that processes the two data sources individually and uses a novel dense fusion network to extract pixel-wise dense feature embedding, from which the pose is estimated. Furthermore, we integrate an end-to-end iterative pose refinement procedure that further improves the pose estimation while achieving near real-time inference. Our experiments show that our method outperforms state-of-the-art approaches in two datasets, YCB-Video and LineMOD. We also deploy our proposed method to a real robot to grasp and manipulate objects based on the estimated pose.",https://ieeexplore.ieee.org/document/8953386/,2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),15-20 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.00598,DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation,IEEE,Conferences,"We present DepthInSpace, a self-supervised deep-learning method for depth estimation using a structured-light camera. The design of this method is motivated by the commercial use case of embedded depth sensors in nowadays smartphones. We first propose to use estimated optical flow from ambient information of multiple video frames as a complementary guide for training a single-frame depth estimation network, helping to preserve edges and reduce over-smoothing issues. Utilizing optical flow, we also propose to fuse the data of multiple video frames to get a more accurate depth map. In particular, fused depth maps are more robust in occluded areas and incur less in flying pixels artifacts. We finally demonstrate that these more precise fused depth maps can be used as self-supervision for fine-tuning a single-frame depth estimation network to improve its performance. Our models&#x2019; effectiveness is evaluated and compared with state-of-the-art models on both synthetic and our newly introduced real datasets. The implementation code, training procedure, and both synthetic and captured real datasets are available at https://www.idiap.ch/paper/depthinspace.",https://ieeexplore.ieee.org/document/9710070/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2019.8852005,Design Space Evaluation of a Memristor Crossbar Based Multilayer Perceptron for Image Processing,IEEE,Conferences,"This paper describes a simulated memristor-based neuromorphic system that can be used for ex-situ training of a multi-layer perceptron algorithm. The presented programming technique can be used to map the weights required of a neural algorithm directly onto the grid of resistances in a memristor crossbar. Using this weight-to-crossbar mapping approach along with the dot product calculation circuit, neural algorithms can be easily implemented using this system. To show the effectiveness of this circuit, a Multilayer Perceptron is trained to perform Sobel edge detection. Following these simulations, an analysis was presented that shows how memristor programming accuracy and network size are related to output error; the results show that network size can be increased to reduce testing error. In some cases, the memristors in the circuit may be capable of operating with at lower precision if the network size is increased. This means that less precise (or lower resolution) memristor devices may be used to implement the proposed system. Furthermore, a power, timing, and energy analysis shows that this circuit has a computation throughput that allows it to process 4K UHD video in real time at approximately 337mW.",https://ieeexplore.ieee.org/document/8852005/,2019 International Joint Conference on Neural Networks (IJCNN),14-19 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IEMCON53756.2021.9623141,Design and Development of an Integrated Internet of Audio and Video Sensors for COVID-19 Coughing and Sneezing Recognition,IEEE,Conferences,"There are a lot of ongoing efforts to combat the COVID-19 pandemic using different combinations of low-cost sensing technologies, information/communication technologies, and smart computation. To provide COVID-19 situational awareness and early warnings, a scalable, real-time sensing solution is needed to recognize risky behaviors in COVID-19 virus spreading such as coughing and sneezing. Various coughing and sneezing recognition methods use audio-only or video-only sensors and Deep Learning (DL) algorithms for smart event recognition. However, each of these recognition processes experiences several types of failure behaviors due to false detection. Sensor integration is a solution to overcome such failures. Moreover, it improves event recognition precision. With the wide availability of low-cost audio and video sensors, we proposed a real-time integrated Internet of Things (IoT) architecture to improve the results of coughing and sneezing recognition. Implemented architecture joins edge and cloud computing. In edge computing, the microphone and camera are connected to the internet and embedded with a DL engine. Audio and video streams are fed to edge computing to detect coughing and sneezing actions in realtime. Cloud computing, which is developed based on the Amazon Web Service (AWS), combines the results of audio and video processing. In this paper, a scenario of a person coughing and sneezing was developed to demonstrate the capabilities of the proposed architecture. The experimental results show that the proposed architecture improved the reliability of coughing and sneezing recognition in the integrated cloud system compared to audio-only and video-only detectors. Three factors have been considered to compare the results of the proposed architecture: F-score, precision, and recall. The precision and recall of the cloud detector are improved on average by &#x0025;43 and &#x0025;15, respectively, compared to audio-only and video-only detectors. The F-score improved on average 1.24 times.",https://ieeexplore.ieee.org/document/9623141/,"2021 IEEE 12th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)",27-30 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP40778.2020.9191146,Design and FPGA Implementation of an Adaptive video Subsampling Algorithm for Energy-Efficient Single Object Tracking,IEEE,Conferences,"Image sensors with programmable region-of-interest (ROI) readout are a new sensing technology important for energyefficient embedded computer vision. In particular, ROIs can subsample the number of pixels being readout while performing single object tracking in a video. In this paper, we develop adaptive sampling algorithms which perform joint object tracking and predictive video subsampling. We utilize an object detection consisting of either mean shift tracking or a neural network, coupled with a Kalman filter for prediction. We show that our algorithms achieve mean average precision of 0.70 or higher on a dataset of 20 videos in software. Further, we implement hardware acceleration of mean shift tracking with Kalman filter adaptive subsampling on an FPGA. Hardware results show a 23 × improvement in clock cycles and latency as compared to baseline methods and achieves 38FPS real-time performance. This research points to a new domain of hardware-software co-design for adaptive video subsampling in embedded computer vision.",https://ieeexplore.ieee.org/document/9191146/,2020 IEEE International Conference on Image Processing (ICIP),25-28 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC51589.2020.9327314,Design and Implementation of Anti-clamp System for Learning Platform Based on Deep Learning,IEEE,Conferences,"In this paper, a kind of deep learning - based learning table automatic closing anti - clamping technology is proposed. Based on image processing, this technology builds a machine vision model of deep convolutional neural network, collects samples in sections for video analysis, conducts batch image preprocessing on massive data, trains corresponding models respectively, and then conducts real-time monitoring on the closing process of the learning desk. If there is an obstacle, the information will be fed back to the system to stop the closing of the learning desk. If not, continue to perform the closing action until the learning table is completely closed. This method through the actual test, has high response speed and accuracy of the obstacle detection, detection blind area is smaller, the advantages of non-contact and active prevention clip. At the same time, it has strong adaptability, low requirements on hardware, and has a very good application prospect.",https://ieeexplore.ieee.org/document/9327314/,2020 Chinese Automation Congress (CAC),6-8 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC53003.2021.9727568,Design and Implementation of Infrared Image Preprocessing System based on FPGA,IEEE,Conferences,"Infrared image processing is a conventional topic, but the hardware design and implementation for infrared image processing algorithms are full of challanges, especially for special purpose infrared detectors which have customized interfaces and protocols. Moreover, some customized infrared detectors only generate the infrared image with very poor quality. Therefore, in this paper, we design and implement an infrared image preprocessing system based on FPGA for customized infrared detectors. We propose a hardware architecture that includes the whole process from the input of detector, image processing, to final infrared video stream output and display. Our hardware architecture is able to adapt to different customized infrared detectors, which has the characteristics of real-time, low-power, dynamic parameter configuration, high processing efficiency and stable imaging effect. We have implemented it on Xilinx FPGA. The infrared imaging effect is significantly improved with limited FPGA resources (about only 1.6M LUT, 420 BRAM for all algorithm models on Xilinx zc706 evaluation board), the image processing speed can reach to 225Hz with the resolution of 320*256. Our preprocessing system can generate real-time video for display with 1920*1080@60Hz.",https://ieeexplore.ieee.org/document/9727568/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSIP49896.2020.9339439,Design and Implementation of Multimodal Fatigue Detection System Combining Eye and Yawn Information,IEEE,Conferences,"Fatigue can harm people's physiology and psychology, and in serious cases it can even endanger people's lives. To meet the needs of real-time detection of human fatigue status, this paper designs and implements an online fatigue detection system which is capable of interactive fatigue status analysis. Based on AdaBoost framework and cascading technology, a classifier which is capable of identifying faces and eyes in video images at the same time is constructed, and the stable tracking of human eyes is realized by KLT algorithm. After extracting the features from the eye area, the eye opening and closing coefficient, or eye aspect ratio (EAR), is obtained. The k-means method is used to determine the thresh value of the eye opening and closing state to achieve the detection of blink frequency. Combined with nodding frequency and yawn frequency, the state of human fatigue is determined. The experimental results of different subjects in different scenarios show that the detection accuracy of this system can reach up to 95%.",https://ieeexplore.ieee.org/document/9339439/,2020 IEEE 5th International Conference on Signal and Image Processing (ICSIP),23-25 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACCC54619.2021.00018,Design and Implementation of Safety Helmet Detection System Based on YOLOv5,IEEE,Conferences,"In order to reduce safety accidents caused by non-standard wearing of helmets, deep learning target detection technology is applied to construction safety detection scenarios, and a helmet detection algorithm based on YOLO v5 is proposed, which can realize real-time detection of helmet wearing. The deep learning part uses the K-means algorithm to cluster the dimensions of the target frame, and Yolov5s.pt is used for deep learning training. During training, the size of the input image is changed to increase the adaptability of the model, and the hyperparameters and optimizer are adjusted to be the best after improvement. The detection model has an accuracy rate of 90&#x0025;, and the detection speed has reached 37.8fps, which meets the requirements of real-time detection of helmets. Through the combination of this model and hardware such as cameras, a real-time detection of whether a person wears a helmet is designed and implemented. The system realizes the three functions of picture detection, video detection and real-time monitoring.",https://ieeexplore.ieee.org/document/9681420/,2021 2nd Asia Conference on Computers and Communications (ACCC),24-27 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICACT53585.2022.9728813,Design and Implementation of a Real-time Target Detection and Tracking System,IEEE,Conferences,"Target detection is to quickly and accurately locate the object to be measured in the video image and correctly classify it. Target tracking is to obtain the motion parameters of the object in the image sequence, and to detect, extract, identify and track the specific object. Combining target detection and tracking to follow specific objects is one of the research hotspots in the realization of intelligent embedded terminals. This paper designs and implements a system that combines target detection and tracking, which can realize real-time tracking of specific objects. Finally, the experiments prove the reliability of the proposed system, which can be used in intelligent systems such as robots.",https://ieeexplore.ieee.org/document/9728813/,2022 24th International Conference on Advanced Communication Technology (ICACT),13-16 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSP.2019.8698009,Design and Implementation of the Smart Glove to Aid the Visually Impaired,IEEE,Conferences,"Locating objects of daily use is a strenuous task for the visually impaired. The objective of this paper is to design a smart glove by using Deep Neural Networks (DNN) and object tracking algorithm which will guide the hand of the visually impaired to the desired object in an indoor environment. The smart glove has five micro-vibrating motors, each one used to guide the user's hand in five different directions namely, forward, upward, downward, rightward and leftward. The palm of the glove has a Universal Serial Bus (USB) camera which feeds the real-time video to the Raspberry Pi for processing. The camera also has an inbuilt microphone. The user vocally commands the system to identify the desired object. The camera then detects the object using DNN. Once the object is tagged, object tracking begins. Based on the relative position of the camera and the object, the micro-vibrating motors vibrate accordingly to guide the users hand in the required direction. An additional feature which is incorporated includes differentiation of similar objects by color.",https://ieeexplore.ieee.org/document/8698009/,2019 International Conference on Communication and Signal Processing (ICCSP),4-6 April 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTC49870.2020.9289246,Design and Verification of Low Latency Access Network based on Mobile Edge Computing,IEEE,Conferences,"This paper deals with an access system for end-to-end low latency services in mobile communication systems. The MEC (Mobile Edge Computing) concept was introduced to process information near the user's location, and the virtualization concept was introduced for network flexibility. The system has traffic offloading effect due to path shortening and computing offloading effect due to load sharing. The system thus enables real-time low latency services that are sensitive to response time. This paper presents the design and verification of such a system. The implemented system verified the low latency results through deep learning based video analysis service.",https://ieeexplore.ieee.org/document/9289246/,2020 International Conference on Information and Communication Technology Convergence (ICTC),21-23 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1992.226908,Design and development of a real-time neural processor using the Intel 80170NX ETANN,IEEE,Conferences,"The Naval Air Warfare Center Weapons Division is designing and developing a real-time neural processor for missile seeker applications. The system uses a high-speed digital computer as the user interface and as a monitor for processing. The use of a standard digital computer as the user interface allows the user to develop the process in whatever programming environment desired. With the capability to store up to 64 k of output data on each frame, it is possible to process two-dimensional image data in excess of video rates. The real-time communication bus, with user-defined interconnect structures, enables the system to solve a wide variety of problems. The system is best suited to perform local area processing on two-dimensional images. Using this system, each layer has the capacity to represent up to 65536 neurons. The fully operational system may contain up to 12 of these layers, giving the total system a capacity in excess of 745000 neurons.<>",https://ieeexplore.ieee.org/document/226908/,[Proceedings 1992] IJCNN International Joint Conference on Neural Networks,7-11 June 1992,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMA.2010.5589229,Design and implementation of efficient hardware solution based sub-window architecture of Haar classifiers for real-time detection of face biometrics,IEEE,Conferences,"In this paper we propose a hardware solution by the use of FPGA based circuit for real time face detection. We have built a sub-window architecture for the extraction of Haar-like features, which are the basic elements of weak classifiers according to AdaBoost learning algorithm. The main contribution is that the proposed architecture removes traditional frame buffer, and only reserve the line buffer and sub-window register array. When the video data is in the progress of delivery, every pixel will be sent to the line buffer one by one and then be moved into sub-window for the circulation of weak classifiers calculation directly. Because integral image can be calculated by sub-window register array immediately, it provides the best detection performance without delay, and executes the minimal step of sub-window movement for best face detection accuracy. We have implemented hardware circuit and analyze it by Xilinx System Generator. The outcome shows that our design provides face detection speed up to 471.6 fps in the resolution of 640 × 480.",https://ieeexplore.ieee.org/document/5589229/,2010 IEEE International Conference on Mechatronics and Automation,4-7 Aug. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6010075,Design and implementation of network camera based on TMS320DM365,IEEE,Conferences,"The design and implementation of network camera based on embedded Linux system and the TMS320DM356 processor is put forward which can be operated on 5-megapixel. At 30 frames per second in full frame rates, It is the video stream up to 2Mbps bit rate to provide resolution of 1280×720 pixels. The sampled video stream which compressed and encoded by the protocol of H.264 can be transferred in the network applied with the real-time transport protocol (RTP) and Real-time Transport Control Protocol (RTCP)",https://ieeexplore.ieee.org/document/6010075/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2017.8108946,Design and implementation of smoke early warning system based on video analysis technology,IEEE,Conferences,"In order to obtain real-time accurate detection of smoke in the panoramic area, a visual -based smoke early warning system is designed. A 180-degree holographic and high speed camera is equipped as the video capture part of the system, and the video acquisition and target lock functions are implemented. The intelligent recognition function is separated into three steps. Firstly, the suspected smoke area is extracted by Vibe algorithm. Secondly, the Camshift method is used to track the suspected area of moving smoke. Thirdly, the smoke is detected by the proposed fusion dynamic features. The interface of system is developed by the software of QT. The designed system can not only realize sensitive area monitoring function in manual mode, but also the intelligent smoke early warning and detection function are achieved in automatic mode. The experimental results showed the designed system obtained the advantages of simple operation, rapid smoke alarm and the satisfied accuracy for industry requirement.",https://ieeexplore.ieee.org/document/8108946/,2017 International Conference on Machine Learning and Cybernetics (ICMLC),9-12 July 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITAIC49862.2020.9339101,Design of Embedded Real-time Target Tracking System Based on KNN Algorithm,IEEE,Conferences,"With the rapid development of science and technology, more and more high-tech products have been applied to all aspects of life. In the field of security and defense, video monitoring plays an important role. In order to solve the problem of real-time and miniaturization of target tracking system, this paper applies a tracking algorithm based on KNN(k-nearest neighbor) background splitter combined with Kalman filter. When tracking a moving target, it is important to determine the moving target in the initial frame. In this paper, the motion detection method in KNN background segmentation is adopted to automatically initialize the moving target. Kalman filter can predict the motion state of the moving target in the next frame according to the information of the moving target in the current frame, and track the target with the predicted value as the input of CamShift algorithm. The implementation of the tracking algorithm on the embedded raspberry pi platform is beneficial to the miniaturization of smart devices. The experimental results show that the algorithm has better tracking ability, faster operation speed and stronger robustness for moving targets in raspberry pi embedded environment, which improves the performance of the algorithm and gets good effects.",https://ieeexplore.ieee.org/document/9339101/,2020 IEEE 9th Joint International Information Technology and Artificial Intelligence Conference (ITAIC),11-13 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2014.6888621,Design of a third generation Real-Time Cellular Neural Network emulator,IEEE,Conferences,"In this paper, the features of the next generation Real-Time Cellular Neural Network Processor (RTCNNP-v3) are discussed. The RTCNNP-v2 structure is the only CNN implementation that is reported to be capable of processing full-HD 1080p@60 (1920×1080 resolution at 60 Hz frame rate) video images in real-time, due to its fully-pipelined architecture, however, it has some weaknesses like the inability to divide the processing in spatial domain, record and recall intermediate results to an external memory and has some issues in its internal memory coding. Those shortcomings are to be addressed in the next design of our CNN emulator - RTCNNP-v3, which will increase the range of applications and enable the implementation to match the requirements of the cutting-edge movie production technologies like UHD (4K) and the future FUHD (8K).",https://ieeexplore.ieee.org/document/6888621/,2014 14th International Workshop on Cellular Nanoscale Networks and their Applications (CNNA),29-31 July 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICoAC44903.2018.8939110,Designing an Efficient Framework for Violence Detection in Sensitive Areas using Computer Vision and Machine Learning Techniques,IEEE,Conferences,"Human security against violence is one of the major concern in sensitive areas like ATMs, Government offices, Hospitals etc. For such incidents, there is a need of generating timely and automated alerts to concern officials to take further action. For this, surveillance cameras deployed in sensitive areas can be very much helpful. However, existing systems face the problems of low accuracy, high false alerts and high computational cost in monitoring and analyzing the video streams from surveillance cameras and making the decision about violence in real-time. In this paper, we propose an efficient framework to detect violence in sensitive areas using feasible computer vision and machine learning techniques. It collects the video streams of human activities and generates the violence related features by applying motion tracking which slices the video frames based on the presence of moving objects. To find the violent flow descriptors, it calculates the optical flow for each pixel of the frame. These violent flow descriptors are applied to different machine learning techniques to detect the violence events. For the feasibility analysis, different machine learning techniques are analyzed and compared. In addition, the fusion of feasible techniques is tested for improving the accuracy and reducing the error rate.",https://ieeexplore.ieee.org/document/8939110/,2018 Tenth International Conference on Advanced Computing (ICoAC),13-15 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiSPNET45539.2019.9032774,Detecting Abnormal Event in Traffic Scenes using Unsupervised Deep Learning Approach,IEEE,Conferences,"In today's digital age, detecting abnormal events in the traffic scene is a more significant problem in video surveillance. The proposed scheme is adopted for detecting abnormalities such as wrong side driving, people crossing the road illegally, vehicle moving in pedestrain pathway and which uses spatio-temporal attributes. Although numerous machine learning approaches are described in existing literature focusing on pixel-wise differnce with less performance in identifying the abnormalities in video traffic scenes. In this paper, a novel frame work for identifying abnormal detection using unsupervised deep learning algorithm. The scheme uses joint based methodology (ConvLSTM with kmeans) with a combination of reconstruction and clustering loss respectively. The convolutional neural network with LSTM is used to detect and recognize anomalies in traffic scenes with refernce of previous frame information. To identify the normal or abnormal events, the proposed work is composed of training and testing phase. The proposed model is implemented in real time and it is found that the accuracy rate for detecting abnormalities 93.02% abnormality than state-of-art technique.",https://ieeexplore.ieee.org/document/9032774/,2019 International Conference on Wireless Communications Signal Processing and Networking (WiSPNET),21-23 March 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData47090.2019.9005655,Detecting Pedestrian Crossing Events in Large Video Data from Traffic Monitoring Cameras,IEEE,Conferences,"Pedestrian safety on the road is a priority for transportation system managers and operators. While there are a number of treatments and technologies to effectively improve pedestrian safety, identifying the location where these are most needed remains a challenge. Mid-block locations, where safety countermeasures are often needed the most, are typically harder to monitor. Current practice often requires manual observation of candidate locations for limited time periods, leading to an identification process that is often time consuming, lags behind traffic pattern changes over time, and lacks scalability. As a result, target locations are often selected reactively, after serious traffic incidents reveal an underlying safety issue. We propose an approach to use data collected by existing traffic monitoring cameras to automatically identify pedestrian activities on the road. We propose an algorithm to detect pedestrian crossing events based on the detection of individuals on individual video frames using a deep neural network model. Resulting pedestrian locations and movement trajectories can be visualized on a background image, which is automatically extracted at the analyzed location from the video. We demonstrate and evaluate our approach with a real-world use case. The case study considered in this work uses cameras owned by the City of Austin, Texas to study pedestrian road use before and after the deployment of a pedestrian-hybrid beacon. We explore qualitative and quantitative metrics to describe pedestrian activity and corresponding changes, which may be used to prioritize the deployment of pedestrian safety solutions, or evaluate their performance. We compared the number of crossing events detected per hour with manually reviewed results from a selected day. The result shows 67 percent overall accuracy, although we observe significant variability across times-of-day. Despite observed limitations, our work illustrates how the value of existing traffic camera networks can be augmented beyond everyday traffic monitoring, and used to collect valuable information on road usage by pedestrians.",https://ieeexplore.ieee.org/document/9005655/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV51458.2022.00135,Detecting Tear Gas Canisters With Limited Training Data,IEEE,Conferences,"Human rights investigations often entail triaging large volumes of open source images and video in order to find moments that are relevant to a given investigation and warrant further inspection. Searching for instances of tear gas usage online manually is laborious and time-consuming. In this paper, we study various object detection models for their potential use in the discovery and identification of tear gas canisters for human rights monitors. CNN based object detection typically requires large volumes of training data, and prior to our work, an appropriate dataset of tear gas canisters did not exist. We benchmark methods for training object detectors using limited labelled data: we fine-tune different object detection models on the limited labelled data and compare performance to a few shot detector and augmentation strategies using synthetic data. We provide a dataset for evaluating and training tear gas canister detectors and indicate how such detectors can be deployed in real-world contexts for investigating human rights violations. Our experiments show that various techniques can improve results, including fine-tuning state of the art detectors, using few shot detectors, and including synthetic data as part of the training set.",https://ieeexplore.ieee.org/document/9706699/,2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FarEastCon.2018.8602625,Detection and Recognition of Emergency Situations in Continuous Video Stream of Information and Telecommunication Systems,IEEE,Conferences,"The problem of events detecting and classifying in the continuous video stream of information and telecommunication security systems is solved. As a basic algorithm, it is proposed to use the ensemble of deep neural networks - convolutional networks and feedback networks, in particular, for classification and annotation, which makes it possible to recognize abnormal emergency situations. A training set of abnormal situations was collected, various architectures of deep neural networks were trained and tested. It is shown that the use of the indRNN layers achieves up to 70% when recognitions multi-class events in the video stream. The new is strengthening of classification estimation of a video segment by extracting the keywords from the automatic annotation. The developed software package can be implemented in the integrated security system of abnormal situations recognition in real time.",https://ieeexplore.ieee.org/document/8602625/,2018 International Multi-Conference on Industrial Engineering and Modern Technologies (FarEastCon),3-4 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCSP52374.2021.9465345,Detection of Electronic Devices in real images using Deep Learning Techniques,IEEE,Conferences,"Object Detection from real world scenario is a subset of Computer Vision, that uses state-of-the-art algorithms and techniques in deep learning to identify and locate the objects in an image or video. Latest advancements in Deep Learning, especially Convolutional Neural Networks (CNN) and in the field of image processing has further improved the process of object detection. Deep learning algorithms that are developed over the years aim to solve several challenges associated with object detection which includes localizing the object in an image, classifying the object correctly with a high confidence score and realtime detection of objects. The performance of the existing algorithms involves a tradeoff between accuracy and detection speed. Algorithms like Faster Region based Convolutional Neural Networks (R-CNN) and Single Shot Detector (SSD) that achieved high accuracy in classifying objects were slow in detecting the objects. Such algorithms were not able to keep up with the pace of detection with video input in realtime and thus were not suitable for implementation in critical applications. The drawbacks associated with these algorithms can be eliminated by following a unified one-state approach. The approach is to fully identify and classify the required objects of interest by passing the image only once through the network. This approach thus decreases detection time considerably. You Only Look Once (YOLO) family of algorithms is one such single shot detector that uses CNNs to detect objects. In our work, we have used the YOLOv3 algorithm to develop a model that detects electronic devices. The model was also tested against realtime input from webcam and mean Average Precision (mAP) of YOLOv3 has been computed and compared with another model developed using Faster R-CNN.",https://ieeexplore.ieee.org/document/9465345/,"2021 5th International Conference on Computer, Communication and Signal Processing (ICCCSP)",24-25 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAAIC53929.2022.9793140,Detection of Helmetless Riders and Automatic Number Plate Recognition Using Machine Learning,IEEE,Conferences,"Today, motorcycles have become so accessible and common as a means of transportation, there has been a sharp increase in the number of motorcycle accidents, due to the fact that many bicycles do not wear protective helmets, making riding a motorcycle an everyday hazard. Every motorcyclist must wear a helmet. Even though the use of the helmet is a major factor in many lands, there are some motorcyclists who do not use it or abuse it. About 20 to 30 cyclists face daily accidents that can lead to death or serious injury and also lead to permanent bed injuries on Indian roads in 2018 due to negligence in not taking safety measures while riding a bicycle such as wearing helmets. The current system based on video testing works well, but requires extensive human intervention, which reduces its efficiency over time and imposes human bias. As a result, performing this procedure automatically is highly desirable. In this study a method will be developed to find non-helmet motorcyclists in real time using surveillance videos. Here is software that uses CNN to identify motorcyclists who do not wear protective helmets. Because of its high accuracy, CNN is used for image classification and recognition.",https://ieeexplore.ieee.org/document/9793140/,2022 International Conference on Applied Artificial Intelligence and Computing (ICAAIC),9-11 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISP-BMEI48845.2019.8966045,Detection of Workers Without the Helments in Videos Based on YOLO V3,IEEE,Conferences,"In the construction site with complex environment, there are a variety of potential risk factors threatening personal safety. Since the head is the most critical part of a human body and is the most vulnerable to fatal injuries, lots of accidents caused by workers not wearing safety helmets have occurred from time to time. So, the staff working in construction site has the requirement to wear the safety helmet. In order to reduce the incidence of safety accidents caused by not wearing a protective helmet, a realtime detection application for wearing helmet based on YOLO (You Look Only Once) v3 was proposed and encapsulated into a real-time detection software with alert function and simple efficient operation, which has been successfully deployed to several construction sites. YOLO v3 model has a good response to the identification of aśpesonas'. Therefore, firstly, the workers in video are identified and intercepted by YOLO v3 model, and positive and negative samples are made. Then the worker in the dataset is identified whether wearing helmets or not. Compared with y-OLO v3, the mAP (Mean Average Precision) reached 93.5%, and the detection rate reached 35fps. Theoretical analysis and experimental results show that the proposed algorithm not only satisfies the helmet wearing detection task in real time, but also has high detection accuracy.",https://ieeexplore.ieee.org/document/8966045/,"2019 12th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",19-21 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCSP49186.2020.9315272,Detection of sharp objects using deep neural network based object detection algorithm,IEEE,Conferences,"Deep learning algorithms have the ability to learn complex functions and provide state-of-the-art results for com-puter vision problems. In recent times, these algorithms far exceeded the existing computer vision based techniques for object detection in X-ray imaging systems. So far, in literature single class of object namely gun and its parts were considered for detection using the SIXray10 database. We propose deep learning-based solution for the detection of sharp objects namely knife, scissors, wrench, pliers in the SIXray 10database. We propose two models namely model A and model B using a common object detection algorithm- YOLOv3 (You Only Look Once) with InceptionV3 and ResNet-50. YOLO is a deep neural network based object detection algorithm that performs the task in one-shot which allows real time inference in video of 15-30 fps. The model is FCN (Fully Convolutional Network) as has the capacity to perform both regression and classification by sharing weights for both the tasks. The network predicts a rectangular box called bounding box around the predicted object of interest along with the associated class. We analyze the performance of both model in terms of mAP. We achieve mean accuracy of 59.95% for model-A and 63.35% for Model-B. The most daunting part of the project is the low ratio of harmful to nonharmful items. By performing rigorous experiments we came up with the best set of possible results which uses varied pretrained neural networks for feature extraction in tandem with YOLO model for object detection. We endeavor to improve on these existing results so as these systems can be successfully deployed in airports to minimize human error and improve security in such environments.",https://ieeexplore.ieee.org/document/9315272/,"2020 4th International Conference on Computer, Communication and Signal Processing (ICCCSP)",28-29 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2007.4398962,Detection of thrown objects in indoor and outdoor scenes,IEEE,Conferences,"We present a novel technique for the detection of thrown objects and other free-flying bodies in video sequences. Our method runs in real-time and was designed to be used as a component in a deployed surveillance system. We detect regions of interesting motion that fit certain size, compactness and speed criteria, and use the expectation maximization algorithm to detect objects on parabolic trajectories over a short time window. The system was shown to successfully detect thrown objects of various sizes in a large test set of indoor and outdoor videos.",https://ieeexplore.ieee.org/document/4398962/,2007 IEEE/RSJ International Conference on Intelligent Robots and Systems,29 Oct.-2 Nov. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GHTC46280.2020.9342873,Developing a Deep Learning-enabled Guide for the Visually Impaired,IEEE,Conferences,"With visual impairment being a major detriment to quality of life, It is worth exploring options available to them to improve that quality. In this paper we propose using deep learning, real-time object recognition, and text-to-speech capabilities to develop an application to aid the visually impaired. Learning was implemented on the Convolutional Neural Network AlexNet, using two different types of image datasets to recognize both objects and buildings. We integrate a video webcam with our trained model to recognize objects in real-time so the visually impaired will be able to perceive their environment. Finally, using text-to-speech, our application audibly speaks what our trained model recognizes so they will know what's around them. After obtaining initial results from retrained AlexNet, we attempted two modifications of the original architecture to improve its performance for our application for image recognition for the visually impaired, the first change being to the fully connected layers and the second change being to the convolutional layers. Our results show recognition of 92% for internal object data and 88% for external object data. This will go a long way to achieve UN SDG3 goals for good health and well-being for a large percentage of visually impaired people worldwide.",https://ieeexplore.ieee.org/document/9342873/,2020 IEEE Global Humanitarian Technology Conference (GHTC),29 Oct.-1 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IntelliSys.2017.8324230,Developing video games with elementary adaptive artificial intelligence in unity: An intelligent systems approach,IEEE,Conferences,"Video games have increasingly demonstrated a great deal of audiovisual realism, in par with the massive performance improvement of computer systems. At the same time, their Artificial Intelligence (AI) component falls short in terms of realism because it is usually based on non-adaptive methods. Adaptive AI mechanisms can help increase video game realism allowing the game to adapt in real-time to the game progress and the user behavior. Following a short overview of the progress of AI in video games in the past years, this paper highlights the creation of modern video games with basic and elementary adaptive game AI using the Unity game development framework. Particular emphasis is on the details of the AI component. First, a shooter game with basic AI is created. Finally, an action-adventure video game is created featuring elementary case-based adaptive AI. The objective in this game is to create enemies which are able to perceive changes in the environment and adapt their strategies accordingly. Proposed AI practices can migrate into relevant real world applications, such as video surveillance and intrusion detection systems, mission critical autonomous networked patrolling and/or save and rescue robots, vision and hearing assistive applications, intelligent video and behavioral analytics to detect and predict threats etc.",https://ieeexplore.ieee.org/document/8324230/,2017 Intelligent Systems Conference (IntelliSys),7-8 Sept. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DSMP.2018.8478575,Development and Implementation of Human Face Alignment and Tracking in Video Streams,IEEE,Conferences,"The paper presents a method that allows detection, alignment and tracking of a human face in a real time in video streams. To detect and to align face on an image a face shape regression approach is used. The developed method uses scanning window, a cascade of ensembles of regression and classification trees, and adaptive boosting. The same trees are used for classification whether the given window contains a face and for regression of a face shape. For face tracking a starting position for face search is taken from the found shape on the previous frame. Conducted analysis of the proposed method implementation gave good performance results but revealed shortcomings and directions for future work. Sensitivity of face detection is 78% and accuracy of face alignment is 95%. The implementation can track faces in real time with a speed of at least 20 frames per second.",https://ieeexplore.ieee.org/document/8478575/,2018 IEEE Second International Conference on Data Stream Mining & Processing (DSMP),21-25 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTIS.2019.8883727,Development of AtoN Real-time Video Surveillance System Based on the AIS Collision Warning,IEEE,Conferences,"In view of the challenges with Aids to Navigation (AtoN) managements and emergency response, the present study designs and presents an AtoN real-time video surveillance system based on the AIS collision warning. The key technologies regarding with AtoN cradle head control and testing algorithms, video image fusion, system operation and implementation are demonstrated in details. Case study is performed at Guan River (China) to verify the effectiveness of the AtoN real-time video surveillance system for maritime security supervision. The research results indicate that the intellective level of the AtoN maintenance and managements could be significantly improved. The idea of designing modules brings a good flexibility and a high portability for the present surveillance system, therefore provides a guidance for the design of similar maritime surveillance systems.",https://ieeexplore.ieee.org/document/8883727/,2019 5th International Conference on Transportation Information and Safety (ICTIS),14-17 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAAI52226.2021.9508033,Development of Gasoline-Electric Hybrid Propulsion Surveillance and Reconnaissance VTOL UAV,IEEE,Conferences,"Vertical Take-Off and Landing (VTOL) Unmanned Aerial Vehicles (UAV) have been a high potential topic in the aerospace industry during the last decades due to its multirotor and fixed-wing nature of the aircraft. Besides, having the ability to rapidly deploy from a tight airstrip and gathering Intelligence, Surveillance, and Reconnaissance (ISR) information is the best way to be one step ahead of the enemy. In this paper, we present the implementation and development of gasoline-electric hybrid propulsion VTOL Unmanned Aerial vehicle respectively. The Hybrid propulsion VTOL UAV offers image and real-time video transmission to the ground station with fully autonomous control to get the best view of the enemy from the sky. The gasoline-electric hybrid propulsion system provides long flight endurance with efficient power consumption. The fundamentals of the multirotor and the conventional fixed-wing aircraft present the theoretical background of the aircraft. The accomplished design consists of high-performance multirotor motors with an efficient gasoline engine. Furthermore, the control system architecture, avionics, and power distribution system presented with addressing cost-effective trending design techniques. The performance of the system has been improved using commercially off-the-shelf (COTS) hardware.",https://ieeexplore.ieee.org/document/9508033/,"2021 IEEE International Conference on Robotics, Automation and Artificial Intelligence (RAAI)",21-23 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITNT52450.2021.9649275,Development of a software system for event detection in real-time image analysis,IEEE,Conferences,"The developed software system is necessary for storing and analyzing incoming video streams using traditional methods of image analysis and machine learning methods. The system consists of two parts: the first saves the frames received from the IP camera when it detected motion, second analyzes the saved frames to find objects and further analysis.",https://ieeexplore.ieee.org/document/9649275/,2021 International Conference on Information Technology and Nanotechnology (ITNT),20-24 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AISP53593.2022.9760604,Development of an Inspection Software towards Detection and Location of Cracks and Foreign Objects in Boiler header or Pipes,IEEE,Conferences,"Industry 4.0 offers a radical transformation to increase cost-effective, flexible, and efficient production of higher-quality fully automated systems by collecting and analyzing data across machines. From the last few decades, power industry has started to focus on real-time systems instead of using static methodology in periodical boiler inspection. The power plant undergoes sudden break down due to cracks and foreign bodies causing huge economic loss to the plant as well as the country. To avoid such unforeseen breakdown, most of the power plants has adopted inspection and monitoring system as a regular solution. Visual inspection is one of the most popular techniques for such inspections using a tiny camera with high-power LEDs (Known as Borescope). But it has several limitations for circumferential (360&#x00B0;) and longitudinal (2000mm) coverage and also equidistance inspection from the center of the header is not possible using a conventional Borescope. A specific Digital Video Recorder (DVR) used for the inspection and monitoring is not sufficient to resolve multipurpose requirements such as position of the foreign body and crack, feature of magnification, and more important is data log including plant information and crack details with images. A real-time inspection module has been developed integrated with robotic (AI) based on computer vision to make the inspection dynamic and fully automated.",https://ieeexplore.ieee.org/document/9760604/,2022 2nd International Conference on Artificial Intelligence and Signal Processing (AISP),12-14 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1994.374443,Differential vector quantization of real-time video using entropy-biased ANN codebooks,IEEE,Conferences,"Describes hardware that has been built to compress video in real time using full-search vector quantization (VQ). This architecture implements a differential-vector-quantization (DVQ) algorithm which features entropy-biased codebooks designed using an artificial neural network (ANN). A special-purpose digital associative memory, the VAMPIRE chip, performs the VQ processing. The authors describe the DVQ algorithm, its adaptations for sampled NTSC composite-color video, and details of its hardware implementation. The authors conclude by presenting results drawn from real-time operation of the DVQ hardware.<>",https://ieeexplore.ieee.org/document/374443/,Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94),28 June-2 July 1994,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/I2CT54291.2022.9825379,Digitization of Chess Board and Prediction of Next Move,IEEE,Conferences,"In recent years digital technologies and gadgets have grown so much. Along with other things, the field of Artificial Intelligence has also grown dramatically. Computer vision is one of the most compelling types of AI, which everyone must have experienced in one or the other way like in OCR, Image Recognition, object detection, google lens, etc. One might even come to the assumption that given any image or video to a computer, the computer can understand what&#x2019;s going on in it and can comment a few things on it, just like WE HUMANS. But that assumption is not yet very true. It may be possible in the future but at the present these need some work to become reality. Leveraging this same motivation, we intend to give some contribution for our dream future to become reality. Recognizing and understanding any arbitrary chess board from the image, thereby digitizing it, is the thing that we will be achieving from the proposed system. Along with all that, giving a comment on the current state of chess board, thereby predicting the next optimal move in the game is another thing which is included in proposed system. Think of a human-like robot trying to play chess. Robots must have something like this in order to play the game. In the proposed research work digitization of Chess Board is done by server application from chess board image acquired by user. Current state of the board is generated and sent back as a response to client application. With the help of GUI, the chessboard is generated from the received Forsyth-Edwards Notation (FEN) on the client side. Depending upon blacks turn or whites turn next optimal move to be played is shown to the user. With our proposed system digital instance is generated with accuracy of over 90%. All blank cells of the board are generated with almost 100% accuracy.",https://ieeexplore.ieee.org/document/9825379/,2022 IEEE 7th International conference for Convergence in Technology (I2CT),7-9 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FCCM.2007.25,Discrete-Time Cellular Neural Networks in FPGA,IEEE,Conferences,"This paper describes a novel architecture for the hardware implementation of non-linear multi-layer cellular neural networks. This makes it feasible to design CNNs with millions of neurons accommodated in low price FPGA devices, being able to process standard video in real time.",https://ieeexplore.ieee.org/document/4297271/,15th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 2007),23-25 April 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2013.321,Discriminative Segment Annotation in Weakly Labeled Video,IEEE,Conferences,"The ubiquitous availability of Internet video offers the vision community the exciting opportunity to directly learn localized visual concepts from real-world imagery. Unfortunately, most such attempts are doomed because traditional approaches are ill-suited, both in terms of their computational characteristics and their inability to robustly contend with the label noise that plagues uncurated Internet content. We present CRANE, a weakly supervised algorithm that is specifically designed to learn under such conditions. First, we exploit the asymmetric availability of real-world training data, where small numbers of positive videos tagged with the concept are supplemented with large quantities of unreliable negative data. Second, we ensure that CRANE is robust to label noise, both in terms of tagged videos that fail to contain the concept as well as occasional negative videos that do. Finally, CRANE is highly parallelizable, making it practical to deploy at large scale without sacrificing the quality of the learned solution. Although CRANE is general, this paper focuses on segment annotation, where we show state-of-the-art pixel-level segmentation results on two datasets, one of which includes a training set of spatiotemporal segments from more than 20,000 videos.",https://ieeexplore.ieee.org/document/6619165/,2013 IEEE Conference on Computer Vision and Pattern Recognition,23-28 June 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9649760,Distracted Driver Recognizer with Simple and Efficient Convolutional Neural Network for Real-time System,IEEE,Conferences,"The traffic accident is a big problem in the world and it is happening every day. One of the main causes is distracted driving. Those are the actions of the driver when they are not focusing on driving on the road such as using the cellphone, drinking, makeup, talking to others, etc. For driver warning purpose, this paper proposes a distracted driver recognizer with a simple and efficient Convolutional Neural Network (CNN). The evaluation results on the State Farm Distracted Driver Detection dataset with ten activities achieved an accuracy of 99.51 &#x0025; and on video with the latency allowed for deployment in the real-time system based on a low-computation device.",https://ieeexplore.ieee.org/document/9649760/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2016.7844524,Distributed embedded deep learning based real-time video processing,IEEE,Conferences,"There arises the needs for fast processing of continuous video data using embedded devices, for example the one needed for UAV aerial photography. In this paper, we proposed a distributed embedded platform built with NVIDIA Jetson TX1 using deep learning techniques for real time video processing, mainly for object detection. We design a Storm based distributed real-time computation platform and ran object detection algorithm based on convolutional neural networks. We have evaluated the performance of our platform by conducting real-time object detection on surveillance video. Compared with the high end GPU processing of NVIDIA TITAN X, our platform achieves the same processing speed but a much lower power consumption when doing the same work. At the same time, our platform had a good scalability and fault tolerance, which is suitable for intelligent mobile devices such as unmanned aerial vehicles or self-driving cars.",https://ieeexplore.ieee.org/document/7844524/,"2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",9-12 Oct. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IoT-SIU.2018.8519852,Docker container based analytics at IoT edge Video analytics usecase,IEEE,Conferences,The number of internet connected devices is growing rapidly. It is getting difficult to gather and process the huge volume of the data generated from these devices. Edge computing paradigm is evolving that helps to process the data locally close to the source. This paper highlights using docker container based analytics services at the edge for data processing. Feasibility is studied for setting up deep learning framework on Raspberry Pi for real-time analysis of video feed from the surveillance camera. Performance benchmarking is also done to figure out overhead of containerization.,https://ieeexplore.ieee.org/document/8519852/,2018 3rd International Conference On Internet of Things: Smart Innovation and Usages (IoT-SIU),23-24 Feb. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBDACI.2017.8070802,Dominant feature based convolutional neural network for faces in videos,IEEE,Conferences,"Standard face recognition modules are fabricated for general-purpose applications while few have been designed with speed in mind. This paper proposes an efficient architecture for face recognition in which two self-contained Convolutional Neural Networks (CNNs) are used to detect and recognize faces in regions containing a dense grouping of Features from Accelerated Segment Test (FAST). This configuration proves to be practical for videos as it is selective in its analysis of an input frame. City surveillance and public safety is a critical issue in smart cities and the deployment of Smart Video Surveillance systems is the need of the hour. Typically, the problem at hand will be person identification which is the association of a biometric trait with a particular human being. FAST key points can be generated and analyzed in near real-time and that data can be used to extract and process faces in the background. The CNNs were trained using a combination of datasets of labelled faces, videos and trivial objects. The results obtained upon analyzing the performance of the system on the ChokePoint dataset proved very insightful. This configuration leads to a very effective face recognition system.",https://ieeexplore.ieee.org/document/8070802/,2017 International Conference on Big Data Analytics and Computational Intelligence (ICBDAC),23-25 March 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MASS.2017.47,Dos and Donts Integrated System Based on GPS and Cloud Technologies for Pilgrims,IEEE,Conferences,"All over the world, the crowd management is a big challenge facing the organizations of any event with large number of people. It is clear that this task is becoming more difficult and complicated when the event is larger. In the recent few years, the there are many new technologies developed that can be utilized in crowed management. Among these new technologies is wireless communications, cloud computing, GPS services and many others. Moreover, the wide spread of smart phones and there useful applications made it easier to benefit directly from these technologies. One of the largest worldwide people gathering event is the Pilgrim. Every year, millions of people visit the holy city of Makkah from different countries of the world with different languages and cultural backgrounds. Pilgrim includes certain doings at certain times at certain locations which emphasizes the complexity of crowd management task of the biggest event worldwide. Besides the transportation, housing, and feeding management, there is the dos and donts management of this huge event since it is being a timed and location-based worship. In this research, we utilize the advanced technologies to develop a mobile application to guide the visitors of Makkah (including the elders and those of hearing disability) to perform the correct worship doings and to avoid the worship donts at the holy place of Makkah. The application detects the GPS location of the visitor and sends notifications if he/she commits any donts at that location using a cloud-based decision system. The notifications can be sent as text, image, and video (with deaf sign language integrated). More features of our application, and given that the diversity of Pilgrim, it supports three main languages; Arabic, English, and Urdu, and it is compatible with iOS and Android. The application development process followed the best practices in software engineering that is shown in this paper. To test the application efficiency and functionality, real testing experiments and demos were conducted and presented.",https://ieeexplore.ieee.org/document/8108811/,2017 IEEE 14th International Conference on Mobile Ad Hoc and Sensor Systems (MASS),22-25 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOEI53556.2022.9777182,Driver Drowsiness Detection System using Convolutional Neural Network,IEEE,Conferences,"Driver drowsiness is a condition that impairs judgment. Many accidents have occurred due to sleepy drivers behind the wheel. Multiple studies require drivers to wear health sensors to gather useful physiological health data. This technique is intrusive and makes the driver feel uneasy while driving. Also, this method is less successful since the sensors must remain in specific positions for reliable results. Accurate detection of tiredness and prevention of a potential mishap on the road is a tedious and practically impossible process. Hence this paper proposes to build a driver sleepiness detection system utilizing a behavioral approach to warn drivers before an accident occurs. This system can help reduce traffic accidents and save the lives of drivers. This work trains a Convolutional Neural Network and uses it to assess whether the driver&#x0027;s eyes are closed or open. The dataset comprises images acquired from a large portion of the MRL eye dataset. Before training, the proposed model processes the images in the dataset using computer vision techniques such as edge detection, grayscale conversion, and dilation. The Google MediaPipe Face mesh model is then used to track facial landmarks from video frames in real-time. The eyes region is retrieved, processed, and fed to the proposed trained model for prediction. The model detects drowsiness and alarms the driver to take safety measures. This paper proposes and implements a CNN model that achieves an overall accuracy of 95&#x0025;, outperforming all previous studies on drowsiness detection.",https://ieeexplore.ieee.org/document/9777182/,2022 6th International Conference on Trends in Electronics and Informatics (ICOEI),28-30 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE53722.2022.9823645,Driver Drowsiness Detection Using DLIB,IEEE,Conferences,"The main two reasons for vehicle accidents are Drowsiness and fatigue. They can be avoided by getting adequate sleep before driving, drinking coffee or an energy drink before driving, or taking a break when the indicators of tiredness appear. Complex methods, such as EEG and ECG, are used in the popular drowsiness detection approach. This method offers a high level of measurement accuracy, but it requires touch measurement and has a number of drawbacks when it comes to driver&#x0027;s dizziness monitoring. As a result, it is inconvenient to utilize in real-time driving. The developed system is responsible for detecting the facial area of a video image. The face region is used to narrow down the monitoring of mouth and eyes inside the face area. After the face has been discovered, a dotted map of mouth and eyes is developed by constructing eye by detection of both left and right eye as well as mouth detection. Within the face image, the parameters for detecting both mouth and eyes are created. This project is based on research and falls under the field of Machine Learning to build a system for detecting driver drowsiness in order to reduce accidents caused when driver loses conciseness from sleepiness. The report included the findings and recommendations for the project&#x0027;s limited execution of the various strategies. The project&#x0027;s implementation, on the other hand, provides a practical understanding of how the system works and what changes can be made to improve the overall system&#x0027;s efficiency.",https://ieeexplore.ieee.org/document/9823645/,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),28-29 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAIE.2018.8405495,Driver drowsiness monitoring system using visual behaviour and machine learning,IEEE,Conferences,"Drowsy driving is one of the major causes of road accidents and death. Hence, detection of driver's fatigue and its indication is an active research area. Most of the conventional methods are either vehicle based, or behavioural based or physiological based. Few methods are intrusive and distract the driver, some require expensive sensors and data handling. Therefore, in this study, a low cost, real time driver's drowsiness detection system is developed with acceptable accuracy. In the developed system, a webcam records the video and driver's face is detected in each frame employing image processing techniques. Facial landmarks on the detected face are pointed and subsequently the eye aspect ratio, mouth opening ratio and nose length ratio are computed and depending on their values, drowsiness is detected based on developed adaptive thresholding. Machine learning algorithms have been implemented as well in an offline manner. A sensitivity of 95.58% and specificity of 100% has been achieved in Support Vector Machine based classification.",https://ieeexplore.ieee.org/document/8405495/,2018 IEEE Symposium on Computer Applications & Industrial Electronics (ISCAIE),28-29 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FG47880.2020.00041,DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro Hand Gestures and a Real-Time Recognition Framework,IEEE,Conferences,"The use of hand gestures provides a natural alternative to cumbersome interface devices for Human-Computer Interaction (HCI) systems. However, real-time recognition of dynamic micro hand gestures from video streams is challenging for in-vehicle scenarios since (i) the gestures should be performed naturally without distracting the driver, (ii) micro hand gestures occur within very short time intervals at spatially constrained areas, (iii) the performed gesture should be recognized only once, and (iv) the entire architecture should be designed lightweight as it will be deployed to an embedded system. In this work, we propose an HCI system for dynamic recognition of driver micro hand gestures, which can have a crucial impact in automotive sector especially for safety related issues. For this purpose, we initially collected a dataset named Driver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and infrared modalities. The challenges for dynamic recognition of micro hand gestures have been addressed by proposing a lightweight convolutional neural network (CNN) based architecture which operates online efficiently with a sliding window approach. For the CNN model, several 3-dimensional resource efficient networks are applied and their performances are analyzed. Online recognition of gestures has been performed with 3D-MobileNetV2, which provided the best offline accuracy among the applied networks with similar computational complexities. The final architecture is deployed on a driver simulator operating in real-time. We make DriverMHG dataset and our source code publicly available 1.",https://ieeexplore.ieee.org/document/9320259/,2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020),16-20 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EHB50910.2020.9280165,Drivers’ Drowsiness Detection and Warning Systems for Critical Infrastructures,IEEE,Conferences,"Road traffic accidents, due to driver fatigue, tend to inflict high mortality rates comparing with accidents involving rested drivers. Currently there is an emerging automotive industry trend towards equipping vehicles with various driver-assistance technologies. Third parties also started producing complementary systems, including ones that can detect the driver's degree of fatigue, but this growing field requires further research and development. The main purpose of this paper is the development and implementation of a system capable to detecting and alert, in real-time, the driver's level of fatigue. A system like this is expected to make the driver aware of the assumed danger when his level of driving and taking decisions are reduced and is indicating a sleep break as the necessary approach. By monitoring the state of the human eyes, it is assumed that the signs of driver fatigue can be detected early enough to prevent a possible road accident, which could result in severe injuries or ultimately, in fatalities. Hence, in this work the authors are focused on the video monitoring of the driver face, especially on his eyes position in time, when open or closed, using a machine learning object detection algorithm, the Haar Cascade. Two pretrained Haar classifiers, a face cascade, and an eye cascade were imported from the OpenCV GitHub repository. The OpenCV library, as well as other required packages, were installed on a BeagleBone Black Wireless development board. The software implementation, in order to achieve the driver's drowsiness detection, was made through the Python software program. The proposed system manages to alert if the eyes of the driver are being kept closed for more than a certain amount of time by triggering a set of warning lights and sounds. The large-scale implementation of this type of system will drop the number of road accidents caused by the drivers' fatigue, thus saving countless lives and bringing a reduction of the socio-economic costs associated with these tragic events.",https://ieeexplore.ieee.org/document/9280165/,2020 International Conference on e-Health and Bioengineering (EHB),29-30 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAICS51191.2020.9332476,Drowsy Driver Detection Using Two Stage Convolutional Neural Networks,IEEE,Conferences,"Driver drowsiness and fatigue is one of the most significant causes of road accidents. Accidents involving drowsy drivers have claimed millions of lives in the past years making automated driver drowsiness detection an important computer vision problem. In order to tackle this problem, a real time driver drowsiness detection system is implemented based on the deep learning using Convolutional Neural Network (CNN). In the proposed method, drowsiness detection is treated as an object detection and classification task. In this method, detection and localization of face region is done using the YOLOv3 real-time object detection algorithm, while the Inception-v3 pre-trained neural network is used to classify the detected face as either drowsy or non-drowsy. The deep learning model was trained and tested on the standard datasets: Closed Eyes in the Wild (CEW) database, National Tsuing Hua University (NTHU) Driver Drowsiness Detection database and a custom database. The proposed methodology gives an accuracy of 80.32%, 79.34%and 89.90% respectively, on the three databases. Proposed system can process the input video stream in real-time without any expensive hardware like GPUs and hence is computationally efficient and cost effective, since it is able to process an incoming video stream in real-time, on a standalone device, without the need for any expensive hardware support. Also the methodology adopted does not involve face feature extraction which is a common procedure in most of the vision based drowsiness detection systems, making the model efficient without compromising on its prediction speed or accuracy.",https://ieeexplore.ieee.org/document/9332476/,2020 IEEE Recent Advances in Intelligent Computational Systems (RAICS),3-5 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2012.6359563,Dual background modeling of traffic image based on LBP and Gaussian,IEEE,Conferences,"The detection of moving objects is a key step in the traffic video monitoring system. The most common way to detect moving objects is background subtraction and the critical technique is the background modeling. In this paper, we propose a method combining LBP with Gauss for the detection of the moving objects. We adopt the method of parallel processing in order to improve the processing speed in the implementation of the algorithm. And the video sequences are used to test the proposed method. Experiments show that our methods have high real-time in background updating.",https://ieeexplore.ieee.org/document/6359563/,2012 International Conference on Machine Learning and Cybernetics,15-17 July 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCC54389.2021.9674459,Dupopt: A Redundancy Video Analysis Mechanism,IEEE,Conferences,"The video analysis based on Deep Neural Network (DNN) model has been widely deployed to provide various services in mobile applications. DNN-based video analysis with high accuracy requires massive computation power beyond the hardware capability of mobile devices. Offloading to an edge server or remote cloud server is a promising solution to solve the above challenge. However, offloading computation leads to higher network latency which will reduce analysis accuracy. This paper uses redundant transmission to reduce network latency in video analysis and proposes a mechanism named Dupopt, which can select the optimal combination of copy number and frame resolution to maximize analysis accuracy. Numerical results show that the mechanism Dupopt can significantly improve the accuracy of video analysis compared with the transmission mechanism without redundancy.",https://ieeexplore.ieee.org/document/9674459/,2021 7th International Conference on Computer and Communications (ICCC),10-13 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA48506.2021.9560730,Dynamic Object Aware LiDAR SLAM based on Automatic Generation of Training Data,IEEE,Conferences,"Highly dynamic environments, with moving objects such as cars or humans, can pose a performance challenge for LiDAR SLAM systems that assume largely static scenes. To overcome this challenge and support the deployment of robots in real world scenarios, we propose a complete solution for a dynamic object aware LiDAR SLAM algorithm. This is achieved by leveraging a real-time capable neural network that can detect dynamic objects, thus allowing our system to deal with them explicitly. To efficiently generate the necessary training data which is key to our approach, we present a novel end-to-end occupancy grid based pipeline that can automatically label a wide variety of arbitrary dynamic objects. Our solution can thus generalize to different environments without the need for expensive manual labeling and at the same time avoids assumptions about the presence of a predefined set of known objects in the scene. Using this technique, we automatically label over 12000 LiDAR scans collected in an urban environment with a large amount of pedestrians and use this data to train a neural network, achieving an average segmentation IoU of 0.82. We show that explicitly dealing with dynamic objects can improve the LiDAR SLAM odometry performance by 39.6% while yielding maps which better represent the environments. A supplementary video1 as well as our test data2 are available online.",https://ieeexplore.ieee.org/document/9560730/,2021 IEEE International Conference on Robotics and Automation (ICRA),30 May-5 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMSNETS53615.2022.9668605,Dynamic Split Computing of PoseNet Inference for Fitness Applications in Home IoT-Edge Platform,IEEE,Conferences,"In the next generation wireless networks, implementations of advanced computation tasks such as running Deep Neural Network (DNN) models in Internet of Things (IoT) devices is a challenging task due to their limited computation & processing capabilities. To address this issue, we propose an Extended Dynamic Split Computation (E-DSC) algorithm which finds an optimal partition point of DNN inference layer based on the available network bandwidth; where one inference sub-model is computed at IoT device and the other inference sub-model is computed by a home edge. To validate the E-DSC algorithm, we consider a fitness application as a use-case. The application captures live video of a person performing an arm exercise using a camera attached to a Raspberry-Pi (RPi) considered as an IoT device. The PoseNet DNN model is used to estimate the pose of the person using split-inference of the DNN are partitioned among RPi and Samsung Galaxy S20 device (considered as a home edge). The S20 device sends the final inference result back to the RPi which then checks if the exercise is done correctly and displays the exercise count on a television/monitor. We conduct extensive experiments to compare the performance of PoseNet inference execution on RPi device, Galaxy S20 device and performing dynamic split using E-DSC algorithm over Wi-Fi networks in a real time deployment scenario.",https://ieeexplore.ieee.org/document/9668605/,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),4-8 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPSR54439.2022.9831237,Dynamic Task Division and Allocation in Mobile Edge Computing Systems: A Latency Oriented Approach via Deep Q-Learning Network,IEEE,Conferences,"With the rapid development of Internet of Things (IoTs), various sensors are deployed to collect different physical information. Smart surveillance is one of applications by analyzing the real-time video generated by camera sensors. However, due to the limited computing capability of camera sensors, running video analysis models (e.g., AlexNet and YOLO3) on camera sensors directly consumes a lot of computing time. In addition, transferring video to the remote cloud suffers a long-distance transmission latency. Fortunately, edge computing has been considered as a promising solution for enabling computation-intensive yet latency-sensitive applications at resource-constrained devices. Thanks to edge computing, camera sensors can upload video to different edge servers employed at the edge of networks for processing. Moreover, the lightweight Kubernetes for edge computing, i.e., K3s, enable a fine-grained task division and parallel computing. In this paper, we consider a heterogeneous edge cooperative video analysis, i.e., face recognition, with the objective of minimizing the processing latency. Specifically, we use a Deep Q-Learning network (DQN) to dynamically adjust the size of pieces video allocated to different edge servers connected via wireless networks. In addition, to improve the resource utilization of edge servers and reduce the processing latency, each edge server further divides the received video into multiple segments that are processed by different containers in parallel. To validate the effectiveness of our scheme, we implement a small-scale prototype system and conduct numerous experiments. Experimental results show that our proposed algorithm outperforms the other four schedule schemes by testing on the tasks of face recognition and pose recognition.",https://ieeexplore.ieee.org/document/9831237/,2022 IEEE 23rd International Conference on High Performance Switching and Routing (HPSR),6-8 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IC3.2015.7346705,Dynamic facial emotion recognition from 4D video sequences,IEEE,Conferences,"Emotions are characterized as responses to internal and external events of a person. Emotion recognition through facial expressions from videos plays a vital role in human computer interaction where the dynamic changes in face movements needs to be realized quickly. In this work, we propose a simple method, using the geometrical based approach for the recognition of six basic emotions in video sequences of BU-4DFE database. We have chosen optimum feature points out of the 83 feature points provided in the BU-4DFE database. A video expressing emotion will have frames containing neutral, onset, apex and offset of that emotion. We have dynamically identified the frame that is most expressive for an emotion (apex). The Euclidean distance between the feature points in apex and neutral frame is determined and their difference in corresponding neutral and the apex frame is calculated to form the feature vector. The feature vectors thus formed for all the emotions and subjects are given to Neural Networks (NN) and Support Vector Machine (SVM) with different kernels for classification. We have compared the accuracy obtained by NN & SVM. Our proposed method is simple, uses only two frames and yields good accuracy for BU-4DFE database. Very complex algorithms exist in literature using BU-4DFE database and our proposed simple method gives comparable results. It can be applied for real time implementation and kinesics in future.",https://ieeexplore.ieee.org/document/7346705/,2015 Eighth International Conference on Contemporary Computing (IC3),20-22 Aug. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2008.5035620,Dynamic formations in real-time strategy games,IEEE,Conferences,"Current approaches to organising units in strategic video games are typically implemented via static formations. Static formations are not capable of adapting effectively to opponent tactics. In this paper we discuss an approach to organising units by learning the effectiveness of a formation in actual play, and directly applying learned formations according to the classification of the opponent player. This approach to establish so-called dynamic formations, is tested in the ORTS game environment. From our results, we may conclude that the approach to established dynamic formations can be successfully applied in actual video-game environments.",https://ieeexplore.ieee.org/document/5035620/,2008 IEEE Symposium On Computational Intelligence and Games,15-18 Dec. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/APSIPA.2018.8659738,EEG Hyperscanning for Eight or more Persons - Feasibility Study for Emotion Recognition using Deep Learning Technique,IEEE,Conferences,"Multi-user electroencephalogram (EEG) system is necessary to study concurrent activity among many persons. It is difficult to find a system that measures multiple EEG signals from more than even three people simultaneously. Therefore, we suggested a framework that is able to acquire EEG signals of more than eight persons at the same time and investigated the feasibility of this system. Acquisition was performed by using OpenViBE software developed by INRIA. Wireless EEG devices for our proposed framework were manufactured by BioBrain, Corp. in Korea. A device consists of eight channels measuring frontal EEG at a speed of 1 KHz sampling rate. While participants wore this system and did emotional video watching task as a group audience, their brain signals were acquired. To show its feasibility and efficacy, our preliminary result is analyzed using deep learning technique.",https://ieeexplore.ieee.org/document/8659738/,2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),12-15 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData50022.2020.9378219,EGAD: Evolving Graph Representation Learning with Self-Attention and Knowledge Distillation for Live Video Streaming Events,IEEE,Conferences,"In this study, we present a dynamic graph representation learning model on weighted graphs to accurately predict the network capacity of connections between viewers in a live video streaming event. We propose EGAD, a neural network architecture to capture the graph evolution by introducing a self-attention mechanism on the weights between consecutive graph convolutional networks. In addition, we account for the fact that neural architectures require a huge amount of parameters to train, thus increasing the online inference latency and negatively influencing the user experience in a live video streaming event. To address the problem of the high online inference of a vast number of parameters, we propose a knowledge distillation strategy. In particular, we design a distillation loss function, aiming to first pretrain a teacher model on offline data, and then transfer the knowledge from the teacher to a smaller student model with less parameters. We evaluate our proposed model on the link prediction task on three real-world datasets, generated by live video streaming events. The events lasted 80 minutes and each viewer exploited the distribution solution provided by the company Hive Streaming AB. The experiments demonstrate the effectiveness of the proposed model in terms of link prediction accuracy and number of required parameters, when evaluated against state-of-the-art approaches. In addition, we study the distillation performance of the proposed model in terms of compression ratio for different distillation strategies, where we show that the proposed model can achieve a compression ratio up to 15:100, preserving high link prediction accuracy. For reproduction purposes, our evaluation datasets and implementation are publicly available at https://stefanosantaris.github.io/EGAD.",https://ieeexplore.ieee.org/document/9378219/,2020 IEEE International Conference on Big Data (Big Data),10-13 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTON.2016.7550672,Managing services in the telecom cloud: An example for CDN,IEEE,Conferences,"Telecom operators are considering the deployment of Content Delivery Networks (CDN) to better control and manage video contents injected into the network. Cache nodes placed close to end users can manage access contents and adapt them to users' devices, while reducing video traffic in the core. By adopting the recently standardized MPEG-DASH technique, video contents can be delivered over HTTP, so HTTP servers can be used to serve contents, while packagers running as software can prepare live contents. This paves the way for virtualizing the CDN function. In this talk, a CDN manager is proposed to adapt the virtualized CDN function to current and future demand. A Big Data architecture, fulfilling the ETSI NFV guidelines, allows controlling virtualized components while collecting and pre-processing data. Re-optimization problems minimizing CDN costs while ensuring the highest quality are triggered based on threshold violations; data stream mining sketches transform collected into modelled data and statistical linear regression and machine learning techniques are proposed to produce estimation of future scenarios. Exhaustive simulation over a realistic scenario reveal remarkable costs reduction by dynamically reconfiguring the CDN. Finally, the feasibility of the proposed architecture is experimentally assessed in a real environment.",https://ieeexplore.ieee.org/document/7550672/,2016 18th International Conference on Transparent Optical Networks (ICTON),10-14 July 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMNWC52512.2021.9688542,Masked & Unmasked Face Recognition Using Support Vector Machine Classifier,IEEE,Conferences,"Face masks become a need in epidemic scenarios such as the Corona virus pandemic of 2020-21. Most companies prefer face authentication instead of fingerprint, signature, and card verification. Face mask gives protection against Corona virus than other traditional methods used for identification. In the case of facial recognition, a machine must detect and recognise the face in a picture. In this paper used methods are supported by machine learning that permits a machine to evolve through a learning process and to perform recognition tasks. Caffe model of deep learning is used for face detection. The training dataset contains both masked and non-masked faces. This project and outcome has developed an approach to recognize faces in a real-time video stream that can also be used in the existing recognition systems to identify masked faces. Facial recognition has been done with a Support Vector Machine classifier. All are implemented in Python with OpenCv with tools modules and sub-modules.",https://ieeexplore.ieee.org/document/9688542/,2021 IEEE International Conference on Mobile Networks and Wireless Communications (ICMNWC),3-4 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA51294.2020.00021,MaskedFusion: Mask-based 6D Object Pose Estimation,IEEE,Conferences,"MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D data, with an architecture that leverages multiple sub-tasks in a pipeline to achieve accurate 6D poses. 6D pose estimation is an open challenge due to complex world objects and many possible problems when capturing data from the real world, e.g., occlusions, truncations, and noise in the data. Achieving accurate 6D poses will improve results in other open problems like robot grasping or positioning objects in augmented reality. MaskedFusion improves the state-of-the-art by using object masks to eliminate non-relevant data. With the inclusion of the masks on the neural network that estimates the 6D pose of an object we also have features that represent the object shape. MaskedFusion is a modular pipeline where each sub-task can have different methods that achieve the objective. MaskedFusion achieved 97.3% on average using the ADD metric on the LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset, which is an improvement, compared to the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9356139/,2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA),14-17 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSNT.2012.6526229,Miner face detection is based on improved AdaBoost algorithm,IEEE,Conferences,"This article connects with Coal mine video monitoring image be impacted for special environment, which be vulnerable to mineral dust in coal mines, light, as well as miner's safety helmet for the realization of face detection in real-time and accuracy, I will study on face identification and analysis on the characters of behavior in the follow-up work for getting a good foundation, which will be in intelligent Coal mine video monitoring. This article simulates rectangle Haar-like character and Extended Haar-like character of the AdaBoost algorithm about face detection in real-time and accuracy, is based on OpenCV, also describes briefly the rectangular Haar-like characteristic model and about computational algorithm and faster algorithm of the characteristic value, analysis detailedly extended Haar-like character model and the characteristic value of computational algorithm-integral image. Experimental resulted show that extended Haar-like characteristic model can be implemented more quickly and more accurately in the miners' face detection, as well as real-time.",https://ieeexplore.ieee.org/document/6526229/,Proceedings of 2012 2nd International Conference on Computer Science and Network Technology,29-31 Dec. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IICAIET51634.2021.9573772,Mobile Machine Vision for Railway Surveillance System using Deep Learning Algorithm,IEEE,Conferences,"Trains have been a popular transportation in our daily life. However, there is no proper surveillance system for obstacle detection at the railway, leading to the happen of unwanted accidents. In order to overcome this issue, machine vision embedded with deep learning algorithm can be implemented. Obstacle detection can be achieved through vision-based object detection, where the object classification model computes the images similarity to its respective classes, classifying its potential as an obstacle. In this paper, object detection model is developed and implemented with deep learning algorithm. Object classification model is produced through the model training with Deep Neural Networks (DNN). The detection model used in this paper is Single-Shot multibox Detection (SSD) MobileNet detection model. This model can be implemented with Raspberry Pi to simulate the object detection algorithm virtually. During simulation, the object recognition algorithm is able to detect and classify various objects into its respective classes. By applying past research approaches, the developed object detection model is able to analyze image as well as real-time video feed to identify multiple objects. Any object that has been detected at the Region of Interest (ROI) can be characterized as an obstacle.",https://ieeexplore.ieee.org/document/9573772/,2021 IEEE International Conference on Artificial Intelligence in Engineering and Technology (IICAIET),13-15 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMITCon.2019.8862200,Modified Architecture for Detection of Moving Objects,IEEE,Conferences,"Object detection is one key aspect in most computer vision applications. The most common technique used for moving article identification is backdrop subtraction. Designing a robust algorithm for background subtraction has to overcome many obstacles primarily in form of illumination variations, poor video quality leading to noise, dynamic background, etc. This paper proposes a modified architecture using background subtraction and frame difference technique for the identification of moving articles. The proposed technique helps to adapt to sudden transformations in the background image. The results exhibit that the suggested method is simple, efficient and real-time with better operational speed and more accuracy. The entire design is implemented on Zed board manufactured by Xilinix consisting of Zynq 7000 FPGA using Vivado 2018.1.",https://ieeexplore.ieee.org/document/8862200/,"2019 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing (COMITCon)",14-16 Feb. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NUICONE.2012.6493235,Modified architecture for real-time face detection using FPGA,IEEE,Conferences,"In this paper, we introduce modified hardware architecture with key features of lessening the resource usage of the FPGA and elevating the face detection frame rate. The system is based on well-known Viola Jones Framework which consists of AdaBoost algorithm integrated with Haar features. We also enlist the modification in hardware design techniques to achieve more parallel processing and higher detection speed of the system. The system implemented on Xilinx Virtex-5 FPGA development board outputs a high face detection rate (91.3%) at 60 frame/second for a VGA (640 × 480) video source. The power consumption of the implementation is 2.1 W.",https://ieeexplore.ieee.org/document/6493235/,2012 Nirma University International Conference on Engineering (NUiCONE),6-8 Dec. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCIS51004.2021.9397180,Monitoring Driver’s Drowsiness Status at Night Based on Computer Vision,IEEE,Conferences,"Drivers drowsiness and fatigue decreases the vehicle management skills of a driver. The operator driving vehicle in night has become a significant downside today. Driver in a drowsiness state is the one among the important reason of increasing amount of road accidents and death. Hence the drowsiness detection of driver is considering as most active research field. Many ways are created recently to detect the drowsiness of driver. Existing methods can be classified in three categories based on physiological measures, performance measures of vehicles and ocular measures. Few ways are intrusive and distract the driver from comfortable driving. Some of the methods need expensive sensors for information handling. Therefore, a low cost, real time system to detect the driver's drowsiness is developed in this paper. In this proposed system, real time video of driver records using a digital camera. Using some image processing techniques, face of the driver is detected in each frame of video. Facial landmarks points on the driver's face is localized using one shape predictor and calculating eye aspect ratio, mouth opening ratio, yawning frequency subsequently. Drowsiness is detected based on the values of these parameters. Adaptive thresholding method is used to set the thresholds. Machine learning algorithms were also implemented in an offline manner. Proposed system tested on the Face Dataset and also tested in real-time. The experimental results shows that the system is accurate and robust.",https://ieeexplore.ieee.org/document/9397180/,"2021 International Conference on Computing, Communication, and Intelligent Systems (ICCCIS)",19-20 Feb. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9635912,Monocular Teach-and-Repeat Navigation using a Deep Steering Network with Scale Estimation,IEEE,Conferences,"This paper proposes a novel monocular teach-and-repeat navigation system with the capability of scale awareness, i.e. the absolute distance between observation and goal images. It decomposes the navigation task into a sequence of visual servoing sub-tasks to approach consecutive goal/node images in a topological map. To be specific, a novel hybrid model, named deep steering network is proposed to infer the navigation primitives according to the learned local feature and scale for each visual servoing sub-task. A novel architecture, Scale-Transformer, is developed to estimate the absolute scale between the observation and goal image pair from a set of matched deep representations to assist repeating navigation. The experiments demonstrate that our scale-aware teach-and-repeat method achieves satisfying navigation accuracy, and converges faster than the monocular methods without scale correction given an inaccurate initial pose. The proposed network is integrated into an onboard system deployed on a real robot to achieve real-time navigation in a real environment. A demonstration video can be found online: https://youtu.be/ctlwDaMKnHw",https://ieeexplore.ieee.org/document/9635912/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2017.8080415,Monte Carlo tree search based algorithms for dynamic difficulty adjustment,IEEE,Conferences,"Maintaining player immersion is a crucial step in making an enjoyable video game. One aspect of player immersion is the level of challenge the game presents to the player. To avoid a mismatch between a player's skill and the challenge of a game, which can result from traditional manual difficulty selection mechanisms (e.g. easy, medium, hard), Dynamic Dif­ficulty Adjustment (DDA) has previously been proposed as a means of automatically detecting a player's skill and adjusting the level of challenge the game presents accordingly. This work contributes to the field of DDA by proposing a novel approach to artificially intelligent agents for opponent control. Specifically, we propose four new DDA Artificially Intelligent (AI) agents: Reactive Outcome Sensitive Action Selection (Reactive OSAS), Proactive OSAS, and their ""True"" variants. These agents provide the player with an level of difficulty tailored to their skill in real-time by altering the action selection policy and the heuristic playout evaluation of Monte Carlo Tree Search. The DDA AI agents are tested within the FightingICE engine, which has been used in the past as an environment for AI agent competitions. The results of the experiments against other AI agents and human players show that these novel DDA AI agents can adjust the level of difficulty in real-time, by targeting a zero health difference as the outcome of the fighting game. This work also demonstrates the trade-off existing between targeting the outcome exactly (Reactive OSAS) and introducing proactive behaviour (i.e., the DDA AI agent fights even if the health difference is zero) to increase the agents believability (Proactive OSAS).",https://ieeexplore.ieee.org/document/8080415/,2017 IEEE Conference on Computational Intelligence and Games (CIG),22-25 Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNISC54316.2021.00053,Motion Detection and Object Detection: Yolo (You Only Look Once),IEEE,Conferences,"Recently, the field of artificial intelligence has seen many advances thanks to deep learning and image processing. It is now possible to recognize images or even find objects inside an image with a standard GPU. Image processing is a recent science that aims to provide specialists from different areas, as to the general public, tools for manipulating these digital data from the real world. The detection of moving objects is a crucial step for systems based on image processing. The movements detected by the classic algorithms are not necessarily interesting for a thorough information search, and the need to distinguish the coherent movements of parasitic movements exists in most cases. In this paper we are going to use a simply webcam and YOLO algorithm for this implementation. The YOLOv3 (Version 3) model makes predictions with a single network evaluation, making this method extremely fast, running in real time with a capable GPU. From there we&#x0027;ll use OpenCV, Python, and deep learning to apply the YOLOv3 object to images and apply YOLOv3 to video streams.",https://ieeexplore.ieee.org/document/9603899/,2021 7th Annual International Conference on Network and Information Systems for Computers (ICNISC),23-25 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ELECSYM.2019.8901580,Moving Objects Counting Dashboard Web Application Design,IEEE,Conferences,"The ability to detect and count people using digital image processing techniques has a lot of applications. Retail stores need to understand foot traffic to organise their goods, understand peak times or prevent theft. Business or government agencies could use the counter to design or manage buildings and other public places. A system of object detection algorithm runs on CCTV footages needs a good platform to display the result that easy to access and interactive. This paper will explain the design of dashboard web application used to display the number of objects in a CCTV video and in our implementation the objects are people in a building and vehicles on the traffic intersections. The main focus of this paper is the novel software architecture that is consisted of four different programs that are communicating with each other. Those programs are publisher, object detection, video combiner, and dashboard program. Object detection system is using YOLOv3, an algorithm using deep learning and convolutional neural networks (CNN) that can detect objects fast and real-time. The terminology ""real-time"" means that the web application can detect the number of objects in a live CCTV video with few seconds of delay. The computer specification that is used in the test to achieve real-time result is Intel Xeon E3 CPU, NVIDIA GTX1060 GPU, and 8 GB of RAM. OpenCV, an open source library for computer vision and machine learning is also used. The dashboard design is using Dash by Plotly, an open source Python based framework to create web application.",https://ieeexplore.ieee.org/document/8901580/,2019 International Electronics Symposium (IES),27-28 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEC.2011.5949599,Ms Pac-Man versus Ghost Team CEC 2011 competition,IEEE,Conferences,"Games provide an ideal test bed for computational intelligence and significant progress has been made in recent years, most notably in games such as Go, where the level of play is now competitive with expert human play on smaller boards. Recently, a significantly more complex class of games has received increasing attention: real-time video games. These games pose many new challenges, including strict time constraints, simultaneous moves and open-endedness. Unlike in traditional board games, computational play is generally unable to compete with human players. One driving force in improving the overall performance of artificial intelligence players are game competitions where practitioners may evaluate and compare their methods against those submitted by others and possibly human players as well. In this pa per we introduce a new competition based on the popular arcade video game Ms Pac-Man: Ms Pac-Man versus Ghost Team. The competition, to be held at the Congress on Evolutionary Computation 2011 for the first time, allows participants to develop controllers for either the Ms Pac-Man agent or for the Ghost Team and unlike previous Ms Pac-Man competitions that relied on screen capture, the players now interface directly with the game engine. In this paper we introduce the competition, including a review of previous work as well as a discussion of several aspects regarding the setting up of the game competition itself.",https://ieeexplore.ieee.org/document/5949599/,2011 IEEE Congress of Evolutionary Computation (CEC),5-8 June 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS52415.2021.9466014,Multi-Pole Road Sign Detection Based on Faster Region-based Convolutional Neural Network (Faster R-CNN),IEEE,Conferences,"Building an approach system that is able to serve various types of traffic signs is a challenge. The important stages in handling an object are finding objects, dividing them into several categories, and marking objects with bounding boxes. However, in reality, monitoring traffic signs objects is quite difficult because it is based on various factors such as; other closed objects, driving times, or traffic sign conditions. This study aims to measure the level of precision in monitoring traffic signs (detection speed of 4-6 frames per second) from video recording (single camera) using the Faster Region based Convolutional Neural Network (Faster R-CNN) algorithm. The traffic sign detection system uses the Faster R-CNN algorithm with Inception v2 model which is implemented in the TensorFlow API framework. The Faster R-CNN consists of 2 different modules. The first module is a deep convolutional neural network which functions to build the area to be detected, which is called the Regional Proposal Network (RPN), and the second module is the Fast R-CNN detector which functions to use the previously proposed area. This system is one unit, a detection network based on the results of the manufacture and testing of a traffic sign detection system based on the Faster R-CNN method, so it can be shown that there is no difference in the results of detection of traffic signs in day and night conditions. Where the precision testing for detection of traffic signs during the day and at night is 100%.",https://ieeexplore.ieee.org/document/9466014/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI50040.2020.00088,Multi-Robot Collision Avoidance with Map-based Deep Reinforcement Learning,IEEE,Conferences,"Multi-robot collision avoidance in a communication-free environment is one of the key issues for mobile robotics and autonomous driving. In this paper, we propose a map-based deep reinforcement learning (DRL) approach for collision avoidance of multiple robots, where robots do not communicate with each other and only sense other robots' positions and the obstacles around them. We use the egocentric grid map of a robot to represent the environmental information around it, which can be easily generated by using multiple sensors or sensor fusion. The learned policy generated from the DRL model directly maps 3 frames of egocentric grid maps and the robot's relative local goal positions into low-level robot control commands. We first train a convolutional neural network for the navigation policy in a simulator of multiple mobile robots using proximal policy optimization (PPO). Then we deploy the trained model to real robots to perform collision avoidance in their navigation. We evaluate the approach with various scenarios both in the simulator and on three differential-drive mobile robots in the real world. Both qualitative and quantitative experiments show that our approach is efficient with a high success rate. The demonstration video can be found at https://youtu.be/jcLKlEXuFuk.",https://ieeexplore.ieee.org/document/9288300/,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI.2011.89,Multi-agent Simulation Design Driven by Real Observations and Clustering Techniques,IEEE,Conferences,"The multi-agent simulation consists in using a set of interacting agents to reproduce the dynamics and the evolution of the phenomena that we seek to simulate. It is considered now as an alternative to classical simulations based on analytical models. But, its implementation remains difficult, particularly in terms of behaviors extraction and agents modelling. This task is usually performed by the designer who has some expertise and available observation data on the process. In this paper, we propose a novel way to make use of the observations of real world agents to model simulated agents. The modelling is based on clustering techniques. Our approach is illustrated through an example in which the behaviors of agents are extracted as trajectories and destinations from video sequences analysis. This methodology is investigated with the aim to apply it, in particular, in a retail space simulation for the evaluation of marketing strategies. This paper presents experiments of our methodology in the context of a public area modelling.",https://ieeexplore.ieee.org/document/6103379/,2011 IEEE 23rd International Conference on Tools with Artificial Intelligence,7-9 Nov. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAMP.2000.875968,Multi-sensors and environment simulator for collision avoidance applications,IEEE,Conferences,"The ""PICAR"" project is based on ""Prometheus Prochip"" experience and results. The goal is to design an embedded multi sensors collision avoidance system for automotive application. The system includes sensors like video camera, ultrasonic sensors, a PC hardware computer, a CAN/sup 2/ network and a dedicated software for signal and image processing, data fusion and AI expert system. The design of such a system is difficult under real time constraints. Therefore, a simulator is a good solution to test different parts of the system and then to help to choose the overall architecture. However simulating all the embedded architecture in real-time becomes a very complex work. It is necessary to have an environment simulator where sensors can be virtually implemented. The data are then processed leading to results without hardware costs. The designed environment software allows the simulation of physical sensors, and also the emulation of these sensors. The simulator is interfaced to the physical embedded hardware by a network bridge. So, we can emulate some sensors to experiment data processing on the physical embedded PICAR computer. This paper presents the physical smart car PICAR and its embedded system, the virtual world simulator. We explain how we can mix the virtual world (produced by virtual sensors) and real world (physical embedded system) to implement some scenarios (like automatic parking) and to validate the physical architecture. An alternate goal can be to design customized sensors.",https://ieeexplore.ieee.org/document/875968/,Proceedings Fifth IEEE International Workshop on Computer Architectures for Machine Perception,11-13 Sept. 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CAC53003.2021.9728242,Multichannel real-time video container numbers recogntion in container yard,IEEE,Conferences,"The improvement of automation level of container yard operation is conducive to the improvement of operation efficiency and safety of container terminal and port. In practical engineering application, in the case of limited computing power, how to achieve the balance between speed and accuracy is worth discussing. This paper focus on how to balance the accuracy and speed for the real-time video container numbers recognition during monitoring. Through the research on the methods of text detection and text recogntion in deep learning, DBNet is used to the detection of container number, and SAR is used to the recognition of container number. In order to meet the real-time requirements, the feature extraction part of DBNet is modified to the lightweight network MobileNetV3, the feature fusion part is modified to the FPEM and FFM module, and the loss function is modified to DiceLoss; The feature extraction part of SAR is modified to lightweight network MobileNetV3, the sequence feature extraction part is modified to GRU module, and more data augmentation modules are added to enhance the generalization ability of the model. Experiments in container images dataset collected during the actual operation show that the proposed scheme can meet the requirements of accuracy and speed at the same time. Finally, through the application verification of multi-channel video monitoring and real-time container number recognition in container terminal yard, the scheme is implemented on the platform of Xeon e5-2690V3, GPU: RTX2080super(8G), memory of computer: 16G, in the case of reasoning using the shortest edge 1080, one recognition cost time is about 200ms, the video memory occupation include detection and recogntion is about 2.0G, and the recognition accuracy is more than 95%.",https://ieeexplore.ieee.org/document/9728242/,2021 China Automation Congress (CAC),22-24 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONECCT.2018.8482366,Multilayer vehicle classification integrated with single frame optimized object detection framework using CNN based deep learning architecture,IEEE,Conferences,"Here we have rendered a functional and architectural model of a system that assists the driver of a vehicle to detect, identify and track objects while driving. The objects detected include vehicle type as well as common on-road objects such as pedestrians. Layer structure for the system involves the design of a state-of-the-art deep learning classifier using a novel database for obtaining higher classification accuracy and another layer consisting of a single-frame object detection method to make the system more robust while limiting the processing time involved. Sub-systems integrated to facilitate the driver with relevant real-time information about his driving umwelt include vehicle identifier, number plate recognition system and creation of database consisting of collected information along with time-stamp. Performance degradation under various ambient conditions and variable environments with various synthetic noises being introduced in the video frames have been studied. Trade-off between speed and accuracy of a state-of-the-art real-time detection system implemented on various processing platforms is studied. Layers of deep learning classifier were trained using an optimized dataset consisting of static and dynamic images of vehicles to yield suitable prediction accuracy and this was combined with a system pre-trained on COCO dataset for YOLO. This helped complete the Intelligent Driver Assistant System. This paper also includes the implementation of real-time object detection on a single board computer. This concept can be tapped to create compact and portable driver assistant systems.",https://ieeexplore.ieee.org/document/8482366/,"2018 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",16-17 March 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISKE.2010.5680764,Multilevel fuzzy navigation control scheme applied to a monitoring mobile robot,IEEE,Conferences,"The design, construction and real time performance of a mobile monitoring system based on a Khepera mobile robot are presented. The functions performed by the system are: (a) line following, (b) obstacle avoidance, (c) identification of test points along the path, (d) recognition of the mark (bar code) located at each test point and, (e) measuring of a physical parameter. For the navigation, an innovative multilevel fuzzy control scheme is implemented in which the fuzzy sensor fusion, related to the perception of the environment, reduces the complexity of the navigation function. Other distinctive characteristics are the identification of test points by means of a Kohonen's neural network and the processing of a one-dimensional video signal for recognition of landmarks located at each test point.",https://ieeexplore.ieee.org/document/5680764/,2010 IEEE International Conference on Intelligent Systems and Knowledge Engineering,15-16 Nov. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC48978.2021.9564792,Multimodal human interaction analysis in vehicle cockpit,IEEE,Conferences,"Nowadays, every car maker is thinking about the future of mobility. Electric vehicles, autonomous vehicles and sharing vehicles are one of the most promising opportunities. The lack of authority in autonomous and sharing vehicles raises different issues from which one of the main issues is passenger safety. To ensure it, new systems able to understand interactions and possible conflicts between passengers have to be designed. They should be able to predict critical situations in the car cockpit, and alert remote controllers to act accordingly. In order to better understand the features of these insecure situations, we recorded an audio-video dataset in real vehicle context. Twenty-two participants playing three different scenarios (“curious”, “argued refusal” and “not argued refusal”) of interactions between a driver and a passenger were recorded. We propose a deep learning model to identify conflict situations in a car cockpit. Our approach achieves a balanced accuracy of 81%. Practically, we highlight the importance that combining multimodality namely video, audio and text as well as temporality are the keys to perform such accurate predictions in scenario recognitinn.",https://ieeexplore.ieee.org/document/9564792/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ANDESCON50619.2020.9272196,Multipurpose unmanned system: an efficient solution to increase the capabilities of the UAVs,IEEE,Conferences,"The results of this research propose the implementation of a system that significantly increases the capacity of unmanned vehicles, turning them into multifunctional vehicles. The system has a logistics dispatch module and a video analytics module. The first module allows the delivery of medical, food, smoke, disinfectant, etc. The module is practical, safe and economical, features that denote the feasibility of immediate implementation in unmanned vehicles of any rank and / or classification. Note that the implementation of the dispatch module does not require additional radio frequency systems. The second module includes a video analysis process in real time, an aspect that constitutes a significant contribution to the proposed solution, since it allows obtaining important information during the flight; it also reduces the risk in air operations and simultaneously increases the efficiency of themselves. Note that video analytics optimizes resources and avoids jeopardizing the lives of aircraft pilots and crews who traditionally should carry out these activities. In times of pandemic, this innovation avoids direct contact with an infected population and can guarantee the sanitary conditions required in certain circumstances. The solution increases the capabilities of unmanned vehicles and makes them useful tools in various scenarios, whether caused by natural or man-made disasters. Our proposal is very flexible, reliable, and scalable and can be adapted to various models and makes of unmanned vehicles. The system has been implemented on fixed-wing and rotary-wing unmanned vehicles, showing satisfactory results.",https://ieeexplore.ieee.org/document/9272196/,2020 IEEE ANDESCON,13-16 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICADEE51157.2020.9368944,NON-CONTACT PULSE RATE MEASUREMENT USING FACIAL VIDEOS,IEEE,Conferences,"Pulse rate (PR) is one of the vital physiological parameters which indicates the physiological state of individuals thus proving to be an important parameter to be monitored. In the last decade, more emphasis is given to non-contact based systems that are low-cost and are easy to use. Despite these advancements, most of these systems are suitable for a lab environment in offline situations. This project presents an effective system for the estimation of a pulse rate from facial videos. A dataset of 160 videos with pulse rate has been introduced. The dataset is obtained from 20 subjects performing 4 activities in 2 lighting conditions. Each activity is captured by a smartphone camera placed on a tripod. This dataset with facial videos and pulse rate is trained on different Convolutional Neural Network (CNN) models to predict the pulse rate. Their performances were compared to obtain better results. Another method called Eulerian video magnification (EVM) was also implemented with the same dataset and the results were compared with the CNN results for better accuracy. This technology possesses a high potential in advancing personal health care and in the field of telemedicine. Additional improvements to the proposed system with regards to movement and illumination can prove to be useful in many real-time applications.",https://ieeexplore.ieee.org/document/9368944/,2020 IEEE International Conference on Advances and Developments in Electrical and Electronics Engineering (ICADEE),10-11 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC.2011.6022169,Near range pedestrian collision detection using bio-inspired visual neural networks,IEEE,Conferences,"New vehicular safety standards require the development of pedestrian collision detection systems that can trigger the deployment of active impact alleviation measures from the vehicle prior to a collision. In this paper, we propose a new vision-based system for near-range pedestrian collision detection. The low-level system uses a bio-inspired visual neural network, which emulates the visual system of the locust, to detect visual cues relevant to objects in front of a moving car. At a higher level, the system employs a neural-network classifier to identify dangerous pedestrian positions, triggering an alarm signal. The system was tuned via simulation and tested using recorded video sequences of real vehicle impacts. The experiment results demonstrate that the system is able to discriminate between pedestrians in dangerous and safe positions, triggering alarms accordingly.",https://ieeexplore.ieee.org/document/6022169/,2011 Seventh International Conference on Natural Computation,26-28 July 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCMC51019.2021.9418238,Neural Network based Real Time Sign Language Interpreter for Virtual Meet,IEEE,Conferences,"Due to the Covid-19 pandemic, people were forced to stay home, and most professions switched to Work From Home mode. The world switched to Video Conferencing to stay connected from remote environment. This made Translators unavailable to the hearing impaired. This makes it challenging for deaf-mute people to communicate with other people since there are no credible tools present to translate it real-time in these applications. So, a Translator built within these video conferencing applications would be helpful in communication for these people. Neural Network algorithm is used in our model to predict the signs and translate them. This model is implemented in a video conferencing application which will make the use of the Sign Gesture Translation feature and the other person using the application will receive the translation in text on a real time basis.",https://ieeexplore.ieee.org/document/9418238/,2021 5th International Conference on Computing Methodologies and Communication (ICCMC),8-10 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BEC.2010.5630663,Neural Network based system for real-time organ recognition by analysis of sequence of endoscopic images received during surgical operation,IEEE,Conferences,"This paper designs two Neural Network (NN) based systems for distinguishing and real-time recognition of internal organs on sequence of endoscopic images during abdominal surgery. First NN-based system proposed in this paper is designed for recognition of several different internal organs on color endoscopic images. Second NN-based system is designed for real-time recognition of presence of a particular internal organ on a sequence of color images (video stream) from endoscope. Restricted connectivity structure of the network makes possible decomposition of the image during the analysis and significantly reduces the number of parameters thus making training easier, faster and more accurate. The algorithms proposed in the paper are implemented in software application and their effectiveness is demonstrated on simulations.",https://ieeexplore.ieee.org/document/5630663/,2010 12th Biennial Baltic Electronics Conference,4-6 Oct. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NNSP.1991.239481,Neural networks for signal/image processing using the Princeton Engine multi-processor,IEEE,Conferences,"The authors describe a modular neural network system for the removal of impulse noise from the composite video signal of television receivers, and the use of the Princeton Engine multi-processor for real-time performance assessment. This system out-performs alternative methods, such as median filters and matched filters. The system uses only eight neurons, and can be economically implemented in VLSI.<>",https://ieeexplore.ieee.org/document/239481/,Neural Networks for Signal Processing Proceedings of the 1991 IEEE Workshop,30 Sept.-1 Oct. 1991,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCSim.2014.6903727,NeuroCGRA: A CGRA with support for neural networks,IEEE,Conferences,"Today, Coarse Grained Reconfigurable Architectures (CGRAs) are becoming an increasingly popular implementation platform. In real world applications, the CGRAs are required to simultaneously host processing (e.g. Audio/video acquisition) and estimation (e.g. audio/video/image recognition) tasks. For estimation problems, neural networks, promise a higher efficiency than conventional processing. However, most of the existing CGRAs provide no support for neural networks. To realize realize both neural networks and conventional processing on the same platform, this paper presents NeuroCGRA. NeuroCGRA allows the processing elements and the network to dynamically morph into either conventional CGRA or a neural network, depending on the hosted application. We have chosen the DRRA as a vehicle to study the feasibility and overheads of our approach. Synthesis results reveal that the proposed enhancements incur negligible overheads (4.4% area and 9.1% power) compared to the original DRRA cell.",https://ieeexplore.ieee.org/document/6903727/,2014 International Conference on High Performance Computing & Simulation (HPCS),21-25 July 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.2013.6637970,New content-based features for the distinction of violent videos and martial arts,IEEE,Conferences,"Real violence is unwanted content in video portals as it is forensically relevant in video surveillance systems. Naturally, both domains have to deal with mass data which makes the detection of violence by hand an impossible task. We introduce one component of a system for automated violence detection from video content: the differentiation of real violence and martial arts videos. In particular, we introduce two new feature transformations for jitter detection and local interest point detection with Gestalt laws. Descriptions are classified in a two-step machine learning process. The experimental results are highly encouraging: the novel features perform exceptionally well and the classification process delivers practically acceptable recall and precision.",https://ieeexplore.ieee.org/document/6637970/,"2013 IEEE International Conference on Acoustics, Speech and Signal Processing",26-31 May 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCWD54268.2022.9776224,Node-Imbalance Learning on Heterogeneous Graph for Pirated Video Website Detection,IEEE,Conferences,"With the rapid development of video streaming, the problem of copyright infringement has become increasingly severe. Despite its explicit illegality in many countries, a large variety of pirated video websites are still active, causing huge damage to copyright holders and security risks to users. Traditional methods for detecting malicious websites, such as blacklists or feature-based classifiers, can be easily bypassed by evading approaches like Domain-Flux. Some researchers recently proposed sophisticated graph-based methods to utilize various relations between websites and convert the detection task into node representation learning. However, the node imbalance issue impairs their performance on real-world datasets. In this paper, given the limitations of the above methods, we propose a model named Heterogeneous Graph Node Re-weighting (HGNR) to detect pirated video websites. We construct a heterogeneous graph with diverse meta relations and design a weight adjustment mechanism to deal with node imbalance issue. The experiments with different imbalance ratios show that HGNR outperforms state-of-the-art graph-based methods. Furthermore, we analyze the best-performed meta relation and disclose how video pirates gain profits, which can help the security community thwart video piracy.",https://ieeexplore.ieee.org/document/9776224/,2022 IEEE 25th International Conference on Computer Supported Cooperative Work in Design (CSCWD),4-6 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIIS53135.2021.9660677,Non - Invasive Technique to Automate General Movements Assessment,IEEE,Conferences,"The General Movements Assessment (GMA) is a non-invasive procedure to diagnose developmental defects that may lead to Cerebral Palsy (CP) and other physical disabilities in infants under the age of three months. This project aims to categorize movements of patients using deep neural networks (DNN) which in the long run can be used in automating GMA. The primary objective here is to determine the optimal neural network (NN) model for classifying videos. For this purpose, three network models were developed to assess their ability to manage the videos, namely, Long short-term memory (LSTM), Convolutional LSTM (ConvLSTM), and Temporal Convolutional Network (TCN). The implementation results suggested that using a TCN structure may be well-suited for use with videos. Convolutional neural networks (CNNs) are the most often used image classification architecture, and these systems often serve as the basis for all three versions. The results of the three models in video classifying indicate that classifying videos with TCN can be an effective and robust process, and the ConvLSTM model can also be very effective with further development.",https://ieeexplore.ieee.org/document/9660677/,2021 IEEE 16th International Conference on Industrial and Information Systems (ICIIS),9-11 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICICT46008.2019.8993251,Non-contact Heart Rate Monitoring Using Machine Learning,IEEE,Conferences,"Health monitoring is an important parameter to determine the health status of a person. Measuring the heart rate is an easy way to gauge our health. Normal heart rate may vary from person to person and a usually high or low resting heart rate can be a sign of trouble. There are several methods for the measurement of heart rate monitoring such as ecg, ppg etc. Such methods having a disadvantage that these are invasive and have a continuous contact with the human body. In order to overcome this problem a new system is proposed using camera. In this method a blind source separation algorithm is used for extracting the heart rate signal from the face image. Viola jones based face detection algorithm is used to track the face. FastICA algorithm is exploited to separate heart rate signal from noise and artefacts. Machine learning algorithm is implemented to standardize the signal. The data is successfully tested with real time video.",https://ieeexplore.ieee.org/document/8993251/,"2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)",5-6 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/5GWF52925.2021.00041,Non-cooperative Personnel Tracking with Cross Modality Learning in 5G-enabled Warehouse Application,IEEE,Conferences,"Asset and personnel visibility is crucial for improving workflow efficiency and reducing waste in smart facility, e.g., warehouse applications. 5G networks and technologies provide the high bandwidth and low latency necessary for communicating and fusing multi-modality sensor data, such as high-definition video, time series with high temporal resolution. In this work, we propose to use cross modality learning to develop a self-learning system for locating and tracking indoor personnel with video and WiFi channel state information (CSI) data. We use video data and our computer vision system to provide location annotation automatically, and train a feedforward neural network model for WiFi CSI data in our localization algorithm. Our experimental results show that our localization system is capable of locating a person with submeter accuracy in real-time without laborious manual data annotation.",https://ieeexplore.ieee.org/document/9605053/,2021 IEEE 4th 5G World Forum (5GWF),13-15 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIEA53260.2021.00073,Non-dense Feature Aggregation for Video Object Detection,IEEE,Conferences,"Video object detection is a challenging task in computer vision. Dynamic background change, motion blur, motion occlusion, and other problems bring great interference to the detection results. The existing detection methods often sacrifice the detection speed for the sake of accuracy or sacrifice the accuracy for the sake of faster detection speed. Therefore, we propose a non-dense feature aggregation method based on an optical flow network. The feature extraction is only carried out for keyframes, and then the feature aggregation of non-key frames is completed by the non-dense connection of adjacent frames, and the idea of dynamic programming is used to achieve the transmission of historical information, so as to obtain more effective features for detection. Our algorithm achieves a real-time detection effect without significantly increasing the amount of computation. And the algorithm achieves competitive detection results on the ImageNet VID dataset.",https://ieeexplore.ieee.org/document/9525613/,2021 International Conference on Artificial Intelligence and Electromechanical Automation (AIEA),14-16 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2012.6331419,Nonlinear spatio-temporal wave computing for real-time applications on GPU,IEEE,Conferences,"In this work, active wave simulation on Cellular Nonlinear Network was computed for path planning on the GPU of a NVIDIA GTX275 video card. In software part, QtOpenCL, which is a wrapper library of OpenCL, was used to make code portable for systems with different GPUs. We achieved promising results comparing to results achieved by both CPU and FPGA. We have implemented different hardware and software solutions to path planning problem for 2-D media in real-time. They were almost at limit of real-time requirements because of some bottlenecks such as low communication bandwidth and low resolution of network. In this work, by utilizing GPUs, we performed 60000 iterations per second for simulation of 128×128 node network while we achieved at most 35 iterations per second with software on an Intel Core 2 Duo P8700 processor. We also achieved 36 iterations per second for 3-D active wave simulation of a 256 × 256 × 256 network on GPU.",https://ieeexplore.ieee.org/document/6331419/,2012 13th International Workshop on Cellular Nanoscale Networks and their Applications,29-31 Aug. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP40776.2020.9054439,Notice of Violation of IEEE Publication Principles: Intensity-Image Reconstruction for Event Cameras Using Convolutional Neural Network,IEEE,Conferences,"Event cameras have many benefits than conventional cameras, such as high temporal resolution, high dynamic range. However, because the outputs of event cameras are asynchronous event streams than intensity images, Frame-based algorithms cannot be directly used. It is also necessary to present intensity images of event cameras on the display for human viewing. In this paper, ""event frames"" are recovered from event streams in an attenuation method and they are fed into the U-net network to generate intensity images. Our model is trained on a large amount of simulated data and gradually reduces the perceptual loss through training. In order to evaluate the model, we compare the generated image with the target image on the simulated data and the real data. This proves that our model can reconstruct intensity images of event cameras very well.",https://ieeexplore.ieee.org/document/9054439/,"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",4-8 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIoT48696.2020.9089525,Novel Face Liveness Detection Using Fusion of Features and Machine Learning Classifiers,IEEE,Conferences,"As the technology is growing which is birth to different types of frauds into areas like face detection, finger print detection etc. Moreover, it becomes very difficult for service providers to maintain security of data. In addition, these systems needs to be protected from spoofing by attackers. Fraudsters can use dummy Eyes, photographs for identification of faces for authentication purposes. These facial recognitions can also be done through face detection from video streams by replaying videos & capturing specific moments of person. Also these kind of attacks can be done successfully as system can't detect the real life faces & faces extracted from videos & photos. In addition, these videos & photos will be easily available on internet & other stored media, so anyone can challenge the system by using it. Now, many algorithms are implemented to detect face liveness for authentication. The paper represents novel face liveness detection using fusion of luminance-based features with help of assorted machine learning classifiers.",https://ieeexplore.ieee.org/document/9089525/,"2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)",2-5 Feb. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VETECS.2000.851569,Novel mobile video transmission system for off-road navigation services,IEEE,Conferences,"One of the problems faced in vehicular transmission systems is the protection of highly sensitive real-time video information against multipath fading environments. The technique proposed is the implementation of a M-level QAM scheme in association with a novel fade estimation and compensation technique. A half-pilot symbol assisted modulation scheme, HP-SAM, a Reed-Solomon error correction coding as well as a non-uniform partitioning of the signal constellation are also used. They result in considerable improvements in the bit and frame error rate performance. It is essential that the video is encoded with high compression rate. In this paper, we also propose a highly efficient interframe coding technique that uses transmission via separate channels, each having differing degrees of error protection. The main focus of this paper is on the transmission aspects of the scheme and the performance enhancement they can provide. The codec is currently under development.",https://ieeexplore.ieee.org/document/851569/,VTC2000-Spring. 2000 IEEE 51st Vehicular Technology Conference Proceedings (Cat. No.00CH37026),15-18 May 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WF-IoT51360.2021.9595421,OWSNet: Towards Real-time Offensive Words Spotting Network for Consumer IoT Devices,IEEE,Conferences,"Every modern household owns at least a dozen of IoT devices like smart speakers, video doorbells, smartwatches, where most of them are equipped with a Keyword spotting (KWS) system-based digital voice assistant like Alexa. The state-of-the-art KWS systems require a large number of operations, higher computation, memory resources to show top performance. In this paper, in contrast to existing resource-demanding KWS systems, we propose a light-weight temporal convolution based KWS system named OWSNet, that can comfortably execute on a variety of IoT devices around us and can accurately spot multiple keywords in real-time without disturbing the device&#x2019;s routine functionalities. When OWSNet is deployed on consumer IoT devices placed in the workplace, home, etc., in addition to spotting wake/trigger words like &#x2018;Hey Siri&#x2019;, &#x2018;Alexa&#x2019;, it can also accurately spot offensive words in real-time. If regular wake words are spotted, it activates the voice assistant; else if offensive words are spotted, it starts to capture and stream audio data to speech analytics APIs for autonomous threat and insecurities detection in the scene. The evaluation results show that the OWSNet is faster than state-of-the-art models as it produced $\approx$ 1-74 times faster inference on Raspberry Pi 4 and $\approx$ 1-12 times faster inference on NVIDIA Jetson Nano. In this paper, to optimize IoT use-case models like OWSNet, we present a generic multi-component ML model optimization sequence that can reduce the memory and computation demands of a wide range of ML models thus enabling their execution on low resource, cost, power IoT devices.",https://ieeexplore.ieee.org/document/9595421/,2021 IEEE 7th World Forum on Internet of Things (WF-IoT),14 June-31 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IC4ME247184.2019.9036531,Object Detection Based Security System Using Machine learning algorthim and Raspberry Pi,IEEE,Conferences,"Conventional security systems that use surveillance cameras to monitor the property lacks the ability to notify the security administrator in the event of trespassing. A security camera when used along with a digital video recorder (DVR) is only effective as a source to gather evidence unless the video feed is constantly being monitored by a dedicated personnel. This paper discusses the implementation of a cost effective, intelligent security system that overcomes drawbacks of conventional security cameras by utilizing a machine learning and Viola-Jones algorithm under image processing literature to identify trespassers and multiple object detection in real time. The paper presents the design and implementation details of the intelligent object detection based security system in two different computing environment, MATLAB and Python respectively using Raspberry Pi 3 B single board computer. The security system is capable of alerting the security administrator through email via internet while activating an alarm locally.",https://ieeexplore.ieee.org/document/9036531/,"2019 International Conference on Computer, Communication, Chemical, Materials and Electronic Engineering (IC4ME2)",11-12 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE49325.2020.9200139,Object Detection and Tracking Turret based on Cascade Classifiers and Single Shot Detectors,IEEE,Conferences,"The involvement of embedded systems and computer vision is increasing day by day in various segments of consumer market like industrial automation, traffic monitoring, medical imaging, modern appliance market, augmented reality systems, etc. These technologies are bound to make new developments in the domain of commercial and home security surveillance. Our project aims to make contributions to the domain of video surveillance by making use of embedded computer vision systems. Our implementation, built around the Raspberry Pi 4 SBC aims to utilize computer vision techniques like motion detection, face recognition, object detection, etc to segment the region of interest from the captured video footage. This technique is superior as compared to traditional surveillance systems as it requires minimum human interaction and intervention at the control room of such security systems. The proposed system is capable of sensing suspicious events like detection of an unknown face in the captured video or motion detection/object detection in a closed section of a building. Moreover, with the help of the turret mechanism built using servo motors, the camera integrated in the system is capable of having 360° rotation and can track a detected face or object of interest within its range. Apart from automated tracking, the system can also be manually controlled by the operator.",https://ieeexplore.ieee.org/document/9200139/,2020 International Conference on Computational Performance Evaluation (ComPE),2-4 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI.2017.81,Object Movement Detection by Real-Time Deep Learning for Security Surveillance Camera,IEEE,Conferences,Developing a smart Web Video Player application connected to a security surveillance camera to keep track of the object of interest is an ongoing research. This paper presents a methodology to real time data mining of the sequence of frames from a live stream collected by security camera by processing trajectories of an object of interest. Two classifiers and a clustering method are implemented all working in real-time. Real-time Deep Learning and Support Vector Machines (SVM) machine learning algorithms are implemented on a local server without the use of cloud computing. This is a popular architecture for many buildings and industries who want to have an in-house smart security camera application.,https://ieeexplore.ieee.org/document/8560838/,2017 International Conference on Computational Science and Computational Intelligence (CSCI),14-16 Dec. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IoTaIS47347.2019.8980440,Object Recognition with Machine Learning: Case Study of Demand-Responsive Service,IEEE,Conferences,"To assist the disabled, a system was developed and designed to provide passenger transport services for disabled persons in wheelchairs. Machine-learning image-recognition technology was applied to provide accessible bus rides for the disabled. Based on the video footage at the bus stop, this system can judge whether there are wheelchair riders in the waiting area. YOLOv3, a real-time image-recognition model, was used for object recognition and notification. By combining LINE with the Flask Web application framework, it helps the disabled transmit messages and actively notifies bus drivers. The API server was established using the Flask framework. When the system at the bus stop detects the disabled, the images of the disabled waiting for the bus are transmitted to the server, and the images and related messages are transmitted in real time to the users through the LINE Message API. Passengers can also use the LINE chatbot to enter keywords to confirm the real-time location of the bus. The dynamic bus information comes from the Open Data of the Bureau of Transportation. The LINE BOT API program was developed using the PyCharm tool and then transmitted to the Heroku cloud platform for deployment. The LINE BOT API provides real-time wheelchair image recognition, and receives notifications and bus information queries through the LINE chatbot. The main contributions of this study are the use of artificial intelligence (AI) technology to provide solutions to facilitate bus rides for the disabled and to improve the quality of barrier-free transport services in modern society.",https://ieeexplore.ieee.org/document/8980440/,2019 IEEE International Conference on Internet of Things and Intelligence System (IoTaIS),5-7 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOSST48232.2019.9043975,Object Segmentation in Video Sequences by using Single Frame Processing,IEEE,Conferences,"Object segmentation, detection and tracking in videos is one of the most important task of computer vision. It is necessary in all of the real time deployed surveillance systems. Various unsupervised and semi-supervised video object segmentation techniques have been implemented and shown efficient results. But all of these techniques process all of the frames of a video sequence, which requires a huge training data and results in a large computational time. In this paper, a semi-supervised technique is proposed which segments an object in a video by just processing a single frame of the sequence. In this framework, a fully convolutional network is used to separate the foreground from the image, create the mask of the object and then segments the object with the help of this mask. The foreground separation in a frame is done by using pre-trained network while, training and testing of rest of the network is done using a specified dataset named as DAVIS. The results show that, the proposed framework takes less computational time and has also improved the overall accuracy of video object segmentation by 10% as compared to previous techniques.",https://ieeexplore.ieee.org/document/9043975/,2019 13th International Conference on Open Source Systems and Technologies (ICOSST),17-19 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2016.462,Object Tracking via Dual Linear Structured SVM and Explicit Feature Map,IEEE,Conferences,"Structured support vector machine (SSVM) based methods have demonstrated encouraging performance in recent object tracking benchmarks. However, the complex and expensive optimization limits their deployment in real-world applications. In this paper, we present a simple yet efficient dual linear SSVM (DLSSVM) algorithm to enable fast learning and execution during tracking. By analyzing the dual variables, we propose a primal classifier update formula where the learning step size is computed in closed form. This online learning method significantly improves the robustness of the proposed linear SSVM with lower computational cost. Second, we approximate the intersection kernel for feature representations with an explicit feature map to further improve tracking performance. Finally, we extend the proposed DLSSVM tracker with multi-scale estimation to address the ""drift"" problem. Experimental results on large benchmark datasets with 50 and 100 video sequences show that the proposed DLSSVM tracking algorithm achieves state-of-the-art performance.",https://ieeexplore.ieee.org/document/7780831/,2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),27-30 June 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEIC54506.2022.9748822,Object detection based on deep learning techniques in resource-constrained environment for healthcare industry,IEEE,Conferences,"Advanced technologies and algorithms such as the Internet of Things (IoT), computer vision (CV), and deep learning are widely used in the healthcare industry to enhance global med-ical care. Internet of Things (IoT) has the potential to be limitless due to increased network agility, integrated artificial intelligence (AI), and the ability to deploy and automate systems. Embedded systems playa vital role in IoT due to real-time computing, low power consumption, and low maintenance cost. Object detection is a computer vision technique that aims to process and identify certain objects such as people, cars, animals, or buildings in digital images or videos. The goal of object detection is to develop computational models for computer vision applications. Recently, rapid advancement in deep learning techniques accelerated the momentum of object detection. In this study, we proposed a mechanism to perform object detection based on deep learning techniques in resource-constrained IoT devices. Due to limited computational powers in embedded systems, the performance of deep learning algorithms is not good enough. To achieve this, we compressed the video using a codec and streamed it to the amazon cloud for object detection. Video codec was used to uncompress the video in its original format so that there is no loss of video quality. A pre-trained YOLO model was deployed for object detection in medical images. The output is sent to the client using a lightweight protocol for data communication. Results indicate that the proposed mechanism worked well in a resource-constrained environment without compromising accuracy and time.",https://ieeexplore.ieee.org/document/9748822/,"2022 International Conference on Electronics, Information, and Communication (ICEIC)",6-9 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMART52563.2021.9675310,Object-Text Detection and Recognition System,IEEE,Conferences,"The object recognition system based on deep learning has been applied in different domains, e.g. in the Intelligent transportation system, Autonomous driving system, etc. Along with object detection, in numerous scenes, text detection and recognition have conjointly brought abundant attention and analysis application.Real-time object detection and dimensioning as well as text recognition are important topics for many branches of the industry today.The projected system consists of object – text, detection and recognition module, and a dimension measuring module. This system offers an improved method of classifying objects and calculating their measures in real-time from video sequences. The proposed system uses OpenCV libraries, which comprise erosion algorithms, canny edge detection, dilation, and contour detection. To accomplish the task of the text recognition Tesseract OCR engine is employed.",https://ieeexplore.ieee.org/document/9675310/,2021 10th International Conference on System Modeling & Advancement in Research Trends (SMART),10-11 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICCT.2018.8473331,Objects Talk - Object Detection and Pattern Tracking Using TensorFlow,IEEE,Conferences,"Objects in household that are frequently in use often follow certain patterns with respect to time and geographical movement. Analysing these patterns can help us keep better track of our objects and maximise efficiency by minimizing time wasted in forgetting or searching for them. In our project, we used TensorFlow, a relatively new library from Google, to model our neural network. The TensorFlow Object Detection API is used to detect multiple objects in real-time video streams. We then introduce an algorithm to detect patterns and alert the user if an anomaly is found. We consider the research presented by Laube et al., Finding REMO-detecting relative motion patterns in geospatial lifelines, 201–214, (2004)[1].",https://ieeexplore.ieee.org/document/8473331/,2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT),20-21 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.1993.378163,Occam algorithms for computing visual motion,IEEE,Conferences,"By drawing an analogy with machine learning, the author proposes to define visual motion as a predictor that can accurately predict future frames. Under this new definition, visual motion can be specified by a collection of image patches, each moving in a simple motion. An implementation with rectangular patches determined recursively by a binary decision tree is described. Experimental results on real video sequences verify the algorithm assumptions and show that motion in typical sequences can be accurately described in terms of a few parameters.<>",https://ieeexplore.ieee.org/document/378163/,1993 (4th) International Conference on Computer Vision,11-14 May 1993,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/OCEANS44145.2021.9705808,Ocean current observations by infrared and visual Large Scale Particle Image Velocimetry (LSPIV),IEEE,Conferences,"The natural dynamics of tidal currents continuously change the appearance of the Wadden Sea. Especially coastal regions and bays with shallow waters are strongly influenced by tidal dynamics. Meanwhile, in offshore study areas with harsh environmental conditions as well as dynamic ocean currents and water levels, continuous in-situ investigations of the prevailing current conditions requires a high effort. In-situ measurement technology is thereby maintenance-intensive and partly limited in time or space by limited accessibility or restricted by the measurement method. In hence, remote sensing techniques based on video image information, such as Large Scale Particle Image Velocimetry (LSPIV) for sensing, and computer vision algorithms for long-term investigation of the prevailing dynamic near-surface ocean flow conditions, have become highly relevant. However, in some environmental situations there may be not sufficient or significant natural textures evaluable in the visual camera images, or only insufficient contrast ratio under varying ambient illumination at the water observation site, to perform image based velocimetry. Thus, we consider with this study an extended approach of visual and infrared video data analysis by an automated LSPIV measurement technique under real offshore deployment conditions. The treatise of this paper first introduces the reader to the subject area and technologies as well as the principles of the LSPIV measurement method. Followed by the depiction of the motivation for an extended approach to visual and infrared video data analysis by an automated LSPIV measurement method in real offshore applications. Subsequently, related research is discussed. Thereafter, the remote sensing setup and sensor-test-bed-system for offshore deployment on an observation platform is presented, therein we also addresses routines of necessary calibration procedures for camera sensors. We then depict details of our automated LSPIV measurement procedure. This is followed with an overview of the preceding validation procedures and LSPIV multispectral remote sensing results of long-term monitoring of horizontal flow dynamics over several days. Finally, we discuss uncertainties of the LSPIV velocity measurement method encountered in real applications and conclude with a brief outlook on further developments and applications.",https://ieeexplore.ieee.org/document/9705808/,OCEANS 2021: San Diego – Porto,20-23 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISM46123.2019.00057,"OmniTrack: Real-Time Detection and Tracking of Objects, Text and Logos in Video",IEEE,Conferences,"The automatic detection and tracking of general objects (like persons, animals or cars), text and logos in a video is crucial for many video understanding tasks, and usually real-time processing as required. We propose OmniTrack, an efficient and robust algorithm which is able to automatically detect and track objects, text as well as brand logos in real-time. It combines a powerful deep learning based object detector (YoloV3) with high-quality optical flow methods. Based on the reference YoloV3 C++ implementation, we did some important performance optimizations which will be described.",https://ieeexplore.ieee.org/document/8959038/,2019 IEEE International Symposium on Multimedia (ISM),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISM52913.2021.00035,Open-Domain Trending Hashtag Recommendation for Videos,IEEE,Conferences,"We describe a novel algorithm for an open-domain trending hashtag recommendation task using zero-shot hashtag prediction in an online learning paradigm. Our method utilizes joint representation learning of latent embeddings for features extracted from long-form videos and semantic embeddings of hashtags trending on social platforms. In particular, we apply graph convolutional networks to a link prediction task using videos and hashtags as nodes in a heterogeneous graph. Comparing it to the existing models for closely related tasks, we demonstrate state-of-the-art results in trending hashtag recommendations for videos. The architecture is designed to be modular in a plug-and-play fashion to enable quick and easy incorporation of the latest advances in natural language understanding and image and video processing, with a practical view to its implementation in a real-time, online setting.",https://ieeexplore.ieee.org/document/9666058/,2021 IEEE International Symposium on Multimedia (ISM),29 Nov.-1 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMSNETS53615.2022.9668364,Open-air Off-street Vehicle Parking Management System Using Deep Neural Networks: A Case Study,IEEE,Conferences,"Smart parking solution aims to output real-time parking occupancy information. It helps to reduce parking bay search time, traffic, fuel consumption, and thereby vehicular emissions with increased road safety. A computer vision-based solution using camera video data is most reliable and rational since it allows monitoring the entire open-air parking area at once. A real-time parking solution (cloud-based, server processing, or onboard processing) helps bring the occupancy information to the end-user. It comes with many challenges such as viewing angles, lighting conditions, model optimization, reducing inference time, and many more real-world challenges. Hence, this paper presents a case study on real-time open-air off-street intelligent parking management using a deep neural network. Also, most of the earlier research works focus on day-time data and do not discuss the night data. So, in this work, we perform experiments on realtime 24-hour data from an input camera video source mounted to monitor parking at IIT Hyderabad (IITH) parking lot. Our experiments demonstrate the real-world challenges and can help improve parking performance, deployment at IITH, and relevant parking systems in general.",https://ieeexplore.ieee.org/document/9668364/,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),4-8 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WiCOM.2006.313,Optimization of an AVS Audio Decoder on DSP,IEEE,Conferences,"The part III of Audio Video Standard of China (AVS) is the first Hi-Fi audio coding standard in China public domain. Through nonlinear quantization and context-dependent bitplane entropy coding (CBC), it offers distinct features of nonlinear quantization noise adaptation and high efficiency fine grain scalability (FGS), but also imposes great challenges on fixed-point implementation with high precision requirement and stringent computational resource restriction. This paper presents tuned optimization strategies for fixed-point arithmetic, buffer manipulation, entropy decoding, and CBC decoding loop reduction to address the challenges. The demonstration real-time implementation on fixed-point DSP TMS320C5509A shows that our design optimization cuts down computational complexity from over 200 MIPs to 60 MIPs while preserving over 15 effective bits precision for 16-bit PCM output",https://ieeexplore.ieee.org/document/4149490/,"2006 International Conference on Wireless Communications, Networking and Mobile Computing",22-24 Sept. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICFTIC54370.2021.9647400,Optimized Yolov3 Deployment on Jetson TX2 With Pruning and Quantization,IEEE,Conferences,"Pruning and Quantization are commonly techniques deployed on deep convolutional networks to accelerate the model and reducing size, taking the resolution as a sacrifice. In embedded systems where computation power is limited due to power and cost constraints, pruning and quantization is mandatory for such networks especially where real-time processing is required, such as image classification in live video streams. In this paper, a pruned and quantized YOLOv3 model is deployed on Nvidia's industrial standard model Jetson TX2, which demonstrated an increase of 5 FPS in image classification via an 640×480 USB camera, while allocating only 16.7% storage on disk.",https://ieeexplore.ieee.org/document/9647400/,2021 IEEE 3rd International Conference on Frontiers Technology of Information and Computer (ICFTIC),12-14 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE-Berlin47944.2019.8966156,Optimizing Deep Learning Based Semantic Video Segmentation on Embedded GPUs,IEEE,Conferences,"Decision making in many industries today is being improved drastically thanks to artificial intelligence and deep learning. New algorithms address challenges such as genome mapping, medical diagnostics, self-driving cars, autonomous robots and more. Deep learning in embedded systems requires high optimization due to the high computational demand, given that power, heat dissipation, size and price constraints are numerous. In this paper we analyze several acceleration methods which include utilization of GPUs for most complex variants of deep learning, such as semantic video segmentation operating in real time. Specifically, we propose mapping of acceleration routines commonly present within deep learning SDKs to different network layers in semantic segmentation. Finally, we evaluate one implementation utilizing the enumerated techniques for semantic segmentation of front camera in autonomous driving front view.",https://ieeexplore.ieee.org/document/8966156/,2019 IEEE 9th International Conference on Consumer Electronics (ICCE-Berlin),8-11 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR46437.2021.00058,Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks,IEEE,Conferences,"Deep neural networks for video classification, just like image classification networks, may be subjected to adversarial manipulation. The main difference between image classifiers and video classifiers is that the latter usually use temporal information contained within the video. In this work we present a manipulation scheme for fooling video classifiers by introducing a flickering temporal perturbation that in some cases may be unnoticeable by human observers and is implementable in the real world. After demonstrating the manipulation of action classification of single videos, we generalize the procedure to make universal adversarial perturbation, achieving high fooling ratio. In addition, we generalize the universal perturbation and produce a temporal-invariant perturbation, which can be applied to the video without synchronizing the perturbation to the input. The attack was implemented on several target models and the transferability of the attack was demonstrated. These properties allow us to bridge the gap between simulated environment and real-world application, as will be demonstrated in this paper for the first time for an over-the-air flickering attack.",https://ieeexplore.ieee.org/document/9578177/,2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),20-25 June 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3524613.3527807,PSDoodle: Searching for App Screens via Interactive Sketching,IEEE,Conferences,"Keyword-based mobile screen search does not account for screen content and fails to operate as a universal tool for all levels of users. Visual searching (e.g., image, sketch) is structured and easy to adopt. Current visual search approaches count on a complete screen and are therefore slow and tedious. PSDoodle employs a deep neural network to recognize partial screen element drawings instantly on a digital drawing interface and shows results in real-time. PSDoodle is the first tool that utilizes partial sketches and searches for screens in an interactive iterative way. PSDoodle supports different drawing styles and retrieves search results that are relevant to the user&#x0027;s sketch query. A short video demonstration is available online at: https://youtu.be/3cVLHFm5pY4",https://ieeexplore.ieee.org/document/9797318/,2022 IEEE/ACM 9th International Conference on Mobile Software Engineering and Systems (MobileSoft),17-18 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CogMI52975.2021.00035,Parallel Detection for Efficient Video Analytics at the Edge,IEEE,Conferences,"Deep Neural Network (DNN) trained object detec-tors are widely deployed in many mission-critical systems for real time video analytics at the edge, such as autonomous driving, video surveillance and Internet of smart cameras. A common per-formance requirement in these mission-critical edge services is the near real-time latency of online object detection on edge devices. However, even with well-trained DNN object detectors, the online detection quality at edge may deteriorate for a number of reasons, such as limited capacity to run DNN object detection models on heterogeneous edge devices, and detection quality degradation due to random frame dropping when the detection processing rate is significantly slower than the incoming video frame rate. This paper addresses these problems by exploiting multi-model multi-device detection parallelism for fast object detection in edge systems with heterogeneous edge devices. First, we analyze the performance bottleneck of running a well-trained DNN model at edge for real time online object detection. We use the offline detection as a reference model, and examine the root cause by analyzing the mismatch among the incoming video streaming rate, the video processing rate for object detection, and the output rate for real time detection visualization of video streaming. Second, we study performance optimizations by exploiting multi-model detection parallelism. We show that the model-parallel detection approach can effectively speed up the FPS detection processing rate, minimizing the FPS disparity with the incoming video frame rate on heterogeneous edge devices. We evaluate the proposed approach using SSD300 and YOLOv3 (pre-trained DNN models) on benchmark videos of different video stream rates. The results show that exploiting multi-model detection parallelism can speed up the online object detection processing rate and deliver near real-time object detection performance for efficient video analytics at edge.",https://ieeexplore.ieee.org/document/9750319/,2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI),13-15 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCT52966.2021.9670146,Parallel Implementation of Real-Time Object Detection using OpenMP,IEEE,Conferences,"One of the urgent problems of the modern world is the detection and recognition of objects on a video stream in real-time. To work with a video stream, you need to split it into frames that are more convenient for processing. There is a possibility of using multi-core processors for image processing. Typically, the operating system allows only one core of multi-core processors to execute a task. But there are such parallelization tools that allow parallelizing execution when processing data on multi-core processors. And OpenMP is one of them that can be used in real-time object detection processing procedures. But in this case, the main thing is to find that part of the algorithm that is suitable for parallelization. Using OpenMP simply does not give an efficient accelerated result. In this paper, we discuss the implementation of OpenMP technology in real-time object detection algorithms on multi-core processors. Experimental results show that the use of OpenMP technology significantly improves the processing speed for real-time object detection on multi-core processors, when there are no GPUs.",https://ieeexplore.ieee.org/document/9670146/,2021 International Conference on Information Science and Communications Technologies (ICISCT),3-5 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MLSP.2006.275561,Parallel Linear Hashtable Motion Estimation Algorithm,IEEE,Conferences,"This paper presents a parallel linear hashtable motion estimation algorithm (LHMEA). Most parallel video compression algorithms focus on group of picture (GOP). Based on LHMEA we proposed earl[1][2], we developed a parallel motion estimation algorithm focus inside of frame. We divide each reference frames into equally sized regions. These regions are going to be processed in parallel to increase the encoding speed significantly. The theory and practice speed up of parallel LHMEA according to the number of PCs in the cluster are compared and discussed. Motion vectors (MV) are generated from the first-pass LHMEA and used as predictors for second-pass hexagonal search (HEXBS) motion estimation, which only searches a small number of Macroblocks (MBs). We evaluated distributed parallel implementation of LHMEA of TPA for real time video compression.",https://ieeexplore.ieee.org/document/4053660/,2006 16th IEEE Signal Processing Society Workshop on Machine Learning for Signal Processing,6-8 Sept. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMSP48831.2020.9287135,Parallelized Rate-Distortion Optimized Quantization Using Deep Learning,IEEE,Conferences,"Rate-Distortion Optimized Quantization (RDOQ) has played an important role in the coding performance of recent video compression standards such as H.264/AVC, H.265/HEVC, VP9 and AV1. This scheme yields significant reductions in bit-rate at the expense of relatively small increases in distortion. Typically, RDOQ algorithms are prohibitively expensive to implement on real-time hardware encoders due to their sequential nature and their need to frequently obtain entropy coding costs. This work addresses this limitation using a neural network-based approach, which learns to trade-off rate and distortion during offline supervised training. As these networks are based solely on standard arithmetic operations that can be executed on existing neural network hardware, no additional area-on-chip needs to be reserved for dedicated RDOQ circuitry. We train two classes of neural networks, a fully-convolutional network and an auto-regressive network, and evaluate each as a post-quantization step designed to refine cheap quantization schemes such as scalar quantization (SQ). Both network architectures are designed to have a low computational overhead. After training they are integrated into the HM 16.20 implementation of HEVC, and their video coding performance is evaluated on a subset of the H.266/VVC SDR common test sequences. Comparisons are made to RDOQ and SQ implementations in HM16.20. Our method achieves 1.64% BD-rate savings on luminosity compared to the HM SQ anchor, and on average reaches 45% of the performance of the iterative HM RDOQ algorithm.",https://ieeexplore.ieee.org/document/9287135/,2020 IEEE 22nd International Workshop on Multimedia Signal Processing (MMSP),21-24 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPL.2014.6927506,Patra: Parallel tree-reweighted message passing architecture,IEEE,Conferences,"Maximum a posteriori probability inference algorithms for Markov Random Field are widely used in many applications, such as computer vision and machine learning. Sequential tree-reweighted message passing (TRW-S) is an inference algorithm which shows good quality in finding optimal solutions. However, the performance of TRW-S in software cannot meet the requirements of many real-time applications, due to the sequential scheme and the high memory, bandwidth and computational costs. This paper proposes Patra, a novel parallel tree-reweighted message passing architecture, which involves a fully pipelined design targeting FPGA technology. We build a hybrid CPU/FPGA system to test the performance of Patra for stereo matching. Experimental results show that Patra provides about 100 times faster than a software implementation of TRW-S, and 12 times faster than a GPU-based message passing algorithm. Compared with an existing design in four FPGAs, we can achieve 2 times speedup in a single FPGA. Moreover, Patra can work at video rate in many cases, such as a rate of 167 frame/sec for a standard stereo matching test case, which makes it promising for many real-time applications.",https://ieeexplore.ieee.org/document/6927506/,2014 24th International Conference on Field Programmable Logic and Applications (FPL),2-4 Sept. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2002.1187941,Pattern matching as the nucleus for either autonomous driving or driver assistance systems,IEEE,Conferences,"Concerns autonomous vehicle driving by pattern matching combined with reinforcement learning. In specific, this research focuses on the requirement to steer an autonomous car along a curvy and hilly road course with no intersections and no other vehicle or obstacle but with the strict requirement to self-improve driving behaviour. A camera is used to build quickly an abstract complete description (ACSD) of vehicle's current situation. This combines traditional edge finding operators with a new technique of Bayes prediction for each part of the video image. Those ACSD's are being stored together with the steering commands issued at that time and serve as the pattern database of possible driving behaviour which are being retrieved using an approximate nearest neighbour pattern matching algorithm with a O(n log m) characteristic compared to O(n/spl middot/m) for the conventional nearest neighbour calculation. In addition to this, any feedback on the quality or appropriateness of the driving behaviour has to be self-created (e.g. time measurement for a whole road section) and is therefore delayed and unspecific in relation to single issued steering commands. Consequently, a machine learning algorithm coping with those conditions is being implemented based on Reinforcement Learning.",https://ieeexplore.ieee.org/document/1187941/,"Intelligent Vehicle Symposium, 2002. IEEE",17-21 June 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIITME47687.2019.8990686,Pedestrian Detection and Behaviour Characterization for Video Surveillance Systems,IEEE,Conferences,"Nowadays, to identify potentially dangerous situations created by pedestrians, performing video surveillance systems are more than necessary. The application presented in this paper is designed to automatically detect individual pedestrians and analyze and characterize their behavior. Based on this result, an informed security decision can be made. The implementation uses YOLO (You-Only-Look-Once) [1] to detect people in a real-time video stream and evaluates and characterizes their movement, based on data collected from the current video frame and a certain number of past video frames. A series of tests revealed that our system successfully detects both regular and irregular pedestrian trajectory type and also the type of movement: walk, run or stand. The processing speed of up to 77 FPS for YOLOv3-tiny and 21 FPS for YOLOv3 qualifies our solution for real-time operation in a surveillance system, if it is running on a computer platform equipped with a NVIDIA GPU with CUDA capabilities.",https://ieeexplore.ieee.org/document/8990686/,2019 IEEE 25th International Symposium for Design and Technology in Electronic Packaging (SIITME),23-26 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3524494.3527624,Pedestrian motion in simulation applications using deep learning,IEEE,Conferences,"The goal of this paper is to provide a framework for simu-lating pedestrian motion in simulation applications by using real-world examples of human motion. This process has two implications. The first one refers to the reduction of the development time, since a deep learning model can replace the classical pedestrian behavior development process for the targeted applications. The second relates to improving the quality of pedestrian movements, as manual development of behavior using classical methods can result in movements that appear too robotic or predictable. We propose a new deep learning model based on an encoder-decoder strategy and Graph Attention Networks, able to take into account both the semantics of the scene and the correlations between the simulated pedestrian movements. The evaluation shows that the methods are suitable for real-time simulations, even for applications with performance constraints such as video games.",https://ieeexplore.ieee.org/document/9826177/,2022 IEEE/ACM 6th International Workshop on Games and Software Engineering (GAS),20-20 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/iV.2015.88,Perceived Realism of Crowd Behaviour with Social Forces,IEEE,Conferences,"This paper investigates the development of an urban crowd simulation for the purposes of psychophysical experimentation. Whilst artificial intelligence (AI) is advancing to produce more concise and interesting crowd behaviours, the number or sophistication of the algorithms implemented within a system does not necessarily guarantee its perceptual realism. Human perception is highly subjective and does not always conform to the reality of the situation. Therefore it is important to consider this aspect when dealing with A implementations within a crowd system aimed at humans. In this research an initial two-alternative forced choice (2AFC) with constant stimuli psychophysical experiment is presented. The purpose of the experiment is to assess whether human participants perceive crowd behaviour with a social forces model to be more realistic. Results from the experiment suggest that participants do consider crowd behaviour with social forces to be more realistic. This research could inform the development of crowd-based systems, especially those that consider viewer perception to be important, such as for example video games and other media.",https://ieeexplore.ieee.org/document/7272647/,2015 19th International Conference on Information Visualisation,22-24 July 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCNC49053.2021.9417599,Performance Analysis of Adaptive Bitrate Algorithms for Multi-user DASH Video Streaming,IEEE,Conferences,"With the increasing video demand in daily network traffic, it is an urgent task to develop effective algorithms to facilitate high-quality content delivery service. Recently, numerous adaptive streaming algorithms have been proposed to improve the user perceived experience. However, these algorithms were mainly developed from the perspective of single user. There is not yet systematical evaluation and comparison of the bitrate adaptation methods for multi-user video streaming. Besides, the Quality of Experience (QoE) metrics were not unified.In this work, we propose a new mininet-based testbed framework which is able to conduct real-time video streaming emulation in various multi-user scenarios. Seven state-of-the-art adaptation methods are incorporated into the testbed. Meanwhile, ITU-T P.1203 model, the world's first standard for measuring QoE of HTTP adaptive streaming, is implemented to calculate the mean opinion scores of different methods. Using the developed testbed, the performance of current adaptation methods in multi-user network is analyzed and compared. A variety of experiments are carried out by changing the user number and network conditions, in which the QoE of different users are investigated. It is found that current algorithms perform inconsistently in various network scenarios. In the excessive user and limited bandwidth cases, machine learning and scheduling techniques show superiority in providing high and equal QoE for all users. While in the high-delay case, the buffer-based approaches show robust performance. Overall, the findings of this work give an insight for designing and choosing adaptive streaming strategies in different multi-user network conditions.",https://ieeexplore.ieee.org/document/9417599/,2021 IEEE Wireless Communications and Networking Conference (WCNC),29 March-1 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICCAS52745.2021.9649792,Performance Evaluation of YOLOv3 and YOLOv4 Detectors on Elevator Button Dataset for Mobile Robot,IEEE,Conferences,"The performance evaluation of an AI network model is the important part for building an effective solution before its deployment in real-world on the robot. In our study, we have implemented YOLOv3-tiny and YOLOv4-tiny darknet based frameworks for performance evaluation of the elevator button recognition task and tested both variants on image and video datasets. The objective of our study is two-fold: First, to overcome the limitation of elevator buttons dataset by creating new dataset and increasing its quantity without compromising the quality; Second, to provide a comparative analysis through experimental results and the performance evaluation of both detectors using four machine learning metrics. The purpose of our work is to assist the researchers and developers in decision making of suitable detector selection for deployment in the elevator robot towards button recognition application. The results show that YOLOv4-tiny outperforms YOLOv3-tiny with an overall accuracy of 98.60% compared to 97.91% at 0.5 IoU.",https://ieeexplore.ieee.org/document/9649792/,"2021 21st International Conference on Control, Automation and Systems (ICCAS)",12-15 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIAS.2007.4658337,Performance guarantees for agent-based Hierarchical Diff-EDF scheduler,IEEE,Conferences,"Packet networks are currently enabling the integration of heterogeneous traffic with a wide range of characteristics that extend from video traffic with stringent QoS requirements to best-effort traffic requiring no guarantees. QoS guarantees can be provided in packet networks by the use of proper packet scheduling algorithms. Similar to the trends of computer revolution, many scheduling algorithms have been proposed to meet this goal. In this paper, we propose a new priority assignment scheduling algorithm, Hierarchical Diff-EDF, which can meet the real-time needs while continuing to provide best effort service over heterogeneous real-time network traffic. The Hierarchical Diff-EDF service meets the flow miss rate requirements through the combination of single step hierarchal scheduling for the different network flows, and the admission control mechanism that detects the overload conditions to modify packetspsila priorities. The implementation of this scheduler is based on the multi-agent simulation that takes the inspiration from object-oriented programming. The implementation itself is aimed to the construction of a set of elements which, when fully elaborated, define an agent system specification. When evaluating our proposed scheduler, it was extremely obvious that the Hierarchical Diff-EDF scheduler performs much better than both EDF and Diff-EDF schedulers.",https://ieeexplore.ieee.org/document/4658337/,2007 International Conference on Intelligent and Advanced Systems,25-28 Nov. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMSIT50672.2020.9254863,Personalized Quality of Experience (QOE) Management using Data Driven Architecture in 5G Wireless Networks,IEEE,Conferences,"the aim of this paper is to Personalized Quality of Experience (QOE) Management using Data driven Architecture in 5G Wireless Networks that consume less resources. The proposed research will be the part of the overall research project, which focuses on addressing a problem that many organizations experience that introduce an Enterprise Architecture to support the integration of different services across the enterprise. With the rapid growth in mobile network usage and video streaming being the most popular service, Quality of Experience of video in mobile networks is of extreme importance to both service providers and their customers. The ability to effectively predict Quality of Experience of video is key for QoE adaptation and higher levels of customer satisfaction. In this work machine learning algorithms were used to create models that predict QoE with network QoS parameters, including wireless-specific and 5Gspecific parameters. An 5G simulation that reflects the current mobile traffic landscape was created to obtain the data set for training. An objective tool for video QoE evaluation was used to gather QoE data necessary to train the prediction models. Support Vector Machines, Random Forest, Gradient Boosted Trees and Neural Networks were chosen as the machine learning algorithms for Quality of Experience prediction, and it was shown that they achieve high accuracy. Influence of wireless-specific parameters on QoE prediction was also investigated, and it was discovered that they are suitable for use in Quality of Experience prediction models.The problem is that; organizations do not know where they either have or may encounter weaknesses in their Enterprise Architecture with Data Driven Architecture (DDA). The framework presented is based on concepts from Wireless Networks with Driven Architecture will be designed to support both Transitional Gap Analysis (TGA) and Comparative Gap Analysis (CGA). TGA is supported by comparing a baseline Data Driven Architecture (DDA) to a target QoE where both DDA have been defined from the management perspective. DDA is facilitated by mapping a QoE to two or more 5G networks. The research methodology used in the paper is design science research for the QoE management based 5G network. The QOE for implementation of 5th generation network and apply it in many different real-world organizations. The goal of the paper is to present a framework in the form an implementation and management model, called QOE, that visualizes the gaps (weaknesses) in proposed or existing enterprise architectures and to support a comparative analysis process for different a5Grnative solution approaches. a set of requirements on the QOE management can be presented and the frameworks are applied on Matlab for implementation.",https://ieeexplore.ieee.org/document/9254863/,2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT),22-24 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC51774.2021.00042,Personalized and Dynamic top-k Recommendation System using Context Aware Deep Reinforcement Learning,IEEE,Conferences,"With the ever-increasing number of choices in just about every domain, ranging from e-commerce platforms to music and video streaming apps, it requires a highly personalized and best-in-class experience to make the users stay. Relevant, dynamic and smart item recommendations play a significant role in providing quality user experience. The majority of existing work either relies on explicit user feedback or gives static recommendations that poorly adapt to the changing user preferences. This paper proposes novel model architectures, namely, Sequential Feature Bandits(SFB) and Cross Feature Bandits(CFB) for providing online top-k context-aware recommendations. It leverages the item features, and uses implicit user feedback with a reinforcement learning backed dynamic algorithm. We use deep-q learning to maximize the cumulative estimated reward(CMR) and click-through rate(CTR) along with an experience replay strategy to prevent the proposed models from getting destabilized. Detailed experiments were conducted on a commercially available news article data set with implicit user-feedback and user-item context features. A thorough experimentation shows an average increase of 23.4% in CTR values and proves the effectiveness of the proposed SFB and CFB architectures for real-time recommendation.",https://ieeexplore.ieee.org/document/9529656/,"2021 IEEE 45th Annual Computers, Software, and Applications Conference (COMPSAC)",12-16 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA.2011.5979792,Physical human robot interaction in imitation learning,IEEE,Conferences,"This video presents our recent research on the integration of physical human-robot interaction (pHRI) into imitation learning. First, a marker control approach for real time human motion imitation is shown. Secondly, physical coaching in addition to observational learning is applied for the incremental learning of motion primitives. Last, we extend imitation learning to learning pHRI which includes the establishment of intended physical contacts. The proposed methods were implemented and tested using the IRT humanoid robot and DLR's humanoid upper-body robot Justin.",https://ieeexplore.ieee.org/document/5979792/,2011 IEEE International Conference on Robotics and Automation,9-13 May 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICIF.2018.8455599,Physics-Based and Human-Derived Information Fusion Video Activity Analysis,IEEE,Conferences,"With ubiquitous data acquired from sensors, there is an ever increasing ability to abstract content from the combination of physics-based and human-derived information fusion (PHIF). The advancement of PHIF tools include graphical information fusion methods, target tracking techniques, and natural language understanding. Current discussions revolve around dynamic data-driven applications systems (DDDAS) which seeks to leverage high-dimensional modeling with real-time physical systems. An example of a model includes a learned dictionary that can be leveraged as information queries. In this paper, we discuss the DDDAS paradigm of sensor measurements, information processing, environmental modeling, and software implementation to deliver content for PHIF systems. Experimental results demonstrate the DDDAS-based Live Video Computing DataBase Modeling Systems (LVC-DBMS) approach affording data discovery and query-based flexibility for awareness to provide narratives of unknown situations.",https://ieeexplore.ieee.org/document/8455599/,2018 21st International Conference on Information Fusion (FUSION),10-13 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2015.7280599,Pixel characteristics based feature extraction approach for roadside object detection,IEEE,Conferences,"Classification of roadside objects is very important task in identifying fire risk regions, analysing roadside conditions and improving roadside safety. This paper introduces a novel and effective way to detect soil, grass, road and tree from roadside images thus giving a better decision-making system for analysing roadside video data. A new feature extraction approach is proposed to detect and classify the roadside objects. Feature set is based on colour characteristics which are obtained by analysing components of image pixels. Choosing an appropriate feature set is one of the great challenges for successful identification of roadside objects. Based on the proposed feature set and the Support Vector Machine, the detection and classification approach is implemented. The proposed approach is evaluated using the training and test data from real-world roadside video images. The results show that the proposed approach is able to accurately detect grass, soil, road and tree.",https://ieeexplore.ieee.org/document/7280599/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData52589.2021.9671866,Placement of DNN Models on Mobile Edge Devices for Effective Video Analysis,IEEE,Conferences,"The pervasive deployment of IoT devices along with the advancements in Deep Neural Network (DNN) models have enabled video analytics at the edge, the so-called Edge AI systems, in support of various large smart-city applications such as automatic road damage evaluation and fire detection. Current solutions require the model developer to make the placement decision by manually assigning models to edge devices. However, an Edge AI solution could entail hundreds of mobile edge devices operating in a large geographical region (e.g., installed on vehicles) with various resource capabilities and different DNN models, hence rendering manual placement ineffective. This paper presents alternative methods to automatically place various models on a diverse set of edge devices, considering the geospatial coverage of video data, resource capabilities of edge devices, and the characteristics of the trained models. First, we mathematically formulate the model placement as an optimization problem which is proven to be NP-Hard. We then propose several heuristics to solve it efficiently and evaluate them with a real-world dataset collected along the 165 bus route trajectories in the City of San Francisco. Our placement algorithm yields a higher recall in object detection and is more robust to the uncertainty of the underlying location context, without sacrificing much utilization cost.",https://ieeexplore.ieee.org/document/9671866/,2021 IEEE International Conference on Big Data (Big Data),15-18 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICISCE48695.2019.00086,Pose Evaluation Method Based on Part-based Hierarchical Bidirectional Recurrent Neural Network,IEEE,Conferences,"Human body posture evaluation can be applied to many important real-world fields such as sports referee assistance. One of the key challenges is to capture the dynamic changes of human bone key points from the video. In this paper, we propose a new network structure based on part-based hierarchical RNN (PHRNN) [6] to analyze the time series information on key points of bones, which can effectively to extract the temporal features of human pose from continuous video frames. We conduct experiments on our own volleyball data set. The experimental results show that the proposed method is effective against attitude evaluation.",https://ieeexplore.ieee.org/document/9107743/,2019 6th International Conference on Information Science and Control Engineering (ICISCE),20-22 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMEW53276.2021.9455974,Practical Video Quality Assessment Of User Generated Content,IEEE,Conferences,"During the past few years, video quality assessment (VQA) of user generated content (UGC) has attracted considerable attention in the research community. In this paper, we propose a practical architecture for a versatile video quality model, designed for assessing user generated videos in particular. The proposed architecture is based on our earlier design of two-level video quality model with a convolutional neural network (CNN-TLVQM), with various improvements and re-designed elements. We have built a fast implementation of the proposed model in C++, demonstrating that the model is practical for real-life applications. The implementation of the model has been submitted for evaluation in ICME UGCVQA Challenge in 2021.",https://ieeexplore.ieee.org/document/9455974/,2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),5-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INM.2015.7140318,Predicting real-time service-level metrics from device statistics,IEEE,Conferences,"While real-time service assurance is critical for emerging telecom cloud services, understanding and predicting performance metrics for such services is hard. In this paper, we pursue an approach based upon statistical learning whereby the behavior of the target system is learned from observations. We use methods that learn from device statistics and predict metrics for services running on these devices. Specifically, we collect statistics from a Linux kernel of a server machine and predict client-side metrics for a video-streaming service (VLC). The fact that we collect thousands of kernel variables, while omitting service instrumentation, makes our approach service-independent and unique. While our current lab configuration is simple, our results, gained through extensive experimentation, prove the feasibility of accurately predicting client-side metrics, such as video frame rates and RTP packet rates, often within 10-15% error (NMAE), also under high computational load and across traces from different scenarios.",https://ieeexplore.ieee.org/document/7140318/,2015 IFIP/IEEE International Symposium on Integrated Network Management (IM),11-15 May 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNSM.2015.7367349,Predicting service metrics for cluster-based services using real-time analytics,IEEE,Conferences,"Predicting the performance of cloud services is intrinsically hard. In this work, we pursue an approach based upon statistical learning, whereby the behaviour of a system is learned from observations. Specifically, our testbed implementation collects device statistics from a server cluster and uses a regression method that accurately predicts, in real-time, client-side service metrics for a video streaming service running on the cluster. The method is service-agnostic in the sense that it takes as input operating-systems statistics instead of service-level metrics. We show that feature set reduction significantly improves prediction accuracy in our case, while simultaneously reducing model computation time. We also discuss design and implementation of a real-time analytics engine, which processes streams of device statistics and service metrics from testbed sensors and produces model predictions through online learning.",https://ieeexplore.ieee.org/document/7367349/,2015 11th International Conference on Network and Service Management (CNSM),9-13 Nov. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/APNOMS50412.2020.9237059,Prequalification of VDSL2 copper customers for G.fast services with artificial intelligence technology,IEEE,Conferences,"In recent years, due to customers have higher requirements for 4K/BK video and high-speed internet, telecom operators have begun to deploy FTTH network, but found that it is generally difficult to deploy fiber to the home, so G.fast technology has been favored by most telecom operators around the world and have begun to actively deploy. For the most widely deploy VDSL2 line with maximum rate that can only provide 100M internet service, a intelligent and accurate G.fast 300M high speed service prequalification technology, is a major research topic for telecom operators to promote 300M high-speed internet service. This paper proposes to use AI machine learning to estimate the G.fast line rate by using VDSL2 line attenuation to meet the real-site provision needs of telecommunications operators.",https://ieeexplore.ieee.org/document/9237059/,2020 21st Asia-Pacific Network Operations and Management Symposium (APNOMS),22-25 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2018.8485962,Proactive Video Push for Optimizing Bandwidth Consumption in Hybrid CDN-P2P VoD Systems,IEEE,Conferences,"Decentralizing content delivery to edge devices has become a popular solution for saving the bandwidth consumption of CDN when the CDN bandwidth is expensive. One successful realization is the hybrid CDN-P2P VoD system, where a client is allowed to request video content from a number of seeds (seed clients) in the P2P network. However, the seed scarcity problem may arise for a video resource when there are an insufficient number of seeds to satisfy requests to the video. To alleviate this problem, many commercial VoD systems have employed a video push mechanism that directly sends the recent scarce video resources to randomly-chosen seeds to serve more requests. However, the current video push mechanism fails to consider which videos will become scarce in the future, or differentiate the uploading capability of different seeds. In this paper, we propose Proactive-Push, a video push mechanism that lowers the bandwidth consumption of CDN by predicting future scarce videos and proactively sending them to competent seeds with strong uploading capabilities. Proactive-Push trains neural network models to correctly predict 80% of future scarce video resources, and identify over 90% of competent seeds. We evaluate Proactive-Push using a trace-driven emulation and a real-world pilot deployment over a commercial VoD system. Results show that Proactive-Push can further reduce the proportion of direct download from CDN by 21%, and save the CDN bandwidth cost at peak time by 18%.",https://ieeexplore.ieee.org/document/8485962/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,16-19 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNNSP.2003.1279183,Proceedings of 2003 International Conference on Neural Networks and Signal Processing (IEEE Cat. No.03EX641),IEEE,Conferences,The following topics are dealt with: neural information processing; supervised learning; unsupervised learning; reinforcement learning; neural modeling; neural architectures; neurodynamics; learning theory; neural networks for image and signal processing; CMOS implementation of neural networks; neural optimization; neural control; hardware implementation; neural real world applications; evolutionary computation; evolutionary design; evolutionary neural networks; hybrid systems; signal processing theory; digital filters; chaos analysis; time series analysis; adaptive signal processing; speech processing; image processing; video signal processing; array signal processing; multi-channel signal processing; independent component analysis; blind signal separation; blind channel estimation; blind equalization; MIMO communications; digital watermarking; network signal processing; and information security.,https://ieeexplore.ieee.org/document/1279183/,"International Conference on Neural Networks and Signal Processing, 2003. Proceedings of the 2003",14-17 Dec. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMEAE.2015.24,Proposal of a Computer Vision System to Detect and Track Vehicles in Real Time Using an Embedded Platform Enabled with a Graphical Processing Unit,IEEE,Conferences,"This paper presents a development proposal of a systemcapable of detecting, localizing and tracking vehicles inreal time using computer vision algorithms in an embeddedplatform for an advanced driver assistance system (ADAS). This project will use a Flea3 monocular color camera fromPointGrey. The video will be processed using support vectormachines (SVM) and convolutional neural networks (CNN). These algorithms will be implemented in a Jetson TK1 developmentboard from Nvidia which has a 192-core KeplerGPU, a quad-core ARM Cortex A15, 2 GB of RAM, 16 GBof internal memory, and a set of basic peripherals for automotiveapplications. Finally, this project will use librariessuch as: CUDA, OpenCV, and CudNN which are optimizedfor the Jetson TK1 as well as the LibSVM and Caffe librariesto train SVM and CNN models.",https://ieeexplore.ieee.org/document/7386198/,"2015 International Conference on Mechatronics, Electronics and Automotive Engineering (ICMEAE)",24-27 Nov. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.541,QoS Performance Analysis in Deployment of DiffServ-aware MPLS Traffic Engineering,IEEE,Conferences,"It is a trend that the integrated voice, video and data will be transported in the converged IP/MPLS core network. The combined use of the differentiated services (DiffServ) and multiprotocol label switching (MPLS) technologies is envisioned to provide guaranteed quality of service (QoS). However, such a scheme only dictates per hop behavior (PHB) and it does not control the end to end path the traffic is taking. If some link of the path is congested, packets will be dropped and QoS can not be guaranteed. Another attractive application of MPLS is for Traffic Engineering (TE), which sets up end to end routing path before forwarding data. Unfortunately, MPLS TE only reserves resource in one aggregated class, so that it can not provide QoS for differentiated services. MPLS DiffServ-aware TE makes MPLS TE aware of QoS, by combining the functionalities of both DiffServ and TE. In this paper, the QoS performance is analyzed for different type of services including VoIP, Real time Video, and best effort data traffic. The results show that the guaranteed bandwidth service can give better QoS for real time traffic such as VoIP, but worse QoS for the variable video traffic.",https://ieeexplore.ieee.org/document/4287988/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC.2012.6181070,Quality of experience estimation for adaptive HTTP/TCP video streaming using H.264/AVC,IEEE,Conferences,"Video services are being adopted widely in both mobile and fixed networks. For their successful deployment, the content providers are increasingly becoming interested in evaluating the performance of such traffic from the final users' perspective, that is, their Quality of Experience (QoE). For this purpose, subjective quality assessment methods are costly and can not be used in real time. Therefore, automatic estimation of QoE is highly desired. In this paper, we propose a no-reference QoE monitoring module for adaptive HTTP streaming using TCP and the H.264 video codec. HTTP streaming using TCP is the popular choice of many web based and IPTV applications due to the intrinsic advantages of the protocol. Moreover, these applications do not suffer from video data loss due to the reliable nature of the transport layer. However, there can be playout interruptions and if adaptive bitrate video streaming is used then the quality of video can vary due to lossy compression. Our QoE estimation module, based on Random Neural Networks, models the impact of both factors. The results presented in this paper show that our model accurately captures the relation between them and QoE.",https://ieeexplore.ieee.org/document/6181070/,2012 IEEE Consumer Communications and Networking Conference (CCNC),14-17 Jan. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FPL53798.2021.00089,Quantised Siamese Tracker for 4K/UltraHD Video Stream – a demo,IEEE,Conferences,"This demo presents a hardware architecture for an object tracker based on a quantised Siamese neural network. The system is designed to work with a 4K/UltraHD input video stream and to meet the real time and energy efficiency constraints. The network is designed using Brevitas and implemented with FINN tool from Xilinx. The Siamese tracker runs on the ZCU 104 board, with the Zynq UltraScale+ MPSoC device from Xilinx.",https://ieeexplore.ieee.org/document/9556326/,2021 31st International Conference on Field-Programmable Logic and Applications (FPL),30 Aug.-3 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/icp.2021.1338,RESEARCH ON REAL-TIME DETECTION METHOD OF FACE WEARING MASK WITH LARGE TRAFFIC BASED ON DEEP LEARNING,IET,Conferences,"Aiming at the problem of low accuracy of traditional face detection methods for large-volume mask-wearing people during the prevention and control of the new crown pneumonia epidemic, this paper proposes a real-time detection method for large-volume mask-wearing faces based on deep learning. The method uses the overall design of the backbone network, the FPN feature fusion network, the detection network and the parameter optimization method of the algorithm, and completes the model training on the mask-wearing face training set. In the detection process, the NMS algorithm is used to post-process the prediction results to realize multi-scale perception of the input face and improve the detection accuracy of the face wearing a mask. Experimentally verified, the detection accuracy of this method on the mask-wearing face test set is 0.919, and the average of Easy-0.841, Medium-0.802 and Hard-0.600 is obtained on the three subsets of the WIDER FACE test set. Detection accuracy (mAP). Compared with traditional face detection methods, it has universal advantages, and the video inference speed of the method in this paper reaches 55fps, which can meet the task requirements of real-time face detection with large traffic. In addition, the project team has successfully deployed this method to a fully automatic infrared thermal imaging temperature measurement warning system and put it into use in many places in Beijing, which is of great significance to preventing the spread of the epidemic.",https://ieeexplore.ieee.org/document/9538372/,The 8th International Symposium on Test Automation & Instrumentation (ISTAI 2020),28-29 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNS50378.2020.9223013,RF Fingerprint Measurement For Detecting Multiple Amateur Drones Based on STFT and Feature Reduction,IEEE,Conferences,"Underlying the easy accessibility and popularity of amateur unmanned aerial vehicles (UAVs, or drones), an effective multi-UAV detection method is desired. In this paper, we proposed a novel radio frequency (RF) signal detection method for recognizing multiple UAVs’ intrusion. The single transient control and video signal is transformed by Short Time Fourier Transform (STFT) to obtain its time-frequency-energy distribution features. To reduce the dimensionality of the RF feature vector, the principal component analysis (PCA) is applied in the signal characteristic subspace transformation. A remapped UAVs RF signal feature data is used in the training of the support vector machine (SVM) and K-nearest neighbor (KNN) algorithm for classifying the presence and number of intruding UAVs. In addition, a real-time test of UAV attacks on an airport area is implemented. The test results show that the accuracy for detecting the number of intruding UAVs is effective. This method could similarly apply to protect the public from unsafe and unauthorized UAV operations near security sensitive facilities.",https://ieeexplore.ieee.org/document/9223013/,2020 Integrated Communications Navigation and Surveillance Conference (ICNS),8-10 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSCI51800.2020.00085,Radically Simplifying Game Engines: AI Emotions & Game Self-Evolution,IEEE,Conferences,"Today, video games are a multi-billion-dollar industry, continuously evolving through the incorporation of new technologies and innovative design. However, current video game software content creation requires extensive and often-times ambiguous planning phases for developing aesthetics, online capabilities, and gameplay mechanics. Design elements can vary significantly relative to the expertise of artists, designers, budget, and overall game engine/software features and capabilities. Game development processes are often extensively long coding sessions, usually involving a highly iterative creative process, where user requirements are rarely provided. Therefore, we propose significantly simplifying game design and development with novel Artificial Cognition Architecture real-time scalability and dynamic emotion core. Rather than utilizing more static emotion state weighting emotion engines (e.g. ExAI), we leverage significant ACA research in successful implementation of analog neural learning bots with Maslowan objective function algorithms. We also leverage AI- based Artificial Psychology software which utilizes ACA's fine grained self-evolving emotion modeling in humanistic avatar patients for Psychologist training. An ACA common cognitive core provides the gaming industry with wider applications across video game genres. A modular, scalable, and cognitive emotion game architecture implements Non-Playable Character (NPC) learning and self-evolution. ACA models NPC's with fine grained emotions, providing interactive dynamic personality traits for a more realistic game environment and enables NPC self-evolution under the influence of both other NPC's and players. Furthermore, we explore current video game design engine architecture (e.g. Unity, Unreal Engine) and propose an ACA integration approach. We apply artificial cognition and emotion intelligence modeling to engender video games with more distinct, realistic consumer gaming experiences, while simultaneously minimizing software gaming development efforts and costs.",https://ieeexplore.ieee.org/document/9457899/,2020 International Conference on Computational Science and Computational Intelligence (CSCI),16-18 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP40778.2020.9190763,"Railcar Detection, Identification and Tracking for Rail Yard Management",IEEE,Conferences,"We present a video analytics system combining railcar detection, classification, Federal Railroad Admin. (FRA) text identification, and logo detection into a system for locomotive transportation and yard management. Existing RFID-based systems are limited by sensor deployment and cannot visually identify railcars when they are away. As there are typically tens of tracks and hundreds of railcars in a yard, an automatic vision system is desirable. The proposed AI system is developed for autonomous yard inventory checking, such that the arrival, departure, and movement of individual railcars can be automatically monitored and managed in the facility. Our system consists of multiple cameras with edge computing devices installed at check points (track entrances and branches), such that visual detection and tracking of railcars can be performed and meta-data can be exchanged. After knowing the railcar locations and types, scene text detection is performed to search and recognize FRA ID markings and logos that can uniquely identify each railcar. Information fusion a database in the central hub can further improve railcar identification and reduce errors. Early results on real-world field collected data demonstrate the efficacy of the proposed approach.",https://ieeexplore.ieee.org/document/9190763/,2020 IEEE International Conference on Image Processing (ICIP),25-28 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTA49490.2019.9144867,Real Time Anomaly detection-Based QoE Feature selection and Ensemble Learning for HTTP Video Services,IEEE,Conferences,"Using Machine Learning to the user perception prediction has largely increased in the last decade. Although it is difficult to deduce the best category model to address the predicted Quality of Experience QoE in operational networks. Also, sometimes the result obtained by ML is not relevant i.e not expected according to the present conditions. We talk here about problem of anomaly in Machine Learning context. To fill this gap, in this paper, we try to solve the problem of Machine Learning ML anomaly detection in the QoE context for video streaming service in Software Defined Networking SDN environment. We explore in one hand feature selection methodology to extract the most correlated features to the video QoE. In other hand we investigate different ensemble learning approaches to improve the detection of QoE anomalies following the known stacking or super learning model. Results show that the proposed model presents a high performance comparing to other types of ensemble learning and single learning techniques.",https://ieeexplore.ieee.org/document/9144867/,2019 7th International conference on ICT & Accessibility (ICTA),13-15 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOSEC49089.2020.9215415,Real Time Automatic Detection of Motorcyclists With and Without a Safety Helmet,IEEE,Conferences,"In the developing countries like India, the motorcycle riders are increasing day-by-day, wherein it also constitutes to the unprecedented increase in the number of motorcycle accidents across the country. To overcome this drawback, the proposed research work explains and demonstrates a method to enforce better safety protocols through the automatic detection of motorcyclists with and without a safety helmet by using a real-time traffic surveillance footage. The real-time automatic detection of motorcyclists with and without a safety helmet is established through detecting a vehicle and track pipelining it with OpenCV, sklearn, utilizing a descriptor known as the histogram of oriented gradients (HOG), and support vector classification (SVC), which are the combination of tools pertaining to machine learning and image processing mechanisms. With OpenCV Library method, a bike rider is identified in the surveillance video. Further by using a popular machine learning algorithm model called LinearSVC, the classifier label identifies whether the rider is wearing a safety helmet. The data attained in correspondence to the count of bike riders with and without safety helmet is stored in MySQL database with respective timestamps and is also visualized through tabular and graphical views in the developed desktop interface application. With 87.6% model accuracy, our paper proposes a solution to enhance the existing safety measures and provide a time-efficient approach to handle traffic regulations.",https://ieeexplore.ieee.org/document/9215415/,2020 International Conference on Smart Electronics and Communication (ICOSEC),10-12 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IICAIET49801.2020.9257843,Real Time Bangla Number Plate Recognition using Computer Vision and Convolutional Neural Network,IEEE,Conferences,"Automatic number plate identification in today's world plays a vital role in vehicle tracking and organization. Our proposed model of automation in the detection and recognizing vehicles through the use of number plate computerization is expected to create a new scope of evolution for large cities. The system can be used for the parking system of motor vehicles, as well as to collect tolls. The detection of the Bangla number plates from different cities and multi-class vehicles is the first step of the proposed system. The number plate detection has been performed with the computer vision approach, and the You Only Look Once v3 (YOLOv3) algorithm. Next, the Tesseract optical character recognition system, in conjunction with the Bangla character recognition model, has been used for vehicle indexing and convolutional neural network for the character recognition from the detected number plate. Numerical results demonstrate that the accuracies of license plate detection for the computer vision and YOLOv3 are 91% and 95%, respectively. For the character recognition, the accuracy for Tesseract and convolutional neural network are 90% if the license plate is detected and cropped successfully and 91.38%, respectively. Finally, our system has been tested using the convolutional neural network method in an environment of real-world where our system's Pi Camera captured video as input, which has a total of 18 different cars. From 18 cars, it has successfully detected 17 cars, which makes our overall system accuracy 88.89%.",https://ieeexplore.ieee.org/document/9257843/,2020 IEEE 2nd International Conference on Artificial Intelligence in Engineering and Technology (IICAIET),26-27 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2007.4370512,Real Time Emotion-Diagnosis of Video Game Players from their Facial Expressions and its Applications to Voice Feed-Backing to Game Players,IEEE,Conferences,"There has been a considerable amount of research done into the detection and evaluation of human emotions from implicit communication channels, including facial expressions. However, most studies have extracted facial features for some specific emotions in specific situations. This paper describes a real time emotion diagnosis system, which can judge the emotions of a video game player from his/her facial expression and navigate him/her by voice feed-backing messages. Criteria for classifying six emotions were established using a time sequential subjective evaluation of the subject's emotions as well as the time sequential analysis of the subject's facial expressions. A voice feed-back system for game players was constructed by using the above mentioned emotion criteria, emotion diagnosis algorithm and synthesized voice generation software.",https://ieeexplore.ieee.org/document/4370512/,2007 International Conference on Machine Learning and Cybernetics,19-22 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEEIE52663.2021.9616846,Real Time Implementation of Drone Detection using TensorFlow and MobileNetV2-SSD,IEEE,Conferences,"Drones are Unmanned Aerial Vehicles (UAV) that can be controlled by remote control or software methods. In recent years with the developing technology, drones have started to be used in all areas of our lives. The two most important factors that make drones attractive to use are low price and easy control. Today, UAVs, which provide convenience to people in many fields such as photography, video shooting, advertising, firefighting and search-rescue activities. Also, UAV is used by terrorist organizations in attack actions. Anti-drone systems are used to detect and locate such malicious drones. In this study, a prototype drone detection system called ATA-DRN is designed against drone threats. While creating ATA-DRN, 2 digital servo motors, Raspberry Pi 4 development board, high resolution Web Cam, 7inch HDMI LCD and power supply is used. With this designed system, drone approaching to the target is automatically detected with image processing methods without the need for manpower. In the study, first of all, a data set is obtained from pictures of different types of drones. The data set obtained is divided into two groups as train (80%) and test (20%) in order to train. The model to be used in real-time drone analysis is obtained using TensorFlow and MobileNetV2-SSD. Experiments are carried out by transferring the obtained model to the Raspberry Pi 4 development board. The accuracy of the model created as a result of experimental studies has been proven.",https://ieeexplore.ieee.org/document/9616846/,"2021 7th International Conference on Electrical, Electronics and Information Engineering (ICEEIE)",2-2 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPC2T53885.2022.9776899,Real Time Multiple-Object Detection Based On Enhanced SSD,IEEE,Conferences,"Object detection and recognition are widely employed in a variety of computer vision applications, and numerous techniques have been suggested in the literature. Object detection is one of the areas of computer vision that has seen a lot of achievement. In the last decade, various applications of computer vision have been presented. Some of these include autonomous cars, video surveillance, and scene interpretation. Visual recognition systems, including image categorization, localization, and detection, are at the heart of all of these applications and have gathered a lot of research attention. These visual identification algorithms have achieved extraordinary performance because of considerable advancements in neural networks, particularly deep learning (DL). Object detection techniques in real-time systems, on the other hand, necessitate minimal computing time and a thorough performance assessment prior to deployment. In this paper multiple image detection approach is used to identify and locate the object concerning their class label, and an enhanced feature map is used for object detection. All this is possible due to the recent advancement in DL along with image processing. These images, fed to the model, come from the video source. To tackle the problem related to the aspect ratio, the model uses the multi-scale feature map together with a distinct filter for diverse default boxes.",https://ieeexplore.ieee.org/document/9776899/,"2022 Second International Conference on Power, Control and Computing Technologies (ICPC2T)",1-3 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FABS52071.2021.9702559,Real time alert system to prevent Car Accident,IEEE,Conferences,"The use of artificial intelligence to solve a complex problem and to solve a complex issue such as foreseeing and avoiding accidents on roads implies a robust and proven procedure. An effective and proven method is needed to predict and prevent accidents on the road. Artificial vision is an area of artificial intelligence to explain and understand the visual world. With machine vision and Intelligent Transportation Systems (ITS), we can provide drivers with a safety net. These technologies help reduce human error in the automotive industry and provide riders with tools and features to help them avoid serious mistakes and accidents. In this paper we present a Drowsiness Detection System which indicates the drowsiness of the car driver by detecting the drivers face in real time video and considering the various eye landmarks indicates the ratio of each eye telling how much the driver&#x2019;s eyes are open or close. This system is developed using the state-of-art Computer Vision technology making it easy to implement highly effective. This system is implemented as an Android Application which uses android mobile back camera for recording car driver&#x2019;s live video and implementing the above developed model to calculate the eye aspect ratio and indicate it along with the eye in the video.",https://ieeexplore.ieee.org/document/9702559/,"2021 International Conference on Forensics, Analytics, Big Data, Security (FABS)",21-22 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ATSIP49331.2020.9231902,"Real time emotion recognition in video stream, using B-CNN and F-CNN",IEEE,Conferences,"Despite the diversity of methods developed in recent years, the implementation of an efficient system for automatic recognition of facial emotions remains a technological challenge that has not been fully resolved. Many problems have not yet been resolved. The occlusion problem remains a challenge today for the research community, certain features characterizing several different emotions may seem similar, etc. High performing and precise techniques are therefore necessary to perfectly distinguish between two different emotions, even though they might be difficult to distinguish. The objective of this work is the development of an automatic method for recognizing basic facial emotions (joy, anger, sadness, disgust, surprise, fear and neutral) in video streams. The method of deep learning, known for its great performance in image classification, becomes essential. In order to be able to benefit from several feature maps at the same time, we propose to use two techniques: bilinear pooling (B-CNN), and Fusion Feature Net (F-CNN). This technique is more efficient and more precise than conventional techniques, whether based on deep learning or not.",https://ieeexplore.ieee.org/document/9231902/,2020 5th International Conference on Advanced Technologies for Signal and Image Processing (ATSIP),2-5 Sept. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2012.6252595,Real time gesture recognition system using posture classifier and Jordan recurrent neural network,IEEE,Conferences,"This paper proposes a Jordan recurrent neural network (JRNN) based dynamic hand gesture recognition system. A set of allowed gestures is modeled by a sequence of representative static images, i.e., postures. The proposed system first classifies the input postures contained in the input video frames, and the JRNN finds the input gesture by detecting the temporal behavior of the posture sequence. To enhance the ability of the JRNN to identify the temporal behavior of its input sequence, a new training method has been proposed. Due to the proposed method, the system can recognize the reverse gestures. Implemented on a PC equipped with a USB camera for the live image acquisition, the proposed system can process 12.5 frames per second (fps). Experimental results show that the system can recognize 5 gestures with the accuracy of 99.0%, and recognize 9 gestures with 94.3% accuracy, respectively.",https://ieeexplore.ieee.org/document/6252595/,The 2012 International Joint Conference on Neural Networks (IJCNN),10-15 June 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FIE.1999.839219,Real time object recognition for teaching neural networks,IEEE,Conferences,"Undergraduate students in computer science learn best when they are given the opportunity to apply hardware and software concepts to real world systems, and neural-network applications present attractive possibilities for giving them such opportunity. An example of how to take advantage of these possibilities is given in this paper, which describes a specific neural network technique that has been developed and applied to the problem of identifying real world objects in real time. Those objects can be as simple as paper cut-outs or they can be mechanical objects such as a nut or a bolt. These objects are placed on a plane, and are ""examined"" by an ""identifying system"" consisting of a camera attached to a PC through a video capture card. The pixels collected from the image of the object are fed to a set of neural network nodes for pattern recognition. Patterns are recognized by a multi-layer neural network where the output of a neuron, which can be characterized by sigmoid ""activation function"", is a function of weighted inputs. Lisp is used as the programming language due to its simple syntax and powerful recursive features for processing lists. The binary equivalent of the computed output is evaluated as a recognition signature which is compared to the signature of objects in a list. Of course, the artificial neural network is ""trainable"". To master the technique, the students start by learning about neurons and forward and backpropagation methods, but they soon find themselves ""training"" a multilayered neural network that they themselves have built. The learning experience encompasses video capturing, image handling, filtering, and image compression, and it demystifies neural network programming.",https://ieeexplore.ieee.org/document/839219/,FIE'99 Frontiers in Education. 29th Annual Frontiers in Education Conference. Designing the Future of Science and Engineering Education. Conference Proceedings (IEEE Cat. No.99CH37011,10-13 Nov. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TENCON.2018.8650524,Real-Time American Sign Language Recognition Using Skin Segmentation and Image Category Classification with Convolutional Neural Network and Deep Learning,IEEE,Conferences,A real-time sign language translator is an important milestone in facilitating communication between the deaf community and the general public. We hereby present the development and implementation of an American Sign Language (ASL) fingerspelling translator based on skin segmentation and machine learning algorithms. We present an automatic human skin segmentation algorithm based on color information. The YCbCr color space is employed because it is typically used in video coding and provides an effective use of chrominance information for modeling the human skin color. We model the skin-color distribution as a bivariate normal distribution in the CbCr plane. The performance of the algorithm is illustrated by simulations carried out on images depicting people of different ethnicity. Then Convolutional Neural Network (CNN) is used to extract features from the images and Deep Learning Method is used to train a classifier to recognize Sign Language.,https://ieeexplore.ieee.org/document/8650524/,TENCON 2018 - 2018 IEEE Region 10 Conference,28-31 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RAAICON48939.2019.45,Real-Time Bangla License Plate Recognition System using Faster R-CNN and SSD: A Deep Learning Application,IEEE,Conferences,"Traffic control and vehicle owner identification become major problems in Bangladesh. Most of the time it is difficult to identify the driver or the owner of the vehicles who violate the traffic rules or do any accidental work on the road. Moreover, it is very time-consuming for a traffic police officer to physically check the license plate of every vehicle. So, an automatic license plate recognition system is a much-needed solution to solve these problems. The existing Bangla license plate recognition systems are mostly based on character segmentation and these methods are not implemented in real-time. In this study, two separate Deep Convolutional Neural Network (DCNN) models are used to identify the license plate and the characters on the license plate from the real-time video streaming. The first CNN model detects the license plate from the live video of a vehicle on the road. Than it crop the license plate area from the video frames. The cropped frame is then fed into the second CNN to detect the characters on that license plate. The characters are detected as individual objects. After detecting all the characters and numbers on the license plate, they are rearranged according to their position on the plate. To train the proposed model total of 292 images are collected used. Moreover, an open-sourced Bangla handwritten character dataset named BanglaLekha-Isolated is also used to train the model with synthetic character data. The trained model is tested using 18 live videos and 6 still image data. Finally, the proposed methodology gains a 100% precision on detecting the license plate, and 91.67% precision for detecting the characters on the license plate for the given test dataset.",https://ieeexplore.ieee.org/document/9087526/,"2019 IEEE International Conference on Robotics, Automation, Artificial-intelligence and Internet-of-Things (RAAICON)",29 Nov.-1 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DASC52595.2021.9594332,Real-Time Drone Surveillance System for Violent Crowd Behavior Unmanned Aircraft System (UAS) – Human Autonomy Teaming (HAT),IEEE,Conferences,"Unmanned Aerial Systems (UASs), or drones, continue to increase in capabilities and sophistication across a wide range of applications. UASs have high mobility, are easily deployed, and capable of real-time monitoring of crowd behavior by utilizing multi-sensor-based detection and remote sensing of objects. These capabilities make UASs a very useful tool for Human Autonomy Teaming (HAT) applications, such as Law Enforcement (LE), capitalizing on Human Factors (HF). This study examines the concept of leveraging drone technology together with Artificial Intelligence (AI) and Machine Learning (ML) methods to produce a UAS system that can assist LE in the monitoring and assessment of crowd behaviors during peaceful and non-peaceful events. LE agencies are increasingly being tasked with engaging in dynamic environments that exist at public events. Utilized as a force multiplier and autonomous tool, would benefit from an AI-UAS platform utilizing artificial intelligence assisting in identifying behavior of peaceful people as opposed to malevolent participants or instigators that may attempt to take control. AI-UASs of this type would allow LE to leverage existing resources within their organizational structures and provide increased situation awareness via a Live Virtual Constructive (LVC) broadcast and monitoring of these dynamic environments. Information provided from these AI-UAS systems would provide real-time information to field forces as well as command and control operations that may be remotely located. AI-UAS Sensors can be dynamically allocated as needed for monitoring/documenting crowd behavior and police actions. Video recordings would provide evidence in court as well counter truth-bending recordings published by professional protestors and agenda driven main-stream media outlets. The benefits and impact of this type of LE AI-UAS platform would be profound. Traditional visible light based sensors can be greatly influenced by environmental factors preventing their ability to determine variations regarding abnormal crowd behaviors. In order to overcome this challenge, this project proposes to utilize four types of collection methods, Multitask Cascading CNN (MC-CNN), ScatterNet Hybrid Deep Learning Network, multiscale infrared optical flow (MIR-OF), and Event Cameras such as Event-based Vision, and Event Camera SLAM (Simultaneous Localization and Mapping). AI methods will be developed to monitor crowd density, average ground speed, human pose estimations, and movement behaviors, as well as identification of primary violent instigators. This proposed system will detect violent individuals in real-time by leveraging onboard image processing as well as cloud processing. Fundamental research for this project is inspired and built upon recent Drone Surveillance System (DSS) publications from IEEE and MDPI.",https://ieeexplore.ieee.org/document/9594332/,2021 IEEE/AIAA 40th Digital Avionics Systems Conference (DASC),3-7 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVCNZ.2018.8634661,Real-Time Drone Surveillance and Population Estimation of Marine Animals from Aerial Imagery,IEEE,Conferences,"Video analysis is being rapidly adopted by marine biologists to asses the population and migration of marine animals. Manual analysis of videos by human observers is labor intensive and prone to error. The automatic analysis of videos using state-of-the-art deep learning object detectors provides a cost-effective way for the study of marine animals population and their ecosystem. However, there are many challenges associated with video analysis such as background clutter, illumination, occlusions, and deformation. Due to the high-density of objects in the images and sever occlusion, current state-of-the-art object often results in multiple detections. Therefore, customized Non-Maxima-Suppression is proposed after the detections to suppress false positives which significantly improves the counting and mean average precision of the detections. An end-to-end deep learning framework of Faster-RCNN [1] was adopted for detections with base architectures of VGG16 [2], VGGM [3] and ZF [4].",https://ieeexplore.ieee.org/document/8634661/,2018 International Conference on Image and Vision Computing New Zealand (IVCNZ),19-21 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INCET54531.2022.9824894,Real-Time Emotional Analysis from A Live Webcam Using Deep Learning,IEEE,Conferences,"In the Covid-19 age, we are becoming increasingly reliant on virtual interactions like as Zoom and Google meetings / Teams chat. The videos received from live webcamera in virtual interactions become great source for researchers to understand the human emotions. Due to the numerous applications in human-computer interaction, analysis of emotion from facial expressions has piqued the interest of the newest research community (HCI). The primary objective of this study is to assess various emotions using unique facial expressions captured via a live web camera. Traditional approaches (Conventional FER) rely on manual feature extraction before classifying the emotional state, whereas Deep Learning, Convolutional Neural Networks, and Transfer Learning are now widely used for emotional classification due to their advanced feature extraction mechanisms from images. In this implementation, we will use the most advanced deep learning models, MTCNN and VGG-16, to extract features and classify seven distinct emotions based on their facial landmarks in live video. Using the FER2013 standard dataset, we achieved a maximum accuracy of 97.23 percent for training and 60.2 percent for validation for emotion classification.",https://ieeexplore.ieee.org/document/9824894/,2022 3rd International Conference for Emerging Technology (INCET),27-29 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACMI53878.2021.9528130,Real-Time Facemask Detection and Analytics,IEEE,Conferences,"The COVID-19 pandemic has emphasized the need for preventive measures in any pandemic situation. As we are now well aware, wearing a mask while stepping out in public places is imperative. In this context, our current work aims to build an end-to-end application-based system that scans people’s faces to determine whether they are wearing a face mask or not, or if they are wearing it improperly. It is a real-time system that provides feedback instantly. At the same time, different types of data obtained from the triple classification system are stored in a simple database to further visualize the same. The system we have created utilizes Single Shot Detector (SSD), a deep learning architecture, to extract a face from a given video frame, and MobileNetV2, another deep learning architecture, to classify it into one of the three categories mentioned above. Those without or improperly wearing a mask, are warned by the system. This work also makes use of QR (Quick Response) codes to identify the people in the video feed. Dataframes from Pandas in Python are used to organize the acquired data into a structured format which is then exported to Tableau. From this, we have visualized and observed the data and from the inferences made, meaningful insights have been arrived at, which can then be used to make informed decisions.",https://ieeexplore.ieee.org/document/9528130/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IHMSC.2018.10109,Real-Time Flow Identification Based on Neural Network and OpenFlow Over SDN,IEEE,Conferences,"This paper introduces a novel approach to flow identification by applying a backpropagation Neural Network over SDN. Thanks to the advantages of the global abstract of SDN controller, we get a convenient way to collect data from data-path and implement a real-time predictor of traffic types. Some flow features are selected from OpenFlow counters for each flow. In addition, we get some other flow features by intermittently duplicating flows and leading them to the controller. The data from OpenFlow counters and duplicated flows are integrated as input vector for Neural Network. Especially, we take into consideration packet length distribution and connection count with the same source and destination IP, which improves the accuracy and efficiency of video flow and P2P flow identification.",https://ieeexplore.ieee.org/document/8530170/,2018 10th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC),25-26 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV48630.2021.00350,Real-Time Gait-Based Age Estimation and Gender Classification from a Single Image,IEEE,Conferences,"In this paper, we propose a unified real-time framework for gait-based age estimation and gender classification that uses just a single image, which reduces the latency in video capturing compared with the existing methods based on a gait cycle. To cope with the problem of lacking motion information in the input single image, we first reconstruct a gait cycle of a silhouette sequence from the input image via a gait cycle reconstruction network. The reconstructed gait cycle is then fed into a state-of-the-art gait recognition network for feature representation learning, which is further used to obtain the class of the gender and the estimated probability distribution of integer age labels. Unlike the existing methods focusing on the gait sequences captured from the side view, the proposed method is applicable to the gait images from an arbitrary view with a single trained model, which is more suitable for real-world application scenarios (e.g., automatic access control). Stand-alone and client-server online systems were implemented based on the proposed method, which validates the real-time/online property in actual scenes. The experiments on the world's largest multi-view gait dataset demonstrate the effectiveness of the proposed method, which achieves performance improvement compared with the benchmark algorithms.",https://ieeexplore.ieee.org/document/9423216/,2021 IEEE Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDGE.2018.00025,Real-Time Human Detection as an Edge Service Enabled by a Lightweight CNN,IEEE,Conferences,"Edge computing allows more computing tasks to take place on the decentralized nodes at the edge of networks. Today many delay sensitive, mission-critical applications can leverage these edge devices to reduce the time delay or even to enable real-time, online decision making thanks to their onsite presence. Human objects detection, behavior recognition and prediction in smart surveillance fall into that category, where a transition of a huge volume of video streaming data can take valuable time and place heavy pressure on communication networks. It is widely recognized that video processing and object detection are computing intensive and too expensive to be handled by resource-limited edge devices. Inspired by the depthwise separable convolution and Single Shot Multi-Box Detector (SSD), a lightweight Convolutional Neural Network (L-CNN) is introduced in this paper. By narrowing down the classifier's searching space to focus on human objects in surveillance video frames, the proposed L-CNN algorithm is able to detect pedestrians with an affordable computation workload to an edge device. A prototype has been implemented on an edge node (Raspberry PI 3) using openCV libraries, and satisfactory performance is achieved using real-world surveillance video streams. The experimental study has validated the design of L-CNN and shown it is a promising approach to computing intensive applications at the edge.",https://ieeexplore.ieee.org/document/8473387/,2018 IEEE International Conference on Edge Computing (EDGE),2-7 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IST55454.2022.9827719,Real-Time Hysteresis Foreground Detection in Video Captured by Moving Cameras,IEEE,Conferences,"Foreground detection is an important first step in video analytics. While the stationary cameras facilitate the foreground detection due to the apparent motion between the moving foreground and the still background, the moving cameras make such a task more challenging because both the foreground and the background appear in motion in the video. To tackle this challenging problem, an innovative real-time foreground detection method is presented, that models the foreground and the background simultaneously and works for both moving and stationary cameras. In particular, first, each input video frame is partitioned into a number of blocks. Then, assuming the background takes the majority of each video frame, the iterative pyramidal implementation of the Lucas-Kanade optical flow approach is applied on the centers of the background blocks in order to estimate the global motion and compensate for the camera movements. Subsequently, each block in the background is modeled by a mixture of Gaussian distributions and a separate Gaussian mixture model is constructed for the foreground in order to enhance the classification. However, the errors in motion compensation can contaminate the foreground model with background values. The novel idea of the proposed method matches a set of background samples to their corresponding block for the most recent frames in order to avoid contaminating the foreground model with background samples. The input values that do not fit into either the statistical or the sample-based background models are used to update the foreground model. Finally, the foreground is detected by applying the Bayes classification technique to the major components in the background and foreground models, which removes the false positives caused by the hysteresis effect. Experimental evaluations demonstrate the feasibility of the proposed method in the foreground segmentation when applied to videos in public datasets.",https://ieeexplore.ieee.org/document/9827719/,2022 IEEE International Conference on Imaging Systems and Techniques (IST),21-23 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/APNOMS52696.2021.9562691,Real-Time License Plate Recognition and Vehicle Tracking System Based on Deep Learning,IEEE,Conferences,"Traditional license plate recognition technology mostly uses traditional image processing methods to find out the characteristics of the license plate, and then crop and recognize the characters. The process needs to be modified due to the different environments, scenes and conditions. In recent years, many studies have implemented license plate and character recognition by using deep learning algorithms. Although it has a good recognition accuracy, the calculation speed still cannot reach the level of real-time recognition. This research proposes a real-time license plate recognition system based on YOLOv3, which uses deep learning model to realize the vehicle license plate recognition, lane identification and vehicle trajectory tracking. In this study, a web-based platform is established to present the result of license plate recognition and trajectory, and the streaming roadside video in the campus. In the platform, license plates of driving vehicles can be identified in real-time, and the user can search and track specific vehicle intuitively. In the experiment, the average accuracy of the system performs 84.3&#x0025; in real-time license plate recognition, and 100&#x0025; in lane identification. The system can process in 40 FPS, which can meet the level of real-time system. In the future, the system can cooperate with traffic access control in campus or community to improve the efficiency of traffic control.",https://ieeexplore.ieee.org/document/9562691/,2021 22nd Asia-Pacific Network Operations and Management Symposium (APNOMS),8-10 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DEVLRN.2005.1490961,Real-Time Multi-View Face Tracking for Human-Robot Interaction,IEEE,Conferences,"For face tracking in a video sequence, various face tracking algorithms have been proposed. However, most of them have difficulty in finding the initial position and size of a face automatically. In this paper, we present a fast and robust method for fully automatic multi-view face detection and tracking. Using a small number of critical rectangle features selected and trained by the Adaboost learning algorithm, we can detect the initial position, size and view of a face correctly. Once a face is reliably detected, we can extract face and upper body color distribution from the detected facial regions and upper body regions for building robust color modeling respectively. Simultaneously, each color modeling is performed by using k-means clustering and multiple Gaussian models. Then, fast and efficient multi-view face tracking is executed by using several critical features. Our proposed algorithm is robust to rotation, partial occlusions, and scale changes in front of dynamic, unstructured background. In addition, our proposed method is computationally efficient. Therefore, it can be executed in real-time",https://ieeexplore.ieee.org/document/1490961/,"Proceedings. The 4th International Conference on Development and Learning, 2005",19-21 July 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC.2018.00074,Real-Time Orthopedic Surgery Procedure Recognition Method with Video Images from Smart Glasses Using Convolutional Neural Network,IEEE,Conferences,"At present, orthopedic surgery has a large variety of surgical techniques. Procedures are complicated, and many types of equipment have been using in the surgery. So, operating room nurses who deliver surgical instruments to surgeon are supposed to be forced to incur a heavy burden. Although there is a navigation system for assisting surgeons in artificial joint replacement surgery, but no system exists for assisting operating room nurses. This work proposes a computer-aided navigation system that indicates the current procedure and procedure progress for nurses, and also instructs nurses to prepare surgical instruments to be used in the next procedure using smart glasses. Firstly, the system estimates the current status of the surgery procedure using a convolutional neural network (CNN) by utilizing real-time video images taken from smart glasses which was worn by operating surgeon. Then, the system indicates nurses the surgical instrument to be used at the next procedure in the smart glass worn by the nurses. The system was implemented with the object detection technology and the augmented reality. Experiment results demonstrated a satisfactory performance of our proposed system of recognizing surgery procedures.",https://ieeexplore.ieee.org/document/8616069/,"2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",7-10 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICAECT49130.2021.9392506,"Real-Time Parking Lot Occupancy Detection System with VGG16 Deep Neural Network using Decentralized Processing for Public, Private Parking Facilities",IEEE,Conferences,"Efforts have been taken to develop an efficient image-based parking occupancy detection system employing the camera systems (CCTV) pre installed for security purposes. The present detection system has experienced the problem of identifying the subject with high accuracy and applicability in real time environments. A transfer learning methodology where CNN's feature extraction was used with binary classifier has proved to provide optimum accuracy. This paper proposes a decentralized system to detect the occupancy status of a parking slot from the CCTV video footage using VGG16 CNN architecture for feature extraction followed by binary classification to detect vacant or occupied spaces. To deliver a complete parking solution, the model has been tested on Live CCTV footages collected from existing parking slots and have provided a complete integrated system that allows real time availability check and booking of parking slots. This paper provides a detailed explanation of the execution of the above system. The performance efficiency and detection are improved by testing the designed model on two datasets Pklot and CNRPark, CNRParkEXT comprising daylight images with multiple weather conditions. The proposed technique is compared with the current state of the art implementations [1],[2]. The dataset has been developed to test the model under low intensity light conditions. With significant accuracy and high-performance throughput observed, the proposed system can be effectively implemented at the commercial level.",https://ieeexplore.ieee.org/document/9392506/,"2021 International Conference on Advances in Electrical, Computing, Communication and Sustainable Technologies (ICAECT)",19-20 Feb. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI50040.2020.00139,Real-Time Polyp Detection for Colonoscopy Video on CPU,IEEE,Conferences,"Colorectal cancer(CRC), which is derived from polyps, is a common malignant tumor with a high incidence and mortality. Currently the colonoscopy is the most effective approach for early detection of the colorectal cancer. With the development of object detection, some polyp detection algorithms have achieved satisfied results. However, most of them need the support of hardware accelerators such as GPU, which is unsuitable for actual clinical environment due to its high power consumption and loud noise. To solve the problem, this paper proposes a solution to implement real-time polyp detection for colonoscopy video on general-purpose computing platforms, i.e. CPU. Our solution incorporates several methods to improve efficiency of polyp detection. Firstly, by analyzing the similarity and difference between successive frames in colonoscopy videos, we use the abrupt shot detection method to reduce the unnecessary detection for frames that contain non-polyps; secondly, for the frames that need to be detected, we iterated a procedure of sparse training, pruning and knowledge distillation to search for compressed models. Our approach is evaluated with both public datasets and actual colonoscopy videos, and results show that our approach can process 35.28 frames per second on colonoscopy videos with satisfied precision on CPU.",https://ieeexplore.ieee.org/document/9288341/,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCS.2017.167,Real-Time Power Cycling in Video on Demand Data Centres Using Online Bayesian Prediction,IEEE,Conferences,"Energy usage in data centres continues to be a major and growing concern as an increasing number of everyday services depend on these facilities. Research in this area has examined topics including power smoothing using batteries and deep learning to control cooling systems, in addition to optimisation techniques for the software running inside data centres. We present a novel real-time power-cycling architecture, supported by a media distribution approach and online prediction model, to automatically determine when servers are needed based on demand. We demonstrate with experimental evaluation that this approach can save up to 31% of server energy in a cluster. Our evaluation is conducted on typical rack mount servers in a data centre testbed and uses a recent real-world workload trace from the BBC iPlayer, an extremely popular video on demand service in the UK.",https://ieeexplore.ieee.org/document/7980159/,2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS),5-8 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONFLUENCE.2019.8776934,Real-Time Smart Attendance System using Face Recognition Techniques,IEEE,Conferences,"The management of the attendance can be a great burden on the teachers if it is done by hand. To resolve this problem, smart and auto attendance management system is being utilized. But authentication is an important issue in this system. The smart attendance system is generally executed with the help of biometrics. Face recognition is one of the biometric methods to improve this system. Being a prime feature of biometric verification, facial recognition is being used enormously in several such applications, like video monitoring and CCTV footage system, an interaction between computer & humans and access systems present indoors and network security. By utilizing this framework, the problem of proxies and students being marked present even though they are not physically present can easily be solved. The main implementation steps used in this type of system are face detection and recognizing the detected face.This paper proposes a model for implementing an automated attendance management system for students of a class by making use of face recognition technique, by using Eigenface values, Principle Component Analysis (PCA) and Convolutional Neural Network (CNN). After these, the connection of recognized faces ought to be conceivable by comparing with the database containing student's faces. This model will be a successful technique to manage the attendance and records of students.",https://ieeexplore.ieee.org/document/8776934/,"2019 9th International Conference on Cloud Computing, Data Science & Engineering (Confluence)",10-11 Jan. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SoutheastCon42311.2019.9020591,Real-Time Traffic Incidence dataset,IEEE,Conferences,"This paper focuses on developing (i) a benchmark dataset for identification of traffic incidences, (ii) a congestion aware navigation application which uses this dataset for real-time detection and classification of traffic incidents, (iii) the System Level Software (or Middleware) required for Distributed Computing in such a system with Rapid Mobility, and (iv) a hardware prototype of the distributed computing and storage infrastructure. The video bandwidth requirement of 10-100 GigaBytes of data per minute per vehicular camera makes it a Big Data problem. With millions of smart vehicles predicted to be deployed within the next 5 years, BigData from a single vehicle, multiplied with the large number of vehicles, presents a Big-Squared-Data computing space which will easily overwhelm any Cloud infrastructure with its Real-Time or near Real-Time demands. Hence the need for a Fog tier between the Edge nodes and the Cloud to bring distributed computation (servers) and storage closer to the Edge nodes. Such a Fog consists of multiple Fog instances, each one of which services cells or Virtual Clusters of Edge nodes. Results show that Fog-Cloud computing framework outperforms a Cloud-only platform by 55.8% reduction in total latency or response time.",https://ieeexplore.ieee.org/document/9020591/,2019 SoutheastCon,11-14 April 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EDGE.2018.00028,Real-Time Traffic Pattern Collection and Analysis Model for Intelligent Traffic Intersection,IEEE,Conferences,"The traffic congestion hits most big cities in the world - threatening long delays and serious reductions in air quality. City and local government officials continue to face challenges in optimizing crowd flow, synchronizing traffic and mitigating threats or dangerous situations. One of the major challenges faced by city planners and traffic engineers is developing a robust traffic controller that eliminates traffic congestion and imbalanced traffic flow at intersections. Ensuring that traffic moves smoothly and minimizing the waiting time in intersections requires automated vehicle detection techniques for controlling the traffic light automatically, which are still challenging problems. In this paper, we propose an intelligent traffic pattern collection and analysis model, named TPCAM, based on traffic cameras to help in smooth vehicular movement on junctions and set to reduce the traffic congestion. Our traffic detection and pattern analysis model aims at detecting and calculating the traffic flux of vehicles and pedestrians at intersections in real-time. Our system can utilize one camera to capture all the traffic flows in one intersection instead of multiple cameras, which will reduce the infrastructure requirement and potential for easy deployment. We propose a new deep learning model based on YOLOv2 and adapt the model for the traffic detection scenarios. To reduce the network burdens and eliminate the deployment of network backbone at the intersections, we propose to process the traffic video data at the network edge without transmitting the big data back to the cloud. To improve the processing frame rate at the edge, we further propose deep object tracking algorithm leveraging adaptive multi-modal models and make it robust to object occlusions and varying lighting conditions. Based on the deep learning based detection and tracking, we can achieve pseudo-30FPS via adaptive key frame selection.",https://ieeexplore.ieee.org/document/8473390/,2018 IEEE International Conference on Edge Computing (EDGE),2-7 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AVSS.2019.8909855,Real-Time Video-Based Person Re-Identification Surveillance with Light-Weight Deep Convolutional Networks,IEEE,Conferences,"Today's person re-ID system mostly focuses on accuracy and ignores efficiency. But in most real-world surveillance systems, efficiency is often considered the most important focus of research and development. Therefore, for a person re-ID system, the ability to perform real-time identification is the most important consideration. In this study, we implemented a real-time multiple camera video-based person re-ID system using the NVIDIA Jetson TX2 platform. This system can be used in a field that requires high privacy and immediate monitoring. This system uses YOLOv3-tiny based light-weight strategies and person re-ID technology, thus reducing 46% of computation, cutting down 39.9% of model size, and accelerating 21% of computing speed. The system also effectively upgrades the pedestrian detection accuracy. In addition, the proposed person re-ID example mining and training method improves the model's performance and enhances the robustness of cross-domain data. Our system also supports the pipeline formed by connecting multiple edge computing devices in series. The system can operate at a speed up to 18 fps at 1920×1080 surveillance video stream. The demo of our developed systems can be found at https://sites.google.com/g.ncu.edu.tw/video-based-person-re-id/.",https://ieeexplore.ieee.org/document/8909855/,2019 16th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),18-21 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2018.8500455,"Real-time Detection, Tracking, and Classification of Moving and Stationary Objects using Multiple Fisheye Images",IEEE,Conferences,"The ability to detect pedestrians and other moving objects is crucial for an autonomous vehicle. This must be done in real-time with minimum system overhead. This paper discusses the implementationof a surround view system to identify moving as well as static objects that are close to the ego vehicle. The algorithm works on 4 views captured by fisheye cameras which are merged into a single frame. The moving object detection and tracking solution uses minimal system overhead to isolate regions of interest (ROIs) containing moving objects. These ROIs are then analyzed using a deep neural network (DNN) to categorize the moving object. With deployment and testing on a real car in urban environments, we have demonstrated the practical feasibility of the solution.11The video demos of our algorithm have been uploaded to Youtube: https://youtu.be/vpoCfC724iA, https://youtu.be/2X4aqH2bMBs",https://ieeexplore.ieee.org/document/8500455/,2018 IEEE Intelligent Vehicles Symposium (IV),26-30 June 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9283312,Real-time Golf Ball Detection and Tracking Based on Convolutional Neural Networks,IEEE,Conferences,"This paper focuses on the problem of real-time detection and tracking of a golf ball from video sequences. We propose an efficient and effective solution by integrating object detection and a discrete Kalman model. For ball detection, three classical convolutional neural network based detection models are implemented, including Faster R-CNN, YOLOv3, and YOLOv3 tiny. At the tracking stage, a discrete Kalman filter is employed to predict the location of the golf ball based on the previous observations. To increase the detection accuracy and speed, we propose to use image patches rather than the entire images for detection. In order to train the detection models and test the tracking algorithm, we collect and annotate a collection of golf ball dataset. Extensive experimental results are performed to demonstrate the effectiveness and superior performance of the proposed approach.",https://ieeexplore.ieee.org/document/9283312/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.2014.6901124,Real-time HDR video imaging on FPGA with compressed comparametric lookup tables,IEEE,Conferences,"Video cameras can only take photographs with limited dynamic range. One method to overcome this is to combine differently exposed images of the same subject matter (i.e. a Wyckoff Set), producing a High Dynamic Range (HDR) result. Implementations for real-time HDR videos have relied upon methods that are less accurate. Instead of weighted-sum approaches that adds up noise for large number of LDR images, lookup tables that contains results calculated using a probabilistic model can be generated. This lookup table, when compressed using quadtree structure, can be implemented on a mediumsized FPGA. The work presented in this paper improves over the earlier publication to support high-definition videos. To achieve the required bandwidth for HDR composition, the evaluation of the camera response function is performed within a compressed lookup table that has its address retrieved using a pipelined multiplexer network. We are able to generate the entire circuit using software. It is parameterizable by user-specified error constraints, allowing us to explore the trade-offs in resource usage and precision of the implementation.",https://ieeexplore.ieee.org/document/6901124/,2014 IEEE 27th Canadian Conference on Electrical and Computer Engineering (CCECE),4-7 May 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECO55406.2022.9797113,Real-time HOG+SVM based object detection using SoC FPGA for a UHD video stream,IEEE,Conferences,"Object detection is an essential component of many vision systems. For example, pedestrian detection is used in advanced driver assistance systems (ADAS) and advanced video surveillance systems (AVSS). Currently, most detectors (e.g. the YOLO - You Only Look Once - family) use deep convolutional neural networks. However, due to their high computational complexity, they are not able to process a very high-resolution video stream in real-time, especially within a limited energy budget. In this paper we present a hardware implementation of the well-known pedestrian detector with HOG (Histogram of Oriented Gradients) feature extraction and SVM (Support Vector Machine) classification. Our system running on <tex>$\text{AMD} \text{Xilinx\ Zynq\ UltraScale}+ \text{MPSoC}$</tex> (Multiprocessor System on Chip) device allows real-time processing of 4K resolution (UHD - Ultra High Definition, <tex>$3840 \times 2160\ \text{pixels}$</tex>) video for 60 frames per second. The system is capable of detecting a pedestrian in a single scale. The results obtained confirm the high suitability of reprogrammable devices in the real-time implementation of embedded vision systems.",https://ieeexplore.ieee.org/document/9797113/,2022 11th Mediterranean Conference on Embedded Computing (MECO),7-10 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICONIC.2018.8601211,Real-time Multiple Vehicle Detection using a Rear Camera Mounted on a Vehicle,IEEE,Conferences,"An increase in demand for traffic monitoring of densely populated areas has been the subject of many discussions concerning the security on the road. Cameras and other sensors are used to monitor, identify and manage potential incidents that can occur. Therefore, there is a need for computer vision system and algorithms to detect and track vehicles, which will facilitate the management of traffic and driving assistance. In this paper, a vehicle detection and tracking system is proposed using an efficient background cancelation technique and Haar-like features with a modified Adaboost algorithm in a cascade configuration for maximum accuracy and robustness. The proposed system is implemented on a passenger car with a camera mounted at the rear to detect vehicles behind it. Video data are collected and processed in real-time and the performance of the system is verified experimentally.",https://ieeexplore.ieee.org/document/8601211/,2018 International Conference on Intelligent and Innovative Computing Applications (ICONIC),6-7 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SMC42975.2020.9283130,Real-time Object Tracking Based on Improved Adversarial Learning,IEEE,Conferences,"With the development of deep learning and the emergence of massive video data, object tracking has great application prospects in many fields. However, most tracking algorithms can hardly get top performance with real-time speed. In this paper, we improved tracking model based on adversarial learning and to accelerate feature extraction we proposed an efficient and accurate method. We also present a Precise ROI Pooling (PrROIPooling) based algorithm for extracting more accurate representations of targets. Furthermore, a novel regularization term is defined to ensure the similarity between the generated features and the real features. Finally, the improved objective function with modulating factors is designed to handle the problem of imbalance in the number of positive and negative samples. Extensive experiments on three datasets have demonstrated our effectiveness and achieved competitive results compared with state-of-the-art methods.",https://ieeexplore.ieee.org/document/9283130/,"2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",11-14 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEIEC.2019.8784510,Real-time Traffic Congestion Detection with SIGHTA Regression Network,IEEE,Conferences,"This work proposes a novel SIGHTA regression network to detect traffic congestion in real-time via video stream. The network inherits the architecture of YOLOv3 [1] which uses Darknet53 backbone. Referring to SPP network [2], the network structure has been adjusted so that more objects can be detected. Additionally, a novel regression algorithm and loss function are presented to regress quadrilateral objects. Experimental results demonstrate that the SIGHTA network significantly outperforms the existing object detection methods by mAP. Meanwhile the speed is faster.",https://ieeexplore.ieee.org/document/8784510/,2019 IEEE 9th International Conference on Electronics Information and Emergency Communication (ICEIEC),12-14 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2018.8486321,Real-time Video Quality of Experience Monitoring for HTTPS and QUIC,IEEE,Conferences,"The widespread deployment of end-to-end encryption protocols such as HTTPS and QUIC has reduced the visibility for operators into traffic on their networks. Network operators need the visibility to monitor and mitigate Quality of Experience (QoE) impairments in popular applications such as video streaming. To address this problem, we propose a machine learning based approach to monitor QoE metrics for encrypted video traffic. We leverage network and transport layer information as features to train machine learning classifiers for inferring video QoE metrics such as startup delay and rebuffering events. Using our proposed approach, network operators can detect and react to encrypted video QoE impairments in real-time. We evaluate our approach for YouTube adaptive video streams using HTTPS and QUIC. The experimental evaluations show that our approach achieves up to 90% classification accuracy for HTTPS and up to 85 % classification accuracy for QUIC.",https://ieeexplore.ieee.org/document/8486321/,IEEE INFOCOM 2018 - IEEE Conference on Computer Communications,16-19 April 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNP52444.2021.9651927,Real-time Video Transmission Optimization Based on Edge Computing in IIoT,IEEE,Conferences,"In the Industrial Internet of Things (IIoT) scenario, the increase of surveillance equipment brings challenges to the transmission of real-time video. It needs more efficient approaches to finish video transmission with more stability and accuracy. Therefore, we propose a self-adaptive transmission scheme of videos for multi-capture terminals under IIoT in this paper. To fit for the constant variation of network environment, we compress the videos that wait for transmitting from multi-capture terminals by reducing the non-key frames with Graph Convolutional Network (GCN). Moreover, a self-adaptive strategy of transmission is implemented on the Mobile Edge Computing (MEC) server to adjust the transmission volume of processed videos, and a multi-objective optimization algorithm is utilized to optimize the strategy of transmission during the video transmission. The relative experiments are conducted to validate the performance of the proposed scheme.",https://ieeexplore.ieee.org/document/9651927/,2021 IEEE 29th International Conference on Network Protocols (ICNP),1-5 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CONECCT52877.2021.9622739,Real-time Violence Activity Detection Using Deep Neural Networks in a CCTV camera,IEEE,Conferences,"In surveillance systems one of the most difficult and critical tasks is detecting violence, most video surveillance systems are faced with the challenges of false alarms and working in a real-time environment. This paper introduces a lightweight system that can be employed in real-time systems. The system proposed uses OpenPose for multi-person 2D pose estimation, YoloV3 for person detection, and a CNN to classify the violent action. OpenPose unlike other systems uses a bottom-up approach which decouples the running time from the number of people in the frame, YoloV3 deployed for person detection has a very fast processing time and the CNN used was trained on the skeleton dataset that was generated and had very good accuracy. We also propose an skeleton image dataset of three action categories, namely kicking, punching, and non-violent.",https://ieeexplore.ieee.org/document/9622739/,"2021 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",9-11 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APSIPAASC47483.2019.9023094,Real-time and interactive tools for vocal training based on an analytic signal with a cosine series envelope,IEEE,Conferences,"We introduce real-time and interactive tools for assisting vocal training. In this presentation, we demonstrate mainly a tool based on real-time visualizer of fundamental frequency candidates to provide information-rich feedback to learners. The visualizer uses an efficient algorithm using analytic signals for deriving phase-based attributes. We start using these tools in vocal training for assisting learners to acquire the awareness of appropriate vocalization. The first author made the MATLAB implementation of the tools open-source. The code and associated video materials are accessible in the first author's GitHub repository.",https://ieeexplore.ieee.org/document/9023094/,2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC),18-21 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCICA.2000.863254,Real-time bilateral control of Internet-based teleoperation,IEEE,Conferences,"The growth of the Internet has been accompanied by an increase in its applications. One of the most interesting of these is teleoperation, where the Internet is used as a bridge between operators and machines. However, teleoperation over the Internet comes with several problems: delay, lost packets and disconnection. All of these limitations may cause instability in teleoperation systems, especially for those systems include haptic feedback. Most of the previous work in Internet based teleoperation rests on many limiting assumptions, for example, time delay is constant or has an upper bound, control is not in real-time. This paper presents a new real time haptic feedback system that deals with these limitations and difficulties without making any assumptions regarding the time delay. The approach is based on the event based control, which has been implemented or a mobile robot over the Internet. The haptic information include real-time feedback of force and video.",https://ieeexplore.ieee.org/document/863254/,Proceedings of the 3rd World Congress on Intelligent Control and Automation (Cat. No.00EX393),26 June-2 July 2000,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSE.2017.8085486,Real-time detection algorithm of abnormal behavior in crowds based on Gaussian mixture model,IEEE,Conferences,"Recently, abnormal evens detection in crowds has received considerable attention in the field of public safety. Most existing studies do not account for the processing time and the continuity of abnormal behavior characteristics. In this paper, we present a new motion feature descriptor, called the sensitive movement point (SMP). Gaussian Mixture Model (GMM) is used for modeling the abnormal crowd behavior with full consideration of the characteristics of crowd abnormal behavior. First, we analyze the video with GMM, to extract sensitive movement point in certain speed by setting update threshold value of GMM. Then, analyze the sensitive movement point of video frame with temporal and spatial modeling. Identify abnormal behavior through the analysis of mutation duration occurs in temporal and spatial model, and the density, distribution and mutative acceleration of sensitive movement point in blocks. The algorithm can be implemented with automatic adapt to environmental change and online learning, without tracking individuals of crowd and large scale training in detection process. Experiments involving the UMN datasets and the videos taken by us show that the proposed algorithm can real-time effectively identify various types of anomalies and that the recognition results and processing time are better than existing algorithms.",https://ieeexplore.ieee.org/document/8085486/,2017 12th International Conference on Computer Science and Education (ICCSE),22-25 Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSESS.2014.6933582,Real-time discrimination of frontal face using integral channel features and Adaboost,IEEE,Conferences,"In this paper we present a novel approach for discrimination of frontal face in video, using integral channel features(ICF) and Adaboost. We have two stages for this approach based on classification, the first stage is training process, we utilize ICF exacted from training database to train strong classifier, which is implemented by Adaboost. The second stage is discriminating process by scoring, we compute ICF of the face detection window, and then scoring the window using the trained classifier, and at last the most frontal face will be chosen by the highest scores. Furthermore, we then apply the approach to the ChokePoint database and compare with different approaches, showing a good performance.",https://ieeexplore.ieee.org/document/6933582/,2014 IEEE 5th International Conference on Software Engineering and Service Science,27-29 June 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.1991.155250,Real-time early vision neurocomputing,IEEE,Conferences,"Summarizes recent work on real-time implementations of early vision neural computations on the PIPE video-rate parallel computer. PIPE is an eight-stage machine for real-time image processing, performing nearly one billion 8-bit operations per second. Temporal computations are implemented using the integral solutions of the differential equations that govern the neurodynamics via an exponentially fading memory. Spatial computations are performed by convolving the images with masks. Dynamic neural networks that perform light adaptation, spatial contrast enhancement, and image feature velocity extraction have been implemented to process live imagery (256*256 pixels, 8-bit data) at 30 frames/sec. These computations simulate spatiotemporal processing in the retina and the motion pathway of the brain. Some examples of these computations are described.<>",https://ieeexplore.ieee.org/document/155250/,IJCNN-91-Seattle International Joint Conference on Neural Networks,8-12 July 1991,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INTECH.2017.8102423,Real-time emotional state detection from facial expression on embedded devices,IEEE,Conferences,"From the last decade, researches on human facial emotion recognition disclosed that computing models built on regression modelling can produce applicable performance. However, many systems need extensive computing power to be run that prevents its wide applications such as robots and smart devices. In this proposed system, a real-time automatic facial expression system was designed, implemented and tested on an embedded device such as FPGA that can be a first step for a specific facial expression recognition chip for a social robot. The system was built and simulated in MATLAB and then was built on FPGA and it can carry out real time continuously emotional state recognition at 30 fps with 47.44% accuracy. The proposed graphic user interface is able to display the participant video and two dimensional predict labels of the emotion in real time together.",https://ieeexplore.ieee.org/document/8102423/,2017 Seventh International Conference on Innovative Computing Technology (INTECH),16-18 Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2008.4712084,Real-time face alignment with tracking in video,IEEE,Conferences,"Real-time face alignment in video is very critical in many applications such as facial expression analysis, driver fatigue monitoring, etc. This paper presents a real time algorithm for face alignment in video that combines Active Shape Model (ASM) based face alignment and spatial-temporal continuity based tracking strategy. To guarantee the correctness of the tracked shape in each frame, a verification procedure is introduced so that when inter-frame shape tracking failed the intra-frame ASM algorithm can be restored to initialize a new shape for tracking. Experiments show that the implemented system can run totally automatic with a quite good accuracy that may have many practical applications.",https://ieeexplore.ieee.org/document/4712084/,2008 15th IEEE International Conference on Image Processing,12-15 Oct. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2017.7995825,Real-time hand posture and gesture-based touchless automotive user interface using deep learning,IEEE,Conferences,"In this study, a vision based in-car entertainment user interface is presented. The user interface is designed using a hand posture and gesture recognition algorithm in deep learning framework. The hand posture recognition algorithm is formulated using the convolutional neural network to perform the fundamental tasks in the user interface. The hand gesture recognition algorithm is formulated using the long-term recurrent convolutional neural network to intuitively interact with the touchless automotive user interface in a detailed manner. In the recurrent deep learning framework, typically, the gesture frames are taken from a uniformly sampled image sequence. In this work, the recurrent structure is enhanced using a reduced number of input frames captured from the image sequence. The reduced input frames or key frames represent the action present in the video sequence. Sparse dictionary learning provide reliable key frame extraction from video sequences. However, sparse dictionary learning is computationally expensive, and are individually optimized for every video sequence. In this paper, we propose to approximate sparse dictionary learning using a non-linear regression framework. The multilayer perceptron is utilized to model the non-linear regression framework. The optimal neural network architecture is identified after a detailed evaluation. We evaluate the proposed recognition methods on public datasets. The proposed methods yield a recognition accuracy of 92% and 90% for pose and gestures, respectively. The combined hand posture and gesture recognition takes 82ms which is a reasonable for real time implementation.",https://ieeexplore.ieee.org/document/7995825/,2017 IEEE Intelligent Vehicles Symposium (IV),11-14 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNN.1995.487705,Real-time head detection,IEEE,Conferences,"This paper describes a syntactic approach to the classification of moving objects in front of a video camera. The implementation of syntactic pattern recognition techniques is examined for a real-time system, focusing on the detection of human heads in front of a video camera. In particular, the author describes the technique used to classify objects between heads and non-heads, the results achieved, and the implications of the real-time property of this system. In a syntactic approach the higher level information of an image is extracted from the lower level structure of that image. The techniques used by this application to extract the higher level of information from the lower level structure of the image is discussed in this paper. This paper also examines how a quadratic neural network may be used as both a classifier, and as a tool for modelling data. In this system quadratic neural networks are used both to model the lower level structure of the images from the video camera, and as a tool for classifying the objects in front of the video camera.",https://ieeexplore.ieee.org/document/487705/,Proceedings of ICNN'95 - International Conference on Neural Networks,27 Nov.-1 Dec. 1995,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2002.1007557,Real-time object classification and novelty detection for collaborative video surveillance,IEEE,Conferences,"To conduct real-time video surveillance using low-cost commercial off-the-shelf hardware, system designers typically define the classifiers prior to the deployment of the system so that the performance of the system can be optimized for a particular mission. This implies the system is restricted to interpreting activity in the environment in terms of the original context specified. Ideally the system should allow the user to provide additional context in an incremental fashion as conditions change. Given the volumes of data produced by the system, it is impractical for the user to periodically review and label a significant fraction of the available data. We explore a strategy for designing a real-time object classification process that aids the user in identifying novel, informative examples for efficient incremental learning.",https://ieeexplore.ieee.org/document/1007557/,Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290),12-17 May 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MMVIP.2007.4430731,Real-time tracking of a moving contacting load using the distributive tactile sensing method.,IEEE,Conferences,"Motion measurement is a key function for patient diagnosis, therapy and rehabilitation in clinical disciplines ranging physiology, audiology, orthopaedics, neurophysiology. The equipment can be by video camera recording, machine vision systems and force plates. Motion analysis systems supported by multi-camera machine vision systems are an expensive solution for many applications. Force plates are another measurement tool that can be used in conjunction with a machine vision system or as a separate system. In the case of the latter set-up, the force plate is able to measure the centroid and vector of the ground reaction force, and the data retrieved can be interpreted by clinical staff to determine the nature of balance or stance of a patient. There is valuable information in the transients that can be detected by this approach, however the force plate cannot be utilised to discriminate other spatial factors in the way that the loads are applied by the patient on the surface. This paper reports the distributive approach to tactile sensing applied to infer the 3 dimensional motion of a moving mass in a supporting mechanism placed on the 2 dimensional sensing surface. The distributive approach has the advantage over forceplates on constructional costs and the ability to discriminate many motion metrics of patients. Implementation of the system using only three low cost deflection sensing elements, positioned under the surface with the resulting signals interpreted by neural network implemented on a field programmable gate array (FPGA) output near real time sampling rates greater than 70 KHz. The investigation demonstrates that the performance is sufficiently accurate for the intended clinical application, having backing errors of less than 5% in all three dimensions.",https://ieeexplore.ieee.org/document/4430731/,2007 14th International Conference on Mechatronics and Machine Vision in Practice,4-6 Dec. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2015.7280718,Real-time video object recognition using convolutional neural network,IEEE,Conferences,"A convolutional neural network (CNN) is implemented on a field-programmable gate array (FPGA) and used for recognizing objects in real-time video streams. In this system, an image pyramid is constructed by successively down-scaling the input video stream. Image blocks are extracted from the image pyramid and classified by the CNN core. The detected parts are then marked on the output video frames. The CNN core is composed of six hardware neurons and two receptor units. The hardware neurons are designed as fully-pipelined digital circuits synchronized with the system clock, and are used to compute the model neurons in a time-sharing manner. The receptor units scan the input image for local receptive fields and continuously supply data to the hardware neurons as inputs. The CNN core module is controlled according to the contents of a table describing the sequence of computational stages and containing the system parameters required to control each stage. The use of this table makes the hardware system more flexible, and various CNN configurations can be accommodated without re-designing the system. The system implemented on a mid-range FPGA achieves a computational speed greater than 170,000 classifications per second, and performs scale-invariant object recognition from a 720×480 video stream at a speed of 60 fps. This work is a part of a commercial project, and the system is targeted for recognizing any pre-trained objects with a small physical volume and low power consumption.",https://ieeexplore.ieee.org/document/7280718/,2015 International Joint Conference on Neural Networks (IJCNN),12-17 July 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCS49078.2020.9118581,Realization of Intelligent Invigilation System Based on Adaptive Threshold,IEEE,Conferences,"In order to reduce the labor cost of invigilation, improve invigilation efficiency and deal with violations in real time, this paper designs and implements an intelligent invigilation system from two aspects of hardware and software. The system on the basis of the standardized test in video monitoring system, aiming at solving the problem that the traditional EM algorithm is sensitive to initial value, this paper puts forward an improved method to make supervised learning image and human body's contour recognition, to extract and analyse the scene of the abnormal information feature, and use adaptive threshold algorithm to improve the accuracy of the automatic alarm. these technology make the monitor platform be able to find abnormal information, and timely feed them back to the On-site invigilators. Finally, it can realize intelligent invigilation, improve the precision of supervision system, and has promotion value.",https://ieeexplore.ieee.org/document/9118581/,2020 5th International Conference on Computer and Communication Systems (ICCCS),15-18 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECCTD.2013.6662238,Realization of preprocessing blocks of CNN based CASA system on FPGA,IEEE,Conferences,"In this paper, hardware optimization of the preprocessing part of a computer aided semen analysis (CASA) system is proposed, which is also implemented on an FPGA device as a working prototype. A real-time cellular neural network (CNN) emulator (RTCNNP-v2) is used for the realization of the image processing algorithms, whose regular, flexible and reconfigurable infrastructure simplifies the prototyping process. For future work, the post-processing part of the CASA system is proposed to be implemented on the same FPGA device as software, using either a soft or hard processor core. By the integration of the pre- and post-processing parts, the designed CASA system will be capable of processing full-HD 1080p@60 (1080×1920) video images in real-time.",https://ieeexplore.ieee.org/document/6662238/,2013 European Conference on Circuit Theory and Design (ECCTD),8-12 Sept. 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAS.2014.6865576,Realization of processing blocks of CNN based CASA system on CPU and FPGA,IEEE,Conferences,"In this paper, hardware optimization of the preprocessing and software implementation of the processing blocks of a computer-aided semen analysis (CASA) system are proposed, which is also implemented on an FPGA and ARM device as a working prototype. The software implementation of the track initialization, track maintenance, data validation and classification blocks of the processing part are implemented on a Zynq7000 ARM Cortex-A9 processor. In the preprocessing part, a real-time cellular neural network (CNN) emulator (RTCNNP-v2) is used for the realization of the image processing algorithms, whose regular, flexible and reconfigurable infrastructure simplifies the prototyping process. The CASA system introduced in this paper is capable of processing full-HD 1080p@60 (1080 × 1920) video images in real-time.",https://ieeexplore.ieee.org/document/6865576/,2014 IEEE International Symposium on Circuits and Systems (ISCAS),1-5 June 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCECE.2012.6335012,"Realtime HDR (High Dynamic Range) video for eyetap wearable computers, FPGA-based seeing aids, and glasseyes (EyeTaps)",IEEE,Conferences,"Realtime video HDR (High Dynamic Range) is presented in the context of a seeing aid designed originally for task-specific use (e.g. electric arc welding). It can also be built into regular eyeglasses to help people see better in everyday life. Our prototype consists of an EyeTap (electric glasses) welding helmet, with a wearable computer upon which are implemented a set of image processing algorithms that implement realtime HDR (High Dynamic Range) image processing together with applications such as mediated reality, augmediatedTM, and augmented reality. The HDR video system runs in realtime and processes 120 frames per second, in groups of three frames or four frames (e.g. a set of four differently exposed images captured every thirtieth of a second). The processing method, for implementation on FPGAs (Field Programmable Gate Arrays), achieves a realtime performance for creating HDR video using our novel compositing methods, and runs on a miniature self-contained battery-operated head-worn circuit board, without the need for a host computer. The result is an essentially self-contained miniaturizable hardware HDR camera system that could be built into smaller eyeglass frames, for use in various wearable computing and mediated/ aug-mediated reality applications, as well as to help people see better in their everyday lives.",https://ieeexplore.ieee.org/document/6335012/,2012 25th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE),29 April-2 May 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBMI.2019.8877421,Recognising Irish Sign Language Using Electromyography,IEEE,Conferences,"Sign language is the non-verbal communication used by people with hearing and speaking impairments. The automatic recognition of sign languages is usually based on video analysis of the signer though this is difficult when considering different light levels or the surrounding environment. The work in this paper uses electromyography (EMG) and focuses on letters of the Irish Sign Language (ISL) alphabet. EMG is the recording of the electrical activity produced to stimulate movement in the skeletal muscles. We capture muscle signals and inertial movement data using the Thalmic MYO armband and, in real time, recognise the ISL alphabet. Our implementation is based on signal processing, feature extraction and machine learning. The only input required to translate the ISL gestures are EMG and movement data, thus our approach is usable in scenarios where using video for automatic recognition video is not possible.",https://ieeexplore.ieee.org/document/8877421/,2019 International Conference on Content-Based Multimedia Indexing (CBMI),4-6 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICAC50006.2021.9594189,Recognition of Abnormal Human Behavior in Elevators based on CNN,IEEE,Conferences,"This study explores a CNN based model to identify abnormal behavior in elevator cabs, which I have named S-LRCN ( S-Long-term Recurrent Convolutional Network). It starts with the detection of key points of the human skeleton by using the Openpose method, then further detects and tracks the human body through the CenterNet and DeepSort methods, and finally integrates the Long Short Term Memory Network (LSTM) and Convolutional Neural Network (CNN) to form a deep learning model. In this study, a large dataset (500 video clips) collected from real elevator cabs with different backgrounds has been applied to ensure the robustness and generalizability of the proposed model. At last, this study applies the two mainstream dangerous human behaviors, i.e., door blocking and door picking as case studies to test and evaluate the usability and availability. Experimental results show that the model has a 85% recognition rate of abnormal behavior.",https://ieeexplore.ieee.org/document/9594189/,2021 26th International Conference on Automation and Computing (ICAC),2-4 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ZINC52049.2021.9499300,Recognition of maximal speed limit traffic signs for use in advanced ADAS algorithms,IEEE,Conferences,"Advanced Driver Assistance Systems (ADAS) have been increasingly developing, specifically in the last decade. One of such ADAS is that intended for traffic signs recognition. This paper deals with the recognition of a specific subset of traffic signs, i.e. speed limit traffic signs. The complete solution is based on the usage of machine learning and finally implemented in the C programming language. After the optimization process, the final solution is implemented on the real ADAS board, to check its performance in a real operational environment. Due to the limited resources of the ADAS board itself, a simple Convolutional Neural Network (CNN) was created to recognize speed limit traffic signs. For CNN training a large database of 6891 training images is used. When testing the solution, 731 test images from the real traffic are used, as well as 123 real video sequences. The test results show that in certain situations the proposed solution is capable of achieving high performance in terms of precision, while in some cases additional improvements of the solution should be investigated. It is capable of processing 12 frames per second when operating with state-of-the-art automotive camera resolution, i.e. 1280x720 pixels.",https://ieeexplore.ieee.org/document/9499300/,2021 Zooming Innovation in Consumer Technologies Conference (ZINC),26-27 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSMC.2012.6378255,Recognizing temporary changes on highways for reliable autonomous driving,IEEE,Conferences,"In order to be deployed in real-world driving environments, autonomous vehicles must be able to recognize and respond to exceptional road conditions, such as highway workzones, because such unusual events can alter previously known traffic rules and road geometry. In this paper, we present a set of computer vision methods which recognize the bounds of a highway workzone and temporary changes in highway driving environments through recognition of workzone signs. Our approach filters out irrelevant image regions, localizes potential sign image regions using a learned color model, and recognizes signs through classification. Performance of individual unit tests is promising; still, it is unrealistic to expect perfect performance in sign recognition. Performance errors with individual modules in sign recognition will cause our system to misread temporary highway changes. To handle potential recognition errors, our method utilizes the temporal redundancy of sign occurrences and their corresponding classification decisions. Through testing, using video data recorded under various weather conditions, our approach was able to perfectly identify the boundaries of workzones and robustly detect a majority of driving condition changes.",https://ieeexplore.ieee.org/document/6378255/,"2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)",14-17 Oct. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2014.7090222,Reconfigurable robotic system based on mono-camera guidance,IEEE,Conferences,"The paper proposes an intelligent robotic system which is able to be (re)configured, at demand, for two deployment scenarios. a) The first task is to move the platform after a trajectory determined by the direction to a fixed point and avoid any obstacles occurring in the route. a) The second task is to identify and track a spherical object. The robot is equipped with a navigation system designed to maintain direction in case of interruption of video contact with the target (landmark), meaning when an obstacle interposes. It has two main subsystems: the mobile platform, which is equipped with a video camera and sensors for path correction, and the central processing system for the analysis of received information. The task control is based on extracted features from images. The communication between them is done via a wireless protocol. Algorithms for controlling the mobile platform are implemented on the embedded microcontroller and algorithms for image processing are implemented on the central system.",https://ieeexplore.ieee.org/document/7090222/,"Proceedings of the 2014 6th International Conference on Electronics, Computers and Artificial Intelligence (ECAI)",23-25 Oct. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISP-BMEI53629.2021.9624462,Recursive Video Denoising Algorithm for Low Light Surveillance Applications,IEEE,Conferences,"We designed a video denoising algorithm for surveillance applications under low light conditions which is targeted to run on weak CPU. State of art algorithms don't meet these requirements because of huge memory bandwidth consumption. Among them, algorithms based on neural network have generalization issues especially when there are no references for training. Besides, the complexity of such methods is unaffordable in realtime embedding application. Hence, we propose three techniques, including: 1) adaptive noise strength estimation to fit into noise profile in real applications; 2) multi-resolution background segmentation inspired by human vision system, and 3) multi-pass denoise strategy. It's recursive and of first-order Markov property. Adaptive noise strength estimation also eliminates pre-calibration steps usually required by denoising algorithm and leads to easy deployment. Experiments show that our method can achieve better subjective denoising quality compared to state of art methods in target applications, especially in extremely low light scenes. Moreover, it requires small computation loads and small storage which makes it very suitable for implementation on weak CPUs.",https://ieeexplore.ieee.org/document/9624462/,"2021 14th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)",23-25 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV48630.2021.00340,Red Carpet to Fight Club: Partially-supervised Domain Transfer for Face Recognition in Violent Videos,IEEE,Conferences,"In many real-world problems, there is typically a large discrepancy between the characteristics of data used in training versus deployment. A prime example is the analysis of aggression videos: in a criminal incidence, typically suspects need to be identified based on their clean portraitlike photos, instead of their prior video recordings. This results in three major challenges; large domain discrepancy between violence videos and ID-photos, the lack of video examples for most individuals and limited training data availability. To mimic such scenarios, we formulate a realistic domain-transfer problem, where the goal is to transfer the recognition model trained on clean posed images to the target domain of violent videos, where training videos are available only for a subset of subjects. To this end, we introduce the ""WildestFaces"" dataset, tailored to study cross-domain recognition under a variety of adverse conditions. We divide the task of transferring a recognition model from the domain of clean images to the violent videos into two sub-problems and tackle them using (i) stacked affine-transforms for classifier-transfer, (ii) attention-driven pooling for temporal-adaptation. We additionally formulate a self-attention based model for domain-transfer. We establish a rigorous evaluation protocol for this ""clean-to-violent"" recognition task, and present a detailed analysis of the proposed dataset and the methods. Our experiments highlight the unique challenges introduced by the WildestFaces dataset and the advantages of the proposed approach.",https://ieeexplore.ieee.org/document/9423049/,2021 IEEE Winter Conference on Applications of Computer Vision (WACV),3-8 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR.2008.4761610,Refining PTZ camera calibration,IEEE,Conferences,"Due to the increased need for security and surveillance, PTZ cameras are now being widely used in many domains. Therefore, it is very important for the applications like video mosaic generation or automatic surveillance that these camera be accurately calibrated. In this paper, we address the problem of parameter refinement for such pan-tilt-zoom (PTZ) cameras. Use of bundle-adjustment for parameter refinement has widely been adopted in the computer vision field. However, as has been shown by researchers, in presence of noise, this Maximum Likelihood estimate looses its optimality. We propose a novel statistically optimal error function that is shown to experimentally outperform this ML estimate in presence of significant noise. We perform tests on synthetic as well as on real data to verify our method.",https://ieeexplore.ieee.org/document/4761610/,2008 19th International Conference on Pattern Recognition,8-11 Dec. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME46990.2020.00074,Regression Testing of Massively Multiplayer Online Role-Playing Games,IEEE,Conferences,"Regression testing aims to check the functionality consistency during software evolution. Although general regression testing has been extensively studied, regression testing in the context of video games, especially Massively Multiplayer Online Role-Playing Games (MMORPGs), is largely untouched so far. One big challenge is that game testing requires a certain level of intelligence in generating suitable action sequences among the huge search space, to accomplish complex tasks in the MMORPG. Existing game testing mainly relies on either the manual playing or manual scripting, which are labor-intensive and time-consuming. Even worse, it is often unable to satisfy the frequent industrial game evolution. The recent process in machine learning brings new opportunities for automatic game playing and testing. In this paper, we propose a reinforcement learning-based regression testing technique that explores differential behaviors between multiple versions of an MMORPGs such that the potential regression bugs could be detected. The preliminary evaluation on real industrial MMORPGs demonstrates the promising of our technique.",https://ieeexplore.ieee.org/document/9240641/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AICAI.2019.8701308,Rendering of Impaired Visual Effects on Genesis of Streak - Recognizing Car License Plate,IEEE,Conferences,"Rain Streak (RS) makes the recognition of License Plate (LP) more difficult as it partially hides the alphanumeric characters in the LP by creating patches on the LP. The high velocity rain drops over an image or video creates an undesirable interference. In real time, this streak is a form of occlusion which results in imperception of detecting the required candidate region like contours, boundary region etc. This results in impaired visual effects. Detection and removal of RS is a challenging task. It is observed that rain streak removal on License Plate (LP) results in loss of information. This paper provides an overview of rain streak removal schemes and implementation results of rain streak detection using guided filter and removal using Dictionaty learning. From the results, it is found that the rain streak removal creates impairment in License Plate alphanumeric character recognition. To rebuild the alphanumeric characters, context based character recognition using CNN can be attempted.",https://ieeexplore.ieee.org/document/8701308/,2019 Amity International Conference on Artificial Intelligence (AICAI),4-6 Feb. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRA46639.2022.9812394,RepAr-Net: Re-Parameterized Encoders and Attentive Feature Arsenals for Fast Video Denoising,IEEE,Conferences,"Real-time video denoising finds applications in several fields like mobile robotics, satellite television, and surveillance systems. Traditional denoising approaches are more common in such systems than their deep learning-based counterparts despite their inferior performance. The large size and heavy computational requirements of neural network-based denoising models pose a serious impediment to their deployment in real-time applications. In this paper, we propose RepAr-Net, a simple yet efficient architecture for fast video de noising. We propose to use temporally separable encoders to generate feature maps called arsenals that can be cached for reuse. We also incorporate re-parameterizable blocks that improve the representative power of the network without affecting the run-time. We benchmark our model on the Set-8 and 2017 DAVIS-Test datasets. Our model achieves state-of-the-art results with up to 29.62&#x0025; improvement in PSNR and a 50&#x0025; decrease in run times over existing methods. Our codes are open-sourced at: github.com/spider-tronix/RepAr-Net.",https://ieeexplore.ieee.org/document/9812394/,2022 International Conference on Robotics and Automation (ICRA),23-27 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACII.2009.5349393,Requirements and software framework for adaptive multimodal affect recognition,IEEE,Conferences,"This work presents a software framework for real time multimodal affect recognition. The framework supports categorical emotional models and simultaneous classification of emotional states along different dimensions. The framework also allows to incorporate diverse approaches to multimodal fusion, proposed by the current state of the art, as well as to adapt to context-dependency of expressing emotions and to different application requirements. The results of using the framework in audio-video based emotion recognition of an audience of different shows (this is a useful information because emotions of co-located people affect each other) confirm the capability of the framework to provide desired functionalities conveniently and demonstrate that use of contextual information increases recognition accuracy.",https://ieeexplore.ieee.org/document/5349393/,2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops,10-12 Sept. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCDC52312.2021.9601742,Research and Implementation of Tire Tracking Algorithm Based on Multi-Feature Fusion,IEEE,Conferences,"In order to improve highway utility, the national government is vigorously promoting automatic toll at highway entrances. Cameras, as indispensable facilities, have been installed on sides of toll lanes. When vehicles gradually drive through the video area, wheels are detected through target detection and tracking technologies. The number of axles is accordingly detected and it is a factor mattering toll for large trucks. This is turned out an essential methodology for axle counting. Multiple Object Tracking(MOT) is the key point for it, while most existing algorithms fail for the proximity of tire positions and the similarity of tire appearance. For the challenge of proximate positions, a relative position relation model is proposed. It defines adjacent tires as a group according to their proximity. Furthermore, Kalman filter is introduced to calibrate preliminary results by reducing matching range. To conquer the challenge in reference to similar appearance, factors in terms of relative position, background, texture and color of grouped tires are involved. These factors are synthetically integrated with weighted convolution features to improve association in tires. Based on the real video data collected from toll highway stations, it is tested and evaluated that the proposed algorithm can effectively improve the accuracy of tire tracking in adjacent positions and similar appearance.",https://ieeexplore.ieee.org/document/9601742/,2021 33rd Chinese Control and Decision Conference (CCDC),22-24 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CECNET.2011.5768334,Research and design of the intelligent surveillance system based on DirectShow and OpenCV,IEEE,Conferences,"With the development of video surveillance gradually from the traditional security surveillance to intelligent surveillance, in order to find an intelligent surveillance model to facilitate the implementation and promotion, we present a method, performing the moving object detection by using OpenCV based on DirectShow framework, which will make intelligent surveillance come true consequently. In this paper, it will first introduce a real-time priority method and some improvement of moving object detection. Secondly, the filter components that have moving object detection function will be designed by the combination method of DirectShow and OpenCV encapsulation technologies. Finally, it will present the design method and its advantages of building intelligent surveillance system on the basis of the above description.",https://ieeexplore.ieee.org/document/5768334/,"2011 International Conference on Consumer Electronics, Communications and Networks (CECNet)",16-18 April 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1260027,Research and implementation of a real time approach to lip detection in video sequences,IEEE,Conferences,"Locating the lip in video sequences is one of the primary steps of the automatic lipreading system. In this paper a new approach to lip detection, which is based on Red Exclusion and Fisher transform, is presented. In this approach, firstly, we locate face region with skin-color model and motion correlation, then trisect the face image and take into account the lowest part, in which the lip lies, for the next processing. Secondly, we exclude R-component in RGB color space, then use G-component and B-component as the Fisher transform vector to enhance the lip image. Finally, in the enhanced image, we adaptively set the threshold to separate the lip color and the skin color in the light of the normal distribution of the gray value histogram. The experimental results showed that this fast approach is very efficient in detecting the whole lip and not affected by illuminant and different speakers.",https://ieeexplore.ieee.org/document/1260027/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2003.1260018,"Research and implementation of real-time face detection, tracking and protection",IEEE,Conferences,"Privacy protection in video images is becoming one of the research focuses in the field of remote collaborative system. In this paper, a method for face detection, tracking and privacy protection is presented. According to skin-color distribution in the color space, we developed a statistical skin-color model through interactive sample training. Using this model we convert the color image to binary image and then segment face candidate region. Then we use a facial feature matching scheme for further detection. The presence or absence of a face in each region is verified by means of mouth detector. Real time detection and tracking can be achieved by using this method in video images. In order to speed up tracking, we improve the traditional method by adding motion prediction, which works better when several disturbing objects appear simultaneously. Finally we make the tracking region blurring and transmit the frames to the remote collaborative sites to obtain the privacy protection. The level of privacy protection can be dynamically adjusted according to collaborators' requests and credibility of remote sites. The experiment results show the proposed method not only has high speed and efficiency, but also is robust to head rotation to some extent.",https://ieeexplore.ieee.org/document/1260018/,Proceedings of the 2003 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.03EX693),5-5 Nov. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIID51893.2021.9456477,Research on Privacy Protection Technology in Face Identity Authentication System Based on Edge Computing,IEEE,Conferences,"In today's society, the rapid development of the Internet makes People's Daily life become more intelligent and diversified. Today's society has entered a multifaceted era where everything is interconnected. Artificial intelligence technology is gradually replacing some traditional human services, such as intelligent robot customer service instead of traditional human customer service, intelligent face scanning security check in railway stations instead of traditional manual ticket checking, unmanned supermarket automatic checkout has liberated some social labor costs. All these changes are the result of the development of artificial intelligence technology in today's society. In recent years, unicorn startups focused on biometrics have sprung up all around us, such as BTU and its MEG VII (Face ++). Thanks to the development of Internet and artificial intelligence technology, in many application fields, the traditional access control and identity authentication technology based on password verification is gradually transforming to the scheme based on biometric identification verification. Secure identity authentication is very important to the application of Internet. Face recognition is the most popular technology among all biometric identification technologies. In the field of biometric identification technology, it has become the most widely used technology in the field of identity authentication because of its unique non-invasive, support for infrared and visible light, no need for user cooperation and many other advantages. In the field of education, examinee identification, pedestrian identification detection at the entrance of railway stations, face electronic payment, intelligent video surveillance system, intelligent attendance and access control system, intelligent unmanned supermarkets and customs clearance ports become the pioneer fields of face recognition applications. It can be seen that the era of “national face brushing” has arrived, and the application of face recognition technology will only be more and more widespread in the current era and in the future. However, due to the sensitivity of biometric data and the heterogeneity and openness of network environment, the privacy leakage of biometric data is difficult to avoid. At present, fog computing and edge computing have been paid more and more attention in many fields. In the case that cloud service providers are unable to provide sufficient security, edge computing shows its advantages. In this paper, mobile edge computing is introduced for the first time into the face privacy protection identity authentication system based on cloud server outsourcing computing. It can not only greatly reduce the interaction frequency between users and cloud server, improve the availability and fault tolerance of the system, but also contribute to the implementation of privacy protection scheme. A deep constitutional neural network for face feature extraction is trained using deep learning framework Cafe. Cosine similarity is used to complete face verification. A privacy protection scheme based on the secure nearest neighbor algorithm is proposed, which can not only protect the security of the face feature data at the edge computing node, but also allow the edge computing node to complete the face recognition operation against the encrypted face feature data. In addition, the encryption scheme does not require large computing resources, and the accuracy of face recognition in cipher text is exactly the same as that in explain. At present, most of the solutions either have high computational complexity or poor security performance. How to reduce the computational complexity and improve the real-time performance of the system while ensuring the high security of the private data has important research significance and value. Therefore, in the cloud server outsourcing computing environment, how to complete biometric identification on the premise of protecting the privacy of biological data has become a research hot spot.",https://ieeexplore.ieee.org/document/9456477/,2021 IEEE International Conference on Artificial Intelligence and Industrial Design (AIID),28-30 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE51474.2020.00047,Research on Virtual Camera Automatic Shooting Based on Film Aesthetics,IEEE,Conferences,"The virtual camera is used in the virtual scene like the motion lens of traditional photography and video animation. It has a high degree of similarity with the way of image generation in reality, and its shooting directly affects and determines the final dynamic state of the picture. The use of virtual camera can not only effectively simplify settings, reduce complex operating procedures and improve the production, but also ensure the aesthetics of image generation. The article will mainly focus on the automatic virtual camera based on film aesthetics.",https://ieeexplore.ieee.org/document/9403840/,2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),30 Oct.-1 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSIE.2009.231,Research on the Design of the Categorization System for Moving Information in Video Stream and Its Implementation,IEEE,Conferences,"The moving objects are what attract most attention in the video surveillance system, and also the key part for study. Currently, the video surveillance system relies much on the subjective initiative of the observers while having the real-time surveillance. In this study, applying the mixture Gaussian model algorithm, the profile image of the moving objects in the picture got from the video surveillance is obtained, and then denoised so as to extract the feature vector of the image. Further, utilizing the already trained neural network, the feature vectors are categorized to elevate the intelligence of the surveillance system and to implement the automatic categorization on the moving objects. It is proved to be effective through the simulation test.",https://ieeexplore.ieee.org/document/5170730/,2009 WRI World Congress on Computer Science and Information Engineering,31 March-2 April 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIME.2010.5477962,Research on the embedded system of facial expression recognition based on HMM,IEEE,Conferences,"With the rapid development of artificial intelligence, how to make robots have feelings has become a research highlight today. A portable device for facial expression recognition has been designed, based on the hardware platform of Intel embedded processor PXA270, and the software platform of embedded operation system Linux. The functions of video capture, image processing and face tracking have been achieved, through the application of the USB, Video4Linux programming technology, image processing technology, and artificial psychology theory. At the same time, an emotion model more consistent with the characteristics of human emotions is proposed under the theoretical framework of HMM according to the characteristics of human emotions. A new real-time facial expression recognition method is proposed based on the face detection and facial feature detection technology. Experimental results show that, the effect of face recognition is satisfactory, and the method is effective.",https://ieeexplore.ieee.org/document/5477962/,2010 2nd IEEE International Conference on Information Management and Engineering,16-18 April 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIITME53254.2021.9663613,Retrofitting Video Surveillance Systems using Deep Learning Technologies,IEEE,Conferences,"Steps toward the development of an embedded system that is able to enhance the capabilities of a video surveillance system are described within the proposed paper. The hardware support for our solution is based on the newly introduced Google Coral development board having, as a prominent feature, the on-board Edge Tensor Processing Unit, capable of execute state-of-the-art mobile vision models such as MobileNet v2 at 100+ FPS, in a power efficient manner. The Google Coral is proven to outperform competing Machine Learning hardware accelerators currently available on the market and so represents the best accuracy/power efficiency choice. The software part includes some deep learning-based modules for video analytics such as detection, classification and tracking objects, including cars and people, using MobileNet + SSD architecture. Also, the experimental setup is described: compiling OpenCV for Google Coral, connecting to the video stream of the surveillance cameras. The system supports video equipment by any manufacturer by interfacing to it using Real Time Streaming Protocol/Open Network Video Interface Forum connections.",https://ieeexplore.ieee.org/document/9663613/,2021 IEEE 27th International Symposium for Design and Technology in Electronic Packaging (SIITME),27-30 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICST53961.2022.00048,RiverGame - a game testing tool using artificial intelligence,IEEE,Conferences,"As is the case with any very complex and interactive software, many video games are released with various minor or major issues that can potentially affect the user experience, cause security issues for players, or exploit the companies that deliver the products. To test their games, companies invest important resources in quality assurance personnel who usually perform the testing mostly manually. The main goal of our work is to automate various parts of the testing process that involve human users (testers) and thus to reduce costs and run more tests in less time. The secondary goal is to provide mechanisms to make test specification writing easier and more efficient. We focus on solving initial real-world problems that have emerged from several discussions with industry partners. In this paper, we present RiverGame, a tool that allows game developers to automatically test their products from different points of view: the rendered output, the sound played by the game, the animation and movement of the entities, the performance and various statistical analyses. We also address the problem of input priorities, scheduling, and directing the testing effort towards custom and dynamic directions. At the core of our methods, we use state-of-the-art artificial intelligence methods for analysis and a behavior-driven development (BDD) methodology for test specifications. Our technical solution is open-source, independent of game engine, platform, and programming language.",https://ieeexplore.ieee.org/document/9787838/,"2022 IEEE Conference on Software Testing, Verification and Validation (ICST)",4-14 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM41043.2020.9155467,Rldish: Edge-Assisted QoE Optimization of HTTP Live Streaming with Reinforcement Learning,IEEE,Conferences,"Recent years have seen a rapidly increasing traffic demand for HTTP-based high-quality live video streaming. The surging traffic demand, as well as the real-time property of live videos, make it challenging for content delivery networks (CDNs) to guarantee the Quality-of-Experiences (QoE) of viewers. The initial video segment (IVS) of live streaming plays an important role in the QoE of live viewers, particularly when users require fast join time and smooth view experience. State-of-the-art research on this regard estimates network throughput for each viewer and thus may incur a large overhead that offsets the benefit. To tackle the problem, we propose Rldish, a scheme deployed at the edge CDN server, to dynamically select a suitable IVS for new live viewers based on Reinforcement Learning (RL). Rldish is transparent to both the client and the streaming server. It collects the real-time QoE observations from the edge without any client-side assistance, then uses these QoE observations as real-time rewards in RL. We deploy Rldish as a virtualized network function (VNF) in a real HTTP cache server, and evaluate its performance using streaming servers distributed over the world. Our experiments show that Rldish improves the state- of-the-art IVS selection scheme w.r.t. the average QoE of live viewers by up to 22%.",https://ieeexplore.ieee.org/document/9155467/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WIECON-ECE52138.2020.9397954,Road Object Detection in Bangladesh using Faster R-CNN: A Deep Learning Approach,IEEE,Conferences,"The importance of object detection in our lives is increasing day by day. The role of object detection is very important in autonomous cars, intelligent driving assistance, and advanced traffic analysis. In the case of traffic analysis and intelligent driving assistance in Bangladesh, it is very important to properly identify all the objects from real-time video. Because in both cases the main responsibility of the system is to give the driver or authority a clear idea about the road or the environment around the vehicle. And for this, we need to use modern algorithms and architecture based neural network models with much better object detection accuracy such as Faster R-CNN. There are currently a couple of algorithms that work faster than Faster R-CNN but cannot detect objects accurately, as is the case with small-to-medium objects. We used Faster R-CNN on our data to analyze the environment around the road and the environment around the car. We trained the network for 19 object classes and tested its ability to detect objects with real-time video analysis with an accuracy of 86.42%. Moreover, FPR(false positive rate) and FNR(false negative rate) is calculated to evaluate the proposed model from confusion matrices. In this study, the FPR of the Faster R-CNN model is 15.97% and the FNR of the Faster R-CNN model is 12.2%.",https://ieeexplore.ieee.org/document/9397954/,2020 IEEE International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE),26-27 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBIO49542.2019.8961517,Robot Control in Human Environment using Deep Reinforcement Learning and Convolutional Neural Network,IEEE,Conferences,"Deep reinforcement learning (DRL) has been employed in numerous applications where complex decision-making is needed. Robot control in a human environment is an example. Such algorithm offers possibilities to achieve end-to-end training which learns from image directly. However, training on a physical robotic system under human environments using DRL is inefficient and even dangerous. Several recent works have used simulators for training models before implementing to physical robots. Although simulation provides efficiency to obtain DRL trained models, it poses challenges for the transformation from simulation to reality. Since a human environment is often cluttered, dynamic and complex, the policy trained with simulation images is not applicable for reality. Therefore, in this paper, we propose a DRL method to achieve end-to-end training in simulation, as well as to adapt to reality without any further finetune. Firstly, a Deep Deterministic Policy Gradient algorithm (DDPG) is employed to learn policy for robot control. Secondly, a pre-trained Convolutional Neural Network algorithm (CNN) is used to visually track the target in image. This technique provides the efficient and safe DRL training in simulation while offering robust application when a real robot is placed in dynamic human environment. Simulation and experiment are conducted for validation and can be seen in the attached video. The results have shown successful demonstration under various complex environments.",https://ieeexplore.ieee.org/document/8961517/,2019 IEEE International Conference on Robotics and Biomimetics (ROBIO),6-8 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IISA.2017.8316452,Robot painting recognition based on deep belief learning,IEEE,Conferences,"In a society where the number of elderly people is increasing rapidly, autonomous wheelchair robots are expected to be widely used for mobility of elderly people. In this paper we focus on how we can utilize wheelchair robots operating in museums. In this paper, we propose a deep learning based painting recognition and its application for the wheelchair robot. We consider the case when the user clicks on the painting he/she wants to see. The robot searches, recognizes and reaches the painting using deep learning. This is in difference from the most traditional methods where the robot explains the exhibited objects in a sequential order. The deep neural network generates a series of high dimensional features for each painting resulting in a high recognition rate. In our implementation, the wheelchair robot recognizes the painting in real time using the video stream.",https://ieeexplore.ieee.org/document/8316452/,"2017 8th International Conference on Information, Intelligence, Systems & Applications (IISA)",27-30 Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.2007.366063,Robust Object Tracking with Radial Basis Function Networks,IEEE,Conferences,"Visual tracking has been a challenging problem in computer vision over the decades. The applications of visual tracking are far-reaching, ranging from surveillance and monitoring to smart rooms. In this paper we present a novel object tracker based on fast learning radial basis function (RBF) networks. Here, the object and background pixel-based color features are used to develop object/non-object RBF classifiers. The posterior probability information of these classifiers are used for developing an efficient object model for tracking in the subsequent frames. The performance of the proposed tracker is tested with many video sequences of real-life complexity and compared against the color-based mean-shift tracker. The proposed tracker is illustrated to be suitable for real-time robust object tracking due to its low computational complexity.",https://ieeexplore.ieee.org/document/4217235/,"2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07",15-20 April 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE51222.2021.9404632,Robust Real Time Mask Detection Using Deep Learning in an Complex Background,IEEE,Conferences,"Our team will be building a Mask Detection System as in this epidemic that would be a great help in the identification whether a person wears a mask or not. The system will be build with the help of TensorFlow and Deep Learning technology using Projection and recognition algorithm. There are certain tools that will be used while building the model like Tensorflow, Keras and OpenCV. We will also test the software on a Live Video Camera. By optimizing more this model can easily be integrated with security cameras to detect and identify people without mask. This model is trained and thus doesn't require any dataset. Due to the help of projection and architecture, the model is accurate and could easily used in embedded system. This project can easily be used in live cctv cameras in places like Hospitals, Schools, Airports and Railway Stations by which one can easily monitor and ensure public safety in this Covid-19 outbreak.",https://ieeexplore.ieee.org/document/9404632/,2021 International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),4-5 March 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PRML52754.2021.9520696,Robust Real-Time Human Action Detection through the Fusion of 3D and 2D CNN,IEEE,Conferences,"Recent approaches for human action detection often rely on appearance and optical flow networks for frame-level detections before linking them to form action tubes. However, they achieve unsatisfactory performance in real-time due to their huge computational complexity and large parameter usage during training. In this paper, we design and implement a unified end-to-end convolutional neural network (CNN) architecture that consists of two branches, extracting both spatial and temporal information concurrently before predicting bounding boxes and action probabilities from video clips. We also design a novel mechanism that exploits the inter-channel dependencies for an effective fusion of features from the branches. Specifically, we propose a Channel Fusion and Relation-Global Attention (CFRGA) module to aggregate the two features smoothly and model their inter-channel dependencies by considering their global scope structural relation information when inferring attention. We conduct experiments on the untrimmed video dataset, UCF101-24, and achieved impressive results in frame-mAP and video-mAP. The experimental results show that our channel fusion and relation-global attention module contributes to its good performance.",https://ieeexplore.ieee.org/document/9520696/,2021 IEEE 2nd International Conference on Pattern Recognition and Machine Learning (PRML),16-18 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HUMANOIDS.2014.7041487,Robust fall detection with an assistive humanoid robot,IEEE,Conferences,"Summary form only given. In this video we introduce a robot assistant that monitors a person in a household environment to promptly detect fall events. In contrast to the use of a fixed sensor, the humanoid robot will track and keep the moving person in the scene while performing daily activities. For this purpose, we extended the humanoid Nao1 with a depth sensor2 attached to its head. The tracking framework implemented with OpenNI3 segments and tracks the person's position and body posture. We use a learning neural framework for processing the extracted body features and detecting abnormal behaviors, e.g. a fall event [1]. The neural architecture consists of a hierarchy of self-organizing neural networks for attenuating noise caused by tracking errors and detecting fall events from video stream in real time. The tracking application, the neural framework, and the humanoid actuators communicate over Robot Operating System (ROS)4. We use communication over the ROS network implemented with publisher-subscriber nodes. When a fall event is detected, Nao will approach the person and ask whether assistance is needed. In any case, Nao will take a picture of the scene that can be sent to the caregiver or a relative for further human evaluation and agile intervention. The combination of this sensor technology with our neural network approach allows to tailor the robust detection of falls independently from the background surroundings and in the presence of noise (tracking errors and occlusions) introduced by a real-world scenario. The video shows experiments run in a home-like environment.",https://ieeexplore.ieee.org/document/7041487/,2014 IEEE-RAS International Conference on Humanoid Robots,18-20 Nov. 2014,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCKE.2015.7365866,Robust hand gestures tracking method in cluttered background based on multilayer perceptron,IEEE,Conferences,"This paper presents the use of multilayer perceptron (MLP) neural network with supervised learning algorithm to track hand gestures. Two networks are used. The first network is trained to detect skin and the second identifies face, hand and its gesture. A binary image made from the first network is transmitted to second network as its input. Both networks have two output classes, first network: skin and non skin in the other network due to existence of hand and face in binary image, first class is gestures of hand and second class is face. Hand gestures are divided into 5 cases: hand closed, hand open, two fingers (victory), three fingers and Index and Little finger. The two networks are applied on video sequence frame to frame. Features in the second network are selected so that tracking is independent from size and rotation of hand and computation is decreased because of excising common terms between DFT and DCT. Results show both networks have high accuracy. Matlab is used and not claim real time processing (2 fps) but in our future work these two networks are implemented on FPGA in real time.",https://ieeexplore.ieee.org/document/7365866/,2015 5th International Conference on Computer and Knowledge Engineering (ICCKE),29-29 Oct. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RATFG.1999.799239,Robust real-time human hand localization by self-organizing color segmentation,IEEE,Conferences,"This paper describes a robust tracking algorithm used to localize a human hand in video sequences. The localization system relies mainly on an automatic color-based segmentation scheme combined with the motion cue. An automatic self-organizing clustering algorithm, is proposed to learn the color clusters unsupervisedly in the HSI space without specifying the number of clusters in advance. The schemes of growing, pruning and merging of 1-D self-organizing map (SOM) are facilitated to find an appropriate number of clusters in the forming stage of SOM. The training and segmentation in our approach is fast enough to make possible real-time applications. This segmentation scheme is capable of tracking multiple objects of different colors simultaneously. A motion cue is employed to focus the attention of the tracking algorithm. This approach is also applied to other tasks such as human face tracking and color indexing. Our localization system implemented on a SGI O2 R10000 workstation is reliable and efficient at 20-30 Hz.",https://ieeexplore.ieee.org/document/799239/,"Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)",26-27 Sept. 1999,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ROBOT.2009.5152197,Robust servo-control for underwater robots using banks of visual filters,IEEE,Conferences,"We present an application of machine learning to the semi-automatic synthesis of robust servo-trackers for underwater robotics. In particular, we investigate an approach based on the use of Boosting for robust visual tracking of color objects in an underwater environment. To this end, we use AdaBoost, the most common variant of the Boosting algorithm, to select a number of low-complexity but moderately accurate color feature trackers and we combine their outputs. The novelty of our approach lies in the design of this family of weak trackers, which enhances a straightforward color segmentation tracker in multiple ways. From a large and diverse family of possible filters, we select a small subset that optimizes the performance of our trackers. The tracking process applies these trackers on the input video frames, and the final tracker output is chosen based on the weights of the final array of trackers. By using computationally inexpensive, but somewhat accurate trackers as members of the ensemble, the system is able to run at quasi real-time, and thus, is deployable on-board our underwater robot. We present quantitative cross-validation results of our spatio-chromatic visual tracker, and conclude by pointing out some difficulties faced and subsequent shortcomings in the experiments we performed, along with directions of future research in the area of ensemble tracking in real-time.",https://ieeexplore.ieee.org/document/5152197/,2009 IEEE International Conference on Robotics and Automation,12-17 May 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIEA49774.2020.9102061,Route Planning for Automatic Indoor Driving of Smart Cars,IEEE,Conferences,"While various types of traffic navigation maps play an important role in our life, the complex layout of large number of buildings in the city makes it difficult for people to find destinations and walking routes indoors. Since GPS does not work very accurately inside the buildings, for indoor navigation problem we still have to depend on the traditional front desk human resource consultation. In recent years, the rapid development of artificial intelligence technology has made it possible for smart car navigation instead of the manual consultation. Not only that, smart cars can even handle short-distance goods transportation. Combining indoor positioning technology with different video signals or any other non-GPS satellite positioning technology for directing the smart car to the designated position is a hot research direction these days. In this paper, we mainly study the realization and application of a smart car in the indoor automated driving system. Our proposed smart car, built using the Raspberry Pi 3, uses a series of famous path planning algorithms such as Dijkstra's algorithm, Best priority search and A*(A star) algorithm to plan the driving path of the car's automatic driving. Finally, the real-time obstacle avoidance function is also realized by using ultrasonic ranging, infrared ranging and pan-tilt camera. Several experiments were conducted to compare the efficiency of using different path finding algorithms and the results of using infrared ranging and ultrasonic ranging during obstacle avoidance.",https://ieeexplore.ieee.org/document/9102061/,2020 IEEE 7th International Conference on Industrial Engineering and Applications (ICIEA),16-21 April 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3079856.3080244,SCALEDEEP: A scalable compute architecture for learning and evaluating deep networks,IEEE,Conferences,"Deep Neural Networks (DNNs) have demonstrated state-of-the-art performance on a broad range of tasks involving natural language, speech, image, and video processing, and are deployed in many real world applications. However, DNNs impose significant computational challenges owing to the complexity of the networks and the amount of data they process, both of which are projected to grow in the future. To improve the efficiency of DNNs, we propose SCALEDEEP, a dense, scalable server architecture, whose processing, memory and interconnect subsystems are specialized to leverage the compute and communication characteristics of DNNs. While several DNN accelerator designs have been proposed in recent years, the key difference is that SCALEDEEP primarily targets DNN training, as opposed to only inference or evaluation. The key architectural features from which SCALEDEEP derives its efficiency are: (i) heterogeneous processing tiles and chips to match the wide diversity in computational characteristics (FLOPs and Bytes/FLOP ratio) that manifest at different levels of granularity in DNNs, (ii) a memory hierarchy and 3-tiered interconnect topology that is suited to the memory access and communication patterns in DNNs, (iii) a low-overhead synchronization mechanism based on hardware data-flow trackers, and (iv) methods to map DNNs to the proposed architecture that minimize data movement and improve core utilization through nested pipelining. We have developed a compiler to allow any DNN topology to be programmed onto SCALEDEEP, and a detailed architectural simulator to estimate performance and energy. The simulator incorporates timing and power models of SCALEDEEP's components based on synthesis to Intel's 14nm technology. We evaluate an embodiment of SCALEDEEP with 7032 processing tiles that operates at 600 MHz and has a peak performance of 680 TFLOPs (single precision) and 1.35 PFLOPs (half-precision) at 1.4KW. Across 11 state-of-the-art DNNs containing 0.65M-14.9M neurons and 6.8M-145.9M weights, including winners from 5 years of the ImageNet competition, SCALEDEEP demonstrates 6×-28× speedup at iso-power over the state-of-the-art performance on GPUs.",https://ieeexplore.ieee.org/document/8192466/,2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA),24-28 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IC-NIDC54101.2021.9660473,SDN-Based QoE Evaluation Methods for HTTP Adaptive Video Streaming,IEEE,Conferences,"Nowadays, video streaming has become the main traffic on the Internet. The demand of people for video services is growing higher and higher. In order to meet the requirements of users, the service providers such as network providers and application providers must guarantee good Quality of Experience (QoE). This paper reviews the influencing factors and main evaluation methods of user QoE, and introduces the research status and progress in this field. Then, combining with the most popular HTTP Adaptive Streaming (HAS) and SDN (Software-Defined Networking) technologies, we build an SDN-based QoE experimental platform for video streaming based some open-source software packages, in which we integrate a variety of QoE evaluation models and real extensible network environment modules. Finally, in the SDN-based virtual network, we perform videos multiple times, using different QoE evaluation methods each time, and obtain visual results. Our results prove that the platform architecture has the ability of helping researchers to carry out QoE evaluation testing in a variety of virtual network environments.",https://ieeexplore.ieee.org/document/9660473/,2021 7th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC),17-19 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/LCN48667.2020.9314837,SETA: Scalable Encrypted Traffic Analytics in Multi-Gbps Networks,IEEE,Conferences,"While end-to-end encryption brings security and privacy to the end-users, it makes legacy solutions such as Deep Packet Inspection ineffective. Despite the recent work in machine learning-based encrypted traffic classification, these new techniques would require, if they were to be deployed in real enterprise-scale networks, an enhanced flow sampling due to sheer volume of data being traversed. In this paper, we propose a holistic architecture that can cope with encryption and multi-Gbps line rate with sampling and sketching flow statistics, which allows network operators to both accurately estimate the flow size distribution and identify the nature of VPN-obfuscated traffic. With over 6000 video traffic traces, we show that it is possible to achieve 99% accuracy for service provider classification even with sampled possibly inaccurate data.",https://ieeexplore.ieee.org/document/9314837/,2020 IEEE 45th Conference on Local Computer Networks (LCN),16-19 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData47090.2019.9005472,Scalable Object Tracking in Smart Cities,IEEE,Conferences,"In smart cities equipped with cameras, one desirable use-case is to detect and track objects. While object detection has been implemented using various methods, object tracking poses a different problem; to track an object requires object permanence to be established between each frame of video. While many technologies have been proposed as a solution for problem, an implementation with scalability in mind has not been developed and poses many new challenges. This paper proposes e-SORT, a solution for scalable object tracking using an enhanced version of the Simple Online and Realtime Tracking (SORT) algorithm. Beyond its scalability, e-SORT stores a mapping of each objects' locations such that the full path of each object is available and several metrics (such as velocity and acceleration) can be calculated. Both e-SORT's abilities and our proposed solution to scalable object tracking are tested and evaluated on Chattanooga Tennessee's live urban testbed.",https://ieeexplore.ieee.org/document/9005472/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNNB.2005.1614810,Security Assurance Using Face Recognition & Detection System Based On Neural Networks,IEEE,Conferences,"In this paper, we have proposed a new method of implementing an assurance system using the facial information of the people, this is a different approach to the conventional security system which uses biometric information or cryptography for assurance, here we use an efficient self-scaling face recognition system supported with a face detection system, the system is capable enough to extract the human faces from a real time video and to recognize the people using a face recognition system, we are designing the framework for face recognition system with a hybrid RBF neural network, the real advantage of the system lies in its capability to inculcate some basic features of the self organizing map (SOM) so that the system can scale on its own and it doesn't get outdated with time, for the face detection system we use a content based face detection algorithm, the facial feature so detected is inputted into the face recognition system, if the person's information is already present in the system, authentication can be accomplished, this system can be used in public places like airports and supermarkets, the information of criminals can be stored in the system and in case any of the criminals are detected by the system, the security personnel can be signaled, this system can also be implemented in robotics, the system can help the computer to identify individual users distinctly, our facial recognition system has been tested and found to be persistent in recognizing the individual even if the input facial image is of different gesture or holds some extra lineament like beard, moustache or spectacles so we can definitely state that the system is reliable and efficient",https://ieeexplore.ieee.org/document/1614810/,2005 International Conference on Neural Networks and Brain,13-15 Oct. 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APUAVD47061.2019.8943841,Segmentation of Digital Images of Aerial Photography,IEEE,Conferences,"For today, autonomous UAVs are a combination of artificial intelligence, mechanical devices, and navigational instruments. To save computing resources and improve the quality of their navigation, motion control systems, recognition, etc., real-time UAV's video has to be preprocessed by segmentation or clustering algorithms. In this work, the analysis of parameters of effective graph-based (EGB) and pyramidal segmentation algorithms (PSA) was obtained for digital images of aerial photography. The authors used RGB, Lab, and HSV (BHS) color models. According to the results of testing EGB algorithm is faster, but its result of segmentation is ordinary. Whereas, PSA produces a result that is closer to human perception but its disadvantage is a long processing time. Authors recommended Lab and HBS formats of image, since their segmentation results are more effective than, for example, at RGB.",https://ieeexplore.ieee.org/document/8943841/,2019 IEEE 5th International Conference Actual Problems of Unmanned Aerial Vehicles Developments (APUAVD),22-24 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412558,Self-Training for Domain Adaptive Scene Text Detection,IEEE,Conferences,"Though deep learning based scene text detection has achieved great progress, well-trained detectors suffer from severe performance degradation for different domains. In general, a tremendous amount of data is indispensable to train the detector in the target domain. However, data collection and annotation are expensive and time-consuming. To address this problem, we propose a self-training framework to automatically mine hard examples with pseudo-labels from unannotated videos or images. To reduce the noise of hard examples, a novel text mining module is implemented based on the fusion of detection and tracking results. Then, an image-to-video generation method is designed for the tasks that videos are unavailable and only images can be used. Experimental results on standard benchmarks, including ICDAR2015, MSRA-TD500, ICDAR2017 MLT, demonstrate the effectiveness of our self-training method. The simple Mask R-CNN adapted with self-training and fine-tuned on real data can achieve comparable or even superior results with the state-of-the-art methods.",https://ieeexplore.ieee.org/document/9412558/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INDICON.2016.7839069,Selfie continuous sign language recognition using neural network,IEEE,Conferences,"This works objective is to bring sign language closer to real time implementation on mobile platforms with a video database of Indian sign language created with a mobile front camera in selfie mode. Pre-filtering, segmentation and feature extraction on video frames creates a sign language feature space. Artificial Neural Network classifier on the sign feature space are trained with feed forward nets and tested. ASUS smart phone with 5M pixel front camera captures continuous sign videos containing on average of 220 frames for 18 single handed signs at a frame rate of 30fps. Sobel edge operator's power is enhanced with morphology and adaptive thresholding giving a near perfect segmentation of hand and head portions. Word matching score (WMS) gives the performance of the proposed method with an average WMS of around 90% for ANN with an execution time of 0.5221 seconds during classification. Fully novel method of implementing sign language to put sign language recognition systems on smart phones to make it a real time usage application.",https://ieeexplore.ieee.org/document/7839069/,2016 IEEE Annual India Conference (INDICON),16-18 Dec. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS.2016.7759769,Sensor substitution for video-based action recognition,IEEE,Conferences,"There are many applications where domain-specific sensing, such as accelerometers, kinematics, or force sensing, provide unique and important information for control or for analysis of motion. However, it is not always the case that these sensors can be deployed or accessed beyond laboratory environments. For example, it is possible to instrument humans or robots to measure motion in the laboratory in ways that it is not possible to replicate in the wild. An alternative, which we explore in this paper, is to address situations where accurate sensing is available while training an algorithm, but for which only video is available for deployment. We present two examples of this sensory substitution methodology. The first variation trains a convolutional neural network to regress real-valued signals, including robot end-effector pose, from video. The second example regresses binary signals derived from accelerometer data which signifies when specific objects are in motion. We evaluate these on the JIGSAWS dataset for robotic surgery training assessment and the 50 Salads dataset for modeling complex structured cooking tasks. We evaluate the trained models for video-based action recognition and show that the trained models provide information that is comparable to the sensory signals they replace.",https://ieeexplore.ieee.org/document/7759769/,2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),9-14 Oct. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCICC46617.2019.9146103,Sequence Learning for Images Recognition in Videos with Differential Neural Networks,IEEE,Conferences,"Sequence learning from real-time videos is one of the hard challenges to current machine learning technologies and classic neural networks. Since existing supervised learning technologies are heavily dependent on intensive data and prior training, new methodologies for learning temporal sequences by unsupervised learning theories and technologies are yet to be developed. This paper presents the design and implementation of a novel Differential Neural Network (∇NN) for unsupervised sequence learning. The methodology is developed with a set of fundamental theories and enabling technologies for solving the problems of visual object recognition, motion detection, and visual semantic analysis in video sequence. A set of experiments on ∇NN for sequence learning is demonstrated. This work has not only led to a theoretical breakthrough to novel machine sequence learning, but also applicable to a wide range of challenging problems in computational intelligence and the AI industry.",https://ieeexplore.ieee.org/document/9146103/,2019 IEEE 18th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC),23-25 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ITSC48978.2021.9565074,SiamPolar: Realtime Video Object Segmentation with Polar Representation in Traffic Scenes,IEEE,Conferences,"Video object segmentation (VOS) is an essential part of intelligent transportation systems (ITS). In the real traffic scenes, not only stable accuracy but also real-time speed are important metrics. In this paper, we propose a semi-supervised real-time video object segmentation method based on a Siamese network with a novel polar representation method. This polar representation method could reduce the parameters for encoding masks, so that the inference speed is enhanced significantly. Besides, an asymmetric Siamese network is designed to extract the features from different spatial scales. To reduce the antagonism among the branches of the head part, we propose the idea of peeling convolution. Based on this idea, repeated cross-correlation and semi-FPN are designed as the neck part of the neural network. The experiments on the DAVIS-2016 dataset demonstrate that SiamPolar achieves 71.4% J-mean and 59.2fps. In the real traffic scene dataset, TSD-max dataset, SiamPolar performs the 80.5% J-mean.",https://ieeexplore.ieee.org/document/9565074/,2021 IEEE International Intelligent Transportation Systems Conference (ITSC),19-22 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DICTA.2018.8615804,Similar Gesture Recognition using Hierarchical Classification Approach in RGB Videos,IEEE,Conferences,"Recognizing human actions from the video streams has become one of the very popular research areas in computer vision and deep learning in the recent years. Action recognition is wildly used in different scenarios in real life, such as surveillance, robotics, healthcare, video indexing and human-computer interaction. The challenges and complexity involved in developing a video-based human action recognition system are manifold. In particular, recognizing actions with similar gestures and describing complex actions is a very challenging problem. To address these issues, we study the problem of classifying human actions using Convolutional Neural Networks (CNN) and develop a hierarchical 3DCNN architecture for similar gesture recognition. The proposed model firstly combines similar gesture pairs into one class, and classify them along with all other class, as a stage-1 classification. In stage-2, similar gesture pairs are classified individually, which reduces the problem to binary classification. We apply and evaluate the developed models to recognize the similar human actions on the HMDB51 dataset. The result shows that the proposed model can achieve high performance in comparison to the state-of-the-art methods.",https://ieeexplore.ieee.org/document/8615804/,2018 Digital Image Computing: Techniques and Applications (DICTA),10-13 Dec. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIRCA.2018.8597327,Smart Cap - Wearable Visual Guidance System for Blind,IEEE,Conferences,"Science and technology always try to make human life easier. The people who are having complete blindness or low vision faces many difficulties during their navigation. Blindness can occur due to many reasons including disease, injury or other conditions that limit vision. The main purpose of this paper is to develop a navigation aid for the blind and the visually impaired people. In this paper, we design and implement a smart cap which helps the blind and the visually impaired people to navigate freely by experiencing their surroundings. The scene around the person will be captured by using a NoIR camera and the objects in the scene will be detected. The earphones will give a voice output describing the detected objects. The architecture of the system includes the processor Raspberry Pi 3, NoIR camera, earphones and a power source. The processor collects the frames of the surroundings and convert it to voice output. The device uses TensorFlow API, open-source machine learning library developed by the Google Brain Team for the object detection and classification. TensorFlow helps in creating machine learning models capable of identifying and classifying multiple objects in a single image. Thus, details corresponding to various objects present within a single frame are obtained using TensorFlow API. A Text to Speech Synthesiser (TTS) software called eSpeak is used for converting the details of the detected object (in text format) to speech output. So the video captured by using the NoIR camera is finally converted to speech signals and thus narration of the scene describing various objects is done. Objects which come under 90 different classes like cell phone, vase, person, couch etc are detected.",https://ieeexplore.ieee.org/document/8597327/,2018 International Conference on Inventive Research in Computing Applications (ICIRCA),11-12 July 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00226,Smart City Traffic Intersection: Impact of Video Quality and Scene Complexity on Precision and Inference,IEEE,Conferences,"Traffic intersections are prime locations for deployment of infrastructure sensors and edge computing nodes to realize the vision of a smart city. It is expected that the needs of a smart city, in regards to traffic and pedestrian traffic systems monitored by cameras/video, can be met by using state-of-the-art artificial-intelligence (AI) based object detectors and trackers. A critical component in designing an effective real-time object detection/tracking pipeline is the understanding of how object density, i.e., the number of objects in a scene, and image-resolution and frame rate influence the performance metrics. This study explores the accuracy and speed metrics with the goal of supporting pipelines that meet the precision and latency needs of a real-time environment. We examine the impact of varying image-resolution, frame rate and object-density on the object detection performance metrics. The experiments on the COSMOS testbed dataset show that varying the frame width from 416 pixels to 832 pixels, and cropping the images to a square resolution, result in the increase in average precision for all object classes. Decreasing the frame rate from 15 fps to 5 fps preserves more than 90&#x0025; of the highest F1 score achieved for all object classes. The results inform the choice of video preprocessing stages, modifications to established AI-based object detection/tracking methods, and suggest optimal hyper-parameter values.",https://ieeexplore.ieee.org/document/9781018/,"2021 IEEE 23rd Int Conf on High Performance Computing & Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)",20-22 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITR54349.2021.9657285,Smart Photo Editor for Differently-abled People using Assistive Technology,IEEE,Conferences,"Differently-abled people are significant minority groups, have many limitations to live everyday life, are starved of services, and are mostly ignored by society due to many types of disabilities. This problem may lead to massive frustrations and disappointments among themselves. Assistive technology may easily fill this gap with the enhanced different aiding applications. It helped in various ways, such as helping them to interact, communicate with people and allow people to do basic daily tasks by themselves. Nevertheless, there were fewer applications developed to entertain their leisure time. Therefore, the proposed solution provides a Smart Photo Editor, a hand-free mobile application with attractive photo editing features. This application focused on people having hand motor disabilities. Users can interact with the application using simple head gestures, eye gaze, and eye blinks. Head gestures and eye blinks are used as commands for application operations, while eye gaze is used for cursor enabling, which is helpful for navigation on application screens. Machine Learning (ML) and image processing techniques are used for real-time gesture recognition and eye gaze detection, using the input video frames captured by the inbuilt front-facing camera of the mobile phone. Moreover, in addition to the image processing techniques, Mask Recurrent-Convolutional Neural Network (R-CNN) is used to develop enhanced image adjusting features and customization of backgrounds based on object detection.",https://ieeexplore.ieee.org/document/9657285/,2021 6th International Conference on Information Technology Research (ICITR),1-3 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICRITO.2018.8748703,Smart Security Surveillance using IoT,IEEE,Conferences,"With the advancement of technology and ICT, a radical change in life is reflecting all around. In this 21st century, with the gift of Internet, Internet of Things (IoT) has become an integral part of our everyday life. Today IoT not only enable sensations of life but also imparts new genre of innovations. It has been found that IoT plays a significant role in achieving security. In this proposed research using IoT for Smart Security Surveillance System aimed at enhancing the security of home with the help of IoT. The research intends to develop a smartest version of doorbell with more security, flexibility and connectivity. In this work, with the help of sensor and camera, the system detects motion in front of the door and capture picture which is sent to the user's cell phone along with the picture in the email inbox and a detection message. In addition a push button for the guest interaction is also considered. The user is able to see what is happening in front of the door in real time sitting on the Internet. A camera module is deployed to capture the picture of the visitor and stream a video on the basis of passive infra-red (PIR) sensor and the button. In the future, further AI will be added to the system for recognizing and identifying the gesture of the visitor using CV.",https://ieeexplore.ieee.org/document/8748703/,"2018 7th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions) (ICRITO)",29-31 Aug. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCI50826.2021.9402589,Smart staff attendance system using Convolutional Neural Network,IEEE,Conferences,"In this paper we have implemented Deep Learning model Convolutional Neural Network architecture for face detection to build a smart attendance system that will detect the faces of all the staff members and the attendance is marked automatically. This is a real time application which comes with day-to-day activities of handling attendance system in an institution. The process involves recognizing the face of staff members from the video taken through surveillance camera kept at different locations in the institution and other information technologies associated with the system. The proposed system will be able to find and recognize staff member faces fast and precisely with an accuracy of 90%. In addition, various data augmentation techniques are employed in the proposed system that improves the system accuracy further from 90% to 96%",https://ieeexplore.ieee.org/document/9402589/,2021 International Conference on Computer Communication and Informatics (ICCCI),27-29 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICTAI50040.2020.00081,SoMem: A Self-optimizing Memory Network for Distributed Person Re-identification,IEEE,Conferences,"Person Re-Identification (Re-ID) aims to match the persons contained in surveillance videos, and is usually run on powerful servers in a supervised mode. However, centralized processing of massive video from thousands of cameras in a city is very costly and causes serious problems of privacy protection. Moreover, the labeling of numerous data for supervised training is also infeasible in this scenario. To address this problem, we propose a novel Self-optimizing Memory Network model, namely SoMem, which runs person Re-ID on edge devices in a totally unsupervised and distributed way. Specifically, SoMem adopts a random walk based collaborative training procedure to optimize the visual model on each camera based on locally collected images, and builds a distributed memory network to memorize and match the observed persons by using a distributed mutual ranking algorithm. Based on the cross-camera person matching results learned by the memory network, the visual models on edge devices are further optimized in a self-organized manner. Comprehensive experiments are conducted on several real person Re-ID datasets and deployed on edge devices to show the effectiveness and efficiency of this novel distributed Re-ID model.",https://ieeexplore.ieee.org/document/9288284/,2020 IEEE 32nd International Conference on Tools with Artificial Intelligence (ICTAI),9-11 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/APCCAS51387.2021.9687720,Social Distancing Surveillance System via Inverse Perspective Mapping and Fixed-point Quantization,IEEE,Conferences,"During the Coronavirus Disease 2019 (COVID-19) pandemic, many countries have introduced the social distancing policy in public areas to stop the spread of disease by maintaining a physical distance between people. This paper proposes an Artificial Intelligence (AI)-powered social distancing surveillance system that can detect pedestrians through video surveillance and monitor the social distance between them via Inverse Perspective Mapping (IPM) in real-time. The proposed system was deployed on the devices located at the network edge such as IoT devices and mobile devices to enable real-time response with low data transmission latency. To bypass the restriction on the computational and memory capacity for the edge devices, the proposed system was optimized through fixed-point quantization. From the evaluation results, the optimized models are almost 4 times smaller as compared to the original models. The best trade-off between speed and accuracy can be achieved with a 27.1&#x0025; improvement in speed and 2&#x0025; degradation in accuracy.",https://ieeexplore.ieee.org/document/9687720/,2021 IEEE Asia Pacific Conference on Circuit and Systems (APCCAS),22-26 Nov. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICASSP.2005.1416273,Soft SVM and its application to video object extraction,IEEE,Conferences,"Support vector machines (SVM) are state-of-the-art learning machines and have found a great deal of success in a wide range of applications. In the framework of SVM, each sample belongs to either one class or the other. This requirement, however, makes it difficult to apply SVM to the applications where the data exhibit partial or unclear class memberships. To address this problem, this paper reformulates the standard SVM to be a new learning machine that is capable of dealing with binary (or hard) as well as real-valued (or soft) class memberships. The new machine, which is named soft SVM (S-SVM), has been integrated into a classification-based video object extraction approach, and the experimental results demonstrate the effectiveness of the new approach.",https://ieeexplore.ieee.org/document/1416273/,"Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005.",23-23 March 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSECS52883.2021.00010,SonoNet: Despeckling of Medical Ultrasound Scans Using Convolutional Neural Network Architecture,IEEE,Conferences,"This paper proposes a deep learning framework using convolutional neural networks (CNNs), for the despeckling in ultrasound scans. The SonoNet model proposed in this paper consists of seven layers deep net along with activation function and batch normalization. Training of the network is carried out by utilizing synthetically modeled ultrasound frames with varying speckle content. The network uses the assumption of multiplicative noise to adopt a division approach for filtering speckle content from input frames. Finally, edge features are extracted using the Canny operator and enhanced using a twodimensional 3×3 convolutional mask. Thus, the proposed CNN method effectively accomplishes speckle elimination and visual feature preservation without inducing much blurring effects. The subjective evaluation by four subject matter experts and experimental analysis using three image quality metrics indicate that the proposed system gives a superior performance as compared to the state-of-the-art methods in the field, and therefore could find applications designed for speckle suppression in ultrasound imaging systems and real-time ultrasound video segmentation and classification algorithms.",https://ieeexplore.ieee.org/document/9537031/,2021 International Conference on Software Engineering & Computer Systems and 4th International Conference on Computational Science and Information Management (ICSECS-ICOCSIM),24-26 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICME51207.2021.9428121,Spatial-Temporal Correlation Learning for Real-Time Video Deinterlacing,IEEE,Conferences,"Deinterlacing is a classical issue in video processing area, which aims to generate the progressive video from the interlaced instance. Although numerous algorithms have been proposed in the past decades, their performances are still not satisfactory from both quality of experience and processing efficiency. This paper focuses on the spatial-temporal correlation in the given frame, and design a network for recovering the missing field. Intra-frame motion compensation is considered between the given fields for detail refinement. Furthermore, we address the inherent correlations among image features with channel attention for better exploration. Extensive experimental results on different video sequences show that our method outperforms state-of-the-art methods according to both objective and subjective evaluations satisfying the real-time requirement.",https://ieeexplore.ieee.org/document/9428121/,2021 IEEE International Conference on Multimedia and Expo (ICME),5-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/DELCON54057.2022.9753157,Speech Enhancement Using Open-Unmix Music Source Separation Architecture,IEEE,Conferences,"Speech enhancement using sound source separation finds usefulness in prevention of degradation of the quality of human speech on voice call/video call, voice assistant commands due to background noises. Open-Unmix is popular architecture used by researchers for music source separation. This paper proposes a modified implementation of the Open-Unmix model, to attain the goal of sound source separation for speech enhancement. This paper explains in detail, the custom dataset collection and pre-processing methods to generate the training data. The improved Open-Unmix model is a deep neural network that estimates separation masks in the short-time Fourier transform domain. It is based on three-layer bidirectional long short-term memory (Bi-LSTM) with completely connected encoding and decoding layers. The source to artifact ratio (SAR), scale invariant source to distortion ratio (SI-SDR), short time objective intelligibility (STOI), and source to distortion ratio (SDR) are all popular subjective measures for blind source separation of audio signals used in our overall assessment. Experimental results show that our proposed method can separate noise background signals from human speech and provide enhancement in real environment.",https://ieeexplore.ieee.org/document/9753157/,2022 IEEE Delhi Section Conference (DELCON),11-13 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ACMI53878.2021.9528185,Speed Bump & Pothole Detection with Single Shot MultiBox Detector Algorithm & Speed Control for Autonomous Vehicle,IEEE,Conferences,"The development of self-driving cars has always been an extensive research field for the automobile industry. To make a capable self-driving car, many challenges need to be resolved. Detection of the road condition is one of them. This paper focuses on a particular part-detection of speed bumps and potholes using a camera and analyzing the video feed with the help of artificial intelligence. To solve this problem a popular and lightweight algorithm, SSD (Single Shot Multibox Detector) is used. This is an optimal choice because of being lightweight and also accurate enough to run on mobile devices and to use in real-life situations. For detecting speed bumps and potholes, a dataset has been created based on the road structure of Bangladesh as the main priority of this system is to work on the local environment. Raspberry Pi has been used as the main processing unit because of being small but powerful. A warning system has been implemented so that it can warn the onboard driver about the upcoming pothole or speed bump. This system can also send a signal to the speed controller unit of the car to reduce the speed on detection to avoid accidents or damages to the car. The speed control unit is a microcontroller-based system that uses an ATmega328 microcontroller and L298 motor driver. This paper summarizes the combination of an artificial intelligence-based detection system injunction with a microcontroller-based speed control system in a cost-effective way that can be used in building self-driving cars.",https://ieeexplore.ieee.org/document/9528185/,"2021 International Conference on Automation, Control and Mechatronics for Industry 4.0 (ACMI)",8-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2019.00022,State Summarization of Video Streams for Spatiotemporal Query Matching in Complex Event Processing,IEEE,Conferences,"Modelling complex events in unstructured data like videos not only requires detecting objects but also the spatiotemporal relationships among objects. Complex Event Processing (CEP) systems discretize continuous streams into fixed batches using windows and apply operators over these batches to detect patterns in real-time. To this end, we apply CEP techniques over video streams to identify spatiotemporal patterns by capturing window state. This work introduces a novel problem where an input video stream is converted to a stream of graphs which are aggregated to a single graph over a given state. Incoming video frames are converted to a timestamped Video Event Knowledge Graph (VEKG) [1] that maps objects to nodes and captures spatiotemporal relationships among object nodes. Objects coexist across multiple frames which leads to the creation of redundant nodes and edges at different time instances that results in high memory usage. There is a need for expressive and storage efficient graph model which can summarize graph streams in a single view. We propose Event Aggregated Graph (EAG), a summarized graph representation of VEKG streams over a given state. EAG captures different spatiotemporal relationships among objects using an Event Adjacency Matrix without replicating the nodes and edges across time instances. These enable the CEP system to process multiple continuous queries and perform frequent spatiotemporal pattern matching computations over a single summarised graph. Initial experiments show EAG takes 68.35% and 28.9% less space compared to baseline and state of the art graph summarization method respectively. EAG takes 5X less search time to detect pattern as compare to VEKG stream.",https://ieeexplore.ieee.org/document/8999043/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM41043.2020.9155411,Stick: A Harmonious Fusion of Buffer-based and Learning-based Approach for Adaptive Streaming,IEEE,Conferences,"Off-the-shelf buffer-based approaches leverage a simple yet effective buffer-bound to control the adaptive bitrate (ABR) streaming system. Nevertheless, such approaches in standard parameters fail to always provide high quality of experience (QoE) video streaming services under all considered network conditions. Meanwhile, state-of-the-art learning-based ABR approach Pensieve outperforms existing schemes but is impractical to deploy. Therefore, how to harmoniously fuse the buffer-based and learning-based approach has become a key challenge for further enhancing ABR methods. In this paper, we propose Stick, an ABR algorithm that fuses the deep learning method and traditional buffer-based method. Stick utilizes the deep reinforcement learning (DRL) method to train the neural network, which outputs the buffer-bound to control the buffer-based approach for maximizing the QoE metric with different parameters. Trace-driven emulation illustrates that Stick betters Pensieve by 3.5% - 9.41% with an overhead reduction of 88%. Moreover, aiming to further reduce the computational costs while preserving the performances, we propose Trigger, a light-weighted neural network that determines whether the buffer-bound should be adjusted. Experimental results show that Stick+Trigger rivals or outperforms existing schemes in average QoE by 1.7%-28%, and significantly reduces the Stick&#x2019;s computational overhead by 24%-61%. Meanwhile, we show that Trigger also helps other ABR schemes mitigate the overhead. Extensive results on real-world evaluation demonstrate the superiority of Stick over existing state-of-the-art approaches.",https://ieeexplore.ieee.org/document/9155411/,IEEE INFOCOM 2020 - IEEE Conference on Computer Communications,6-9 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICOIACT46704.2019.8938457,Strawberry Ripeness Classification System Based On Skin Tone Color using Multi-Class Support Vector Machine,IEEE,Conferences,"This research aims to build an automatic sorting system for strawberry ripeness into three categories: unripe, partially ripe, and ripe. Manual fruit sorting has many weaknesses and limitations. One of the disadvantages is human error in the sorting process. Therefore, the implementation of artificial intelligence as replacement of human worker can mitigate the problem. Fruit ripeness is identified based on color characteristic, which is the Red, Green, Blue (RGB) value of the object. Multi-Class Support Vector Machine (SVM) with Radial Basis Function (RBF) kernel function is implemented to classify the ripeness. The data was taken using the Logitech C920 web camera which then divided into training and testing data video. In this research, prototype of strawberries sorting is built with real-time video which is never been considered in previous researches. Training data consists of 70 unripe strawberries, 70 partially ripe strawberries, and 70 ripe strawberries. Meanwhile testing data comprises of 30 unripe strawberries, 30 partially ripe strawberries, and 30 ripe strawberries. The result shows that the strawberry ripeness classification system using Multi-class SVM with RBF kernel function produces up to 85.64% accuracy, where the parameters are C = 7 and gamma (γ) = 10-2.",https://ieeexplore.ieee.org/document/8938457/,2019 International Conference on Information and Communications Technology (ICOIACT),24-25 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDARW.2019.10036,StreetOCRCorrect: An Interactive Framework for OCR Corrections in Chaotic Indian Street Videos,IEEE,Conferences,"Obtaining a high-quality OCR output in smart cities, with human-in-the-loop, is an interesting problem for surveillance and other similar applications. Achieving high accuracy while reading license plates in the real world videos is cumbersome due to complexities like multiple vehicles, high-density traffic in spatial and temporal domains, varying camera angles and illumination, occlusions and multiple resolutions. We present a modular framework for OCR corrections in the chaotic Indian traffic videos that especially involve complex license plate patterns. Such patterns are obtained from a state-of-the-art deep learning model trained on video frames. Since such a model reads the text from videos (instead of images), we incorporate multi-frame consensus for generating suggestions in our framework. To ease the correction process, our human-interactive framework first breaks down the multi-vehicle videos into multiple clips, each containing a single vehicle from the video using an object detector and a tracker. Our framework then provides suggestions for an individual vehicle using multi-frame consensus. Our framework then selectively presents these extracted clips to the user to verify/correct the predictions with minimal human efforts via interactive suggestions. Such high-quality output can be used to continuously update a large database for surveillance and can be further used to improve the accuracy of deep models in the complex real-world scenarios.",https://ieeexplore.ieee.org/document/8893132/,2019 International Conference on Document Analysis and Recognition Workshops (ICDARW),22-25 Sept. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE46568.2020.9042995,Stroke Signs Detection System by SNS Agency Robot,IEEE,Conferences,"This paper proposes a system which implements the Cincinnati Prehospital Stroke Scale (CPSS), the widely used screening method for the initial symptoms of a stroke, in a communication robot. AI on cloud analyses an acquired video through a conversation with the robot in real time and automatically determines the abnormalities. The judgement result is informed to his/her families by SNS. This study implemented two of the three CPSS scales such as “Arms” and “Speech”, we confirmed that the system enables to acquire, analyze and notify the information in real time.",https://ieeexplore.ieee.org/document/9042995/,2020 IEEE International Conference on Consumer Electronics (ICCE),4-6 Jan. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICETA48886.2019.9040157,Students’ Attendance Monitoring through the Face Recognition,IEEE,Conferences,"This paper describes experimental software solution for monitoring of students' attendance through the face detection and recognition in the video. The analysis of existing face detection and recognition algorithms was carried out, and the design of the system solution is described, implementing the previously analyzed algorithms. The Viola-Jones algorithm and HOG are used to detect the face in the video. To recognize an identity of a student, the convolutional neural network (NN) is utilized. The system was tested in the real university environment on selected courses. Once the face recognition is finished, the attendance list of present people may be generated. The system then provides additional visual verification of recognized faces to the lecturer (administrator).",https://ieeexplore.ieee.org/document/9040157/,2019 17th International Conference on Emerging eLearning Technologies and Applications (ICETA),21-22 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2007.4370173,Study on Crop Image Feature Extraction of Vehicle-Based Estimation System on Large Scale Crop Acreage,IEEE,Conferences,"Vehicle-based estimation system on large scale crop acreage, which was equipped with GPS receivers, GIS software and a video camera on a off-road vehicle, can capture the images of video and calculate the crop acreage proportion based on matching between GPS information and image recognition. The system provides the credible data for government to make decision and provides technological method. In order to enhance the real-time and reliable character of the system, namely improve the precision of image recognition and estimation result, the grid algorithm of the crop image feature extraction was proposed. The principle of the grid algorithm was that the every crop image was segmented to 16 grids averagely and the four corner grids were regarded as the beginning area during feature extraction and crop recognition. Whether or not to continue extracting feature and recognizing in other grids as well as the recognizing sequence was determined according to the result of crop recognition in the four beginning areas. The texture feature, the shape feature and the color feature were extracted in terms of the particularity of the crop recognition, then the different features or the feature combination were used in order to recognize the crop. The detailed analysis methods of different crops and different cultivating condition were discussed in this paper.",https://ieeexplore.ieee.org/document/4370173/,2007 International Conference on Machine Learning and Cybernetics,19-22 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IMCCC.2012.184,Study on Embedded Software Applied to Vehicle Intelligent Monitoring System,IEEE,Conferences,"Combined GPRS network data transmission, the advanced GPS satellite positioning and other key technologies, an embedded vehicle intelligent monitoring system could be achieved. This paper introduced the development of main device driver, application program design of embedded video capture card, GPS, and GPRS. This build of embedded software system will help to achieve the intelligent vehicle monitoring system and improve the degree of networking. With the embedded µC/OS-II real-time multi-threading technology to solve the problem of multi-task parallel processing in system, in order to ensure system stability, reliability and flexibility, make the system more practical application value[1].",https://ieeexplore.ieee.org/document/6429019/,"2012 Second International Conference on Instrumentation, Measurement, Computer, Communication and Control",8-10 Dec. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IROS51168.2021.9636743,Success Weighted by Completion Time: A Dynamics-Aware Evaluation Criteria for Embodied Navigation,IEEE,Conferences,"We present Success weighted by Completion Time (SCT), a new metric for evaluating navigation performance for mobile robots. Several related works on navigation have used Success weighted by Path Length (SPL) as the primary method of evaluating the path an agent makes to a goal location, but SPL is limited in its ability to properly evaluate agents with complex dynamics. In contrast, SCT explicitly takes the agent’s dynamics model into consideration, and aims to accurately capture how well the agent has approximated the fastest navigation behavior afforded by its dynamics. While several embodied navigation works use point-turn dynamics, we focus on unicycle-cart dynamics for our agent, which better exempli-fies the dynamics model of popular mobile robotics platforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present RRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest collision-free path and completion time from a starting pose to a goal location in an environment containing obstacles. We experiment with deep reinforcement learning and reward shaping to train and compare the navigation performance of agents with different dynamics models. In evaluating these agents, we show that in contrast to SPL, SCT is able to capture the advantages in navigation speed a unicycle model has over a simpler point-turn model of dynamics. Lastly, we show that we can successfully deploy our trained models and algorithms outside of simulation in the real world. We embody our agents in a real robot to navigate an apartment, and show that they can generalize in a zero-shot manner. A video summary is available here: https://youtu.be/QOQ56XVIYVE",https://ieeexplore.ieee.org/document/9636743/,2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/MECON53876.2022.9752182,Surface traffic monitoring using OpenCV for various weather conditions through enhanced spatial correlation method,IEEE,Conferences,"It is a big challenge in the present urban traffic management system to develop a vehicle detection and traffic surveillance monitoring system through video capture using real time data capture model whereby machine learning may be used to recognise and categorise things. The idea is to analyse the motion and approach of cars acquired from CCTV cameras footage installed on the roads in different weather conditions and create an interactive software which should be able to gather and update important statistics like vehicle type classification and counts, lane usage details, for better regulation and control with ease. The core of this working model is to enhance vehicle detection and counting module which is implemented by analysing consecutive frames of video using frame differencing technique. The government has tried the best to ensure swift movement by implementing the broad usage of intelligent traffic controller system design. According to the WHO, around 2 million road traffic accidents result in deaths, with some nations devoting 2&#x0025; of their GDP to road traffic accidents. Opportunities in the intelligent transport system market is growing at a global compound annual growth rate (CAGR) from 2019 to 2025 as an effective tool for road safety.",https://ieeexplore.ieee.org/document/9752182/,2022 International Mobile and Embedded Technology Conference (MECON),10-11 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECAI.2013.6636188,Surveillance system using IP camera and face-detection algorithm,IEEE,Conferences,"This paper presents an efficient video surveillance system which consists of a network camera and an algorithm for automatically detection of the human faces in the monitoring area via real time video contents analysis. The main contribution of this research consists in a software application which is able to process the images received from the camera in order to detect human faces and trigger the process of saving the live stream as a video file. The algorithm for face-detection is based on the integral image (summed area table) - a model which allows the calculation of the sum of all pixels values from any rectangular area in the original image by doing only four operations of addition or subtraction. In addition, the application also includes a file containing data about needed tests in the detection algorithm by analyzing multiple images as templates. The application is written in C# language and experimental results are presented for images with different sizes and backgrounds.",https://ieeexplore.ieee.org/document/6636188/,"Proceedings of the International Conference on ELECTRONICS, COMPUTERS and ARTIFICIAL INTELLIGENCE - ECAI-2013",27-29 June 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACCS51430.2021.9441670,"Synchronous System for Driver Drowsiness Detection Using Convolutional Neural Network, Computer Vision and Android Technology",IEEE,Conferences,"One of the major reasons behind car accidents is the drowsy nature acquired by a driver while driving any vehicle. Owing to the ongoing scenario, in this project, we aim to develop a real time driver drowsiness detection system in order to detect the drivers' fatigue status, such as dozing, flickering of eye lids and time span of eye closure without having to equip their bodies with devices. The objective of this project is to build a drowsiness detection system that will detect that a person's eyes are closed for a few seconds. This system will alert the driver when drowsiness is detected. This approach is based on the Convolutional Neural Network that can be implemented on Android applications with high accuracy. Apart from CNN, computer vision also plays a major role to detect the drowsiness pattern of the driver. Cloud architecture has also proved to be beneficial in case of capturing and analyzing real time video streams.",https://ieeexplore.ieee.org/document/9441670/,2021 7th International Conference on Advanced Computing and Communication Systems (ICACCS),19-20 March 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CISTI.2015.7170450,System based on machine vision for translation of fingerspelling alphabet to latin alphabet,IEEE,Conferences,"On the nowadays society exist a lot of communication problems, particularly when the persons has sensory disabilities as deafness or blindness. This problem take place at the moment of interpreting the sign language. The present paper shows the development of a current research project that integrates an intelligent system in the recognition of images and its reproduction in hardware interpretations ends. For those purposes, a system of image acquisition with a webcam and an interface was implemented in Matlab, through which the video was displayed in real time, the image of the point gained, together with the translation of Colombian sign language. This system was trained to recognize 4-letter alphabet, obtaining an average error of 2%, concluding that such application was effective for translating letters acquired both the right hand or the left; similarly concluded that the type of radial neural network proves to be very useful for this type of operation and the higher classification have this training, the results will be more accurate cast. Finally it is important to note that the system integrates hardware with Arduino system that displays real-time translation in this system was trained to recognize 4 letters of the alphabet, giving an average error of 2%, the same way the PNN neural network proves to be very useful for this type of sorting operations.",https://ieeexplore.ieee.org/document/7170450/,2015 10th Iberian Conference on Information Systems and Technologies (CISTI),17-20 June 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1145/3524843.3528094,TD Classifier: Automatic Identification of Java Classes with High Technical Debt,IEEE,Conferences,"To date, the identification and quantification of Technical Debt (TD) rely heavily on a few sophisticated tools that check for violations of certain predefined rules, usually through static analysis. Different tools result in divergent TD estimates calling into question the reliability of findings derived by a single tool. To alleviate this issue, we present a tool that employs machine learning on a dataset built upon the convergence of three widely-adopted TD Assessment tools to automatically assess the class-level TD for any arbitrary Java project. The proposed tool is able to classify software classes as high-TD or not, by synthesizing source code and repository ac-tivity information retrieved by employing four popular open source analyzers. The classification results are combined with proper vi-sualization techniques, to enable the identification of classes that are more likely to be problematic. To demonstrate the proposed tool and evaluate its usefulness, a case study is conducted based on a real-world open-source software project. The proposed tool is expected to facilitate TD management activities and enable fur-ther experimentation through its use in an academic or industrial setting. Video: https://youtu.be/umgXU8u7lIA Running Instance: http://160.40.52.130:3000/tdclassifier Source Code: https://gitlab.seis.iti.gr/root/td-classifier.git",https://ieeexplore.ieee.org/document/9804507/,2022 IEEE/ACM International Conference on Technical Debt (TechDebt),22-23 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WACV45572.2020.9093437,TKD: Temporal Knowledge Distillation for Active Perception,IEEE,Conferences,"Deep neural network-based methods have been proved to achieve outstanding performance on object detection and classification tasks. Despite the significant performance improvement using the deep structures, they still require prohibitive runtime to process images and maintain the highest possible performance for real-time applications. Observing the phenomenon that human visual system (HVS) relies heavily on the temporal dependencies among frames from the visual input to conduct recognition efficiently, we propose a novel framework dubbed as TKD: temporal knowledge distillation. This framework distills the temporal knowledge from a heavy neural network-based model over selected video frames (the perception of the moments) to a light-weight model. To enable the distillation, we put forward two novel procedures: 1) a Long-short Term Memory (LSTM)-based keyframe selection method; and 2) a novel teacher-bounded loss design. To validate our approach, we conduct comprehensive empirical evaluations using different object detection methods over multiple datasets including Youtube-Objects and Hollywood scene dataset. Our results show consistent improvement in accuracy-speed trad- offs for object detection over the frames of the dynamic scene, compared to other modern object recognition methods. It can maintain the desired accuracy with the throughput of around 220 images per second. Implementation: https://github.com/mfarhadi/TKD-Cloud.",https://ieeexplore.ieee.org/document/9093437/,2020 IEEE Winter Conference on Applications of Computer Vision (WACV),1-5 March 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVIDL51233.2020.00-44,Teaching Quality Assessment and Feasibility Analysis of Surgical Nursing Based on Network Teaching Software,IEEE,Conferences,"Objective: To explore the application effects of the interactive online teaching mode in nursing teaching. Methods: The interns at our hospital were selected as investigation subjects and randomly divided into an experimental group and a control group. In the experimental group, the video interactive and other communication tools were used to assist teaching based on fundamental online teaching. The teaching effectiveness was assessed by questionnaires and test scores. Results: The overall recognition of online teaching in the experimental group was higher than that in the control group. The teaching vividness, synchronous guidance, sharing of teaching resources, real-time confusion clearing and concentration of students in the experimental group were all superior to those in the control group. The test scores in the experimental group were significantly higher than those in the control group. Conclusions: The online teaching mode based on video interaction is vivid and interesting, conducive to memory, synchronous guidance, real-time confusion clearing, and sharing of teaching resources, which can improve the academic performance of students.",https://ieeexplore.ieee.org/document/9270477/,"2020 International Conference on Computer Vision, Image and Deep Learning (CVIDL)",10-12 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSME46990.2020.00098,Teddy: Automatic Recommendation of Pythonic Idiom Usage For Pull-Based Software Projects,IEEE,Conferences,"Pythonic code is idiomatic code that follows guiding principles and practices within the Python community. Offering performance and readability benefits, Pythonic code is claimed to be widely adopted by experienced Python developers, but can be a learning curve to novice programmers. To aid with Pythonic learning, we create an automated tool, called Teddy, that can help checking the Pythonic idiom usage. The tool offers a prevention mode with Just-In-Time analysis to recommend the use of Pythonic idiom during code review and a detection mode with historical analysis to run a thorough scan of idiomatic and non-idiomatic code. In this paper, we first describe our tool and an evaluation of its performance. Furthermore, we present a case study that demonstrates how to use Teddy in a real-life scenario on an Open Source project. An evaluation shows that Teddy has high precision for detecting Pythonic idiom and non-Pythonic code. Using interactive visualizations, we demonstrate how novice programmers can navigate and identify Pythonic idiom and non-Pythonic code in their projects. Our video demo with the full interactive visualizations is available at https://youtu.be/vOCQReSvBxA.",https://ieeexplore.ieee.org/document/9240709/,2020 IEEE International Conference on Software Maintenance and Evolution (ICSME),28 Sept.-2 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMSNETS53615.2022.9668498,Tele-driving an electric vehicle over a private LTE network,IEEE,Conferences,"We demonstrate tele-driving operation for an electric vehicle capable of stopping itself in case of system failure over a captive LTE network deployed in a university campus. Our electronically controlled vehicle is driven remotely by an operator from a control room which receives the multi-camera real-time video feed from the vehicle over this network. Our primary contribution includes the responsive emergency braking mechanism for the vehicle, modular vehicle design based on CAN bus, low latency LTE MAC scheduler design, and modifications to popular video tool, FFMPEG to support low latency real time video streaming. Our demonstration shows complete integration of the different components, i.e., the vehicle, the LTE network and the remote driving application. Another salient feature of our system is the O-RAN compliant RAN awareness module and KPI (Key Performance Indicator) application which enables real-time network performance monitoring.",https://ieeexplore.ieee.org/document/9668498/,2022 14th International Conference on COMmunication Systems & NETworkS (COMSNETS),4-8 Jan. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/VCIP53242.2021.9675330,Telemoji: A video chat with automated recognition of facial expressions,IEEE,Conferences,"Autism spectrum disorder (ASD) is frequently ac-companied by impairment in emotional expression recognition, and therefore individuals with ASD may find it hard to interpret emotions and interact. Inspired by this fact, we developed a web-based video chat to assist people with ASD, both for real-time recognition of facial emotions and for practicing. This real-time application detects the speaker's face in a video stream and classifies the expressed emotion into one of the seven categories: neutral, surprise, happy, angry, disgust, fear, and sad. The classification is then displayed as the text label below the speaker's face. We developed this application as a part of the undergraduate project for the B.Sc. degree in Software Engineering. Its development and testing were made with the cooperation of the local society for children and adults with autism. The application has been released for unrestricted use on https://telemojii.herokuapp.com/. The demo is available at http://www.filedropper.com/telemojishortdemoblur.",https://ieeexplore.ieee.org/document/9675330/,2021 International Conference on Visual Communications and Image Processing (VCIP),5-8 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IHCI.2012.6481798,Temporal correlation and probabilistic prediction based face detection framework in real time environment,IEEE,Conferences,"Conventionally, the object detection algorithm proposed by Viola Jones (using Haar-like features, Integral image and AdaBoost algorithm) is implemented in majority of the face detection applications. In this article, an improvement to the application of Viola Jones algorithm in real time environment is presented by exploiting the analogy between video motion estimation and continuous object detection. Using the temporal correlation between successive frames of a real time input, various modifications improving the robustness and computational complexity of face detection are proposed. The proposed method focuses on reducing the search area for face detection on the basis of probabilistic prediction. In addition, approximation of minimum face size renders an improved performance. The modified Face Detection Framework (FDF) is applied in two scenarios, canned video sequences from public databases and real time inputs from a low resolution camera, yielding improved results in both the cases.",https://ieeexplore.ieee.org/document/6481798/,2012 4th International Conference on Intelligent Human Computer Interaction (IHCI),27-29 Dec. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2002.1035109,Test-bed board for 16/spl times/64 stereo vision CNN chip,IEEE,Conferences,"The implementation of an artificial vision algorithm in real time is really attractive in such an application as the field of environment sensing. The SVCNN (stereo vision cellular neural network) chip is an analogue circuit able to compute in real time the Disparity Map from a couple of images by using a stereo visual system algorithm. A ""test-bed"" board for the 16/spl times/64 SVCNN chip is presented in this paper. This board is composed of an analogue processing core implemented by two 16/spl times/64 SVCNN chips together with a digital high performance pre-processing unit and a video grabbing section.",https://ieeexplore.ieee.org/document/1035109/,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,24-24 July 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SIBCON.2017.7998591,Text detection algorithm on real scenes images and videos on the base of discrete cosine transform and convolutional neural network,IEEE,Conferences,In this work we present algorithms which are applied in such task as text recognition on images and video. Proposed algorithm is based on the combination of discrete cosine transform and convolutional neural networks. Description of the applying features of discrete cosine transform for text detection is provided. We list the main advantages and disadvantages of CNN and DCT combination. Also in this article we are going to consider methods of convolution neural networks for the task of text recognition.,https://ieeexplore.ieee.org/document/7998591/,2017 International Siberian Conference on Control and Communications (SIBCON),29-30 June 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPRW50498.2020.00321,The 4th AI City Challenge,IEEE,Conferences,"The AI City Challenge was created to accelerate intelligent video analysis that helps make cities smarter and safer. Transportation is one of the largest segments that can benefit from actionable insights derived from data captured by sensors, where computer vision and deep learning have shown promise in achieving large-scale practical deployment. The 4th annual edition of the AI City Challenge has attracted 315 participating teams across 37 countries, who leverage city-scale real traffic data and high-quality synthetic data to compete in four challenge tracks. Track 1 addressed video-based automatic vehicle counting, where the evaluation is conducted on both algorithmic effectiveness and computational efficiency. Track 2 addressed city-scale vehicle re-identification with augmented synthetic data to substantially increase the training set for the task. Track 3 addressed city-scale multi-target multi-camera vehicle tracking. Track 4 addressed traffic anomaly detection. The evaluation system shows two leader boards, in which a general leader board shows all submitted results, and a public leader board shows results limited to our contest participation rules, that teams are not allowed to use external data in their work. The general leader board shows results more close to real-world situations where annotated data are limited. Our results show promise that AI technology can enable smarter and safer transportation systems.",https://ieeexplore.ieee.org/document/9150577/,2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),14-19 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/FSKD.2012.6234222,The Design of Intelligent Monitoring System Based on DaVinci Platform,IEEE,Conferences,"In this paper, an embedded video monitoring system for intelligent control is presented. This presented system uses DaVinci-based high-speed TMS320DM6467 chip as hardware component and uses Embedded Linux Operating System as software component, in addition, it takes target detection and tracking algorithms as its core. So it can not only meet the requirements of real-time video processing and heavy computation by the ARM+DSP dual-core processors, but also meet the requirements of the intellectualization of video monitoring by the target detection and tracking algorithms.",https://ieeexplore.ieee.org/document/6234222/,2012 9th International Conference on Fuzzy Systems and Knowledge Discovery,29-31 May 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/PCCC.2018.8710767,The Frame Latency of Personalized Livestreaming Can Be Significantly Slowed Down by WiFi,IEEE,Conferences,"The popular personalized livestreaming (PL) in China, arguably the largest PL market in the world, is more monetized than PL in US and hence demands much lower interactive latencies to ensure a good quality of user experience. However, our pilot experiment shows that the video frame latency, dominant component of PL's interactive latency, can be significantly slowed down by WiFi, the primary Internet access method for PL. Understanding and further improving the frame latency over WiFi, however, have difficulties in 1) measuring end-to-end latency; 2) parsing encrypted PL's traffic and 3) modeling complex relationships between WiFi radio factors and the latency. To tackle these challenges, we design and prototype Latency Doctor (LTDr), a practical system which aims to model and optimize PL's video frame latency over WiFi. We deploy LTDr in our campus and obtain several key observations based on 13.9M video frames extracted from 12K individual views on three leading PLs in China. We observe that 40% frame latencies over WiFi hop are more than 30ms, and channel utilization should be less than 64% for low latency. Then we build a predictive model based on the dataset using the machine learning methodologies. Two real cases show that the median frame latencies are decreased by LTDr from 130ms to 22ms, and 50ms to 12ms respectively over WiFi networks.",https://ieeexplore.ieee.org/document/8710767/,2018 IEEE 37th International Performance Computing and Communications Conference (IPCCC),17-19 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCCE46687.2019.9015300,The Implementation of Traffic Analytics Using Deep Learning and Big Data Technology with Garuda Smart City Framework,IEEE,Conferences,"Nowadays the population in the city grow rapidly [1]. It may cause many problems and those must be tackled with smart city solution because manual policy cannot handle that situation due to limited physical supporting tools [1] [2]. One of the problems is traffic congestion and its management. Moreover, other smart city initiatives such as traffic control and violence detection are applied to solve that issue in order to improve quality of life [3]. In this paper, a cloud-based with big data and video analytics with deep learning are proposed in the context of efficiency where accuracy and speed of processing and transmissions play a critical role into access policy and control with our integration platform based on Garuda Smart City Framework (GSCF) [1] [4].",https://ieeexplore.ieee.org/document/9015300/,2019 IEEE 8th Global Conference on Consumer Electronics (GCCE),15-18 Oct. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/UIC-ATC.2017.8397673,The NVIDIA AI City Challenge,IEEE,Conferences,"Web image analysis has witnessed an AI renaissance. The ILSVRC benchmark has been instrumental in providing a corpus and standardized evaluation. The NVIDIA AI City Challenge is envisioned to provide similar impetus to the analysis of image and video data that helps make cities smarter and safer. In its first year, this Challenge has focused on traffic video data. While millions of traffic video cameras around the world capture data, albeit low-quality, very little automated analysis and value creation results. Lack of labeled data, and trained models that can be deployed at the edge of the city fabric, ensure that most traffic video data goes through little or no automated analysis. Real-time and batch analysis of this data can provide vital breakthroughs in real-time traffic management as well as pedestrian safety. The NVIDIA AI City Challenge brought together 29 teams from universities in 4 continents to collaboratively annotate a 125 hour data set and then compete on detection, localization and classification tasks as well as traffic and safety application analytics tasks. The result is the largest high quality annotated data set, a set of models trained using NVIDIA AI City Edge to Cloud platform and ready to be deployed at the edge solving traffic and safety problems for cities worldwide.",https://ieeexplore.ieee.org/document/8397673/,"2017 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computed, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",4-8 Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNC.2008.315,The Research and Application of Human Detection Based on Support Vector Machine Using in Intelligent Video Surveillance System,IEEE,Conferences,"This paper presented a special design and implementation of human detection based on SVM (support vector machine) and this method is used in intelligent video surveillance system. In order to simplify the design of the SVM classifier and improve efficiency of machine learning, both a grid vector representation and a center radiating vector representation are proposed to abstract features of the object. The sample data is obtained through processing and analysis including human and no-human which forms the training input to SVM. Finally, we used the trained recognizer to identify whether there is somebody broken into the object region. If there is, the automatic warning device gives the alarm, which guarantees a real-time surveillance.",https://ieeexplore.ieee.org/document/4666973/,2008 Fourth International Conference on Natural Computation,18-20 Oct. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ECBS.2004.1316727,The networked photo-enforcement and traffic monitoring system Unicam,IEEE,Conferences,"The presented system (Unicam) offers a complex state-of-the-art machine vision equipment and technology to provide automated video image vehicle detection devices dedicated for traffic monitoring applications. The system provides real time video image capturing, digital signal processing, compression, storage, and transmission over communication interfaces. It uses proprietary artificial intelligence algorithms and special image processing modules to achieve highly accurate vehicles detection. According to the users' needs, the system can be used for detection of red-light violations at road intersections, speed measurement, traffic data collection, video recording, or surveillance. Yet another possible application of the system is surveys based on license plate recognition for transportation engineers, stolen car searching, or toll-tag data collection. The system functionality has been improved by coupling camera sensors with specialized real-time processing units and adding networking capability. Implementation of video detection algorithms, hardware design units, and networking features are also discussed.",https://ieeexplore.ieee.org/document/1316727/,"Proceedings. 11th IEEE International Conference and Workshop on the Engineering of Computer-Based Systems, 2004.",27-27 May 2004,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2010.5580745,The study and implementation of real-time face recognition and tracking system,IEEE,Conferences,"During the past several years, face recognition in video has received significant attention. For the video monitoring and artificial vision, real time face recognition has very important meaning. The current method is still very susceptible to the illumination condition, non-real time and very common to fail to track the target face especially when partly covered or moving fast. In this paper, we propose to use AdaBoost Cascade for face detection and then in order to recognize the candidate faces, they will be analyzed by the hybrid Wavelet and LDA method. After that, Mean shift will be invoked to track the face. The implementation shows that the algorithm has quite good performance in terms of real-time and the tracking procedure is triggered accurately.",https://ieeexplore.ieee.org/document/5580745/,2010 International Conference on Machine Learning and Cybernetics,11-14 July 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMSEC.2011.6010964,The video surveillance system based on DSP and wireless network,IEEE,Conferences,"This paper presents a video surveillance system as the main chip based on DSP, and RTL8019AS as ethernet interface chip. Realization of upper computer software system based on the socket class of visual C, and the local computers is composed of the camera, DSP control unit, CPLD interface, ethernet interface, wireless router, etc. the function modules of system. This paper introduces the interface realization of different functional module and software flowchart. Proved by the experiment, the video image is flowing in this system, and the system has realized the effective PMP data transmission in IP networks. So the system has good real-time, practicability and accuracy.",https://ieeexplore.ieee.org/document/6010964/,"2011 2nd International Conference on Artificial Intelligence, Management Science and Electronic Commerce (AIMSEC)",8-10 Aug. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCD50377.2020.00060,Throughput-Oriented Spatio-Temporal Optimization in Approximate High-Level Synthesis,IEEE,Conferences,"Current and emerging systems for high-throughput applications, such as machine learning, cloud computing, and real-time video encoding demand real-time processing computations, heavily constrained by latency and power requirements. To deal with the increasing computational complexity, designers may resort to approximate accelerators for error-resilient compute-intensive kernels to meet such requirements with acceptable deviation from the exact implementation. However, since time-to-market is crucial when dealing with evolving applications, technologies, and standards, hand-crafting approximate accelerators may impose prohibitive development time and cost overheads. In this scenario, approximate High-Level Synthesis (HLS) methodologies have been proposed to deal with the complexity of exploring approximation techniques. Nevertheless, current tools are not suitable for exploring throughput optimizations, being instead constrained to perform specific improvements on area, power, and performance. In this work, we propose the use of HLS to generate Pareto-optimal accelerators for throughput-constrained applications. Particularly, we present a throughput-oriented approximate HLS methodology that explores both delay and area optimizations to increase the reuse over time and parallelism of such accelerators. Results show that our method is able to improve throughput by up to 80 % with no additional area costs or to sustain the same throughput of the exact design with about 45 % less area while introducing manageable error for most applications. Moreover, our method can attain throughput improvements of up to 18% when compared with recent works focusing only on performance or area optimizations, with no additional costs.",https://ieeexplore.ieee.org/document/9283566/,2020 IEEE 38th International Conference on Computer Design (ICCD),18-21 Oct. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLC.2009.5212738,Time series analysis during the releasing arrow stage,IEEE,Conferences,"In this paper, during the releasing arrow stage, the relationship between the time series trajectory and the archery performance has been studied. With the aid of the high sampling rate 1200 frames/second of video camera, the time series aiming trajectory during the releasing stage can be captured for analysis, especially the releasing string motion. The linear time invariant auto-regressive exogenous (ARX) process is adopted to model the intended aiming trajectory before the releasing arrow stage. Then based on the model the estimated trajectory during the releasing arrow stage is evaluated as the reference, so its difference between the real one is the main focus in the paper. According to these discrepancies some key parameters are defined first, and the implementation of correlation approach can result in some significant relationships which can characterize his particular releasing string motion during this stage. For example, his releasing arrow trajectory always has an upper linear trend along the vertical direction, and the vertical deviation on the target plays more important role than the radial deviation.",https://ieeexplore.ieee.org/document/5212738/,2009 International Conference on Machine Learning and Cybernetics,12-15 July 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMEW53276.2021.9455998,Time-Scalable Low-Latency Video Bitrate Adaptation,IEEE,Conferences,"High-quality real-time video requires large bandwidth and low latency. These requirements bring huge challenges to network performance. In this paper, a dynamically balanced framework for the bandwidth and delay of the video is proposed. The frame rate of the video segment being played will adaptively change with the download time. For example, When the throughput is slightly lower than the selectable bitrate, a longer download time is required, then the playback frame rate will slow down, and vice versa. This method not only reduces the buffer occupancy and enhances the real-time performance, but equivalent to increases the optional bitrate of the video. We have implemented our libVLC player using the SDK provided by VLC. The simulation results show that our method improves significantly the downloaded bitrate at low, medium, and high throughput.",https://ieeexplore.ieee.org/document/9455998/,2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),5-9 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC40277.2020.9149260,Towards Continuous Subject Identification Using Wearable Devices and Deep CNNs,IEEE,Conferences,"Subject identification has several applications. In transportation companies, knowing who is driving their vehicles might prevent theft or other ill-intended actions. On the other hand, privacy concerns, and the respective legislation, hinder the applicability of several traditional recognition techniques based on invasive technologies, such as video cameras. Moreover, in order to keep the driver's distractions to a minimum, this technologies must be non-disruptive, that is, they must be able to identify the subject seamlessly without them taking any action. In this context, we propose using deep learning applied to smart watch data for recognizing the person driving a vehicle based on a training set. Our proposal relies on the possibility of using transfer learning to avoid long training sessions for new drivers and to deliver a solution which can be deployed in practice. In this paper, we describe the convolutional neural network used in the solution and present results according to a real data-set collected by us, achieving accuracies ranging from 75 to 94%.",https://ieeexplore.ieee.org/document/9149260/,ICC 2020 - 2020 IEEE International Conference on Communications (ICC),7-11 June 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2013.129,Towards Motion Aware Light Field Video for Dynamic Scenes,IEEE,Conferences,"Current Light Field (LF) cameras offer fixed resolution in space, time and angle which is decided a-priori and is independent of the scene. These cameras either trade-off spatial resolution to capture single-shot LF or tradeoff temporal resolution by assuming a static scene to capture high spatial resolution LF. Thus, capturing high spatial resolution LF video for dynamic scenes remains an open and challenging problem. We present the concept, design and implementation of a LF video camera that allows capturing high resolution LF video. The spatial, angular and temporal resolution are not fixed a-priori and we exploit the scene-specific redundancy in space, time and angle. Our reconstruction is motion-aware and offers a continuum of resolution tradeoff with increasing motion in the scene. The key idea is (a) to design efficient multiplexing matrices that allow resolution tradeoffs, (b) use dictionary learning and sparse representations for robust reconstruction, and (c) perform local motion-aware adaptive reconstruction. We perform extensive analysis and characterize the performance of our motion-aware reconstruction algorithm. We show realistic simulations using a graphics simulator as well as real results using a LCoS based programmable camera. We demonstrate novel results such as high resolution digital refocusing for dynamic moving objects.",https://ieeexplore.ieee.org/document/6751235/,2013 IEEE International Conference on Computer Vision,1-8 Dec. 2013,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IE.2011.24,Towards Producing Artificial Humans for Intelligent Environments Research,IEEE,Conferences,"This paper and the accompanying video, (which can be viewed at http://www.youtube.com/watch?v=jtZodTAoLqQ) present the current findings of a project researching how computer games and their related technologies can be adapted to create virtual and mixed reality intelligent environments. After describing the developed strategy and the methods of deployment used a new research focus for the project is introduced. A new mechanism, still under development, aims to modify user data gathered from physical, virtual and mixed reality intelligent environments to produce artificial intelligence controllers for human non-player character avatars. The generated controllers would potentially be usable by avatars in computer games and virtual or mixed reality intelligent environments. For research purposes, these computer-controlled avatars may potentially provide an alternative to using real people during evaluations.",https://ieeexplore.ieee.org/document/6063346/,2011 Seventh International Conference on Intelligent Environments,25-28 July 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA.2019.00264,Towards Real-Time Detection of Squamous Pre-Cancers from Oesophageal Endoscopic Videos,IEEE,Conferences,"This study investigates the feasibility of applying state of the art deep learning techniques to detect precancerous stages of squamous cell carcinoma (SCC) cancer in real time to address the challenges while diagnosing SCC with subtle appearance changes as well as video processing speed. Two deep learning models are implemented, which are to determine artefact of video frames and to detect, segment and classify those no-artefact frames respectively. For detection of SCC, both mask-RCNN and YOLOv3 architectures are implemented. In addition, in order to ascertain one bounding box being detected for one region of interest instead of multiple duplicated boxes, a faster non-maxima suppression technique (NMS) is applied on top of predictions. As a result, this developed system can process videos at 16-20 frames per second. Three classes are classified, which are 'suspicious', 'high grade' and 'cancer' of SCC. With the resolution of 1920x1080 pixels of videos, the average processing time while apply YOLOv3 is in the range of 0.064-0.101 seconds per frame, i.e. 10-15 frames per second, while running under Windows 10 operating system with 1 GPU (GeForce GTX 1060). The averaged accuracies for classification and detection are 85% and 74% respectively. Since YOLOv3 only provides bounding boxes, to delineate lesioned regions, mask-RCNN is also evaluated. While better detection result is achieved with 77% accuracy, the classification accuracy is similar to that by YOLOYv3 with 84%. However, the processing speed is more than 10 times slower with an average of 1.2 second per frame due to creation of masks. The accuracy of segmentation by mask-RCNN is 63%. These results are based on the date sets of 350 images. Further improvement is hence in need in the future by collecting, annotating or augmenting more datasets.",https://ieeexplore.ieee.org/document/8999079/,2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA),16-19 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN48605.2020.9207125,Towards Real-time Video Content Detection in Resource Constrained Devices,IEEE,Conferences,"Convolutional neural networks have been successfully applied for video content detection in the last years. However, such cognitive models usually demand the availability of several gigabytes of memory and present a low detection throughput, as a result, they are not feasible for resource-constrained devices, especially for real-time applications like video streaming. In this paper, we address real-time video content detection in resource-constrained devices in a threefold manner. First, we improve detection throughput by means of a frame sampling technique. Then, we propose a new evaluation measure towards proper deployment of convolutional neural networks in resource-constrained devices. Finally, we address the accuracy degradation caused by the porting of the convolutional neural network, applying a lightweight classification verification technique. The evaluation results, through a real-time demanding application, show that the proposed approach can detect up to 301 frames/sec, demanding only 9 megabytes of memory while reaching up to 89.3% of accuracy. Besides, we can increase the detection throughput by up to 10 times, with no effects on accuracy, and further increase accuracy without effects on processing demands.",https://ieeexplore.ieee.org/document/9207125/,2020 International Joint Conference on Neural Networks (IJCNN),19-24 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SAHCN.2019.8824876,Towards Scalable Video Analytics at the Edge,IEEE,Conferences,"Breakthroughs in deep learning, GPUs, and edge computing have paved the way for always-on, live video analytics. However, to achieve real-time performance, a GPU needs to be dedicated amongst a few video feeds. But, GPUs are expensive resources and a large-scale deployment requires supporting hundreds of video cameras - exorbitant cost prohibits widespread adoption. To ease this burden, we propose Tetris, a system comprising of several optimization techniques from computer vision and deep-learning literature blended in a synergistic manner. Tetris is designed to maximize the parallel processing of video feeds on a single GPU, with a marginal drop in inference accuracy. Tetris performs CPU-based tiling of active regions to combine activities across video feeds. resulting in a condensed input volume. It then runs the deep learning model on this condensed volume instead of individual feeds, which significantly improves the GPU utilization. Our evaluation on Duke MTMC dataset reveals that Tetris can process 4x video feeds in parallel compared to any of the existing methods used in isolation.",https://ieeexplore.ieee.org/document/8824876/,"2019 16th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON)",10-13 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICDCSW53096.2021.00009,Towards Understanding the Adaptation Space of AI-Assisted Data Protection for Video Analytics at the Edge,IEEE,Conferences,"Edge computing facilitates the deployment of distributed AI applications, capable of processing video data in real time. AI-assisted video analytics can provide valuable information and benefits in various domains. Face recognition, object detection, or movement tracing are prominent examples enabled by this technology. However, such mechanisms also entail threats regarding privacy and security, for example if the video contains identifiable persons. Therefore, adequate data protection is an increasing concern in video analytics. AI-assisted data protection mechanisms, such as face blurring, can help, but are often computationally expensive. Additionally, the heterogeneous hardware of end devices and the time-varying load on edge services need to be considered. Therefore, such systems need to adapt to react to changes during their operation, ensuring that conflicting requirements on data protection, performance, and accuracy are addressed in the best possible way. Sound adaptation decisions require an understanding of the adaptation options and their impact on different quality attributes. In this paper, we identify factors that can be adapted in AI-assisted data protection for video analytics using the example of a face blurring pipeline. We measure the impact of these factors using a heterogeneous edge computing hardware testbed. The results show a large and complex adaptation space, with varied impacts on data protection, performance, and accuracy.",https://ieeexplore.ieee.org/document/9545916/,2021 IEEE 41st International Conference on Distributed Computing Systems Workshops (ICDCSW),7-10 July 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CIG.2009.5286473,Towards conscious-like behavior in computer game characters,IEEE,Conferences,"The main sources of inspiration for the design of more engaging synthetic characters are existing psychological models of human cognition. Usually, these models, and the associated Artificial Intelligence (AI) techniques, are based on partial aspects of the real complex systems involved in the generation of human-like behavior. Emotions, planning, learning, user modeling, set shifting, and attention mechanisms are some remarkable examples of features typically considered in isolation within classical AI control models. Artificial cognitive architectures aim at integrating many of these aspects together into effective control systems. However, the design of this sort of architectures is not straightforward. In this paper, we argue that current research efforts in the young field of Machine Consciousness (MC) could contribute to tackle complexity and provide a useful framework for the design of more appealing synthetic characters. This hypothesis is illustrated with the application of a novel consciousness-based cognitive architecture to the development of a First Person Shooter video game character.",https://ieeexplore.ieee.org/document/5286473/,2009 IEEE Symposium on Computational Intelligence and Games,7-10 Sept. 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9413129,Tracking Fast Moving Objects by Segmentation Network,IEEE,Conferences,"Tracking Fast Moving Objects (FMO), which appear as blurred streaks in video sequences, is a difficult task for standard trackers, as the object position does not overlap in consecutive video frames and texture information of the objects is blurred. Up-to-date approaches tuned for this task are based on background subtraction with a static background and slow deblurring algorithms. In this article, we present a tracking-by-segmentation approach implemented using modern deep learning methods that perform near real-time tracking on real-world video sequences. We have developed a physically plausible FMO sequence generator to be a robust foundation for our training pipeline and demonstrate straightforward network adaptation for different FMO scenarios with varying foreground.",https://ieeexplore.ieee.org/document/9413129/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICBASE53849.2021.00031,Tracking and detecting network traffic based on capacity data prediction,IEEE,Conferences,"In order to improve the traffic tracking and monitoring ability of heterogeneous multi-core data migration network, a network traffic tracking and detection method based on capacity data prediction is proposed to construct a parameter structure model of audio and video data information collection and dynamic caching of heterogeneous multi-core data migration network traffic. Linear regression analysis and heterogeneous reorganization are used to reconstruct the time series of heterogeneous multi-core data migration network traffic, and matched filtering and time series analysis are used. The capacity data clustering model of heterogeneous multi-core data migration network traffic is established, and the spatial spectrum frequency-division feature quantity of heterogeneous multi-core data migration network traffic is extracted. The real-time prediction of heterogeneous multi-core data migration network traffic is realized through the traffic transmission channel equilibrium control method, and the transmission prediction of communication traffic is realized through the traffic feature fusion analysis. The simulation results show that the traffic tracking and detection accuracy of heterogeneous multi-core data migration network is high and the transmission reliability of the system is good.",https://ieeexplore.ieee.org/document/9696017/,2021 2nd International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE),24-26 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2005.348,Tracking multiple colored blobs with a moving camera,IEEE,Conferences,"This paper concerns a method for tracking multiple blobs exhibiting certain color distributions in images acquired by a possibly moving camera. The method encompasses a collection of techniques that enable modeling and detecting the blobs possessing the desired color distribution(s), as well as inferring their temporal association across image sequences. Appropriately colored blobs are detected with a Bayesian classifier, which is bootstrapped with a small set of training data. Then, an online iterative training procedure is employed to refine the classifier using additional training images. Online adaptation of color probabilities is used to enable the classifier to cope with illumination changes. Tracking over time is realized through a novel technique, which can handle multiple colored blobs. Such blobs may move in complex trajectories and occlude each other in the field of view of a possibly moving camera, while their number may vary over time. A prototype implementation of the developed system running on a conventional Pentium IV processor at 2.5 GHz operates on 320/spl times/240 live video in real time (30Hz). It is worth pointing out that currently, the cycle time of the tracker is determined by the maximum acquisition frame rate that is supported by our IEEE 1394 camera, rather than the latency introduced by the computational overhead for tracking blobs.",https://ieeexplore.ieee.org/document/1467577/,2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05),20-25 June 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/KSE.2015.59,Tracking-Learning-Detection Adopted Unsupervised Learning Algorithm,IEEE,Conferences,"In this paper, we research the real-time object tracking technology. The object tracking algorithm discussed in this paper is developed based on the Tracking-Learning-Detection(TLD) and the Centroid Neural Network(CNN). The object is unknown ahead of tracking, the model of the object is composed of objects transformed geometrically immediately after tracking. The TLD framework is useful for long-term object tracking in a video stream because the TLD framework applies a novel learning algorithm called P-N learning. We propose a method that applies the CNN algorithm to the TLD framework. The CNN algorithm is an unsupervised learning algorithm that provides a stable result, regardless of initial values of learning coefficients and neurons. The object tracking algorithm discussed in this paper has a higher accuracy than that of TLD in terms of detection. Additionally, it exhibits better processing performance than that of TLD.",https://ieeexplore.ieee.org/document/7371788/,2015 Seventh International Conference on Knowledge and Systems Engineering (KSE),8-10 Oct. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CEIT.2018.8751898,Traffic Sign Detection and Recognition System for Autonomous RC Cars,IEEE,Conferences,"Traffic signs play an important role to regulate daily traffic by providing necessary information to the drivers. For unmanned driving systems, real time and robust detection and recognition of traffic signs is one of the main concerns. Therefore, a traffic sign detection and recognition system for autonomous radio controlled cars is proposed. In this work, traditional image processing methods and deep neural networks techniques are combined. First, the online video is streamed from the car camera and the input frame region of interest is detected. Secondly, a convolutional neural network is used to recognize these candidate images. Experimental results show that the proposed system works efficiently up to %87.36 of images. However, calibration is needed for image processing techniques for various environments.",https://ieeexplore.ieee.org/document/8751898/,2018 6th International Conference on Control Engineering & Information Technology (CEIT),25-27 Oct. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIMS52415.2021.9466006,Traffic Signs Detection and Recognition System Using the YOLOv4 Algorithm,IEEE,Conferences,"Traffic signs are one of the important road equipment facilities to inform road users about regulations and visual directions. Currently, an automatic Traffic Sign Recognition (TSR) system is being developed which is implemented in an advanced driver system (ADAS) so that road users can be safe and secure while on the road. Therefore, this paper aims to be able to detect and recognize traffic signs on the highway to provide information on the meaning of these traffic signs automatically. In this study, 35 classes of signs were used which consisted of warning signs, prohibitions signs, mandatory signs, and instructions signs. This system is implemented using the Darknet framework with the You Only Look Once version 4 (YOLOv4) model. The investigation carried out in this study is a system that detects and recognizes traffic signs evaluated on offline-based video in one-way traffic during the day. The result of mAP (mean Average Precision) in this system is 95.15%.",https://ieeexplore.ieee.org/document/9466006/,2021 International Conference on Artificial Intelligence and Mechatronics Systems (AIMS),28-30 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECE51571.2020.9393109,Traffic Surveillance using Vehicle License Plate Detection and Recognition in Bangladesh,IEEE,Conferences,"Computer vision coupled with Deep Learning (DL) techniques bring out a substantial prospect in the field of traffic control, monitoring and law enforcing activities. This paper presents a YOLOv4 object detection model in which the Convolutional Neural Network (CNN) is trained and tuned for detecting the license plate of the vehicles of Bangladesh and recognizing characters using tesseract from the detected license plates. Here we also present a Graphical User Interface (GUI) based on Tkinter, a python package. The license plate detection model is trained with mean average precision (mAP) of 90.50% and performed in a single TESLA T4 GPU with an average of 14 frames per second (fps) on real time video footage.",https://ieeexplore.ieee.org/document/9393109/,2020 11th International Conference on Electrical and Computer Engineering (ICECE),17-19 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCSD.2019.8843009,Train Carriage Warning System Based on Zigbee Wireless Sensor Network,IEEE,Conferences,"In order to realize the timely and accurate alarm for abnormal conditions in train carriage, a scheme of safety warning system based on zigbee technology is proposed. The hardware and software architecture of the system are designed. Meanwhile, the workflow and implementation process of each node are elaborated in detail. The system is composed of five parts: sensor node, zigbee router, coordinator, video module and the intelligent terminal. Firstly, the decibels received by the sound sensor are judged through threshold and sent to the coordinator through zigbee router module. Secondly, the video module analyzes the video of the alarming carriage by means of artificial intelligence. Finally, the detailed alarm information is sent to the intelligent terminal. The development cost of the system is low and its installation and maintenance is convenient. It improves the monitoring facilities of train carriages, which can guarantee the accuracy and real-time of the warning. It has good expansibility and portability, which is of practical significance for improving the safety of train operation.",https://ieeexplore.ieee.org/document/8843009/,"2019 IEEE 3rd International Conference on Circuits, Systems and Devices (ICCSD)",23-25 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEEICT53079.2022.9768500,Transfer Learning with Deep Representations is Used to Recognition Yoga Postures,IEEE,Conferences,"Human activity identification is the automated interpretation of the movements happen in a video done by a human. Iterative Due to its wide applications in fields such as autonomous driving, biomedical imaging, and machine intelligence vision, among others, recognizing human activity in an image remains a tough and crucial research subj ect in the field of computer vision. Deep learning techniques have recently advanced, and models for image identification and classification, object detection, and speech recognition have been successfully implemented. Only a few examples include different aspects of human structure and movement, diffraction, a busy background, and so on. Moving cameras, changing lighting conditions and changing perspectives are all things to think about. Yoga is an excellent kind of physical activity. It&#x0027;s critical to maintain proper yoga posture. This research provides a unique technique for yoga asana detection based on feature extraction and representation Using a deep CNN model that has already been trained, followed by yoga asana recognition using a hybrid Support Vector Machine (SVM) and K-Nearest Neighbor (KNN) classifier. With the constrained training datasets, it was discovered that previously learned CNN-based representations on large-scale annotated datasets may be applied to yoga asana recognition tasks. In real-time datasets, the suggested approach is tested on seven yoga asana (Pranamasana, Dhanurasan, Dandasana, Gomukhasan, Garudasana, Padmavrikshasana and Padmasan). The results show that the proposed scheme outperforms the state of the art methods.",https://ieeexplore.ieee.org/document/9768500/,"2022 First International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)",16-18 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CVPR.2012.6248013,Understanding collective crowd behaviors: Learning a Mixture model of Dynamic pedestrian-Agents,IEEE,Conferences,"In this paper, a new Mixture model of Dynamic pedestrian-Agents (MDA) is proposed to learn the collective behavior patterns of pedestrians in crowded scenes. Collective behaviors characterize the intrinsic dynamics of the crowd. From the agent-based modeling, each pedestrian in the crowd is driven by a dynamic pedestrian-agent, which is a linear dynamic system with its initial and termination states reflecting a pedestrian's belief of the starting point and the destination. Then the whole crowd is modeled as a mixture of dynamic pedestrian-agents. Once the model is unsupervisedly learned from real data, MDA can simulate the crowd behaviors. Furthermore, MDA can well infer the past behaviors and predict the future behaviors of pedestrians given their trajectories only partially observed, and classify different pedestrian behaviors in the scene. The effectiveness of MDA and its applications are demonstrated by qualitative and quantitative experiments on the video surveillance dataset collected from the New York Grand Central Station.",https://ieeexplore.ieee.org/document/6248013/,2012 IEEE Conference on Computer Vision and Pattern Recognition,16-21 June 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/MIPRO52101.2021.9596823,Underwater ROV Software for Fish Cage Inspection,IEEE,Conferences,"This paper deals with the development of control software for a remotely operated vehicle (ROV) as part of an automated fish cage inspection system in aquaculture. The ROV navigates autonomously around the fish cages streaming video to the topside computer that runs the algorithms for vertical rope detection. The topside computer sends back the velocity references to the ROV in real-time in order to successfully complete the inspection task. These images are also used to determine the state of net biofouling using a pre-trained convolutional neural network to help determine which nets are in need of cleaning. Robot Operating System (ROS) framework is developed to enable the topside computer to access the video stream from the ROV, process the images, and send back velocity references that would result in the complete inspection of fish cages. The inspection task is planned by following the recognizable rope segments of the outer structure of the fish cage downwards by controlling the vehicle's yaw, heave, and depth.",https://ieeexplore.ieee.org/document/9596823/,"2021 44th International Convention on Information, Communication and Electronic Technology (MIPRO)",27 Sept.-1 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV48922.2021.01060,"Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation",IEEE,Conferences,"Current state-of-the-art object detection and segmentation methods work well under the closed-world assumption. This closed-world setting assumes that the list of object categories is available during training and deployment. However, many real-world applications require detecting or segmenting novel objects, i.e., object categories never seen during training. In this paper, we present, UVO (Unidentified Video Objects), a new benchmark for openworld class-agnostic object segmentation in videos. Besides shifting the focus to the open-world setup, UVO is significantly larger, providing approximately 6 times more videos compared with DAVIS, and 7 times more mask (instance) annotations per video compared with YouTube-VO(I)S. UVO is also more challenging as it includes many videos with crowded scenes and complex background motions. We also demonstrated that UVO can be used for other applications, such as object tracking and super-voxel segmentation. We believe that UVO is a versatile testbed for researchers to develop novel approaches for open-world class-agnostic object segmentation, and inspires new research directions towards a more comprehensive video understanding beyond classification and detection.",https://ieeexplore.ieee.org/document/9710887/,2021 IEEE/CVF International Conference on Computer Vision (ICCV),10-17 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVISP54630.2021.00045,Unified Physical Threat Monitoring System Aided by Virtual Building Simulation,IEEE,Conferences,"With increasing physical threats in recent years targeted at critical infrastructures, it is crucial to establish a reliable threat monitoring system integrating video surveillance and digital sensors based on cutting-edge technologies. A physical threat monitoring solution unifying the floorplan, cameras, and sensors for smart buildings has been set up in our study. Computer vision and deep learning models are used for video streams analysis. When a threat is detected by a rule engine based on the real-time analysis results combining with feedback from related digital sensors, an alert is sent to the Video Management System so that human operators can take further action.A physical threat monitoring system typically needs to address complex and even destructive incidents, such as fire, which is unrealistic to simulate in real life. Restrictions imposed during the Covid-19 pandemic and privacy concerns have added to the challenges. Our study utilises the Unreal Engine to simulate some typical suspicious and intrusion scenes with photorealistic qualities in the context of a virtual building. Add-on programs are implemented to transfer the video stream from virtual PTZ cameras to the Milestone Video Management System and enable users to control those cameras from the graphic client application. Virtual sensors such as fire alarms, temperature sensors and door access controls are implemented similarly, fulfilling the same programmatic VMS interface as real-life sensors. Thanks to this simulation system&#x2019;s extensibility and repeatability, we have consolidated this unified physical threat monitoring system and verified its effectiveness and user-friendliness. Both the simulated Unreal scenes and the software add-ons developed during this study are highly modulated and thereby are ready for reuse in future projects in this area.",https://ieeexplore.ieee.org/document/9700859/,"2021 5th International Conference on Vision, Image and Signal Processing (ICVISP)",18-20 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BCD51206.2021.9581431,Unsupervised Ensemble Learning and Its Application to Temporal Data Mining: Keynote Address,IEEE,Conferences,"Ensemble learning is originally proposed for classification tasks in a manner of supervised learningthe basic concept of ensemble learning is to train multiple base learners as ensemble members and combine their predictions into a single output that should have better performance on average than any other ensemble member. Recently, ensemble learning has been extended to unsupervised learning with different strategies, named as clustering/unsupervised ensemble. This has led to many real-world applications, such as gene classification, image segmentation, video retrieval, and so on. This presentation is going to concentrate on unsupervised ensemble learning technique, and talk about how such technique can deal with the challenge in clustering temporal data with various and high dimensionality, large volume, very high-feature correlation, and a substantial amount of noise.",https://ieeexplore.ieee.org/document/9581431/,"2021 IEEE/ACIS 6th International Conference on Big Data, Cloud Computing, and Data Science (BCD)",13-15 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IV51971.2022.9827285,Unsupervised Network Intrusion Detection System for AVTP in Automotive Ethernet Networks,IEEE,Conferences,"Network Intrusion Detection Systems (NIDSs) are widely regarded as efficient tools for securing in-vehicle networks against diverse cyberattacks. However, since cyberattacks are always evolving, signature-based intrusion detection systems are no longer adopted. An alternative solution can be the deployment of deep learning based intrusion detection system which play an important role in detecting unknown attack patterns in network traffic. Hence, in this paper, we compare the performance of different unsupervised deep and machine learning based anomaly detection algorithms, for real-time detection of anomalies on the Audio Video Transport Protocol (AVTP), an application layer protocol implemented in the recent Automotive Ethernet based in-vehicle network. The numerical results, conducted on the recently published &#x201C;Automotive Ethernet Intrusion Dataset show that deep learning models significantly outperfom other state-of-the art traditional anomaly detection models in machine learning under different experimental settings.",https://ieeexplore.ieee.org/document/9827285/,2022 IEEE Intelligent Vehicles Symposium (IV),4-9 June 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/M2VIP.2018.8600864,Unsupervised Video Prediction Network with Spatio-temporal Deep Features,IEEE,Conferences,"Predicting the future states of things is an important performance form of intelligence and it is also of vital importance in real-time systems such as autonomous cars and robotics. This paper aims to tackle a video prediction task. Previous methods for future frame prediction are always subject to restrictions from environment, leading to poor accuracy and blurry prediction details. In this work, we present an unsupervised video prediction framework which iteratively anticipates the raw RGB pixel values in future video frames. Extensive experiments are implemented on advanced datasets — KTH and KITTI. The results demonstrate that our method achieves a good performance.",https://ieeexplore.ieee.org/document/8600864/,2018 25th International Conference on Mechatronics and Machine Vision in Practice (M2VIP),20-22 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICC.1998.685192,Use of object-oriented intelligent simulation tools for Web based education,IEEE,Conferences,"This study explains the development of a Web based computer aided intelligent education tool. The tool is designed to enhance the knowledge and decision making capabilities of students studying engineering related environmental issues. The system enables freshmen engineering students to enhance critical thinking ability. The power of interactive simulation, animation, artificial intelligence and object-oriented technologies are incorporated to guide the students in the decision-making processes of real-life case studies. The system developed combines theory, experiments, and software tools and can be used in classroom situations or for self/group-learning. The use of intelligent simulation modules is aimed at introductory level environmental chemistry. The currently available three modules are environmentally safe paint preparation process, automobile pollution and fundamentals of gas laws. The teaching strategy of each section of courseware is centered around a text that introduces the fundamentals, theory and links to simulated laboratories. The text also contains hyperlinks to other text pictures, audio and video resources, and interactive models.",https://ieeexplore.ieee.org/document/685192/,ICC '98. 1998 IEEE International Conference on Communications. Conference Record. Affiliated with SUPERCOMM'98 (Cat. No.98CH36220),7-11 June 1998,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INNOVATIONS.2008.4781725,Use of ontologies for bridging semantic gaps in distant communication,IEEE,Conferences,"In this paper, we show how ontology can be utilized to support distant communication. Although augmented with audio and video functions, and screen sharing facilities, the users of real-time communication tools are facing difficulties due to their inability to provide appropriate meaning for keywords or conversation topics. Our system fills the semantic gap and supports the knowledge of keywords related to a specific topic. This paper presents an extended approach that uses different domain ontologies to carry out mapping by agents. Assisting a teacher by providing appropriate information to students as an intelligent tutoring system is one example application. This paper describes a sample scenario of computer usage operations in an English composition class that shows how the semantic gap between teachers and students can be filled using ontologies.",https://ieeexplore.ieee.org/document/4781725/,2008 International Conference on Innovations in Information Technology,16-18 Dec. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COGINF.2009.5250777,User-oriented healthcare support system based on symbiotic computing,IEEE,Conferences,"We propose a multi-agent-based healthcare support system in ubiquitous computing environment. By utilizing knowledge about healthcare and various information including vital sign, physical location, and video data of a user under observation from real space, the system provides useful information regarding health condition effectively and in user-oriented manner. This paper describes a user-oriented healthcare support system based on concept of symbiotic computing, focusing on design and initial prototype implementation of the system.",https://ieeexplore.ieee.org/document/5250777/,2009 8th IEEE International Conference on Cognitive Informatics,15-17 June 2009,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AIVR46125.2019.00048,Using CNNs For Users Segmentation In Video See-Through Augmented Virtuality,IEEE,Conferences,"In this paper, we present preliminary results on the use of deep learning techniques to integrate the user's self-body and other participants into a head-mounted video see-through augmented virtuality scenario. It has been previously shown that seeing user's bodies in such simulations may improve the feeling of both self and social presence in the virtual environment, as well as user performance. We propose to use a convolutional neural network for real time semantic segmentation of users' bodies in the stereoscopic RGB video streams acquired from the perspective of the user. We describe design issues as well as implementation details of the system and demonstrate the feasibility of using such neural networks for merging users' bodies in an augmented virtuality simulation.",https://ieeexplore.ieee.org/document/8942263/,2019 IEEE International Conference on Artificial Intelligence and Virtual Reality (AIVR),9-11 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECCT52121.2021.9616791,Usual and Unusual Human Activity Recognition in Video using Deep Learning and Artificial Intelligence for Security Applications,IEEE,Conferences,"The main objective of Human Activity Recognition (HAR) is to detect various activities in video frames. Video surveillance is an import application for various security reasons, therefore it is essential to classify activities as usual and unusual. This paper implements the deep learning model that has the ability to classify and localize the activities detected using a Single Shot Detector (SSD) algorithm with a bounding box, which is explicitly trained to detect usual and unusual activities for security surveillance applications. Further this model can be deployed in public places to improve safety and security of individuals. The SSD model is designed and trained using transfer learning approach. Performance evaluation metrics are visualised using Tensor Board tool. This paper further discusses the challenges in real-time implementation.",https://ieeexplore.ieee.org/document/9616791/,"2021 Fourth International Conference on Electrical, Computer and Communication Technologies (ICECCT)",15-17 Sept. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/COMPSAC.2019.00041,VISA: A Supervised Approach to Indexing Video Lectures with Semantic Annotations,IEEE,Conferences,"Many universities adopt educational systems where the teacher lecture is video recorded and the video lecture is made available to students with minimum post-processing effort. These cost-effective solutions suffer from the limited amount of annotations associated with the video content, which strongly limits the usability of the service when students need to retrieve specific portions of video, e.g., to revise unclear aspects covered in the past lectures. This paper presents, as a real case study, the system developed and implemented in our university for video lecture annotation and indexing. The original video recordings, which last around 1.5 hour, are first partitioned into smaller segments and then annotated by mapping their content with the entities in a multilingual knowledge base. To this purpose, the proposed approach analyzes both the transcription of the teacher's speech and the text appearing in the video (e.g., the slide content, the note written on the whiteboard) by means of an ad hoc Named Entity Recognition and Disambiguation (NERD) step. NERD relies on a supervised classification approach tailored to the domain under analysis. More specifically, to identify the most salient entities of the knowledge base matching the video content it considers not only text similarity measures but also the semantic pertinence of the candidate entities to the main subject of the video lectures. The performance of the proposed system was validated on a ground truth against the techniques available in the general entity annotation system GERBIL. The preliminary results demonstrate the effectiveness of the proposed approach.",https://ieeexplore.ieee.org/document/8754468/,2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC),15-19 Jul 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2015.7351007,VLSI friendly fast CU/PU mode decision for HEVC intra encoding: Leveraging convolution neural network,IEEE,Conferences,"To alleviate the computational intensity of Intra encoding for High Efficiency Video Coding (HEVC), we introduce the convolution neural network to reduce the number of the promising CU/PU candidate modes to carry out the exhaustive RDO processing. The practical merits include: Firstly, the proposed algorithm reduces the maximum computational complexity at the grain of 64 &#x00D7; 64 coding tree unit(CTU), which makes it efficient to ameliorate the complexity of the real-time hardwired encoder implementation. Secondly, because the CU/PU mode decision is made based on the analysis of source block textures, our algorithm does not depend on intermediate results of encoding. That is, the proposed algorithm will not deteriorate the processing schedule of CTU encoding. Experimental results show that, when our algorithm is integrated with HM12.0, the 61.1% Intra encoding time was saved, whereas the averaging BDBR augment is merely 3.39%.",https://ieeexplore.ieee.org/document/7351007/,2015 IEEE International Conference on Image Processing (ICIP),27-30 Sept. 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ASONAM49781.2020.9381430,VStreamDRLS: Dynamic Graph Representation Learning with Self-Attention for Enterprise Distributed Video Streaming Solutions,IEEE,Conferences,"Live video streaming has become a mainstay as a standard communication solution for several enterprises worldwide. To efficiently stream high-quality live video content to a large amount of offices, companies employ distributed video streaming solutions which rely on prior knowledge of the underlying evolving enterprise network. However, such networks are highly complex and dynamic. Hence, to optimally coordinate the live video distribution, the available network capacity between viewers has to be accurately predicted. In this paper we propose a graph representation learning technique on weighted and dynamic graphs to predict the network capacity, that is the weights of connections/links between viewers/nodes. We propose VStreamDRLS, a graph neural network architecture with a self-attention mechanism to capture the evolution of the graph structure of live video streaming events. VStreamDRLS employs the graph convolutional network (GCN) model over the duration of a live video streaming event and introduces a self-attention mechanism to evolve the GCN parameters. In doing so, our model focuses on the GCN weights that are relevant to the evolution of the graph and generate the node representation, accordingly. We evaluate our proposed approach on the link prediction task on two real-world datasets, generated by enterprise live video streaming events. The duration of each event lasted an hour. The experimental results demonstrate the effectiveness of VStreamDRLS when compared with state-of-the-art strategies. Our evaluation datasets and implementation are publicly available at https://github.com/stefanosantaris/vstreamdrls.",https://ieeexplore.ieee.org/document/9381430/,2020 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM),7-10 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCV.2005.246,Vector boosting for rotation invariant multi-view face detection,IEEE,Conferences,"In this paper, we propose a novel tree-structured multiview face detector (MVFD), which adopts the coarse-to-fine strategy to divide the entire face space into smaller and smaller subspaces. For this purpose, a newly extended boosting algorithm named vector boosting is developed to train the predictors for the branching nodes of the tree that have multicomponents outputs as vectors. Our MVFD covers a large range of the face space, say, +/-45/spl deg/ rotation in plane (RIP) and +/-90/spl deg/ rotation off plane (ROP), and achieves high accuracy and amazing speed (about 40 ms per frame on a 320 /spl times/ 240 video sequence) compared with previous published works. As a result, by simply rotating the detector 90/spl deg/, 180/spl deg/ and 270/spl deg/, a rotation invariant (360/spl deg/ RIP) MVFD is implemented that achieves real time performance (11 fps on a 320 /spl times/ 240 video sequence) with high accuracy.",https://ieeexplore.ieee.org/document/1541289/,Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1,17-21 Oct. 2005,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICECA49313.2020.9297561,Vehicle Density Analysis and Classification using YOLOv3 for Smart Cities,IEEE,Conferences,"Incorporation of the digital technologies in the surveillance of urban mobility such as monitoring the traffic density will help in improving the quantity of vehicles/arrangements to be provided for the public commutation, facility to be incorporated in reducing the traffic, infrastructure to be provided such as road widening, pedestrian path, over bridge, under pass etc., where traffic and transport is an issue. This can be implemented in the city, at which it is recognized to be developed as a smart city. The proposed research work analyzes the vehicle density using python-OpenCV and YOLOv3. Real time videos are recorded in four directions from Sony HD IP cameras in a designated area. Image frames from video sequence are used to detect moving vehicles. The background extraction method is applied for each frame which is used in subsequent analysis to detect and count all the vehicles. The blobs are detected for each vehicle which helps to track the vehicle in motion. The center of each vehicle with blob gives the count of vehicle based on the lanes considered. This work not only counts the vehicle in real time but also classifies the different vehicles using deep learning technique. YoloV3(You only look once) object detection system is used along with a pretrained model called darknet to classify the vehicle into different categories (bus, car, motorcycle etc). This deep learning method showed better classification and detection rate compared to blobs and morphological method used for counting the vehicles. Classification is shown for vehicle and also person classification is considered to analyze the percentage of people and vehicles. The analysis of percentage of vehicles is shown using pie chart.",https://ieeexplore.ieee.org/document/9297561/,"2020 4th International Conference on Electronics, Communication and Aerospace Technology (ICECA)",5-7 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/ICACT51234.2021.9370867,Vehicle Detection Counting Algorithm Based on Background Subtraction Algorithm and SORT,IEEE,Conferences,"At present, the deep neural network model is commonly used to detect the vehicle in the video, and the detection accuracy is relatively high. However, the neural network model requires high computing performance and high demand for network transmission bandwidth. In many cases, the edge computing device used is of small computing power, so the neural network model is not applicable. However, background subtraction algorithm is easy to realize because of its low requirement on hardware calculation force and fast and accurate detection speed. Using SORT algorithm to track with accurate detection results can improve the speed again and reduce the consumption of computing resources. Therefore, this paper proposes an algorithm that uses the background subtraction algorithm to detect the vehicles in the video, and then uses the SORT algorithm to track the detected vehicles. The vehicle counter will automatically count when the vehicle in the video passes the traffic flow counting line. The accuracy of traffic flow counting results in this paper is 88%, which proves the feasibility and effectiveness of vehicle detection counting method based on background subtraction algorithm and SORT.",https://ieeexplore.ieee.org/document/9370867/,2021 23rd International Conference on Advanced Communication Technology (ICACT),7-10 Feb. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICEEICT53079.2022.9768653,Vehicle Detection and Counting using Deep Learning basedYOLO and Deep SORT Algorithm for Urban Traffic Management System,IEEE,Conferences,"Vehicle counting is a process to estimate traffic density on roads to assess the traffic conditions for intelligent transportation systems (ITS). Real-time traffic management systems have become popular recently due to the availability of high end cameras and technology. The present traffic management systems focus on speed detection, signal jumping, zebra crossing but not on traffic density estimation. Proposed video-based vehicle counting and tracking method using a video captured on CCTV and handheld mobile cameras. The system can be used in smart cities to create smart traffic light signals, in which duration of each signal depends on real time vehicle density in a particular lane of road. Vehicle counting is performed in two steps: the captured video is sent to You Only Look Once (YOLO) based deep learning framework to detect, count and classify the vehicles. Multi vehicular tracking is adopted using Deep SORTalgorithm to track the vehicles in video frames. Model was trained for six different classes, using Google Colaboratory. Datasets of vehicles specifically pertaining to Indian roads environment is considered for implementation. The performance of the model was analyzed, proposed model has tested and obtained an average counting accuracy of 86.56&#x0025; while the average precision is 93.85&#x0025;. The model can be implemented for ascertaining the traffic density on roads and this provides knowledge for infrastructural development to authorities. It can also be an integral part of smart city projects to develop intelligent and smart traffic surveillance system.",https://ieeexplore.ieee.org/document/9768653/,"2022 First International Conference on Electrical, Electronics, Information and Communication Technologies (ICEEICT)",16-18 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCAIE51753.2021.9431784,Vehicle Detection and Tracking using YOLO and DeepSORT,IEEE,Conferences,"Every year, the number of vehicles on the road will be increasing. as claimed by a road transport department (JPJ) data in Malaysia, there were around 31.2 million units of motor vehicles recorded in Malaysia as of December 31, 2019. While as, from the mid-2017, there were around 28.18 million units of motor vehicles recorded in Malaysia. Consequently, accurate and fast detection of vehicles on the road is needed by using the volume of vehicles as valuable data for detecting traffic congestion which then benefits for traffic management. Using the implemented deep learning for vehicle detection, this paper project is using TensorFlow which is platform for machine learning and you only look once (yolo) which is object detection algorithm for real-time vehicle detection. By combining this two and other dependencies with python as programming language, the suggested method in this paper determine the improvement of YOLOv4 latest algorithm compared to the previous model in vehicle detection system. This vehicle detection also uses DeepSORT algorithm to help counting the number of vehicles pass in the video effectively. From this paper, the best model between YOLO model is Yolov4 which had achieved state-of-the-art results with 82.08% AP50 using the custom dataset at a real time speed of around 14 FPS on GTX 1660ti.",https://ieeexplore.ieee.org/document/9431784/,2021 IEEE 11th IEEE Symposium on Computer Applications & Industrial Electronics (ISCAIE),3-4 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCE.2018.8539298,Vehicle Detection on Embedded Single Board Computers,IEEE,Conferences,"In this paper we present a vision-based vehicle detection pipeline for autonomous vehicles. This pipeline is based on Histograms of Oriented Gradients (HOG) and Support Vector Machines (SVM) for classification. Primary motivation to conduct this research is to evaluate the performance of vision intensive tasks on embedded single board computers which is used to process the real time video frames in vehicles. Complete system is implemented on two different embedded boards that is Raspberry Pi 3 and Odroid C2. We evaluated our proposed system using the KITTI Vision Benchmark Suite, a standard in computer vision research. The results show that the implemented pipeline reaches the accuracy up to 70% on Odroid C2 and 65% on Raspberry Pi 3, given the limited training dataset size of 1200 examples. Furthermore, these embedded systems don't have any on board GPU that may improve the system's performance.",https://ieeexplore.ieee.org/document/8539298/,2018 7th International Conference on Computer and Communication Engineering (ICCCE),19-20 Sept. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICCS51141.2021.9432144,Vehicle Identification from Traffic Video Surveillance Using YOLOv4,IEEE,Conferences,"Traffic Video Surveillance has emerged as an important tool in our lives. The ability to detect & follow vehicles, both individual & public transport ones, is extremely useful in numerous applications such as the safety of the drivers, preventing & monitoring road accidents etc. Using continuous video stream from CCTV cameras from all over the world, this project deals with the concept of Vehicle Detection with the support of Computer Vision algorithm in real-time frame. The proposed framework capitalizes on YOLOv4 to achieve faster object detection in real time and using the proposed dataset it was tested on various conditions such as rain, low visibility, daylight, snow, and night. Pre-Processing steps of the dataset is done in the first phase. After the first phase, Various types of Vehicle images are captured. In the final phase, depending on the properties computed or calculated in the earlier phases, the Precision, Mean Average Precision (map) and Average Intersection Over Union (IoU) of the model with which it can recognize the various types of Vehicles were found. This model can further help in the development of a framework for Vehicular accident detection in real time.",https://ieeexplore.ieee.org/document/9432144/,2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS),6-8 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIMTech.2019.8843812,Vehicle Recognition Systems Using Speed-Up Robust Features and Non-Maxima Suppression,IEEE,Conferences,"The proposed work aims to implement non learning computer vision method using features detection in real live traffic condition to recognize vehicle by its physical features. The algorithm uses speed up robust features that is invariant to both scaling and rotation of an image. The use of speed up robust features (SURF) technique is based on its capability to correctly recognize images with different sizes and various angles of rotation. The proposed work uses a modified SURF that uses non maxima suppression in finding local maxima in order to point exactly the highest value of gradient magnitude. Object cloning was as well implemented in order to avoid memory violation of the observed and model images and increases the speed of algorithm. The system was tested against several conditions including tested towards finding moving and stationed object from the observed video. Additionally, it was as well tested towards real live traffic with minimum light (at night) and during light rain that increase the noise and distortion level of a live monitored environment. The accuracy result is promising with an average of 90% correctness and average performance of 688 millisecond per frame. The work as well aimed to provide conclusion whether the use of a modified scale and rotation invariant technique such as SURF and non-maxima suppression could contribute positively in the area of live vehicle recognition systems.",https://ieeexplore.ieee.org/document/8843812/,2019 International Conference on Information Management and Technology (ICIMTech),19-20 Aug. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICMLA52953.2021.00055,Vehicle Tracking with Crop-based Detection,IEEE,Conferences,"End-to-end production of vehicle tracking data from video in real-time and with high accuracy remains a challenging problem due to the computational cost of object detection on each frame. In this work we present Tracking with Crop-based Detection, a method for speeding object tracking in constrained contexts (with stable cameras and relatively-predictable object motion) such as vehicle traffic monitoring. We leverage this context to provide a strong prior for object locations, which we use to 1.) boost detection speed by detecting objects only in regions corresponding to object priors on most frames and 2.) inform the selection of the detector output for each object. We evaluate Crop-based Detection as an extension to the KIOU object tracker (Crop-KIOU) on the UA-DETRAC dataset. The proposed tracker outperforms all other reported algorithms in terms of PR-MOTA, PR-MOTP, and mostly tracked objects on the UA-DETRAC benchmark, establishing a new state-of-the-art. Relative to tracking by detection with KIOU, Crop-KIOU achieves a 26% higher frame-rate and increases accuracy. Furthermore, Tracking with Crop-based Detection can be combined with frame skipping; we show a 149% increase in framerate relative to KIOU with no decrease in accuracy using this combination of methods.",https://ieeexplore.ieee.org/document/9680148/,2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA),13-16 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/GCCE.2017.8229368,Vehicle traffic and flood monitoring with reroute system using Bayesian networks analysis,IEEE,Conferences,"Heavy vehicle traffic and flooded areas are problems experienced on roads because of unimproved road infrastructures and environmental deviations. These factors affect vehicle drivers negatively as they contribute to stress, health problems, and wastefulness of time. This study developed a system called ArRoad that monitors and analyzes vehicle traffic and flooded areas using network of sensors and real-time image processing which then predicts and visualizes possible alternative rerouting paths using machine learning. Water level sensor nodes are used to monitor the flooded areas while real-time video images from cameras are processed to extract the vehicle volume on the streets. A Bayesian Network is generated from the water level sensors and image processing data which provides possible reroute areas to avoid traffic congestion and flooded areas. All data are sent to a cloud platform through the Internet that can be accessed through a mobile user interface. This mobile user application provides information about the condition of the streets and possible reroute maps to users. The accuracy of the system is tested by actual implementation on a specific road. Results showed minimum accessing delay from using the ArRoad to navigate in rerouted paths to prevent impassable roads due to heavy traffic and flood. If effect, it lessens the amount of time experienced by drivers from heavy traffic condition and flooded streets which then improves the quality of life by preventing waste of resources such as time and money.",https://ieeexplore.ieee.org/document/8229368/,2017 IEEE 6th Global Conference on Consumer Electronics (GCCE),24-27 Oct. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WCSP.2016.7752672,Vehicle type classification via adaptive feature clustering for traffic surveillance video,IEEE,Conferences,"Vehicle type classification has become an important part of intelligent traffic. However traditional methods can not deal with the varying situations in the reality. In this paper, a novel method is proposed to handle this task in the real road traffic surveillance video. In order to distinguish different vehicles, we categorize vehicles into three types: compact cars, mid-size cars, and heavy-duty vehicles. For a certain video, our method has four steps. First, a deep convolutional neural network is used to detect vehicles in the candidate region and a data set would be generated. Second, the main features of vehicles can be extracted using a fully-connected network. Also, for the sake of higher accuracy, weak labels given by pre-trained extreme learning machine (ELM) are fused into the final features, adding prior information proportionally. Third, K-means is implemented to learn three vehicle-type cluster centers adaptively. Finally, vehicle type will be recognized according to the closest distance principal. Experimental results show that the recognition rate outperforms other traditional methods, verifying the feasibility and effectiveness of the proposed method.",https://ieeexplore.ieee.org/document/7752672/,2016 8th International Conference on Wireless Communications & Signal Processing (WCSP),13-15 Oct. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/BigData47090.2019.9006018,VidCEP: Complex Event Processing Framework to Detect Spatiotemporal Patterns in Video Streams,IEEE,Conferences,"Video data is highly expressive and has traditionally been very difficult for a machine to interpret. Querying event patterns from video streams is challenging due to its unstructured representation. Middleware systems such as Complex Event Processing (CEP) mine patterns from data streams and send notifications to users in a timely fashion. Current CEP systems have inherent limitations to query video streams due to their unstructured data model and lack of expressive query language. In this work, we focus on a CEP framework where users can define high-level expressive queries over videos to detect a range of spatiotemporal event patterns. In this context, we propose- i) VidCEP, an in-memory, on the fly, near real-time complex event matching framework for video streams. The system uses a graph-based event representation for video streams which enables the detection of high-level semantic concepts from video using cascades of Deep Neural Network models, ii) a Video Event Query language (VEQL) to express high-level user queries for video streams in CEP, iii) a complex event matcher to detect spatiotemporal video event patterns by matching expressive user queries over video data. The proposed approach detects spatiotemporal video event patterns with an F-score ranging from 0.66 to 0. S9. VidCEP maintains near real-time performance with an average throughput of 70 frames per second for 5 parallel videos with sub-second matching latency.",https://ieeexplore.ieee.org/document/9006018/,2019 IEEE International Conference on Big Data (Big Data),9-12 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACCCN51052.2020.9362900,Video Analytics for Face Detection and Tracking,IEEE,Conferences,"Machine Learning has substantially grown over a period of decade. Deep Learning the new field of Machine Learning is gaining ever-increasing interest in research due to its implicit capability appropriate for successful applications in the field of computer vision, speech processing, image processing, object detection, human and face sub-attribute detection, and many more analytical systems. Intelligent Video Systems and Video Analytics is managed by a wide collection in transportation, security, health care and customer analytics. A Deep-Learning- based approach helps researchers control the massive, complex and diverse dataset to improve the performance by proficiently extracting only the vital features from a large video dataset. In this paper we present a system to autonomously detect facial images from a video surveillance and extract feature points in the frame. We use Viola-Jones algorithm and the KLT algorithm in our system to track faces in real-time. We also present a brief survey on Deep Learning (DL) models deployed in the field of Video Analytics. We cover architectures, tasks and related analytical methods and demonstrate the importance of Video Systems.",https://ieeexplore.ieee.org/document/9362900/,"2020 2nd International Conference on Advances in Computing, Communication Control and Networking (ICACCCN)",18-19 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCCBDA55098.2022.9778285,Video Content Classification Using Time-Sync Comments and Titles,IEEE,Conferences,"The Time-Sync Comment (TSC) is a novel comment used by online video websites and has been gradually recognized by Internet users. Attracted by the active viewer-creator interaction, numerous content creators upload their videos to online TSC video websites. Facing the increasing number of videos, it is difficult to efficiently classify video in traditional manual review methods. Given the close comment-video correlation, achievements of natural language processing can be applied to the TSC video classification task, processing texts instead of images. In this paper, a new method of video classification based on TSCs and titles is proposed. It combines the BERT (Bidirectional Encoder Representation from Transformers) model with the machine learning classifier, and obtains classification results via analyzing TSCs and titles. In particular, this method can work with few TSCs and bypass the limit of the BERT for the input sequence length. The experimental results on the real-world dataset show that BERT-SVM can achieve a better performance than the baseline methods, and the maximum accuracy is up to 0.9396. This research can help online video websites to manage TSC videos more efficiently and intelligently, and provide novel ideas for researchers to study classical tasks.",https://ieeexplore.ieee.org/document/9778285/,2022 7th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA),22-24 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSPC51351.2021.9451782,Video Frame-Based Deep Learning Face Detection-A Review,IEEE,Conferences,"Face detection is hotly discussed issues in computer vision, not just because of the difficult nature of the face as an object, mostly because of the numerous implementations that require the incremental approach of the face detection program. Important progress has been made over the last 15 years due to the accessibility of data in unrestricted capturing situations (so-called' in-the-wild through the Internet, the public's initiative to establish freely accessible standards, and even success in creating robust machine vision algorithms). Because of the explosive increase of video content, the face detection issue has attracted extensive interest among researchers. In this study, we look at the most recent advancements in real-world face detectors, beginning with the technique of the pioneering Viola-Jones face detector. This strategies are classified into two sections: rigid structures, which are taught primarily via strategies based on deep learning that are boosted or implemented, and deformable structures, which are defined by their elements and characterize the face. Fair representation techniques will be outlined in detail, as well as a few other efficient strategies that will be discussed shortly after the end. Finally, the most important resources for analyzing face detection algorithms and recent optimization efforts are addressed, as well as the potential of face detection.",https://ieeexplore.ieee.org/document/9451782/,2021 3rd International Conference on Signal Processing and Communication (ICPSC),13-14 May 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CBD.2017.75,Video Object Detection for Tractability with Deep Learning Method,IEEE,Conferences,"In this paper, a video based objection detection method is proposed for traceability system with deep learning method. The surveillance video is collected first, from which an annotated image database of target object such as people or vehicle was constructed to train convolutional neural network model off-line. With the trained model, a real-time target detection and recognition system is designed and implemented. The proposed method mainly includes three aspects: video processing, target detection and object recognition. It provides a variety of video interfaces to support the downloaded video and real-time video stream. The experimental results indicate that the proposed deep learning based detection method is efficient for the traceability application.",https://ieeexplore.ieee.org/document/8026970/,2017 Fifth International Conference on Advanced Cloud and Big Data (CBD),13-16 Aug. 2017,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICVRV.2011.42,Video Semantic Concept Detection Based on Conceptual Correlation and Boosting,IEEE,Conferences,"Semantic concept detection is a key technique to video semantic indexing. Traditional approaches did not take account of conceptual correlation adequately. A new approach based on conceptual correlation and boosting is proposed in this paper, including three steps: the context based conceptual fusion models using correlative concepts selection are built at first, then a boosting process based on inter-concept correlation is implemented, finally multi-models generated in boosting are fusioned. The experimental results on Trecvid2005 dataset show that the proposed method achieves more remarkable and consistent improvement.",https://ieeexplore.ieee.org/document/6092740/,2011 International Conference on Virtual Reality and Visualization,4-5 Nov. 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICPR48806.2021.9412057,Video Summarization with a Dual Attention Capsule Network,IEEE,Conferences,"In this paper, we address the problem of video summarization, which aims at selecting a subset of video frames as a summary to represent the original video contents compactly and completely. We propose a simple but effective supervised approach with a dual attention capsule network towards this end. Unlike existing LSTM based methods, it pays attention to short- and long-term dependencies among video frames through an elaborate dual self-attention architecture, which can handle longer-term dependencies and admit parallel computing. To reconcile the outputs of dual self-attention, we rely on a two-stream capsule network to learn the underlying frame selection criteria. Experiments on real-world datasets show the advantages of the proposed approach compared with state-of-the-art methods.",https://ieeexplore.ieee.org/document/9412057/,2020 25th International Conference on Pattern Recognition (ICPR),10-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INISTA52262.2021.9548475,Video Surveillance Framework Based on Real-Time Face Mask Detection and Recognition,IEEE,Conferences,"In this paper we proposed a real-time face mask detection and recognition for CCTV surveillance camera videos. The proposed work consists of six steps: video acquisition and keyframes selection, data augmentation, facial parts segmentation, pixel-based feature extraction, Bag of Visual Words (BoVW) generation, face mask detection, and face recognition. In the first step, a set of keyframes are selected using histogram of gradient (HoG) algorithm. Secondly, data augmentation is involved with three steps as color normalization, illumination correction (CLAHE), and poses normalization (Angular Affine Transformation). In third step, facial parts are segmented using clustering approach i.e. Expectation Maximization with Gaussian Mixture Model (EM-GMM), in which facial regions are segmented into Eyes, Nose, Mouth, Chin, and Forehead. Then, Pixel-based Feature Extraction is performed using Yolo Nano approach, which performance is higher and lightweight model than the Yolo Tiny V2 and Yolo Tiny V3, and extracted features are constructed into Codebook by Hassanat Similarity with K-Nearest neighbor (H-M with KNN) algorithm. For mask detection, L2 distance function is used. The final step is face recognition which is implemented by a Kernel-based Extreme Learning Machine with Slime Mould Optimization (SMO). Experiments conducted using Python IDLE 3.8 for the proposed Yolo Nano model and also previous works as GMM with Deep learning (GMM+DL), Convolutional Neural Network (CNN) with VGGF, Yolo Tiny V2, and Yolo Tiny V3 in terms of various performance metrics.",https://ieeexplore.ieee.org/document/9548475/,2021 International Conference on INnovations in Intelligent SysTems and Applications (INISTA),25-27 Aug. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOCOM.2015.7218361,Video acuity assessment in mobile devices,IEEE,Conferences,"The quality of mobile videos is usually quantified through the Quality of Experience (QoE), which is usually based on network QoS measurements, user engagement, or post-view subjective scores. Such quantifications are not adequate for real-time evaluation. They cannot provide on-line feedback for improvement of visual acuity, which represents the actual viewing experience of the end user. We present a visual acuity framework which makes fast online computations in a mobile device and provide an accurate estimate of mobile video QoE. We identify and study the three main causes that impact visual acuity in mobile videos: spatial distortions, types of buffering and resolution changes. Each of them can be accurately modeled using our framework. We use machine learning techniques to build a prediction model for visual acuity, which depicts more than 78% accuracy. We present an experimental implementation on iPhone 4 and 5s to show that the proposed visual acuity framework is feasible to deploy in mobile devices. Using a data corpus of over 2852 mobile video clips for the experiments, we validate the proposed framework.",https://ieeexplore.ieee.org/document/7218361/,2015 IEEE Conference on Computer Communications (INFOCOM),26 April-1 May 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/NICRSP.1996.542792,Video compression with random neural networks,IEEE,Conferences,"We summarize a novel neural network technique for video compression, using a ""point-process"" type neural network model we have developed, which is closer to biophysical reality and is mathematically much more tractable than standard models. Our algorithm uses an adaptive approach based upon the users' desired video quality Q, and achieves compression ratios of up to 500:1 for moving gray-scale images, based on a combination of motion detection, compression and temporal subsampling of frames. This leads to a compression ratio of over 1000:1 for full-color video sequences with the addition of the standard 4:1:1 spatial subsampling ratios in the chrominance images. The signal-to-noise-ratio obtained varies with the compression level and ranges from 29 dB to over 34 dB. Our method is computationally fast so that compression and decompression could possibly be performed in real-time software.",https://ieeexplore.ieee.org/document/542792/,"Proceedings of International Workshop on Neural Networks for Identification, Control, Robotics and Signal/Image Processing",21-23 Aug. 1996,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2015.7176186,Video gender recognition using temporal coherent face descriptor,IEEE,Conferences,"In this paper, we propose a new temporal coherent face descriptor for video gender recognition. The proposed face descriptor is constructed from detected faces of continuous video frames. Because it describes detected faces under variant changes in continuous video frames and provides a unified feature description, face normalization and alignment processes can be avoided during gender recognition. Based on the face descriptor, a support vector machine classifier is applied to identify the gender of the subject in the videos. As shown in the experiments, our method not only achieves better results compared to the state-of-the-art methods but also the real-time performance for video processing.",https://ieeexplore.ieee.org/document/7176186/,"2015 IEEE/ACIS 16th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)",1-3 June 2015,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/WorldS4.2019.8903929,Video lectures intellectual analysis algorithms and software system development,IEEE,Conferences,"The paper is devoted to solving the problem of preparing educational video content without an operator. The paper describes the development of algorithms and scenarios for identification, localization, recognition and classification elements in a frame, including using neural network technologies. The developed complex of algorithms and scenarios is implemented in the software system of intellectual analysis of educational video lectures, real-time temporal and sense synchronization of video lectures and presentation materials. It is assumed that the use of the developed system will lead to a significant acceleration and reduction in the cost of the preparing educational video lectures process.",https://ieeexplore.ieee.org/document/8903929/,2019 Third World Conference on Smart Trends in Systems Security and Sustainablity (WorldS4),30-31 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ColCACI.2019.8781798,Video processing inside embedded devices using SSD-Mobilenet to count mobility actors,IEEE,Conferences,"The actual number of surveillance cameras and the different methods for counting vehicles originate the question: What is the best place to process video flows? This work performs the implementation of a counting system for mobility actors like cars, pedestrians, motorcycles, bicycles, buses, and trucks in the context of an Edge computing application using deep learning. However, the implementation of Deep Neural Networks for Object Detection in low-capacity embedded devices make it difficult to perform tasks that require high processing or must be carried out in real time. To solve this problem this study presents the analysis and implementation of different techniques based on the use of an additional hardware element as is the case of a Vision Processing Unit (VPU) in combination with methods that affect the resolution, bit rate, and time of video processing. For this purpose we consider the Mobilenet-SSD model with two approaches: a pre-trained model with known data sets and a trained model with images from our specific scenarios. The use of SSD-Mobilenet's model generates different results in terms of accuracy and time of video processing in the system. Results show that the use of an embedded device in combination with a VPU and video processing techniques reach 18.62 Frames per Second (FPS). Thus, video processing time is slightly superior (5.63 minutes) for a video of 5 minutes. Recall and precision values of 91% and 97% are reported in the best case (class car) for the vehicle counting system.",https://ieeexplore.ieee.org/document/8781798/,2019 IEEE Colombian Conference on Applications in Computational Intelligence (ColCACI),5-7 June 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2016.7532424,Video system for human attribute analysis using compact convolutional neural network,IEEE,Conferences,"Convolutional neural networks show their advantage in human attribute analysis (e.g. age, gender and ethnicity). However, they experience issues (e.g. robustness and responsiveness) when deployed in an intelligent video system. We propose one compact CNN model and apply it in our video system motivated by the full consideration of performance and usability. With the proposed web image mining and labelling strategy, we construct a large training set which covers various image conditions. The proposed CNN model successfully achieves a mean absolute error (MAE) of 3.23 years on the Morph 2 dataset, using the same test policy as our counterparts. This is the state-of-the-art score to our knowledge using CNN for age estimation. The proposed video analysis system employs this compact CNN model and demonstrated good performance in both dataset tests and deployment in real-world environments.",https://ieeexplore.ieee.org/document/7532424/,2016 IEEE International Conference on Image Processing (ICIP),25-28 Sept. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ANTS47819.2019.9117960,Video-based Real-time Intrusion Detection System using Deep-Learning for Smart City Applications,IEEE,Conferences,"There is a huge demand of video surveillance based intelligent security systems which can automatically detect the unauthorized entry or mal-intentional intrusion to the unattended sensitive areas and notify to the concerned authorities in real-time. A novel video-based Intrusion Detection System (IDS) using deep learning is proposed. Here, You Only Look Once (YOLO) algorithm is used for object detection and intrusion is decided using our proposed algorithm based on the shifted center of mass of the detected object. Further, Simple Online and Real-time Tracking (SORT) algorithm is used for the tracking of the intruder in real-time. The developed system is also implemented and tested for live video stream using NVIDIA Jetson TX2 development platform with an accuracy of 97% and average fps of 30. Here, the proposed IDS is a generic one where the user can select the region of interest (the area to be intrusion free) of any size and shape from the reference (starting) frame and potential intruders such as a person, vehicle, etc. from the list of trained object classes. Hence, it can have a wide range of smart city applications such as person intrusion free zone, no vehicle entry zone, no parking zone, smart home security, etc.",https://ieeexplore.ieee.org/document/9117960/,2019 IEEE International Conference on Advanced Networks and Telecommunications Systems (ANTS),16-19 Dec. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IVS.2011.5940533,Video-based roll angle estimation for two-wheeled vehicles,IEEE,Conferences,"Video-based driver assistance systems are a key component for intelligent vehicles today. Applications for lane detection, traffic sign recognition, and collision avoidance have been successfully deployed in cars and trucks. State-of-the art algorithms rely on machine learning and therefore depend on invariance conditions, e.g. a fixed image perspective. In order to apply current modules in two-wheeled vehicles one needs to determine the roll angle, i.e. the angle between the road plane and the slanted vehicle. It can either be used for parametrisation of the algorithms or for rotation of the video image back to a horizontal alignment. Using an inertial measurement unit to acquire this data is unreasonably expensive. We propose a video-based module that estimates the current roll angle based on gradient orientation histograms to overcome this flaw. Due to the visual structure of a traffic scene we are able to derive possible roll angles from the gradient statistics by correlation with learnt data. Analogously, we estimate the roll rate by correlating subsequent image statistics and stabilise both measures within a linear Kalman filter. Experiments on real image data from various test scenarios show high accuracy of the proposed approach. Thus, estimating the roll angle / rate from video only, enables us to employ established video-based assistance modules for two-wheeled vehicles without any additional hardware expense.",https://ieeexplore.ieee.org/document/5940533/,2011 IEEE Intelligent Vehicles Symposium (IV),5-9 June 2011,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICNP.2016.7784458,Viewing 360 degree videos: Motion prediction and bandwidth optimization,IEEE,Conferences,"360-degree video transmission consumes 4∼6× the bandwidth of a regular video, and thus imposes significant challenges to networks. To address this challenge, in this paper, we propose a motion-prediction-based transmission mechanism that matches network video transmission to viewer needs. Ideally, if viewer motion is perfectly known in advance, we could reduce bandwidth consumption by 80%. Practically, however, we have to address the random nature of viewer motion, in order to guarantee the quality of the viewing experience. Based on our experimental study of viewer motion (comprising 16 video clips and over 150 subjects), we propose a machine learning mechanism that predicts viewer motion. Based on such predictions, we propose a partial-content-transmission mechanism that reduces the overall bandwidth consumption while providing probabilistic performance guarantees. Real-trace-based evaluations show that the proposed scheme significantly reduces bandwidth consumption with negligible performance degradation. For example, given a failure ratio of 0.1%, we can reduce bandwidth consumption by more than 40%.",https://ieeexplore.ieee.org/document/7784458/,2016 IEEE 24th International Conference on Network Protocols (ICNP),8-11 Nov. 2016,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACITE53722.2022.9823655,Violence Detection in Real Life Videos using Convolutional Neural Network,IEEE,Conferences,"Detecting violence in video recordings through artificial intelligence is critical. Just because violence detection seems complex now, it doesn&#x0027;t mean that the problem should be ignored. With this paper, we aimed to show how we can do violence detection and how it can be implemented easily with the simplest methods at hand thanks to the progressing advances in deep learning and AI. Furthermore, it could be an excellent tool for preventing children from accessing inappropriate content and assisting guardians in making better decisions about what their children should watch. This is a challenging topic since the definition of violence is broad and quite abstract. As a result, recognizing such intricacies from recordings without human intervention is not just a technical issue, but also a theoretical one. In light of this, using a convolutional neural network, we will investigate how to depict the concept of violence. For image categorization, there are numerous pre-trained convolutional neural networks available. These models have figured out how to bring out dynamic and useful highlights from regular pictures and use them as a beginning stage to get familiar with new tasks. These networks can classify images into various object categories after being trained on over a million photos. We used pre-trained networks with transfer learning since it&#x0027;s regularly much quicker and more straightforward than preparing or training a network from scratch. Using several deep learning techniques, we will investigate which model will provide the best accuracy for recognizing violence in videos for this project.",https://ieeexplore.ieee.org/document/9823655/,2022 2nd International Conference on Advance Computing and Innovative Technologies in Engineering (ICACITE),28-29 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1049/icp.2022.0587,Violence detection for surveillance systems using lightweight CNN models,IET,Conferences,"Due to significant safety concerns, today, many places are filled with CCTV cameras, but it still uses manual security to overlook these, which leaves much space for human error, be it negligence or a dangerous situation. This may result in many anomalies such as violence, hostage, or fires. To prevent this, an intelligent and automated system has been implemented that tries to overcome this using deep learning techniques. Using Deep Learning models utilizing Convolutional Neural Networks (CNN), specifically the lightweight models - MobileNetV2 and ResNet50V2 are used to identify threat movements in the given frames over three datasets, namely UCF crime, Real life violence situations, and UBI-fights. The violence in the video is detected using frames, and accuracy is measured. A threat is detected by the system from the video frame, based on which the system for the situation generates an alert. In extreme cases, alerts are sent directly to nearby police stations and emergency services with accurate location information while also indicating the suspicious activities at an instance of time.",https://ieeexplore.ieee.org/document/9800155/,7th International Conference on Computing in Engineering & Technology (ICCET 2022),12-13 Feb. 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/TISC.2010.5714650,Vision based moving object tracking through enhanced color image segmentation using Haar classifiers,IEEE,Conferences,"In this paper we implement a vision based moving Object Tracking system with Wireless Surveillance Camera which uses a color image segmentation and color histogram with background subtraction for tracking any objects in non-ideal environment. The implementation of the moving video objects can be based on any one of the tracking algorithms such as Template matching, Continuously Adaptive Mean Shift (CAMSHIFT), SIFT, Mean Shift, SIFT, Cross correlation algorithm is presented by optimizing the kernel variants by adjusting the HSV value for various environmental conditions. The object occlusions are also removed by calculating the minimal distance between the two objects using Bhattacharya coefficients and it is robust to changes in shape with complete occlusion. The object to be tracked can also be classified using HAAR classifier through machine learning. A software approach for real time implementation of moving object tracking is done through MATLAB.",https://ieeexplore.ieee.org/document/5714650/,Trendz in Information Sciences & Computing(TISC2010),17-19 Dec. 2010,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICITR51448.2020.9310872,Vision-Based Adaptive Traffic Light Controller for Single Intersection,IEEE,Conferences,"In this paper, a vision-based adaptive traffic light controller is proposed. The proposed controller was successfully deployed and tested as a complete system in a complex roundabout in Colombo city at a highly congested time. There were two main parts to this implementation. The first part was a vision-based traffic monitoring system. In this part, a system was developed so that it monitored lanes in a junction with cameras and extracted a traffic index based on traffic density, vehicle type, and pixel-wise velocity of vehicles by processing the video streams coming from cameras. The traffic signal light controlling part was the second part of the project. This part dealt with estimating a better timing adjustment for the existing system using a mathematical modeling approach while taking the extracted traffic index as input. This system was operated with the existing system with minimum alterations for easy real-world implementation. The developed prototype was plugged into the existing system to change traffic light phase timing according to the existing traffic level.",https://ieeexplore.ieee.org/document/9310872/,2020 5th International Conference on Information Technology Research (ICITR),2-4 Dec. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SICE.2006.315291,Vowel Recognition System by Lip-Reading Method Using Active Contour Models and its Hardware Realization,IEEE,Conferences,"Even in noisy environments such as in machinery factories or in crushes, we sometimes wish to control equipments by using human voices. Although word recognition methods have been successfully developed, noises in environments cause serious recognition problems. In such cases, lip shape movements are expected to be useful as the supporting data to improve the performance of recognitions. In this paper, a method to extract outer and inner lip shapes from input face images by using active contour models is proposed. The extracted lip shape data are utilized to control equipments by lip-readings in noisy circumstances. Normalization method to reduce the effect of the lip size change depending on the distance from a camera to human faces and to improve the recognition rate is also mentioned. A three-hierarchy neural network is used for the recognition method. The proposed method is implemented as hardware circuits in a FPGA chip to process NTSC video signals in real-time. Experimental results of vowel recognition are shown to confirm effectiveness of the proposed method",https://ieeexplore.ieee.org/document/4109134/,2006 SICE-ICASE International Joint Conference,18-21 Oct. 2006,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/AVSS.2018.8639165,WatchNet: Efficient and Depth-based Network for People Detection in Video Surveillance Systems,IEEE,Conferences,"We propose a deep-learning approach for people detection on depth imagery. The approach is designed to be deployed as an autonomous appliance for identifying people attacks and intrusion in video surveillance scenarios. To this end, we propose a fully-convolutional and sequential network, named WatchNet, that localizes people in depth images by predicting human body landmarks such as head and shoulders. We use a large synthetic dataset to train the network with abundant data and generate automatic annotations. Adaptation to real data is performed via fine tuning with real depth images.The proposed method is validated in a novel and challenging database with about 29k top view images collected from several sequences including different people assaults. A comparative evaluation is given between our approach and other standard methods, showing remarkable detection results and efficiency. The network runs in 10 and 28 FPS using CPU and GPU, respectively.",https://ieeexplore.ieee.org/document/8639165/,2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS),27-30 Nov. 2018,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CNNA.2002.1035038,Watermarking for the authentication of video on CNN-UM,IEEE,Conferences,"Digital watermarks have been proposed for authentication of both video and still images. In such applications, the watermark is embedded within a host image such that subsequent alteration to the watermarked image can be detected with high probability. In this paper the possibility of implementing real time watermarking on a video stream is presented. In fact the new CNN-UM implementation offers time operation of only microseconds working on 64/spl times/64 images.",https://ieeexplore.ieee.org/document/1035038/,Proceedings of the 2002 7th IEEE International Workshop on Cellular Neural Networks and Their Applications,24-24 July 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IJCNN.2002.1007484,WaveNet processing brassboards for live video via radio,IEEE,Conferences,"Recent advances in areas of both discrete wavelet transforms (DWT) and continuous wavelet transforms (CWT), representing human visual system neural networks, have resulted in improved video compression, restoration, and filtering techniques. These software techniques are capable of achieving quality performance in video. However, the computational complexity requires specially designed hardware to run real time live video through radio. This hardware, called a brassboard, when integrated with computers, can potentially provide us with many applications including remote sensors, security systems, commercial and home video teleconferencing. This paper describes a low cost board to support a video compression, restoration, and filter system in real time processing. This WaveNet board has been optimized for wavelet-based image and video compression and enhancement techniques.",https://ieeexplore.ieee.org/document/1007484/,Proceedings of the 2002 International Joint Conference on Neural Networks. IJCNN'02 (Cat. No.02CH37290),12-17 May 2002,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICESC48915.2020.9155832,Weapon Detection using Artificial Intelligence and Deep Learning for Security Applications,IEEE,Conferences,"Security is always a main concern in every domain, due to a rise in crime rate in a crowded event or suspicious lonely areas. Abnormal detection and monitoring have major applications of computer vision to tackle various problems. Due to growing demand in the protection of safety, security and personal properties, needs and deployment of video surveillance systems can recognize and interpret the scene and anomaly events play a vital role in intelligence monitoring. This paper implements automatic gun (or) weapon detection using a convolution neural network (CNN) based SSD and Faster RCNN algorithms. Proposed implementation uses two types of datasets. One dataset, which had pre-labelled images and the other one is a set of images, which were labelled manually. Results are tabulated, both algorithms achieve good accuracy, but their application in real situations can be based on the trade-off between speed and accuracy.",https://ieeexplore.ieee.org/document/9155832/,2020 International Conference on Electronics and Sustainable Communication Systems (ICESC),2-4 July 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/INFOTEH53737.2022.9751279,Weather Condition Classification in Vehicle Environment Based on Front-View Camera Images,IEEE,Conferences,"The current environmental conditions should be monitored during autonomous driving since the different weather conditions can have a different impact on implemented sensor system or on the efficiency of the implemented control system. In this paper, the classification of weather conditions in the vehicle environment is based on images captured by a front-view camera, which are further processed by the simple Convolutional Neural Network (CNN). For model development purposes, training and validation data sets were created from two sources: the BDD100K database and by extracting frames from the collected video sequences. The solution implements an additional mechanism to filter out false predictions based on a circular buffer. The proposed solution achieves the F1 measure of 98.3&#x0025; for the entire test video frames data set, where it achieves the best results in snowy weather detection (Precision of 100&#x0025;, F1 of 100.00&#x0025;) and the worst in foggy weather detection (Precision of 97.25&#x0025;, F1 of 98.00&#x0025;).",https://ieeexplore.ieee.org/document/9751279/,2022 21st International Symposium INFOTEH-JAHORINA (INFOTEH),16-18 March 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICIP.2003.1246745,Weighted order statistic image filter chip based on cellular neural network architecture,IEEE,Conferences,This paper describes a VLSI chip of an analog image weighted order statistic (WOS) filter based on cellular neural network (CNN) architecture for real-time applications. The chip has been implemented in CMOS AMS 0.8 /spl mu/m CYE technology. This filter consists of feedforward nonlinear template B operating within the window of 3 by 3 pixels around the central pixel being filtered. The feedforward nonlinear CNN coefficients have been realized using programmable nonlinear coupler circuits. The WOS filter chip allows for processing of images with 300 pixels horizontal resolution. Functional tests of the chip have been performed using a special test set-up for PAL composite video signal processing. Using the set-up real images have been filtered by WOS filter chip under test.,https://ieeexplore.ieee.org/document/1246745/,Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429),14-17 Sept. 2003,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/SNPD.2007.139,What Affects Decoding Complexity of Distributed Video Codec Based on Turbo Code,IEEE,Conferences,"The distributed video coding (DVC) is a new coming video compression technology that utilizes a different computing complexity mode from the traditional video codec. As a new paradigm there are some fundamental and hard questions in DVC that are not sufficiently addressed, for example the complexity balance between DVC encoder and decoder. This will be an important factor for some promising real time applications. In this context, this paper studies the factors which affect the decoding complexities of turbo code based DVC, especially focuses on quantization, side information and turbo code bit rates. Analyzing and simulation results show that precise side information will decrease the turbo decoding complexity, large reductions in computations can be traded against relatively small increases in bit rate, and uniform quantizer possesses the advantage than the non-uniform quantizer in computation load at a expense of a little decrease in rate distortion performance.",https://ieeexplore.ieee.org/document/4287672/,"Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)",30 July-1 Aug. 2007,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/RITAPP.2019.8932854,Which LSTM Type is Better for Interaction Force Estimation?,IEEE,Conferences,"Tactile, one of the five senses classified into the main senses of human, is the first sensation developed when human beings are formed. The tactile includes various information such as pressure, temperature, and texture of objects, it also helps the person to interact with the surrounding environment. One of the tactile information, the pressure is used in various fields such as medical, beauty, mobile devices and so on. However, humans can perceive the real world with multi-modal senses such as sound, vision. In this paper, we study interaction force estimation using haptic sensor and video. Interact ion force estimation through video analysis is one of a cross-modal approach that is applicable such as a software haptic feedback method that can give haptic feedback to remote control of robot arm by predicting interaction force even in absence of haptic sensor. we compare and analyze three types of a deep neural network to predict the interaction force. In particular, the best model for the stacking structure of CNN and LSTM is selected through a detailed analysis of how the structure change of LSTM affects the video regression problem. The average error of the best suit model is MSE 0.1306, RMSE 0.2740, MAE 0.1878.",https://ieeexplore.ieee.org/document/8932854/,2019 7th International Conference on Robot Intelligence Technology and Applications (RiTA),1-3 Nov. 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISMAR.2012.6402531,Wide-area scene mapping for mobile visual tracking,IEEE,Conferences,"We propose a system for easily preparing arbitrary wide-area environments for subsequent real-time tracking with a handheld device. Our system evaluation shows that minimal user effort is required to initialize a camera tracking session in an unprepared environment. We combine panoramas captured using a handheld omnidirectional camera from several viewpoints to create a point cloud model. After the offline modeling step, live camera pose tracking is initialized by feature point matching, and continuously updated by aligning the point cloud model to the camera image. Given a reconstruction made with less than five minutes of video, we achieve below 25 cm translational error and 0.5 degrees rotational error for over 80% of images tested. In contrast to camera-based simultaneous localization and mapping (SLAM) systems, our methods are suitable for handheld use in large outdoor spaces.",https://ieeexplore.ieee.org/document/6402531/,2012 IEEE International Symposium on Mixed and Augmented Reality (ISMAR),5-8 Nov. 2012,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CSSE.2008.1264,Widest K-Shortest Paths Q-Routing: A New QoS Routing Algorithm in Telecommunication Networks,IEEE,Conferences,"Actually, various kinds of sources (such as voice, video or data) with diverse traffic characteristics and quality of service requirements (QoS), which are multiplexed at very high rates, leads to significant traffic problems such as packet losses, transmission delays, delay variations, etc, caused mainly by congestion in the networks. The prediction of these problems in real time is quite difficult, making the effectiveness of ""traditional"" methodologies based on analytical models questionable. This article proposed and evaluates a QoS routing policy in packets topology and irregular traffic of communications network called widest K-shortest paths Q-routing. The technique used for the evaluation signals of reinforcement is Q-learning. Compared to standard Q-routing, the exploration of paths is limited to K best non loop paths in term of hops number (number of routers in a path) leading to a substantial reduction of convergence time. In this work a proposal for routing which improves the delay factor and is based on the reinforcement learning is concerned. We use Q-learning as the reinforcement learning technique and introduce K-shortest idea into the learning process. The proposed algorithm are applied to two different topologies. The OPNET is used to evaluate the performance of the proposed algorithm. The algorithm evaluation is done for two traffic conditions, namely low load and high load.",https://ieeexplore.ieee.org/document/4722795/,2008 International Conference on Computer Science and Software Engineering,12-14 Dec. 2008,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.23919/EuCAP53622.2022.9769195,Wireless Capsule Video Endoscopy For Population-Based Colon Cancer Screening Using 5G network,IEEE,Conferences,"We develop a battery-free communication system for a wireless video capsule endoscope with potential video streaming of a rate of up to 15 Mbps. We apply our innovative approach using backscatter for implants, a RADAR approach that remotely reads the information from the deep implants, such as the video capsule endoscope. We use the 5G deployed network with edge computing and slicing to transmit the video data from the capsule to a high-performance computing platform in a secure way with guaranteed end-to-end latency to perform polyp detection and localization. This way, the energy-intensive inference using deep learning neural networks for polyp detection and localization can be completed in the edge where control signals are sent back to the pill to increase the spatial and temporal resolution of the video, to obtain high-quality images for further analysis. The paper provides a system-level description, demonstrating the capsule data streaming to the network.",https://ieeexplore.ieee.org/document/9769195/,2022 16th European Conference on Antennas and Propagation (EuCAP),27 March-1 April 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/CCNC49032.2021.9369651,Wireless Channel Quality Prediction using Sparse Gaussian Conditional Random Fields,IEEE,Conferences,"Accurate wireless channel quality prediction over 4G LTE networks continues to be an important problem as future channel predictions are widely leveraged to meet the strict requirements of applications such as 360-degree video, ARlVR, and online games. The availability of large amounts of wireless channel data, the increase in computational power and the advancements in the field of machine learning provide us the opportunity to design learning-based approaches to address the channel quality prediction problem. In this paper, we design discriminative sequence-to-sequence probabilistic graphical models, specifically sparse Gaussian Conditional Random Fields (GCRF) models to accurately predict future channel quality variations in 4G LTE networks based on past channel quality data. In contrast to prior work that has primarily focused on designing parsimonious Markovian models or computationally-intensive deep learning models, the sparse GCRF models designed here provide superior performance while being highly interpretable and computationally efficient, thus making them an ideal choice for practical deployment. To validate the efficacy of our sparse GCRF model, we compare its performance (i.e., root mean squared error and mean absolute error) with i) linear regression and ii) ARIMA and iii) the state-of-the-art deep learning model on real-world 4G LTE channel quality data collected under varying levels of user mobility for two cellular operators and observe that the GCRF model provides significantly higher performance improvement.",https://ieeexplore.ieee.org/document/9369651/,2021 IEEE 18th Annual Consumer Communications & Networking Conference (CCNC),9-12 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/IPEC51340.2021.9421075,Wireless Mesh Video Based on Neural Network in Cloud Edge Collaborative Management and Control Software,IEEE,Conferences,"In the cloud edge collaborative management and control software, due to the large amount of wireless mesh video data and high requirements for real-time, packet loss is easy to occur in the transmission process. How to improve the transmission reliability of wireless mesh video has always been a research hotspot. Aiming at the problem that the reliability of data transmission in cloud edge collaborative management and control software is not high, this paper proposes a wireless mesh network load balancing protocol NNP based on neural network prediction model_L2MPM. The protocol takes the length of MAC interface queue as the measure of traffic load, and then uses RBF neural network prediction model to predict the traffic load of nodes in mesh network. In order to avoid the congestion in the next time, we can improve the performance of the network according to the load of the next node. The simulation results show that: the protocol uses RBF neural network prediction model to achieve the node traffic load prediction, and then achieves load balancing. Experiments show that nnp-l2mpm protocol has good performance.",https://ieeexplore.ieee.org/document/9421075/,"2021 IEEE Asia-Pacific Conference on Image Processing, Electronics and Computers (IPEC)",14-16 April 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICICICT46008.2019.8993200,Wireless Real Time Suspicious Activity Detection using Smart Glass,IEEE,Conferences,"Ahstract- Today we live in a less secure world. That being said we are constantly under some threat, be it accidents on road or robbery etc. There are number of security systems which are installed to tackle these problems. Instead they record video and consume memory. It does not give any implication about the incident. To tackle these problems a real time suspicious activity detection system should be developed. This system will have advantage over conventional system as it will continuously monitor the frame from particular camera. This can be implemented in any field using less amount of hardware. The system which we are designing is used to monitor the events taking place in frame of camera using image processing. In this paper we are using a Raspberry Pi as our main processor to which camera will be interfaced.",https://ieeexplore.ieee.org/document/8993200/,"2019 2nd International Conference on Intelligent Computing, Instrumentation and Control Technologies (ICICICT)",5-6 July 2019,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ISCMI51676.2020.9311597,World Coordinate Virtual Traffic Cameras: Edge-Based Transformation and Merging of Multiple Surveillance Video Sources,IEEE,Conferences,"Video cameras are an important high-fidelity source of surveillance information. They are especially useful in traffic monitoring scenarios in smart cities to reduce congestion, regulate traffic and enforce regulations. Unfortunately, increasing the number of cameras at a specific location makes it harder to keep quality attention on the scene due to high amount of unstructured data mixed with privacy breaching non-essential data, like faces of pedestrians etc. In this paper, we propose a method for real-time merging of multiple surveillance video sources to extract the required information in a single, world coordinate-based, virtual traffic camera view. Such a composite view allows for future improvements in quality of critical details using techniques as super resolution, while at the same time removing unnecessary private information. The implementation is validated on real world traffic data using NVIDIA Jetson TX2 as an edge device and consists of perspective transformation, image merging and object detection/tracking using YOLOv4 machine learning model for the extraction of significant objects only.",https://ieeexplore.ieee.org/document/9311597/,2020 7th International Conference on Soft Computing & Machine Intelligence (ISCMI),14-15 Nov. 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICCE48956.2021.9352144,YOLO Based Real-Time Human Detection for Smart Video Surveillance at the Edge,IEEE,Conferences,"Recently, smart video surveillance at the edge has become a trend in developing security applications since edge computing enables more image processing tasks to be implemented on the decentralised network note of the surveillance system. As a result, many security applications such as behaviour recognition and prediction, employee safety, perimeter intrusion detection and vandalism deterrence can minimise their latency or even process in real-time when the camera network system is extended to a larger degree. Technically, human detection is a key step in the implementation of these applications. With the advantage of high detection rates, deep learning methods have been widely employed on edge devices in order to detect human objects. However, due to their high computation costs, it is challenging to apply these methods on resource limited edge devices for real-time applications. Inspired by the You Only Look Once (YOLO), residual learning and Spatial Pyramid Pooling (SPP), a novel form of real-time human detection is presented in this paper. Our approach focuses on designing a network structure so that the developed model can achieve a good trade-off between accuracy and processing time. Experimental results show that our trained model can process 2 FPS on Raspberry PI 3B and detect humans with accuracies of 95.05 % and 96.81 % when tested respectively on INRIA and PENN FUDAN datasets. On the human COCO test dataset, our trained model outperforms the performance of the Tiny-YOLO versions. Additionally, compare to the SSD based L-CNN method, our algorithm achieves better accuracy than the other method.",https://ieeexplore.ieee.org/document/9352144/,2020 IEEE Eighth International Conference on Communications and Electronics (ICCE),13-15 Jan. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/EAIS48028.2020.9122693,YOLO TrashNet: Garbage Detection in Video Streams,IEEE,Conferences,"Cities today are beginning their transformation into ""smart cities"". Beside smart traffic, lighting, and energy management, smart waste is an integral part of any smart city. Particular attention should be given to the abandoned garbage both in public city areas or in places outside of town, such as countryside or suburban roads. Beyond causing the degradation of the area, abandoned garbage can cause pollution and have a negative impact on the quality of life of residents in these areas. Our work focuses on developing a software that is able to detect and report the presence of abandoned waste through the analysis of video streams in real-time. An improved YOLOv3 network model is adopted to perform garbage detection and recognition. The network has been fine-tuned on dataset collected for this purpose. The results show that the proposed approach may represent a major contribution for a more efficient waste management in smart cities.",https://ieeexplore.ieee.org/document/9122693/,2020 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS),27-29 May 2020,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ComPE53109.2021.9752229,YOLOv3 based Real Time Social Distance Violation Detection in Public Places,IEEE,Conferences,"The prevalent COVID 19 pandemic is incessantly taking toll on the lives of people throughout the world. Moreover, the dearth of effectual remedies has caused an expeditious rise in the total COVID 19 cases. Though vaccines have been developed, the enormous task of vaccinating a large population is still challenging. Also, as new variants emanate, the resilience from infections conceivably decreases. Hence, it&#x2019;s most unlikely that we&#x2019;ll achieve herd immunity globally so soon. Thus, since the transmission of COVID causing coronavirus roots mainly to social proximity between people, it is necessary to stringently comply to the non pharmaceutical preventive measures of wearing masks and maintaining physical distancing. Howbeit, it has evidently been found that people are being lethargically ignorant to the social distancing norms with passing time. Hence, an autonomous mechanism intended at social distancing violation detection through monitoring of people is needed to be introduced at an authority level. In this paper, the implementation of YOLO Object detection transfer learning process has been used for accomplishing this aim of real time detection of social distancing violation. Our social distance prediction approach uses a pre-trained YOLOv3 object tracking algorithm for identifying people in an input video stream. A Distance estimation algorithm is further used, that works by computing euclidean distance between the centroids of each pair of detected people. This approach highlights the people violating the social distancing criteria as well as calculates the number of times social distancing gets violated as any two people get closer than a set threshold value of minimum permissible distance. A number of experiments on various pre-recorded video streams has been conducted in order to estimate the viability of this method. Through experimental outcomes, it has been found that this YOLO based object detection method with the proposed social distance prediction algorithm produces favourable results for tracking social distancing in public spaces.",https://ieeexplore.ieee.org/document/9752229/,2021 International Conference on Computational Performance Evaluation (ComPE),1-3 Dec. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICACC-202152719.2021.9708269,YOLOv5 based Open-Source UAV for Human Detection during Search And Rescue (SAR),IEEE,Conferences,"Floods and earthquakes are among the most frequently occurring natural disasters. They account for high mortality rates due to their rapidness and uncertainty of occurrence. Inundated lands require a quick response for rapid evacuation, arresting fatalities, and consequential economic losses. People tend to seek shelter at dry and open lands at times of calamity. The manual Search and Rescue (SAR) operations have their shortcomings due to the difficulties in identifying the human presence. It requires a longer time for evacuation and therefore increased mortalities. This paper proposes a quadcopter for real-time monitoring of isolated places and automatically detecting stranded humans during floods using image processing techniques at affordable rates. Live video streaming is possible with a camera and a video transmission system attached to the quadcopter. The rescue centers automatically receive the location of humans in case of human detection. Our model integrates an Open-Source autopilot system model, APM 2.8 multicopter flight controller that efficiently stabilizes the flight, and a YOLOv5 object tracking convolutional neural network algorithm for faster detection of human beings. The model is trained using a dedicated dataset of more than 1000 images and attains 0.954 mAP. We have developed a drone using open-source hardware and software tools, conducted test flights to check its stability and the efficiency of the object detection algorithm. We also conducted a mini-survey of one of the most flood-prone areas of Thrissur district in Kerala, using Mission Planner open-source software to evaluate how quickly our drone can assess the entire area. The aim is to save more human lives by quick and efficient aerial assessment in the most cost-efficient manner.",https://ieeexplore.ieee.org/document/9708269/,2021 International Conference on Advances in Computing and Communications (ICACC),21-23 Oct. 2021,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
10.1109/ICSE-Companion55297.2022.9793829,gDefects4DL: A Dataset of General Real-World Deep Learning Program Defects,IEEE,Conferences,"The development of deep learning programs, as a new programming paradigm, is observed to suffer from various defects. Emerging research works have been proposed to detect, debug, and repair deep learning bugs, which drive the need to construct the bug benchmarks. In this work, we present gDefects4DL, a dataset for general bugs of deep learning programs. Comparing to existing datasets, gDefects4DL collects bugs where the root causes and fix solutions can be well generalized to other projects. Our general bugs include deep learning program bugs such as (1) violation of deep learning API usage pattern (e.g., the standard to implement cross entropy function y&#x2022;log(y), y &#x2192; 0, without NaN error), (2) shape-mismatch of tensor calculation, (3) numeric bugs, (4) type-mismatch (e.g., confusing similar types among numpy, pytorch, and tensorflow), (5) violation of model architecture design convention, and (6) performance bugs.For each bug in gDefects4DL, we describe why it is general and group the bugs with similar root causes and fix solutions for reference. Moreover, gDefects4DL also maintains (1) its buggy/fixed versions and the isolated fix change, (2) an isolated environment to replicate the defect, and (3) the whole code evolution history from the buggy version to the fixed version. We design gDefects4DL with extensible interfaces to evaluate software engineering methodologies and tools. We have integrated tools such as ShapeFlow, DEBAR, and GRIST. gDefects4DL contains 64 bugs falling into 6 categories (i.e., API Misuse, Shape Mismatch, Number Error, Type Mismatch, Violation of Architecture Convention, and Performance Bug). gDefects4DL is available at https://github.com/llmhyy/defects4dl, its online web demonstration is at http://47.93.14.147:9000/bugList, and the demo video is at https://youtu.be/0XtaEt4Fhm4.",https://ieeexplore.ieee.org/document/9793829/,2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),22-24 May 2022,ieeexplore,multimedia,'multimedia' AND 'machine learning' AND ('real-world' AND 'deploy')
